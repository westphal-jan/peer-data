{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Paper Judge BoW-Abstracts.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lym_glPd23n7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61KeHP4iDHOc"
      },
      "source": [
        "%cd /content/\n",
        "!git clone -b reduced-data https://github.com/westphal-jan/peer-data\n",
        "%cd /content/peer-data\n",
        "# !git checkout huggingface\n",
        "!git submodule update --init --recursive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzxkH1uGFGKb"
      },
      "source": [
        "# !pip install pytorch-lightning wandb python-dotenv catalyst sentence-transformers numpy requests nlpaug sentencepiece nltk\n",
        "!pip install wandb nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ludytPTqS4"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, AdamW\n",
        "import wandb\n",
        "# from datetime import datetime\n",
        "# import pickle\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "# import nlpaug.augmenter.word as naw\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "# from copy import copy\n",
        "import copy\n",
        "from datetime import datetime\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDvvl-q839tO"
      },
      "source": [
        "class PaperDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]).float() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZktYlUTlqq"
      },
      "source": [
        "def read_dataset(data_dir: Path, num_texts=None, restrict_file=None):\n",
        "    file_paths = glob.glob(f\"{data_dir}/*.json\")\n",
        "    if restrict_file:\n",
        "        with open(restrict_file, \"r\") as f:\n",
        "            filter_file_names = f.read().splitlines()\n",
        "            file_paths = [p for p in file_paths if p.split(\"/\")[-1] in filter_file_names]\n",
        "\n",
        "    if num_texts != None:\n",
        "        file_paths = file_paths[:num_texts]\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for i, file_path in enumerate(tqdm(file_paths)):\n",
        "        with open(file_path) as f:\n",
        "            paper_json = json.load(f)\n",
        "            accepted = paper_json[\"review\"][\"accepted\"]\n",
        "            abstract = paper_json[\"review\"][\"abstract\"]\n",
        "            \n",
        "            texts.append(abstract)\n",
        "            labels.append(int(accepted))\n",
        "    return texts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRG6QKXXT3lF"
      },
      "source": [
        "# dataset = \"data/original\"\n",
        "\n",
        "# data_dir = Path(dataset)\n",
        "# texts, labels = read_dataset(data_dir)\n",
        "# texts, labels = np.array(texts), np.array(labels)\n",
        "\n",
        "# num_accepted = len(list(filter(lambda x: x == 1, labels)))\n",
        "# num_not_accepted = len(list(filter(lambda x: x == 0, labels)))\n",
        "\n",
        "# print(num_accepted, num_not_accepted)\n",
        "# label_weight = num_not_accepted / np.array([num_not_accepted, num_accepted])\n",
        "\n",
        "# # Get random index split for train/val/test.\n",
        "# idx = list(range(len(texts)))\n",
        "# # Get constant split across runs\n",
        "# rnd = np.random.RandomState(42)\n",
        "# rnd.shuffle(idx)\n",
        "# total_len = len(idx)\n",
        "# train_len, val_len = int(0.8*total_len), int(0.1*total_len)\n",
        "# train_idx = idx[:train_len]\n",
        "# val_idx = idx[train_len:(train_len + val_len)]\n",
        "# test_idx = idx[(train_len + val_len):]\n",
        "\n",
        "# train_texts, train_labels = texts[train_idx], labels[train_idx]\n",
        "# val_texts, val_labels = texts[val_idx], labels[val_idx]\n",
        "# text_texts, test_labels = texts[test_idx], labels[test_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oas1gPizlOS9"
      },
      "source": [
        "data_dir = Path(\"data/original\")\n",
        "\n",
        "train_texts, train_labels = read_dataset(data_dir, restrict_file=\"data/train.txt\")\n",
        "val_texts, val_labels = read_dataset(data_dir, restrict_file=\"data/val.txt\")\n",
        "test_texts, test_labels = read_dataset(data_dir, restrict_file=\"data/test.txt\")\n",
        "# val_texts, val_labels = read_dataset(data_dir, restrict_file=\"data/test.txt\")\n",
        "# test_texts, test_labels = read_dataset(data_dir, restrict_file=\"data/val.txt\")\n",
        "\n",
        "num_accepted = len(list(filter(lambda x: x == 1, train_labels)))\n",
        "num_rejected = len(list(filter(lambda x: x == 0, train_labels)))\n",
        "\n",
        "print(num_accepted, num_rejected)\n",
        "label_weight = num_rejected / np.array([num_rejected, num_accepted])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57HZY0nxv8U8"
      },
      "source": [
        "def label_distribution(labels):\n",
        "    num_rejected, num_accepted = labels.count(0), labels.count(1)\n",
        "    print(num_rejected, num_rejected / len(labels), num_accepted, num_accepted / len(labels))\n",
        "\n",
        "label_distribution(train_labels)\n",
        "label_distribution(val_labels)\n",
        "label_distribution(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OCSAoEeFlQ1"
      },
      "source": [
        "label_weight[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GfVVi3txDI6"
      },
      "source": [
        "special_characters = set(['-', '\\'', '.', ',', '!','\"','#','$','%','&','(',')','*','+','/',':',';','<','=','>','@','[','\\\\',']','^','`','{','|','}','~','\\t'])\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize(text: str, token_to_id=None, encode=False):\n",
        "    for c in special_characters:\n",
        "        text = text.replace(c, ' ')\n",
        "    text = text.lower()\n",
        "    tokens = [t for t in text.split(' ') if t]\n",
        "    # tokens = [t for t in tokens if t.isalpha()]\n",
        "    tokens = [t for t in tokens if not t in english_stopwords]\n",
        "    if token_to_id:\n",
        "        tokens = [t for t in tokens if t in token_to_id]\n",
        "        if encode:\n",
        "            tokens = [token_to_id[t] for t in tokens]\n",
        "            onehot = np.zeros(len(token_to_id))\n",
        "            onehot[tokens] = 1\n",
        "            return onehot\n",
        "    return tokens\n",
        "\n",
        "train_tokens = list(map(tokenize, train_texts))\n",
        "flattened_tokens = [item for sublist in train_tokens for item in sublist]\n",
        "vocab = sorted(list(set(flattened_tokens)))\n",
        "token_to_id = {t: i for i, t in enumerate(vocab)}\n",
        "val_tokens = list(map(lambda x: tokenize(x, token_to_id), val_texts))\n",
        "test_tokens = list(map(lambda x: tokenize(x, token_to_id), test_texts))\n",
        "print(\"Vocab Size:\", len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAfZ_-Hb0pYh"
      },
      "source": [
        "train_encodings = list(map(lambda x: tokenize(x, token_to_id, True), train_texts))\n",
        "val_encodings = list(map(lambda x: tokenize(x, token_to_id, True), val_texts))\n",
        "test_encodings = list(map(lambda x: tokenize(x, token_to_id, True), test_texts))\n",
        "\n",
        "train_dataset = PaperDataset({\"tokens\": train_encodings}, train_labels)\n",
        "val_dataset = PaperDataset({\"tokens\": val_encodings}, val_labels)\n",
        "test_dataset = PaperDataset({\"tokens\": test_encodings}, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7_qdTMs8549"
      },
      "source": [
        "samples_weight = [label_weight[t] for t in train_labels]\n",
        "samples_weight = torch.Tensor(samples_weight)\n",
        "\n",
        "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgxRi1Rcf2_5"
      },
      "source": [
        "def compute_metrics(eval_pred, prefix=\"eval\"):\n",
        "    logits, labels = eval_pred\n",
        "    # predictions = np.argmax(logits, axis=1)\n",
        "    predictions = np.array(logits) >= 0\n",
        "    actual = np.array(labels)\n",
        "\n",
        "    tp = ((predictions == 1) & (actual == 1)).sum()\n",
        "    fp = ((predictions == 1) & (actual == 0)).sum()\n",
        "    fn = ((predictions == 0) & (actual == 1)).sum()\n",
        "    tn = ((predictions == 0) & (actual == 0)).sum()\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    mcc = (tp * tn - fp * fn) / (((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))**0.5)\n",
        "    mcc = 0.0 if np.isnan(mcc) else mcc\n",
        "    p = tp + fn\n",
        "    n = tn + fp\n",
        "    metrics = {\"metric/precision\": precision, \"metric/recall\": recall, \"metric/f1\": f1, \"metric/accuracy\": accuracy,\n",
        "            \"classification/tp\": tp, \"classification/fp\": fp, \"classification/fn\": fn, \"classification/tn\": tn, \"classification/n\": n, \"classification/p\": p}\n",
        "    metrics = {f\"{prefix}/{key}\": metric for key, metric in metrics.items()}\n",
        "    # \"augmentation/train\": train_dataset.num_augmentations, \"augmentation/val\": val_dataset.num_augmentations\n",
        "    return metrics\n",
        "\n",
        "logits = [0, 0]\n",
        "labels = [0, 1]\n",
        "\n",
        "result = compute_metrics((logits, labels))\n",
        "print(result)\n",
        "\n",
        "# my_callback = MyCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2aycOMA51io"
      },
      "source": [
        "class BowClassifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size=1):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.classifier = nn.Linear(self.input_size, self.output_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.classifier(x)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7iddk0kKS7x"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = BowClassifier(len(vocab))\n",
        "model.to(device)\n",
        "\n",
        "train_batch_size, val_batch_size = 64, 64\n",
        "use_sampler = False\n",
        "is_unweighted = False\n",
        "if use_sampler:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, sampler=sampler)\n",
        "else:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "# _label_weight = torch.from_numpy(label_weight).float().to(device)\n",
        "# loss_func = torch.nn.CrossEntropyLoss(_label_weight, reduction=\"sum\")\n",
        "pos_weight = torch.tensor(label_weight[1]).float().to(device)\n",
        "weighted_loss_func = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "if use_sampler or is_unweighted:\n",
        "    print(\"Use unweighted BCE\")\n",
        "    weighted_name = \"unweighted\"\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    if is_unweighted:\n",
        "        weighted_loss_func = loss_func\n",
        "else:\n",
        "    print(\"Use weighted BCE\")\n",
        "    weighted_name = \"weighted\"\n",
        "    loss_func = weighted_loss_func\n",
        "\n",
        "wandb_logging = False\n",
        "\n",
        "run_name = datetime.now().strftime('%d-%m-%Y_%H_%M_%S')\n",
        "optimizer_name = \"adam\" if isinstance(optimizer, optim.AdamW) else \"sgd\"\n",
        "oversampling_name = \"-oversampling\" if use_sampler else \"\"\n",
        "num_train_epochs = 20\n",
        "run_name += f\"-{optimizer_name}-e{num_train_epochs}-b{train_batch_size}-{weighted_name}{oversampling_name}-remove_stopwords\"\n",
        "print(\"Run name:\", run_name)\n",
        "if wandb_logging:\n",
        "    wandb.login()\n",
        "    wandb.init(entity=\"paper-judging\", project=\"final-evaluation-bow\", name=run_name)\n",
        "\n",
        "output_dir=f'results/{run_name}'\n",
        "os.makedirs(output_dir)\n",
        "\n",
        "# logging_steps, eval_steps = 10, 50\n",
        "# logging_steps, eval_steps = 20, 200\n",
        "# logging_steps, eval_steps = 5, 10\n",
        "logging_steps, eval_steps = 50, len(train_loader)\n",
        "_steps = 0\n",
        "\n",
        "def run_model(_model, _batch, _loss_func=loss_func):\n",
        "    inputs = {key: val.to(device) for key, val in _batch.items()}\n",
        "    labels = inputs.pop(\"labels\").float()\n",
        "    tokens = inputs.pop(\"tokens\")\n",
        "    outputs = _model(tokens)\n",
        "\n",
        "    logits = outputs.squeeze(dim=1)\n",
        "    loss = _loss_func(logits, labels)\n",
        "    return loss, logits\n",
        "\n",
        "min_val_loss, best_model, best_metrics, best_loss_epoch = None, None, None, None\n",
        "best_f1, best_epoch = None, None\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch: {epoch + 1}/{num_train_epochs}\")\n",
        "    train_losses = []\n",
        "    for batch in tqdm(train_loader, position=0):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss, _ = run_model(model, batch)\n",
        "        train_losses.append(train_loss)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metrics = {}\n",
        "        if _steps % logging_steps == 0:\n",
        "            _train_loss = sum(train_losses) / len(train_losses)\n",
        "            metrics.update({\"train/loss\": _train_loss.item(), \"steps\": _steps})\n",
        "            train_losses = []\n",
        "        \n",
        "        if _steps % eval_steps == 0:\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "            logits = []\n",
        "            with torch.no_grad():\n",
        "                for val_batch in val_loader:\n",
        "                    _val_loss, _logits = run_model(model, val_batch, weighted_loss_func)\n",
        "                    val_losses.append(_val_loss.item())\n",
        "                    logits.extend(_logits.tolist())\n",
        "\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            _metrics = compute_metrics((logits, val_labels))\n",
        "            metrics[\"eval/loss\"] = val_loss\n",
        "            metrics.update(_metrics)\n",
        "\n",
        "            if min_val_loss == None:\n",
        "                min_val_loss = val_loss\n",
        "            elif min_val_loss > val_loss:\n",
        "                min_val_loss = val_loss\n",
        "                best_loss_epoch = epoch\n",
        "\n",
        "            f1 = metrics[\"eval/metric/f1\"]\n",
        "            if best_f1 == None:\n",
        "                best_f1 = f1\n",
        "            elif f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_model = copy.deepcopy(model)\n",
        "                best_metrics = metrics\n",
        "                best_epoch = epoch\n",
        "        \n",
        "        if metrics:\n",
        "            print(metrics)\n",
        "            if wandb_logging:\n",
        "                wandb.log(metrics)\n",
        "\n",
        "        _steps += 1\n",
        "\n",
        "print(f\"Best val metrics during training epoch {best_epoch}:\")\n",
        "print(best_metrics)\n",
        "test_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)\n",
        "best_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = []\n",
        "    for val_batch in val_loader:\n",
        "        _, _logits = run_model(best_model, val_batch)\n",
        "        logits.extend(_logits.tolist())\n",
        "    metrics = compute_metrics((logits, val_labels))\n",
        "    print(\"Val metrics:\")\n",
        "    print(metrics)\n",
        "\n",
        "    logits = []\n",
        "    for test_batch in test_loader:\n",
        "        _, _logits = run_model(best_model, test_batch)\n",
        "        logits.extend(_logits.tolist())\n",
        "    metrics = compute_metrics((logits, test_labels))\n",
        "    print(\"Test metrics:\")\n",
        "    print(metrics)\n",
        "\n",
        "# snapshot_dir = f\"results/{run_name}/network-snapshot-latest\"\n",
        "# model.save_pretrained(snapshot_dir)\n",
        "\n",
        "if wandb_logging:\n",
        "    # wandb.save(f\"{snapshot_dir}/*\", os.path.dirname(snapshot_dir))\n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubcp815R3LWK"
      },
      "source": [
        "# wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBywLtI0GgET"
      },
      "source": [
        "params = list(model.parameters())[0][0]\n",
        "k = 20\n",
        "val, ind = params.topk(k)\n",
        "print(\"Positive:\")\n",
        "for i in ind:\n",
        "    print(vocab[i], params[i].item())\n",
        "print()\n",
        "print(\"Negative:\")\n",
        "val, ind = (-params).topk(k)\n",
        "for i in ind:\n",
        "    print(vocab[i], params[i].item())\n",
        "\n",
        "print()\n",
        "print(\"Unnecessary:\")\n",
        "val, ind = (-(params.abs())).topk(k)\n",
        "for i in ind:\n",
        "    print(vocab[i], params[i].item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4o3YqkK3JuH"
      },
      "source": [
        "def find_word(word, tokens, labels):\n",
        "    total = [0, 0]\n",
        "    classes = [0, 0]\n",
        "    for i, tokens in enumerate(tokens):\n",
        "        tokens = set(tokens)\n",
        "        label = labels[i]\n",
        "        if word in tokens:\n",
        "            classes[label] += 1\n",
        "        total[label] += 1\n",
        "    # print(f\"Not accepted: {classes[0]}/{total[0]} ({classes[0]/total[0]}), Accepted: {classes[1]}/{total[1]} ({classes[1]/total[1]})\")\n",
        "    return classes[0]/total[0], classes[1]/total[1]\n",
        "\n",
        "def analyze(params, k, tokens, labels):\n",
        "    val, ind = params.topk(k)\n",
        "    for i in ind:\n",
        "        word = vocab[i]\n",
        "        rejected, accepted = find_word(word, tokens, labels)\n",
        "        n = 2\n",
        "        print(f\"{word} & {np.round(params[i].item(), 3)} & {np.round(rejected*100, n)} & {np.round(accepted*100, n)}\")\n",
        "\n",
        "tokens = train_tokens + val_tokens + test_tokens\n",
        "labels = train_labels + val_labels + test_labels\n",
        "params = list(model.parameters())[0][0]\n",
        "k = 8\n",
        "\n",
        "print(\"Positive:\")\n",
        "analyze(params, k, tokens, labels)\n",
        "print()\n",
        "\n",
        "print(\"Negative:\")\n",
        "analyze(-params, k, tokens, labels)\n",
        "print()\n",
        "\n",
        "print(\"Unnecessary:\")\n",
        "val, ind = (-(params.abs())).topk(k)\n",
        "for i in ind:\n",
        "    print(vocab[i], params[i].item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1bBz90e303_"
      },
      "source": [
        "def find_word(word, all_tokens, labels):\n",
        "    total = [0, 0]\n",
        "    classes = [0, 0]\n",
        "    for i, tokens in enumerate(all_tokens):\n",
        "        tokens = set(tokens)\n",
        "        label = labels[i]\n",
        "        if word in tokens:\n",
        "            classes[label] += 1\n",
        "        total[label] += 1\n",
        "    print(f\"Not accepted: {classes[0]}/{total[0]} ({classes[0]/total[0]}), Accepted: {classes[1]}/{total[1]} ({classes[1]/total[1]})\")\n",
        "\n",
        "word = \"theoretical\"\n",
        "print(\"Train:\")\n",
        "find_word(word, train_tokens, train_labels)\n",
        "print(\"Val:\")\n",
        "find_word(word, val_tokens, val_labels)\n",
        "print(\"Test:\")\n",
        "find_word(word, test_tokens, test_labels)\n",
        "\n",
        "n = 10\n",
        "for tokens, text in zip(train_tokens, train_texts):\n",
        "    if word in tokens:\n",
        "        print(text)\n",
        "        n -= 1\n",
        "    if n == 0:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXJP6lSnhSJU"
      },
      "source": [
        "paper_abstract = \"\"\"Generative adversarial networks (GANs) have shown\n",
        "outstanding performance on a wide range of problems in\n",
        "computer vision, graphics, and machine learning, but often require numerous training data and heavy computational resources. To tackle this issue, several methods introduce a transfer learning technique in GAN training. They,\n",
        "however, are either prone to overfitting or limited to learning small distribution shifts. In this paper, we show that\n",
        "simple fine-tuning of GANs with frozen lower layers of\n",
        "the discriminator performs surprisingly well. This simple\n",
        "baseline, FreezeD, significantly outperforms previous techniques used in both unconditional and conditional GANs.\n",
        "We demonstrate the consistent effect using StyleGAN and\n",
        "SNGAN-projection architectures on several datasets of Animal Face, Anime Face, Oxford Flower, CUB-200-2011, and\n",
        "Caltech-256 datasets. The code and results are available at\n",
        "https://github.com/sangwoomo/FreezeD.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDqbwDCReV9a"
      },
      "source": [
        "v_idx = 9\n",
        "input = [\"It's incredibly bad\", paper_abstract, \"hello\", \"darkness\", val_texts[v_idx]]\n",
        "print(val_labels[v_idx])\n",
        "print(val_texts[v_idx])\n",
        "print(tokenize(val_texts[v_idx]))\n",
        "encoded = list(map(lambda x: tokenize(x, token_to_id, True), input))\n",
        "encoded = torch.tensor(encoded).float()\n",
        "# print(encoded)\n",
        "output = model(encoded)\n",
        "print(output)\n",
        "# print(torch.softmax(output.logits, dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti9x-w_erAaX"
      },
      "source": [
        "# prediction = output.logits.argmax(dim=1)\n",
        "# actual = prediction\n",
        "# n = tp = fp = fn = tn = 0\n",
        "# tp += (prediction == 1) & (actual == 1)\n",
        "# fp += (prediction == 1) & (actual == 0)\n",
        "# fn += (prediction == 0) & (actual == 1)\n",
        "# tn += (prediction == 0) & (actual == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ews38ukTEbpb"
      },
      "source": [
        "# !python huggingface_train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy500R68NdLG"
      },
      "source": [
        "# LIME\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-vIZ4vhNfTX"
      },
      "source": [
        "import numpy as np\n",
        "import lime\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "class_names = ['positive','negative', 'neutral']\n",
        "\n",
        "def predictor(texts):\n",
        "outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
        "probas = F.softmax(outputs.logits).detach().numpy()\n",
        "return probas\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "str_to_predict = \"surprising increase in revenue in spite of decrease in market share\"\n",
        "exp = explainer.explain_instance(str_to_predict, predictor, num_features=20, num_samples=2000)\n",
        "exp.show_in_notebook(text=str_to_predict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}