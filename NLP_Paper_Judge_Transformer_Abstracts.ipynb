{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Paper Judge Transformer-Abstracts.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lym_glPd23n7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61KeHP4iDHOc"
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/westphal-jan/peer-data\n",
        "%cd /content/peer-data\n",
        "!git submodule update --init --recursive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzxkH1uGFGKb"
      },
      "source": [
        "!pip install pytorch-lightning wandb python-dotenv catalyst sentence-transformers numpy requests nlpaug sentencepiece nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ludytPTqS4"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, AdamW\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nlpaug.augmenter.word as naw\n",
        "from torch.utils.data import DataLoader\n",
        "from copy import copy\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZktYlUTlqq"
      },
      "source": [
        "class PaperDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# class AugmentedPaperDataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, texts, labels, tokenizer, augmenter, aug_p=0.0, seed=0):\n",
        "#         \"\"\"Augmenter needs to have the method 'augment'\"\"\"\n",
        "#         self.texts = texts\n",
        "#         self.labels = labels\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.augmenter = augmenter\n",
        "#         self.encodings = tokenizer(train_texts, truncation=True, padding='max_length', max_length=512)\n",
        "#         self.aug_p = aug_p\n",
        "#         self.usages = [0] * len(texts)\n",
        "#         self.rnd = np.random.RandomState(seed)\n",
        "#         self.num_augmentations = 0\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         if self.usages[idx] > 0 and self.rnd.random_sample() < self.aug_p:\n",
        "#             self.texts[idx] = self.augmenter.augment(self.texts[idx])\n",
        "#             new_encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=512)\n",
        "#             for key, val in self.encodings.items():\n",
        "#                 val[idx] = new_encoding[key]\n",
        "#             self.usages[idx] = 0\n",
        "#             self.num_augmentations += 1\n",
        "        \n",
        "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "#         item['labels'] = torch.tensor(self.labels[idx])\n",
        "#         self.usages[idx] += 1\n",
        "#         return item\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "def read_dataset(data_dir: Path, num_texts=None):\n",
        "    file_paths = glob.glob(f\"{data_dir}/*.json\")\n",
        "    if num_texts != None:\n",
        "        file_paths = file_paths[:num_texts]\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for i, file_path in enumerate(tqdm(file_paths)):\n",
        "        with open(file_path) as f:\n",
        "            paper_json = json.load(f)\n",
        "            accepted = paper_json[\"review\"][\"accepted\"]\n",
        "            abstract = paper_json[\"review\"][\"abstract\"]\n",
        "            \n",
        "            texts.append(abstract)\n",
        "            labels.append(int(accepted))\n",
        "    return texts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRG6QKXXT3lF"
      },
      "source": [
        "dataset = \"data/original\"\n",
        "\n",
        "data_dir = Path(dataset)\n",
        "train_texts, train_labels = read_dataset(data_dir)\n",
        "\n",
        "num_accepted = len(list(filter(lambda x: x == 1, train_labels)))\n",
        "num_not_accepted = len(list(filter(lambda x: x == 0, train_labels)))\n",
        "\n",
        "print(num_accepted, num_not_accepted)\n",
        "label_weight = num_not_accepted / np.array([num_not_accepted, num_accepted])\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = PaperDataset(train_encodings, train_labels)\n",
        "val_dataset = PaperDataset(val_encodings, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cg8qlC28VXY"
      },
      "source": [
        "# back_translation_aug = naw.BackTranslationAug(\n",
        "#     from_model_name='facebook/wmt19-en-de', \n",
        "#     to_model_name='facebook/wmt19-de-en',\n",
        "#     device=\"cuda\",\n",
        "#     max_length=512\n",
        "# )\n",
        "\n",
        "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "# train_dataset = AugmentedPaperDataset(train_texts, train_labels, tokenizer, back_translation_aug, 0.5)\n",
        "# val_dataset = AugmentedPaperDataset(val_texts, val_labels, tokenizer, back_translation_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmm28NNrIVf1"
      },
      "source": [
        "# # Experiment with nlp augmentation\n",
        "# import nlpaug.augmenter.word as naw\n",
        "# import nlpaug.augmenter.sentence as nas\n",
        "\n",
        "# text, label = train_texts[1], train_labels[1]\n",
        "# print(text)\n",
        "# print(len(text.split(\" \")), label)\n",
        "\n",
        "# # Synonym Augmenter\n",
        "# import nltk\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# aug = naw.SynonymAug(aug_src='wordnet', aug_min=10, aug_max=512, aug_p=0.25)\n",
        "# augmented_text = aug.augment(text)\n",
        "# print(augmented_text)\n",
        "\n",
        "# # Back Translation\n",
        "# back_translation_aug = naw.BackTranslationAug(\n",
        "#     from_model_name='facebook/wmt19-en-de', \n",
        "#     to_model_name='facebook/wmt19-de-en',\n",
        "#     device=\"cuda\",\n",
        "#     max_length=512\n",
        "# )\n",
        "# back_translated_text = back_translation_aug.augment(text) # Unique outputs only working partly\n",
        "# print(back_translated_text)\n",
        "\n",
        "# # Contextual Sentence Augmentation\n",
        "# # https://nlpaug.readthedocs.io/en/latest/augmenter/sentence/context_word_embs_sentence.html\n",
        "# # context_aug = nas.ContextualWordEmbsForSentenceAug(model_path='gpt2')\n",
        "# # augmented_text = context_aug.augment(text)\n",
        "# # print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydFXCKC9y8x-"
      },
      "source": [
        "# back_translation_aug.augment(back_translated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY1NMzkQQxL4"
      },
      "source": [
        "# synonym_aug = naw.SynonymAug(aug_src='ppdb', model_path='ppdb-2.0-tldr')\n",
        "# augmented_text = synonym_aug.augment(text)\n",
        "# print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgxRi1Rcf2_5"
      },
      "source": [
        "# class MyCallback(TrainerCallback):\n",
        "\n",
        "#     def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "#         \"\"\"\n",
        "#         Event called after an evaluation phase.\n",
        "#         \"\"\"\n",
        "#         print(\"Callback\")\n",
        "#         eval_dataloader = kwargs[\"eval_dataloader\"]\n",
        "#         eval_model = kwargs[\"model\"]\n",
        "#         step = state.global_step\n",
        "#         eval_model.eval()\n",
        "#         tp = fp = fn = tn = 0\n",
        "#         for x in tqdm(eval_dataloader):\n",
        "#             input_ids = x[\"input_ids\"].to(device)\n",
        "#             output = eval_model(input_ids=input_ids)\n",
        "\n",
        "#             prediction = output.logits.argmax(dim=1).cpu()\n",
        "#             actual = x[\"labels\"]\n",
        "#             tp += ((prediction == 1) & (actual == 1)).sum()\n",
        "#             fp += ((prediction == 1) & (actual == 0)).sum()\n",
        "#             fn += ((prediction == 0) & (actual == 1)).sum()\n",
        "#             tn += ((prediction == 0) & (actual == 0)).sum()\n",
        "        \n",
        "#         precision = tp / (tp + fp)\n",
        "#         recall = tp / (tp + fn)\n",
        "#         f1 = 2 * precision * recall / (precision + recall)\n",
        "#         accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "#         metrics = {\"step\": step, \"precision\": precision, \"recall\": recall, \"f1\": f1,\n",
        "#                    \"accuracy\": accuracy, \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn}\n",
        "#         print(step, metrics)\n",
        "#         try:\n",
        "#             wandb.log(metrics)\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "\n",
        "# class ImballancedLabelTrainer(Trainer):\n",
        "#     def __init__(self, label_weight, **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#         self._loss = torch.nn.CrossEntropyLoss(label_weight, reduction=\"sum\")\n",
        "\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
        "#         labels = inputs.pop(\"labels\")\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.logits\n",
        "#         loss = self._loss(logits, labels)\n",
        "#         return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred, prefix=\"eval\"):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    actual = np.array(labels)\n",
        "\n",
        "    tp = ((predictions == 1) & (actual == 1)).sum()\n",
        "    fp = ((predictions == 1) & (actual == 0)).sum()\n",
        "    fn = ((predictions == 0) & (actual == 1)).sum()\n",
        "    tn = ((predictions == 0) & (actual == 0)).sum()\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    \n",
        "    metrics = {\"metric/precision\": precision, \"metric/recall\": recall, \"metric/f1\": f1, \"metric/accuracy\": accuracy,\n",
        "            \"classification/tp\": tp, \"classification/fp\": fp, \"classification/fn\": fn, \"classification/tn\": tn}\n",
        "    metrics = {f\"{prefix}/{key}\": metric for key, metric in metrics.items()}\n",
        "    # \"augmentation/train\": train_dataset.num_augmentations, \"augmentation/val\": val_dataset.num_augmentations\n",
        "    return metrics\n",
        "\n",
        "logits = [[1.0, 2.5], [1.3, 0.7]]\n",
        "labels = [0, 1]\n",
        "\n",
        "result = compute_metrics((logits, labels))\n",
        "print(result)\n",
        "\n",
        "# my_callback = MyCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7pRdO0ebAC"
      },
      "source": [
        "# training_args = TrainingArguments(\n",
        "#     output_dir=f'test',          # output directory\n",
        "#     num_train_epochs=10,              # total number of training epochs\n",
        "#     per_device_train_batch_size=8,  # batch size per device during training\n",
        "#     per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "#     warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
        "#     weight_decay=0.01,               # strength of weight decay\n",
        "#     logging_dir='./logs',            # directory for storing logs\n",
        "#     logging_steps=20,\n",
        "#     eval_steps=200,\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     report_to=\"none\",\n",
        "#     logging_first_step=True,\n",
        "# )\n",
        "\n",
        "# trainer = ImballancedLabelTrainer(\n",
        "#     model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "#     args=training_args,                  # training arguments, defined above\n",
        "#     train_dataset=train_dataset,         # training dataset\n",
        "#     eval_dataset=val_dataset,            # evaluation dataset\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     label_weight=_label_weight         \n",
        "# )\n",
        "\n",
        "# loader = trainer.get_train_dataloader()\n",
        "# for x in loader:\n",
        "#     x = {key: t.to(device) for key, t in x.items()}\n",
        "#     print()\n",
        "#     break\n",
        "# trainer.compute_loss(model, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGbX7T7RdtYk"
      },
      "source": [
        "# device = torch.device(\"cuda\")\n",
        "\n",
        "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RL5WKGkUw9o"
      },
      "source": [
        "# wandb_logging = True\n",
        "# wandb.login()\n",
        "\n",
        "# run_name = datetime.now().strftime('%d-%m-%Y_%H_%M_%S')\n",
        "# print(\"Run name:\", run_name)\n",
        "# if wandb_logging:\n",
        "#     wandb.init(entity=\"paper-judging\", project=\"huggingface\", name=run_name)\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=f'results/{run_name}',          # output directory\n",
        "#     num_train_epochs=5,              # total number of training epochs\n",
        "#     per_device_train_batch_size=16,  # batch size per device during training\n",
        "#     per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "#     warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
        "#     weight_decay=0.01,               # strength of weight decay\n",
        "#     logging_dir='./logs',            # directory for storing logs\n",
        "#     logging_steps=20,\n",
        "#     eval_steps=200,\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     report_to=\"wandb\" if wandb_logging else \"none\",\n",
        "#     logging_first_step=True,\n",
        "# )\n",
        "\n",
        "# _label_weight = torch.from_numpy(label_weight).float().to(device)\n",
        "# trainer = ImballancedLabelTrainer(\n",
        "#     model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "#     args=training_args,                  # training arguments, defined above\n",
        "#     train_dataset=train_dataset,         # training dataset\n",
        "#     eval_dataset=val_dataset,            # evaluation dataset\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     label_weight=_label_weight         \n",
        "# )\n",
        "\n",
        "# trainer.evaluate()\n",
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOFAch3tUPCU"
      },
      "source": [
        "def augment_texts(_texts, aug_p, augmenter, seed=0, verbose=True):\n",
        "    # TODO: Maybe only encode the augmented texts\n",
        "    texts = np.array(_texts)\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    r = rnd.random_sample(len(texts))\n",
        "    aug_idx = np.argwhere(r <= aug_p).flatten()\n",
        "    if verbose:\n",
        "        print(\"Augmenting:\", len(aug_idx))\n",
        "    texts_to_augment = texts[aug_idx]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        augmented_texts = augmenter.augment(texts_to_augment.tolist())\n",
        "    texts[aug_idx] = augmented_texts\n",
        "    return texts.tolist(), aug_idx.tolist()\n",
        "\n",
        "# back_translation_aug = naw.BackTranslationAug(\n",
        "#     from_model_name='facebook/wmt19-en-de', \n",
        "#     to_model_name='facebook/wmt19-de-en',\n",
        "#     device=\"cuda\",\n",
        "#     max_length=512\n",
        "# )\n",
        "# \n",
        "# augmented_texts = augment_texts(train_texts[:10], aug_p=0.5, augmenter=back_translation_aug)\n",
        "# for text, augmented_text in zip(train_texts[:10], augmented_texts):\n",
        "#     if text != augmented_text:\n",
        "#         print(text)\n",
        "#         print(augmented_text)\n",
        "#         print()\n",
        "# augmented_train_encodings = tokenizer(augmented_train_texts, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7iddk0kKS7x"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model.to(device)\n",
        "\n",
        "train_batch_size, val_batch_size = 16, 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "_train_texts = copy(train_texts)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "_label_weight = torch.from_numpy(label_weight).float().to(device)\n",
        "loss_func = torch.nn.CrossEntropyLoss(_label_weight, reduction=\"sum\")\n",
        "\n",
        "wandb_logging = False\n",
        "\n",
        "run_name = datetime.now().strftime('%d-%m-%Y_%H_%M_%S')\n",
        "print(\"Run name:\", run_name)\n",
        "if wandb_logging:\n",
        "    wandb.login()\n",
        "    wandb.init(entity=\"paper-judging\", project=\"huggingface\", name=run_name)\n",
        "\n",
        "output_dir=f'results/{run_name}'\n",
        "os.makedirs(output_dir)\n",
        "num_train_epochs = 5\n",
        "logging_steps, eval_steps = 20, 200\n",
        "# logging_steps, eval_steps = 5, 10\n",
        "aug_p = 0 # Set to 0 to turn of augmentation\n",
        "_steps = 0\n",
        "\n",
        "if aug_p > 0:\n",
        "    ## Very slow ca. 30 texts / min\n",
        "    # augmenter = naw.BackTranslationAug(\n",
        "    #     from_model_name='facebook/wmt19-en-de', \n",
        "    #     to_model_name='facebook/wmt19-de-en',\n",
        "    #     device=\"cuda\",\n",
        "    #     batch_size=train_batch_size,\n",
        "    #     max_length=512\n",
        "    # )\n",
        "\n",
        "    import nltk\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    augmenter = naw.SynonymAug(aug_src='wordnet', aug_min=10, aug_max=512, aug_p=0.2)\n",
        "\n",
        "def run_model(_batch):\n",
        "    inputs = {key: val.to(device) for key, val in _batch.items()}\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    loss = loss_func(logits, labels)\n",
        "    return loss, logits\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    print(f\"Epoch: {epoch + 1}/{num_train_epochs}\")\n",
        "    for batch in tqdm(train_loader):\n",
        "        model.train()\n",
        "        optim.zero_grad()\n",
        "        train_loss, _ = run_model(batch)\n",
        "        \n",
        "        train_loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        metrics = {}\n",
        "        if _steps % logging_steps == 0:\n",
        "            metrics.update({\"train/loss\": train_loss.item(), \"steps\": _steps})\n",
        "        \n",
        "        if _steps % eval_steps == 0:\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "            logits = []\n",
        "            with torch.no_grad():\n",
        "                for val_batch in val_loader:\n",
        "                    _val_loss, _logits = run_model(val_batch)\n",
        "                    val_losses.append(_val_loss.item())\n",
        "                    logits.extend(_logits.tolist())\n",
        "\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            _metrics = compute_metrics((logits, val_labels))\n",
        "            metrics[\"eval/loss\"] = val_loss\n",
        "            metrics.update(_metrics)\n",
        "        \n",
        "        if metrics:\n",
        "            print(metrics) \n",
        "            if wandb_logging:\n",
        "                wandb.log(metrics)\n",
        "\n",
        "        _steps += 1\n",
        "\n",
        "    # TODO: Augmentation after epoch\n",
        "    if aug_p > 0 and epoch + 1 != num_epochs:\n",
        "        print(\"Augmenting with p:\", aug_p, datetime.now())\n",
        "        _train_texts, augmented_idx = augment_texts(_train_texts, aug_p=aug_p, augmenter=augmenter, seed=epoch, verbose=False)\n",
        "        print(\"#Augmented texts:\", len(augmented_idx), datetime.now())\n",
        "        print()\n",
        "        _train_encodings = tokenizer(_train_texts, truncation=True, padding=True)\n",
        "        _train_dataset = PaperDataset(_train_encodings, train_labels)\n",
        "        train_loader = DataLoader(_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "        if wandb_logging:\n",
        "            wandb.log({\"augmentation/aug_p\": aug_p, \"augmentation/num_texts\": len(augmented_idx), \"epoch\": epoch})\n",
        "\n",
        "snapshot_dir = f\"results/{run_name}/network-snapshot-latest\"\n",
        "model.save_pretrained(snapshot_dir)\n",
        "\n",
        "if wandb_logging:\n",
        "    wandb.save(f\"{snapshot_dir}/*\", os.path.dirname(snapshot_dir))\n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW6Q0AmmAE3m"
      },
      "source": [
        "# wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZPWnjVxU0or"
      },
      "source": [
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "# model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOjvyeLkTUBz"
      },
      "source": [
        "# val_batch_size = 16\n",
        "# val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
        "# _label_weight = torch.from_numpy(label_weight).float().to(device)\n",
        "# loss_func = torch.nn.CrossEntropyLoss(_label_weight, reduction=\"sum\")\n",
        "\n",
        "# model.eval()\n",
        "# val_losses = []\n",
        "# logits = []\n",
        "# with torch.no_grad():\n",
        "#     for val_batch in tqdm(val_loader):\n",
        "#         inputs = {key: val.to(device) for key, val in val_batch.items()}\n",
        "#         _labels = inputs.pop(\"labels\")\n",
        "#         _outputs = model(**inputs)\n",
        "\n",
        "#         _logits = _outputs.logits\n",
        "#         _val_loss = loss_func(_logits, _labels)\n",
        "#         val_losses.append(_val_loss.item())\n",
        "#         logits.extend(_logits.tolist())\n",
        "\n",
        "# val_loss = sum(val_losses) / len(val_losses)\n",
        "# metrics = compute_metrics((logits, val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2AZLJF9zFCo"
      },
      "source": [
        "# snapshot_dir = f\"results/{run_name}/network-snapshot-latest\"\n",
        "# model.save_pretrained(snapshot_dir)\n",
        "\n",
        "# if wandb_logging:\n",
        "#     wandb.save(f\"{snapshot_dir}/*\", os.path.dirname(snapshot_dir))\n",
        "#     wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6gdbuMQsaZT"
      },
      "source": [
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "# wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-2Ct3_vc921"
      },
      "source": [
        "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "# t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# t5_model = AutoModelWithLMHead.from_pretrained(\"t5-base\").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md8S9addYunG"
      },
      "source": [
        "# input = \"That's great\"\n",
        "# input_enc = t5_tokenizer(input, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "# output = t5_model(**input_enc.to(device))\n",
        "# print(output)\n",
        "# print(torch.softmax(output.logits, dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXJP6lSnhSJU"
      },
      "source": [
        "paper_abstract = \"\"\"Generative adversarial networks (GANs) have shown\n",
        "outstanding performance on a wide range of problems in\n",
        "computer vision, graphics, and machine learning, but often require numerous training data and heavy computational resources. To tackle this issue, several methods introduce a transfer learning technique in GAN training. They,\n",
        "however, are either prone to overfitting or limited to learning small distribution shifts. In this paper, we show that\n",
        "simple fine-tuning of GANs with frozen lower layers of\n",
        "the discriminator performs surprisingly well. This simple\n",
        "baseline, FreezeD, significantly outperforms previous techniques used in both unconditional and conditional GANs.\n",
        "We demonstrate the consistent effect using StyleGAN and\n",
        "SNGAN-projection architectures on several datasets of Animal Face, Anime Face, Oxford Flower, CUB-200-2011, and\n",
        "Caltech-256 datasets. The code and results are available at\n",
        "https://github.com/sangwoomo/FreezeD.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDqbwDCReV9a"
      },
      "source": [
        "input = [\"It's incredibly bad\", paper_abstract, \"hello\", \"darkness\"]\n",
        "input_enc = tokenizer(input, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "output = model(**input_enc.to(device))\n",
        "print(output)\n",
        "# print(torch.softmax(output.logits, dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti9x-w_erAaX"
      },
      "source": [
        "# prediction = output.logits.argmax(dim=1)\n",
        "# actual = prediction\n",
        "# n = tp = fp = fn = tn = 0\n",
        "# tp += (prediction == 1) & (actual == 1)\n",
        "# fp += (prediction == 1) & (actual == 0)\n",
        "# fn += (prediction == 0) & (actual == 1)\n",
        "# tn += (prediction == 0) & (actual == 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ews38ukTEbpb"
      },
      "source": [
        "# !python huggingface_train.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}