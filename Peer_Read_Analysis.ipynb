{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Peer Read Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lym_glPd23n7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61KeHP4iDHOc"
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/westphal-jan/peer-data\n",
        "%cd /content/peer-data\n",
        "# !git checkout huggingface\n",
        "!git submodule update --init --recursive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzxkH1uGFGKb"
      },
      "source": [
        "# !pip install pytorch-lightning wandb python-dotenv catalyst sentence-transformers numpy requests nlpaug sentencepiece nltk\n",
        "# !pip install wandb nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ludytPTqS4"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, AdamW\n",
        "# import wandb\n",
        "# from datetime import datetime\n",
        "# import pickle\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "# import nlpaug.augmenter.word as naw\n",
        "from torch.utils.data import DataLoader\n",
        "# from copy import copy\n",
        "from datetime import datetime\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download(\"stopwords\")\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "from collections import defaultdict, Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDvvl-q839tO"
      },
      "source": [
        "class PaperDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]).float() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IZktYlUTlqq"
      },
      "source": [
        "def raw_read_dataset(data_dir: Path, num_texts=None):\n",
        "    file_paths = glob.glob(f\"{data_dir}/*.json\")\n",
        "    if num_texts != None:\n",
        "        file_paths = file_paths[:num_texts]\n",
        "    raws = []\n",
        "    for i, file_path in enumerate(tqdm(file_paths)):\n",
        "        with open(file_path) as f:\n",
        "            paper_json = json.load(f)\n",
        "            raws.append(paper_json)\n",
        "    return raws\n",
        "\n",
        "def read_dataset(data_dir: Path, num_texts=None, text_key=\"\"):\n",
        "    file_paths = glob.glob(f\"{data_dir}/*.json\")\n",
        "    if num_texts != None:\n",
        "        file_paths = file_paths[:num_texts]\n",
        "    abstracts = []\n",
        "    sections = []\n",
        "    labels = []\n",
        "    for i, file_path in enumerate(tqdm(file_paths)):\n",
        "        with open(file_path) as f:\n",
        "            paper_json = json.load(f)\n",
        "            accepted = paper_json[\"review\"][\"accepted\"]\n",
        "            abstract = paper_json[\"review\"][\"abstract\"]\n",
        "            _sections = paper_json[\"pdf\"][\"metadata\"][\"sections\"]\n",
        "            _sections = _sections if _sections else []\n",
        "\n",
        "            abstracts.append(abstract)\n",
        "            labels.append(int(accepted))\n",
        "            sections.append(_sections)\n",
        "    return abstracts, sections, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUxowRtHOCfG"
      },
      "source": [
        "dataset = \"data/original\"\n",
        "data_dir = Path(dataset)\n",
        "raw_submissions = raw_read_dataset(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5tgvK1tOOY-"
      },
      "source": [
        "len(raw_submissions)\n",
        "\n",
        "abstracts = list(map(lambda x: x[\"review\"][\"abstract\"], raw_submissions))\n",
        "sections = list(map(lambda x: x[\"pdf\"][\"metadata\"][\"sections\"], raw_submissions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrzHoC1GPUGw"
      },
      "source": [
        "words_abstract = list(map(lambda x: len(x.split(\" \")), abstracts))\n",
        "print(min(words_abstract), max(words_abstract))\n",
        "\n",
        "n_bins = 30\n",
        "plt.figure()\n",
        "plt.hist(words_abstract, weights=np.ones(len(raw_submissions)) / len(raw_submissions), bins=n_bins)\n",
        "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "plt.title(f\"Number of words in abstracts\")\n",
        "plt.xlabel(\"Number of words\")\n",
        "plt.ylabel(\"Occurrences\")\n",
        "plt.show()\n",
        "# plt.savefig(f\"abstract_words.png\", dpi=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJFZrUNVb2TI"
      },
      "source": [
        "accepted = [i for i in range(len(raw_submissions)) if raw_submissions[i][\"review\"][\"accepted\"]]\n",
        "not_accepted = [i for i in range(len(raw_submissions)) if not raw_submissions[i][\"review\"][\"accepted\"]]\n",
        "print(len(accepted), len(not_accepted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QmZiH0ocjJG"
      },
      "source": [
        "accepted_sections = np.array(sections, dtype=object)[accepted]\n",
        "rejected_sections = np.array(sections, dtype=object)[not_accepted]\n",
        "print(accepted_sections[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mVCRJjqdPq6"
      },
      "source": [
        "def get_length_to_occurrence(sections, percent=True):\n",
        "    num_sections = list(map(lambda x: len(x) if x else 0, sections))\n",
        "    count = Counter(num_sections)\n",
        "    total_sections = sum(num_sections)\n",
        "    count = {k: v for k, v in sorted(count.items(), key=lambda item: item[0])}\n",
        "    if percent:\n",
        "        count = {k: v / total_sections for k, v in count.items()}\n",
        "    return count, total_sections\n",
        "\n",
        "percent = True\n",
        "accepted_count, num_accepted_sections = get_length_to_occurrence(accepted_sections, percent)\n",
        "rejected_count, num_rejected_sections = get_length_to_occurrence(rejected_sections, percent)\n",
        "print(num_accepted_sections, rejected_count)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(accepted_count.keys()), list(accepted_count.values()), label=\"Accepted\")\n",
        "plt.plot(list(rejected_count.keys()), list(rejected_count.values()), label=\"Rejected\")\n",
        "if percent:\n",
        "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "plt.title(f\"Number of sections\")\n",
        "plt.xlabel(\"Number of sections\")\n",
        "plt.ylabel(\"Occurrences\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbu5mp4LPuVz"
      },
      "source": [
        "# num_sections = list(map(lambda x: len(x) if x else 0, sections))\n",
        "# print(max(num_sections))\n",
        "# print(sorted(num_sections, reverse=True)[:100])\n",
        "# num_usable = num_sections.count(0)\n",
        "# print(num_usable, len(raw_submissions) - num_usable)\n",
        "\n",
        "# counts = defaultdict(int)\n",
        "# for s in num_sections:\n",
        "#     counts[s] += 1\n",
        "# counts = {k: counts[k] for k in sorted(counts)}\n",
        "# print(counts)\n",
        "\n",
        "# n_bins = 3*len(set(num_sections))\n",
        "# plt.figure()\n",
        "# plt.hist(num_sections, weights=np.ones(len(raw_submissions)) / len(raw_submissions), bins=n_bins)\n",
        "# # plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "# plt.title(f\"Number of sections\")\n",
        "# plt.xlabel(\"Number of sections\")\n",
        "# plt.ylabel(\"Occurrences\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRG6QKXXT3lF"
      },
      "source": [
        "dataset = \"data/original\"\n",
        "\n",
        "data_dir = Path(dataset)\n",
        "abstracts, sections, labels = read_dataset(data_dir)\n",
        "abstracts, sections, labels = np.array(abstracts), np.array(sections, dtype=object), np.array(labels)\n",
        "\n",
        "# num_accepted = len(list(filter(lambda x: x == 1, labels)))\n",
        "# num_not_accepted = len(list(filter(lambda x: x == 0, labels)))\n",
        "\n",
        "# print(num_accepted, num_not_accepted)\n",
        "# label_weight = num_not_accepted / np.array([num_not_accepted, num_accepted])\n",
        "\n",
        "# # Get random index split for train/val/test.\n",
        "# idx = list(range(len(texts)))\n",
        "# # Get constant split across runs\n",
        "# rnd = np.random.RandomState(42)\n",
        "# rnd.shuffle(idx)\n",
        "# total_len = len(idx)\n",
        "# train_len, val_len = int(0.8*total_len), int(0.1*total_len)\n",
        "# train_idx = idx[:train_len]\n",
        "# val_idx = idx[train_len:(train_len + val_len)]\n",
        "# test_idx = idx[(train_len + val_len):]\n",
        "\n",
        "# train_texts, train_labels = texts[train_idx], labels[train_idx]\n",
        "# val_texts, val_labels = texts[val_idx], labels[val_idx]\n",
        "# text_texts, test_labels = texts[test_idx], labels[test_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r5d7j5Tk5P-"
      },
      "source": [
        "num_accepted = len(list(filter(lambda x: x == 1, labels)))\n",
        "num_not_accepted = len(list(filter(lambda x: x == 0, labels)))\n",
        "\n",
        "print(num_accepted, num_not_accepted)\n",
        "\n",
        "# Get random index split for train/val/test.\n",
        "idx = list(range(len(texts)))\n",
        "# Get constant split across runs\n",
        "rnd = np.random.RandomState(42)\n",
        "rnd.shuffle(idx)\n",
        "total_len = len(idx)\n",
        "train_len, val_len = int(0.8*total_len), int(0.1*total_len)\n",
        "train_idx = idx[:train_len]\n",
        "val_idx = idx[train_len:(train_len + val_len)]\n",
        "test_idx = idx[(train_len + val_len):]\n",
        "\n",
        "train_abstracts, train_sections, train_labels = texts[train_idx], sections[train_idx], labels[train_idx]\n",
        "val_abstracts, val_sections, val_labels = texts[val_idx], sections[val_idx], labels[val_idx]\n",
        "test_abstracts, test_sections, test_labels = texts[test_idx], sections[test_idx], labels[test_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7nb6rWummN1"
      },
      "source": [
        "train_sections[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvMnuIkAl6ic"
      },
      "source": [
        "def extract_sections(sections, labels):\n",
        "    all_sections = []\n",
        "    all_labels = []\n",
        "    for _sections, label in zip(sections, labels):\n",
        "        if len(sections) == 0:\n",
        "            continue\n",
        "        texts = list(map(lambda x: x[\"text\"], _sections))\n",
        "        all_sections.extend(texts)\n",
        "        all_labels.extend([label] * len(_sections))\n",
        "    return all_sections, all_labels\n",
        "\n",
        "flattened_train_sections, flattened_train_labels = extract_sections(train_sections, train_labels)\n",
        "flattened_val_sections, flattened_val_labels = extract_sections(val_sections, val_labels)\n",
        "print(len(flattened_train_sections), len(flattened_val_sections))\n",
        "\n",
        "num_accepted, num_rejected = flattened_train_labels.count(1), flattened_train_labels.count(0)\n",
        "print(num_accepted, num_rejected)\n",
        "label_weight = num_rejected / np.array([num_rejected, num_accepted])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OCSAoEeFlQ1"
      },
      "source": [
        "label_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAfZ_-Hb0pYh"
      },
      "source": [
        "train_encodings = list(map(lambda x: tokenize(x, token_to_id, True), train_texts))\n",
        "val_encodings = list(map(lambda x: tokenize(x, token_to_id, True), val_texts))\n",
        "\n",
        "train_dataset = PaperDataset({\"tokens\": train_encodings}, train_labels)\n",
        "val_dataset = PaperDataset({\"tokens\": val_encodings}, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7_qdTMs8549"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}