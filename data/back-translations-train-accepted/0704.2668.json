{"id": "0704.2668", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2007", "title": "Supervised feature selection via dependence estimation", "abstract": "We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.", "histories": [["v1", "Fri, 20 Apr 2007 08:26:29 GMT  (373kb,D)", "http://arxiv.org/abs/0704.2668v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["le song", "alexander j smola", "arthur gretton", "karsten m borgwardt", "justin bedo"], "accepted": true, "id": "0704.2668"}, "pdf": {"name": "0704.2668.pdf", "metadata": {"source": "CRF", "title": "Supervised Feature Selection via Dependence Estimation", "authors": ["Le Song", "Alex Smola", "Arthur Gretton", "Justin Bedo"], "emails": ["lesong@it.usyd.edu.au", "alex.smola@gmail.com", "arthur.gretton@tuebingen.mpg.de", "borgwardt@dbs.ifi.lmu.de", "bedo@ieee.org"], "sections": [{"heading": "1 Introduction", "text": "The task is to find a functional interdependence between the individual countries that see themselves as capable of surpassing themselves, both in the United States and in the United States. (...) It is a question of whether the individual countries are capable of surpassing each other. (...) It is a question of the extent to which the individual countries are able to surpass each other. (...) It is a question of the extent to which they are able to surpass each other. (...) It is a question of the extent to which they are able to surpass themselves. (...) It is a question of the extent to which \"outweigh\" each other. (...) It is a question of the extent to which \"outweighs\" each other. (...) It is a question of the extent to which \"outweighs\" each other. (...)"}, {"heading": "2 Measures of Dependence", "text": "We define X and Y broadly as two areas from which we take samples (x, y): these can be real evaluated, vector evaluated, class markers, strings, graphs and so on. We define a (possibly non-linear) mapping \u03c6 (x > HSS dependence on each HSS-X on a attribute space F, so that the inner product between the characteristics is given by a core function k (x, x). We can now define a cross-covariance operator between these attribute maps, in accordance with Baker (1973); Fukumizu et al al. (2004): this is a linear operator Cxy: G 7 \u2212 \u2192 F-Exy."}, {"heading": "3 Feature Selection via HSIC", "text": "With the help of HSIC, we can perform both backward (BAHSIC) and forward (FOHSIC) selection of attributes. In particular, if we use a linear kernel on the data (there is no such requirement for the labels), we can select the attributes forward and backward, which are equivalent: the objective function breaks down into individual coordinates, and thus the feature selection can be done without recursion in one step. Although forward selection is more mathematically efficient, reverse selection generally yields better attributes, since the quality of the attributes is evaluated in the context of all other attributes. Consequently, we present the backward-going version of our algorithm here (a forward-greedy selection version can be derived similarly). BAHSIC adds the attributes of S to the end of a list S, so that the elements towards the end of the S \u2020 have greater relevance for the learning task."}, {"heading": "4 Connections to Other Approaches", "text": "To this end, the maximum mean discrepancy (MMD) (Borgwardt et al., 2006) could be used to directly test whether there is a correlation between data and labels. KTA was also used for the selection of characteristics. Formally, it is defined as tr K L / VP-L. In practice (Neumann et al., 2005), normalization is often omitted to directly check whether there is a correlation between data and labels. We discuss this unnormalized variant of feature selection. Let's use the starting kernel l (y, y)."}, {"heading": "5 Variants of BAHSIC", "text": "There are three examples with a Gaussian kernel on the data, while we apply a linear kernel on the labels. (MUL) We apply a linear kernel on the labels with the labels. (BIN) We use m \u2212 1 + as a label for positive class members, and m \u2212 1 \u2212 for negative class members. (MI) We then apply a linear kernel on the labels. (MUL) We apply a linear kernel on the labels using the label vectors below, as described for a 3-class example. (MI) The number of labels in class i and 1MI indicates a vector of all m3 m3 m3 m3 m3 m3 m3 = 1m1 m3 m3 m3 m3 m3 m3 m3 m3 m3."}, {"heading": "6 Experimental Results", "text": "We conducted three sets of experiments: the characteristics of the data sets and the objectives of the experiments are (i) artificial data sets that illustrate the properties of BAHSIC; (ii) real data sets that compare BAHSIC with other methods; and (iii) a data set that shows that BAHSIC selects meaningful characteristics."}, {"heading": "6.1 Artificial datasets", "text": "We constructed 3 artificial datasets, as shown in Figure 1, to illustrate the difference between BAHSIC variants with linear and nonlinear nuclei. Each dataset has 22 dimensions - only the first two dimensions are associated with the prediction task and the rest are just Gaussian noise. These datasets are (i) binary XOR data: samples belonging to the same class have multimodal distributions; (ii) multiclass data: There are 4 classes, but 3 of them are collinear; (ii) nonlinear regression data: HSNames refer to the first two dimensions of the data according to y = x1 exp. (\u2212 x21 \u2212 x22) +, where additive Gaussian noise is designated. We compare BAHSIC with FOHSIC, Peers correlation, reciprocal information (Zaffalon & Hutter, 2002) and RELIEF (RELIEF only works for linear problems), if we want to show that very binary."}, {"heading": "6.2 Real world datasets", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We have never lost as much time as we have had in recent years, \"he said."}, {"heading": "6.3 Brain-computer interface dataset", "text": "The data contained EEG signals (118 channels, sampled at 100 Hz) from five healthy subjects (\"aa,\" al, \"av\" and \"ay\"), which were recorded during two types of motor performance, the task being to classify the imagination for individual trials. Our experiment was conducted in three steps: (i) A Fast Fourier Transformation (FFT) was performed on each channel and the power spectrum was calculated. (ii) The power spectrum of all channels was averaged."}, {"heading": "7 Conclusion", "text": "The idea behind the resulting algorithm, BAHSIC, is to choose the feature subset that maximizes dependence between the data and labels. With this interpretation, BAHSIC provides a uniform feature selection framework for each form of supervised learning. The absence of bias and good convergence properties of the empirical HSIC estimate provides a strong theoretical jutification for the use of HSIC in this context. Although BAHSIC is a filtering method, it still demonstrates good performance compared to more specialized methods in the artificial and real world. It is also very competitive in terms of runtime performance. 6Acknowledgments NICTA is funded by the Australian government."}], "references": [{"title": "Joint measures and cross-covariance operators", "author": ["C. Baker"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Baker,? \\Q1973\\E", "shortCiteRegEx": "Baker", "year": 1973}, {"title": "Integrating structured biological data by kernel maximum mean discrepancy", "author": ["K.M. Borgwardt", "A. Gretton", "M.J. Rasch", "H.P. Kriegel", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Bioinformatics (ISMB),", "citeRegEx": "Borgwardt et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Borgwardt et al\\.", "year": 2006}, {"title": "On optimizing kernel alignment", "author": ["N. Cristianini", "J. Kandola", "A. Elisseeff", "J. Shawe-Taylor"], "venue": "Tech. rep., UC Davis Department of Statistics", "citeRegEx": "Cristianini et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2003}, {"title": "Boosting bit rates in non-invasive EEG singletrial classifications by feature combination and multiclass paradigms", "author": ["G. Dornhege", "B. Blankertz", "G. Curio", "K. M\u00fcller"], "venue": "IEEE Trans. Biomed. Eng.,", "citeRegEx": "Dornhege et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dornhege et al\\.", "year": 2004}, {"title": "Optimizing spatio-temporal filters for improving BCI", "author": ["G. Dornhege", "B. Blankertz", "M. Krauledat", "F. Losch", "G. Curio", "K. M\u00fcller"], "venue": "In NIPS,", "citeRegEx": "Dornhege et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dornhege et al\\.", "year": 2006}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": null, "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Measuring statistical dependence with HilbertSchmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Sch\u00f6lkopf"], "venue": "In ALT,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "Guyon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2002}, {"title": "A practical approach to feature selection", "author": ["K. Kira", "L. Rendell"], "venue": "In Proc. 9th Intl. Workshop on Machine Learning,", "citeRegEx": "Kira and Rendell,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell", "year": 1992}, {"title": "Toward optimal feature selection", "author": ["D. Koller", "M. Sahami"], "venue": "In ICML,", "citeRegEx": "Koller and Sahami,? \\Q1996\\E", "shortCiteRegEx": "Koller and Sahami", "year": 1996}, {"title": "Spatio-spectral filters for improving the classification of single trial EEG", "author": ["S. Lemm", "B. Blankertz", "G. Curio", "M\u00fclller", "K.-R"], "venue": "IEEE Trans. Biomed. Eng.,", "citeRegEx": "Lemm et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lemm et al\\.", "year": 2005}, {"title": "Entropy and inference, revisited", "author": ["I. Nemenman", "F. Shafee", "W. Bialek"], "venue": "In NIPS,", "citeRegEx": "Nemenman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Nemenman et al\\.", "year": 2002}, {"title": "Combined SVM-based feature selection and classification", "author": ["J. Neumann", "C. Schn\u00f6rr", "G. Steidl"], "venue": "Machine Learning,", "citeRegEx": "Neumann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Neumann et al\\.", "year": 2005}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Approximation Theorems of Mathematical Statistics", "author": ["R. Serfling"], "venue": null, "citeRegEx": "Serfling,? \\Q1980\\E", "shortCiteRegEx": "Serfling", "year": 1980}, {"title": "On the influence of the kernel on the consistency of svms", "author": ["I. Steinwart"], "venue": null, "citeRegEx": "Steinwart,? \\Q2002\\E", "shortCiteRegEx": "Steinwart", "year": 2002}, {"title": "Use of zero-norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Sch\u00f6lkopf", "M. Tipping"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2003}, {"title": "Feature selection for SVMs", "author": ["J. Weston", "S. Mukherjee", "O. Chapelle", "M. Pontil", "T. Poggio", "V. Vapnik"], "venue": "In NIPS,", "citeRegEx": "Weston et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2000}, {"title": "Robust feature selection using distributions of mutual information", "author": ["M. Zaffalon", "M. Hutter"], "venue": "In UAI", "citeRegEx": "Zaffalon and Hutter,? \\Q2002\\E", "shortCiteRegEx": "Zaffalon and Hutter", "year": 2002}], "referenceMentions": [{"referenceID": 18, "context": "Examples include the leave-one-out error bound of SVM (Weston et al., 2000) and the mutual information (Koller & Sahami, 1996).", "startOffset": 54, "endOffset": 75}, {"referenceID": 6, "context": "We sidestep these problems by employing a mutual-information like quantity \u2014 the Hilbert Schmidt Independence Criterion (HSIC) (Gretton et al., 2005).", "startOffset": 127, "endOffset": 149}, {"referenceID": 17, "context": "Finding a global optimum for (1) is in general NP-hard (Weston et al., 2003).", "startOffset": 55, "endOffset": 76}, {"referenceID": 8, "context": "Forward selection tries to increase Q(T ) as much as possible for each inclusion of features, and backward elimination tries to achieve this for each deletion of features (Guyon et al., 2002).", "startOffset": 171, "endOffset": 191}, {"referenceID": 0, "context": "We may now define a cross-covariance operator between these feature maps, in accordance with Baker (1973); Fukumizu et al.", "startOffset": 93, "endOffset": 106}, {"referenceID": 0, "context": "We may now define a cross-covariance operator between these feature maps, in accordance with Baker (1973); Fukumizu et al. (2004): this is a linear operator Cxy : G 7\u2212\u2192 F such that", "startOffset": 93, "endOffset": 130}, {"referenceID": 6, "context": "Gretton et al. (2005) show that HSIC can be expressed in terms of kernels as", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Previous work used HSIC to measure independence between two sets of random variables (Gretton et al., 2005).", "startOffset": 85, "endOffset": 107}, {"referenceID": 6, "context": "Property (I) Gretton et al. (2005, Theorem 4) show that whenever F ,G are RKHSs with universal kernels k, l on respective compact domains X and Y in the sense of Steinwart (2002), then HSIC(F ,G,Prxy) = 0 if and only if x and y are independent.", "startOffset": 13, "endOffset": 179}, {"referenceID": 6, "context": "Furthermore, its convergence in probability to HSIC(F ,G,Prxy) occurs with rate 1/ \u221a m which is a slight improvement over the convergence of the biased estimator by Gretton et al. (2005). Theorem 2 (HSIC is Concentrated) Assume k, l are bounded almost everywhere by 1, and are nonnegative.", "startOffset": 165, "endOffset": 187}, {"referenceID": 6, "context": "Applying Hoeffing\u2019s bound as in Gretton et al. (2005) proves the result.", "startOffset": 32, "endOffset": 54}, {"referenceID": 15, "context": "It follows from Serfling (1980) that under the assumptions E(h) < \u221e and that the data and labels are not independent, the empirical HSIC converges in distribution to a Gaussian random variable with mean HSIC(F ,G,Prxy) and variance", "startOffset": 16, "endOffset": 32}, {"referenceID": 1, "context": "For this purpose one could use Maximum Mean Discrepancy (MMD) (Borgwardt et al., 2006).", "startOffset": 62, "endOffset": 86}, {"referenceID": 2, "context": "Likewise, one could use Kernel Target Alignment (KTA) (Cristianini et al., 2003) to test directly whether there exists any correlation between data and labels.", "startOffset": 54, "endOffset": 80}, {"referenceID": 13, "context": "For computational convenience the normalisation is often omitted in practise (Neumann et al., 2005), which leaves us with tr K L.", "startOffset": 77, "endOffset": 99}, {"referenceID": 8, "context": "Algorithms In this experiment, we show that the performance of BAHSIC can be comparable to other state-of-the-art feature selectors, namely SVM Recursive Feature Elimination (RFE) (Guyon et al., 2002), RELIEF (Kira & Rendell, 1992), L0-norm SVM ( L0) (Weston et al.", "startOffset": 180, "endOffset": 200}, {"referenceID": 17, "context": ", 2002), RELIEF (Kira & Rendell, 1992), L0-norm SVM ( L0) (Weston et al., 2003), and R2W2 (Weston et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 18, "context": ", 2003), and R2W2 (Weston et al., 2000).", "startOffset": 18, "endOffset": 39}, {"referenceID": 3, "context": "In this experiment, we show that BAHSIC selects features that are meaningful in practise: we use BAHSIC to select a frequency band for a brain-computer interface (BCI) data set from the Berlin BCI group (Dornhege et al., 2004).", "startOffset": 203, "endOffset": 226}, {"referenceID": 3, "context": "The result was then passed to a normal CSP method (Dornhege et al., 2004) for feature extraction, and then classified using a linear SVM.", "startOffset": 50, "endOffset": 73}, {"referenceID": 11, "context": "We compared automatic filtering using BAHSIC to other filtering approaches: normal CSP method with manual filtering (8-40 Hz), the CSSP method (Lemm et al., 2005), and the CSSSP method (Dornhege et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 4, "context": ", 2005), and the CSSSP method (Dornhege et al., 2006).", "startOffset": 30, "endOffset": 53}, {"referenceID": 3, "context": "Dornhege et al. (2006) report that the \u03bc rhythm (approx.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "and the bias is bounded by O(m\u22121), as shown by Gretton et al. (2005). An estimator of MMD with bias O(m\u22121) is", "startOffset": 47, "endOffset": 69}], "year": 2013, "abstractText": "We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.", "creator": "TeX"}}}