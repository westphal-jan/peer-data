{"id": "0712.3402", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2007", "title": "Graph kernels between point clouds", "abstract": "Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such ob jects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.", "histories": [["v1", "Thu, 20 Dec 2007 13:06:50 GMT  (67kb)", "http://arxiv.org/abs/0712.3402v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["francis r bach"], "accepted": true, "id": "0712.3402"}, "pdf": {"name": "0712.3402.pdf", "metadata": {"source": "CRF", "title": "Graph kernels between point clouds", "authors": ["Francis R. Bach"], "emails": ["francis.bach@mines.org"], "sections": [{"heading": null, "text": "ar Xiv: 071 2.34 02v1 [cs.LG] 2 0D ec2 00"}, {"heading": "1. Introduction", "text": "In recent years, cores have been designed for structured data in many fields, such as bioinformatics [1], language processing [3], and computer vision [4], providing an elegant way to incorporate known a-priori information by using the natural topological structure of objects. In this paper, we propose a kernel between point clouds that enables the classification of line drawings (such as handwritten digitizations [5] or the reuse of existing data representations already developed by experts."}, {"heading": "2. Graph kernels", "text": "In this section, we will consider two undirected graphs called G = (V, E, a, x) and H = (W, F, b, y). Two types of labels are considered: attributes designated as a (v) and A for vertex v, V, and B (w), A for vertex w, W, and positions designated as x (v), X, and y (w), X, and X. Our motivating examples are line drawings where X = A = R2.The definition of graph cores between G and H is based on a series of substructures of graphs, the most natural of which are paths, subtrees, and more general subgraphs, but they do not result in efficient enumerations, and recent work [11, 13] has focused on larger groups of substructures that we are now presenting."}, {"heading": "2.1 Paths, walks, subtrees and tree-walks", "text": "Considering the undirected diagram G with the vertex V, a path is a sequence of different, connected peaks, while a walk is a sequence of possibly different, connected peaks. For each positive integer \u03b2, we define \u03b2-walks as walks, so that all \u03b2 + 1 consecutive peaks are different (1-walks are regular walks). Note that if the diagram G is a tree (not cycles), then the sequence of 2-walks is equal to the sequence of paths (see examples in Figure 1). More generally, \u03b2-walks of length \u03b2 + 1 trees are exactly paths of length \u03b2 + 1. A partial tree is an image of T without cycles. We can represent a partial tree of G by a sequence of paths that T is placed above the apex....."}, {"heading": "2.2 Graph kernels", "text": "Assuming that a local nucleus qT, I, J (G, H) is given between two tree paths that share the same structure, following [11] we can define the tree kernel as the sum of all matching tree paths of G and H of the local kernel: kT\u03b1, \u03b2, \u03b3 (G, H) = \u2211 T-T\u03b1, \u03b3f\u03bb, \u03bd (T) \u2211 I-J\u03b2 (T, G) \u2211 J-J\u03b2 (T, H) qT, I, J (G, H). (1) If the kernel qT, I, J (G, H) has positive values and is equal to 1 if the two tree paths are equal, it can be considered a soft matching indicator, and then the kernel counts in equation. (1) simply counts the softly matching tree paths in the two graphs. We add up a non-negative punishability f\u0435, \u03bd (T), depending only on the tree structure kell. In addition to the usual reduction in the number of nodes, the penalty number we get a higher number of J | T (T)."}, {"heading": "2.3 Local kernel", "text": "We use a combination (product) of a kernel for attributes and a kernel for positions. For attributes, we use the following usual factorized form kA (a (I), b (J)) = 1 kA (a (Ip), b (Jp), where kA is a positively defined kernel on A (J). This allows separate comparison of each customized pair of points and efficient dynamic programming curves [11, 4]. However, for our local kernel on positions, we need a kernel that collectively depends on the total vectors x (I) and y (J), and not only on pairs (x (Ip), y (Jp)."}, {"heading": "3. Positive matrices and graphical models", "text": "The main idea underlying the factorization of the nucleus is to consider symmetrical positive matrices as covariance matrices and to look at graphical models defined for Gaussian random vectors with these covariance matrices. In this section we assume that we have n random variables Z1,..., Zn with probability distribution p (z) = p (z1,.., zn). In a nuclear matrix K (in our case defined as Kij = e \u2212 \u03b1 xi \u2212 xj, 2, for positions x1,..., xn) we consider random variables Z1,..., Zn such that cov (Zi, Zj) = Kij. In this section we consider covariance matrices with this identification as kernel matrices and vice versa."}, {"heading": "3.1 Graphical models and junction trees", "text": "Graphic models offer a flexible and intuitive way to define factored probability distributions. In the face of an undirected graph Q with vertices in {1,..., n}, it is said that the distribution p (z) in Q is factored if it is the product of potentials across all cliques (fully connected subgraphs) of the graph Q. If the distribution is Gauss with the covariance matrix K-Rn, the distribution is factored if (K \u2212 1) ij = 0 for each (i, j) that is not an edge in Q [15, 16]. In this essay, we will only consider decomposable graphic models for which the graph Q is triangular (i.e. there are no chord cycles with a length greater than 4). In this case, the common distribution is clearly defined from its margins p (zC) on the clicks Q. Namely, if C (Q) is the set of a length greater than 4)."}, {"heading": "3.2 Graphical models and projections", "text": "We have the covariance matrix factoring in Q, which K comes closest to the Kullback-Leibler divergence between normal distributions with the covariance matrices K and L., projected onto a graphical model, which is a classic tool in probabilistic modeling [16]. We leave the investigation of the approximation properties of such a projection (especially along the lines of [17]) to future work. Since our kernel on kernel matrices contains determinants, we simply have to create the determinant as a determinant. For decomposing graphical models, Q (K) can be obtained in closed form [15], and its determinant has the following simple expression: Log | E-Q (K) | = D-piquant C (K) Log | K-piquant S (Q) | K-piquant S-Log (Q), if Q (Q) Q (Q) is defined as a pedigree, Q-K (Q) (Q) (Q), then (Q) (Q) (Q-K) (Q-rotional)."}, {"heading": "3.3 Graphical models and kernels", "text": "D \"i\" s \"i\" s \"i.\" D \"W\" s tis rf\u00fc ide rf\u00fc \"iS.\" D \"E\" W \"s tis rf\u00fc ide rf\u00fc\", \"eaD\" s \"i.\" D \"\" W \"s tis rf\u00fc ide,\" eaD \",\" \"\", \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \",\" \",\" \",\", \"\", \",\", \"\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \",\", \"\", \",\", \",\" \",\" \",\" \",\", \"\", \"\", \"\", \",\", \"\" \",\", \"\", \"\", \",\" \",\" \"\", \",\" \",\" \",\", \"\" \",\", \"\", \"\", \",\""}, {"heading": "3.4 Choice of graphical models", "text": "In view of the entrenched tree structure of a tree-walk, we now have to define the graphic model Q\u03b2 (T) with which we project our kernel matrices. We define Q\u03b2 (T) so that for all nodes in T the node forms a clique together with all its descendants, i.e. one node is connected to its descendants and all \u03b2 descendants are also connected to each other (see Figure 3 for \u03b2 = 1): The group of cliques is thus a group of families of depth (i.e. with 1 generations). Thus, our final core reads: kT\u03b1, \u03b3 (G, H) = \u2211 T-T\u03b1, \u03b3f\u0445 (T) \u0445 I-JK (T) \u0445 J-JK (T) QK (T) kA (KI, LJ), \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "4. Dynamic programming recursions", "text": "To derive dynamic programming field trips, we follow [13] and rely on the fact that \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 tree migrations from G are essentially due to 1-tree migrations on the extended graph of all sub-trees of G with a depth of not more than \u03b2 \u2212 1 and a depth of less than \u03b1. We therefore consider the tree set V\u03b1, \u03b2 of not completely rooted (disordered) sub-trees of G = (V, E), of depths less than \u03b2 \u2212 1 and a depth less than \u03b1. In the face of two differently rooted trees, they are considered equivalent if they share the same tree structure, and this is defined as. \"On this basis, we define a directed graph of G = (V, E) with a depth of less than \u03b2 \u2212 1 and a depth less than \u03b1."}, {"heading": "4.1 Computational complexity", "text": "The complexity of calculating a nucleus between two graphs is linear in \u03b3 (the order of the treetops) and square in the size of V\u03b1, \u03b2 and W\u03b1, \u03b2. However, these groups have exponential size in \u03b2 and \u03b1 in general. Therefore, we are limited to small values (typically \u03b1 6 3 and \u03b2 6 6 6) sufficient for a good classification performance (see section 5). For example, the average number of nodes in the graphs for the handwritten digits we use in simulations is 18 \u00b1 4, while the average cardinal number of V\u03b1, \u03b2 37 \u00b1 13 (\u03b1 = 1, \u03b2 = 4) and 70 \u00b1 44 (\u03b1 = 2, \u03b2 = 4)."}, {"heading": "5. Application to handwritten character recognition", "text": "We tested our new kernels on the task of isolated handwritten character recognition, handwritten Arabic \u03b2 numbers (MNIST dataset), and Chinese characters (ETL9B dataset). We selected the first 100 examples for the ten classes in the MNIST dataset, while we selected the five hardest classes for the ETL9B dataset to distinguish between the 3,000 (by calculating the distances between the class averages) and then selected the first 50 examples per class. Our learning task is to classify these characters; we use a one-on-one scheme with 2-standard support for vector machines [10]. We consider characters as drawings in R2 that may intersect contours, which are of course presented as undirected planar graphs. We have thinned and subsampled the individual characters to reduce the size of the graphs (see two examples in Figure 5). The kernel on positions is X (exemplary)."}, {"heading": "6. Conclusion", "text": "We have presented a new core for point clouds based on comparisons of local subsets of point clouds. These comparisons are illustrated by (a) consideration of subsets based on tree paths and walks, and (b) the use of a specific factorized form for local nuclei between tree paths, namely factorization based on a graphic model.In addition, we report on handwritten character recognition applications where we have shown that the nuclei were able to capture the relevant information to make good predictions from a few training examples.8 We are currently investigating other areas of application of point clouds, such as shape mining in computer vision [8, 18] and predicting protein functions and interactions from their three-dimensional structures [19].8"}], "references": [{"title": "Local alignment kernels for biological sequences. In Kernel Methods in Computational Biology", "author": ["J.-P. Vert", "H. Saigo", "T. Akutsu"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A kernel for time series based on global alignments", "author": ["M. Cuturi", "J.-P. Vert", "O. Birkenes", "T. Matsui"], "venue": "In Proc. ICASSP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Image classification with segmentation graph kernels", "author": ["Z. Harchaoui", "F. Bach"], "venue": "In Proc. CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Offline Chinese handwriting recognition: A survey", "author": ["S.N. Srihari", "X. Yang", "G.R. Ball"], "venue": "Frontiers of Computer Science in China,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Feature kernel functions: Improving svms using high-level knowledge", "author": ["Q. Sun", "G. DeJong"], "venue": "In Proc. CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Computer Vision: A Modern Approach", "author": ["D.A. Forsyth", "J. Ponce"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Expressivity versus efficiency of graph kernels", "author": ["J. Ramon", "T. G\u00e4rtner"], "venue": "In First International Workshop on Mining Graphs, Trees and Sequences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Kernels for graphs. In Kernel Methods in Computational Biology", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Graph kernels based on tree patterns for molecules", "author": ["P. Mah\u00e9", "J.-P. Vert"], "venue": "Technical Report CCSD- 00095488,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "A kernel between sets of vectors", "author": ["R.I. Kondor", "T. Jebara"], "venue": "In Proc. ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Graphical models", "author": ["M.I. Jordan"], "venue": "Statistical Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Graphical models and point pattern matching", "author": ["T.S. Caetano", "T.M. Caelli", "D. Schuurmans", "D.A.C. Barone"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Kernel on bag of paths for measuring similarity of shapes", "author": ["F. Suard", "A. Rakotomamonjy", "A. Benrshrair"], "venue": "In Proc. ESANN,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A structural alignment kernel for protein", "author": ["J. Qiu", "M. Hue", "A. Ben-Hur", "J.-P. Vert", "W.S. Noble"], "venue": "structures. Bioinformatics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "Introduction In recent years, kernels for structured data have been designed in many domains, such as bioinformatics [1], speech processing [2], text processing [3] and computer vision [4].", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 164, "endOffset": 170}, {"referenceID": 6, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 164, "endOffset": 170}, {"referenceID": 7, "context": "In this paper, we propose a kernel between point clouds, with applications to classification of line drawings (such as handwritten digits [5] or Chinese characters [6, 7]) or shapes [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 8, "context": "The natural geometrical structure of point clouds is hard to represent in a few real-valued features [9], in particular because of (a) the required local or global invariances by rotation, scaling, and/or translation, (b) the lack of pre-established registrations of the point clouds (i.", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "Following one of the leading principles for designing kernels between structured data, we propose to look at all possible partial matches between two point clouds [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 10, "context": "More precisely, we assume that each point cloud has a graph structure (most often a neighborhood graph), and we consider recently introduced graph kernels [11, 12].", "startOffset": 155, "endOffset": 163}, {"referenceID": 11, "context": "More precisely, we assume that each point cloud has a graph structure (most often a neighborhood graph), and we consider recently introduced graph kernels [11, 12].", "startOffset": 155, "endOffset": 163}, {"referenceID": 9, "context": ", [10]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "The most natural ones are paths, subtrees and more generally subgraphs; however, they do not lead to efficient enumerations, and recent work [11, 13] has focused on larger sets of substructures that we now present.", "startOffset": 141, "endOffset": 149}, {"referenceID": 12, "context": "The most natural ones are paths, subtrees and more generally subgraphs; however, they do not lead to efficient enumerations, and recent work [11, 13] has focused on larger sets of substructures that we now present.", "startOffset": 141, "endOffset": 149}, {"referenceID": 10, "context": "Note that if \u03b1 = 1, we get back \u03b2-walks and the graph kernels that we use are often referred to as random walk kernels [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "2 Graph kernels Assuming a local kernel qT,I,J(G,H) between two tree-walks that share the same structure is given, following [11], we can define the tree-kernel as the sum over all matching tree-walks of G and H of the local kernel: k \u03b1,\u03b2,\u03b3(G,H) = \u2211", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "This penalization suggested by [13] is essential in our situation to avoid that trees with nodes of higher degrees dominate the sum.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "This allows the separate comparison of each matched pair of points and efficient dynamic programming recursions [11, 4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 3, "context": "This allows the separate comparison of each matched pair of points and efficient dynamic programming recursions [11, 4].", "startOffset": 112, "endOffset": 119}, {"referenceID": 13, "context": "We use the following kernel on positive matrices K and L, the (squared) Bhattacharyya kernel kB, defined as [14]: kB(K,L) = |K||L| \u2223", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "When the distribution is Gaussian with covariance matrix K \u2208 R, the distribution factorizes if and only if (K)ij = 0 for each (i, j) which is not an edge in Q [15, 16].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": "When the distribution is Gaussian with covariance matrix K \u2208 R, the distribution factorizes if and only if (K)ij = 0 for each (i, j) which is not an edge in Q [15, 16].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": ", we project all our covariance matrices onto a graphical model, which is a classical tool in probabilistic modelling [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "We leave the study of the approximation properties of such a projection (in particular along the lines of [17]) to future work.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "For decomposable graphical models, \u03a0Q(K) can be obtained in closed form [15] and its determinant has the following simple expression: log |\u03a0Q(K)| = \u2211", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "Dynamic programming recursions In order to derive dynamic programming recursions, we follow [13] and rely on the fact that \u03b1-ary \u03b2-tree-walks of G can essentially be defined through 1-tree-walks on the augmented graph of all subtrees of G of depth at most \u03b2 \u2212 1 and arity less than \u03b1.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Our learning task it to classify those characters; we use a one-vs-one multiclass scheme with 2-norm support vector machines [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "We are currently investigating other domains of applications of points clouds, such as shape mining in computer vision [8, 18], and prediction of protein functions and interactions from their three-dimensional structures [19].", "startOffset": 119, "endOffset": 126}, {"referenceID": 17, "context": "We are currently investigating other domains of applications of points clouds, such as shape mining in computer vision [8, 18], and prediction of protein functions and interactions from their three-dimensional structures [19].", "startOffset": 119, "endOffset": 126}, {"referenceID": 18, "context": "We are currently investigating other domains of applications of points clouds, such as shape mining in computer vision [8, 18], and prediction of protein functions and interactions from their three-dimensional structures [19].", "startOffset": 221, "endOffset": 225}], "year": 2008, "abstractText": "Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such objects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.", "creator": "LaTeX with hyperref package"}}}