{"id": "0801.4716", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2008", "title": "Methods to Integrate a Language Model with Semantic Information for a Word Prediction Component", "abstract": "Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.", "histories": [["v1", "Wed, 30 Jan 2008 17:10:24 GMT  (198kb)", "http://arxiv.org/abs/0801.4716v1", "10 pages ; EMNLP'2007 Conference (Prague)"]], "COMMENTS": "10 pages ; EMNLP'2007 Conference (Prague)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tonio wandmacher", "jean-yves antoine"], "accepted": true, "id": "0801.4716"}, "pdf": {"name": "0801.4716.pdf", "metadata": {"source": "CRF", "title": "Methods to integrate a language model with semantic information for a word prediction component", "authors": ["Tonio Wandmacher"], "emails": [], "sections": [{"heading": "1 Introduction: NLP for AAC systems", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "2 Language modeling and semantics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Statistical Language Models", "text": "For about 10 to 15 years, statistical language modelling has achieved remarkable success in various areas of NLP, such as speech recognition, machine translation, language building blocks, and word prediction systems. N-gram-based language models (LM) estimate the likelihood of a word occurring in a chain of n-1 preceding words. However, computers have only recently become powerful enough to estimate probabilities on a reasonable amount of training data. As n-gram-like language models become larger, the problem of combinatorial explosion in probability estimation becomes more important. A reasonable balance between performance and number of estimated events therefore appears to be an n of 3 to 5, including sophisticated techniques to estimate the likelihood of invisible events (smoothing methods). While n-gram-like language models already work quite well in many applications (Hirst manic), their capabilities are also very limited in that they cannot exploit deeper linguistic structures."}, {"heading": "2.2 Latent Semantic Analysis", "text": "Several papers have suggested the use of latent semantic analysis (LSA) to integrate semantic similarity on the basis of simultaneous word distributions (cf. Bellegarda, 1997; Coccaro and Jurafsky, 1998). LSA models semantic similarity and is able to relate coherent relationships to certain substantive words, and is good at predicting the occurrence of a substantive word in the presence of other thematically related terms. However, since it does not take into account the word order (Landauer et al, 1997), it is very bad at predicting its actual position within the sentence, and is completely useless at predicting a substantive word in the presence of other thematically related terms. Since it does not take into account the word order (\"bag-ofwords\" model), it is very poor at predicting its actual position within the sentence, and is completely useless at predicting functional words."}, {"heading": "2.3 Transforming LSA similarities into probabilities", "text": "We assume that a statement or text that needs to be entered is normally semantically cohesive, and we then expect all word vectors to be close to the current context vector whose corresponding words belong to the semantic field of the context. This forms the basis for a simple probabilistic model of LSA: After calculating the cosinine similarity for each word, the vector iw r is with the vector h of the kur-rent context, we could use the normalized similarities as probability values. However, this probability distribution is usually quite flat (i.e. the dynamic range is low), and for this reason a contrasting (or temperature-related) factor \u03b3 is applied (cf. Coccaro and Jurafsky, 1998), which raises the cosine to a certain power (would normally go between 3 and 8). After normalization, we obtain a probability distribution that can be used for predictive purposes."}, {"heading": "2.4 Density as a confidence measure", "text": "Wandmacher (2005) pointed out that the reliability of LSA relationships varies greatly from term to term. He also showed that the entropy of a term does not correlate with the quality of the relationship (i.e. the number of semantically related terms in an LSA-generated term cluster), but he found an average correlation (Pearson coeff. = 0.56) between the number of semantically related terms and the average cosmic similarity of the m nearest neighbors (density). The closer the neighbors of a term vector are, the more likely it is to find semantically related terms for the given word. Terms with a high density, in turn, are more likely to be semantically associated with a given context (i.e. their specificity is higher). We define the density of a term wi as follows: \u0445 = \u22c5 = mj ijiim wNw wD 1) (cos (ropmetr (100) In the following word, we will predict the reliability of this component as W."}, {"heading": "3 Integrating semantic information", "text": "In the following, we present different methods for integrating semantic information, as provided by an LSA model, into a standard LM."}, {"heading": "3.1 Semantic cache model", "text": "The underlying idea is that words that have already occurred in a text are more likely to occur another time. Therefore, their probability is increased by a constant or an exponentially decreasing factor, depending on the position of the element in the cache. The highest probability of recurrence is usually after 15 to 20 words. Similar to Clarkson and Robinson (1997), we have implemented an exponentially decreasing cache of length l (usually between 100 and 1000), with the following decay function used for a word wi and its position p in the cache.2), (\u2212 \u2212 = relative absence epwf (5).coscos i when the following decay function is used for a word wi and its position p in the cache.2), (\u2212 \u2212 \u2212 = relative absence epwf (5)."}, {"heading": "3.2 Partial reranking", "text": "The underlying idea of a partial re-evaluation is to consider only the best n candidates from the basic language model of the semantic model in order to prevent the LSA model from making completely implausible (i.e. unlikely) predictions. Words that are unlikely for a given context are ignored, as well as words that do not appear in the semantic model (e.g. function words), because LSA is unable to make correct estimates for this group of words (here the basic probability remains unchanged). For the best n candidates, their semantic probability is calculated and each of these words is assigned an additional value after a fraction of its base probability has been subtracted (jackpot strategy). For a given context h, we calculate the ordered setBESTn (h) = < w1,..., wn >, so that P (w1 | h) acquires an additional value (w2 | h)."}, {"heading": "3.3 Standard interpolation", "text": "While in a linear combination we simply add the weighted probabilities of two (or more) models, geometric interpolation multiplies the probabilities weighted by an exponential coefficient (0 \u2264 \u03bb1 \u2264 1): Linear interpolation (LI):) () 1 () () ('11 isibi wP\u03bbwP \u22c5 \u2212 + \u22c5 = (8) Geometric interpolation (GI): \u2211 = \u2212 \u2212 \u22c5 = nj\u03bbjs\u03bbibiwPwPwP wP1) 11 (1) 11 () () () (' (9) The main difference between the two methods is that the latter takes into account the similarity of two models. Only when each of the models assigns a high probability to a given event will the combined probability be given a high value."}, {"heading": "3.4 Confidence-weighted interpolation", "text": "While in standard settings the coefficients are stable for all probabilities, some approaches use confidence-weighted coefficients that are adjusted for each probability. To integrate n-gram and LSA probabilities, Coccaro and Jurafsky (1998) proposed an entropy-related confidence value for the LSA component, which is based on the observation that words that occur in many different contexts (i.e. have high entropy) cannot be well predicted by the LSA. We use a density-related measurement here (see Section 2.2), because it is more reliable than entropy in previous experiments. For interpolation purposes, we calculate the coefficient of the LSA component as follows:) (ii wD\u03b2 =, iff D (wi) > 0; otherwise (10), where \u03b2 is a weighting constant to control the influence of the LSA predictor, i.e., we set it to \u2264 (0.4) for all experiments."}, {"heading": "4 Results", "text": "In a statement, the company said it was \"deeply saddened\" to learn of the death of a young man who was shot and killed by a police officer in the early hours of Saturday morning."}, {"heading": "5 Conclusion and further work", "text": "Adapting a statistical language model with semantic information derived from a distribution analysis such as LSA has proved to be a non-trivial problem. Considering the task of word prediction in an AAC system, we tested various methods to integrate an n-gram LM with LSA: a semantic cache model, a partial reranking approach, and some variants of interpolation. We evaluated the methods with two different metrics, the keystroke saving rate (ksr) and the perplexity, and we found significant gains for all methods containing LSA information compared to the baseline. With respect to ksr, the most successful method was confidence-weighted geometric interpolation (CWGI; + 1.05% in ksr); for perplexity, the largest reduction was achieved for both standard and trust-weighted LSA information compared to the baseline."}, {"heading": "Acknowledgements", "text": "This research is partially supported by the UFA (Universit\u00e9 Franco-Allemande) and the French foundations APRETREIMC (ESAC _ IMC project) and AFM (Voltaire project) and we would also like to thank the developers of the SRI and the Infomap toolkits for providing their programs."}], "references": [{"title": "A Latent Semantic Analysis Framework for Large-Span Language Modeling", "author": ["J. Bellegarda"], "venue": "Proceedings of the Eurospeech 97,", "citeRegEx": "Bellegarda,? \\Q1997\\E", "shortCiteRegEx": "Bellegarda", "year": 1997}, {"title": "VITIPI : Versatile interpretation of text input by persons with impairments", "author": ["Boissi\u00e8re Ph", "Dours D"], "venue": "Proceedings ICCHP'1996", "citeRegEx": "Ph. and D.,? \\Q1996\\E", "shortCiteRegEx": "Ph. and D.", "year": 1996}, {"title": "Word association norms, mutual information and lexicography", "author": ["K. Church", "P. Hanks"], "venue": "Proceedings of ACL,", "citeRegEx": "Church and Hanks,? \\Q1989\\E", "shortCiteRegEx": "Church and Hanks", "year": 1989}, {"title": "Language Model Adaptation using Mixtures and an Exponentially Decaying Cache", "author": ["P.R. Clarkson", "A.J. Robinson"], "venue": "in Proc. of the IEEE ICASSP-97,", "citeRegEx": "Clarkson and Robinson,? \\Q1997\\E", "shortCiteRegEx": "Clarkson and Robinson", "year": 1997}, {"title": "Towards better integration of semantic predictors in statistical language modeling", "author": ["N. Coccaro", "D. Jurafsky"], "venue": "Proc. of the ICSLP-98,", "citeRegEx": "Coccaro and Jurafsky,? \\Q1998\\E", "shortCiteRegEx": "Coccaro and Jurafsky", "year": 1998}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S.C. Deerwester", "S. Dumais", "T. Landauer", "G. Furnas", "R. Harshman"], "venue": "JASIS", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Testing the efficacy of part-of-speech information in word completion", "author": ["A. Fazly", "G. Hirst"], "venue": "Proceedings of the Workshop on Language Modeling for Text Entry Methods on EACL, Budapest", "citeRegEx": "Fazly and Hirst,? \\Q2003\\E", "shortCiteRegEx": "Fazly and Hirst", "year": 2003}, {"title": "A Bit of Progress in Language Modeling\u201d, Extended Version Microsoft Research Technical Report MSR-TR-2001-72", "author": ["J. Goodman"], "venue": null, "citeRegEx": "Goodman,? \\Q2001\\E", "shortCiteRegEx": "Goodman", "year": 2001}, {"title": "Self-organized Language Models for Speech Recognition", "author": ["F. Jelinek"], "venue": "Readings in Speech Recognition,", "citeRegEx": "Jelinek,? \\Q1990\\E", "shortCiteRegEx": "Jelinek", "year": 1990}, {"title": "A Cache-Based Natural Language Model for Speech Reproduction", "author": ["R. Kuhn", "R. De Mori"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kuhn and Mori,? \\Q1990\\E", "shortCiteRegEx": "Kuhn and Mori", "year": 1990}, {"title": "How well can passage meaning be derived without using word order? A comparison of LSA and humans", "author": ["T.K. Landauer", "D. Laham", "B. Rehder", "M.E. Schreiner"], "venue": "Proceedings of the 19th annual meeting of the Cognitive Science Society,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Limits of human word prediction performance", "author": ["G.W. Lesher", "Moulton", "B. J", "D.J. Higginbotham", "B. Alsofrom"], "venue": "Proceedings of the CSUN", "citeRegEx": "Lesher et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lesher et al\\.", "year": 2002}, {"title": "Semantic knowledge in a word completion task", "author": ["J. Li", "G. Hirst"], "venue": "Proc. of the 7 Int. ACM Conference on Computers and Accessibility,", "citeRegEx": "Li and Hirst,? \\Q2005\\E", "shortCiteRegEx": "Li and Hirst", "year": 2005}, {"title": "Exploiting long distance collocational relations in predictive typing", "author": ["H. Matiasek", "M. Baroni"], "venue": "Proceedings of the EACL-03 Workshop on Language Modeling for Text Entry Methods,", "citeRegEx": "Matiasek and Baroni,? \\Q2003\\E", "shortCiteRegEx": "Matiasek and Baroni", "year": 2003}, {"title": "A maximum entropy approach to adaptive statistical language modelling", "author": ["R. Rosenfeld"], "venue": "Computer Speech and Language,", "citeRegEx": "Rosenfeld,? \\Q1996\\E", "shortCiteRegEx": "Rosenfeld", "year": 1996}, {"title": "Sibyl - AAC system using NLP techniques", "author": ["I. Schadle", "J.-Y. Antoine", "B. Le P\u00e9v\u00e9dic", "F. Poirier"], "venue": "Proc. ICCHP\u20192004,", "citeRegEx": "Schadle et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schadle et al\\.", "year": 2004}, {"title": "Entropy-based pruning of backoff language models", "author": ["A. Stolcke"], "venue": "Proc.s of the DARPA Broadcast News Transcription and Understanding Workshop", "citeRegEx": "Stolcke,? \\Q1998\\E", "shortCiteRegEx": "Stolcke", "year": 1998}, {"title": "SRILM - An Extensible Language Modeling Toolkit", "author": ["A. Stolcke"], "venue": "Proc. of the Intl. Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Topic Modeling in Fringe Word Prediction for AAC", "author": ["K. Trnka", "D. Yarrington", "K.F. McCoy", "C. Pennington"], "venue": "In Proceedings of the 2006 International Conference on Intelligent User Interfaces,", "citeRegEx": "Trnka et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Trnka et al\\.", "year": 2006}, {"title": "The Language Component of the FASTY Text Prediction System", "author": ["H. Trost", "J. Matiasek", "M. Baroni"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "Trost et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Trost et al\\.", "year": 2005}, {"title": "How semantic is Latent Semantic Analysis?", "author": ["T. Wandmacher"], "venue": "Proceedings of TALN/RECITAL", "citeRegEx": "Wandmacher,? \\Q2005\\E", "shortCiteRegEx": "Wandmacher", "year": 2005}], "referenceMentions": [{"referenceID": 19, "context": "Basically, an AAC system, such as FASTY (Trost et al. 2005) or SIBYLLE (Schadle et al, 2004), consists of four components.", "startOffset": 40, "endOffset": 59}, {"referenceID": 6, "context": "There have been attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction.", "startOffset": 65, "endOffset": 88}, {"referenceID": 2, "context": "Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989).", "startOffset": 123, "endOffset": 152}, {"referenceID": 5, "context": "There have been attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989).", "startOffset": 66, "endOffset": 520}, {"referenceID": 5, "context": "There have been attempts to integrate part-of-speech information (Fazly and Hirst, 2003) or more complex syntactic models (Schadle et al, 2004) to achieve a better prediction. In this paper, we will nevertheless limit our study to a standard 4-gram model as a baseline to make our results comparable. Our main aim is here to investigate the use of long-distance semantic dependencies to dynamically adapt the prediction to the current semantic context of communication. Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989).", "startOffset": 66, "endOffset": 551}, {"referenceID": 2, "context": "Similar work has been done by Li and Hirst (2005) and Matiasek and Baroni (2003), who exploit Pointwise Mutual Information (PMI; Church and Hanks, 1989). Trnka et al. (2005) dynamically interpolate a high number of topic-oriented models in order to adapt their predictions to the current topic of the text or conversation.", "startOffset": 129, "endOffset": 174}, {"referenceID": 18, "context": "(2005) and Trnka et al. (2005), we assume that one additional keystroke is required for the selection of a word from the list and that a space is automatically inserted afterwards.", "startOffset": 11, "endOffset": 31}, {"referenceID": 14, "context": "(Rosenfeld, 1996), (Goodman, 2002) or in a word prediction task: (Fazly and Hirst, 2003), (Schadle, 2004), (Li and Hirst, 2005)).", "startOffset": 0, "endOffset": 17}, {"referenceID": 6, "context": "(Rosenfeld, 1996), (Goodman, 2002) or in a word prediction task: (Fazly and Hirst, 2003), (Schadle, 2004), (Li and Hirst, 2005)).", "startOffset": 65, "endOffset": 88}, {"referenceID": 12, "context": "(Rosenfeld, 1996), (Goodman, 2002) or in a word prediction task: (Fazly and Hirst, 2003), (Schadle, 2004), (Li and Hirst, 2005)).", "startOffset": 107, "endOffset": 127}, {"referenceID": 4, "context": "Several works have suggested the use of Latent Semantic Analysis (LSA) in order to integrate semantic similarity to a language model (cf. Bellegarda, 1997; Coccaro and Jurafsky, 1998).", "startOffset": 133, "endOffset": 183}, {"referenceID": 10, "context": ", wm) can be represented by the sum of the (already normalized) vectors corresponding to the words it contains (Landauer et al. 1997):", "startOffset": 111, "endOffset": 133}, {"referenceID": 20, "context": "Measuring relation quality in an LSA space, Wandmacher (2005) pointed out that the reliability of LSA relations varies strongly between terms.", "startOffset": 44, "endOffset": 62}, {"referenceID": 3, "context": "Similar to Clarkson and Robinson (1997), we implemented an exponentially decaying cache of length l (usually between 100 and 1000), using the", "startOffset": 11, "endOffset": 40}, {"referenceID": 4, "context": "In order to integrate n-gram and LSA probabilities, Coccaro and Jurafsky (1998) proposed an entropy-related confidence measure for the LSA component, based on the observation that words that occur in many different contexts (i.", "startOffset": 52, "endOffset": 80}, {"referenceID": 17, "context": "Using the SRI toolkit (Stolcke, 2002) we computed a 4-gram LM over a controlled 141,000 word vocabulary, using modified Kneser-Ney discounting (Goodman, 2001), and we applied Stolcke pruning (Stolcke, 1998) to reduce the model to a manageable size (\u03b8 = 10).", "startOffset": 22, "endOffset": 37}, {"referenceID": 7, "context": "Using the SRI toolkit (Stolcke, 2002) we computed a 4-gram LM over a controlled 141,000 word vocabulary, using modified Kneser-Ney discounting (Goodman, 2001), and we applied Stolcke pruning (Stolcke, 1998) to reduce the model to a manageable size (\u03b8 = 10).", "startOffset": 143, "endOffset": 158}, {"referenceID": 16, "context": "Using the SRI toolkit (Stolcke, 2002) we computed a 4-gram LM over a controlled 141,000 word vocabulary, using modified Kneser-Ney discounting (Goodman, 2001), and we applied Stolcke pruning (Stolcke, 1998) to reduce the model to a manageable size (\u03b8 = 10).", "startOffset": 191, "endOffset": 206}, {"referenceID": 12, "context": "We could not provide here a comparison with other models that make use of distributional information, like the trigger approach by Rosenfeld (1996), Matiasek and Baroni (2003) or the model presented by Li and Hirst (2005), based on Pointwise Mutual Information (PMI).", "startOffset": 131, "endOffset": 148}, {"referenceID": 12, "context": "We could not provide here a comparison with other models that make use of distributional information, like the trigger approach by Rosenfeld (1996), Matiasek and Baroni (2003) or the model presented by Li and Hirst (2005), based on Pointwise Mutual Information (PMI).", "startOffset": 149, "endOffset": 176}, {"referenceID": 12, "context": "We could not provide here a comparison with other models that make use of distributional information, like the trigger approach by Rosenfeld (1996), Matiasek and Baroni (2003) or the model presented by Li and Hirst (2005), based on Pointwise Mutual Information (PMI).", "startOffset": 202, "endOffset": 222}], "year": 2007, "abstractText": "Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4gram baseline, and most of them to a simple cache model as well.", "creator": "PDFCreator Version 0.9.0"}}}