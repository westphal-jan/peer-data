{"id": "0804.0188", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2008", "title": "Support Vector Machine Classification with Indefinite Kernels", "abstract": "We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as a noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several classic data sets.", "histories": [["v1", "Tue, 1 Apr 2008 14:55:33 GMT  (25kb)", "https://arxiv.org/abs/0804.0188v1", null], ["v2", "Tue, 4 Aug 2009 11:48:14 GMT  (31kb)", "http://arxiv.org/abs/0804.0188v2", "Final journal version. A few typos fixed"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["ronny luss", "alexandre d'aspremont"], "accepted": true, "id": "0804.0188"}, "pdf": {"name": "0804.0188.pdf", "metadata": {"source": "CRF", "title": "Support Vector Machine Classification with Indefinite Kernels", "authors": ["Ronny Luss", "Alexandre d\u2019Aspremont"], "emails": ["rluss@alumni.princeton.edu", "aspremon@princeton.edu"], "sections": [{"heading": null, "text": "ar Xiv: 080 4.01 88v2 [cs.LG] 4 Aug 200 9"}, {"heading": "1 Introduction", "text": "In fact, most of us are able to go in search of a solution."}, {"heading": "1.1 Current results", "text": "A first direction embeds data in a pseudo-euclidean (pE) space: Haasdonk (2005), for example, formulates the classification problem with an indefinite core as that of minimizing the distance between convex hulls formed from the two categories of data in the pE space; the inseparable case is treated in the same way with reduced convex hulls. (See Bennet and Bredensteiner (2000) for a discussion of geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite nuclei: reversing the negative eigenvalues or shifting the eigenvalues and reconstructing the core with the original eigenvectors to produce a positive semidefinitive core (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004)."}, {"heading": "1.2 Contributions", "text": "In this work, instead of directly transforming the indefinite kernel, we simultaneously learn the eigenweights of the support vectors and a proxy mercer kernel matrix by punishing the distance between that proxy kernel and the original indefinite kernel. Our main result is that the learning part of the kernel can be explicitly solved, which means that the indefinite kernel classification problem can simply be formulated as a disruption of the positive semidefinitive case. Our formulation can be interpreted as a punished core learning problem with uncertainty on the input kernel matrix. In this sense, indefinite similarity matrices are seen as noisy observations of a true positive semidefinitive kernel, and we learn a kernel that increases generalization performance, from a point of view of complexity, while the original SVM classification problem with an indefinite kernel is undefined."}, {"heading": "2 SVM with indefinite kernels", "text": "In this section, we modify the SVM kernel learning problem and formulate a punished kernel learning problem for indefinite kernels. We also describe how our framework applies to kernels that meet Mercer's condition."}, {"heading": "2.1 Kernel learning", "text": "We formulate the kernel learning problem as in Lanckriet et al. (2004), where the authors minimize an upper limit on the probability of misclassification when using SVM with a given kernel K. This upper limit is the general unit of performance C (K) = max {0 \u2264 \u03b1 \u2264 C, \u03b1T y = 0} \u03b1T e \u2212 Tr (K (Y) \u03b1 (Y \u03b1) T) / 2 (1), where \u03b1 Rn and C are the SVM misclassification penalty. This is also the classic 1-norm soft margin SVM problem. They show that \u03c9C (K) is convex in K and solve problems of the formmine constant K (K) (2) to learn an optimal kernel K configuration that achieves good generalization performance. If K problems are limited to a constant, they can have a constant of K (2)."}, {"heading": "2.2 Learning from indefinite kernels", "text": "The measure of performance in (1) is the dual of the SVM classification problem with hinge loss and quadratic penalty. If K is positively semidefinite, this problem is a convex square program. Now, suppose we get an indeterminate problem of the kernel matrix K0 - Sn. We formulate a new instance of the problem (2) by using K as a positive semidefinitive kernel matrix in any given neighborhood of the original (indeterminate) kernel matrix K0 and solvemin {K \u2212 K \u2212 2F \u2264 \u03b2 \u00b2 max {\u03b1T = 0, 0 \u2264 \u03b1."}, {"heading": "2.3 Interpretation", "text": "Our explicit solution of the optimal kernel specified in (5) is the projection of a punished rank-one update onto the indefinite core on the cone of positive semi-defined matrices. Since \u03c1 tends to infinity, the rank-one update has less effect, and at the limit, the optimal kernel is given by zeroing the egative eigenvalues of the indefinite kernel. This means that if the indefinite kernel contains a very small amount of noise, the best positive semi-defined kernel that can be used with SVM within our framework is the positive part of the indefinite kernel. This limit also tends to infinity and also motivates a heuristics for the transformation of the kernel on the test set. As negative eigenvalues in the training kernel are reduced to zero, the limit should occur the same transformation for the test kernel. Thus, to measure the performance of the generalization, we update the entries of the full kernel according to the training cases by the rank-one update, which results in the optimal value from the negative solution of the full kernel on the matrix."}, {"heading": "2.4 Dual problem", "text": "As discussed above, the problems are (3) and (4) dual. The internal maximization in problem (3) is a quadratic program in which dual is the quadratic minimization problem, minimizing 1 2 (e \u2212 \u03b4 + \u00b5 + y\u03bd) T (Y KY) \u2212 1 (e \u2212 \u03b4 + y\u03bd) + C\u00b5T e is subject to \u03b4 0. (9) Replacing (9) internal maximization in problem (3) allows us to write a common minimization problem, the TK \u2212 1 (e \u2212 \u03b4 + y\u03bd) (1 \u2212 \u03b4 + y\u03bd) (1 \u2212 \u03b4 + y\u03bd) T) / 2 + C0 + C0 Facilitization problem, the TK \u2212 1 (e \u2212 \u03b4 + y\u03bd) -1 (e \u2212 \u03b4 + y\u03bd) T) / 0 + C0 Facilitating problem, the TK \u2212 1 (e \u2212 \u03b4 + y\u03bd)"}, {"heading": "3 Algorithms", "text": "We now execute two algorithms that can be used to solve problem (7), which maximize an undifferentiated concave function subject to convex constraints. An optimal point always exists, because the realizable quantity is limited and not empty. In order to ensure numerical stability in both algorithms, we smooth our target square (\u03b1) to calculate a gradient. First, we describe a simple projected gradient method that has numerically cheap iterations but yields less predictable performance in practice. Then, we show how to apply the method of the analytical intersection plane, whose iterations are numerically more complex but converge linearly. To ensure completeness, we also describe an exchange method of Chen and Ye (2008), which is used to solve problem (8), where numerical bottleneck is a quadratically limited linear program that is solved at each iteration."}, {"heading": "3.1 Computing proxy kernels", "text": "Since the proxy kernel in (5) requires only an update of a (fixed) eigenvalue, in this case we efficiently decompose ourselves (see Demmel (1997) for more details. We refer the reader to Kulis et al. (2006) for another kernel study using this method. Given the eigenvalue substitution X = V DV T, this problem can be reduced by changing the basis to the decomposition of the diagonal plus rank-one matrix, D + \u03c1uuuuT, where u = V T\u03b1 is used. First, the updated eigenvalues are determined by solving the secular equations (D + ecuuT) = 0, which can be done in O (n2). While there is an explicit solution for the eigenvalues corresponding to these eigenvalues, they are not stable because the eigenvalues are safe values."}, {"heading": "3.2 Projected gradient method", "text": "The projected gradient method takes a steepest downward step and then projects the new point back onto the realisable region (see, for example, Bertsekas (1999).The method is only efficient if the projection step is numerically cheap.The complexity of each iteration is then broken down as follows: Step 1. This requires a eigenvalue substitution calculated in O (n2) plus a matrix multiplication as described above. Experiments below use a step size of 5 / k for indefiniteSVM and 10 / k for perturbSVM (described in Section 4.3), where k is the iteration number. A good step size is critical to performance and must be selected separately for each dataset, as there is no rule of thumb."}, {"heading": "3.3 Analytic center cutting plane method", "text": "This method requires no differentiation. The method is described in Algorithm 2 (see Bertsexi (1999) for a more complete treatment of cutting methods. The complexity of each iteration is described as follows: Step 1. This step calculates the analytical center of a polyhedron and can be solved in O (n3) operations using internal point methods."}, {"heading": "3.4 Exchange method for SIQCLP", "text": "The algorithm considered in Chen and Ye (2008) to solve the problem (8) falls under a class of algorithms called exchange methods (as in Hettich and Kortanek (1993)), which iteratively solve problems constrained by a finite subset of the infinite number of constraints, where the solution in each iteration has an improved lower limit to the maximization problem. The subproblem solved in each iteration here is maximizing tsubject to \u03b1T = 0, 0 \u2264 Ct \u2264 T e \u2212 1 Tr (Y \u03b1) (Y \u03b1) T) + \u03c1 Ki \u2212 K0 \u04322 F = 1,.., p) where p is used to approximate the number of constraints to the infinite number of constraints of the problem (8)."}, {"heading": "3.5 Matlab Implementation", "text": "The first two algorithms discussed here were implemented in Matlab for cases of indefinite (indefinite SVM) and positive semi-definite (PerturbSVM) core and can be downloaded from the authors web pages in a package called IndefiniteSVM. \u03c1 penalty parameter is one-dimensional in implementation, using the LIBSVM code of Chang and Lin (2001) to create sub-optimality boundaries and track convergence. A Matlab implementation of the exchange method (based on the authors of Chen and Ye (2008), which MOSEK (MOSEK ApS 2008) uses to solve the problem (13), is compared with the projected gradient method in Section 5."}, {"heading": "4 Extensions", "text": "In this section, we extend our findings to other kernel methods, namely support for vector regression and single-class vector engines. In addition, we apply our method to the use of Mercer cores and show how we can use more general penalties in our formulation."}, {"heading": "4.1 SVR with indefinite kernels", "text": "The practicality of indeterminate cores in the SVM classification similarly motivates the use of indeterminate cores to support vector regression (SVR). Here we extend the formulations in Section 2 to SVR with linear, insensitive losses. The indefinite SVR formulation follows directly as in Section 2.2 and the optimal kernel is learned by solvingmax (\u03b1) - Tr (K\u03b1T) / 2 (14), where \u03b1 Rn and C are the SVR penalty parameters. (The indefinite SVR formulation follows directly as in Section 2.2 and the optimal kernel is learned by solvingmax (\u03b1). (\u03b1T e = 0, \u2212 C)."}, {"heading": "4.2 One-class SVM with indefinite kernels", "text": "The same reformulation can also be applied to single-class carrier vector machines having the formula (see Schoolhead and Smola (2002) \u03c9\u03bd (K) = max {0 \u2264 \u03b1 \u2264 1\u03bdl, \u03b1T e = 1} \u2212 Tr (K\u03b1\u03b1T) / 2 (18), where \u03b1 Rn, \u03bd is the single-class SVM parameter and l is the number of training points. The indefinite single-class SVM formulation follows again as with binary SVM and SVR; the optimum kernel is learned by solvingmax {\u03b1T e = 1.0 \u2264 \u2264 1\u03bdl} min {K 0} \u2212 12 Tr (K\u03b1T) + \u03b5 K \u2212 K0 \u04322 F (19) in the variables K \u0432 Sn and \u03b1 \u0438 Rn; the inner minimizing problem is identical with the indefinite SVR and the optimum kernel has the same shape as in Corollary 2. Plugging (16) yields a slightly higher result (19)."}, {"heading": "4.3 Learning from Mercer kernels", "text": "While our central motivation is to use indefinite cores for SVM classification, we also want to analyze what happens when a Mercer kernel is used as input in (4). In this case, we learn another kernel that lowers the upper limit of generalization performance and produces disruptive support vectors. We can again interpret the input as a noisy kernel, and as one that achieves suboptimal performance. If the input kernel is the best kernel to use (i.e., it is not noisy), we will find that our framework achieves optimal performance as it tends toward infinity (through cross-validation), otherwise we will simply learn a better kernel by using a finite ecosystem. If the similarity measurement K0 is positively semidefined, the proxy kernel in theory 1 simplifies the proxy core K-1 to a ranking-one update of K0K-1 = K0 + (Y)."}, {"heading": "4.4 Componentwise penalties", "text": "We generalize the problem (4) tomax {\u03b1T y = 0,0 \u2264 \u03b1 \u2264 C} min {K 0} \u03b1T e \u2212 12 Tr (K (Y \u03b1) (Y \u03b1) T) + \u2211 i, jHij (Kij \u2212 K0ij) 2 (23), where H \u2212 Rn is the support vector coefficient and the label matrix Y = diag (y), where the optimal kernel K \u00b2 can be derived explicitly as follows. Theorem 3 In view of a similarity matrix K0 \u2012 S n, a vector \u03b1 Rn of the support vector coefficient and the label matrix Y = diag (y), where H-One is Hij = hihj, the optimal kernel in the problem (23) has the express formK \u2212 Kham \u2212 Rn of the support vector coefficient and the label matrix Y = diag (y), which is H-jj (Hiy)."}, {"heading": "5 Experiments", "text": "In this section, we compare the generalization performance of our technique with other methods that apply SVM classification to indefinite similarities. We also examine the classification performance using Mercer cores. We conclude with experiments that show the convergence of our algorithms. All experiments with Mercer cores use the LIBSVM library."}, {"heading": "5.1 Generalization with indefinite kernels", "text": "We compare our method for SVM classification with indefinite kernels so as not to be optimal.) We compare the first three methods for SVM classification with the first three methods we discussed earlier. (The first three methods perform spectral transformations on the indefinite kernel.) We compare the first three methods to perform spectral transformations on the indefinite kernel. (The first three methods perform spectral transformations on the indefinite kernel and add a constant to each eigenvalue, which makes them all positive. See Wu et al. (2005) for more details. We also implement an SVM modification (referred to as Mod SVM) proposed in Lin and Lin (2003), where a constant is added to each eigenvalue, which makes an objective function convex by replacing the indefinite kernel with the identity matrix. The kernel appears only in linear inequality constraints that ensure the ite separates the data from the stationary one, and finally we compare our direct classification with an SVM solution that is optimal."}, {"heading": "5.2 Generalization with Mercer kernels", "text": "Using these time-linear and Gaussian (both positive semidefinitive, i.e. Mercer) cores on the USPS dataset, we will now compare the classification performance with regular SVM and the punished kernel learning problem (22) of Section 4.3, which we call PerturbSVM here. We will also test these two techniques on positive semidefinitive cores formed using noisy USPS datasets (created by adding uniformly distributed noise in [-1.1] each pixel prior to normalization to [0.1]). In this case, PerturbSVM can be considered an optimally denosified support vector machine classification. We will cross over again on a training set and test on the same independent group of examples used in the above experiments. Optimal parameters from the classification of unperformed data were used to train disrupted data classifiers."}, {"heading": "5.3 Convergence", "text": "We have run our two algorithms on datasets generated by random disruption of the four USPS datasets used above. Average results and standard deviations are shown on the semilogical scale in Figure 1 (note that the codes were not stopped here and that the target gap is usually much smaller than 10 \u2212 8). However, as expected, ACCPM converges much faster (indeed linear) to higher precision, while any iteration requires the solution of a linear program of size n. Initially, the gradient projection gap converges with higher precision, but each iteration requires only an update to eigenvalue decomposition. Finally, we examine the computation time of indefiniteSVM using the projected gradient method and ACCPM and compare it with the SIQCLP method of Chen and Ye (2008). Figure 2 shows the total runtime (left) and the average runtime (right) for different problem dimensions."}, {"heading": "6 Conclusion", "text": "We have proposed a technique to support vector machine classification with indeterminate cores, where a proxy kernel can be explicitly calculated. We also show how this framework can be used to improve generalization performance with potentially noisy Mercer cores, as well as to extend it to other core methods such as support vector regression and single-class support vector engines. We provide two proven convergent algorithms to solve this problem on relatively large datasets. Our initial experiments show that our method performs reasonably well compared to other techniques that deal with indefinite cores in the SVM framework, and provides a clear interpretation at the boundary for some of these heuristics."}, {"heading": "Acknowledgements", "text": "We are very grateful to Ma'tya's Sustik for its first-class update of the self-depreciation code, and Jianhui Chen and Jieping Ye for their SIQCLP Matlab code. We would also like to acknowledge NSF funding DMS-0625352, NSF CDI funding SES-0835550, NSF CAREER Prize, Peek Junior Faculty Scholarship, and Howard B. Wentz Jr. Junior Faculty Award."}], "references": [{"title": "UCI Machine Learning Repository, University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Asuncion", "D. Newman"], "venue": null, "citeRegEx": "Asuncion and Newman,? \\Q2007\\E", "shortCiteRegEx": "Asuncion and Newman", "year": 2007}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "Proceedings of the 21st International Conference on Machine Learning", "citeRegEx": "Bach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2004}, {"title": "Duality and geometry in svm classifiers", "author": ["K.P. Bennet", "E.J. Bredensteiner"], "venue": "Proceedings of the 17th International conference on Machine Learning", "citeRegEx": "Bennet and Bredensteiner,? \\Q2000\\E", "shortCiteRegEx": "Bennet and Bredensteiner", "year": 2000}, {"title": "Nonlinear Programming, 2nd Edition, Athena Scientific", "author": ["D. Bertsekas"], "venue": null, "citeRegEx": "Bertsekas,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas", "year": 1999}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "Software available at http://www.csie.ntu.edu.tw/cjlin/libsvm", "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Training SVM with indefinite kernels", "author": ["J. Chen", "J. Ye"], "venue": "Proceedings of the 25th International Conference on Machine Learning", "citeRegEx": "Chen and Ye,? \\Q2008\\E", "shortCiteRegEx": "Chen and Ye", "year": 2008}, {"title": "Learning kernels from indefinite similarities", "author": ["Y. Chen", "M.R. Gupta", "B. Recht"], "venue": "Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Permanents, transport polytopes and positive definite kernels on histograms", "author": ["M. Cuturi"], "venue": "Proceedings of the Twentieth International Joint Conference on Artificial Intelligence", "citeRegEx": "Cuturi,? \\Q2007\\E", "shortCiteRegEx": "Cuturi", "year": 2007}, {"title": "Convex nondifferentiable optimization: A survey focused on the analytic center cutting plane method", "author": ["Goffin", "J.-L", "Vial", "J.-P"], "venue": "Optimization Methods and Software", "citeRegEx": "Goffin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Goffin et al\\.", "year": 2002}, {"title": "Feature space interpretation of SVMs with indefinite kernels", "author": ["B. Haasdonk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Haasdonk,? \\Q2005\\E", "shortCiteRegEx": "Haasdonk", "year": 2005}, {"title": "Tangent distance kernels for support vector machines", "author": ["B. Haasdonk", "D. Keysers"], "venue": "Proc. of the 16th Int. Conf. on Pattern Recognition", "citeRegEx": "Haasdonk and Keysers,? \\Q2002\\E", "shortCiteRegEx": "Haasdonk and Keysers", "year": 2002}, {"title": "Semi-infinite programming: Theory, methods, and applications", "author": ["R. Hettich", "K.O. Kortanek"], "venue": "SIAM Review", "citeRegEx": "Hettich and Kortanek,? \\Q1993\\E", "shortCiteRegEx": "Hettich and Kortanek", "year": 1993}, {"title": "Computing the nearest correlation matrix\u2014a problem from finance", "author": ["N. Higham"], "venue": "IMA Journal of Numerical Analysis", "citeRegEx": "Higham,? \\Q2002\\E", "shortCiteRegEx": "Higham", "year": 2002}, {"title": "Convex Analysis and Minimization", "author": ["Hiriart-Urruty", "J.-B", "C. Lemar\u00e9chal"], "venue": null, "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 1993}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Hull,? \\Q1994\\E", "shortCiteRegEx": "Hull", "year": 1994}, {"title": "Learning low-rank kernel matrices", "author": ["B. Kulis", "M. Sustik", "I. Dhillon"], "venue": "Proceedings of the 23rd International Conference on Machine Learning", "citeRegEx": "Kulis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2006}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Kernel-based integration of genomic data using semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "M.I. Jordan", "W.S. Noble"], "venue": "In Kernel Methods in Computational Biology,", "citeRegEx": "Lanckriet et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2003}, {"title": "Nonsmooth analysis of eigenvalues", "author": ["A. Lewis"], "venue": "Mathematical Programming", "citeRegEx": "Lewis,? \\Q1999\\E", "shortCiteRegEx": "Lewis", "year": 1999}, {"title": "A study on sigmoid kernel for SVM and the training of non-PSD kernels by SMO-type methods", "author": ["Lin", "H.-T", "C.-J"], "venue": "National Taiwan University,", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Introductory Lectures on Convex Optimization, Springer", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov,? \\Q2003\\E", "shortCiteRegEx": "Nesterov", "year": 2003}, {"title": "Learning with non-positive kernels", "author": ["C.S. Ong", "X. Mary", "S. Canu", "A.J. Smola"], "venue": "Proceedings of the 21st International Conference on Machine Learning", "citeRegEx": "Ong et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2004}, {"title": "Learning the kernel with hyperkernels", "author": ["C.S. Ong", "A.J. Smola", "R.C. Williamson"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2005}, {"title": "Large-scale optimization of eigenvalues", "author": ["M. Overton"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Overton,? \\Q1992\\E", "shortCiteRegEx": "Overton", "year": 1992}, {"title": "Protein homology detection using string alignment kernels", "author": ["H. Saigo", "J.P. Vert", "N. Ueda", "T. Akutsu"], "venue": "Bioinformatics", "citeRegEx": "Saigo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Saigo et al\\.", "year": 2004}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Transformation invariance in pattern recognition-tangent distance and tangent propogation", "author": ["P.Y. Simard", "Y.A.L. Cun", "J.S. Denker", "B. Victorri"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "Simard et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1998}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenberg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Sonnenberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenberg et al\\.", "year": 2006}, {"title": "Distances and (indefinite) kernels for set of objects", "author": ["A. Wo\u017anica", "A. Kalousis", "M. Hilario"], "venue": "Proceedings of the 6th International Conference on Data Mining pp", "citeRegEx": "Wo\u017anica et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wo\u017anica et al\\.", "year": 2006}, {"title": "An analysis of transformation on non-positive semidefinite similarity matrix for kernel machines", "author": ["G. Wu", "E.Y. Chang", "Z. Zhang"], "venue": "Proceedings of the 22nd International Conference on Machine Learning", "citeRegEx": "Wu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2005}, {"title": "An assessment of alternative strategies for constructing EMDbased kernel functions for use in an SVM for image classification", "author": ["A. Zamolotskikh", "P. Cunningham"], "venue": "Technical Report UCD-CSI-2007-3 ", "citeRegEx": "Zamolotskikh and Cunningham,? \\Q2004\\E", "shortCiteRegEx": "Zamolotskikh and Cunningham", "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "(See Ong et al. (2004) for a discussion.", "startOffset": 5, "endOffset": 23}, {"referenceID": 19, "context": "(See Ong et al. (2004) for a discussion.) In particular, an application of classification with indefinite kernels to image classification using Earth Mover\u2019s Distance was discussed in Zamolotskikh and Cunningham (2004). Similarity measures for protein sequences such as the Smith-Waterman and BLAST scores are indefinite yet have provided hints for constructing useful positive semidefinite kernels such as those decribed in Saigo et al.", "startOffset": 5, "endOffset": 219}, {"referenceID": 19, "context": "(See Ong et al. (2004) for a discussion.) In particular, an application of classification with indefinite kernels to image classification using Earth Mover\u2019s Distance was discussed in Zamolotskikh and Cunningham (2004). Similarity measures for protein sequences such as the Smith-Waterman and BLAST scores are indefinite yet have provided hints for constructing useful positive semidefinite kernels such as those decribed in Saigo et al. (2004) or have been transformed into positive semidefinite kernels with good empirical performance (see Lanckriet et al.", "startOffset": 5, "endOffset": 445}, {"referenceID": 16, "context": "(2004) or have been transformed into positive semidefinite kernels with good empirical performance (see Lanckriet et al. (2003), for example).", "startOffset": 104, "endOffset": 128}, {"referenceID": 18, "context": "described in Simard et al. (1998) or Haasdonk and Keysers (2002), are invariant to various simple image transformations and have also shown excellent performance in optical character recognition.", "startOffset": 13, "endOffset": 34}, {"referenceID": 7, "context": "(1998) or Haasdonk and Keysers (2002), are invariant to various simple image transformations and have also shown excellent performance in optical character recognition.", "startOffset": 10, "endOffset": 38}, {"referenceID": 6, "context": "Finally, it is sometimes impossible to prove that some kernels satisfy Mercer\u2019s condition or the numerical complexity of evaluating the exact positive kernel is too high and a proxy (and not necessarily positive semidefinite) kernel has to be used instead (see Cuturi (2007), for example).", "startOffset": 261, "endOffset": 275}, {"referenceID": 6, "context": "Finally, it is sometimes impossible to prove that some kernels satisfy Mercer\u2019s condition or the numerical complexity of evaluating the exact positive kernel is too high and a proxy (and not necessarily positive semidefinite) kernel has to be used instead (see Cuturi (2007), for example). In both cases, our method allows us to bypass these limitations. Our objective here is to derive efficient algorithms to directly use these indefinite similarity measures for classification. Our work closely follows, in spirit, recent results on kernel learning (see Lanckriet et al. (2004) or Ong et al.", "startOffset": 261, "endOffset": 581}, {"referenceID": 6, "context": "Finally, it is sometimes impossible to prove that some kernels satisfy Mercer\u2019s condition or the numerical complexity of evaluating the exact positive kernel is too high and a proxy (and not necessarily positive semidefinite) kernel has to be used instead (see Cuturi (2007), for example). In both cases, our method allows us to bypass these limitations. Our objective here is to derive efficient algorithms to directly use these indefinite similarity measures for classification. Our work closely follows, in spirit, recent results on kernel learning (see Lanckriet et al. (2004) or Ong et al. (2005)), where the kernel matrix is learned as a linear combination of given kernels, and the result is explicitly constrained to be positive semidefinite.", "startOffset": 261, "endOffset": 602}, {"referenceID": 1, "context": "While this problem is numerically challenging, Bach et al. (2004) adapted the SMO algorithm to solve the case where the kernel is written as a positively weighted combination of other kernels.", "startOffset": 47, "endOffset": 66}, {"referenceID": 1, "context": "While this problem is numerically challenging, Bach et al. (2004) adapted the SMO algorithm to solve the case where the kernel is written as a positively weighted combination of other kernels. In our setting here, we never numerically optimize the kernel matrix because this part of the problem can be solved explicitly, which means that the complexity of our method is substantially lower than that of classical kernel learning algorithms and closer in practice to the algorithm used in Sonnenberg et al. (2006), who formulate the multiple kernel learning problem of Bach et al.", "startOffset": 47, "endOffset": 513}, {"referenceID": 1, "context": "While this problem is numerically challenging, Bach et al. (2004) adapted the SMO algorithm to solve the case where the kernel is written as a positively weighted combination of other kernels. In our setting here, we never numerically optimize the kernel matrix because this part of the problem can be solved explicitly, which means that the complexity of our method is substantially lower than that of classical kernel learning algorithms and closer in practice to the algorithm used in Sonnenberg et al. (2006), who formulate the multiple kernel learning problem of Bach et al. (2004) as a semi-infinite linear program and solve it with a column generation technique similar to the analytic center cutting plane method we use here.", "startOffset": 47, "endOffset": 587}, {"referenceID": 8, "context": "A first direction embeds data in a pseudo-Euclidean (pE) space: Haasdonk (2005), for example, formulates the classification problem with an indefinite kernel as that of minimizing the distance between convex hulls formed from the two categories of data embedded in the pE space.", "startOffset": 64, "endOffset": 80}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.", "startOffset": 5, "endOffset": 37}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example).", "startOffset": 5, "endOffset": 369}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example).", "startOffset": 5, "endOffset": 408}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example). Yet another option is to reformulate either the maximum margin problem or its dual in order to use the indefinite kernel in a convex optimization problem. One reformulation suggested in Lin and Lin (2003) replaces the indefinite kernel by the identity matrix and maintains separation using linear constraints.", "startOffset": 5, "endOffset": 628}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example). Yet another option is to reformulate either the maximum margin problem or its dual in order to use the indefinite kernel in a convex optimization problem. One reformulation suggested in Lin and Lin (2003) replaces the indefinite kernel by the identity matrix and maintains separation using linear constraints. This method achieves good performance, but the convexification procedure is hard to interpret. Directly solving the nonconvex problem sometimes gives good results as well (see Wo\u017anica et al. (2006) and Haasdonk (2005)) but offers no guarantees on performance.", "startOffset": 5, "endOffset": 931}, {"referenceID": 2, "context": "(See Bennet and Bredensteiner (2000) for a discussion on geometric interpretations in SVM.) Another direction applies direct spectral transformations to indefinite kernels: flipping the negative eigenvalues or shifting the eigenvalues and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel (see Wu et al. (2005) and Zamolotskikh and Cunningham (2004), for example). Yet another option is to reformulate either the maximum margin problem or its dual in order to use the indefinite kernel in a convex optimization problem. One reformulation suggested in Lin and Lin (2003) replaces the indefinite kernel by the identity matrix and maintains separation using linear constraints. This method achieves good performance, but the convexification procedure is hard to interpret. Directly solving the nonconvex problem sometimes gives good results as well (see Wo\u017anica et al. (2006) and Haasdonk (2005)) but offers no guarantees on performance.", "startOffset": 5, "endOffset": 951}, {"referenceID": 16, "context": "We formulate the kernel learning problem as in Lanckriet et al. (2004), where the authors minimize an upper bound on the misclassification probability when using SVM with a given kernel K.", "startOffset": 47, "endOffset": 71}, {"referenceID": 5, "context": "A reformulation of problem (4) appears in Chen and Ye (2008) where the authors move the inner minimization problem to the constraints and get the following semi-infinite quadratically constrained linear program (SIQCLP):", "startOffset": 42, "endOffset": 61}, {"referenceID": 5, "context": "In Section 3, we describe algorithms to solve our eigenvalue optimization problem in (7), as well as an algorithm from Chen and Ye (2008) that solves the different formulation in (8), for completeness.", "startOffset": 119, "endOffset": 138}, {"referenceID": 5, "context": "For completeness, we also describe an exchange method from Chen and Ye (2008) used to solve problem (8), where the numerical bottleneck is a quadratically constrained linear program solved at each iteration.", "startOffset": 59, "endOffset": 78}, {"referenceID": 15, "context": "We refer the reader to Kulis et al. (2006) for another kernel learning example using this method.", "startOffset": 23, "endOffset": 43}, {"referenceID": 15, "context": "We refer the reader to Kulis et al. (2006) for another kernel learning example using this method. Given the eigenvalue decomposition X = V DV T , by changing basis this problem can be reduced to the decomposition of the diagonal plus rank-one matrix, D+\u03c1uuT , where u = V T\u03b1. First, the updated eigenvalues are determined by solving the secular equations det(D + \u03c1uu \u2212 \u03bbI) = 0, which can be done in O(n). While there is an explicit solution for the eigenvectors corresponding to these eigenvalues, they are not stable because the eigenvalues are approximated. This instability is circumvented by computing a vector \u00fb such that approximate eigenvalues \u03bb are the exact eigenvalues of the matrix D + \u03c1\u00fb\u00fbT , then computing its stable eigenvectors explicitly, where both steps can be done in O(n) time. The key is that D + \u03c1\u00fb\u00fbT is close enough to our original matrix so that the eigenvalues and eigenvectors are stable approximations of the true values. Finally, the eigenvectors of our original matrix are computed as VW , with W as the stable eigenvectors of D + \u03c1\u00fb\u00fbT . Updating the eigenvalue decomposition is reduced to an O(n) procedure plus one matrix multiplication, which is then the complexity of one gradient computation. We note that eigenvalues of symmetric matrices are not differentiable when some of them have multiplicities greater than one (see Overton (1992) for a discussion), but a subgradient can be used instead of the gradient in all the algorithms detailed here.", "startOffset": 23, "endOffset": 1372}, {"referenceID": 15, "context": "We refer the reader to Kulis et al. (2006) for another kernel learning example using this method. Given the eigenvalue decomposition X = V DV T , by changing basis this problem can be reduced to the decomposition of the diagonal plus rank-one matrix, D+\u03c1uuT , where u = V T\u03b1. First, the updated eigenvalues are determined by solving the secular equations det(D + \u03c1uu \u2212 \u03bbI) = 0, which can be done in O(n). While there is an explicit solution for the eigenvectors corresponding to these eigenvalues, they are not stable because the eigenvalues are approximated. This instability is circumvented by computing a vector \u00fb such that approximate eigenvalues \u03bb are the exact eigenvalues of the matrix D + \u03c1\u00fb\u00fbT , then computing its stable eigenvectors explicitly, where both steps can be done in O(n) time. The key is that D + \u03c1\u00fb\u00fbT is close enough to our original matrix so that the eigenvalues and eigenvectors are stable approximations of the true values. Finally, the eigenvectors of our original matrix are computed as VW , with W as the stable eigenvectors of D + \u03c1\u00fb\u00fbT . Updating the eigenvalue decomposition is reduced to an O(n) procedure plus one matrix multiplication, which is then the complexity of one gradient computation. We note that eigenvalues of symmetric matrices are not differentiable when some of them have multiplicities greater than one (see Overton (1992) for a discussion), but a subgradient can be used instead of the gradient in all the algorithms detailed here. Lewis (1999) shows how to compute an approximate subdifferential of the k-th largest eigenvalue of a symmetric matrix.", "startOffset": 23, "endOffset": 1495}, {"referenceID": 3, "context": "2 Projected gradient method The projected gradient method takes a steepest descent step, then projects the new point back onto the feasible region (see Bertsekas (1999), for example).", "startOffset": 152, "endOffset": 169}, {"referenceID": 20, "context": "See Nesterov (2003) for a complete discussion.", "startOffset": 4, "endOffset": 20}, {"referenceID": 3, "context": "The method is described in Algorithm 2 (see Bertsekas (1999) for a more complete treatment of cutting plane methods).", "startOffset": 44, "endOffset": 61}, {"referenceID": 5, "context": "The algorithm considered in Chen and Ye (2008) in order to solve problem (8) falls under a class of algorithms called exchange methods (as defined in Hettich and Kortanek (1993)).", "startOffset": 28, "endOffset": 47}, {"referenceID": 5, "context": "The algorithm considered in Chen and Ye (2008) in order to solve problem (8) falls under a class of algorithms called exchange methods (as defined in Hettich and Kortanek (1993)).", "startOffset": 28, "endOffset": 178}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL.", "startOffset": 12, "endOffset": 156}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL. Efficient methods to solve MKL with many kernels is an active area of research, most recently in Rakotomamonjy et al. (2008). There, the authors use a gradient method to solve a reformulation of problem (13) as a smooth maximization problem.", "startOffset": 12, "endOffset": 366}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL. Efficient methods to solve MKL with many kernels is an active area of research, most recently in Rakotomamonjy et al. (2008). There, the authors use a gradient method to solve a reformulation of problem (13) as a smooth maximization problem. Each objective value and gradient computation requires computing a support vector machine, hence each iteration requires several SVM computations which can be speeded up using warm-starting. Furthermore, Chen and Ye (2008) prune inactive constraints at each iteration in order to decrease the number of constraints in the QCLP.", "startOffset": 12, "endOffset": 706}, {"referenceID": 5, "context": "As shown in Chen and Ye (2008), the QCLP can be written as a regularized version of the multiple kernel learning (MKL) problem from Lanckriet et al. (2004), where the number of constraints here is equivalent to the number of kernels in MKL. Efficient methods to solve MKL with many kernels is an active area of research, most recently in Rakotomamonjy et al. (2008). There, the authors use a gradient method to solve a reformulation of problem (13) as a smooth maximization problem. Each objective value and gradient computation requires computing a support vector machine, hence each iteration requires several SVM computations which can be speeded up using warm-starting. Furthermore, Chen and Ye (2008) prune inactive constraints at each iteration in order to decrease the number of constraints in the QCLP. Complexity. No rate of convergence is known for this algorithm, but the duality gap given in Chen and Ye (2008) is shown to monotonically decrease.", "startOffset": 12, "endOffset": 923}, {"referenceID": 5, "context": "A Matlab implementation of the exchange method (due to the authors of Chen and Ye (2008)) that uses MOSEK (MOSEK ApS 2008) to solve problem (13) is compared against the projected gradient method in Section 5.", "startOffset": 70, "endOffset": 89}, {"referenceID": 25, "context": "2 One-class SVM with indefinite kernels The same reformulation can also be applied to one-class support vector machines which have the formulation (see Sch\u00f6lkopf and Smola (2002))", "startOffset": 152, "endOffset": 179}, {"referenceID": 12, "context": "2 of Higham (2002).", "startOffset": 5, "endOffset": 19}, {"referenceID": 0, "context": "We finally analyze three data sets (diabetes, german and ala) from the UCI repository (Asuncion and Newman 2007) using the indefinite sigmoid kernel.", "startOffset": 86, "endOffset": 112}, {"referenceID": 24, "context": "See Wu et al. (2005) for further details.", "startOffset": 4, "endOffset": 21}, {"referenceID": 24, "context": "See Wu et al. (2005) for further details. We also implemented an SVM modification (denoted Mod SVM ) suggested in Lin and Lin (2003) where a nonconvex quadratic objective function is made convex by replacing the indefinite kernel with the identity matrix.", "startOffset": 4, "endOffset": 133}, {"referenceID": 10, "context": "We first experiment on data from the USPS handwritten digits database Hull (1994) using the indefinite Simpson score and the one-sided tangent distance kernel to compare two digits.", "startOffset": 70, "endOffset": 82}, {"referenceID": 7, "context": "Our experiments symmetrize the one-sided tangent distance using the square of the mean tangent distance defined in Haasdonk and Keysers (2002) and make it a similarity measure by negative exponentiation.", "startOffset": 115, "endOffset": 143}, {"referenceID": 0, "context": "We finally analyze three data sets (diabetes, german and ala) from the UCI repository (Asuncion and Newman 2007) using the indefinite sigmoid kernel. The data is randomly divided into training and testing data. We apply 5-fold cross validation and use an average of the accuracy and recall measures (described below) to determine the optimal parameters C, \u03c1, and any kernel inputs. We then train a model with the full training set and optimal parameters and test on the independent test set. Table 1 provides summary statistics for these data sets, including the minimum and maximum eigenvalues of the training similarity matrices. We observe that the Simpson are highly indefinite, while the one-sided tangent distance kernel is nearly positive semidefinite. The spectrum of sigmoid kernels varies greatly across examples because it is very sensitive to the sigmoid kernel parameters. Table 2 compares accuracy, recall, and their average for denoise, flip, shift, modified SVM, direct SVM and the indefinite SVM algorithm described in this work. Based on the interpretation from Section 2.3, Indefinite SVM should be expected to perform at least as well as denoise; if denoise were a good transformation, then cross-validation over \u03c1 should choose a high penalty that makes Indefinite SVM and denoise nearly equivalent. The rank-one update provides more flexibility for the transformation and similarities concerning data points xi that are easily classified (\u03b1i = 0) are not modified by the rank-one update. Further interpretation for the specific rank-one update is not currently known. However, Chen et al. (2009) recently proposed spectrum modifications in a similar manner to Indefinite SVM.", "startOffset": 87, "endOffset": 1618}, {"referenceID": 5, "context": "and ACCPM and compare them with the SIQCLP method of Chen and Ye (2008). Figure 2 shows total runtime (left) and average iteration runtime (right) for varying problem dimensions on an example from the USPS data with Simpson kernel.", "startOffset": 53, "endOffset": 72}, {"referenceID": 5, "context": "and ACCPM and compare them with the SIQCLP method of Chen and Ye (2008). Figure 2 shows total runtime (left) and average iteration runtime (right) for varying problem dimensions on an example from the USPS data with Simpson kernel. Experiments are averaged over 10 random data subsets and we fix C = 10 with a tolerance of .1 for the duality gap. For the projected gradient method, increasing \u03c1 increases the number of iterations to converge; notice that the average time per iteration does not vary over \u03c1. SIQCLP also requires more iterations to converge for higher \u03c1, however the average iteration time seems to be less for higher \u03c1, so no clear pattern is seen when varying \u03c1. Note that the number of iterations required varies widely (between 100 and 2000 iterations in this experiment) as a function of \u03c1, C, the chosen kernel and the stepsize. Results for ACCPM and SIQCLP are shown only up to dimensions 500 and 300, respectively, because this sufficiently demonstrates that the projected gradient method is more efficient. ACCPM clearly suffers from the complexity of the analytic center problem each iteration. However, improvements can be made in the SIQCLP implementation such as using a regularized version of an efficient MKL solver (e.g. Rakotomamonjy et al. (2008)) to solve problem (13) rather than MOSEK.", "startOffset": 53, "endOffset": 1281}], "year": 2009, "abstractText": "We propose a method for support vector machine classification using indefinite kernels. Instead of directly minimizing or stabilizing a nonconvex loss function, our algorithm simultaneously computes support vectors and a proxy kernel matrix used in forming the loss. This can be interpreted as a penalized kernel learning problem where indefinite kernel matrices are treated as noisy observations of a true Mercer kernel. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the projected gradient or analytic center cutting plane methods. We compare the performance of our technique with other methods on several standard data sets.", "creator": "LaTeX with hyperref package"}}}