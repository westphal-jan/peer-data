{"id": "0806.4686", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2008", "title": "Sparse Online Learning via Truncated Gradient", "abstract": "We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: The degree of sparsity is continuous -- a parameter controls the rate of sparsification from no sparsification to total sparsification. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online learning guarantees. The approach works well empirically. We apply the approach to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.", "histories": [["v1", "Sat, 28 Jun 2008 14:19:50 GMT  (121kb,D)", "http://arxiv.org/abs/0806.4686v1", null], ["v2", "Fri, 4 Jul 2008 01:58:32 GMT  (121kb,D)", "http://arxiv.org/abs/0806.4686v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["john langford", "lihong li", "tong zhang 0001"], "accepted": true, "id": "0806.4686"}, "pdf": {"name": "0806.4686.pdf", "metadata": {"source": "CRF", "title": "Sparse Online Learning via Truncated Gradient", "authors": ["John Langford", "Lihong Li"], "emails": ["jl@yahoo-inc.com", "lihong@cs.rutgers.edu", "tongz@rci.rutgers.edu"], "sections": [{"heading": null, "text": "We propose a generic method called blunt gradient to make the weights of online learning algorithms with convex loss functions frugal, which has several key characteristics: 1. The degree of frugality is a continuous parameter that controls the savings rate from frugality to total frugality cation.2. The approach is theoretically motivated, and one case can be considered an online subset of the popular L1 batch-setting regulation method. We demonstrate that small frugality rates lead to little additional regret in terms of typical online learning warranties.3. The approach works well empirically. We apply the approach to multiple data sets, and that significant frugality can be seen in datasets with a large number of features."}, {"heading": "1 Introduction", "text": "For example, the largest data set we use here has more than 107 sparse examples and 109 features with about 1011 bytes. In this context, many common approaches fail simply because they cannot load the data set into memory or because they are not sufficiently weighted. There are about two approaches that can work: 1. Paralyzing a batch learning algorithm across many machines (e.g. [3]). Streaming the examples to an online learning algorithm (e.g. [8], [2], and [5]). This paper focuses on the second approach. Typical online learning algorithms carry at least one weight for each feature that is too much in some applications."}, {"heading": "1.1 What Others Do", "text": "\"The green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green\" for the green for the green \"for the green for the green for the green\" for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green"}, {"heading": "2 Online Learning with GD", "text": "In the setting of the standard online learning, we are interested in sequential prediction problems, where repeatedly i = 1, 2,..: 1. An unmarked example xi arrives. 2. We make a prediction based on existing weights wi + 1 (wi). 3. We observe yi, let zi = (xi, yi), and suffer some known losses L (wi, zi) convex in the parameters wi.4. We update the weights according to a rule: wi + 1 (wi). We want to establish an update rule f, which allows us to bind the sum of the lossomest i = 1 L (wi, zi) convex in the parameters wi.4. To this end, we start with the standard stochastic parentage rule f (wi).We want an update rule f (wi, zi), which allows us to bind the sum of the lossomest i = 1 L (wi, zi), as well as achieve thriftiness."}, {"heading": "3 Sparse Online Learning", "text": "In this section, we will examine various methods for achieving thrift in online learning. The first idea is a simple coe cient rounding, which is the most natural method. Afterwards, we will look at its full online implementation and another method, which is the online equivalent of L1 regularization in batch learning. As we will see, all these ideas are closely related."}, {"heading": "3.1 Simple Coe cient Rounding", "text": "To achieve sparseness, the most natural method is to lower small coefficients (which are not greater than a threshold \u03b8 > 0) to zero after each K-Online step. That is, if i / K is not an integer, we use the standard GD rule in (3); if i / K is an integer, we change the rule as follows: f (wi) = T0 (wi \u2212 \u0443 1L (wi, zi), \u03b8), (4) where for a vector v = [v1,.., vd].Rd, and a scalar progression point \u2265 0, T0 (v, \u03b8) = [T0 (v1, \u03b8),., T0 (vd, su)], withT0 (vj, TB) = {0, if | vj | vj | vj | vj otherwise is interresented. that is, we run a standard stochastic gradient rule and round the updated coefficients towards zero."}, {"heading": "3.2 A Sub-gradient Algorithm for L1 Regularization", "text": "In our experiments, we combine rounding at the end of the training with a simple online method for L1 regularization with a regularization parameter g > 0: f (wi) = wi \u2212 \u03b7: 1L (wi, zi) \u2212 \u03b7g sgn (wi), (5) where for a vector v = [v1,.., vd], sgn (v) = [sgn (v1),..., sgn (vd)] and sgn (vj) = 1 at vj > 0, sgn (vj) = \u2212 1 at vj < 0 and sgn (vj) = 0 at vj = 0. In the experiments, the online method (5) plus rounding at the end is used as a simple baseline."}, {"heading": "3.3 Truncated Gradient", "text": "To get an online version of the simple rounding rule in (4), we can observe that the direct rounding to zero is not too aggressive. A less aggressive version is to shrink the Coe cient to zero if we do not consider this idea as a method of shrinkage. However, the amount of shrinkage is measured by a gravitational parameter that uses the gravitational parameters gi > 0: f (wi) = T1 (wi) = T1 (v1, zi), s (6), where for a vector v = [v1), and a scalar g (0, v1), s (v1), os (v1), os (v1)."}, {"heading": "3.4 Regret Analysis", "text": "We assume that L (f, z) + B (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z) + 1 (f, z)."}, {"heading": "3.5 Stochastic Setting", "text": "In this context, we can simplify the analysis instead of assuming that we go through the data one by one, that each additional data point from the training data is randomly drawn with equal probability, corresponding to the standard stochastic optimization environment in which the observed samples are iid from some underlying distributions. The following result is a simple sequence of Theorem 3.1. For simplicity, we only consider the case with equal probability. Theorem 3.2 Let us consider a set of training data zi = (xi, yi) for i = 1., n, and letR (g) = 1 n."}, {"heading": "4 Truncated Gradient Algorithm for Least Squares", "text": "The method in section 3 can be applied directly to the smallest squares of regression, resulting in algorithm 1, which implements the thrift for square losses according to Equation (7). In the description, we use the parent symbol wj to represent the j-th component of the vector w (to denote the erentiate of wi that we have used to denote the i-th weight vector). In practice, we only consider the following choice: gi = {Kg, if i / K is an integer0 otherwise an integer0. This can give a more aggressive truncation (i.e. thrift) after each K-th iteration. Since we do not have a theorem on how much more thrift can be gained from this idea, its e ect will only be examined by experiments."}, {"heading": "5 E cient Implementation", "text": "The learning rate in Vowpal Wabbit is controllable, as is a constant learning rate (and rates between programs), the learning rate in Vowpal Wabbit is controllable, the learning rate in Vowpal Wabbit is controllable, and the learning rate in Vowpal Wabbit is controllable, and the learning rate in Vowpal Wabbit is controllable, and the learning rate in Vowpal Wabbit is controllable, and the learning rate in Vowpal Wabbit is controllable, the learning rate in Vowpal Wabbit is controllable, and the learning rate in Vowpal Wabbit is controllable."}, {"heading": "6 Empirical Results", "text": "We applied Vowpal Wabbit with the Sparsify option as described in the previous section to a selection of datasets, including eleven datasets from the UCI repository [1], the much larger rcv1 [7] dataset, and a large private dataset Big _ Ads related to predicting ad interests. While UCI datasets are useful for benchmark purposes, rcv1 and Big _ Ads are more interesting because they contain real datasets with a large number of features, many of which are less meaningful to predicting than others. The datasets are summarized in Table 1. The UCI datasets we use do not have many features, and a large portion of these features are expected to be useful for predicting. For comparison purposes and to better demonstrate the behavior of our algorithm, we added 1,000 random binary features to these datasets."}, {"heading": "6.1 Feature Sparsi cation of Truncated Gradient Descent", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcnlrVo"}, {"heading": "6.2 The E ects of K", "text": "As we have already explained, the use of a K value greater than 1 in the truncated gradient and in the rounding algorithms may be desirable, and this advantage is demonstrated empirically here. In particular, we are trying it in both algorithms with K = 1, K = 10 and K = 20. As before, cross validation is used to select parameters in the rounding algorithm, including the learning rate \u03b7, the number of data runs during the training and the learning rate that decreases over the training phases. Figures 3 and 4 show the AUC vs. number of feature plans, where each data point is generated by executing a corresponding algorithm using a certain value of g (for truncated gradients) and \u03b8 (for the rounding algorithm). We used Medicare = \u221e in truncated gradients gradients. In truncated gradients, the performances with K = 10 or 20 are at least as good as the performances with K = 10, and with spambase data sets of 10, respectively, the number of K is achieved from the same period of use = 94 to the same number of features =."}, {"heading": "6.3 The E ects of \u03b8 in Truncated Gradient", "text": "The rounding algorithm is similar to the truncated gradient at \u03b8 = g because of its similarity to the truncated gradient. As before, we used cross-validation to select parameters for each tried \u03b8 value, and focused on AUC metrics in the eight UCI classification tasks, except for the degenerated one from wpbc. In both algorithms, we set K = 10. Figure 5 shows the diagrams AUC vs. number of characteristics, in which each data point is generated by 14 execution of the respective algorithms, using a correct value of g (for truncated gradients) and \u03b8 (for the rounding algorithm). Some observations are available. Firstly, the results confirm the observation that the behavior of the truncated gradient with Celsius = g is similar to the rounding algorithm. Secondly, these results suggest that it may be desirable in practice to use a truncated problem because it avoids the local minimum."}, {"heading": "6.4 Comparison to Other Algorithms", "text": "The next series of experiments will be compared with other algorithms in terms of their ability to learn savings and performance. Again, we will focus on the metric in the UCI classification except Wpdc. \u2022 The algorithms for comparison include: \u2022 The abbreviated gradient algorithms: We have exceeded the rounding threshold and changed the focus parameters. \u2022 The rounding algorithm is described in Section 3.1. We have eliminated the cross-validation problems with the cross-validation parameters and changed the rounding threshold. \u2022 The subgradient algorithms are described in Section 3.2: We have used the cross-validation parameters and have changed the regulation parameters g."}, {"heading": "7 Conclusion", "text": "Theorem 3.1 proves that the technique is sound: it never greatly harms performance compared to traditional stochastic gradient descent in adverse situations. Furthermore, we show that the asymptotic solution of an instance of the algorithm essentially corresponds to lasso regression, thus justifying the algorithm's ability to produce sparse weight vectors when the number of features is irrevocably large. The theorem is applied experimentally in a number of problems. In some cases, especially for problems with many irrelevant features, this approach achieves a reduction in the number of features by one or two orders of magnitude."}, {"heading": "A Proof of Theorem 3.1", "text": "The following problem is the essential step in our analysis. \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212 w \u2212"}], "references": [{"title": "University of California, Irvine, School of Information and Computer Sciences, http://www.ics.uci.edu/\u223cmlearn/MLRepository.html", "author": ["Arthur Asuncion", "David J. Newman"], "venue": "UCI machine learning repository,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Worst-case quadratic loss bounds for prediction using linear functions and gradient descent", "author": ["Nicol\u00f2 Cesa-Bianchi", "Philip M. Long", "Manfred Warmuth"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Map-reduce for machine learning on multicore", "author": ["Cheng-Tao Chu", "Sang Kyun Kim", "Yi-An Lin", "YuanYuan Yu", "Gary Bradski", "Andrew Y. Ng", "Kunle Olukotun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithms", "author": ["Nick Littlestone"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "On-line learning of linear functions", "author": ["Nick Littlestone", "Philip M. Long", "Manfred K. Warmuth"], "venue": "Computational Complexity,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Pegasos: Primal Estimated sub- GrAdient SOlver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In Proceedings of the Twenty-Fourth International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Matlab implementation of LASSO, LARS, the elastic net and SPCA", "author": ["Karl Sj\u00f6strand"], "venue": "Version 2.0,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [8], [9], [2], and [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": ", [8], [9], [2], and [5]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": ", [8], [9], [2], and [5]).", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": ", [8], [9], [2], and [5]).", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "1 What Others Do The Lasso algorithm [12] is commonly used to achieve L1 regularization for linear regression.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "The above method has been widely used in online learning such as [9] and [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "The above method has been widely used in online learning such as [9] and [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "For example, the idea has been successfully applied to solve large-scale standard SVM formulations [10, 13].", "startOffset": 99, "endOffset": 107}, {"referenceID": 10, "context": "For example, the idea has been successfully applied to solve large-scale standard SVM formulations [10, 13].", "startOffset": 99, "endOffset": 107}, {"referenceID": 7, "context": "5 Stochastic Setting Stochastic-gradient-based online learning methods can be used to solve large-scale batch optimization problems, often quite successfully [10, 13].", "startOffset": 158, "endOffset": 166}, {"referenceID": 10, "context": "5 Stochastic Setting Stochastic-gradient-based online learning methods can be used to solve large-scale batch optimization problems, often quite successfully [10, 13].", "startOffset": 158, "endOffset": 166}, {"referenceID": 0, "context": "The prediction is clipped to the interval [0, 1], implying that the loss function is not square loss for unclipped predictions outside of this dynamic range.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "We applied Vowpal Wabbit with the e ciently implemented sparsify option, as described in the previous section, to a selection of datasets, including eleven datasets from the UCI repository [1], the much larger dataset rcv1 [7], and a private large-scale dataset Big_Ads related to ad interest prediction.", "startOffset": 189, "endOffset": 192}, {"referenceID": 4, "context": "We applied Vowpal Wabbit with the e ciently implemented sparsify option, as described in the previous section, to a selection of datasets, including eleven datasets from the UCI repository [1], the much larger dataset rcv1 [7], and a private large-scale dataset Big_Ads related to ad interest prediction.", "startOffset": 223, "endOffset": 226}, {"referenceID": 9, "context": "The previous results do not exercise the full power of the approach presented here because they are applied to datasets where standard Lasso regularization [12] is or may be computationally viable.", "startOffset": 156, "endOffset": 160}, {"referenceID": 9, "context": "\u2022 The Lasso [12] for batch L1 regularization: We used a publicly available implementation [11].", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "\u2022 The Lasso [12] for batch L1 regularization: We used a publicly available implementation [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": ", in [13]).", "startOffset": 5, "endOffset": 9}], "year": 2017, "abstractText": "We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous a parameter controls the rate of sparsi cation from no sparsi cation to total sparsi cation. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1-regularization method in the batch setting. We prove that small rates of sparsi cation result in only small additional regret with respect to typical online learning guarantees. 3. The approach works well empirically. We apply the approach to several datasets and nd that for datasets with large numbers of features, substantial sparsity is discoverable.", "creator": "LaTeX with hyperref package"}}}