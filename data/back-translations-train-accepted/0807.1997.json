{"id": "0807.1997", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2008", "title": "Multi-instance learning by treating instances as non-I.I.D. samples", "abstract": "Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multi-instance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments.", "histories": [["v1", "Sat, 12 Jul 2008 20:19:18 GMT  (413kb)", "http://arxiv.org/abs/0807.1997v1", null], ["v2", "Wed, 15 Apr 2009 17:22:40 GMT  (441kb)", "http://arxiv.org/abs/0807.1997v2", null], ["v3", "Fri, 17 Apr 2009 08:43:03 GMT  (436kb)", "http://arxiv.org/abs/0807.1997v3", "ICML, 2009"], ["v4", "Wed, 13 May 2009 16:22:00 GMT  (453kb)", "http://arxiv.org/abs/0807.1997v4", "ICML, 2009"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["zhi-hua zhou", "yu-yin sun", "yu-feng li"], "accepted": true, "id": "0807.1997"}, "pdf": {"name": "0807.1997.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yu-Feng Li"], "emails": ["liyf}@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 080 7,19 97v1 [cs.LG] 1 2Ju l 2"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Related Work", "text": "It is difficult to list all existing methods here. To name just a few: Diverse Density [22], k-next neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [36], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [43], ensemble algorithms MI-Ensembles [42], MIBoosting [36] and MILBoosting \"s multi-boosting,\" logistic regression algorithm MI-LR, \"etc.\""}, {"heading": "3 The MIGraph Method", "text": "In fact, it is in such a way that one will be able to put oneself into another world, in which one must put oneself into another world, in which one is able to put oneself into another world, in which one is able to put oneself into another world, in which one lives in another world, in another world, in another world, in another world, in another world, in another world, in another world, in which one is able to put oneself into another world, in which one lives in another world, in which one is in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another world, in another one, in another world, in another one, in another world, in another one."}, {"heading": "4 Experiments", "text": "We evaluate the performance of the proposed MIGraph method against three tasks, including two multi-level classification tasks and one multi-level regression task."}, {"heading": "4.1 Drug Activity Prediction", "text": "Predicting drug activity is one of the earliest applications of multi-instance learning [14], with Musk data being a real scale studied by many researchers. Here, a bag corresponds to a molecule, and the instances correspond to the alternative low-energy forms of that molecule. Each instance is a 166-dimensional trait vector. For each molecule, there are two sets of data that could bind tightly to the target range of some larger molecules, the molecule is qualified to produce a particular drug, and it is considered a positive drug. Otherwise, it is a negative drug. There are two sets of data, Musk1 and Musk2, both publicly available in the UCI Machine Learning Repository. Musk1 contains 47 positive bags and 45 negative bags, and the number of instances contained in each bag ranges from 2 to 5,40 (on average)."}, {"heading": "4.2 Image Categorization", "text": "In this case, most of them will be able to play by the rules."}, {"heading": "4.3 Multi-Instance Regression", "text": "In this experiment, four multi-level regression datasets are used, described in Amar et al. [1]. The datasets are referred to as LJ-r.f.s, where r is the number of relevant characteristics, f is the number of characteristics, and s is the number of different scale factors used for the relevant characteristics indicating the importance of the characteristics. The suffix S indicates that the dataset only uses labels that do not come close to 1 / 2. Details of the datasets can be found in [1].Leave-one-out test is performed for each dataset in which the parameters are determined by both MIGraph and MI kernels by cross-validation on training sets. The mean square losses are shown in Table 4. For comparison, the table also shows the best results of some multi-level learning methods reported in the literature, where Diverse Density is abbreviated as DD.4. Table 4 shows that multi-level regression sets also work well on these graphs."}, {"heading": "4.4 A Further Study", "text": "The average number of support vectors of these cores and the number of their common support vectors are shown in Table 5.Table 5 that Knode and Kedge are quite different, and their contribution to kgraph is also different. It was shown in [18] that if two cores provide different information and the performance of the two cores is equally good, the upper limit of the generalization error of a combined kernel will be lower. It was also shown in [21] that in practice, even if the cores are not equally good, a combined kernel may be a better choice. This explains why MIGraph is superior to the MI kernel."}, {"heading": "4.5 Using Existing Graph Kernels", "text": "It has been shown that determining all paths, even taking into account the computational complexity we apply in two of their different sections, the number of nodes and edges in Gh and X2, which are represented as graphs, are represented as graphs defined as graphs (X1, X2) as graphs, as graphs (X1) as graphs (X1) as graphs (X1) as graphs (X1) as graphs (X1) as graphs (X1) as graphs (X2) as graphs (X1) as graphs (X2j) as nodes and edges in G1, x2j) as graphs (G2) as graphs and edges (4) as nodes (X1, X2) as graphs (X2) as graphs (X1) as graphs (X1) as graphs (G1) and graphs (1) as graphs (1) and (2) (1) as graphs (1)."}, {"heading": "5 Conclusion", "text": "Previous studies on multi-level learning typically treat the instances in the bags as i.e. samples, neglecting the fact that the instances are rarely independent and that the relationships between instances convey important structural information. In this paper, we propose the MIGraph method, which treats the instances in a non-i.i.d. way that exploits the relationships between instances. Experiments show that MIGraph is simple but effective, which is one of the most powerful methods used in some multi-level classification and regression tasks. An interesting future task is to design a better graphker to capture more useful structural information from multi-level bags. Another future task is to design a better graph construction process that fits well with multi-level learning. Applying graph editing or metric learning methods to the diagrams corresponding to multi-level bags is also worth trying, as it is possible that multi-level learning is multi-level."}], "references": [{"title": "Multiple-instance learning of real-valued data", "author": ["R.A. Amar", "D.R. Dooly", "S.A. Goldman", "Q. Zhang"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "On learning from multi-instance examples: Empirical evaluation of a theoretical approach", "author": ["P. Auer"], "venue": "In Proceedings of the 14th International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "A boosting approach to multiple instance learning", "author": ["P. Auer", "R. Ortner"], "venue": "In Proceedings of the 15th European Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "UCI repository of machine learning databases", "author": ["C. Blake", "E. Keogh", "C.J. Merz"], "venue": "[http://www.ics.uci.edu/\u223cmlearn/MLRepository.html], Department of Information and Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Multi-instance tree learning", "author": ["H. Blockeel", "D. Page", "A. Srinivasan"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H.-P. Kriegel"], "venue": "In Proceedings of the 5th IEEE International Conference on Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Multiple instance learning for sparse positive bags", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In Proceeding of the 24th International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "MILES: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1931}, {"title": "Image categorization by learning and reasoning with regions", "author": ["Y. Chen", "J.Z. Wang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A regularization framework for multiple-instance learning", "author": ["P.-M. Cheung", "J.T. Kwok"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A framework for learning rules from multiple instance data", "author": ["Y. Chevaleyre", "J.-D. Zucker"], "venue": "In Proceedings of the 12th European Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Bray", "C. Dance", "L. Fan"], "venue": "In Proceedings of the ECCV\u201904 Workshop on Statistical Learning in Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Solving the multipleinstance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Multiple instance learning for computer aided diagnosis", "author": ["G. Fung", "M. Dundar", "B. Krishnappuram", "R.B. Rao"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "A survey of kernels for structured data", "author": ["T. G\u00e4rtner"], "venue": "SIGKDD Explorations,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Multi-instance kernels", "author": ["T. G\u00e4rtner", "P.A. Flach", "A. Kowalczyk", "A.J. Smola"], "venue": "In Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Composite kernels for hypertext categorisation", "author": ["T. Joachims", "N. Cristianini", "J. Shawe-Taylor"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Marginalized kernels between labeled graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "In Proceedings of the 20th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Marginalized multi-instance kernels", "author": ["J.T. Kwok", "P.-M. Cheung"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Laerning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-P\u00e9rez"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "A quadratic programming approach to the graph edit distance problem", "author": ["M. Neuhaus", "H. Bunke"], "venue": "In Proceedings of the 6th International Workshop on Graph-based Representations in Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "MISSL: Multiple-instance semi-supervised learning", "author": ["R. Rahmani", "S.A. Goldman"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Supervised versus multiple instance learning: An empirical comparison", "author": ["S. Ray", "M. Craven"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Multiple instance regression", "author": ["S. Ray", "D. Page"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Learning single and multiple instance decision trees for computer security applications", "author": ["G. Ruffo"], "venue": "PhD thesis, Department of Computer Science,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "On generalized multiple-instance learning", "author": ["S.D. Scott", "J. Zhang", "J. Brown"], "venue": "Technical Report UNL-CSE-2003-5, Department of Computer Science,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Multiple-instance active learning", "author": ["B. Settles", "M. Craven", "S. Ray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Toward memory-based reasoning", "author": ["C. Stanfill", "D. Waltz"], "venue": "Communications of the ACM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1986}, {"title": "SVM-based generalized multiple-instance learning via approximate box counting", "author": ["Q. Tao", "S. Scott", "N.V. Vinodchandran", "T.T. Osugi"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Multiple instance boosting for object detection", "author": ["P. Viola", "J. Platt", "C. Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "Solving the multi-instance problem: A lazy learning approach", "author": ["J. Wang", "J.-D. Zucker"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "A two-level learning method for generalized multi-instance problem", "author": ["N. Weidmann", "E. Frank", "B. Pfahringer"], "venue": "In Proceedings of the 14th European Conference on Machine Learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Logistic regression and boosting for labeled bags of instances", "author": ["X. Xu", "E. Frank"], "venue": "In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Image database retrieval with multiple-instance learning techniques", "author": ["C. Yang", "T. Lozano-P\u00e9rez"], "venue": "In Proceedings of the 16th International Conference on Data Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Multiple-instance pruning for learning efficient cascade detectors", "author": ["C. Zhang", "P. Viola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Adapting RBF neural networks to multi-instance learning", "author": ["M. Zhang", "Z. Zhou"], "venue": "Neural Processing Letters,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Content-based image retrieval using multiple-instance learning", "author": ["Q. Zhang", "W. Yu", "S.A. Goldman", "J.E. Fritts"], "venue": "In Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "On the relation between multi-instance learning and semi-supervised learning", "author": ["Z.-H. Zhou", "J.-M. Xu"], "venue": "In Proceeding of the 24th International Conference on Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Ensembles of multi-instance learners", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "In Proceedings of the 14th European Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "Neural networks for multi-instance learning", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "In Proceedings of the International Conference on Intelligent Information Technology,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Multi-instance multi-label learning with application to scene classification", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Solving multi-instance problems with classifier ensemble based on constructive clustering", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Knowledge and Information Systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "In multi-instance learning [14], the training set consists of many bags of instances.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 94, "endOffset": 101}, {"referenceID": 9, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 94, "endOffset": 101}, {"referenceID": 36, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 119, "endOffset": 127}, {"referenceID": 39, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 119, "endOffset": 127}, {"referenceID": 1, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 149, "endOffset": 156}, {"referenceID": 28, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 149, "endOffset": 156}, {"referenceID": 26, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 176, "endOffset": 180}, {"referenceID": 32, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 197, "endOffset": 205}, {"referenceID": 37, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 197, "endOffset": 205}, {"referenceID": 14, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 240, "endOffset": 244}, {"referenceID": 40, "context": "However, previous studies on multi-instance learning typically treat the instances in the bags as independently and identically distributed [41], which neglects the fact that the relations among the instances convey important structure information.", "startOffset": 140, "endOffset": 144}, {"referenceID": 40, "context": "Zhou and Xu [41] showed that if the instances were treated as i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "samples, multi-instance learning is just a special case of semi-supervised learning [46] and thus the advantages of multi-instance representation could not be exploited well.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 31, "endOffset": 35}, {"referenceID": 33, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 147, "endOffset": 150}, {"referenceID": 42, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 185, "endOffset": 189}, {"referenceID": 38, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 242, "endOffset": 246}, {"referenceID": 41, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 280, "endOffset": 284}, {"referenceID": 35, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 297, "endOffset": 301}, {"referenceID": 3, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 318, "endOffset": 321}, {"referenceID": 24, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 359, "endOffset": 363}, {"referenceID": 41, "context": "Most algorithms work by adapting single-instance supervised learning algorithms to the multi-instance representation through shifting the focuses of the algorithms from the discrimination on the instances to the discrimination on the bags [42].", "startOffset": 239, "endOffset": 243}, {"referenceID": 44, "context": "Recently there are also some proposal on adapting the multi-instance representation to single-instance algorithms by representation transformation [45].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "[17] defined the MI-Kernel by regarding each multi-", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proposed mi-SVM and MI-SVM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Cheung and Kwok [11] argued that the sign instead of the value of the margin of the most positive instance is important.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Later, they [20] designed marginalized multi-instance kernels by considering that the contribution of different instances can be different.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "Chen and Wang [10] proposed the DD-SVM method which employs Diverse Density [22] to learn a set of instance prototypes and then maps the bags to a feature space based on the instance prototypes.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Chen and Wang [10] proposed the DD-SVM method which employs Diverse Density [22] to learn a set of instance prototypes and then maps the bags to a feature space based on the instance prototypes.", "startOffset": 76, "endOffset": 80}, {"referenceID": 40, "context": "Zhou and Xu [41] proposed the MissSVM method by regarding instances of negative bags as labeled examples while those of positive bags as unlabeled examples with positive constraints.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 25, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 34, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 168, "endOffset": 176}, {"referenceID": 27, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 168, "endOffset": 176}, {"referenceID": 34, "context": "The main difference between standard multi-instance learning and generalized multi-instance learning is that in standard multi-instance learning there is a single concept, and a bag is positive if it has an instance satisfies this concept; while in generalized multi-instance learning [35, 28] there are multiple concepts, and a bag is positive only when all concepts are satisfied (i.", "startOffset": 285, "endOffset": 293}, {"referenceID": 27, "context": "The main difference between standard multi-instance learning and generalized multi-instance learning is that in standard multi-instance learning there is a single concept, and a bag is positive if it has an instance satisfies this concept; while in generalized multi-instance learning [35, 28] there are multiple concepts, and a bag is positive only when all concepts are satisfied (i.", "startOffset": 285, "endOffset": 293}, {"referenceID": 23, "context": "Recently, research on multiinstance semi-supervised learning [24], multi-instance active learning [29] and multi-instance multi-label learning [44] have also been reported.", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "Recently, research on multiinstance semi-supervised learning [24], multi-instance active learning [29] and multi-instance multi-label learning [44] have also been reported.", "startOffset": 98, "endOffset": 102}, {"referenceID": 43, "context": "Recently, research on multiinstance semi-supervised learning [24], multi-instance active learning [29] and multi-instance multi-label learning [44] have also been reported.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "In this paper we mainly work on standard multi-instance learning [14] and will show that our method is also applicable to multi-instance regression.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Inspired by [32] which shows that \u01eb-graph is helpful for discovering the underlying manifold structure of data, here we establish an \u01eb-graph for every bag.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "According to the manifold property [32], i.", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": "If categorical attributes are involved, we use VDM (Value Difference Metric) [30] as a complement.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "For example, we can build a k-nearest neighbor classifier that employs graph edit distance [23], or we can design a graph kernel [16] to capture the similarity among graphs and then solve classification problems by kernel machines such as SVM.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "For example, we can build a k-nearest neighbor classifier that employs graph edit distance [23], or we can design a graph kernel [16] to capture the similarity among graphs and then solve classification problems by kernel machines such as SVM.", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "1 is exactly the MI-Kernel using Gaussian RBF kernel [17].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "Clearly, the kgraph satisfies the four major properties that should be considered for a graph kernel definition [7]: the kernel should be a good measure of similarity for graphs; the computational complexity should be in polynomial time; the kernel must be positive definite; it should be somewhat general which will not limited in small subsets of graphs.", "startOffset": 112, "endOffset": 115}, {"referenceID": 13, "context": "Drug activity prediction is one of the earliest applications of multi-instance learning [14], where the Musk data is a real-world benchmark that has been studied by many researchers.", "startOffset": 88, "endOffset": 92}, {"referenceID": 4, "context": "Musk1 and Musk2, both publicly available at the UCI machine learning repository [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 16, "context": "We compare MIGraph with MI-Kernel [17] via 10 times 10-fold cross validation (i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "In this experiment we use the COREL data set described in [10, 9].", "startOffset": 58, "endOffset": 65}, {"referenceID": 8, "context": "In this experiment we use the COREL data set described in [10, 9].", "startOffset": 58, "endOffset": 65}, {"referenceID": 16, "context": "1 Note that the performance of MI-Kernel in our implementation is better than that reported in [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 30, "context": "k\u2227 [31] 82.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "3 kemp non-transduction [31] 88.", "startOffset": 24, "endOffset": 28}, {"referenceID": 1, "context": "2 mi-SVM [2] 87.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "6 MI-SVM [2] 77.", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": "3 DD-SVM [10] 85.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "3 MissSVM [41] 87.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "0 MILES [9] 86.", "startOffset": 8, "endOffset": 11}, {"referenceID": 21, "context": "7 Diverse Density [22] 88.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "5 RELIC [27] 83.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "3 MITI [6] 83.", "startOffset": 7, "endOffset": 10}, {"referenceID": 33, "context": "2 Citation-kNN [34] 92.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "3 RIPPER-MI [12] 88.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "sbMIL [8] 91.", "startOffset": 6, "endOffset": 9}, {"referenceID": 35, "context": "7 MIBoosting [36] 87.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "0 MILBoosting [4] 92.", "startOffset": 14, "endOffset": 17}, {"referenceID": 24, "context": "MI-LR [25] 86.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "0 MULTINST [3] 76.", "startOffset": 11, "endOffset": 14}, {"referenceID": 13, "context": "0 Iterated-discrim APR [14] 92.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "We used the processed data 2 such that all the bags and instances are as same as those used in [10, 9].", "startOffset": 95, "endOffset": 102}, {"referenceID": 8, "context": "We used the processed data 2 such that all the bags and instances are as same as those used in [10, 9].", "startOffset": 95, "endOffset": 102}, {"referenceID": 1, "context": "MI-SVM [2, 10] 74.", "startOffset": 7, "endOffset": 14}, {"referenceID": 9, "context": "MI-SVM [2, 10] 74.", "startOffset": 7, "endOffset": 14}, {"referenceID": 9, "context": "5 DD-SVM [10] 81.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "4 MissSVM [41] 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "2 kmeans-SVM [13] 69.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "7 MILES [9] 82.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "We use the same experimental routine as that described in [9].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "ies [10, 9, 41], which owes to the fact that many images of these two categories contain semantically related and visually similar regions such as those corresponding to mountain, river, lake and ocean.", "startOffset": 4, "endOffset": 15}, {"referenceID": 8, "context": "ies [10, 9, 41], which owes to the fact that many images of these two categories contain semantically related and visually similar regions such as those corresponding to mountain, river, lake and ocean.", "startOffset": 4, "endOffset": 15}, {"referenceID": 40, "context": "ies [10, 9, 41], which owes to the fact that many images of these two categories contain semantically related and visually similar regions such as those corresponding to mountain, river, lake and ocean.", "startOffset": 4, "endOffset": 15}, {"referenceID": 0, "context": "[1] are used in this experiment.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Details of the data sets can be found in [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 21, "context": "DD [22, 1] 0.", "startOffset": 3, "endOffset": 10}, {"referenceID": 0, "context": "DD [22, 1] 0.", "startOffset": 3, "endOffset": 10}, {"referenceID": 0, "context": "Citation-kNN [1] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 42, "context": "0025 BP-MIP [43, 39] 0.", "startOffset": 12, "endOffset": 20}, {"referenceID": 38, "context": "0025 BP-MIP [43, 39] 0.", "startOffset": 12, "endOffset": 20}, {"referenceID": 38, "context": "0752 RBF-MIP [39] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "It has been shown in [18] that if two kernels contribute different information and the performance of the two kernels are equally good, the upper bound of generalization error of a combined kernel will be lower.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "It has also been shown in [21] that in practice, even when the kernels are not equally well, a combined kernel can be a better choice.", "startOffset": 26, "endOffset": 30}, {"referenceID": 6, "context": ", all-paths kernel [7], for our purpose.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "It has been proved [7] that determining all paths is NP-hard.", "startOffset": 19, "endOffset": 22}, {"referenceID": 18, "context": "where the definitions of kp and kc are as same as the marginalized kernel [19], i.", "startOffset": 74, "endOffset": 78}], "year": 2017, "abstractText": "Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multiinstance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}