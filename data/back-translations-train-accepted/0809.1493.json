{"id": "0809.1493", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2008", "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning", "abstract": "For supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the l1-norm or the block l1-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.", "histories": [["v1", "Tue, 9 Sep 2008 06:48:10 GMT  (53kb)", "http://arxiv.org/abs/0809.1493v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["francis r bach"], "accepted": true, "id": "0809.1493"}, "pdf": {"name": "0809.1493.pdf", "metadata": {"source": "CRF", "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning", "authors": ["Francis Bach"], "emails": ["francis.bach@mines.org"], "sections": [{"heading": null, "text": "ar Xiv: 080 9.14 93v1 [cs.L"}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world in order to change it."}, {"heading": "2 Hierarchical multiple kernel learning (HKL)", "text": "We consider the problem of predicting a random variable Y-Y-R from a random variable X-X, where X and Y can be quite general spaces. We assume that we obtain n i.i.d. observations (xi, yi) and X-Y-Y-Y-X-X-X-X-R. We define the empirical risk of a function f from X to R as 1 n \u00b2 n i = 1 (yi, f (xi)), where (y, y) 7 \u2192 R + is a loss function. We only assume that a vacuum arises with respect to the second parameter (but not necessarily distinguishable). Typical examples of loss functions are the square loss for regression, i.e. (y, y) = 12 (y \u2212 y) 2 for the y-y-y-R function and the logistic loss (y, y-y) = log (1 + e \u2212 yy) or the hinge loss (max, y} for the {1 or {1}, respectively, which results in a fictitious binary)."}, {"heading": "2.1 Graph-structured positive definite kernels", "text": "We assume that we have a positive definitive kernel k: X \u00b7 X \u00b7 R, and that this kernel can be expressed as a sum, set via an index V, which we know refers to all x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x)."}, {"heading": "2.2 Graph-based structured regularization", "text": "In our environment we are looking for a solution to the problem we have chosen in the past. < # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3 Optimization problem", "text": "In this section we give optimal conditions for the problems in Equation (1) as well as optimization algorithms with polynomial time complexity in the number of selected nuclei. In simulations we consider the total number of nuclei greater than 1030, and therefore such efficient algorithms are indispensable for the success of hierarchical learning of multiple nuclei (HKL)."}, {"heading": "3.1 Reformulation in terms of multiple kernel learning", "text": "Following [9, 10] we can simply derive an equivalent formulation of Eq. (1) In addition (1) we have that a uniform solution (1) can be found for all other countries of the world. (1) In addition (1) we have that for all countries of the world (1) a uniform solution (1) can be found. (2) Note (2) Note (1) Note (2) Note (2) Note (1) Note (2). (1) Note (2). (2) Note (2). (2) Note (2) Note (1). (2) Note (2) Note (1). (2) Note (2). (2) Note (2). (2). (2) Note (2). (2). (2). (2). (2). (2). (2). (2). (2. (2). (2). (2. (2). (2.). (2. (2). (2.). (2. (2.). (2. (2.). (2.). (2. (2.). (2.). (2. (2.). (2.). (2. (2.). (2.). (2. (2.). (2.). (2.). (2.). (2. (2.). (2.).). (2. (2.). (2.). (2.).). (2.). (2. (2.).). (2.). (2.). (2.). (.). (2.).). (2.). (2.). (2. (.). (2.).). (2.). (2.). (2.). (2.). (2.).). (2.). (2.).). (2.).). (2.). (.).). (2.).).). (2. (.). (2.).). (2. (.).).)."}, {"heading": "3.2 Conditions for global optimality of reduced problem", "text": "We therefore consider the optimal solution \u03b2 of the reduced problem (to J), namely min\u03b2J-Q v-JFv 1 n-JFv 1 n-J = 1 (yi, \u2211 v-J < \u03b2v, \u03a6v (xi) >) + \u03bb2 (\u2211 v-V dv-D (v-J) 2, (3) with optimal primary variables \u03b2J, double variables \u03b1 and optimal pairs (\u03b7J-J). We now consider the necessary conditions and sufficient conditions for this solution (extended by zeros for inactive variables, i.e. variables in Jc) as optimal in relation to the full problem \u03b2J. (1) We denounce the difference. v-J dv-J-D (v-J-V-V-D) as optimal in relation to the optimal value of the standard for the reduced problem. proposition 2 (NJ) If the reduced solution is optimal for the full problem in Eq-J-J, then we are essentially sufficient."}, {"heading": "3.3 Dual optimization for reduced or small problems", "text": "If Kernel kv v v v v v has low-dimensional attribute spaces, we can use an initial representation and solve the problem in Equation (1) using generic optimization toolboxes adapted to conical constraints (see e.g. [12]). However, to reuse existing optimized learning code and use high-dimensional cores, it is preferable to use a dual optimization, because we typically use the same technique as [9]: We consider the function B (3) = min\u03b2, Qv, V, V, V, Fv, n = 1 (yi, sp, v, V < \u03b2v (xi) >) + \u03bb 2, V \u2212 1 w, 2, which is the optimal value of the individual kernel learning problem with kernel matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matri"}, {"heading": "3.4 Kernel search algorithm", "text": "We are now ready to present the detailed algorithm that extends the feature search algorithm of [11]. Note that the kernel matrices are never explicitly needed, i.e. we only need them (a) explicitly to solve the small problems (but we only need a few of them) and (b) implicitly to calculate the sufficient condition (SJ, \u03b5) that requires a sum across all the kernels, as shown in Section 3.2. \u2022 Input: kernel matrices Kv, v, V, maximum gap \u03b5, maximum # of the kernels Q \u2022 Algorithm1. Initialization: Set J = sources (V), calculate the source solutions of Eq. (3), using Section 3.32. While (NJ) and (SJ, \u03b5) do not reach the gap, we are not satisfied and # (V) 6 Q- If (NJ) the source solutions are not met, we add infringing Jc in the sources."}, {"heading": "4 Consistency conditions", "text": "As said, the sparseness pattern of the solution of Eq. (1) will correspond to its hull, and therefore we can only hope to obtain the consistency of the hull of the pattern that we will consider in this paragraph. For simplicity, we will consider the case of finite Hilbert dimensional spaces (i.e., Fv = Rfv) and the square loss. We will also note the vertex set of V, i.e., we will assume that the total number of characters is fixed, and we will incline n to use n.Following [4] we will assume the following assumptions about the underlying common distribution of (X, Y): (a) the common covariance matrix of (xv))) v-V (defined with suitable blocks of size fv \u00d7 fw) is invariable, (b) E (Y | X) = 4 correct distributions of the underlying common distribution of (fv): Y (the common matrix) of size of the matriv (cov) with (cov)."}, {"heading": "5 Simulations", "text": "Synthetic examples We generated regression data as follows: n = 1024 samples of p [22, 27] variables were generated by a random covariance matrix. (We then compare the performance of our hierarchical multiple kernel learning method (HKL) with the polynomial kernel decomposition presented in Section 2 with other methods using the same kernel and / or the same decomposition: (a) the greedy strategy of selecting base kernels one by one, a process similar to [13], and (b) regular polynomial kernel regulation with the full kernel (i.e., the sum of all base kernels). In Figure 2, we compare the two approaches to 40 replications in the following two situations: original data (left) and rotated data (right), i.e. after entering these variables, we were slightly transformed."}, {"heading": "6 Conclusion", "text": "We have shown how hierarchical multi-nucleus learning (HKL) can be performed in polynomial time in the number of selected nuclei. This framework can be applied to many positive definitive nuclei, and we have focused on polynomial and Gaussian nuclei used for the nonlinear selection of variable nuclei. In particular, this paper shows that attempting to use type \"1\" penalties within the feature space can be beneficial. We are currently investigating applications to other nuclei, such as the pyramid match kernel [15], string nuclei, and graph nuclei [2]."}, {"heading": "A Optimization results", "text": "V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-V-"}, {"heading": "B Consistency conditions", "text": "We assume that we are in finite dimensioning (i.e., each Fv has finite dimensions fv = finite dimensions fv = finite dimensioning fv = finite dimensioning fv = finite dimensioning fv = finite dimensioning fv = finite dimensioning fv (w). (W = finite dimensioning fv = finite dimensioning fv). (W = finite dimensioning fv = finite dimensioning fv). (W = finite dimensioning fv (w). (W = finite dimensioning fv). (W = finite dimensioning fv = finite dimensioning fv). (W = finite dimensioning fv). (W = finite dimensioning fv). (W = finite dimensioning fv). (W = finite dimensioning fv). (W). (W). (W). (W. (W). (W). (W). (W. (W). (W). (W). (W). (W. (W). (W). (W). (W). (W). (W). (W)."}], "references": [{"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Camb. U. P.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Consistency of the group Lasso and multiple kernel learning", "author": ["F.R. Bach"], "venue": "Technical Report 00164735,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "In Proc. ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Grouped and hierarchical model selection through composite absolute penalties", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics, To appear,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Composite kernel learning", "author": ["M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy"], "venue": "In Proc. ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "The effect of the input density distribution on kernel-based classifiers", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In Proc. ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "More efficiency in multiple kernel learning", "author": ["A. Rakotomamonjy", "F.R. Bach", "S. Canu", "Y. Grandvalet"], "venue": "In Proc. ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Learning the kernel function via regularization", "author": ["M. Pontil", "C.A. Micchelli"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Mark: A boosting algorithm for heterogeneous kernel models", "author": ["K. Bennett", "M. Momma", "J. Embrechts"], "venue": "In Proc. SIGKDD,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "The generalized Lasso", "author": ["V. Roth"], "venue": "IEEE Trans. on Neural Networks,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "The pyramid match kernel: Efficient learning with sets of features", "author": ["K. Grauman", "T. Darrell"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Computing regularization paths for learning multiple kernels", "author": ["F.R. Bach", "R. Thibaux", "M.I. Jordan"], "venue": "In Adv. NIPS", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "J. Mach. Learn. Res., 7:1531\u20131565,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The adaptive Lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Asymptotics for Lasso-type estimators", "author": ["W. Fu", "K. Knight"], "venue": "Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "On the non-negative garrotte estimator", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of The Royal Statistical Society Series B,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": ", [1, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": "While early work has focused on efficient algorithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of such methods, in the linear case [3] or within the multiple kernel learning framework [4].", "startOffset": 220, "endOffset": 223}, {"referenceID": 3, "context": "While early work has focused on efficient algorithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of such methods, in the linear case [3] or within the multiple kernel learning framework [4].", "startOffset": 273, "endOffset": 276}, {"referenceID": 4, "context": "This exactly corresponds to the situation where a large feature space is the concatenation of smaller feature spaces, and we aim to do selection among these many kernels, which may be done through multiple kernel learning [5].", "startOffset": 222, "endOffset": 225}, {"referenceID": 5, "context": "Following [6, 7], we consider in Section 2 a specific combination of l2-norms that is adapted to the DAG, and will restrict the authorized sparsity patterns; in our specific kernel framework, we are able to use the DAG to design an optimization algorithm which has polynomial complexity in the number of selected kernels (Section 3).", "startOffset": 10, "endOffset": 16}, {"referenceID": 6, "context": "Following [6, 7], we consider in Section 2 a specific combination of l2-norms that is adapted to the DAG, and will restrict the authorized sparsity patterns; in our specific kernel framework, we are able to use the DAG to design an optimization algorithm which has polynomial complexity in the number of selected kernels (Section 3).", "startOffset": 10, "endOffset": 16}, {"referenceID": 2, "context": "Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple kernel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization framework by giving necessary and sufficient conditions for model consistency.", "startOffset": 112, "endOffset": 118}, {"referenceID": 3, "context": "Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple kernel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization framework by giving necessary and sufficient conditions for model consistency.", "startOffset": 112, "endOffset": 118}, {"referenceID": 1, "context": ", [2] or the Appendix).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [8]): e\u2212b(x\u2212x \u2032)2 = \u2211\u221e k=0 (b/A) 2kk! [e b A 2Hk( \u221a 2cx)][e b A \u20322Hk( \u221a 2cx\u2032)], where c2 = a2 + 2ab, A = a + b + c, and Hk is the k-th Hermite polynomial.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "The decomposition ends up being close to a polynomial kernel of infinite degree, modulated by an exponential [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": ", [2, 1]), which is equivalent to using the eigenvectors of the empirical covariance operator associated with the data (and not the population one associated with the Gaussian distribution with same variance).", "startOffset": 2, "endOffset": 8}, {"referenceID": 0, "context": ", [2, 1]), which is equivalent to using the eigenvectors of the empirical covariance operator associated with the data (and not the population one associated with the Gaussian distribution with same variance).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "Note that for q = 1, we obtain ANOVA-like decompositions [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "(1) Our Hilbertian norm is a Hilbert space instantiation of the hierarchical norms recently introduced by [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "While with uni-dimensional groups or kernels, the \u201cl1-norm of l\u221e-norms\u201d allows an efficient path algorithm for the square loss and when the DAG is a tree [6], this is not possible anymore with groups of size larger than one, or when the DAG is a not a tree.", "startOffset": 154, "endOffset": 157}, {"referenceID": 5, "context": "Finally, note that in certain settings (finite dimensional Hilbert spaces and distributions with absolutely continuous densities), these norms have the effect of selecting a given kernel only after all of its ancestors [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": "1 Reformulation in terms of multiple kernel learning Following [9, 10], we can simply derive an equivalent formulation of Eq.", "startOffset": 63, "endOffset": 70}, {"referenceID": 9, "context": "1 Reformulation in terms of multiple kernel learning Following [9, 10], we can simply derive an equivalent formulation of Eq.", "startOffset": 63, "endOffset": 70}, {"referenceID": 4, "context": "(2) Following [5], we consider the square of the norm, which does not change the regularization properties, but allow simple links with multiple kernel learning.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "Note that in the case of \u201cflat\u201d regular multiple kernel learning, where the DAG has no edges, we obtain back usual optimality conditions [9, 10].", "startOffset": 137, "endOffset": 144}, {"referenceID": 9, "context": "Note that in the case of \u201cflat\u201d regular multiple kernel learning, where the DAG has no edges, we obtain back usual optimality conditions [9, 10].", "startOffset": 137, "endOffset": 144}, {"referenceID": 10, "context": "Following a common practice for convex sparsity problems [11], we will try to solve a small problem where we assume we know the set of v such that \u2016\u03b2D(v)\u2016 is equal to zero (Section 3.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "Namely, we use the same technique as [9]: we consider for \u03b6 \u2208 Z , the function B(\u03b6) = min\u03b2\u2208Qv\u2208V Fv 1 n \u2211n i=1 l(yi, \u2211 v\u2208V \u3008\u03b2v,\u03a6v(xi)\u3009)+ \u03bb 2 \u2211 w\u2208V \u03b6 \u22121 w \u2016\u03b2w\u2016, which is the optimal value of the single kernel learning problem with kernel matrix \u2211 w\u2208V \u03b6wKw.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "We can then use the same projected gradient descent strategy as [9] to minimize it.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "4 Kernel search algorithm We are now ready to present the detailed algorithm which extends the feature search algorithm of [11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Following [4], we make the following assumptions on the underlying joint distribution of (X,Y ): (a) the joint covariance matrix \u03a3 of (\u03a6(xv))v\u2208V (defined with appropriate blocks of size fv \u00d7 fw) is invertible, (b) E(Y |X) = \u2211 w\u2208W \u3008\u03b2w,\u03a6w(x)\u3009 with W \u2282 V and var(Y |X) = \u03c32 > 0 almost surely.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Note that the last two propositions are not consequences of the similar results for flat MKL [4], because the groups that we consider are overlapping.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "We are currently investigating extensions to the non parametric case [4], in terms of pattern selection and universal consistency.", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "We then compare the performance of our hierarchical multiple kernel learning method (HKL) with the polynomial kernel decomposition presented in Section 2 to other methods that use the same kernel and/or decomposition: (a) the greedy strategy of selecting basis kernels one after the other, a procedure similar to [13], and (b) the regular polynomial kernel regularization with the full kernel (i.", "startOffset": 313, "endOffset": 317}, {"referenceID": 13, "context": "UCI datasets For regression datasets, we compare HKL with polynomial (degree 4) and Gaussian-RBF kernels (each dimension decomposed into 9 kernels) to the following approaches with the same kernel: regular Hilbertian regularization (L2), same greedy approach as earlier (greedy), regularization by the l1-norm directly on the vector \u03b1, a strategy which is sometimes used in the context of sparse kernel learning [14] but does not use the Hilbertian structure of the kernel (lasso-\u03b1), multiple kernel learning with the p kernels obtained by summing all kernels associated with a single variable, a strategy suggested by [5] (MKL).", "startOffset": 412, "endOffset": 416}, {"referenceID": 4, "context": "UCI datasets For regression datasets, we compare HKL with polynomial (degree 4) and Gaussian-RBF kernels (each dimension decomposed into 9 kernels) to the following approaches with the same kernel: regular Hilbertian regularization (L2), same greedy approach as earlier (greedy), regularization by the l1-norm directly on the vector \u03b1, a strategy which is sometimes used in the context of sparse kernel learning [14] but does not use the Hilbertian structure of the kernel (lasso-\u03b1), multiple kernel learning with the p kernels obtained by summing all kernels associated with a single variable, a strategy suggested by [5] (MKL).", "startOffset": 619, "endOffset": 622}, {"referenceID": 14, "context": "We are currently investigating applications to other kernels, such as the pyramid match kernel [15], string kernels, and graph kernels [2].", "startOffset": 95, "endOffset": 99}, {"referenceID": 1, "context": "We are currently investigating applications to other kernels, such as the pyramid match kernel [15], string kernels, and graph kernels [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 11, "context": "v leaf \u03b6vd 2 v 6 1, which is clearly convex [12].", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "2 Fenchel conjugates Following [16, 17], in order to derive optimality conditions for all losses, we need to introduce Fenchel conjugates.", "startOffset": 31, "endOffset": 39}, {"referenceID": 16, "context": "2 Fenchel conjugates Following [16, 17], in order to derive optimality conditions for all losses, we need to introduce Fenchel conjugates.", "startOffset": 31, "endOffset": 39}, {"referenceID": 11, "context": "Let \u03c8i : R 7\u2192 R, be the Fenchel conjugate [12] of the convex function \u03c6i : a 7\u2192 l(yi, a), defined as \u03c8i(b) = max a\u2208R ab\u2212 \u03c6i(a).", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "With these assumptions, we can follow the approach of [18, 19, 20] : that is, if \u03bbn tends to zero faster than n\u22121/2, then the estimate \u03b2\u0302 converges in probability to the generating \u03b2, and we have the expansion \u03b2\u0302 = \u03b2 + \u03bbn\u03b3\u0302 where \u03b3\u0302 is the solution of the following optimization problem, with \u03b4 = \u2211 v\u2208W dv\u2016\u03b2D(v)\u2016: min \u03b3\u2208 Q", "startOffset": 54, "endOffset": 66}, {"referenceID": 18, "context": "With these assumptions, we can follow the approach of [18, 19, 20] : that is, if \u03bbn tends to zero faster than n\u22121/2, then the estimate \u03b2\u0302 converges in probability to the generating \u03b2, and we have the expansion \u03b2\u0302 = \u03b2 + \u03bbn\u03b3\u0302 where \u03b3\u0302 is the solution of the following optimization problem, with \u03b4 = \u2211 v\u2208W dv\u2016\u03b2D(v)\u2016: min \u03b3\u2208 Q", "startOffset": 54, "endOffset": 66}, {"referenceID": 19, "context": "With these assumptions, we can follow the approach of [18, 19, 20] : that is, if \u03bbn tends to zero faster than n\u22121/2, then the estimate \u03b2\u0302 converges in probability to the generating \u03b2, and we have the expansion \u03b2\u0302 = \u03b2 + \u03bbn\u03b3\u0302 where \u03b3\u0302 is the solution of the following optimization problem, with \u03b4 = \u2211 v\u2208W dv\u2016\u03b2D(v)\u2016: min \u03b3\u2208 Q", "startOffset": 54, "endOffset": 66}, {"referenceID": 18, "context": "The consistency condition is then obtained by studying when the first order expansion indeed has the correct sparsity pattern (for more precise statements and arguments, see [19]).", "startOffset": 174, "endOffset": 178}], "year": 2008, "abstractText": "For supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the l-norm or the block l-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.", "creator": "LaTeX with hyperref package"}}}