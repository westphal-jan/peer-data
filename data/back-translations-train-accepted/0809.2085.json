{"id": "0809.2085", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2008", "title": "Clustered Multi-Task Learning: A Convex Formulation", "abstract": "In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem.", "histories": [["v1", "Thu, 11 Sep 2008 19:01:39 GMT  (19kb)", "http://arxiv.org/abs/0809.2085v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["laurent jacob", "francis r bach", "jean-philippe vert"], "accepted": true, "id": "0809.2085"}, "pdf": {"name": "0809.2085.pdf", "metadata": {"source": "CRF", "title": "Clustered Multi-Task Learning: a Convex Formulation", "authors": ["Laurent Jacob"], "emails": ["laurent.jacob@mines-paristech.fr", "francis.bach@mines.org", "jean-philippe.vert@mines-paristech.fr"], "sections": [{"heading": null, "text": "ar Xiv: 080 9. \u0445 Who to contact: 35, rue Saint Honore, F-77300 Fontainebleau, France."}, {"heading": "1 Introduction", "text": "This year, it is time for us to set out in search of new ways to travel the world, to travel the world, to explore the world."}, {"heading": "2 Multi-task learning with clustered tasks", "text": "\"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s."}, {"heading": "3 Convex relaxation", "text": "To formulate a convex relaxation of (11), let us first state that the cluster structure (5) contributes only to the second and third terms of (W) and (W), and that these terms depend only on the centered version of W. In relation to matrices, only the last two terms of (M) \u2212 1 in (7) depend on M, i.e. on clustering, and these terms can be rewritten as: \u03b5B (M \u2212 U) + \u03b5W (I \u2212 M) = 1 in relation to (I \u2212 M). (12) In fact, it is easy to verify that M \u2212 U = M \u2212 M \u2212 U \u2212 (M \u2212 U) = 1 in relation to W \u2212 W."}, {"heading": "3.1 Reinterpretation in terms of norms", "text": "For each convex set Sc, we get a standard for W (which we apply here to its centered version). By circumventing some different constraints on the specified Sc, we get different standards for W, and in fact, all previous multi-task formulations can be cast in this way, i.e. by selecting a certain set of positive matrices Sc (e.g. the tracking constraint for the tracking standard and simply a singleton for the Frobenius standard). Therefore, designing standards for learning with multiple tasks is equivalent to designing a set of positive matrices. In this essay, we have examined a specific set that is adapted to cluster tasks, but other sets could also be designed in other situations. Note that we have chosen a simple spectral convex set Sc to simplify optimization in Section 3.3, but we could add some additional sets if we form the formulas of the matrix (eventually a dot and a block) and a matrix."}, {"heading": "3.2 Reinterpretation as a convex relaxation of K-means", "text": "In this section, we show that the semi-standard we have previously outlined can be interpreted as a convex relaxation of the K-mean in the tasks [9]. In fact, the K-mean, when specified in W-Rd \u00b7 m, aims to decompose it in the form W = \u00b5E, in which \u00b5 Rd \u00b7 r are cluster centers and E is a partition. If specified in the partition E, the matrix \u00b5 is found by minimizing the Min\u00b5-W-E\u00b5-2F. Therefore, a natural strategy outlined in [9] is to switch between optimization \u00b5, partition E and weight vectors W."}, {"heading": "3.3 Primal optimization", "text": "Let us now show in detail how (16) can be efficiently solved. (While a double formula could easily be derived according to [8], a direct approach consists in (16) asmin W-Rd-m (3) + minstrestrestreamststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststststst"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Artificial data", "text": "We generated synthetic data consisting of two clusters of two tasks. The tasks are vectors of Rd, d = 30. For each cluster, a center w \u00b2 c was generated in Rd \u2212 2, so that the two clusters exhibit an orthogonal concept. More specifically, each w \u00b2 c had (d \u2212 2) / 2 random features randomly drawn from N (0, \u03c32r), and the other features were drawn from N (0, \u03c32c). The last two characteristics were not zero for all tasks and drew from N (0, \u03c32c). For each task, 2000 points were generated and a normal noise of variance of 2n = 150 was added.In a first experiment, we compared our clusters with others."}, {"heading": "4.2 MHC-I binding data", "text": "This database contains binding affinities of different peptides, i.e. short amino acid sequences, with different MHC-I molecules. However, this binding process is central to the immune system and its prediction is crucial, for example for the development of vaccines. Affinities are attenuated to provide a predictive problem. Each MHC-I molecule is considered a task, and the goal is to predict whether a peptide binds a molecule. We used orthogonal encoding of the amino acids to represent the peptides and balanced the data by maintaining only one negative example for each positive point, resulting in 15,236 points with 35 different molecules. We chose a logistical loss for the molecules (W).Multi-task learning approaches have already proven useful for this problem, see for example [11, 12]."}, {"heading": "5 Conclusion", "text": "We presented a convex approach to clustered multi-task learning based on the design of a specific standard. Promising results were presented using synthetic examples and iedb datasets. We are currently investigating more sophisticated convex relaxations and the natural extension of nonlinear multi-task learning, as well as the inclusion of specific features in the tasks that have been shown to improve performance in other environments [6]."}], "references": [{"title": "Spline Models for Observational Data, volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics", "author": ["G. Wahba"], "venue": "SIAM, Philadelphia,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Regularization Theory and Neural Networks Architectures", "author": ["F. Girosi", "M. Jones", "T. Poggio"], "venue": "Neural Comput.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc. B.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Task clustering and gating for bayesian multitask learning", "author": ["B. Bakker", "T. Heskes"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Low-rank matrix factorization with attributes", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "J.-P. Vert"], "venue": "Technical Report cs/0611124,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Adv. Neural. Inform. Process Syst", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Learning the Kernel Matrix with Semidefinite Programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "A framework for simultaneous coclustering and learning from complex data", "author": ["Meghana Deodhar", "Joydeep Ghosh"], "venue": "Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A community resource benchmarking predictions of peptide binding to MHC-I molecules", "author": ["Bjoern Peters", "Huynh-Hoa Bui", "Sune Frankild", "Morten Nielson", "Claus Lundegaard", "Emrah Kostem", "Derek Basch", "Kasper Lamberth", "Mikkel Harndahl", "Ward Fleri", "Stephen S Wilson", "John Sidney", "Ole Lund", "Soren Buus", "Alessandro Sette"], "venue": "PLoS Comput Biol,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Leveraging information across HLA alleles/supertypes improves HLA-specific epitope", "author": ["David Heckerman", "Carl Kadie", "Jennifer Listgarten"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Efficient peptide-MHC-I binding prediction for alleles with few known binders", "author": ["L. Jacob", "J.-P. Vert"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": ", [1, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": "For example, the l-norm (the sum of absolute values) imposes some of the components to be equal to zero and is widely used to estimate sparse functions [3], while various combinations of l norms can be defined to impose various sparsity patterns.", "startOffset": 152, "endOffset": 155}, {"referenceID": 3, "context": "That is, assuming a given prior knowledge, how can we design a norm that will enforce it? More precisely, we consider the problem of multi-task learning, which has recently emerged as a very promising research direction for various applications [4].", "startOffset": 245, "endOffset": 248}, {"referenceID": 4, "context": "For example, such constraints are typically that the weight vectors of the different tasks belong (a) to a Euclidean ball centered at the origin [5], which implies no sharing of information between tasks apart from the size of the different vectors, i.", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": ", the amount of regularization, (b) to a ball of unknown center [5], which enforces a similarity between the different weight vectors, or (c) to an unknown low-dimensional subspace [6, 7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": ", the amount of regularization, (b) to a ball of unknown center [5], which enforces a similarity between the different weight vectors, or (c) to an unknown low-dimensional subspace [6, 7].", "startOffset": 181, "endOffset": 187}, {"referenceID": 6, "context": ", the amount of regularization, (b) to a ball of unknown center [5], which enforces a similarity between the different weight vectors, or (c) to an unknown low-dimensional subspace [6, 7].", "startOffset": 181, "endOffset": 187}, {"referenceID": 4, "context": "A key difference with [5], where a similar hypothesis is studied, is that we don\u2019t assume that the groups are known a priori, and in a sense our goal is both to identify the clusters and to use them for multi-task learning.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "We construct such a penalty by first assuming that the partition of the tasks into clusters is known, similarly to [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "We then attempt to optimize the objective function of the inference algorithm over the set of partitions, a strategy that has proved useful in other contexts such as multiple kernel learning [8].", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "For example, [5] suggests to penalize both the norms of the wi\u2019s and their variance, i.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "Alternatively, [7] propose to penalize the trace norm of W :", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "In that case we can follow an approach proposed by [5] which for clarity we rephrase with our notations and slightly generalize now.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "\u2022 For \u03b5W = \u03b5B > \u03b5M , we recover the penalty of [5] without clusters: \u03a9(W ) = trW (\u03b5MU + \u03b5B(I \u2212 U))W\u22a4 = \u03b5Mn\u2016w\u0304\u2016 + \u03b5B m \u2211", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "\u2022 For \u03b5W > \u03b5B = \u03b5M we recover the penalty of [5] with clusters: \u03a9(W ) = trW (\u03b5MM + \u03b5W (I \u2212M))W\u22a4", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Finally, when r = 1 (one clusters) and r = m (one cluster per task), we get back the formulation of [5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "2 Reinterpretation as a convex relaxation of K-means In this section we show that the semi-norm \u2016\u03a0W\u2016c that we have designed earlier, can be interpreted as a convex relaxation of K-means on the tasks [9].", "startOffset": 199, "endOffset": 202}, {"referenceID": 8, "context": "Thus, a natural strategy outlined by [9], is to alternate between optimizing \u03bc, the partition E and the weight vectors W .", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Whereas a dual formulation could be easily derived following [8], a direct approach is to rewrite (16) as min W\u2208Rd\u00d7m ( lc(W ) + min \u03a3c\u2208Sc tr\u03a0W\u03a3\u22121 c W \u03a0 ) (17)", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "\u2022 The k-means approach, that alternates between optimizing the tasks in W given the metric \u03a3 and re-learning \u03a3 by clustering the tasks wi [9].", "startOffset": 138, "endOffset": 141}, {"referenceID": 9, "context": "2 MHC-I binding data We also applied our method to the iedb MHC-I peptide binding benchmark proposed in [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "Multi-task learning approaches have already proved useful for this problem, see for example [11, 12].", "startOffset": 92, "endOffset": 100}, {"referenceID": 11, "context": "Multi-task learning approaches have already proved useful for this problem, see for example [11, 12].", "startOffset": 92, "endOffset": 100}, {"referenceID": 11, "context": "[12] showed in particular that the multi-task approaches were very useful for molecules with few known binders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We are currently investigating more refined convex relaxations and the natural extension to non-linear multi-task learning as well as the inclusion of specific features on the tasks, which has shown to improve performance in other settings [6].", "startOffset": 240, "endOffset": 243}], "year": 2008, "abstractText": "In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the iedb MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem. \u2217To whom correspondance should be addressed: 35, rue Saint Honor\u00e9, F-77300 Fontainebleau, France.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}