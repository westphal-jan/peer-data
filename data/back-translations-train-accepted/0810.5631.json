{"id": "0810.5631", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2008", "title": "Temporal Difference Updating without a Learning Rate", "abstract": "We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(lambda), however it lacks the parameter alpha that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(lambda) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins' Q(lambda) and Sarsa(lambda) and find that it again offers superior performance without a learning rate parameter.", "histories": [["v1", "Fri, 31 Oct 2008 07:15:01 GMT  (376kb)", "http://arxiv.org/abs/0810.5631v1", "12 pages, 6 figures"]], "COMMENTS": "12 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["marcus hutter", "shane legg"], "accepted": true, "id": "0810.5631"}, "pdf": {"name": "0810.5631.pdf", "metadata": {"source": "CRF", "title": "Temporal Difference Updating without a Learning Rate", "authors": ["Marcus Hutter", "Shane Legg"], "emails": ["RSISE@ANU", "SML@NICTA,", "marcus@hutter1.net", "shane@vetta.org"], "sections": [{"heading": null, "text": "ar Xiv: 081 0.56 31KeywordsStrengthening of learning; time difference; entitlement tracing; principle of variation; learning rate."}, {"heading": "1 Introduction", "text": "It is unclear exactly who introduced this first, but the first explicit version of the temporal difference as a learning rule therefore seems to be regarded as Witten [Wit77]. The idea is as follows: The expected future discounted reward of a state is V s: = E {rk + \u03b3rk + 1 + 2 + \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 S}, where the rewards rk + 1,. are geometrically discounted in the future by entering into the states < 1. From this definition it follows that V {rk + 2 + sk = s}. Our task is to calculate an estimate of V s for each state s in due course. The only information we have is to support the current history of state transitions, s1, 2."}, {"heading": "2 Derivation", "text": "The empirical future reward of a state sk is the sum of the actual future rewards that follow from the state sk, as we do in time K = 1,.., where the rewards are subtracted as they go into the future. (3) Where the future rewards are always unknown in practice, since they depend not only on rewards that have already been observed, but also on unknown future rewards. Note: If sm = sn for m 6 = n, that is, we have visited the same state twice at different times m and n, this does not imply that vn = vm are different than the observed rewards after the state visit. Our goal is that for each state s the estimate V should be as close as possible."}, {"heading": "3 A simple Markov process", "text": "For our first test, we consider a simple Markov process with 51 states. In each step, the HL rate is either increased or decreased by one with equal probability, unless the system is in state 0 or 50, in which it always passes to a transition to state 25 in the following step. However, if the state transitions take place from 0 to 25, a reward of 1.0 is generated, and for a transition from 50 to 25 a reward of -1.0 is generated. All other transitions have a reward of 0. We set the discount value \u03b3 = 0.99 and then calculate the actual discount value of each state by running a gross force Monte Carlo simulation. We ran our algorithm 10 times on the above-mentioned Markov chain and calculated the square error in the appreciation between states at each step on average. The optimal value for these tests was 1.0, which was to be expected given that the environment is stationary and thus the discrediting of the old learning rates is not helpful for us to try different T.D values."}, {"heading": "4 Random Markov process", "text": "To test a Markov process with a more complex transition structure, we created a random 50-state Markov process. We did this by creating a 50-by-50 transition matrix in which each element was set to 0 with a probability of 0.9, and otherwise a uniform random number in the interval [0, 1]. We then scaled each line to the sum of 1. To transition between states, we interpreted the ith line as a probability distribution over which the state i is distributed. To calculate the reward associated with each transition, we created a random matrix as described above, but without normalization. We used \u03b3 = 0.9, and then ran a Monte Carlo simulation to calculate the true discounted value of each state."}, {"heading": "5 Non-stationary Markov process", "text": "The \u03bb parameter in HL (\u03bb), introduced in Equation (4), reduces the importance of old observations in the calculation of state value estimates. If the environment is stationary, this is not useful and so we can set \u03bb = 1.0, but in a non-stationary environment we have to reduce this value in order for the state values to adapt properly to changes in the environment. The faster the environment changes, the lower we have to make \u03bb = 1.0 in order to forget old observations faster. To test HL (\u03bb) in such an environment, we used the Markov chain from section 3, but reduced its size to 21 states to accelerate convergence. We used this Markov chain for the first 5,000 time steps. At this point, we changed the reward at the transition from the last state to the mean state to -1.0 to 0.5. At a time when 10,000 we looked back to the original Markov chain, and so alternating between models of the environment every 5,000 steps."}, {"heading": "6 Windy Gridworld", "text": "This suggests that new amplification learning algorithms should be possible based on HL (\u03bb). For our first experiment, we took the standard Sarsa (\u03bb) algorithm and modified it in an obvious way to use a time difference from HL. In the presentation of this algorithm, we slightly changed the notation to make things more consistent with typical amplification learning. Specifically, we dropped the t superscript, since this is implicit in the algorithm specification, and defined Q (s, a): = V (s, a), E (s) and N (s, a): Our new amplification learning algorithm, which we call HLS."}, {"heading": "7 Conclusions", "text": "We have derived a new equation for determining the learning rate in time difference learning with authorization tracks. It replaces the parameter of free learning rate \u03b1, which is normally set experimentally by hand. In each tested setting, be it stationary Markov chains, non-stationary Markov chains or reinforcement learning, our new method led to superior results. In order to expand our theoretical understanding, the next step would be to try to prove that the method converges with correct estimates. This can be done for TD (\u03bb) under certain assumptions about how the learning rate decreases over time. Hopefully, something similar can be demonstrated for our new method. In terms of experimental results, it would be interesting to try different types of reinforcement learning problems and more clearly identify where the ability to set the learning rate differently for different governmental transition pairs promotes performance. It would also be good to generalize the result to episodic tasks."}], "references": [{"title": "Adaptive stepsizes for recursive estimation with applications in approximate dynamic programming", "author": ["A.P. George", "W.B. Powell"], "venue": "Journal of Machine Learning,", "citeRegEx": "George and Powell.,? \\Q2006\\E", "shortCiteRegEx": "George and Powell.", "year": 2006}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr.", "year": 2003}, {"title": "Increamental multi-step Q-learning", "author": ["J. Peng", "R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Peng and Williams.,? \\Q1996\\E", "shortCiteRegEx": "Peng and Williams.", "year": 1996}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technial Report CUED/F-INFENG/TR 166,", "citeRegEx": "Rummery and Niranjan.,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan.", "year": 1994}, {"title": "Rummery. Problem solving with reinforcement learning", "author": ["A. G"], "venue": "PhD thesis, Cambridge University,", "citeRegEx": "G.,? \\Q1995\\E", "shortCiteRegEx": "G.", "year": 1995}, {"title": "Reinforcement learning: An introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Learning from Delayed Rewards", "author": ["C.J.C.H Watkins"], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}, {"title": "An adaptive optimal controller for discrete-time markov environments", "author": ["I.H. Witten"], "venue": "Information and Control,", "citeRegEx": "Witten.,? \\Q1977\\E", "shortCiteRegEx": "Witten.", "year": 1977}], "referenceMentions": [], "year": 2013, "abstractText": "We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(\u03bb), however it lacks the parameter \u03b1 that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(\u03bb) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins\u2019 Q(\u03bb) and Sarsa(\u03bb) and find that it again offers superior performance without a learning rate parameter.", "creator": "LaTeX with hyperref package"}}}