{"id": "0811.1790", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2008", "title": "Robust Regression and Lasso", "abstract": "Lasso, or $\\ell^1$ regularized least squares, has been explored extensively for its remarkable sparsity properties. It is shown in this paper that the solution to Lasso, in addition to its sparsity, has robustness properties: it is the solution to a robust optimization problem. This has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows a principled selection of the regularizer, and in particular, generalizations of Lasso that also yield convex optimization problems are obtained by considering different uncertainty sets.", "histories": [["v1", "Tue, 11 Nov 2008 22:46:10 GMT  (91kb)", "http://arxiv.org/abs/0811.1790v1", null]], "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT", "authors": ["huan xu", "constantine caramanis", "shie mannor"], "accepted": true, "id": "0811.1790"}, "pdf": {"name": "0811.1790.pdf", "metadata": {"source": "CRF", "title": "Robust Regression and Lasso", "authors": ["Huan Xu", "Constantine Caramanis"], "emails": ["(xuhuan@cim.mcgill.ca;", "shie.mannor@mcgill.ca)"], "sections": [{"heading": null, "text": "In this paper we consider the problems to be the smallest ever. - The analysis and the specific results we have received differ in the way they affect individual countries. - It is not just the way they engage with individual countries, but also the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries. - It is the way they engage with individual countries."}, {"heading": "A. Formulation", "text": "Robust linear regression considers the case in which the observed matrix is corrupted by some potentially harmful disturbance. (The goal is to find the optimal solution in the worst-case sense. (1) In this section, we will consider the class of uncertainty that binds the norm of disturbance to each characteristic without making common demands on characteristic disturbances. That is, we will consider the class of uncertainty sets: U, (paragraph 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Problem A). (2) In this section, we will consider the class of uncertainty sets: U, {paragraph \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 si-sets, i-sets, i-sets, i-sets, i-sets, i-sets, i-sets linking the norm of disturbance to each characteristic. (2) (2) We will refer to these uncertainty sets as uncoupled to the uncoupled problem, uncoupled to the uncoupled problem."}, {"heading": "B. Uncertainty Set Construction", "text": "One way to do this is if we know the distribution exactly, or if we only have incomplete information about the uncertainty, such as, for example, first and second moments. This chance-constraint formulation is particularly important when the distribution has a lot of support, which makes the naive robust optimization formulation excessively pessimistic. For the confidence level, the random constraint of the formulation becomes: minimize: tSubject to: tSubject to: Pr \u2212 b \u2212 (A + 2) x. Here are x and t the decision variables."}, {"heading": "A. Robust Optimization, Worst-case Expected Utility and Kernel Density Estimator", "text": "In this section we present some terms and intermediate results. In particular, we link a robust optimization formula with a poorly expected utility (w.r.t. a class of probability measures); we then briefly recall the definition of a kernel density estimator. Such results are used in determining the consistency of lasso, as well as in providing some additional knowledge about robust optimization. We first present a general result about the equivalence between a robust optimization formulation and a worst-case expected utility: Proposition 1: Given function g: Rm + 1 \u2192 R and Borel sets Z1, \u00b7 \u00b7 Zn Rm + 1, letPn, {\u00b5 S {1 \u00b7 \u00b7 \u00b7 \u00b7 n}:. \""}, {"heading": "B. Consistency of Lasso", "text": "We limit our discussion to the case where the magnitude of permissible uncertainty is the same for all characteristics. (i.e., the standard lasso) and establish the statistical consistency of lasso from a standard distributional argument. (i.d) We use cn to represent where there are n samples (). (i.d), and have a density f (). () Denote the2Recall that aij is the jth element of rim 11, 2008 DRAFT12set of the first n samples of Sn. (cn), argmin x (argmin x), argmin x (bi \u2212 r)."}], "references": [{"title": "Perturbation theory for the least-square problem with linear equality constraints", "author": ["L. Elden"], "venue": "BIT, 24:472\u2013476,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1985}, {"title": "Matrix Computation", "author": ["G. Golub", "C. Van Loan"], "venue": "John Hopkins University Press, Baltimore,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Backward error and condition of structured linear systems", "author": ["D. Higham", "N. Higham"], "venue": "SIAM Journal on Matrix Analysis and Applications, 13:162\u2013175,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Collinearity and total least squares", "author": ["R. Fierro", "J. Bunch"], "venue": "SIAM Journal on Matrix Analysis and Applications, 15:1167\u2013 1181,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Solution for Ill-Posed Problems", "author": ["A. Tikhonov", "V. Arsenin"], "venue": "Wiley, New York,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1977}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B, 58(1):267\u2013288,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, 32(2):407\u2013499,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Atomic decomposition by basis pursuit", "author": ["S. Chen", "D. Donoho", "M. Saunders"], "venue": "SIAM Journal on Scientific Computing, 20(1):33\u201361,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "On sparse representation in pairs of bases", "author": ["A. Feuer", "A. Nemirovski"], "venue": "IEEE Transactions on Information Theory, 49(6):1579\u20131581,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory, 52(2):489\u2013509,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J. Tropp"], "venue": "IEEE Transactions on Information Theory, 50(10):2231\u20132242,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Sharp thresholds for noisy and high-dimensional recovery of sparsity using l1-constrained quadratic programming", "author": ["M. Wainwright"], "venue": " Technical Report Available from: http://http://www.stat.berkeley.edu/tech-reports/709.pdf, Department of Statistics, UC Berkeley,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust solutions to least-squares problems with uncertain data", "author": ["L. El Ghaoui", "H. Lebret"], "venue": "SIAM Journal on Matrix Analysis and Applications, 18:1035\u20131064,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Second order cone programming approaches for handling missing and uncertain data", "author": ["P. Shivaswamy", "C. Bhattacharyya", "A. Smola"], "venue": "Journal of Machine Learning Research, 7:1283\u20131314, July", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimal inequalities in probability theory: A convex optimization approach", "author": ["D. Bertsimas", "I. Popescu"], "venue": "SIAM Journal of Optimization, 15(3):780\u2013800,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust regression and Lasso", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Technical report, Gerad, Available from http://www.cim.mcgill.ca/\u223cxuhuan/LassoGerad.pdf,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "The price of robustness", "author": ["D. Bertsimas", "M. Sim"], "venue": "Operations Research, 52(1):35\u201353, January", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal Of The Royal Statistical Society Series B, 67(2):301\u2013320,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Just relax: Convex programming methods for identifying sparse signals", "author": ["J. Tropp"], "venue": "IEEE Transactions on Information Theory, 51(3):1030\u20131051,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "An equivalence between sparse approximation and support vector machines", "author": ["F. Girosi"], "venue": "Neural Computation, 10(6):1445\u2013 1480,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Entropy-based algorithms for best-basis selection", "author": ["R.R. Coifman", "M.V. Wickerhauser"], "venue": "IEEE Transactions on Information Theory, 38(2):713\u2013718,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Matching Pursuits with time-frequence dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing, 41(12):3397\u20133415,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1993}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Transactions on Information Theory, 52(4):1289\u20131306,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "Annals of Mathematical Statistics, 27:832\u2013837,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1956}, {"title": "On the estimation of a probability density function and the mode", "author": ["E. Parzen"], "venue": "Annals of Mathematical Statistics, 33:1065\u2013 1076,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1962}, {"title": "Nonparametric Density Estimation: the l1 View", "author": ["L. Devroye", "L. Gy\u00f6rfi"], "venue": "John Wiley & Sons,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "Multivariate Density Estimation: Theory, Practice and Visualization", "author": ["D. Scott"], "venue": "John Wiley & Sons,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "The necessary and sufficient conditions for consistency in the empirical risk minimization method", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Pattern Recognition and Image Analysis, 1(3):260\u2013284,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1991}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research, 2:499\u2013526,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Weak Convergence and Empirical Processes", "author": ["A. van der Vaart", "J. Wellner"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Sparse algorithms are not stable: A no-free-lunch theorem", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Proceedings of Forty-Sixth Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "It is well known that minimizing the least squared error can lead to sensitive solutions [1]\u2013 [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "It is well known that minimizing the least squared error can lead to sensitive solutions [1]\u2013 [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "Among them, Tikhonov regularization [5] and Lasso [6], [7] are two widely known and cited algorithms.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]\u2013[12] and references therein).", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "2 also for its ability to recover the sparsity pattern exactly with probability one, asymptotically as the number of observations increases (there is an extensive literature on this subject, and we refer the reader to [8]\u2013[12] and references therein).", "startOffset": 222, "endOffset": 226}, {"referenceID": 12, "context": "In itself, this interpretation of Lasso as the solution to a robust least squares problem is a development in line with the results of [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "Most of the research in this area considers either the case where the disturbance is row-wise uncoupled [14], or the case where the Frobenius norm of the disturbance matrix is bounded [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Taking ci = c and normalizing ai for all i, Problem (3) recovers the well-known Lasso [6], [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "We postpone the proof to the appendix, and refer the reader to [15] for similar results using semi-definite optimization.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "For example, it is straightforward to show that if we take both \u2016 \u00b7 \u2016\u03b1 and \u2016 \u00b7 \u2016s as the Euclidean norm, then U \u2032 is the set of matrices with their Frobenious norms bounded, and Corollary 1 reduces to the robust formulation introduced by [13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 15, "context": "We refer the interested reader to [16] for full details.", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "In particular, one can use the polytope uncertainty discussed above, to show (see [16]) that by employing an uncertainty set first used in [17], we can model cardinality constrained noise, where some (unknown) subset of at most k features can be corrupted.", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "The resulting formulation resembles the elastic-net formulation [18], where there is a combination of l and l regularization.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "Lasso\u2019s ability to recover sparse solutions has been extensively studied and discussed (cf [8]\u2013[11]).", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Lasso\u2019s ability to recover sparse solutions has been extensively studied and discussed (cf [8]\u2013[11]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": ", [19]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "Using similar tools, we provide additional results in [16].", "startOffset": 54, "endOffset": 58}, {"referenceID": 7, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "Substantial research regarding sparsity properties of Lasso can be found in the literature (cf [8]\u2013[11], [20]\u2013[23] and many others).", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": ", [19], and are used as standard tools in investigating sparsity of Lasso from the statistical perspective.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "The kernel density estimator for a density \u0125 in R, originally proposed in [24], [25], is defined by hn(x) = (nc d n) \u22121 n \u2211", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "The kernel density estimator for a density \u0125 in R, originally proposed in [24], [25], is defined by hn(x) = (nc d n) \u22121 n \u2211", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "See [26], [27] and the reference therein for detailed discussions.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "See [26], [27] and the reference therein for detailed discussions.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "A celebrated property of a kernel density estimator is that it converges in L1 to \u0125 when cn \u2193 0 and ncn \u2191 \u221e [26].", "startOffset": 108, "endOffset": 112}, {"referenceID": 27, "context": ", [28]) and algorithmic stability (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 28, "context": ", [29]) often work for a limited range of algorithms, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": "1 of [26]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "6 of [30]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "This is a special case of a more general result we prove in [31], where we show that this is a common property for all algorithms that encourage sparsity.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "We recall the definition of uniform stability [29] first.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "However, as shown in [29], Tikhonov-regularized regression has stability that scales as 1/m.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "Stability that scales at least as fast as o( 1 \u221a m ) can be used to establish strong PAC bounds (see [29]).", "startOffset": 101, "endOffset": 105}], "year": 2008, "abstractText": "Lasso, or l regularized least squares, has been explored extensively for its remarkable sparsity properties. It is shown in this paper that the solution to Lasso, in addition to its sparsity, has robustness properties: it is the solution to a robust optimization problem. This has two important consequences. First, robustness provides a connection of the regularizer to a physical property, namely, protection from noise. This allows a principled selection of the regularizer, and in particular, generalizations of Lasso that also yield convex optimization problems are obtained by considering different uncertainty sets. Secondly, robustness can itself be used as an avenue to exploring different properties of the solution. In particular, it is shown that robustness of the solution explains why the solution is sparse. The analysis as well as the specific results obtained differ from standard sparsity results, providing different geometric intuition. Furthermore, it is shown that the robust optimization formulation is related to kernel density estimation, and based on this approach, a proof that Lasso is consistent is given using robustness directly. Finally, a theorem saying that sparsity and algorithmic stability contradict each other, and hence Lasso is not stable, is presented.", "creator": "LaTeX with hyperref package"}}}