{"id": "0812.4952", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2008", "title": "Importance weighted active learning", "abstract": "We propose an importance weighting framework for actively labeling samples. This technique yields practical yet sound active learning algorithms for general loss functions. Experiments on passively labeled data show that this approach effectively reduces the label complexity required to achieve good prediction performance on many learning problems.", "histories": [["v1", "Mon, 29 Dec 2008 18:29:08 GMT  (199kb)", "https://arxiv.org/abs/0812.4952v1", null], ["v2", "Fri, 6 Feb 2009 20:56:24 GMT  (183kb)", "http://arxiv.org/abs/0812.4952v2", null], ["v3", "Thu, 7 May 2009 13:14:45 GMT  (201kb)", "http://arxiv.org/abs/0812.4952v3", null], ["v4", "Wed, 20 May 2009 17:40:23 GMT  (201kb)", "http://arxiv.org/abs/0812.4952v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alina beygelzimer", "sanjoy dasgupta", "john langford"], "accepted": true, "id": "0812.4952"}, "pdf": {"name": "0812.4952.pdf", "metadata": {"source": "CRF", "title": "Importance Weighted Active Learning", "authors": ["Alina Beygelzimer"], "emails": ["beygel@us.ibm.com", "dasgupta@cs.ucsd.edu", "jl@yahoo-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 081 2.49 52v4 [cs.LG] Keywords: active learning, importance weighting, sampling bias"}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Preliminaries", "text": "We consider active learning in the streaming setting, in which a learner observes an unlabeled dot xX at each step t and must decide whether to ask for the label yt-Y. The learner works with a hypotheses space H = {h: X \u2192 Z}, where Z is a prediction space.The algorithm is evaluated with respect to a given loss function l: Z \u00b7 Y \u2192 [0, \u221e) The most common loss function is 0-1 loss, where Y = {\u2212 1, 1} and l (z, y) = 1 (y = 6 = z) = 1 (yz < 0). The following examples deal with the binary case Y = {\u2212 1, 1} with Z-R: \u2022 l (z, y) = (1 \u2212 yz) + (hinge loss), \u2022 l (z \u2212 yz) (logistic loss), \u2022 l (z, y)."}, {"heading": "3. The Importance Weighting Skeleton", "text": "Algorithm 1 describes the basic sketch of meaning-weighted active learning (IWAL). After seeing xt, the learner calls a subroutine rejection threshold (which is instantiated in later sections) that looks at xt and past history to return the probability of the rejection threshold (subroutine rejection threshold) S0 =. The algorithm maintains a series of previously selected examples, each of which has a weight: when yt is queried, its weight is set to 1 / pt. Algorithm 1 IWAL (subroutine rejection threshold) S0 =. For t of 1, 2,... until the data stream runs out: 1. Receive xt.2. set pt = rejection threshold (xt, yi, pi, Qi: 1 \u2264 i < t}) S0 =. 3. Turn a coin Qt \u00b2 (0, 1} with E [Qt] = pt."}, {"heading": "3.1 A safety guarantee for IWAL", "text": "A desirable feature for a learning algorithm is consistency: In view of an infinite budget of unspecified and unspecified examples (< < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4. Setting the Rejection Threshold: Loss Weighting", "text": "T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T T"}, {"heading": "4.1 A generalization bound", "text": "We start with a large deviation that is limited for each of the two types of IWAL (loss weighting). (It is not the sequence of Theorem 1, because it does not require sampling probabilities that are limited below zero. (It is not the sequence). (It is not the sequence of Theorem 1, but it is a minimizer of the loss function with respect to D. Pick any sampling probability > 0. (It is the probability of T). (It is the probability of T). (It is the probability of T). (It is the probability of T). (It is the probability of T). (It is the probability of T). (It is the probability of T)."}, {"heading": "5. Label Complexity", "text": "Dasgupta et al. (2008) examined this question for an active learning scheme below 0-1 loss. For learning problems with limited coefficients of discrepancy (Hanneke, 2007), it was found that the number of queries is O (\u03b7T + d log2 T), where d is the VC dimension of the functional class, and \u03b7 is the best achievable error rate with respect to the underlying distribution by functional class. We will soon see (Section 6) that the term throuT is unavoidable for any active learning scheme; the remaining term merely shows a polylogarithmic dependence on T. We generalize the discrepancy coefficient to arbitrary loss functions and show that under conditions similar to the previous result, the number of queries is O (throughT + dT log2 T), although the second most frequent loss is not inevitable and is the second most frequent."}, {"heading": "5.1 Label Complexity: Main Issues", "text": "Suppose the loss function is decreased by h-value, where L-value = L-value (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-value) (h-h) (h-value) (h-h) (h-value) (h-h) (h-value) (h-value) (h-value)."}, {"heading": "5.2 A subclass of loss functions", "text": "We specify label complexity limits for a class of loss functions that include 0-1 loss and logistical loss, but no hinge loss. Specifically, we require that the loss function has a limited inclination asymmetry, defined below. Definition 3 The inclination asymmetry of a loss function l: Z \u00b7 Y \u2192 [0, \u221e] isKl = sup z, z \u00b7 Z \u2212 l (z, y) \u2212 l (z, y) \u2212 l (z, y) \u2212 l (z, y) \u2212 l (z \u2032, y) \u2212 l (z \u2032, y) |. The inclination asymmetry is 1 for 0-1 loss and z \u2212 z for hinge loss. For differentiable loss functions l (z, y \u2212 l) \u2212 l (z, y) \u2212 l (z, y) \u2212 l (z, y) \u2212 l (z \u2032, y) \u2212 l (y \u2032, y). The inclination asymmetry of a loss function: Z \u00b7 z, y)."}, {"heading": "5.3 Topologizing the space of classifiers", "text": "We perform a simple distance function for the space of classifiers on.Definition 5 For any f, g, H and distribution D define \u03c1 (f, g) = Ex \u0445 D maxy | l (f (x), y) \u2212 l (g (x), y) |. For any r \u2265 0, we leave B (f, r) = {g, h (f, g) \u2264 r}.Suppose that the L \u0445 H L (h) asymmetry of the slope losses is realized.Lemma 3 For each distribution D and each loss function with slope asymmetry Kl, we have a loss of at most L \u0445 + 2 \u0445 t \u2212 1. Does this mean that they are at h distance near H? The ratio between the two can be expressed in terms of the slope asymmetry of the losses. Lemma 3 For each distribution D and each loss function with slope asymmetry Kl (h, h, h, h) (h) with slope asymmetry Kl (h) (h, h) (l) (l) (l, l) (l) (l () (l, l () (l), \u2264 (l) (h, h (h), h (h), h (h (h), h (h), h (h), h (h (h)."}, {"heading": "5.4 A generalized disagreement coefficient", "text": "In analyzing the A2 algorithm (Balcan et al., 2006) for active learning under 0-1 losses, Hanneke (2007) found that its label complexity could be characterized by what he called the inconsistency coefficient of the learning problem. We now generalize this term to arbitrary loss functions. (Definition 6) The inconsistency coefficient is the inferiority value of a linear separator. (Lemma 4 Suppose H consists of linear classifiers {u, r) supy | l (h (x), y (h) \u2212 l (h) \u2212 l (h), y (h). (Definition 6) Here is a simple example of linear separators. (Lemma 4 Suppose H consists of linear classifiers {u, r) supy (h) and the data distribution D is uniform across the surface of the unit sphere in Rd (x)."}, {"heading": "5.5 Upper Bound on Label Complexity", "text": "Theorem 7 For all learning problems D and hypotheses H, if the loss function has a slope asymmetry Kl and the learning problem has a discrepancy coefficient Kl, then for all \u03b4 > 0, with a probability of at least 1 \u2212 \u03b4 above the selection of the data, the expected number of labels requested by IWAL (loss weighting) during the first T iterations is most4\u03b8 \u00b7 Kl \u00b7 (L \u043e T + O (\u221a T (| H | T / \u03b4))), with the expectation exceeding the randomness in the selective sample. Proof h: Soup h: H: H achieves loss L: T \u00b7 (T). Select any time. According to Theorem 2, Ht: H: L (h) \u2264 T: H \u00b7 h: Summa of randomness in the selective sample."}, {"heading": "5.6 Other examples of low label complexity", "text": "It is sometimes also possible to achieve considerable reductions in complexity through passive learning processes, even if the asymmetry of the slope is infinite. Example 1 Let space X be the ball of radius 1 in d dimensions. Let the distribution D to X be a point mass at origin with weight 1 \u2212 \u03b2 and labeling 1 and a point mass (1, 0, 0,.., 0) with weight \u03b2 and labeling \u2212 1 half the time and labeling 0 for the other half of the time.Let the hypotheses space be linear with weight vectors that are satisfactory | | w | \u2264 1. Let the loss of interest square loss: l (h (x), y) = (h (x) \u2212 y) 2, which has an infinite slope asymmetry. Observation 8 For the above example, IWAL (loss weighting) requires only an expected \u03b2-fraction of designated samples to reach the same learning process."}, {"heading": "6. A lower bound on label complexity", "text": "(Ka \ufffd a \ufffd ria \ufffd inen, 2006) showed that for each hypothesis class H and each procedure > procedure > procedure > 0, there is a data distribution, so that (a) the optimal error rate that can be achieved by H is the same; and (b) any active learner who has h'H with error rate \u2264 procedure + procedure (with probability > 1 / 2) must at least make mistakes. (It is well known that if a supervised learner sees T examples (for each procedure), his final hypothesis has an error. (Devroye et al, 1996) with a high probability. Consider this as procedure for such procedure / procedure. Our lower limit now implies that an active learner must have at least 2 / procedures."}, {"heading": "7. Implementing IWAL", "text": "IWAL (loss weighting) can be efficiently implemented if H is the class of linear delimiters of limited length. Each iteration of algorithm 2 involves solving two optimization problems via a limited hypothesis setHt = t \"< t {h \u00b2 H: 1t \u00b2 x \u00b2 t\" i = 1 Qi pi l (h (xi), yi). Each iteration of algorithm 2 involves solving two optimization problems via a limited hypothesis setHt \"< t {h \u00b2 H: 1t \u00b2 x \u00b2 t\" p \"t\" p \"p\" i \"(h (xi), yi\" p \"p\" p \"p\" p \"p.\""}, {"heading": "7.1 Experiments", "text": "Recent consistent active learning algorithms (Balcan et al., 2006; Dasgupta et al., 2008) suffered from computational intractability. This section demonstrates that important active learning is practicable. We implemented IWAL with loss weighting for linear separators under logistic losses. As described above, the algorithm includes two convex optimizations as subroutines, which were encoded using log barrier methods (Section 11.2 of (Boyd and Vandenberghe, 2004). We tested the algorithm using the MNIST dataset of handwritten digits, selecting the 3s and 5s as two classes, selecting 1000 copies each for training and another 1000 from each for the test. We used PCA to reduce the dimension from 784 to 25. The algorithm uses a generalization tied to the form of learning."}, {"heading": "7.2 Bootstrap instantiation of IWAL", "text": "This section reports on another practical implementation of IWAL, using a simple bootstrapping scheme to calculate the reject threshold. A SetH of predictors is trained on some initial examples and serves as an approximation of the version space. In the face of a new unlabeled example x, the sampling probability is set to pmin + (1 \u2212 pmin) [maxy; hi, hj, H L (hi (x), y) \u2212 L (hj (x), y)], where pmin is a lower limit for the sampling probability. We implemented this scheme for binary and multiclass classification losses using 10 decision trees bootstrapped on the initial 1 / 10 of the training set, with pmin = 0.1 set. For simplicity, we did not retrain the predictors for each new queried point, i.e. the predictors were bootstrapped once at the initial sample."}, {"heading": "8. Conclusion", "text": "The IWAL algorithms and analyses presented here overcome many well-founded objections to the use of active learning. IWAL fulfils the same convergence guarantee as conventional supervised learning algorithms, it can use standard algorithms (Section 7.2), it can handle very flexible losses and achieve significant improvements in label complexity in theory and practice. Empirically, in every experiment we have tried, IWAL has significantly reduced label complexity compared to supervised learning, without sacrificing performance in the same number of unlabelled examples. Since IWAL explicitly takes the Sample Select Bias into account, we can be sure that these experiments are valid for the creation of new data sets. This implies another subtle advantage: since the sampling bias is known, it is possible to hypothesize and verify the performance of IWAL algorithms on data sets created by IWAL."}, {"heading": "9. Acknowledgements", "text": "We would like to thank Alex Strehl for a very careful reading that caught a few proof bugs."}], "references": [{"title": "Hierarchical sampling for active learning", "author": ["S. Dasgupta", "D. Hsu"], "venue": null, "citeRegEx": "Dasgupta and Hsu.,? \\Q1994\\E", "shortCiteRegEx": "Dasgupta and Hsu.", "year": 1994}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Springer", "1996. S. Hanneke"], "venue": null, "citeRegEx": "Springer and Hanneke.,? \\Q1996\\E", "shortCiteRegEx": "Springer and Hanneke.", "year": 1996}, {"title": "Active learning for misspecified models", "author": ["M. Sugiyama"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sugiyama.,? \\Q2006\\E", "shortCiteRegEx": "Sugiyama.", "year": 2006}, {"title": "Cost-sensitive learning by cost-proportionate example weighting", "author": ["B. Zadrozny", "J. Langford", "N. Abe"], "venue": "In Proceedings of the Third IEEE International Conference on Data Mining,", "citeRegEx": "Zadrozny et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zadrozny et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "In such cases, they can exhibit a lack of statistical consistency: even with an infinite labeling budget, they might not converge to an optimal predictor (see Dasgupta and Hsu (2008) for a discussion).", "startOffset": 159, "endOffset": 183}, {"referenceID": 2, "context": "The second approach to active learning uses importance weights to correct sampling bias (Bach, 2007; Sugiyama, 2006).", "startOffset": 88, "endOffset": 116}, {"referenceID": 3, "context": "The Costing technique (Zadrozny et al., 2003) was used to remove the importance weights using rejection sampling.", "startOffset": 22, "endOffset": 45}], "year": 2009, "abstractText": "We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process. Experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many learning problems.", "creator": "gnuplot 4.2 patchlevel 0"}}}