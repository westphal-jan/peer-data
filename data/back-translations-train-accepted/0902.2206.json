{"id": "0902.2206", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2009", "title": "Feature hashing for large scale multitask learning", "abstract": "Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case -- multitask learning with hundreds of thousands of tasks.", "histories": [["v1", "Thu, 12 Feb 2009 20:06:36 GMT  (875kb)", "http://arxiv.org/abs/0902.2206v1", null], ["v2", "Wed, 1 Apr 2009 16:18:39 GMT  (875kb)", "http://arxiv.org/abs/0902.2206v2", null], ["v3", "Wed, 20 May 2009 19:05:20 GMT  (554kb)", "http://arxiv.org/abs/0902.2206v3", null], ["v4", "Thu, 21 May 2009 21:18:40 GMT  (875kb)", "http://arxiv.org/abs/0902.2206v4", null], ["v5", "Sat, 27 Feb 2010 15:32:35 GMT  (543kb)", "http://arxiv.org/abs/0902.2206v5", "Fixed broken theorem"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kilian q weinberger", "anirban dasgupta 0001", "john langford", "alexander j smola", "josh attenberg"], "accepted": true, "id": "0902.2206"}, "pdf": {"name": "0902.2206.pdf", "metadata": {"source": "CRF", "title": "Feature Hashing for Large Scale Multitask Learning", "authors": ["Kilian Weinberger", "Anirban Dasgupta", "Josh Attenberg", "John Langford", "Alex Smola"], "emails": ["KILIAN@YAHOO-INC.COM", "ANIRBAN@YAHOO-INC.COM", "JOSH@CIS.POLY.EDU", "JL@HUNCH.NET", "ALEX@SMOLA.ORG"], "sections": [{"heading": null, "text": "ar Xiv: 090 2.22 06v1 [cs.AI] 12 Fe"}, {"heading": "1. Introduction", "text": "That is, given objects x1,.., xn \"X for some ranges X, they rely on onk (xi, xj): = <? (xi), \u03c6 (xj) > (1) to compare the properties of xi (xi) and \u03c6 (xj), implicitly by defining a positive semi-definitive kernel matrix k, without ever having to calculate a vector trick (xi). This can be particularly powerful in classification settings where the initial input of representation systems has a non-linear decision boundary. Frequently, linear separability can be achieved in a high-dimensional attribute."}, {"heading": "2. Hash Functions", "text": "We introduce a variant of the hash core proposed by (Shi et al., 2009), which is modified by introducing a signed sum of hash properties, while the original hash cores use an unsigned sum. This change results in an unbiased estimate, which we will demonstrate and use in the following section. Definition 1 Identified by h as a hash function h: N \u2192 {1,..., m}. In addition, of course, we designate a hash function: N \u2192 {\u00b1 1}. Then, for vectors x \u00b2, we define the hash function and the associated internal product as\u03c6 (h,.) i (x) = prejudice j: h (i) = i) xi (2) and < x, x \u00b2 x > operations: = prejudice (h,.) and the hash function can be individually (h,.)."}, {"heading": "3. Analysis", "text": "The following section is devoted to the theoretical analysis of hash cores and their applications. In this sense, the present work continues where (Shi et al., 2009) is lacking: We prove exponential tail limits. These limits apply to general hash cores, which we later apply to show how hashing enables us to efficiently perform large-scale multitask learning. Let's start with a simple problem regarding the bias and variance of the hash core. The proof for this problem appears in Appenditure A.Lemma 2. The hash kernel is unbiased, that is, E\u03c6 [< x, x, x, x, x. \"In addition, the deviation \u03c32x, x \u2032 = 1 m (\u2211 i6 = j x 2 i x \u2032 j 2 + xix \u2032 iklump \u2032 iklump \u2032 j), and therefore it is within half of the range we are in, for considering the quality of the hash core."}, {"heading": "3.1. Concentration of Measure Bounds", "text": "In this subsection we show that the length of each vector is most likely preserved (Ledoux, 2001). Talagand's inequality (Ledoux, 2001) is a key tool for proving the following theorem (detailed in Appendix B). Theorem 3 Let us be a fixed constant and x be a given instance. Let us allow the hash kernel to also fulfill the following pairs of inequalities (Shi et al., 2009). The only change is the inequality associated with it. (Ledoux, 2001)."}, {"heading": "3.2. Multiple Hashing", "text": "Note that the tightness of the compound bound in Korolla 5 depends crucially on the size of \u03b7. In other words, at large values of \u03b7, that is, if some terms in x are very large, even a single collision can already cause significant distortions of the embedding. This problem can be changed by comparing sparseness with variance. A vector of the unit length can be described as (1, 0, 0, 0,...) or as (1, 2, 1, 2, 0,...) or more generally as a vector with cnonzero terms of size c \u2212 1 2. This is relevant, for example, if the order of size of x follows a known pattern, e.g. if we present documents as bags of words, as we can simply hash common words several times. The following sequence gives an intuition about how the confidence scale scales with respect to replications: Lemma 6 If we use x = nitude c (x,)."}, {"heading": "3.3. Approximate Orthogonality", "text": "For multi-task learning, we need to learn a different parameter vector for each related task. If we place it in the same hash attribute space, we want to make sure that there is little interaction between the different parameter vectors. For each observation x for task u, let us minimize the interaction of w with x in the hashed attribute space. For each x, let the image of x under the hash attribute map for task u (x) = p (x, h) (x, u)).Theorem 7 Let w be a parameter vector for tasks in U\\ {u}. In this case, the value of the inner product < w, \u03c6u (x) > is the ability of learning x (x, u). [Theorem 7 Let w-Rm be a parameter vector for tasks in U\\ {u}."}, {"heading": "4. Applications", "text": "The advantage of feature hashing is that we explore this setting in the context of spam classification."}, {"heading": "5. Results", "text": "We evaluated our algorithms in the personalization setting, linking them to a proprietary email spam classification task of n = 3.2 million emails that were properly anonymized, collected by users. Each email is labeled as spam or non-spam by a user in U. In the email spam literature, the misclassification of non-spam is considered much more harmful than the misclassification of spam. We therefore follow the convention to set the classification threshold during the trial period so that exactly 1% of the non-spam test data is classified as spam. Our implementation of the personalized hash functions is illustrated in Figure 1."}, {"heading": "6. Related Work", "text": "A number of researchers have tackled related, albeit different, problems. (Rahimi & Recht, 2008) Instead, use Bochner's theorem and sampling to obtain approximate inner products for the Radial Base Function cores. (Rahimi & Recht, 2009) Expand them to a sparse approximation of the weighted combinations of basic functions. This is mathematically efficient for many function spaces. Note that the representation is dense. (Li et al., 2007) Take a complementary approach: for sparse feature vectors, they develop a scheme to reduce the number of unequal terms even further. While this is desirable in principle, it does not solve the problem of high dimensions. (x) It is necessary to express the function in dual representation instead of expressing f as a linear function, the magnetor function, where it is unlikely to be compressed."}, {"heading": "7. Conclusion", "text": "In this paper, we analyze theoretically and empirically the hash trick to reduce dimensionality. In our theoretical analysis, we introduce unbiased hash functions and provide exponential tail boundaries for hash nuclei. These provide further information on hash spaces and explain previously made empirical observations. We also deduce that random sub-spaces of the hash space are unlikely to interact, allowing multitasking learning with many tasks. Our empirical results confirm this in a real application in the context of spam filtering. Here, we show that even with a very large number of tasks and features, all mapped into a common low-dimensional hash space, impressive classification results can be achieved with finite storage warranty."}, {"heading": "A. Mean and Variance", "text": "Proof [Lemma 2] To calculate the expectation, let's stretch < x, x, x, x, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "B. Concentration of Measure", "text": "Our proof uses Talagand's convex distance inequality. We first define a weighted hamming distance function between two hash functions and a number of hash functions as a sequence."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas", "year": 2003}, {"title": "The Netflix Prize", "author": ["J. Bennett", "S. Lanning"], "venue": "Proceedings of KDD Cup and Workshop", "citeRegEx": "Bennett and Lanning,? \\Q2007\\E", "shortCiteRegEx": "Bennett and Lanning", "year": 2007}, {"title": "The theory of probabilities. Moscow: Gastehizdat Publishing House", "author": ["S. Bernstein"], "venue": null, "citeRegEx": "Bernstein,? \\Q1946\\E", "shortCiteRegEx": "Bernstein", "year": 1946}, {"title": "An improved data stream summary: The count-min sketch and its applications", "author": ["G. Cormode", "M. Muthukrishnan"], "venue": "LATIN: Latin American Symposium on Theoretical Informatics", "citeRegEx": "Cormode and Muthukrishnan,? \\Q2004\\E", "shortCiteRegEx": "Cormode and Muthukrishnan", "year": 2004}, {"title": "Small statistical models by random feature mixing", "author": ["K. Ganchev", "M. Dredze"], "venue": "Workshop on Mobile Language Processing, Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Ganchev and Dredze,? \\Q2008\\E", "shortCiteRegEx": "Ganchev and Dredze", "year": 2008}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Proceedings of the 25th VLDB Conference (pp. 518\u2013529)", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Vowpal wabbit online learning project (Technical Report)", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": null, "citeRegEx": "Langford et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2007}, {"title": "The concentration of measure phenomenon", "author": ["M. Ledoux"], "venue": null, "citeRegEx": "Ledoux,? \\Q2001\\E", "shortCiteRegEx": "Ledoux", "year": 2001}, {"title": "Conditional random sampling: A sketch-based sampling technique for sparse", "author": ["P. Li", "K. Church", "T. Hastie"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "Random features for largescale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": null, "citeRegEx": "Rahimi and Recht,? \\Q2008\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2008}, {"title": "Randomized kitchen sinks", "author": ["A. Rahimi", "B. Recht"], "venue": null, "citeRegEx": "Rahimi and Recht,? \\Q2009\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "For this common scenario several authors have recently proposed an alternative, but highly complimentary variation of the kernel-trick, which we refer to as the hashing-trick: one hashes the high dimensional input vectors x into a lower dimensional feature space R with \u03c6 : X \u2192 R (Langford et al., 2007; Shi et al., 2009).", "startOffset": 280, "endOffset": 321}, {"referenceID": 7, "context": "Talagrand\u2019s inequality (Ledoux, 2001) is a key tool for the proof of the following theorem (detailed in the appendix B).", "startOffset": 23, "endOffset": 37}, {"referenceID": 2, "context": "Proof We use Bernstein\u2019s inequality (Bernstein, 1946), which states that for independent random variables Xj , with E [Xj ] = 0, if C > 0 is such that |Xj| \u2264 C, then", "startOffset": 36, "endOffset": 53}, {"referenceID": 8, "context": "(Li et al., 2007) take a complementary approach: for sparse feature vectors, \u03c6(x), they devise a scheme of reducing the number of nonzero terms even further.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "(Achlioptas, 2003) provides computationally efficient randomization schemes for dimensionality reduction.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "Instead of performing a dense d\u00b7m dimensional matrix vector multiplication to reduce the dimensionality for a vector of dimensionality d to one of dimensionality m, as is required by the algorithm of (Gionis et al., 1999), he only requires 1 3 of that computation by designing a matrix consisting only of entries {\u22121, 0, 1}.", "startOffset": 200, "endOffset": 221}, {"referenceID": 0, "context": "This is a significant computational saving over locality sensitive hashing (Achlioptas, 2003; Gionis et al., 1999).", "startOffset": 75, "endOffset": 114}, {"referenceID": 5, "context": "This is a significant computational saving over locality sensitive hashing (Achlioptas, 2003; Gionis et al., 1999).", "startOffset": 75, "endOffset": 114}, {"referenceID": 6, "context": "In addition, (Langford et al., 2007) released the Vowpal Wabbit fast online learn-", "startOffset": 13, "endOffset": 36}], "year": 2017, "abstractText": "Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case \u2014 multitask learning with hundreds of thousands of tasks.", "creator": null}}}