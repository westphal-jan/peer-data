{"id": "0903.2851", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2009", "title": "A Parameter-free Hedging Algorithm", "abstract": "In this paper, we consider the decision-theoretic framework for online learning (DTOL) proposed by Freund and Schapire \\cite{FS97}. Previous algorithms for learning in this framework have a tunable learning rate parameter. Tuning the learning rate requires prior knowledge about the sequence and severely limits the practicality of the algorithm. While much progress has been made in the past decade for adaptively tuning the learning rate, all of these methods still ultimately rely on some prior information. We propose a completely parameter-free algorithm for learning in this framework. We show theoretically that our algorithm has a regret bound similar to the best bounds achieved by previous algorithms with optimally-tuned learning rates. We also present a few experiments comparing the performance of the algorithm with that of other algorithms for various tunings.", "histories": [["v1", "Mon, 16 Mar 2009 20:48:33 GMT  (70kb)", "http://arxiv.org/abs/0903.2851v1", null], ["v2", "Mon, 18 Jan 2010 23:58:51 GMT  (29kb)", "http://arxiv.org/abs/0903.2851v2", "Updated Version"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["kamalika chaudhuri", "yoav freund", "daniel j hsu"], "accepted": true, "id": "0903.2851"}, "pdf": {"name": "0903.2851.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kamalika@soe.ucsd.edu", "yfreund@ucsd.edu", "djhsu@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 090 3.28 51v1 [cs.LG] 1 6M ar2 00We propose a completely parameter-free algorithm for learning within this framework. Theoretically, we show that our algorithm has a regret limit similar to the best limits achieved by previous algorithms with optimally tuned learning rates. We also present some experiments comparing the performance of the algorithm with that of other algorithms for different tunings."}, {"heading": "1 Introduction", "text": "In this paper, we look at the Decision Framework for Online Learning (DTOL) proposed by Freund and Schapire [FS97]. DTOL is a variant of the framework of prediction with expert advice introduced by Littlestone and Warmuth. Losses are limited to the range [0, 1]. The loss of the forecaster in each round is the average loss of actions for that round, where the average is calculated based on the current probability mapping of the forecaster. The goal of the forecaster is to achieve a cumulative loss close to the lowest cumulative loss of all individual actions. We call this action the difference between cumulative losses and cumulative losses."}, {"heading": "2 Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Setting", "text": "In turn t, the learner chooses a weight distribution pt = (p1, t,.., pN, t) over actions 1, 2,.., n. Each action results in a loss li, t, and the learner takes the expected loss distribution p = (p1, t,.) tli,.The learner's immediate regret for an action i in round t is ri, t = lA, t \u2212 li, t, and regret for an action i in each of the first t rounds isRi, t = t = 1pi, \u03c4. We assume that the range of losses li, t is an interval of length 1 (e.g. [0, 1] or [\u2212 1 / 2, 1 / 2]; the sign of loss does not matter).The learner's goal is to minimize this value."}, {"heading": "2.2 Normal-Hedge", "text": "Our algorithm, Normal Hedge, is based on a potential function reminiscent of half the normal distribution, where [x] + stands for max {0, x}. It is easy to check whether this function is convex separately in x and c, differentiable and doubly differentiable, except for x = 0. Figure 4 catalogues the derivatives. In addition to tracking the cumulative regret Ri, t for each action i after each turn t, the algorithm also receives a scale parameter ct. This is chosen so that the average of the potential for all actions i evaluated in Ri, t and ct remains constant at e: 1NN-i = 1exp (([Ri, t] +) -t. \u2212 We note that the average of the potential i over all actions i, t and ct, convected in Ri and ct, remains constant at e: 1NN-i = 1exp ([Ri, t])."}, {"heading": "3 Derivation of the algorithm", "text": "Before we get down to work, we have to get down to work."}, {"heading": "4 Related work", "text": "A significant part of the online learning literature is devoted to improving the adaptability of the learning algorithm. Many of the works are variations of the exponential weight algorithm (e.g. hedge), which was originally based on Littlestone and Warmuth's Weighted Majority algorithm [LW94]. In exponential weight classes, the weight assigned to action i in round t is proportional to e\u03b7Ri, t \u2212 1. The parameter \u03b7 is the learning rate that controls the sensitivity of the algorithm to another action. Here, we highlight some of these variations of exponential weights, as well as a few other algorithms, since they relate to normal hedge.The question of time adaptation - i.e. we do not know the time horizon in advance - was originally addressed in [CBFH + 97] with a double trick: guess the horizon (and tune the algorithm accordingly), and double assumptions if it turns out wrong."}, {"heading": "5 Experiments", "text": "We report on a few simple synthetic experiments that illustrate the effect of tuning in terms of the number of actions + 1 + 1 + 1. In these experiments, the momentary losses of the actions are given by a matrix AN based on the Hadamard matrix (in fact, the all-one row is deleted, the result stacked on its negation, and then each line is repeated indefinitely).The loss of the action i in round is the (i, t) -th entry in the matrix AN. We show A6 for concreteness, but it slightly generalizes to each N = 2d + 1 \u2212 2 using the 2d \u00b7 2d Hadamard matrix: A6 = \u2212 1 \u2212 1 \u2212 1 \u2212 1 the actions of the matrix AN. \u2212 1 / 2 \u2212 1 \u2212 1 The best actions are slightly generalized to each N, but it slightly generalizes to each N = 2d + 1 \u2212 2 \u2212 2 the generalizes."}, {"heading": "6 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Main results", "text": "Our main result is the following theory.Theorem 1. If normal hedge has access to N actions, then the regret of the algorithm for all action loss sequences for all t, for all 0 < 1 and for all 0 < \u03b4 \u2264 1 / 2, the regret of the algorithm for the uppermost E quantile of actions is at most \u221a (1 + ln (1 + ln (1 / l))) (3 (1 + 50\u043c) t + 16 ln2 N (10.2 \u03b42 + lnN)).The following conclusion illustrates the performance of our algorithm for large values of t.Corollary 2. If normal hedge has access to N actions, then the regret of normal hedge follows normal hedge (10.2 \u03b42 + lnN).The following conclusion illustrates the performance of our algorithm for large values of t.Corollary 2. If normal hedge has access to N actions, then the regret of normal hedge follows normal hedge (10.2 \u03b42 + lnN).The following conclusion illustrates the performance of our algorithm for large values of t.Corollary 2."}, {"heading": "6.2 Regret bounds from the potential equation", "text": "The following problem relates to the performance of the algorithm at a given time t on the scale ct.Lemma 3. At any time t, the regret about the best act can be limited as follows: max iRi, t \u2264 \u221a 2ct (lnN + 1) In addition, the regret about the best act for each act is at most 2ct (ln (1 / 2) + 1).Proof. We use Et to designate the actions that do not have zero weight on the iteration. The first part of the problem results from the fact that for each act i-Et, exp (((Ri, t) 22ct) = exp (([Ri, t] +) 22ct) \u2264 N-i-1exp ((([Ri, t] +) 22ct) \u2264 Newhich implies Ri, t-t-t, the part of our regret (Ri, t] +) or most of our regret with the greatest potential (Ri, 2ct + 1)."}, {"heading": "6.3 Bounds on the scale ct and proof of Theorem 1", "text": "In Lemmas 4 and 5 we have summarized the growth of the scalenct as a function of time. The main outline of the evidence for theorem 1 is as follows: Since ct increases monotonously with t, we can divide the rounds t into two phases, t < t0 and t \u2265 t0, where t0 is the first time such a phenomenon occurs. 4 ln2 N\u03b4 + 16 lnN\u03b43, for some defined phases. We then show limits for the growth of ct for each phase separately. Lemma 4 shows that ct is not too large at the end of the first phase, while Lemma 5 shows the growth of ct per round in the second phase. Lemma 4. For each time t, ct + 1 \u2264 2ct (1 + lnN) + 3.Lemma 5. Suppose that at some point t0, ct0 \u2265 4 ln 2 ln 2 + 16 lnN \u04323, where 0 \u2264 12 is a constant."}, {"heading": "7 Remaining proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Proof of Lemma 4", "text": "To prove term 4, we first show that the left side of the aforementioned inequality for each i, Ri, t + 1 \u2264 Ri, t + 1 \u2264 iexp (((Ri, t + 1) 24ct (1 + lnN) + 6), which in turn can be explained by 1N \u2211 iexp (R2i, t 4ct (1 + lnN) + 6) \u00b7 exp (2Ri, t 4ct (1 + lnN) + 6) \u00b7 exp (14ct (1 + lnN) + 6).We now bind every term in this equation. Firstly, we find that the use of term 3, the first term asexp (R2i, t 4ct (1 + lnN) + 6, a) exp (2ct), that each term (1N), (ln1), (ln1), (ln1), (1), (1), ln1, 1, (1), (ln1), (1), (ln1), (1), (1), (ln2)."}, {"heading": "7.2 A bootstrap for Lemma 5", "text": "Before we can prove Lemma 5, we first show a slightly weaker bond to the growth of ct with t (Lemma 6); this bond is used in the proof of Lemma 5, which concludes with the tighter bond to ct + 1 \u2212 ct. Lemma 6. Suppose that at some point t0, ct0 \u2265 16 lnN\u03b42, where 0 \u2264 1 / 2 is a constant, is used. First, we use Lemma 7 to show that ct is monotonous in t, and to get an expression for ct + 1 \u2212 ct \u2212 ct as the ratio of some derivatives and double derivatives of the potential function. Next, we use Lemma 8 and Corollary 10 to bind the numerator and denominator of this ratio."}, {"heading": "7.3 Proof of Lemma 5", "text": "Finally, we are ready to prove Lemma 5. As in the proof of Lemma 6, we also start here with an upper limit of + 1 \u2212 ct, obtained from Lemma 7. We then use this upper limit and the limit in Lemma 6 to set a finer limit on the quantity + 1 \u2212 ln.Proof of of Lemma 5. We divide the actions into two sentences: S1 = {i Et, t + 1: [Ri, t + 1] + 1: [Ri, t + 1] + 1: [Ri, t + 1: [2] + 1: [Ri, t]."}], "references": [{"title": "Adaptive and self-confident on-line learning algorithms", "author": ["P. Auer", "N. Cesa-Bianchi", "C. Gentile"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Hembold", "R.E. Schapire", "M. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "On-line prediction and conversion strategies", "author": ["N. Cesa-Bianchi", "Y. Freund", "D.P. Hembold", "M. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1996}, {"title": "Potentialbased algorithms in on-line prediction and game theory", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2003\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2003}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Drifting games and Brownian motion", "author": ["Y. Freund", "M. Opper"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Opper.,? \\Q2002\\E", "shortCiteRegEx": "Freund and Opper.", "year": 2002}, {"title": "A decisiontheoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "The robustness of p-norm algorithms", "author": ["C. Gentile"], "venue": "Machine Learning,", "citeRegEx": "Gentile.,? \\Q2003\\E", "shortCiteRegEx": "Gentile.", "year": 2003}, {"title": "General convergence results for linear discriminant updates", "author": ["A.J. Grove", "N. Littlestone", "D. Schuurmans"], "venue": "Machine Learning,", "citeRegEx": "Grove et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Grove et al\\.", "year": 2001}, {"title": "Approximation to bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["E. Hazan", "S. Kale"], "venue": "In COLT,", "citeRegEx": "Hazan and Kale.,? \\Q2008\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2008}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["M. Hutter", "J. Poland"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "Efficient algorithms for the online optimization", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Learning with continuous experts using drifting games", "author": ["Indraneel Mukherjee", "Robert E. Schapire"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Mukherjee and Schapire.,? \\Q2008\\E", "shortCiteRegEx": "Mukherjee and Schapire.", "year": 2008}, {"title": "A game of prediction witih expert advice", "author": ["V. Vovk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vovk.,? \\Q1998\\E", "shortCiteRegEx": "Vovk.", "year": 1998}, {"title": "How to better use expert advice", "author": ["R. Yaroshinsky", "R. El-Yaniv", "S. Seiden"], "venue": "Machine Learning,", "citeRegEx": "Yaroshinsky et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yaroshinsky et al\\.", "year": 2004}], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we consider the decision-theoretic framework for online learning (DTOL) proposed by Freund and Schapire [FS97]. Previous algorithms for learning in this framework have a tunable learning rate parameter. Tuning the learning rate requires prior knowledge about the sequence and severely limits the practicality of the algorithm. While much progress has been made in the past decade for adaptively tuning the learning rate, all of these methods still ultimately rely on some prior information. We propose a completely parameter-free algorithm for learning in this framework. We show theoretically that our algorithm has a regret bound similar to the best bounds achieved by previous algorithms with optimally-tuned learning rates. We also present a few experiments comparing the performance of the algorithm with that of other algorithms for various tunings.", "creator": "LaTeX with hyperref package"}}}