{"id": "0904.2623", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2009", "title": "Exponential Family Graph Matching and Ranking", "abstract": "We present a simple and efficient approach for learning to rank. It is an instance of a more general method for learning max-weight matching predictors in bipartite graphs, which has applications beyond ranking. The method consists of performing maximum a posteriori estimation in exponential families with suitable sufficient statistics. We apply the method to a standard benchmark dataset for learning web page ranking, obtaining state-of-the-art results.", "histories": [["v1", "Fri, 17 Apr 2009 03:48:02 GMT  (51kb,D)", "https://arxiv.org/abs/0904.2623v1", null], ["v2", "Fri, 5 Jun 2009 03:54:58 GMT  (459kb,D)", "http://arxiv.org/abs/0904.2623v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["james petterson", "tib\u00e9rio s caetano", "julian john mcauley", "jin yu"], "accepted": true, "id": "0904.2623"}, "pdf": {"name": "0904.2623.pdf", "metadata": {"source": "CRF", "title": "Exponential Family Graph Matching and Ranking", "authors": ["James Petterson", "Tib\u00e9rio Caetano", "Julian McAuley", "Jin Yu"], "emails": ["first.last@nicta.com.au"], "sections": [{"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a region and in which it is a country, in which it is a region and in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Structured Prediction", "text": "In recent years, much attention has been paid in machine learning to so-called structured predictors, which are predictors of kindg\u03b8: X 7 \u2192 Y, (1) where X is an arbitrary input space and Y is an arbitrary discrete space, which is typically exponentially large. Y can, for example, be a space of matrices, trees, diagrams, sequences, strings, matches, etc. This structured nature of Y is what structured prediction refers to. Within the framework of this paper, X is the set of vector-weighted bipartite graphics (i.e., each edge has a feature vector associated with it), and Y is the set of perfect matches available through X. IfN graphs, together with corresponding annotated matches (i.e., a series {xn, yn)} Nn = 1, our task will be to estimate forecasters when we apply the new predictor to a predictor."}, {"heading": "2.2 The Matching Problem", "text": "We assume that a bipartite graph with m nodes in each part, G = (V, E, w) is available to us (where V is the set of wells, E is the set of edges, and w: E 7 \u2192 R is a set of real weights associated with the edges. G can be simply represented by a matrix (wij) to reach the graph G = (V, E, w). See Figure 1 for illustration. Gx Gi j i j i j i j xij = < xij > Figure 1. Left: Illustration of an input vector weighted bi-partite graph Gx with 3 \u00d7 3 edges. There is a vector xeassociated to each edge e (for clarity only xij is shown, corresponding to the solid edge). Right: weighted bipar-tite graph G is obtained by evaluating Gx on the nedvector."}, {"heading": "3 The Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Basic Goal", "text": "In this paper, we assume that the weights wij must be estimated from the training data. Specifically, the weight wij associated with edge ij in a diagram will be the result of a suitable composition of a characteristic vector xij (observed) and a parameter vector \u03b8 (estimated from training data). Therefore, our input in practice is a vector-weighted split graph Gx = (V, E, x) (x: E 7 \u2192 Rn, which is \"evaluated\" at a given time (obtained from previous training) in order to obtain the diagram G = (V, E, w). See Figure 1 for illustration. Formally, we assume that a training set {X, = {(xn, yn)} Nn = 1 is available, with xn: = (xn11, x n 12.), x n M (n) M (n) (n))))."}, {"heading": "3.2 Exponential Family Model", "text": "We proceed from an exponential family model in which the probability model becomes isp (y; x; \u03b8) = exp (< \u03c6 (x, y), \u03b8 > \u2212 g (x; \u03b8)), where (3) g (x; \u03b8) = log \u2211 yexp < \u03c6 (x, y), \u03b8 > (4) is the log partitioning function, which is a convex and differentiable function of \u03b8 [35]. The prediction in this model amounts to maximizing the conditional probability of education {X, Y}, i.e., the calculation of argmax y (y | x; \u03b8) = argmax y < \u03c6 (x, y) - and ML estimation amounts to maximizing the conditional probability of education {X, Y}, i.e., the calculation of argmaxyn p (Y | x; lt.)."}, {"heading": "3.3 Feature Parameterization", "text": "The critical observation is now that we equate the solution of the matching problem (2) with the prediction of the exponential family model (5), i.e. that we parameterise the characteristics of individual node pairs (to generate the weight of an edge).Since our goal is to parameterise the characteristics of individual node pairs (to generate the weight of an edge), the most natural model in both x and \u03b8 is linear (see Figure 1, right).The specific shape of xij is discussed in the experimental section. In the light of (10), (2) clearly predicting the best match for Gx in the model now means predicting the best match for Gx."}, {"heading": "4 Learning the Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Basics", "text": "It is a standard result of exponential families that the gradient of the log partition function is the expectation of a sufficient statistic. (11) In order to calculate the gradient of the log partition function, we must therefore have the expectation of the sufficient statistic. (If we open the above expression, we obtain the Ey task p (y) = Ey-p (y) = Ey-p (y; x)] = Ey-p (x, y)]. (11) To perform the gradient descent, we must calculate the above expectation. (If we open the above expression, we obtain Ey-p (y) = Ey-p (y; x) = Ey-p (y, y, y)."}, {"heading": "4.2 Exact Expectation", "text": "The exact partition function itself can be efficiently calculated up to about M = 30 using Ryser's O (M2M) algorithm. [29] For arbitrary expectations, however, we do not know of an exact algorithm that is more efficient than a full enumeration (which would limit traceability to very small graphs), but we will find that even for very small graphs we find a very important application: learning ranking. In our experiments, we successfully apply a tractable instance of our model to compare web page ranking records, achieving very competitive results. For larger graphs, we have alternative options, as shown below."}, {"heading": "4.3 Approximate Expectation", "text": "The best solution we know of is one by Huber and Law, who recently introduced an algorithm to approximate the durability of dense non-negative matrices [13]. The algorithm works by generating exact samples from the distribution of perfect matrices on weighted bipartite graphs, in exactly the same form as the distribution we have here, p (y | x; \u03b8) [13]. We can use this algorithm for applications involving larger graphs.1 We generate K samples from the distribution p (y | x; \u03b8) and approach this algorithm directly (12) with a Monte Carlo estimator Ey \u0445 p (y | x; TB) [\u03c6 (x, y)] -1 KK-i = 1\u0445 (x, yi). (15) In our experiments, we apply this algorithm to a visual application."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Ranking", "text": "It is a fundamental problem with different application areas, such as the collection of data relating to the query qk,.. the evaluation or relevance of each document is usually a nominal value in the list {1,., R}, where R is usually between 2 and 5. We are also able to measure for each retrieved document the relevance of each document in relation to the query qk., to find for each retrieved document a common feature for this document and the query qk.Training."}, {"heading": "5.2 Image Matching", "text": "For our computer vision application, we used a scissor section image from the 2D database Mythological Creatures 4. We randomly selected 20 points on the silhouette as our points of interest and placed scissors on the image, resulting in 200 different images. We then randomly selected N image pairs for training, N for validation, and 500 for the test, and trained our model to match the points of interest in the pairs. In this setup, we used the scanning method described in Section 4.3, where | \u00b7 | marks the elementary difference, and then the shape context feature vector [1] for point i.For a graph of this size, calculating the exact expectation is not feasible, so we again chose the scanning method described in Section 4.3. Once again, the regulation constant \u03bb was chosen by cross-validation. Given that the MAP estimator is consistent with the MAP, while the Max Margin is not a consistent one, the estimation of both is not a consistent one, but an attempt at the MAP is a high one."}, {"heading": "6 Conclusion and Discussion", "text": "We have presented a method with which we can learn how the results can be obtained. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p\" p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p."}, {"heading": "Appendix A", "text": "The algorithm is an accept-reject algorithm. The basic idea of such an algorithm is very simple: suppose that we have to try out a distribution p in a certain domain M, but that such a task is insoluble. Instead, we take samples from a distribution q in a superset N of the original domain (where sampling is easier), whose limitation to the original domain matches the original distribution: q | N = P. We then only accept \"those samples that effectively fall into the original domain M. Clearly, the efficiency of such a procedure is dictated by (i) how efficient it is to try from q to N and (ii) how much mass of q is spoken in M. Grob, the algorithm manages the perfect match of two-part graphs so that both conditions (i) are favorable.The reasoning follows: i consists of agreement with Y = i."}], "references": [{"title": "Matching with shape contexts. CBAIVL00", "author": ["S. Belongie", "J Malik"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Shape matching and object recogniti n sing shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE Trans. on PAMI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Learning graph matching", "author": ["T.S. Caetano", "L. Cheng", "Q.V. Le", "A.J. Smola"], "venue": "IEEE Trans. on PAMI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Z. Cao", "T. Qin", "Liu", "T.-Y", "Tsai", "M.-F", "H. Li"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Frank: A ranking method with fidelity", "author": ["M. Tsai", "T. Liu", "T. Qin", "H. Chen", "W. Ma"], "venue": "loss. SIGIR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iy r", "R.E. S apir", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Multicategory \u03c8-learning and support vector machine", "author": ["Y. Liu", "X. Shen"], "venue": "Computational tools. J. Computational and Graphical Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Approximate inference using planar graph decomposition", "author": ["A. Globerson", "T. Jaakkola"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["A. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "In Advances in Large Margin Classifiers", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Loopy belief propagation for bipartite maximum weight b-matching. AISTATS", "author": ["B. Huang", "T. Jebara"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Structured ranking learning using cumulative distribution networks", "author": ["J.C. Huang", "B.J. Frey"], "venue": "In NIPS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Fast approximation of the permanent for very dense problems", "author": ["M. Huber", "J. Law"], "venue": "SODA. 7,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "B-matching for spectral clustering. ECML", "author": ["T. Jebara", "V. Shchogolev"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. Jarvelin", "J. Kekalainen"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "A shortest augmenting path algorithm for dense and sparse linear assignment problems", "author": ["R. Jonker", "A. Volgenant"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Structured prediction models via the matrix-tree", "author": ["T. Koo", "A. Globerson", "X. Carreras", "M. Collins"], "venue": "theorem. EMNLP", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Conditional random fields: Probabilistic modeling for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F. Pereira"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Direct optimization of ranking measures. http://arxiv.org/abs/0704.3359", "author": ["Q. Le", "A. Smola"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Letor: Benchmark dataset for research on learning to rank for information", "author": ["Liu", "T.-Y", "J. Xu", "T. Qin", "W. Xiong", "H. Li"], "venue": "retrieval. LR4IR", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Generalization bounds and consistency for structured labeling", "author": ["D. McAllester"], "venue": "Predicting Structured Data", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Selection bias in the letor datasets. LR4IR", "author": ["T. Minka", "S. Robertson"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Combinatorial optimization: Algorithms and complexity", "author": ["C.H. Papadimitriou", "K. Steiglitz"], "venue": "New Jersey: Prentice-Hall", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1982}, {"title": "Global ranking using continuous conditional random fields", "author": ["T. Qin", "Liu", "T.-Y", "Zhang", "X.-D", "Wang", "D.-S", "H. Li"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Sortnet: Learning to rank by a neural-based sorting algorithm. LR4IR", "author": ["L. Rigutini", "T. Papini", "M. Maggini", "F. Scarselli"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "On a Theorem of Hardy, Littlewood, Polya, and Blackwell", "author": ["S. Sherman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1951}, {"title": "Learning structured prediction models: a large-margin approach", "author": ["B. Taskar"], "venue": "Doctoral dissertation, Stanford University", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "The complexity of computing the permanent", "author": ["L.G. Valiant"], "venue": "Theor. Comput. Sci. (pp. 189\u2013201)", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1979}, {"title": "Graphical models, exponential families, and variational inference (Technical Report 649)", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "UC Berkeley, Department of Statistics", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Adarank: a boosting algorithm for information", "author": ["J. Xu", "H. Li"], "venue": "retrieval. SIGIR", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Query-level learning to rank using isotonic regression. LR4IR", "author": ["Z. Zheng", "H. Zha", "G. Sun"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}], "referenceMentions": [{"referenceID": 21, "context": "The Maximum-Weight Bipartite Matching Problem (henceforth \u2018matching problem\u2019) is a fundamental problem in combinatorial optimization [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "For example, in computer vision the crucial problem of finding a correspondence between sets of image features is often modeled as a matching problem [2, 3].", "startOffset": 150, "endOffset": 156}, {"referenceID": 2, "context": "For example, in computer vision the crucial problem of finding a correspondence between sets of image features is often modeled as a matching problem [2, 3].", "startOffset": 150, "endOffset": 156}, {"referenceID": 17, "context": "Ranking algorithms can be based on a matching framework [19], as can clustering algorithms [14, 11].", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Ranking algorithms can be based on a matching framework [19], as can clustering algorithms [14, 11].", "startOffset": 91, "endOffset": 99}, {"referenceID": 9, "context": "Ranking algorithms can be based on a matching framework [19], as can clustering algorithms [14, 11].", "startOffset": 91, "endOffset": 99}, {"referenceID": 25, "context": "This idea of \u2018parameterizing algorithms\u2019 and then optimizing for agreement with data is called structured estimation [31, 33].", "startOffset": 117, "endOffset": 125}, {"referenceID": 26, "context": "This idea of \u2018parameterizing algorithms\u2019 and then optimizing for agreement with data is called structured estimation [31, 33].", "startOffset": 117, "endOffset": 125}, {"referenceID": 25, "context": "[31] and [3] describe max-margin structured estimation formalisms for this problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[31] and [3] describe max-margin structured estimation formalisms for this problem.", "startOffset": 9, "endOffset": 12}, {"referenceID": 26, "context": "Max-margin estimators instead minimize a surrogate loss which is easier to optimize, namely a convex upper bound on the structured loss [33].", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "In practice the results are often good, but known convex relaxations produce estimators which are statistically inconsistent [22], i.", "startOffset": 125, "endOffset": 129}, {"referenceID": 6, "context": "The inconsistency of multiclass support vector machines is a well-known issue in the literature that has received careful examination recently [8, 7].", "startOffset": 143, "endOffset": 149}, {"referenceID": 17, "context": "We then compare the performance of our model instance against a large number of state-of-theart ranking methods, including DORM [19], an approach that only differs to our model instance by using max-margin instead of a MAP formulation.", "startOffset": 128, "endOffset": 132}, {"referenceID": 2, "context": "However the fastest suitable sampler is still quite slow for large models, in which case max-margin matching estimators like those of [3] and [31] are likely to be preferable even in spite of their potential inferior accuracy.", "startOffset": 134, "endOffset": 137}, {"referenceID": 25, "context": "However the fastest suitable sampler is still quite slow for large models, in which case max-margin matching estimators like those of [3] and [31] are likely to be preferable even in spite of their potential inferior accuracy.", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": "One is based on max-margin estimators [33, 32, 31], and the other on maximumlikelihood (ML) or MAP estimators in exponential family models [18].", "startOffset": 38, "endOffset": 50}, {"referenceID": 25, "context": "One is based on max-margin estimators [33, 32, 31], and the other on maximumlikelihood (ML) or MAP estimators in exponential family models [18].", "startOffset": 38, "endOffset": 50}, {"referenceID": 16, "context": "One is based on max-margin estimators [33, 32, 31], and the other on maximumlikelihood (ML) or MAP estimators in exponential family models [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 19, "context": "However the resulting estimators are known to be inconsistent in general: in the limit of infinite training data the algorithm fails to recover the best model in the model class [22, 7, 8].", "startOffset": 178, "endOffset": 188}, {"referenceID": 6, "context": "However the resulting estimators are known to be inconsistent in general: in the limit of infinite training data the algorithm fails to recover the best model in the model class [22, 7, 8].", "startOffset": 178, "endOffset": 188}, {"referenceID": 19, "context": "McAllester recently provided an interesting analysis on this issue, where he proposed new upper bounds whose minimization results in consistent estimators, but no such bounds are convex [22].", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "The other approach uses ML or MAP estimation in conditional exponential families with \u2018structured\u2019 sufficient statistics, such as in probabilistic graphical models, where they are decomposed over the cliques of the graph (in which case they are called Conditional Random Fields, or CRFs [18]).", "startOffset": 287, "endOffset": 291}, {"referenceID": 7, "context": "Other tractable models of this type include models that predict spanning trees and models that predict binary labelings in planar graphs [9, 17].", "startOffset": 137, "endOffset": 144}, {"referenceID": 15, "context": "Other tractable models of this type include models that predict spanning trees and models that predict binary labelings in planar graphs [9, 17].", "startOffset": 137, "endOffset": 144}, {"referenceID": 14, "context": "This is a well-studied problem; it is tractable and can be solved inO(m) tim [16, 26].", "startOffset": 77, "endOffset": 85}, {"referenceID": 21, "context": "This is a well-studied problem; it is tractable and can be solved inO(m) tim [16, 26].", "startOffset": 77, "endOffset": 85}, {"referenceID": 2, "context": "This model can be used to match features in images [3], improve classification algorithms [11] and rank webpages [19], to cite a few applications.", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "This model can be used to match features in images [3], improve classification algorithms [11] and rank webpages [19], to cite a few applications.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "This model can be used to match features in images [3], improve classification algorithms [11] and rank webpages [19], to cite a few applications.", "startOffset": 113, "endOffset": 117}, {"referenceID": 28, "context": "is the log-partition function, which is a convex and differentiable function of \u03b8 [35].", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "`(Y |X; \u03b8) is a convex function of \u03b8 since the logpartition function g(\u03b8) is a convex function of \u03b8 [35] and the other terms are clearly convex in \u03b8.", "startOffset": 100, "endOffset": 104}, {"referenceID": 28, "context": "`(Y |X; \u03b8) is a convex and differentiable function of \u03b8 [35], therefore gradient descent will find the global optimum.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "However, unlike the determinant, which is computable efficiently and exactly by standard linear algebra manipulations [17], computing the permanent is a ]P-complete problem [34].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "However, unlike the determinant, which is computable efficiently and exactly by standard linear algebra manipulations [17], computing the permanent is a ]P-complete problem [34].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "The best solution we are aware of is one by Huber and Law, who recently presented an algorithm to approximate the permanent of dense non-negative matrices [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "This is in precisely the same form as the distribution we have here, p(y|x; \u03b8) [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Here we follow the construction used in [19] to map matching problems to ranking problems (indeed the only difference between our ranking model and that of [19] is that they use a max-margin estimator and we use MAP in an exponential family.", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "Here we follow the construction used in [19] to map matching problems to ranking problems (indeed the only difference between our ranking model and that of [19] is that they use a max-margin estimator and we use MAP in an exponential family.", "startOffset": 156, "endOffset": 160}, {"referenceID": 18, "context": "0 [20], a publicly available benchmark data collection for comparing learning to rank algorithms.", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "For each query there are a number of associated documents, with relevance degrees judged by humans on three levels: definitely, possibly or not 2If r(v) denotes the vector of ranks of entries of vector v, then \u3008a, \u03c0(b)\u3009 is maximized by the permutation \u03c0\u2217 such that r(a) = r(\u03c0\u2217(b)), a theorem due to Polya, Littlewood, Hardy and Blackwell [30].", "startOffset": 338, "endOffset": 342}, {"referenceID": 18, "context": "See [20] for more details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "Evaluation Metrics In order to measure the effectiveness of our method we use the normalized discount cumulative gain (NDCG) measure [15] at rank position k, which is defined as", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 232, "endOffset": 236}, {"referenceID": 4, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 244, "endOffset": 247}, {"referenceID": 3, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 257, "endOffset": 260}, {"referenceID": 29, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 270, "endOffset": 274}, {"referenceID": 30, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 297, "endOffset": 301}, {"referenceID": 23, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 311, "endOffset": 315}, {"referenceID": 10, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 328, "endOffset": 332}, {"referenceID": 22, "context": "In Figure 2 we plot the results of our method (named RankMatch), for M = R, compared to those achieved by a number of state-of-the-art methods which have published NDCG scores in at least two of the datasets: RankBoost [6], RankSVM [10], FRank [5], ListNet [4], AdaRank [36], QBRank [38], IsoRank [37], SortNet [28], StructRank [12] and C-CRF [27].", "startOffset": 343, "endOffset": 347}, {"referenceID": 17, "context": "We also included a plot of our implementation of DORM [19], using precisely the same resampling methodology and data for a fair comparison.", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "These results should be interpreted cautiously; [24] presents an interesting discussion about issues with these datasets.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "This is certainly the benefit of the max-margin matching formulations of [3, 19]: it is much faster for large graphs.", "startOffset": 73, "endOffset": 80}, {"referenceID": 17, "context": "This is certainly the benefit of the max-margin matching formulations of [3, 19]: it is much faster for large graphs.", "startOffset": 73, "endOffset": 80}, {"referenceID": 11, "context": "Runtimes for M = 3, 4, 5 are from the ranking experiments, computed by full enumeration; M = 20 corresponds to the image matching experiments, which use the sampler from [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "where |\u00b7| denotes the elementwise difference and \u03c8i is the Shape Context feature vector [1] for point i.", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "The max-margin method is that of [3].", "startOffset": 33, "endOffset": 36}, {"referenceID": 19, "context": "0 [22], a publicly available benchmark data collection for comparing learning to rank algorithms.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "See [22] for more detail.", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "Evaluation Metrics In order to measure the effectiveness of our method we use the normalized discount cumulative gain (NDCG) measure [16] at rank position k, which is defined as", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "If r(v) denotes the vector of ranks of entries of vector v, then \u3008a, \u03c0(b)\u3009 is maximized by the permutation \u03c0\u2217 such that r(a) = r(\u03c0\u2217(b)), a theorem due to Polya, Littlewood, Hardy and Blackwell [31].", "startOffset": 193, "endOffset": 197}], "year": 2009, "abstractText": "We present a method for learning max-weight matching predictors in bipartite graphs. The method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features. Although inference is in general hard, we show that for one very relevant application\u2013web page ranking\u2013exact inference is efficient. For general model instances, an appropriate sampler is readily available. Contrary to existing maxmargin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models. We apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning web page ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants. The drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is comparatively high.", "creator": "LaTeX with hyperref package"}}}