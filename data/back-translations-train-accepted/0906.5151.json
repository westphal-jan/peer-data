{"id": "0906.5151", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2009", "title": "Unsupervised search-based structured prediction", "abstract": "We describe an adaptation and application of a search-based structured prediction algorithm \"Searn\" to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality unsupervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the predict-self idea for unsupervised learning.", "histories": [["v1", "Sun, 28 Jun 2009 17:47:22 GMT  (66kb,D)", "http://arxiv.org/abs/0906.5151v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hal daum\u00e9 iii"], "accepted": true, "id": "0906.5151"}, "pdf": {"name": "0906.5151.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Search-based Structured Prediction", "authors": ["Hal Daum\u00e9 III"], "emails": ["me@hal3.name"], "sections": [{"heading": "1. Introduction", "text": "A widely used and useful version of unattended learning occurs when both the observed data and the latent variables are structured, ranging from hidden alignment variables in speech recognition (Rabiner, 1989) and machine translation (Brown et al., 1993; Vogel et al., 1996) to latent trees in unattended parsing (Paskin, 2001; Klein & Manning, 2004; Smith & Eisner, 2005; Titov & Henderson, 2007) and estimation in computer monitoring (Ramanan et al., 2005). These techniques are all based on probability models. Their applicability depends on the traceability of latent variable expectations, enabling the use of EM (Dempster et al., 1977). In this paper we show that a recently developed search-based algorithm, Searn (thumb \u0301 III et al., 2009), can represent an erroneous prediction (Section et al., 1977)."}, {"heading": "2. Structured Prediction", "text": "The supervised structured prediction problem is the task of mapping input x to complex structured outputs y (e.g. sequences, trees, etc.). Formally, X should be an arbitrary input space and Y a structural output space. Y is typically defined in such a way that it decomposes over some smaller substructures (e.g. labels in a sequence). Y is equipped with a loss function, which is often assumed to take the form of a hamming loss over the substructures. Properties are defined by pairs (x, y) so that they obey the substructures (e.g. one could have characteristics over adjacent label pairs in a sequence). Under strong assumptions about the structures, the loss function, and the characteristics (essentially \"locality assumptions\"), a number of learning algorithms can be used: for example, conditional random fields (Lafferty et al., 2001) or max margin networks can be found either in this structured or in the destructured space (e.g. a taskbar)."}, {"heading": "2.1. Search-based Structured Prediction", "text": "A recently proposed algorithm for solving the structured prediction problem is Searn (thumb \u0301 III et al., 2009 appears). Searn works by taking into account each sub-structure prediction y1,..., yT as a classification problem. A classifier h is trained so that at the time t, ar Xiv: 090 6,51 51v1 [cs.LG] 2 8Ju n20 09 can predict the best value for yt with a feature vector. The feature vector can make yt on any part of the input x and any previous decision y1,..., yt \u2212 1. This leads to a hen-and-egg problem. h should ideally be trained to make the best decision for yt, since h makes all previous decisions y1,..., yt \u2212 1 and all future decisions yt + 1,..., yT. Of course, we do not have access to h at the training point (we are trying to construct it)."}, {"heading": "2.2. Searn", "text": "The representation we give here differs slightly from the original representation of the Searn algorithm in a distribution problem. Our motivation for deviating from the original formulation is because our representation makes the connection between our unsupervised variant of Searn and other standardized, unsupervised learning methods (such as standard algorithms on hidden Markov models) clearer."}, {"heading": "3. Unsupervised Searn", "text": "In an unattended structured prediction, we no longer get a pair (x, y), but observe only an input x instead. Our task is to construct a classifier that produces y even though we have never observed it."}, {"heading": "3.1. Reduction for Unsupervised to Supervised", "text": "This is exactly the intuition we build into our model. Moreover, the observation that makes this practical is that there is nothing in the theory or application of Searn that says \u03c0 cannot be corruptible. Furthermore, there is no requirement that the loss function depends on all components of the prediction. Our model will essentially first predict y and then predict x based on y. Importantly, the loss function for y is agnostic (since we have no real outputs). The general construction is as follows. Let Dunsup be a distribution over inputs x x and let Y be the space of the desired latent structures (e.g. trees). We define a distribution dsup over X \u00d7 (Y \u00d7 X) by defining a sampling method."}, {"heading": "3.2. Sequence Labeling Example", "text": "As a matter of fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "4. Comparison to EM", "text": "In this section, we show an equivalence between expectation maximization in directed probability structures and unattended searching. As a motivating example, we use a mixture of multinomials (primarily for simplicity), but the results can easily be transferred to more complex models (e.g. HMMs: see Section 4.3)."}, {"heading": "4.1. EM for Mixture of Multinomials", "text": "In the mixture of multinomial problems, we get the N documents d1,.., dN, where dn is a vector of the number of words over a vocabulary of size V; i.e., dn, v is the number of times the word v in document n. The mixture of multinomial numbers is a probable cluster model in which we assume an underlying set of K clusters (multinomial numbers) from which the documents emerged. Specify the multinomial parameter associated with cluster k, based on which the previous probability of choosing cluster k exists, and let zn be an indicator vector associating document n with the unique cluster k, so that zn, k = 1. The probability model has the form: p (d) = n (vdn, v)!"}, {"heading": "4.2. An Equivalent Model in Searn", "text": "Now we show how to construct an instance of unattended searches that effectively mimics the behavior of q examples of EM behavior based on the mixture of multinomials. Ingredients are as follows: \u2022 The input space X is the space of documents represented as word number vectors. \u2022 The (latent) output space Y is a single discrete variable in the range [1, K] that specifies the cluster. \u2022 The functionality for predicting y (document counts). \u2022 The functionality for predicting x is the designation y and the total number of words in the document. Predictions for a document are estimated word probabilities, not the words themselves. \u2022 The loss function ignores the prediction y and reflects the protocol loss of the true document x under the word probabilities. \u2022 The cost-sensitive learning algorithm differs depending on whether the latent structure y is predicted or whether the document x is predicted: Structure - Basic document is a multinational."}, {"heading": "4.3. Synthetic experiments", "text": "To demonstrate the benefits of the universality of Searn, we report here the results of some experiments with synthetic data. We generate synthetic data using two different HMMs. The first HMM is a first order model; the initial probabilities, transition probabilities and observation probabilities are all drawn uniformly; the second HMM is a second order model that has also drawn all probabilities uniformly; the lengths of the observations are given by a poisson with a specified meaning. In our experiments, we consider the following learning algorithms: EM, Searn with HMM characteristics and a subordinated Bayes classifier, and Searn with a logistic regression classifier (and an extended feature space: the prediction depends on xt \u2212 1: t + 1. The first Searn should mimic the EM data, but using sampling instead of exact expectation commas the models are all the same."}, {"heading": "5. Analysis", "text": "The first key is that the characteristics of the Y component of the output space are descriptive enough that they be1We have conducted experiments that vary the number of samples that Searn uses in {1, 2, 5}; however, there was no statistically significant difference. The results we report are based on 2 samples. A mindset of this limitation is that if we had labeled data, we could learn well. The second key is that the characteristics of the X component of the output space are inextricably linked to the hidden component. Ideally, these characteristics will be such that X can be predicted with high precision if and only if Y is accurately predicted. The general, though very trivial result is that we can guarantee that the loss of the Y component of the output space is limited by a function f of the loss to X, then the loss to Y after learning guarantees that the vitamin dependent case is limited - although the result is very trivial - to the exercise."}, {"heading": "6. Unsupervised Dependency Parsing", "text": "Dependency formalism is a practical and linguistically interesting model of syntactic structure: one can imagine a dependency structure for a sentence with the length T as a guided tree over a graph with T + 1 nodes: one node for each word plus a unique root node. Edges point from head to dependency. An example of a dependency structure for a sentence with the length T = 7 is in Figure 2. Until now, unattended parsing of dependencies has only been considered in the context of global probability models specified by pairs of dependencies (Paskin, 2001) or spanning trees (Klein & Manning, 2004; Smith & Eisner, 2005)."}, {"heading": "6.1. Shift-reduce dependency parsing", "text": "The algorithm starts with < S, i, A > = < \u2205, 1, \u2205 >: the stack and the arcset are empty and the current index is 1 (the first word), the algorithm then goes through a series of actions until a final state is reached. A final state is one in which i = < S at which set A contains all dependency borders for the parse. Denote by i | I a stack with i at the head and stack I at the tail. There are four actions: LeftArc: < t | S, i, A > \u2212 < S, i, (i, t) the next arc is not valid until there is an arc."}, {"heading": "6.2. Experimental setup", "text": "We follow the same experimental setup as (Smith & Eisner, 2005) by using data from the WSJ10 corpus (sets of no more than ten from Penn Treebank (Marcus et al., 1993)), with no punctuation or analysis based on the parts of the language names, not the words. We use the same move / developer / test split as Smith and Eisner: 5301 sets of training data, 531 sets of development data and 530 sets of blind test data. All algorithm development and tuning was based on the development data. We use a minor modification of SearnShell to develop our algorithm along with a multi-label logistic regression classifier, MegaM.2 Our algorithm uses the following features for tree-based decisions (inspired by (Hall et al., 2006), where t is the top of the stack and i is the next token: the parts of speech within a window of a 2 parent or the pair between the pair (Hall et al)."}, {"heading": "6.3. Experimental results", "text": "The baseline systems are: two random baselines (one generative, one given by the Searn initial policy), Klein and Manning's model (Klein & Manning, 2004) EM-based model (with and without smart initialization), and three variants of Smith and Eisner's model (Smith & Eisner, 2005) (with random initialization, which seems to be better for most of their mod-2SearnShell and MegaM, are available at http: / / searn. hal3.name and http: / hal3.name / megam, resp.: els) We also report an \"upper limit\" performance based on supervised training, both for probabilistic (Smith + Eisner model) and supervised sears.The results are reported in Table 2: Accuracy on the training data, Accuracy on the test data, and the number of iterations required, all averaged over 10 runs; standard deviations are included in the fine copy of the results (the Searn results and not the Smith results)."}, {"heading": "7. A Semi-Supervised Version", "text": "The unattended learning algorithm described above, of course, also extends to the case where some of the described data is available. In fact, the only change to the algorithm is the change in the loss function. In the unattended case, the loss function completely ignores the latent structure and delivers a loss that depends only on the task of \"predicting oneself.\" In the semi-monitored version, a natural loss function is turned on for the \"latent\" structure prediction for the labeled subset of data. In Figure 3, we present the results of the dependency analysis. We show learning curves for unattended, fully monitored and semi-monitored models. The x-axis shows the number of examples used; in the unattended and monitored cases, this is the total number of examples; in the half-monitored case, it is the number of examples labeled. Error bars are two standard deviations. Somewhat is surprising, the half-monitored approach achieves an accuracy of more than 70%, while the half-monitored examples are only about 5,000%."}, {"heading": "8. Conclusions", "text": "We have described the application of a search-based structured prediction algorithm, Searn, to unattended learning, which positively answers an open question in the field of learning reductions (Beygelzimer et al., 2005): Can unattended learning be reduced to unattended learning? We have shown a near-equivalence between the resulting algorithm and the forward-backward algorithm in hidden Markov models. We have shown that applying this algorithm to unattended dependency saving in a layer reduction framework is possible. An obvious extension of this work are structured prediction problems with additional latent structure, such as in machine translation. Instead of using the prediction self-methodology, one could apply a prediction targeting methodology in which \"prediction algorithms\" are applied to unattended prediction problems with additional latent structure, such as in machine translation."}], "references": [{"title": "Error limiting reductions between classification", "author": ["A. Beygelzimer", "V. Dani", "T. Hayes", "J. Langford", "B. Zadrozny"], "venue": "tasks. Proc. Int\u2019l Conf. on Machine Learning (pp. 49\u201356)", "citeRegEx": "Beygelzimer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2005}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P. Brown", "S. Della Pietra", "V. Della Pietra", "R. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "2009 (to appear)). Search-based structured prediction. Machine Learning J", "author": ["H. Daum\u00e9 III", "J. Langford", "D. Marcu"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "J. of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Discriminative classifiers for determining dependency parsing", "author": ["J. Hall", "J. Nivre", "J. Nilsson"], "venue": "Proc. Conf. of the Assoc. for Computational Linguistics (pp. 316\u2013323)", "citeRegEx": "Hall et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2006}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["G. Hinton", "P. Dayan", "B. Frey", "R. Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "Corpus-based induction of syntactic structure: Models of depen", "author": ["D. Klein", "C. Manning"], "venue": null, "citeRegEx": "Klein and Manning,? \\Q2004\\E", "shortCiteRegEx": "Klein and Manning", "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proc. Int\u2019l Conf. on Machine Learning (pp. 282\u2013289)", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Analyzing the errors of unsupervised learning", "author": ["P. Liang", "D. Klein"], "venue": null, "citeRegEx": "Liang and Klein,? \\Q2008\\E", "shortCiteRegEx": "Liang and Klein", "year": 2008}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "On the complexity of non-projective data-driven dependency parsing", "author": ["R. McDonald", "G. Satta"], "venue": "Int\u2019l Wk. on Parsing Technologies", "citeRegEx": "McDonald and Satta,? \\Q2007\\E", "shortCiteRegEx": "McDonald and Satta", "year": 2007}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["A. Ng", "M. Jordan"], "venue": "Proc. Converence on Uncertainty in Artificial Intelligence (pp. 406\u2013415)", "citeRegEx": "Ng and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2000}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["J. Nivre"], "venue": "Int\u2019l Wk. on Parsing Technologies", "citeRegEx": "Nivre,? \\Q2003\\E", "shortCiteRegEx": "Nivre", "year": 2003}, {"title": "Grammatical bigrams", "author": ["M.A. Paskin"], "venue": "Advances in Neural Info. Processing Systems (pp", "citeRegEx": "Paskin,? \\Q2001\\E", "shortCiteRegEx": "Paskin", "year": 2001}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": null, "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Strike a pose: Tracking people by finding stylized poses. Computer Vision and Pattern Recognition", "author": ["D. Ramanan", "D. Forsyth", "A. Zisserman"], "venue": null, "citeRegEx": "Ramanan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ramanan et al\\.", "year": 2005}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["K. Sagae", "A. Lavie"], "venue": "Int\u2019l Wk. on Parsing Technologies", "citeRegEx": "Sagae and Lavie,? \\Q2005\\E", "shortCiteRegEx": "Sagae and Lavie", "year": 2005}, {"title": "Guiding unsupervised grammar induction using contrastive estimation", "author": ["N.A. Smith", "J. Eisner"], "venue": "IJCAI Wk. on Grammatical Inference Apps (pp. 73\u201382)", "citeRegEx": "Smith and Eisner,? \\Q2005\\E", "shortCiteRegEx": "Smith and Eisner", "year": 2005}, {"title": "Learning structured prediction models: A large margin approach", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "Proc. Int\u2019l Conf. on Machine Learning (pp. 897\u2013904)", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "A latent variable model for generative dependency parsing", "author": ["I. Titov", "J. Henderson"], "venue": "Int\u2019l Conf. on Parsing Technologies", "citeRegEx": "Titov and Henderson,? \\Q2007\\E", "shortCiteRegEx": "Titov and Henderson", "year": 2007}, {"title": "HMM-based word alignment in statistical translation", "author": ["S. Vogel", "H. Ney", "C. Tillmann"], "venue": "Proc. Int\u2019l Conf. on Computational Linguistics (pp. 836\u2013841)", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}], "referenceMentions": [{"referenceID": 14, "context": "Examples range from hidden alignment variables in speech recognition (Rabiner, 1989) and machine translation (Brown et al.", "startOffset": 69, "endOffset": 84}, {"referenceID": 1, "context": "Examples range from hidden alignment variables in speech recognition (Rabiner, 1989) and machine translation (Brown et al., 1993; Vogel et al., 1996), to latent trees in unsupervised parsing (Paskin, 2001; Klein & Manning, 2004; Smith & Eisner, 2005; Titov & Henderson, 2007), and to pose estimation in computer vision (Ramanan et al.", "startOffset": 109, "endOffset": 149}, {"referenceID": 20, "context": "Examples range from hidden alignment variables in speech recognition (Rabiner, 1989) and machine translation (Brown et al., 1993; Vogel et al., 1996), to latent trees in unsupervised parsing (Paskin, 2001; Klein & Manning, 2004; Smith & Eisner, 2005; Titov & Henderson, 2007), and to pose estimation in computer vision (Ramanan et al.", "startOffset": 109, "endOffset": 149}, {"referenceID": 13, "context": ", 1996), to latent trees in unsupervised parsing (Paskin, 2001; Klein & Manning, 2004; Smith & Eisner, 2005; Titov & Henderson, 2007), and to pose estimation in computer vision (Ramanan et al.", "startOffset": 49, "endOffset": 133}, {"referenceID": 15, "context": ", 1996), to latent trees in unsupervised parsing (Paskin, 2001; Klein & Manning, 2004; Smith & Eisner, 2005; Titov & Henderson, 2007), and to pose estimation in computer vision (Ramanan et al., 2005).", "startOffset": 177, "endOffset": 199}, {"referenceID": 3, "context": "Their applicability hinges on the tractability of (approximately) computing latent variable expectations, thus enabling the use of EM (Dempster et al., 1977).", "startOffset": 134, "endOffset": 157}, {"referenceID": 7, "context": "Under strong assumptions on the structures, the loss function and the features (essentially \u201clocality\u201d assumptions), a number of learning algorithms can be employed: for example, conditional random fields (Lafferty et al., 2001) or max-margin Markov networks (Taskar et al.", "startOffset": 205, "endOffset": 228}, {"referenceID": 18, "context": ", 2001) or max-margin Markov networks (Taskar et al., 2005).", "startOffset": 38, "endOffset": 59}, {"referenceID": 0, "context": "The key ingredient in Searn is to use the loss function ` and a \u201ccurrent\u201d policy \u03c0 to turn D into a distribution over cost-sensitive (multiclass) classification problems (Beygelzimer et al., 2005).", "startOffset": 170, "endOffset": 196}, {"referenceID": 13, "context": "global probabilistic models specified over dependency pairs (Paskin, 2001) or spanning trees (Klein & Manning, 2004; Smith & Eisner, 2005).", "startOffset": 60, "endOffset": 74}, {"referenceID": 12, "context": "However, there is an alternative, popular method for producing dependency trees in a supervised setting: shift-reduce parsing (Nivre, 2003; Sagae & Lavie, 2005).", "startOffset": 126, "endOffset": 160}, {"referenceID": 12, "context": "Shift-reduce dependency parsing (Nivre, 2003) is a leftto-right parsing algorithm that operates by maintaining three state variables: a stack S, a current position i and a set of arcs A.", "startOffset": 32, "endOffset": 45}, {"referenceID": 12, "context": "This algorithm is guaranteed to terminate in at most 2T steps with a valid dependency tree (Nivre, 2003), unlike standard probabilistic algorithms that have a time-complexity that is cubic in T (McDonald & Satta, 2007).", "startOffset": 91, "endOffset": 104}, {"referenceID": 9, "context": "We follow the same experimental setup as (Smith & Eisner, 2005), using data from the WSJ10 corpus (sentences of length at most ten from the Penn Treebank (Marcus et al., 1993)).", "startOffset": 154, "endOffset": 175}, {"referenceID": 4, "context": "Our algorithm uses the following features for the tree-based decisions (inspired by (Hall et al., 2006)), where t is the top of the stack and i is the next token: the partsof-speech within a window of 2 around t and i; the pair of tokens at t and i; the distance (discretized) between t and i; and the part-of-speech at the head (resp.", "startOffset": 84, "endOffset": 103}, {"referenceID": 0, "context": "This answers positively an open question in the field of learning reductions (Beygelzimer et al., 2005): can unsupervised learning be reduced to supervised learning? We have shown a nearequivalence between the resulting algorithm and the forward-backward algorithm in hidden Markov models.", "startOffset": 77, "endOffset": 103}, {"referenceID": 5, "context": "This is made most precise in the wake-sleep algorithm (Hinton et al., 1995), which explicitly trains a neural network to reproduce its own input.", "startOffset": 54, "endOffset": 75}], "year": 2009, "abstractText": "We describe an adaptation and application of a search-based structured prediction algorithm \u201cSearn\u201d to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality unsupervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the predict-self idea for unsupervised learning.", "creator": "TeX"}}}