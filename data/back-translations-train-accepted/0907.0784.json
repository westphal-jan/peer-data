{"id": "0907.0784", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2009", "title": "Cross-Task Knowledge-Constrained Self Training", "abstract": "We present an algorithmic framework for learning multiple related tasks. Our framework exploits a form of prior knowledge that relates the output spaces of these tasks. We present PAC learning results that analyze the conditions under which such learning is possible. We present results on learning a shallow parser and named-entity recognition system that exploits our framework, showing consistent improvements over baseline methods.", "histories": [["v1", "Sat, 4 Jul 2009 18:42:01 GMT  (62kb,D)", "http://arxiv.org/abs/0907.0784v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["hal daum\u00e9 iii"], "accepted": true, "id": "0907.0784"}, "pdf": {"name": "0907.0784.pdf", "metadata": {"source": "CRF", "title": "Cross-Task Knowledge-Constrained Self Training", "authors": ["Hal Daum\u00e9 III"], "emails": ["me@hal3.name"], "sections": [{"heading": "1 Introduction", "text": "When two NLP systems run on the same data, we expect certain limitations between their results. This is a form of prior knowledge. We propose a self-training framework that uses such information to significantly increase the performance of one of the systems. The key idea is to perform self-training only on results that obey the limitations.Our motivating example in this paper is the task pair: named entity detection (NNP) and shallow parsing (also known as syntactical chunking). Consider a hidden sentence with known POS and syntactic structure below. Let's continue to consider four potential sequences for this sentence. (POS: NNP NNP VBD TO NN Chunk: [- NP -] [-VP-] [-PP-] [-NP-] NER1: [- Per -] [O - NO - 3 - [we].]"}, {"heading": "2 Background", "text": "Self-training works by learning a model based on a small amount of labeled data. This model is then called evalu-1When we refer to NNP, we also include NNPS. 2The sentence reads: \"George Bush spoke to Congress today\" ar Xiv: 090 7.07 84v1 [cs.LG] July 4th 2ated on a large amount of unlabeled data. His predictions are assumed to be correct, and it is retrained based on the unlabeled data according to his own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. His success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence. Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) refers to self-training by training an algorithm based on its own predictions (Ueffing, 2006)."}, {"heading": "3 Model", "text": "In fact, we are in a position to assert ourselves, we are in a position, we are in a position, we are in a position, and we are not in a position, we are in a position, we are in a position, we are in a position, we are in a position, we are in a position, we are in a position, we are in a position, we are in a position, we are in a position, we are in a position, we are in a position."}, {"heading": "3.1 One-sided Learning with Hints", "text": "Let's start by looking at a simplified version of the \"learning with clues\" problem. Suppose all we care about is learning f2. We have a small amount of data labeled with f2 (let's call this Dunlab \"unlab\" because f2 is not labeled).RUNNING EXAMPLEIn our example, this means we have a small amount of labeled NER data and a large amount of labeled POS / chunk data. We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and Section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS / chunk data (8936 sentences). We are only interested in learning to do this."}, {"heading": "3.2 Two-sided Learning with Hints", "text": "In the two-page version, we assume that we have a small amount of data labeled with f1 (let's call this D1), a small amount of data labeled with f2 (let's call this D2), and a large amount of unlabeled data (let's call this Dunlab). The algorithm we suggest for learning hypotheses for both tasks is below: 1: Learn h1 on D1 and h2 on D2. 2: For each example x-Dunlab: 3: Calculate y1 = h1 (x) and y2 = h2 (x) 4: If you add (y1, y2) to D1 (x, y1) to D2 5: Earn h1 on D1 and h2 on D2. 6: Go to (2) if you want, RUNNING EXAMPLEWe use 3500 examples from NER and 1000 from WSJ. We use the remaining 18447 examples as unlabeled data."}, {"heading": "3.3 Analysis", "text": "We want to prove that unilateral learning is possible with references to every possible way. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "4 Experiments", "text": "In this section, we describe our experimental results, some of which we have already discussed in the context of the current example. In Section 4.1, we briefly describe the data sets we use. A complete description of the HMM implementation and its results can be found in Section 4.2. Finally, in Section 4.3, we present results based on a competitive, discriminatively learned sequence marking algorithm."}, {"heading": "4.1 Data Sets", "text": "Our results are based on syntactical data from Penn Treebank (Marcus et al., 1993), in particular the part used by the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000) Our NER data comes from two sources: The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004).The ACE data comprises six separate data sets in six areas: Weblogs (wl), Newswire (nw), Broadcast conversations (bc), United Nations (un), Direct Telephone Speech (dts) and Broadcast News (bn)."}, {"heading": "4.2 HMM Results", "text": "The experiments discussed in the preceding sections are based on a generative hidden Markov model for both the NER and syntactic chunking / POS tagging tasks. The constructed HMMs use first-order transitions and emissions. The emission vocabulary is truncated in such a way that every word that appears \u2264 1 times in the training data is replaced by a unique * unknown * token. Transition and emission probabilities are smoothed out by Dirichlet smoothing, \u03b1 = 0.001 (this has not been aggressively adjusted to a setting). The HMMs are implemented as finite state models in the Carmel toolkit (Graehl and Knight, 2002). The various compatibility functions are also implemented as finite state models. We implement them as converters from POS / chunk labels to NER labels (although they can be performed by reversing in the opposite direction of the MNP) with a single Maps * chunk label."}, {"heading": "4.3 One-sided Discriminative Learning", "text": "In this section, we describe the results of one-sided discriminatory labeling with clues. We use the true syntactic labels from the Penn Treebank to derive the restrictions (this is about 9000 sets). We use the LaSO sequence labeling software (thumb \u0301 III and Marcu, 2005), with its built-in property set.Our goal is to analyze two things: (1) What is the effect of the amount of labeled NER data? (2) What is the effect of the amount of labeled syntactic data from which the clues are constructed? To answer the first question, we keep the amount of syntactic data fixed (at 8936 sets) and vary the amount of labeled data in N {100, 200, 400, 800, 1600}. We compare models with and without the standard gazetteer information from the LaSO software. We have the following models for comparison: \u2022 A baseline default in which we simply use the \"model without syntax.\""}, {"heading": "4.4 Two-sided Discriminative Learning", "text": "In this section, we examine the use of bilateral discriminatory learning to enhance the performance of our syntactic chunking, part of language tagging and designated entity recognition software. We continue to use LaSO (thumb \u0301 III and Marcu, 2005) as the sequence labeling technique. Unfortunately, the results we present are based on an attempt to improve the performance of a state-of-the-art system based on all training data. (This is in contrast to the results in Section 4.3, which examined the effect of using limited amounts of data.) For POS tagging and syntactical chunking, we are labeled with all 8936 sets of training data from CoNLL. For the designated entity recognition, we limit our presentation to results from the CoNLL 2003 shared task. For this data, we have approximately 14k sets of training data, which are all used. In both cases, we reserve 10% as development data."}, {"heading": "5 Discussion", "text": "We have presented a method for learning two tasks simultaneously by using prior knowledge of the relationship between their results, which is related to common conclusions (Daume \u0301 III et al., 2006). However, we do not require a single set of data to be labeled for multiple tasks. In all our examples, we use separate sets of data for shallow analysis as well as for identifying named entities. Although all of our experiments used the LaSO framework for sequence labeling, our method finds that a particular learner is adopted; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc. Our approach, both algorithmically and theoretically, is most related to ideas in co-training (Blum and Mitchell, 1998). The main difference is that in co-training one assumes that \"the two are related.\""}, {"heading": "Acknowledgments", "text": "Many thanks to three anonymous reviewers of this work, whose suggestions greatly improved the work and presentation, which was partially funded by the NSF grant IIS 0712764."}, {"heading": "A Proofs", "text": "The proof for theorem 1 follows that of Blum and Mitchell (1998). The proof (theorem 1, sketch) is derived from the following notation: ck = PrD [h (x) = k], pl = PrD [f (x) = l], ql | k = PrD [f (x) = l | h (x) = k]. Indicated by a series of results that meet the constraints. We are interested in the probability that h (x) = k (x) is wrong because h (x) meets the constraints: p (h) = l (x) | f (x) = p (h) = k (x) = k (x) = k (x) = sum k (x) = k (k) = k (c) ckql) ckql | k / pl \u2264 k (k), k / pl \u2264 k (c) = 1 / pl)."}], "references": [{"title": "Combining labeled and unlabeled data with cotraining", "author": ["Blum", "Mitchell1998] Avrim Blum", "Tom Mitchell"], "venue": "In Proceedings of the Conference on Computational Learning Theory (COLT),", "citeRegEx": "Blum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1998}, {"title": "Learning and inference with constraints", "author": ["Chang et al.2008] Ming-Wei Chang", "Lev Ratinov", "Nicholas Rizzolo", "Dan Roth"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Chang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods", "author": ["Michael Collins"], "venue": "In International Workshop on Parsing Technologies (IWPT)", "citeRegEx": "Collins.,? \\Q2001\\E", "shortCiteRegEx": "Collins.", "year": 2001}, {"title": "PAC generalization bounds for co-training", "author": ["Michael Littman", "David McAllester"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Dasgupta et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2001}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["III Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Multi-view learning over structured and non-identical outputs", "author": ["Joao Graca", "John Blitzer", "Ben Taskar"], "venue": "In Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Ganchev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2008}, {"title": "Carmel finite state transducer package. http://www.isi.edu/licensed-sw/ carmel", "author": ["Graehl", "Knight2002] Jonathan Graehl", "Kevin Knight"], "venue": null, "citeRegEx": "Graehl et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Graehl et al\\.", "year": 2002}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "The Theory and Practice of Discourse Parsing and Summarization", "author": ["Daniel Marcu"], "venue": null, "citeRegEx": "Marcu.,? \\Q2000\\E", "shortCiteRegEx": "Marcu.", "year": 2000}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Marcus et al.1993] Mitch Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "The use of classifiers in sequential inference", "author": ["Punyakanok", "Roth2001] Vasin Punyakanok", "Dan Roth"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Punyakanok et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2001}, {"title": "Text chunking using transformation-based learning", "author": ["Ramshaw", "Marcus1995] Lance A. Ramshaw", "Mitchell P. Marcus"], "venue": "In Proceedings of the Third ACL Workshop on Very Large Corpora", "citeRegEx": "Ramshaw et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ramshaw et al\\.", "year": 1995}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Taskar et al.2005] Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Introduction to the CoNLL-2000 shared task: Chunking", "author": ["Tjong Kim Sang", "Sabine Buchholz"], "venue": "In Proceedings of the Conference on Natural Language Learning (CoNLL)", "citeRegEx": "Sang et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2000}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of Conference on Computational Natural Language Learning,", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Self-training for machine translation", "author": ["Nicola Ueffing"], "venue": "In NIPS workshop on Machine Learning for Multilingual Information Access", "citeRegEx": "Ueffing.,? \\Q2006\\E", "shortCiteRegEx": "Ueffing.", "year": 2006}, {"title": "A theory of the learnable", "author": ["Leslie G. Valiant"], "venue": "Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Valiant.,? \\Q1994\\E", "shortCiteRegEx": "Valiant.", "year": 1994}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky"], "venue": "In Proceedings of the Conference of the Association for Computational Linguistics (ACL)", "citeRegEx": "Yarowsky.,? \\Q1995\\E", "shortCiteRegEx": "Yarowsky.", "year": 1995}], "referenceMentions": [{"referenceID": 15, "context": ", 2006) to machine translation (Ueffing, 2006).", "startOffset": 31, "endOffset": 46}, {"referenceID": 17, "context": "Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions.", "startOffset": 12, "endOffset": 53}, {"referenceID": 3, "context": "It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al., 2001)), but is useful for understanding the process.", "startOffset": 99, "endOffset": 122}, {"referenceID": 16, "context": "Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The original, and simplest analysis of cotraining is due to Blum and Mitchell (1998). It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al.", "startOffset": 13, "endOffset": 706}, {"referenceID": 16, "context": "We define a formal PAC-style (Valiant, 1994) model that we call the \u201chints model\u201d3.", "startOffset": 29, "endOffset": 44}, {"referenceID": 9, "context": "We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences).", "startOffset": 116, "endOffset": 163}, {"referenceID": 2, "context": "Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss.", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss. While not stated directly in terms of PAC learnability, it is clear that his results apply. Taskar et al. (2005) establish tighter bounds for the case of Hamming loss.", "startOffset": 0, "endOffset": 212}, {"referenceID": 9, "context": "Our results are based on syntactic data drawn from the Penn Treebank (Marcus et al., 1993), specifically the portion used by CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000).", "startOffset": 69, "endOffset": 90}, {"referenceID": 7, "context": "Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al.", "startOffset": 191, "endOffset": 214}, {"referenceID": 12, "context": ", 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc.", "startOffset": 87, "endOffset": 108}, {"referenceID": 4, "context": "Recently, Ganchev et al. (2008) proposed a coregularization framework for learning across multiple related tasks with different output spaces.", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "Chang et al. (2008) also propose a \u201csemisupervised\u201d learning approach quite similar to our own model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "With a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (Marcu, 2000), parsing and word alignment, or summarization and information extraction.", "startOffset": 130, "endOffset": 143}], "year": 2013, "abstractText": "We present an algorithmic framework for learning multiple related tasks. Our framework exploits a form of prior knowledge that relates the output spaces of these tasks. We present PAC learning results that analyze the conditions under which such learning is possible. We present results on learning a shallow parser and named-entity recognition system that exploits our framework, showing consistent improvements over baseline methods.", "creator": "LaTeX with hyperref package"}}}