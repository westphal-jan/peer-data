{"id": "0907.0785", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2009", "title": "A Bayesian Model for Discovering Typological Implications", "abstract": "A standard form of analysis for linguistic typology is the universal implication. These implications state facts about the range of extant languages, such as ``if objects come after verbs, then adjectives come after nouns.'' Such implications are typically discovered by painstaking hand analysis over a small sample of languages. We propose a computational model for assisting at this process. Our model is able to discover both well-known implications as well as some novel implications that deserve further study. Moreover, through a careful application of hierarchical analysis, we are able to cope with the well-known sampling problem: languages are not independent.", "histories": [["v1", "Sat, 4 Jul 2009 18:43:16 GMT  (54kb)", "http://arxiv.org/abs/0907.0785v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hal daum\u00e9 iii", "lyle campbell"], "accepted": true, "id": "0907.0785"}, "pdf": {"name": "0907.0785.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["me@hal3.name", "lcampbel@hum.utah.edu"], "sections": [{"heading": null, "text": "ar Xiv: 090 7.07 85v1 [cs.CL] 4 Jul 2"}, {"heading": "1 Introduction", "text": "Linguistic typology aims to distinguish between logically possible languages and languages that are actually observed; a fundamental building block for such understanding is also the universal implication (Greenberg, 1963); these are brief statements that constrain the space of languages in a concrete way (for example, \"object-verb order implies adjective-noun order\"); Croft (2003), Hawkins (1983), and Song (2001) offer excellent introductions to linguistic typology; we present a statistical model for the automatic discovery of such implications from a large typological database (Haspelmath et al., 2005); analyses of universal implications are typically performed by linguists by examining an array of 30-100 languages and a few feature pairs. If you look at all feature pairs (typically several hundred), it is virtually impossible to see these implications by hand. Moreover, it is not enough simply to look at a count."}, {"heading": "2 Data", "text": "The database on which we are conducting our analysis is the World Atlas of Language Structures (Haspelmath et al., 2005), which contains information on 2150 languages (samples from all over the world; Figure 1 shows the locations of languages).There are 139 characteristics in this database, divided into categories such as \"Nominal Categories,\" \"Simple Sentences,\" \"Phonology,\" \"Word Ordering,\" etc. The database is sparse: for many language / trait pairs, the trait value is unknown. In fact, only about 16% of all possible language / trait pairs are known. A sample of five languages and six traits from the database is shown in Table 1. Importantly, the sample density is not random: for certain languages (e.g. English, Chinese, Russian) almost all traits are known, whereas other languages (e.g. Asturian, Omagua, Friesian) have less than five known trait values. In addition, some traits are known for many languages."}, {"heading": "3 Models", "text": "In this section, we propose two models for automatically detecting universal implications from loud, sparse data. First, it should be noted that even well-documented implications are not always without exception. A common example is that verbs before objects (\"VO\") imply nouns after nouns (\"NA\"). This implication (VO \"NA\") has one glaring exception: English.This is a particular form of noise. Another source of noise comes from transcription. WALS contains data on languages documented by fieldlinguists as far back as the 1900s. Much of this older data was collected before there was significant agreement in the documentation style. Different fieldlinguists often had different dimensions along which they segmented language traits into classes, resulting in noise in the characteristics of individual languages. Another difficulty stems from the sampling problem."}, {"heading": "3.1 The FLAT Model", "text": "The order of the two characteristics is important: f1 implies that f2 is logically different from f2 implies, and some of the entries in the matrix will then be unknown. We can certainly remove from consideration all languages for which both are unknown, but we do not remove languages for which only one is unknown. We do this because our model needs to grasp the fact that f2 is always true, then f1 is not interesting. The statistical model is structured as follows: There is a single variable (we will specify this variable \"m\") that corresponds to the question of whether the implication holds. m = 1 means that f1 implies and m = 0 means that it does not work."}, {"heading": "3.2 The HIER Model", "text": "A significant difficulty in working with a large typological database is that the languages are evaluated differently. In our case, this means that implications that apply in the FLAT model may only apply to, say, Indo-Germanic languages, and the other languages are considered noise. While this may be interesting in itself, we are more interested in discovering implication variables that are truly universal. We model this using a hierarchical Bayesian model. In essence, we take the FLAT model and build a concept of linguistic kinship in it. In particular, we force a hierarchy on the m implication variable. For simplicity's sake, we assume that our \"root\" of languages is almost flat. Of the N languages, half are Indo-Germanic and the other half Austronesian. We will use an almost identical model for the FLAT model, but instead of having a single m variable, we have three: one for IE and one for IE."}, {"heading": "4 Statistical Inference", "text": "In this section we describe how we use the Markov chain Monte Carlo methods to draw conclusions from the statistical models described in the previous section; Andrieu et al. (2003) provide an excellent introduction to the MCMC techniques. The basic idea behind the MCMC techniques is to approximate insoluble expectations by drawing random samples from the probability distribution of interest. The expectation can then be approximated by an empirical expectation about this sample. For the FLAT model, we use a combination of Gibbs samples with rejection samples as a subroutine. Essentially, all sample steps are standard steps of Gibbs, with the exception of sampling error rates e. The Gibbs step is not analytically available for these. Therefore, we use rejection samples (sampling from the previous beta and acceptance after the posterior). The sampling method for the HERE model is slightly more complicated for each of the Gibbs values (we use only the anfeet for the internal values)."}, {"heading": "5 Data Preprocessing and Search", "text": "After extracting the raw data from the electronic WALS database (Haspelmath et al., 2005) 2, we perform a small amount of pre-processing. Essen-2This is not trivial - we are currently investigating the possibility of freely sharing this data. We have removed certain characteristic values manually from the database because they are underrepresented. For example, the function \"Glottalized Consonants\" has eight possible values (one for \"none\" and seven for different types of glottalized consonants). We simply reduce these to two values \"has\" or \"does not have.\" 313 languages have no glottalized consonants and 139 have a certain variety of glottalized consonants. We have done something similar with about twenty of the characteristics. For the HERE model, we get the hierarchy in one of two possibilities. The first hierarchy we use is the \"linguistic hierarchy,\" which is specified as part of the WALS data. This hierarchy divides families and subfamilies."}, {"heading": "6 Results", "text": "The task of discovering universal implications is essentially a data mining task. Therefore, it is difficult to evaluate them, as we often do not know the right answers! If our model only found well-documented implications, it would be interesting, but useless from the perspective of supporting linguists who focus their energies on new, plausible implications. In this section, we present the results of our method along with a quantitative and qualitative assessment."}, {"heading": "6.1 Quantitative Evaluation", "text": "In this section, we perform a quantitative evaluation of the results based on predictive power, which means that one would generally prefer a system that finds implications that are likely to be contained in the data. \"General\" is important: this quality is neither necessary nor sufficient for the model to be good. For example, finding 1000 implications of the form A1, A2, X,.., A1000, X is completely uninteresting if X is true 99% of the time. Likewise, we assume that a model 1000 implications of the form X, A1,.., X, but X is true only in five languages. In both cases, these would be ideal systems according to a \"predictive power\" metric, but both are somewhat uninteresting. Despite these difficulties with predictive power-based evaluation, we believe that it is a good way to understand the relative merits of our different models."}, {"heading": "6.2 Cross-model Comparison", "text": "The results in the previous section support the conclusion that the two hierarchical models do something significantly different (and better) than the flat model. The answer to these two questions is \"yes.\" First, we deal with the problem of tree similarity. We look at all the language pairs that are at distance 0 in the family tree (i.e., they have the same parent), and then we look at the mean distance between these languages in the family tree. We do this for all distances in the family tree (due to its construction, there are only three: 0, 2, and 4). The mean distances in the family tree that correspond to these three distances in the family tree are: 2.9, 3.5, and 4.0. This means that languages that are \"close\" to each other statistically."}, {"heading": "6.3 Qualitative Analysis", "text": "In fact, it is so that it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about which it is about a way and in which it is about a way and in which it is about which it is about which it is about a way and in which it is about a way and in which it is about a way and in which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about a way and which it is about which it is about which it is about which it is about which it is about a way and which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which"}, {"heading": "6.4 Multi-conditional Implications", "text": "It is easy to modify our model to look for more than three characteristics, two of which are conditioned and the third predicted. Space precludes an in-depth discussion of these results, but we present the top examples in Table 3 (after removing the tautalogically true examples, which are more numerous in this case, as well as examples available directly from Table 2). It is encouraging that among the 1000 multiconditional implications found, the most commonly used were \"OV\" (176 times), \"postpositions\" (157 times) and \"AdjectiveNoun\" (89 times), which is in line with intuition."}, {"heading": "7 Discussion", "text": "We have presented a Bayesian model for the discovery of universal linguistic implications from a typological database. Our model is capable of explaining noise in a linguistically plausible way. Our hierarchical models address the problem of the sample in a unique way, using previous knowledge of language families to \"group\" related languages. Quantitatively, the hierarchical information is quite useful whether it is phylogenetic or spatially based. Qualitatively, our model can cover many well-known implications as well as many other potential implications that may be the subject of future linguistic studies. We believe that our model is sufficiently general that it could be applied to many different typological databases - we have tried not to \"match\" it to WALS. Our hope is that the automatic discovery of such implications will help not only typologically oriented linguists, but also other groups."}], "references": [{"title": "and Michael I", "author": ["Christophe Andrieu", "Nando de Freitas", "Arnaud Doucet"], "venue": "Jordan.", "citeRegEx": "Andrieu et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Typology and Univerals", "author": ["William Croft"], "venue": null, "citeRegEx": "Croft.,? \\Q2003\\E", "shortCiteRegEx": "Croft.", "year": 2003}, {"title": "Joseph Kurskal", "author": ["Isidore Dyen"], "venue": "and Paul Black.", "citeRegEx": "Dyen et al.1992", "shortCiteRegEx": null, "year": 1992}, {"title": "and Bernard Comrie", "author": ["Martin Haspelmath", "Matthew Dryer", "David Gil"], "venue": "editors.", "citeRegEx": "Haspelmath et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Word Order Universals: Quantitative analyses of linguistic structure", "author": ["John A. Hawkins"], "venue": null, "citeRegEx": "Hawkins.,? \\Q1983\\E", "shortCiteRegEx": "Hawkins.", "year": 1983}, {"title": "Language Classification by Numbers", "author": ["McMahon", "McMahon2005] April McMahon", "Robert McMahon"], "venue": null, "citeRegEx": "McMahon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McMahon et al\\.", "year": 2005}, {"title": "Possible and Probable Languages: A Generative Perspective on Linguistic Typology", "author": ["Frederick J. Newmeyer"], "venue": null, "citeRegEx": "Newmeyer.,? \\Q2005\\E", "shortCiteRegEx": "Newmeyer.", "year": 2005}, {"title": "Language Independent Induction of Part of Speech Class Labels Using only Language Universals. Machine Learning: Beyond Supervision", "author": ["Schone", "Jurafsky2001] Patrick Schone", "Dan Jurafsky"], "venue": null, "citeRegEx": "Schone et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Schone et al\\.", "year": 2001}, {"title": "Linguistic Typology: Morphology and Syntax", "author": ["Jae Jung Song"], "venue": "Longman Linguistics Library", "citeRegEx": "Song.,? \\Q2001\\E", "shortCiteRegEx": "Song.", "year": 2001}], "referenceMentions": [], "year": 2013, "abstractText": "A standard form of analysis for linguistic typology is the universal implication. These implications state facts about the range of extant languages, such as \u201cif objects come after verbs, then adjectives come after nouns.\u201d Such implications are typically discovered by painstaking hand analysis over a small sample of languages. We propose a computational model for assisting at this process. Our model is able to discover both well-known implications as well as some novel implications that deserve further study. Moreover, through a careful application of hierarchical analysis, we are able to cope with the well-known sampling problem: languages are not independent.", "creator": "LaTeX with hyperref package"}}}