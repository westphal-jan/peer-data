{"id": "0907.0806", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2009", "title": "A Noisy-Channel Model for Document Compression", "abstract": "We present a document compression system that uses a hierarchical noisy-channel model of text production. Our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a statistical hierarchical model of text production in order to drop non-important syntactic and discourse constituents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization.", "histories": [["v1", "Sat, 4 Jul 2009 22:26:47 GMT  (34kb)", "http://arxiv.org/abs/0907.0806v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hal daum\\'e iii", "daniel marcu"], "accepted": true, "id": "0907.0806"}, "pdf": {"name": "0907.0806.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hdaume@isi.edu", "marcu@isi.edu"], "sections": [{"heading": null, "text": "ar Xiv: 090 7.08 06v1 [cs.CL] 4 Jul 200 9We introduce a compression system that uses a hierarchical sound channel model of text generation; our compression system first automatically deduces the syntactic structure of each sentence and the general discourse structure of the text specified as input; then uses a statistical hierarchical model of text generation to drop unimportant syntactic and discourse components in order to produce coherent grammatical document compressions of any length; the system surpasses both a baseline and a sentence-based compression system that works by sequentially simplifying all sentences in a text; our results support the assertion that discourse knowledge plays an important role in summarizing documents."}, {"heading": "1 Introduction", "text": "In fact, it is in such a way that most people who are able to feel themselves in a position to move in the world, to stay in the world, in the world in which they live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in"}, {"heading": "2 Document Compression", "text": "The task of document compression is conceptually simple. In the face of a document D = < w1w2.. wn >, our goal is to produce a new document D \u2032 by \"dropping\" words wi from D. To achieve this goal, a number of other systems use the results of extractive summaries and repair them to improve coherence (DUC, 2001; DUC, 2002). Unfortunately, none of them seem flexible enough to produce good summaries that are both coherent and grammatical in a single shot. To the extent of Knight & Marcu's noise channel model (2000), their system compresses sentences by dropping syntactical components but can only apply to whole documents on a sentence basis. As discussed in Section 1, this is not sufficient because the resulting summary may contain many compressed sentences that are irrelevant."}, {"heading": "3 A Noisy-Channel Model", "text": "For a particular document D, we want to find the summary text S that maximizes P (S | D). With Bayes rule, we twist this so that we end up maximizing P (D | S) P (S). We assume that we know the discourse structure of each document and the syntactic structures of each of its terms. The intuitive way of thinking about this application of Bayes rule, which is touted as the noisy channel model, is that we know the discourse structure of each document and the syntactic structures of each of its terms."}, {"heading": "3.1 Source model", "text": "The purpose of the source model is to assign a score P (S) to a compression regardless of the original document, i.e. the source model should measure how good a summary is in English (regardless of whether it is good compression or not). Currently, we use a Bigram quality measure (trigram scores have also been tested, but have not made any difference), combined with non-lexicalized context-free syntactic probabilities and context-free discourse probabilities, which gives P (S) = Pbigram (S) \u0445 PPCFG (S) \u0445 PDPCFG (S) \u0445 PDPCFG (S). It would be better to use a lexicalized context-free grammar, but this was not possible given the decoder used."}, {"heading": "3.2 Channel model", "text": "The reason for this is that most people who are in Germany are not able to help themselves."}, {"heading": "3.3 Decoder", "text": "The goal of the decoder is to combine P (S) with P (D | S) to obtain P (S | D). There is an enormous number of potential compressions of a large DS tree, but we can efficiently package them into a shared forest structure, as Knight & Marcu (2000) described in detail. Each entry in the shared forest structure has three associated probabilities, one from the source syntax PCFG, one from the source discourse PCFG, and one from the probabilities of the expansion template described in Section 3.2. Once we have created a forest that represents all possible compressions of the original document, we want to extract the best (or n-best) trees, with both the expansion probabilities of the channel model and the bigram and syntax and discourse PCFG probabilities of the source model being taken into account. Fortunately, such a generic extractor has already been created (a long list of purposes is already selected according to our 2000 tree extraction results), with the best possible combination of the trees."}, {"heading": "4 System", "text": "The first step along the pipeline, however, is to generate the discourse structure, using the decision-based discourse sparser described by Marcu (2000). Once we have the discourse structure, we send each EDU to a syn-2The discourse sparser achieves an f-score of 38.2 for EDU identification, 50.0 for hierarchical span identification, 39.9 for nuclear identification, and 23.4 for relation day.Tactical parser (Collins, 1997). The syntax trees of the EDUs are then merged with the discourse tree in the forest generator to create a DS tree similar to the one shown in Figure 1. From this DS tree, we create a forest that summarizes all possible compressions, and this forest is then passed on to the forest ranking system used as a decoder."}, {"heading": "5 Results", "text": "This year, it has reached the point where it will be able to retaliate."}], "references": [{"title": "Headline generation based on statistical translation", "author": ["Michele Banko", "Vibhu Mittal", "Michael Witbrock."], "venue": "Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL\u2013 2000), pages 318\u2013325, Hong Kong, October 1\u20138.", "citeRegEx": "Banko et al\\.,? 2000", "shortCiteRegEx": "Banko et al\\.", "year": 2000}, {"title": "Query-relevant summarization using FAQs", "author": ["Adam Berger", "Vibhu Mittal."], "venue": "Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL\u20132000), pages 294\u2013301, Hong Kong, October 1\u20138.", "citeRegEx": "Berger and Mittal.,? 2000", "shortCiteRegEx": "Berger and Mittal.", "year": 2000}, {"title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory", "author": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski."], "venue": "Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue, Eurospeech 2001, Aalborg, Denmark,", "citeRegEx": "Carlson et al\\.,? 2001", "shortCiteRegEx": "Carlson et al\\.", "year": 2001}, {"title": "Practical simplification of english newspaper text to assist aphasic readers", "author": ["John Carroll", "Guidon Minnen", "Yvonne Canning", "Siobhan Devlin", "John Tait."], "venue": "Proceedings of the AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology.", "citeRegEx": "Carroll et al\\.,? 1998", "shortCiteRegEx": "Carroll et al\\.", "year": 1998}, {"title": "Motivations and methods for text simplification", "author": ["R. Chandrasekar", "Christy Doran", "Srinivas Bangalore."], "venue": "Proceedings of the Sixteenth International", "citeRegEx": "Chandrasekar et al\\.,? 1996", "shortCiteRegEx": "Chandrasekar et al\\.", "year": 1996}, {"title": "Three generative, lexicalized models for statistical parsing", "author": ["Michael Collins."], "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL\u201397), pages 16\u201323, Madrid, Spain, July 7-12.", "citeRegEx": "Collins.,? 1997", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "Producing intelligent telegraphic text reduction to provide an audio scanning service for the blind", "author": ["Gregory Grefenstette."], "venue": "Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization, pages 111\u2013118, Stanford University, CA, March 23-", "citeRegEx": "Grefenstette.,? 1998", "shortCiteRegEx": "Grefenstette.", "year": 1998}, {"title": "Deep read: A reading comprehension system", "author": ["L. Hirschman", "M. Light", "E. Breck", "J. Burger."], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hirschman et al\\.,? 1999", "shortCiteRegEx": "Hirschman et al\\.", "year": 1999}, {"title": "Sentence reduction for automatic text summarization", "author": ["H. Jing."], "venue": "Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics NAACL-2000, pages 310\u2013315, Seattle, WA.", "citeRegEx": "Jing.,? 2000", "shortCiteRegEx": "Jing.", "year": 2000}, {"title": "Statistics-based summarization \u2014 step one: Sentence compression", "author": ["Kevin Knight", "Daniel Marcu."], "venue": "The 17th National Conference on Artificial Intelligence (AAAI\u20132000), pages 703\u2013710, Austin, TX, July 30th \u2013 August 3rd.", "citeRegEx": "Knight and Marcu.,? 2000", "shortCiteRegEx": "Knight and Marcu.", "year": 2000}, {"title": "Forest-based statistical sentence generation", "author": ["Irene Langkilde."], "venue": "Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, Seattle, Washington, April 30\u2013May 3.", "citeRegEx": "Langkilde.,? 2000", "shortCiteRegEx": "Langkilde.", "year": 2000}, {"title": "Hypertext summary extraction for fast document browsing", "author": ["Kavi Mahesh."], "venue": "Proceedings of the AAAI Spring Symposium on Natural Language Processing for the World Wide Web, pages 95\u2013103.", "citeRegEx": "Mahesh.,? 1997", "shortCiteRegEx": "Mahesh.", "year": 1997}, {"title": "Advances in Automatic Text Summarization", "author": ["Inderjeet Mani", "Mark Maybury", "editors"], "venue": null, "citeRegEx": "Mani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mani et al\\.", "year": 1999}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C. Mann", "Sandra A. Thompson."], "venue": "Text, 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "The Theory and Practice of Discourse Parsing and Summarization", "author": ["Daniel Marcu."], "venue": "The MIT Press, Cambridge, Massachusetts.", "citeRegEx": "Marcu.,? 2000", "shortCiteRegEx": "Marcu.", "year": 2000}], "referenceMentions": [{"referenceID": 14, "context": "Extractive summarizers simply select and present to the user the most important sentences in a text \u2014 see (Mani and Maybury, 1999; Marcu, 2000; Mani, 2001) for comprehensive overviews of the methods and algorithms used to accomplish this.", "startOffset": 106, "endOffset": 155}, {"referenceID": 0, "context": "Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \u3008Headline, Text\u3009 pairs (Banko et al., 2000; Berger and Mittal, 2000).", "startOffset": 120, "endOffset": 165}, {"referenceID": 1, "context": "Headline generators are noisy-channel probabilistic systems that are trained on large corpora of \u3008Headline, Text\u3009 pairs (Banko et al., 2000; Berger and Mittal, 2000).", "startOffset": 120, "endOffset": 165}, {"referenceID": 4, "context": "Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases.", "startOffset": 32, "endOffset": 151}, {"referenceID": 11, "context": "Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases.", "startOffset": 32, "endOffset": 151}, {"referenceID": 3, "context": "Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases.", "startOffset": 32, "endOffset": 151}, {"referenceID": 6, "context": "Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases.", "startOffset": 32, "endOffset": 151}, {"referenceID": 8, "context": "Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases.", "startOffset": 32, "endOffset": 151}, {"referenceID": 9, "context": "Sentence simplification systems (Chandrasekar et al., 1996; Mahesh, 1997; Carroll et al., 1998; Grefenstette, 1998; Jing, 2000; Knight and Marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases.", "startOffset": 32, "endOffset": 151}, {"referenceID": 13, "context": "Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) provides us this \u201cglue.", "startOffset": 34, "endOffset": 59}, {"referenceID": 13, "context": "extent the noisy-channel model proposed by Knight & Marcu (2000). Their system compressed sentences by dropping syntactic constituents, but could be applied to entire documents only on a sentenceby-sentence basis.", "startOffset": 52, "endOffset": 65}, {"referenceID": 14, "context": "\u201d These are both examples of sentence expansion as used previously by Knight & Marcu (2000). Our system, however, also has the ability to expand on a core message by adding discourse constituents.", "startOffset": 79, "endOffset": 92}, {"referenceID": 9, "context": "We refer the reader to (Knight and Marcu, 2000) for the details.", "startOffset": 23, "endOffset": 47}, {"referenceID": 2, "context": "(See (Carlson et al., 2001) for details concerning the corpus and the annotation process.", "startOffset": 5, "endOffset": 27}, {"referenceID": 10, "context": "Thankfully, such a generic extractor has already been built (Langkilde, 2000).", "startOffset": 60, "endOffset": 77}, {"referenceID": 13, "context": "There are a vast number of potential compressions of a large DS-tree, but we can efficiently pack them into a shared-forest structure, as described in detail by Knight & Marcu (2000). Each entry in the shared-forest structure has three associated probabilities, one from the source syntax PCFG, one from the source discourse PCFG and one from the expansion-template probabilities described in Section 3.", "startOffset": 170, "endOffset": 183}, {"referenceID": 14, "context": "To do this, we use the decision-based discourse parser described by Marcu (2000)2.", "startOffset": 68, "endOffset": 81}, {"referenceID": 5, "context": "tactic parser (Collins, 1997).", "startOffset": 14, "endOffset": 29}, {"referenceID": 10, "context": "This forest is then passed on to the forest ranking system which is used as decoder (Langkilde, 2000).", "startOffset": 84, "endOffset": 101}, {"referenceID": 7, "context": "We call this set the MITRE corpus (Hirschman et al., 1999).", "startOffset": 34, "endOffset": 58}, {"referenceID": 14, "context": "Concat: Each sentence is compressed individually; the results are concatenated together, using Knight & Marcu\u2019s (2000) system here for comparison.", "startOffset": 104, "endOffset": 119}, {"referenceID": 2, "context": "PD-EDU: Same as EDU except using the perfect discourse trees, available from the RST corpus (Carlson et al., 2001).", "startOffset": 92, "endOffset": 114}], "year": 2013, "abstractText": "We present a document compression system that uses a hierarchical noisy-channel model of text production. Our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a statistical hierarchical model of text production in order to drop non-important syntactic and discourse constituents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}