{"id": "0907.0807", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2009", "title": "A Large-Scale Exploration of Effective Global Features for a Joint Entity Detection and Tracking Model", "abstract": "Entity detection and tracking (EDT) is the task of identifying textual mentions of real-world entities in documents, extending the named entity detection and coreference resolution task by considering mentions other than names (pronouns, definite descriptions, etc.). Like NE tagging and coreference resolution, most solutions to the EDT task separate out the mention detection aspect from the coreference aspect. By doing so, these solutions are limited to using only local features for learning. In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDT model and explore the utility of many features, demonstrating their effectiveness on this task.", "histories": [["v1", "Sat, 4 Jul 2009 22:28:15 GMT  (47kb)", "http://arxiv.org/abs/0907.0807v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hal daum\\'e iii", "daniel marcu"], "accepted": true, "id": "0907.0807"}, "pdf": {"name": "0907.0807.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hdaume@isi.edu", "marcu@isi.edu"], "sections": [{"heading": null, "text": "ar Xiv: 090 7.08 07v1 [cs.CL] 4 Jul 2"}, {"heading": "1 Introduction", "text": "This year, it is so far that it is able to retaliate, to retaliate, \"he said.\" It is as if it is able to retaliate, \"he said.\" It is as if it has been able to find a solution, \"he said.\" It is as if it is able to find a solution that is able to find a solution, \"he said."}, {"heading": "2 Learning as Search Optimization", "text": "When trying to apply current, standardized machine learning algorithms to problems with combinatorial structured results Y (2001), the resulting algorithm implicitly assumes that it is possible to find the best structures for a given input (and some model parameters). Furthermore, most models require much more, either in the form of function expectations for conditional probability methods (Lafferty et al., 2001) or local marginal distributions for margin-based methods (Taskar et al., 2003). In many cases - including EDT and correlation - this is a false assumption. Often, we are unable to find the best solution, but must use an approximate search to find the best possible solution, given time and space constraints. The Learning as SearchOptimization (LaSO) framework uses this difficulty as an opportunity and tries to find model parameters that are good in the context of the search."}, {"heading": "3 Joint EDT Model", "text": "The LaSO framework essentially requires us to specify two components: the search space (and the corresponding operations) and the features. These two are inherently linked since the features depend on the search space, but for the time being we will ignore the question of the feature functions and concentrate on the search."}, {"heading": "3.1 Search Space", "text": "We structure the search in a left-to-right decoding frame: A hypothesis is a complete identification of the initial segment of a document. For example, for a document with N words, a hypothesis ending at position 0 < n < N is essentially what you would get if you took the full structured output and took it at the word n. In the example given in the introduction, a hypothesis might agree with \"Bill Clinton gave a\" (which would be a y-good hypothesis) or \"Bill Clinton gave a\" (which would not be a y-good hypothesis), and a hypothesis is expanded by applying search operations. In our case, the search method first selects the number of words it will consume (for example, to make the mention of \"Bill Clinton,\" it would have to consume two words), and then decides on an entity type and a mention type (or it chooses not to label this chunk as an entity, and finally, it is not a noun)."}, {"heading": "3.2 An Example", "text": "Let's say we're at the word \"she\" and the hypothesis we're expanding is right; that is, we've correctly identified \"Bill Clinton\" with the type \"person\" and mentioned the type \"name\"; that we've identified \"the Senate\" with the type \"organization\" and mentioned the type \"name\"; and that \"the president\" refers to the chain < Bill Clinton >; and that \"being\" refers to the chain < Bill Clinton, the president >. At this point in the search, we have two possibilities for length: one or two (because there are only two words left: \"she\" and a period); a first hypothesis would be that the word \"she\" is so pronounced; a second hypothesis would be that \"she\" is a named person and is a new person; a third person would be a hypothesis that an organization could be chosen with \"; a third hypothesis would be called\" organization \"; a third hypothesis would be\" called \"corominal.\""}, {"heading": "3.3 Linkage Type", "text": "A major problem that arises in the context of assigning a hypothesis to a coreference chain is how to calculate characteristics over that chain. As we will discuss in Section 4, most of our coreference-specific characteristics are about chunk pairs: the proposed new mention and a prehistory. However, since a proposed mention can generally have more than one prehistory, we have a decision about how to combine this information. The first, most obvious solution is to do essentially nothing: simply calculate the characteristics over all pairs and add them up as usual. However, this method intuitively has the potential to overestimate the effects of large chains. To compensate for this, one could advocate the use of an average link calculation, in which the score for a coreference chain is calculated by averaging over its elements."}, {"heading": "4 Feature Functions", "text": "All the features we are considering concern the form basic feature \u00d7 decision feature, where basic features are functions of input and decisions are functions of the hypothesis. For example, a basic feature could be something like \"the current chunk contains the word 'Clinton'\" and a decision feature something like \"the current chunk is a named person.\""}, {"heading": "4.1 Base Features", "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules that they have shown in previous years."}, {"heading": "4.2 Decision Features", "text": "Our decision characteristics are divided into three classes: simple, correlation and boundary characteristics. Simple decision characteristics include: Is this part marked as a unit; what is its entity type; what is its entity type; what is its entity type / mention type pair. Key decision characteristics include: is this entity type the beginning of a chain or the continuation of an existing chain; what is the entity type of this started (or continued) chain; what is the entity type of this started (or continued) chain; what is the mention type of this started (or continued) chain; what is the mention type of this started chain; what is the mention type of this continued chain and the mention type of recent history. Boundary. Boundary decision characteristics include: the second and third order Markov characteristics about entity type, entity type and mention type; the characteristics that appear in the next words, and also within three words."}, {"heading": "5 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "We use the official 2004 ACE training and testing set for evaluation purposes; however, we exclude the Fisher conversation data from the training set because it is very different from the other data sets and there is no Fisher data in the 2004 test set. This corresponds to 392 training documents consisting of 8.1 k sets and 160 k words. In total, the data contains 24k mentions corresponding to 10k entities (note that the data is not commented for cross-reference purposes, so cases of \"Bill Clinton\" are counted as two different entities in two different documents). Approximately half of the entities are persons, a fifth are organizations, a fifth are GPEs and the rest are predominantly sites or facilities. The test data are 192 documents, 3.5 k sets and 64 k words, with 10k mentions counting as 4.5 k entities. In all cases, we use a 16 bar for training and testing and ignore features that occur less than five times in the training data."}, {"heading": "5.2 Evaluation Metrics", "text": "This is roughly calculated by first comparing system mentions with reference mentions, and then using them to compare system units with reference mentions. Once this synchronization is complete, there is a cost of typing errors, false alarms, and false alarms, which together produce an ACE score ranging from 0 to 100, with 100 being perfect (we use v.10 of the ACE rating script)."}, {"heading": "5.3 Joint versus Pipelined", "text": "We compare the performance of the Common Model with the Pipelines System. For the Pipelines System, to build the Mention Detection Module, we use the same technique as for the entire system, but simply do not include the information from the Coreference Chain in the hypotheses (though still in a left-to-right manner).1 Run as such, the Common Model achieves an ACE score of 79.4 and the Pipelines Model achieves an ACE score of 78.1, a reasonably significant improvement for performing both tasks at the same time. We have also calculated the performance of these two systems, ignoring the coreference values (this is done by considering each mention as its own unit and recalculating the ACE score)."}, {"heading": "5.4 Feature Comparison for Coreference", "text": "In this section, we analyze the impact of the various base characteristic types on correlation performance. We use a model with perfect mentions, entity types, and mention types (except pronouns: we do not assume that we know pronoun types because it reveals too much information) and measure the performance of the correlation system. When the model is run with full functionality, it reaches an ACE value of 89.1, and when executed without additional features using simple distortions, it reaches 65.4. The most powerful system in the 2004 ACE competition achieved a value of 91.5 for this task; the next best system achieved a value of 88.2, placing us right in the middle of these two values (although statistically it probably does not differ significantly). In addition, the most powerful system used additional data it labeled in the house.In order to calculate the performance of the correlation, we start with all characteristics relevant and remove them from the typical performance (we do not include them in the performance classes that are relevant to the performance)."}, {"heading": "5.5 Linkage Types", "text": "As mentioned in the previous section, the core task with intelligent linkage reaches an ACE value of 89.1. The next best value is with min linkage (88.7) followed by an average linkage with a value of 88.1. There is then a fairly large decrease with max linkage to 86.2, followed by a further decrease for last linkage to 83.5, and first linkage comes out worst with 81.5."}, {"heading": "6 Discussion", "text": "In this paper, we applied the Learning as Search Optimization (LaSO) framework to the task of identifying and tracking entities. This framework is an excellent choice for this problem, since many of the relevant features for the co-reference task (and even for mentioning the detection task) are highly inlocal. This non-locality makes models such as Markov networks insoluble, and LaSO provides an excellent framework for solving this problem. We have introduced a large number of new, useful features for this task, in particular the use of knowledge-based features to support the name-to-nominal problem, which has led to a significant improvement in performance. We have shown that conducting joint learning leads to detection and co-manufacturing leading to a more powerful model that implements learning in pipelines. We have also provided a comparison of the contributions of our different feature classes and compared different linking types for co-reference chains. In the course of this process, we intend our model of failure to match the ACIS with this efficient model, which is the best source of success."}], "references": [{"title": "An algorithm that learns what\u2019s in a name", "author": ["Bikel et al.1999] D. Bikel", "R. Schwartz", "R. Weischedel"], "venue": "Machine Learning,", "citeRegEx": "Bikel et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bikel et al\\.", "year": 1999}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["III Daum\u00e9", "III Marcu2005] H. Daum\u00e9", "Marcu. D"], "venue": null, "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["T. Dunning"], "venue": "Computational Linguistics,", "citeRegEx": "Dunning.,? \\Q1993\\E", "shortCiteRegEx": "Dunning.", "year": 1993}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Offline strategies for online question answering: Answering questions before they are asked", "author": ["E. Hovy", "A. Echihabi"], "venue": null, "citeRegEx": "Fleischman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fleischman et al\\.", "year": 2003}, {"title": "A statistical model for multilingual entity detection and tracking", "author": ["Florian et al.2004] R. Florian", "H. Hassan", "A. Ittycheriah", "H. Jing", "N. Kambhatla", "X. Luo", "N. Nicolov", "S. Roukos"], "venue": "NAACL/HLT", "citeRegEx": "Florian et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2004}, {"title": "A new approximate maximal margin classification", "author": ["C. Gentile"], "venue": null, "citeRegEx": "Gentile.,? \\Q2001\\E", "shortCiteRegEx": "Gentile.", "year": 2001}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Lafferty et al.2001] J. Lafferty", "A. McCallum", "F. Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Improving machine learning approaches to coreference resolution", "author": ["Ng", "Cardie2002] V. Ng", "C. Cardie"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Randomized algorithms and NLP: Using locality sensitive hash functions for high speed noun clustering", "author": ["P. Pantel", "E. Hovy"], "venue": null, "citeRegEx": "Ravichandran et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ravichandran et al\\.", "year": 2005}, {"title": "A machine learning approach to coreference resolution of noun phrases", "author": ["Soon et al.2001] W. Soon", "H. Ng", "D. Lim"], "venue": "Computational Linguistics,", "citeRegEx": "Soon et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Soon et al\\.", "year": 2001}, {"title": "Collective segmentation and labeling of distant entities in information extraction", "author": ["Sutton", "McCallum2004] C. Sutton", "A. McCallum"], "venue": "In ICML workshop on Statistical Relational Learning", "citeRegEx": "Sutton et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "Furthermore, most models require much more, either in the form of feature expectations for conditional likelihood-based methods (Lafferty et al., 2001) or local marginal distributions for margin-based methods (Taskar et al.", "startOffset": 128, "endOffset": 151}, {"referenceID": 6, "context": "The algorithm relies on a parameter update formula; the two suggested by (Daum\u00e9 III and Marcu, 2005) are a standard Perceptron-style update and an approximate large margin update of the sort proposed by (Gentile, 2001).", "startOffset": 203, "endOffset": 218}, {"referenceID": 0, "context": "This includes: the number of words in the current chunk; the unigrams (words) contained in this chunk; the bigrams; the two character prefixes and suffixes; the word stem; the case of the word, computed by regular expressions like those given by (Bikel et al., 1999); simple morphological features (number, person and tense when applicable); and, in the case of coreference, pairs of features between the current mention and an antecedent.", "startOffset": 246, "endOffset": 266}, {"referenceID": 4, "context": "In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman\u2019s web page to generate features between names and nominals (this list contains 2m pairs mined from 15GBs of news data).", "startOffset": 59, "endOffset": 84}, {"referenceID": 9, "context": "stances from news, we have additionally used similar data mined from a 138GB web corpus, for which more general \u201cISA\u201d relations were mined (Ravichandran et al., 2005).", "startOffset": 139, "endOffset": 166}, {"referenceID": 9, "context": "The first class-based feature we use is based on word classes derived from the web corpus mentioned earlier and computed as described by (Ravichandran et al., 2005).", "startOffset": 137, "endOffset": 164}, {"referenceID": 2, "context": "The second attempts to instill knowledge of collocations in the data; we use the technique described by (Dunning, 1993) to compute multi-word expressions and then mark words that are commonly used as such with a feature that expresses this fact.", "startOffset": 104, "endOffset": 119}, {"referenceID": 0, "context": "Finally, we use a list of persons, organizations and locations that were identified at least 100 times in a large corpus by the BBN IdentiFinder named entity tagger (Bikel et al., 1999).", "startOffset": 165, "endOffset": 185}], "year": 2013, "abstractText": "Entity detection and tracking (EDT) is the task of identifying textual mentions of real-world entities in documents, extending the named entity detection and coreference resolution task by considering mentions other than names (pronouns, definite descriptions, etc.). Like NE tagging and coreference resolution, most solutions to the EDT task separate out the mention detection aspect from the coreference aspect. By doing so, these solutions are limited to using only local features for learning. In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDT model and explore the utility of many features, demonstrating their effectiveness on this task.", "creator": "LaTeX with hyperref package"}}}