{"id": "0907.1814", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2009", "title": "Bayesian Query-Focused Summarization", "abstract": "We present BayeSum (for ``Bayesian summarization''), a model for sentence extraction in query-focused summarization. BayeSum leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BayeSum is not afflicted by the paucity of information in short queries. We show that approximate inference in BayeSum is possible on large data sets and results in a state-of-the-art summarization system. Furthermore, we show how BayeSum can be understood as a justified query expansion technique in the language modeling for IR framework.", "histories": [["v1", "Fri, 10 Jul 2009 13:24:55 GMT  (36kb)", "http://arxiv.org/abs/0907.1814v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["hal daum\\'e iii"], "accepted": true, "id": "0907.1814"}, "pdf": {"name": "0907.1814.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 090 7.18 14v1 [cs.CL] 1 0Ju l 2"}, {"heading": "1 Introduction", "text": "In view of a query and a collection of relevant documents, our algorithm works by asking the following question: What is it about these relevant documents that distinguishes them from the non-relevant documents? BAYESUM can be considered a statistical formulation of this exact question. BAYESUM's main requirement is that several relevant documents are known for the query in question. This is not a serious limitation. In the case of two well-investigated problems, it is the de facto standard. In the standard summary of multiple documents (with or without query), we have access to known relevant documents for a specific user need. Similarly, in the case of a web search application, an underlying IR machine can restore several (presumably) relevant documents for a given query. BAYESUM works well for both of these tasks, even if the underlying expansion method is eval."}, {"heading": "2 Bayesian Query-Focused Summarization", "text": "In this section, we describe our Bayesian Query-Based Summarization Model (BAYESUM), which is very similar to the standard ad hoc IR task, with the important difference that we compare query models with set models, not with document models. The brevity of the sentences means that you had to do a good job in creating the query models. To maintain generality, so that our model is applicable to any problem where several relevant documents are known to be relevant to a query, we formulate our model in terms of relevance assessments. For a collection of D documents and Q queries, we assume that we have a D-Q binary matrix r, where rdq = 1 applies if a document d is relevant to query q. When summarizing multiple documents, rdq will be 1 if d is in the document set corresponding to query q; for search engine summarization, it is 1 if d is returned by the search engine for query q."}, {"heading": "2.1 Language Modeling for IR", "text": "BAYESUM is based on the concept of language models for retrieving information. The idea behind the language modeling techniques used in IR is to represent either queries or documents (or both) as probability distributions and then compare them with usual probability distributions. These probability distributions are almost always \"bags of words\" that assign probability to words from a fixed vocabulary V. One approach is to create a probability distribution for a particular document, pd (\u00b7), and to look at the probability of a query under that distribution: pd (q). Documents are sorted according to how likely they are to make the query (Ponte and Croft, 1998). Other researchers have probability distributions via queries pq (\u00b7) and ranked documents according to how likely they look under the query model: pq (d), pppppppa and Pa, 2001."}, {"heading": "2.2 Bayesian Statistical Model", "text": "In the language of information retrieval, the task of query-focused sentence extraction is to estimate a good query model, pq (\u00b7). Once we have such a model, we can estimate sentence models for each sentence in a relevant document and rank sentences by Eq (1). BAYESUM's system is based on the following model: We assume that a sentence appears in a document because it is relevant to a query, because it provides background information about the document (but is not relevant to a known query), or simply because it contains useless, generic English filler. Likewise, we model each word as being relevant to one of these purposes. More specifically, our model assumes that each word can be assigned a discrete, exact source, such as \"that word is relevant to query q1\" or \"that word is generic English.\" At the sentence level, however, sentences are assigned degrees: \"This sentence is 60% about the query 1, 30% defining background information about this model, and 10% about English.\""}, {"heading": "2.3 Generative Story", "text": "The generative history of our model defines a distribution over a corpus of queries, {qj} 1: J, and documents, {dk} 1: K, as follows: 1. For each query j = 1. J: Generate each word qjn in qj by pqj (qjn) 2. For each document k = 1.. K and each sentence s in document k: (a) Select the current sentence \u03c0ks by you (\u03c0ks | \u03b1) rk (b) for each word wksn in sentence s: \u2022 Select the word source zksn in accordance with Mult (z | \u03c0ks) \u2022 Generate the word wksn by pG (wksn), if zksn = 0 pdk (wksn) (wksn), if zksn = k + 1 pqj (wksn), if zksn = 1 ltk (ltksn), if zk = 0 or zqery = 0, if this document corresponds to a component that does not correspond to the document."}, {"heading": "2.4 Graphical Model", "text": "The graphical model corresponding to this generative history can be found in Figure 1. This model represents the four known parameters in square boxes (\u03b1, pQ, pD and pG) with the three observed random variables in shaded circles (the q queries, the relevance assessments r and the words w) and two unobserved random variables in empty circles (the z indicator variables at the word level and the grade of the sentence plane \u03c0). Rounded plates denote replication: There are J queries and K documents that contain S sentences in a given document and N words in a given sentence. Common probability of the observed random variables is given in Equation (2): p (q1: J, r, d1: K) = [1: J queries and K documents that contain S sentences in a given document and N words in a given sentence. Common probability of the observed random variables is given in Equation (J, r, d1: K) and K (J, 1: K)."}, {"heading": "3 Statistical Inference in BAYESUM", "text": "The most popular are Markov Chain Monte Carlo (MCMC), the approximation of Laplace (or saddle point) and the approximation of variations. A third, rarer but very effective technique, especially for handling mixture models, is the propagation of expectations (Minka, 2001). In this paper, we will focus on the propagation of expectations; experiments not reported here have shown that variational EM is comparable, but takes about 50% longer to converge. Expectation propagation (EP) is a technique introduced by Minka (2001) as a generalization of both the propagation of faith and the assumed density filtration. In his thesis, Minka showed that EP is very effective at modelling mixing problems, and later demonstrated its superiority over variation techniques in the model of generative aspects (Minka and Lafferty, 2003)."}, {"heading": "4 Search-Engine Experiments", "text": "The first experiments we are conducting are query-focused single document summaries, where relevant documents are returned by a search engine and a brief summary of each document is desired."}, {"heading": "4.1 Data", "text": "The data we use to train and test BAYESUM comes from the Text REtrieval Conference (TREC) competitions, which consist of queries, documents and relevance assessments, just as our model requires. Queries are typically divided into four areas of increasing length: the title (3-4 words), the abstract (1 sentence), the narrative (2-4 sentences) and the concepts (a list of keywords). Obviously, one would expect that the longer the query, the better a model would be able to do this, and this is confirmed experimentally (Section 4.5). From the TREC data, we have trained our model to perform 350 queries (queries with numbers 51-350 and 401-450) and all relevant documents, amounting to approximately 43k documents, 2.1 million sentences and 65.8 million words. The mean number of relevant documents per query is 137 and the median of 968 most frequently asked documents."}, {"heading": "4.2 Evaluation Criteria", "text": "Since the number of sentences selected by human judges varies per document, precision and retrieval cannot be calculated; instead, we opt for other standard IR performance measures. We consider three related criteria: mean average precision (MAP), mean reciprocity (MRR), and precision at 2 (P @ 2). MAP is calculated by calculating the precision of each sentence as ordered by the system until all relevant sentences are selected and averaged. MRR is the reverse of the rank of the first relevant sentence. P @ 2 is the precision calculated at the first point at which two relevant sentences were selected (in the rare case that Thathumans have selected only one sentence, we use P @ 1)."}, {"heading": "4.3 Baseline Models", "text": "As baselines, we consider four Strawman models and two state-of-the-art information feedback models; the first Strawman, RANDOM, randomly evaluates sentences; the second Strawman, POSITION, evaluates sentences according to their absolute position (in the context of the non-query-focused summary, this is an incredibly powerful baseline); the third and fourth models are based on the vector space interpretation of the IR; the third model, JACCARD, uses standard Jaccard distance evaluation (intersection via union) between each sentence and the query to rank sentences; the fourth, COSINE, uses TF-IDF weighted cosine similarity; and the two state-of-the-art IR models used as comparison systems are based on the language modeling framework described in Section 2.1. These systems calculate a language model for each query and for each sentence in a document."}, {"heading": "4.4 Performance on all Query Fields", "text": "Our first evaluation compares the results using all the query fields (title, summary, description and concepts) in Table 1. As we can see from these results, the JACCARD system alone is not sufficient to exceed the position-based baseline. COSINE beats the position baseline by a small margin (5 points better in MAP, 9 points in MRR and 4 points in P @ 2) and is in turn beaten by the KL system (which is 7 points, 14 points and 4 points better in MAP, MRR and P @ 2, respectively). Blind relevance feedback (whose parameters were selected by an oracle to maximize the P @ 2 measurement) actually impairs the performance of MAP and MRR by 0.3 and 1.2 points, respectively, and increases P @ 2 by 1.5. Compared to the most powerful base system (either KL or KL + REL), BAYESUM wins with a lead of 7.5 points in MAP, 6.7 points for MRP and 4.2 for MRP @."}, {"heading": "4.5 Varying Query Fields", "text": "In Table 2, however, we show the performance of the KL, KL-REL and BAYESUM systems, because we use different query fields. In these results, there are several things to consider: First, the standard KL model performs worse in all fields without blind relevance feedback than the position-based model if only the title with 3-4 words is available. Second, if BAYESUM uses only the title, it performs better in all fields with relevance feedback than the KL model. In fact, you can use BAYESUM without using any of the query fields; in this case, only the relevance assessments are available to tell sense1A reviewers that concepts were later removed from the TREC model because they were \"too good.\" Section 4.5 considers the case to be irrelevant without the concept feedback."}, {"heading": "4.6 Noisy Relevance Judgments", "text": "Our model depends on the assumption that we have access to a collection of known relevant documents for a given query. In most real-world cases, this assumption is violated. Even in the summary of multiple documents, as performed in the DUC competitions, the assumption of access to a collection of documents, all of which are relevant to a user, is unrealistic. In the real world, we will have to deal with document collections that \"accidentally\" contain irrelevant documents. Experiments in this section show that BAYESUM is comparatively robust. For this experiment, we use the IR engine that performed best in the TREC-1 evaluation: Inquery (Callan et al., 1992) We have the offi-cial TREC results of the query of the TREC corpus that we are looking at."}, {"heading": "5 Multidocument Experiments", "text": "We present two results with BAYESUM in the settings for the summary of several documents, based on the official results of the Multilingual Summarization Evaluation (MSE) and Document Understanding Conference (DUC) competitions from 2005."}, {"heading": "5.1 Performance at MSE 2005", "text": "We participated in the Multilingual Summarization Evaluation (MSE) workshop with a system based on BAYESUM. The task of this competition was a general (non-query-oriented) summary of several documents. Fortunately, the absence of a query is not an obstacle to our model. To account for the redundancy in document collections, we used a greedy selection method that selects central sentences within the document cluster, but far from previously selected sentences (thumb \u0301 III and Marcu, 2005a). In MSE, our system performed very well. According to the human \"pyramid evaluation,\" our system came first with a score of 0.529; the next best score was 0.489. In the automatic \"base element\" evaluation, our system scored 0.00704 points (with a confidence interval of 95% of [0.0429, 0.0057]), which was the third best score on a location basis (of 10 locations) and statistically not significantly different from the best system with 081."}, {"heading": "5.2 Performance at DUC 2005", "text": "We also participated in the Document Understanding Conference (DUC). The task chosen for DUC was the query-focused summary of several documents. We entered an almost identical system to DUC, with an additional rule-based sentence compression component (thumb \u0301 III and Marcu, 2005b). Human evaluators rated both responsiveness (how well the summary answered the question) and linguistic quality. Our system achieved the highest response rate in the competition. We performed worse in linguistic quality evaluation, which was worse (only 5 out of about 30 systems), probably due to the sentence compression we built on BAYESUM. In automatic rouge-based evaluations, our system ranked between thirteenth and sixth (depending on the rouge parameters), but was never statistically significantly worse than the most powerful systems."}, {"heading": "6 Discussion and Future Work", "text": "In this paper, we have described a model for automatically generating a query-focused summary when one has access to multiple relevance assessments. Our Bayesian Query-Focused Summarization Model (BAYESUM) consistently outperforms the competing state-of-the-art information retrieval models, even if it is forced to work with significantly less information (either in terms of the complexity of the query terms or the quality of the relevance assessments). If we use our system as a standalone summary model in the 2005 MSE and DUC tasks, we achieve the highest values in the evaluation metrics. The primary weakness of the model is that it currently operates only in a purely extractive setting. A question arises: Why does BAYESUM do KL-Rel so strongly, given that BAYESUM can be regarded as Bayesian formalism for relevance feedback."}], "references": [{"title": "Latent Dirichlet allocation", "author": ["David Blei", "Andrew Ng", "Michael Jordan."], "venue": "Journal of Machine Learning Research (JMLR), 3:993\u20131022, January.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Optimization of relevance feedback weights", "author": ["Chris Buckley", "Gerard Salton."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Buckley and Salton.,? 1995", "shortCiteRegEx": "Buckley and Salton.", "year": 1995}, {"title": "The INQUERY retrieval system", "author": ["Jamie Callan", "Bruce Croft", "Stephen Harding."], "venue": "Proceedings of the 3rd International Conference on Database and Expert Systems Applications.", "citeRegEx": "Callan et al\\.,? 1992", "shortCiteRegEx": "Callan et al\\.", "year": 1992}, {"title": "Bayesian multi-document summarization at MSE", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures.", "citeRegEx": "III and Marcu.,? 2005a", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Bayesian summarization at DUC and a suggestion for extrinsic evaluation", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Document Understanding Conference.", "citeRegEx": "III and Marcu.,? 2005b", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Document language models, query models, and risk minimization for information retrieval", "author": ["John Lafferty", "ChengXiang Zhai."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Lafferty and Zhai.,? 2001", "shortCiteRegEx": "Lafferty and Zhai.", "year": 2001}, {"title": "Crosslingual relevance models", "author": ["Victor Lavrenko", "M. Choquette", "Bruce Croft."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Lavrenko et al\\.,? 2002", "shortCiteRegEx": "Lavrenko et al\\.", "year": 2002}, {"title": "Passage retrieval based on language models", "author": ["Xiaoyong Liu", "Bruce Croft."], "venue": "Processing of the Conference on Information and Knowledge Management (CIKM).", "citeRegEx": "Liu and Croft.,? 2002", "shortCiteRegEx": "Liu and Croft.", "year": 2002}, {"title": "Expectationpropagation for the generative aspect model", "author": ["Thomas Minka", "John Lafferty."], "venue": "Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI).", "citeRegEx": "Minka and Lafferty.,? 2003", "shortCiteRegEx": "Minka and Lafferty.", "year": 2003}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["Thomas Minka."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA.", "citeRegEx": "Minka.,? 2001", "shortCiteRegEx": "Minka.", "year": 2001}, {"title": "A translation model for sentence retrieval", "author": ["Vanessa Murdock", "Bruce Croft."], "venue": "Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 684\u2013", "citeRegEx": "Murdock and Croft.,? 2005", "shortCiteRegEx": "Murdock and Croft.", "year": 2005}, {"title": "Using random walks for questionfocused sentence retrieval", "author": ["Jahna Otterbacher", "Gunes Erkan", "Dragomir R. Radev."], "venue": "Proceedings of the Joint Conference on Human Language Technology Conference and Empirical Methods in Natural Lan-", "citeRegEx": "Otterbacher et al\\.,? 2005", "shortCiteRegEx": "Otterbacher et al\\.", "year": 2005}, {"title": "A language modeling approach to information retrieval", "author": ["Jay M. Ponte", "Bruce Croft."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Ponte and Croft.,? 1998", "shortCiteRegEx": "Ponte and Croft.", "year": 1998}, {"title": "Topic modeling: beyond bagof-words", "author": ["Hanna Wallach."], "venue": "Proceedings of the International Conference on Machine Learning (ICML).", "citeRegEx": "Wallach.,? 2006", "shortCiteRegEx": "Wallach.", "year": 2006}, {"title": "Novelty and redundancy detection in adaptive filtering", "author": ["Yi Zhang", "Jamie Callan", "Thomas Minka."], "venue": "Proceedings of the Conference on Research and Developments in Information Retrieval (SIGIR).", "citeRegEx": "Zhang et al\\.,? 2002", "shortCiteRegEx": "Zhang et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005).", "startOffset": 133, "endOffset": 179}, {"referenceID": 10, "context": "First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005).", "startOffset": 133, "endOffset": 179}, {"referenceID": 12, "context": "That is, BAYESUM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998).", "startOffset": 111, "endOffset": 134}, {"referenceID": 12, "context": "Documents are ranked according to how likely they make the query (Ponte and Croft, 1998).", "startOffset": 65, "endOffset": 88}, {"referenceID": 5, "context": "Other researchers have built probability distributions over queries pq(\u00b7) and ranked documents according to how likely they look under the query model: pq(d) (Lafferty and Zhai, 2001).", "startOffset": 158, "endOffset": 183}, {"referenceID": 6, "context": "A third approach builds a probability distribution pq(\u00b7) for the query, a probability distribution pd(\u00b7) for the document and then measures the similarity between these two distributions using KL divergence (Lavrenko et al., 2002):", "startOffset": 207, "endOffset": 230}, {"referenceID": 9, "context": "less common, but very effective technique, especially for dealing with mixture models, is expectation propagation (Minka, 2001).", "startOffset": 114, "endOffset": 127}, {"referenceID": 8, "context": "In his thesis, Minka showed that EP is very effective in mixture modeling problems, and later demonstrated its superiority to variational techniques in the Generative Aspect Model (Minka and Lafferty, 2003).", "startOffset": 180, "endOffset": 206}, {"referenceID": 8, "context": "Expectation propagation (EP) is an inference technique introduced by Minka (2001) as a generalization of both belief propagation and assumed density filtering.", "startOffset": 69, "endOffset": 82}, {"referenceID": 8, "context": "In the case of our model, we follow Minka and Lafferty (2003), who adapts latent Dirichlet allo-", "startOffset": 36, "endOffset": 62}, {"referenceID": 0, "context": "cation of Blei et al. (2003) to EP.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": "instead direct the interested reader to the description given by Minka and Lafferty (2003).", "startOffset": 65, "endOffset": 91}, {"referenceID": 6, "context": "Sentences are then ranked according to the KL divergence between the query model and the sentence model, smoothed against a general model estimated from the entire collection, as described in the case of document retrieval by Lavrenko et al. (2002). This is the first system we compare against, called KL.", "startOffset": 226, "endOffset": 249}, {"referenceID": 2, "context": "performed best in the TREC 1 evaluation: Inquery (Callan et al., 1992).", "startOffset": 49, "endOffset": 70}, {"referenceID": 1, "context": "Doing something similar with ad-hoc query expansion techniques is difficult due to the enormous number of parameters; see, for instance, (Buckley and Salton, 1995).", "startOffset": 137, "endOffset": 163}, {"referenceID": 14, "context": "Alternatively, one could include user models to account for novelty or user preferences along the lines of Zhang et al. (2002).", "startOffset": 107, "endOffset": 127}, {"referenceID": 11, "context": "Our model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005).", "startOffset": 69, "endOffset": 95}, {"referenceID": 13, "context": "Recent work has shown, in related models, how this can be done for moving from bag-of-words models to bag-ofngram models (Wallach, 2006); more interesting than moving to ngrams would be to move to dependency parse trees, which could likely be accounted for in a similar fashion.", "startOffset": 121, "endOffset": 136}], "year": 2013, "abstractText": "We present BAYESUM (for \u201cBayesian summarization\u201d), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a stateof-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}