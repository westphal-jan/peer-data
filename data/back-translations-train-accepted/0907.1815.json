{"id": "0907.1815", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2009", "title": "Frustratingly Easy Domain Adaptation", "abstract": "We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.", "histories": [["v1", "Fri, 10 Jul 2009 13:25:48 GMT  (35kb)", "http://arxiv.org/abs/0907.1815v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["hal daum\u00e9 iii"], "accepted": true, "id": "0907.1815"}, "pdf": {"name": "0907.1815.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["me@hal3.name"], "sections": [{"heading": null, "text": "ar Xiv: 090 7.18 15v1 [cs.LG] 1 0Ju l 2"}, {"heading": "1 Introduction", "text": "The task of domain adaptation is to develop learning algorithms that can be easily transferred from one domain to another - say, from Newswire to biomedical documents. This problem is particularly interesting in the NLP, because we are often in the situation where we have a large collection of labeled data in a \"source domain\" (say Newswire), but really aspire to a model that performs well in a second \"target domain.\" The approach we present in this paper is based on the idea of turning the domain adaptation problem into a default monitored learning problem to which any standard algorithm can be applied (e.g. maxent, SVMs, etc.) Our transformation is incredibly simple: we expand the feature space of both the source and target data and use the result as input into a standard learning algorithm. There are about two types of domain adaptation problems that have been addressed in the literature: the fully monitored case and the half of the case."}, {"heading": "2 Problem Formalization and Prior Work", "text": "To facilitate the discussion, we will first introduce some notation problems. Denote by X the input space (typically either a real vector or a binary vector), and by Y the output space. We will write Ds to denote the distribution via source examples and Dt to denote the distribution via target examples. We will assume that access to a sample Ds of source examples from the source domain, and samples Dt. \"Our goal is to learn a function h: X \u2192 Y with low expected loss of target examples from the target domain. For the purposes of the discussion, we will assume that X = RF and that Y = {1, + 1}. However, most of the techniques described in this section (as well as our own technique) are more general. There are several\" obvious \"ways to attack the domain matching problem without developing new algorithms."}, {"heading": "3 Adaptation by Feature Augmentation", "text": "In this section we will describe our approach to the domain adaptation problem. < < < < < < < < < < > > We will include each feature in the original problem and create three versions of it: a generic version, a source-specific version, and a target-specific version. Enhanced source data only contain general1For maximum entropy, SVM, and naive Bayes learning algorithms that modify the regulation term, it is easy because it appears explicit. For the Perceptron algorithm, equivalent regulation can be achieved by performing standard perceptron updates, but using (w + ws) x for making predictions and not just w x.and source-specific versions. The extended target data contain generic and target-specific versions. To express this more formally, we will first recall the notation from Section 2: X and Y, respectively, are the input domains and DDs, and DDs respectively."}, {"heading": "3.1 A Kernelized Version", "text": "It is easy to derive a kernel version of the above approach, but by deriving the kernel version we gain some insight into the method. Therefore, we will outline the derivative here. Suppose the data points x come from a reproducing kernel Hilbert space X with kernel K: X: X: X: R, positively semidefined with K. Then K can be written as the point product (in X) of two (perhaps infinitely dimensional) vectors: K (x, x:) = < \u03a6 (x:), \u0445 (x:) > X: X: X: X domain and vice versa in relation to the domain, as: S (x) = < K: X domain (x: x) when we first expand the kernel (x: x), x: x."}, {"heading": "3.2 Analysis", "text": "A more interesting statement would be that it facilitates learning along the lines of the result of (Ben-David et al., 2006) - but note that their results for the \"semi-monitored\" adaptation problem of the domain PRIOR are remarkably similar and therefore not directly applicable. Suppose we learn the weights regulated by a classification in a standard in which the SVMs analyze the fully monitored case. It turns out that the method of trait suction is remarkably similar to the PRIOR model. Suppose we learn the trait suction parameters in a classifier regulated by a norm (e.g. SVMs, maximum entropy) We can represent the sum of the \"source\" and \"general\" components of the learned weight vector, and by which the sum of the \"and\" general \"components on which we make the prediction by means of the respective weights."}, {"heading": "3.3 Multi-domain adaptation", "text": "Our formulation is agnostic about the number of \"source domains.\" In particular, the source data may actually fall into a variety of more specific domains, which is easy to explain in our model. In the case of two domains, we simply expand the attribute space to R (K + 1) F in an obvious way (the \"+ 1\" corresponds to the \"general domain,\" while each of the other 1.... K corresponds to a single task)."}, {"heading": "4 Results", "text": "In this section we describe experimental results in different areas. First we describe the tasks, then we present experimental results and finally we take a closer look at some of the experiments."}, {"heading": "4.1 Tasks", "text": "All tasks we are considering are sequence tagging tasks (either named-entity recognition, flat parsing or part-of-speech tagging) on the following records: ACE-NER. we use data from the 2005 Automatic Content Extraction task, limiting ourselves to the task of the named entity recognition task. The 2005 ACE data comes from 5 domains: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Weblog (wl), Usenet (un) and Converstaional Telephone Speech (cts).CoNLL-NE. Similar to ACE-NER., a named entity identification task is: we use the 2006 ACE data as the source domain and the 2003 CoNLL data as the target domain. PubMed-POS. A partial-speech tagging problem on PubMed."}, {"heading": "4.2 Experimental Results", "text": "The first two columns indicate the task and the domain. For tasks with only a single source and a single target, we simply report the results on the home stretch. For multi-domain adaptation tasks, we report the results for each setting of the target (where all other records are used as different \"sources\"). The next eight columns are the error rates for the task, the use of one of the different techniques (\"AUGMENT\" is our proposed technique). For each line, the error rate of the best technique performed is bold (like all techniques whose performance is not statistically significantly different). The \"T < S\" column contains one \"if TGTONLY\" plays an important role (this will soon become important)."}, {"heading": "4.3 Model Introspection", "text": "There is no explanation for this development: \"I don't think there will be such a development,\" he says, \"but I do believe there will be such a development.\" He is sure that there will be such a development: \"I don't think there will be such a development.\" He is sure that there will be such a development: \"I don't think there will be such a development.\" But he is sure that there will be such a development. \"I think so,\" he says, \"but I don't think there will be such a development.\""}, {"heading": "5 Discussion", "text": "In this paper, we have described an incredibly simple approach to domain customization that - under a common and easy-to-verify condition - surpasses previous approaches. Although it is somewhat frustrating that something so simple does so well, it is perhaps not surprising. By extending the feature space, we are essentially forcing the learning algorithm to do the customization for us. Good supervised learning algorithms have been developed over decades, so we are essentially only using all of this earlier work. Our hope is that this approach is so simple that it can be used for much more real-world tasks than we have presented here with little effort. Finally, it is very interesting to note that our method improves the surface error rate when parsing at the CoNLL section of the tree base from 5.35 to 5.11. Although this improvement is small, it is real and could be applied to full parsing. The most important possibility for future work is to develop a formal framework within which we can adapt (and) other domain models that we can monitor."}], "references": [{"title": "Analysis of representations for domain adaptation", "author": ["John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Ben.David et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2006}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al.2006] John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Adaptation of maximum entropy classifier: Little data can help a lot", "author": ["Chelba", "Acero2004] Ciprian Chelba", "Alex Acero"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chelba et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2004}, {"title": "Domain adaptation for statistical classifiers", "author": ["III Daum\u00e9", "III Marcu2006] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2006}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning Journal (submitted)", "citeRegEx": "III et al\\.,? \\Q2007\\E", "shortCiteRegEx": "III et al\\.", "year": 2007}, {"title": "Doing named entity recognition? Don\u2019t optimize for F1", "author": ["Christopher Manning"], "venue": "Post on the NLPers Blog,", "citeRegEx": "Manning.,? \\Q2006\\E", "shortCiteRegEx": "Manning.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "A more interesting statement would be that it makes learning easier, along the lines of the result of (Ben-David et al., 2006) \u2014 note, however, that their results are for the \u201csemi-supervised\u201d domain adaptation problem and so do not apply directly.", "startOffset": 102, "endOffset": 126}, {"referenceID": 1, "context": "A part-of-speech tagging problem on PubMed abstracts introduced by Blitzer et al. (2006). There are two domains: the source domain is the WSJ portion of the Penn Treebank and the target domain is PubMed.", "startOffset": 67, "endOffset": 89}, {"referenceID": 5, "context": "Second, it is arguable that a measure like F1 is inappropriate for chunking tasks (Manning, 2006).", "startOffset": 82, "endOffset": 97}], "year": 2009, "abstractText": "We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough \u201ctarget\u201d data to do slightly better than just using only \u201csource\u201d data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms stateof-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multidomain adaptation problem, where one has data from a variety of different domains.", "creator": "LaTeX with hyperref package"}}}