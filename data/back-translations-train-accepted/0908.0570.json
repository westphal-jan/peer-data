{"id": "0908.0570", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Aug-2009", "title": "The Infinite Hierarchical Factor Regression Model", "abstract": "We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis.", "histories": [["v1", "Wed, 5 Aug 2009 01:10:09 GMT  (78kb)", "http://arxiv.org/abs/0908.0570v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["piyush rai", "hal daum\u00e9 iii"], "accepted": true, "id": "0908.0570"}, "pdf": {"name": "0908.0570.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["piyush@cs.utah.edu", "hal@cs.utah.edu"], "sections": [{"heading": null, "text": "ar Xiv: 090 8.05 70v1 [cs.LG] 5 August 2We propose a non-parametric Bayesian factor regression model that takes into account uncertainty in the number of factors and the relationship between the factors. To achieve this, we propose a sparse variant of the Indian buffet process and couple it with a hierarchical model of factors based on Kingman's merger. We apply this model to two problems (factor analysis and factor regression) in gene expression data analysis."}, {"heading": "1 Introduction", "text": "Factor analysis is the task of explaining data using a number of latent factors. Factor regression combines this analysis with a predictive task in which predictions are made solely on the basis of factor representation, and latent factor representation achieves a twofold benefit: (1) discovery of the latent process underlying the data; (2) simplified predictive modelling through compact data representation. In particular, (2) is motivated by the problem of predicting in the \"large P-small N\" paradigm [1], where the number of characteristics underlying the data significantly exceeds the number of examplesN, potentially leading to overmatch. We address three fundamental deficiencies in standard factor analysis [2, 3, 4, 1] we do not assume a known number of factors; (2) we do not assume that factors are independent; (3) we do not assume that all characteristics are relevant to factor analysis."}, {"heading": "2 Background", "text": "Our model uses a variant of the Indian buffet process to model the trait-factor (i.e. gene-pathway) relationships. We also use Kingman's merger to model latent path hierarchies."}, {"heading": "2.1 Indian Buffet Process", "text": "The Indian buffet process [7] defines a distribution via infinite binary matrices, originally motivated by the need to model the latent factor structure of a given group of observations. In the standard form, it is parameterized by a scale value, \u03b1, which can be explained by a simple culinary analogy. Customers (in our context, genes) enter an Indian restaurant and select dishes (in our context, paths) from an infinite range of dishes. The first customer i then selects additional poisson (\u03b1 / i) new dishes. Then, each incoming customer i selects a previously selected dish k with a probability mk / (i \u2212 1), where mk is the number of previous customers who have chosen a dish k. The customer i then selects an additional poisson (\u03b1 / i) new dishes. We can easily define a binary matrix Z with a value Zik = 1 precisely when the customer i \u2212 chablices. This stoic process thus defines a matrix distribution via binary binary."}, {"heading": "2.2 Kingman\u2019s Coalescent", "text": "Our model uses a latent hierarchical structure over factors; we use Kingman's coalescence [6] as a convenient prior distribution over hierarchies. Kingman's coalescence arose in the study of population genetics for a number of single-parent organisms. Coalescence is a non-parametric model over a countable set of organisms. It is easiest to understand in terms of its finite dimensional boundary distributions over n individuals, in which case it is referred to as the n-coalescent model. We then take the boundary n \u2192 \u221e. In our case, the individuals are factors.The n-coalescence considers a population of n-dimensional boundary distributions in time t = 0. We follow the ancestry of these individuals backwards in the time where each organism develops exactly one parent at the time t < 0. The n-coalescent is a continuous time that begins with n-rated Markov processes, which develops backwards with cluster = n."}, {"heading": "3 Nonparametric Bayesian Factor Regression", "text": "Remember the problem of standard factor analysis: X = AF + E, for standardized data X. X is a P \u00b7 N matrix consisting of N samples [x1,..., xN] of P characteristics each. A is the factor load matrix of size P \u00b7 K and F = [f1,..., fN] is the factor matrix of size K \u00b7 N. E = [e1,..., eN] is the matrix of idiosyncratic variations. K, the number of factors, is known. Remember that our goal is to treat the problem of factor analysis non-parametrically, to model characteristic relevance and to model hierarchical factors. For expositional reasons, it is easiest to deal with each of these problems one after the other. In our context, we start with the modeling of the gene factor relationship non-parametrically (using IBP). Next, we propose a variant of the gene to model the IBP."}, {"heading": "3.1 Nonparametric Gene-Factor Model", "text": "Although IBP has been applied to non-parametric factor analysis in the past [5], the standard formulation of IBP puts IBP before the factor matrix (F), which associates samples (i.e. a set of characteristics) with factors that are sparse. In such a context, each sample depends on all factors, but each gene within a sample usually depends only on a small number of factors. Therefore, it is more appropriate to model the factor load matrix (A) with IBP. Note that A and F are related via the number of factors K, with IBP usually only a small number of factors.For most gene expression factors, we have an adriatic element (I element)."}, {"heading": "3.2 Feature Selection Prior", "text": "A more realistic model is that certain genes simply do not participate in factor analysis: for a culinary analogy, the genes enter the restaurant and leave before selecting any dishes. We call those genes that \"leave\" \"fake.\" We add an additional preceding term to account for such fake genes; which effectively leads to a sparse solution (above the ranks of the IBP matrix). It is important to note that this notion of sparseness is fundamentally different from the conventional notion of sparseness in the IBP. To see the difference, one must remember that the IBP contains an \"enriching\" phenomenon: Frequently selected factors are more likely to be re-elected. Consider a truly unclean gene and wonder if it is likely to select any factors."}, {"heading": "3.3 Hierarchical Factor Model", "text": "In our basic model, each column of the matrix Z (and the corresponding column in V) is associated with a factor that is considered unrelated. To model the fact that factors are actually recalculated, we introduce a factor hierarchy. Kingman's merger [6] is an attractive prerequisite for integration with IBP for several reasons. It is not parametric and describes interchangeable distributions, meaning that it can model a different number of factors. In addition, efficient inference algorithms exist [8]."}, {"heading": "3.4 Full Model and Extension to Factor Regression", "text": "Our proposed graphical model is shown in Figure 1. Key aspects of this model are: the IBP before over Z, the sparse binary vector T and the coalescence before over V. In the standard Bayean factor regression [1], the factor analysis is followed by the regression task. Regression is only based on F and not on the complete data X. A simple linear regression problem would, for example, involve the estimation of a K-dimensional parameter vector with regression value \u03b8 F. Our model, on the other hand, integrates the factor regression component into the non-parametric factor analysis framework itself. We do this by anticipating the yi responses to the expression vector xi and combining the training and test data (see Figure 2). The unknown responses in the test data are treated as missing variables, which are iteratively implied in our MCMC inference method, and it is easy to see the reactions of the model to further adapt to the problem."}, {"heading": "4 Inference", "text": "Sampling (sampling) Sampling (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling) (sampling)"}, {"heading": "5 Related Work", "text": "In the past, a number of probabilistic approaches have been proposed for the problem of generegulatory network reconstruction, most of which assume that the number of factors is known. To avoid this, one can select models using Reversible Jump MCMC [10] or Evolutionary Stochastic Model Search [11]. A somewhat similar approach to ours is the Infinite Model of Independent Component Analysis (iICA) of [12], which treats factor analysis as a special case of ICA. However, its model is limited to factor analysis and does not take into account feature selection, factor hierarchy and factor regression. As a generalization to the standard ICA model, a model is proposed in which the components can be fixed using a structural model."}, {"heading": "6 Experiments", "text": "In this section we report on our results on synthetic and real datasets. We compare our non-parametric approach with the evolutionary search-based approach proposed in [11], which represents the non-parametric extension to BFRM. We used the E-coli network gene factor connectivity matrix (described in [14]) to generate a synthetic dataset with 100 samples of 50 genes and 8 underlying factors. Knowing the basic truth for factor loads in this case, this dataset was ideal to test the effectiveness of factor loads (binding sites and number of factors)."}, {"heading": "6.1 Nonparametric Gene-Factor Modeling and Variable Selection", "text": "For the synthetic dataset generated by the E-coli network, the results are shown in Figure 4, comparing the actual network used to generate the data with the derived factor charge matrix. As shown in Figure 4, we have recovered exactly the same number (8) factors and almost exactly the same factor charges (binding sites and number of factors) as the basic truth. In comparison, the evolutionary search approach clearly overestimated the number of factors and the derived loads (even module column permutations) in each sample. Our results on real data are shown in Figure 5. To see the effect of variable selection on these data, we also introduced false genes by adding 50 random features to each sample. We observed the following: (1) Without variable selection enabled, contaminated genes lead to an overestimated number of factors and erroneously detected factor charges for these data, we do not overestimate the number of selective factors (see Figure 5)."}, {"heading": "6.2 Hierarchical Factor Modeling", "text": "As shown, the model correctly derives the gene factor associations, the number of factors, and the factor hierarchy. There are several ways to interpret the hierarchy: from the factor hierarchy for E coli data (Figure 6), column 2 (corresponding to factor 2) of the V matrix is the most prominent (it regulates the highest number of genes) and the tree root is the closest, followed by column 2, to which it appears most closely. Columns that correspond to less prominent factors are located further down the hierarchy (with corresponding kinship). Figure 6 (d) can similarly be interpreted for breast cancer data. The hierarchy can be used to find factors in the order of their prominence. The higher we truncate the tree along the hierarchy, the more prominent are the factors we discover."}, {"heading": "6.3 Factor Regression", "text": "We report factor regression results for binary and real responses and compare both variants of our model (Gaussian V and coalescence V) with three different approaches: logistic regression, BFRM, and adjustment of a separate prediction model to the factors detected (see Figure 7 (c)). The Breast Cancer dataset featured two binary response variables (phenotypes) associated with each sample. For this binary prediction task, we divided the data into training groups of 151 samples and test groups of 100 samples. This is essentially a transduction setting, as described in Section 3.4 and in Figure 2. For real prediction tasks, we treated a 30x20 block of the data matrix as our held data and predicted it based on the remaining entries in the matrix. This method of evaluation is similar to the task of image reconstruction [15]. Results are averaged over 20 random initializations, and the variances indicate that our method is fairly low."}, {"heading": "7 Conclusions and Discussion", "text": "We have presented a completely non-parametric Bayesian approach to sparse factor regression by modelling the gene-factor relationship using a sparse variant of IBP. However, the true power of non-parametric priors is reflected in the easy integration of task-specific models into the framework. Both gene selection and hierarchical factor modeling are simple extensions in our model that do not significantly complicate the inference process but result in improved model performance and more understandable results. We have applied Kingman's coalescence as a hierarchical model to V, the matrix that modulates the expression level of genes into factors. An interesting open question is whether the IBP itself can be modelled hierarchically."}], "references": [{"title": "Bayesian Factor Regression Models in the \u201cLarge p, Small n", "author": ["M. West"], "venue": "Paradigm. In Bayesian Statistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Bayesian Sparse Hidden Components Analysis for Transcription Regulation Networks", "author": ["C. Sabatti", "G. James"], "venue": "Bioinformatics 22,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Probabilistic Inference of Transcription Factor Concentrations and Gene-specific", "author": ["G. Sanguinetti", "N.D. Lawrence", "M. Rattray"], "venue": "Regulatory Activities. Bioinformatics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A Bayesian Approach to Reconstructing Genetic Regulatory Networks with Hidden Factors", "author": ["M.J. Beal", "F. Falciani", "Z. Ghahramani", "C. Rangel", "D.L. Wild"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Bayesian Nonparametric Latent Feature Models", "author": ["Z. Ghahramani", "T.L. Griffiths", "P. Sollich"], "venue": "In Bayesian Statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "The coalescent", "author": ["J.F.C. Kingman"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Infinite Latent Feature Models and the Indian Buffet Process", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Bayesian Agglomerative Clustering with Coalescents", "author": ["Y.W. Teh", "H. Daum\u00e9 III", "D.M. Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Modeling Dyadic Data with Binary Latent Factors", "author": ["E. Meeds", "Z. Ghahramani", "R.M. Neal", "S.T. Roweis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Reversible jump markov chain monte carlo computation and bayesian model determination", "author": ["P. Green"], "venue": "Biometrica 82,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "High-Dimensional Sparse Factor Modelling - Applications in Gene Expression Genomics", "author": ["C. Carvalho", "J. Lucas", "Q. Wang", "J. Chang", "J. Nevins", "M. West"], "venue": "JASA,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Infinite Sparse Factor Analysis and Infinite Independent Components Analysis", "author": ["D. Knowles", "Z. Ghahramani"], "venue": "ICA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Beyond independent components: trees and clusters", "author": ["Francis R. Bach", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Factor Analysis for Gene Regulatory Networks and Transcription Factor Activity Profiles", "author": ["I. Pournara", "L. Wernisch"], "venue": "BMC Bioinformatics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Non-linear CCA and PCA by Alignment of Local Models", "author": ["J.J. Verbeek", "S.T. Roweis", "N. Vlassis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "In particular, (2) is motivated by the problem of prediction in the \u201clarge P small N\u201d paradigm [1], where the number of featuresP greatly exceeds the number of examplesN , potentially resulting in overfitting.", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 2, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 3, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 0, "context": "We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1) we do not assume a known number of factors; (2) we do not assume factors are independent; (3) we do not assume all features are relevant to the factor analysis.", "startOffset": 81, "endOffset": 93}, {"referenceID": 4, "context": "In particular, we treat the gene-tofactor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process (IBP) [5], designed to account for the sparsity of relevant genes (features).", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "A natural choice is Kingman\u2019s coalescent [6], a popular distribution over infinite binary trees.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "1 Indian Buffet Process The Indian Buffet Process [7] defines a distribution over infinite binary matrices, originally motivated by the need to model the latent factor structure of a given set of observations.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "It turn out [7] that the stochastic process defined above corresponds to an infinite limit of an exchangeable process over finite matrices with K columns.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "2 Kingman\u2019s Coalescent Our model makes use of a latent hierarchical structure over factors; we use Kingman\u2019s coalescent [6] as a convenient prior distribution over hierarchies.", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "[8] proposed efficient bottom-up agglomerative inference algorithms for the coalescent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Although IBP has been applied to nonparametric factor analysis in the past [5], the standard IBP formulation places IBP prior on the factor matrix (F) associating samples (i.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "For most gene-expression problems [1], a binary factor loadings matrix (A) is inappropriate.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Kingman\u2019s coalescent [6] is an attractive prior for integration with IBP for several reasons.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Moreover, efficient inference algorithms exist [8].", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "In standard Bayesian factor regression [1], factor analysis is followed by the regression task.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "We accept the proposal with an acceptance probability (following [9]) given by a = min{1, p(rest|\u03b7 \u2217) p(rest|\u03b7) }.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Sampling the Factor Tree: Use the Greedy-Rate1 algorithm [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 2, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 3, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 0, "context": "5 Related Work A number of probabilistic approaches have been proposed in the past for the problem of generegulatory network reconstruction [2, 3, 4, 1].", "startOffset": 140, "endOffset": 152}, {"referenceID": 1, "context": "Some take into account the information on the prior network topology [2], which is not always available.", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "To get around this, one can perform model selection via Reversible Jump MCMC [10] or evolutionary stochastic model search [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "To get around this, one can perform model selection via Reversible Jump MCMC [10] or evolutionary stochastic model search [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "A somewhat similar approach to ours is the infinite independent component analysis (iICA) model of [12] which treats factor analysis as a special case of ICA.", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "As a generalization to the standard ICA model, [13] proposed a model in which the components can be related via a tree-structured graphical model.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "no hierarchy over factors) is most similar to the Bayesian Factor Regression Model (BFRM) of [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "We compare our nonparametric approach with the evolutionary search based approach proposed in [11], which is the nonparametric extension to BFRM.", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "We used the gene-factor connectivity matrix of E-coli network (described in [14]) to generate a synthetic dataset having 100 samples of 50 genes and 8 underlying factors.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "This method of evaluation is akin to the task of image reconstruction [15].", "startOffset": 70, "endOffset": 74}], "year": 2009, "abstractText": "We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman\u2019s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}