{"id": "0909.4588", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2009", "title": "Discrete MDL Predicts in Total Variation", "abstract": "The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.", "histories": [["v1", "Fri, 25 Sep 2009 02:57:17 GMT  (18kb)", "http://arxiv.org/abs/0909.4588v1", "15 LaTeX pages"]], "COMMENTS": "15 LaTeX pages", "reviews": [], "SUBJECTS": "math.PR cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "authors": ["marcus hutter"], "accepted": true, "id": "0909.4588"}, "pdf": {"name": "0909.4588.pdf", "metadata": {"source": "CRF", "title": "Discrete MDL Predicts in Total Variation", "authors": ["Marcus Hutter"], "emails": ["RSISE@ANU", "SML@NICTA", "marcus@hutter1.net"], "sections": [{"heading": null, "text": "ar Xiv: 090 9.45 88v1 [m. ath. PR] 2Table of Contents"}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Facts, Insights, Problems 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Notation and Main Result 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Proof for Finite Model Class 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Proof for Countable Model Class 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Implications 12", "text": "7 Variations 14 References 14Keyword minimum description length; countable model class; total variation distance; sequence prediction; discriminatory learning; reinforcement learning."}, {"heading": "1 Introduction", "text": "Minimum Description Length (MDL) is considered in this paper as the most serious problem in this category."}, {"heading": "2 Facts, Insights, Problems", "text": "Before we start with the formal development, we describe how MDL and Bayes function in some restricted settings: Q = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 2. Qi = 1. Qi = 2. Q2 = 2. Qi = 2. Qi = 2. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 1. Qi = 2. Qi = 2. Qi = 2. Qi = 2. Qi = 2. Qi = 2. Qi = 2. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 3. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi = 4. Qi ="}, {"heading": "3 Notation and Main Result", "text": "Formal development starts with this section. We need probability measures and filters for infinite quantum sources. (Q) Q = Q = Q (Q) Q = Q (Q) Q (Q) Q = Q (Q) Q (Q) Q (Q) Q = Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) Q) Q (Q) Q (Q) X) x (P) x (P) x x x x (P) P (P) x x x x x (P) x x (P) x (P) x (P) x x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P) x (P (P) x (P) x (P (P) x (P) x (P) x (P) x (P) x (P (P) x (P) x (P (P) x (P) x (P (P) x (P) x (P) x (Q (Q (Q (Q) x) Q (Q (Q) Q (Q) Q) Q (Q (Q) Q (Q (Q (Q) Q) Q) Q (Q (Q) Q (Q (Q) Q) Q (Q (Q (Q) Q (Q) Q (Q) Q (Q (Q) Q (Q) Q (Q (Q) Q (Q) Q (Q (Q"}, {"heading": "4 Proof for Finite Model Class", "text": "We first prove theorem 1 for finite model classes M. For this we need the following definition and Lemma: Q = 1. Q = 1. Q: Definition 2 (relations between Q and P) For all probability variables Q and P, let us turn Qr + Qs = Q into an absolutely continuous non-negative measurement variable Qr \u00b2 P and a singular non-negative measurement variable Qs \u00b2 P. \u2022 g (P): = dQr / dP = lim \u2192 Q (x1:) / P (x1:)] become (a version of) the radon nicodymium derivative, i.e. Qr [A] = implicit P = implicit P = lims \u2192 Q (x1:) / P (x1:)."}, {"heading": "5 Proof for Countable Model Class", "text": "The proof in the previous section is that the finiteness of M. We want to prove that the probability that MDL (Q = > Q = > Q) is \"complex\" (Q = > Q) is small."}, {"heading": "6 Implications", "text": "Due to their universality, Theorem 1 can be applied to many problem classes."}, {"heading": "7 Variations", "text": "MDL is more of a general principle for model selection than a uniquely defined method. For example, there are raw and refined MDL [Gree07], the related MML principle [Wal05], a static, a dynamic, and a hybrid prediction of MDL for prediction [PH05], and other variations. For our setup, we could have defined the multi-stage prediction as a product of single-stage predictions: MDLI (x1:): = t = 1MDLx < t (xt | x < t), MDLI (z | x) = MDLI (xz) / MDLI (x), which is a more incremental version of MDL. Both MDLx and MDLI are \"static\" in the sense of [PH05], and each allows a dynamic and a hybrid version. Due to its incremental nature, MDLI probably has better predictable properties than MDLx."}], "references": [{"title": "Learning from non-IID data: Theory, Algorithms and Practice (LNIDD\u201909)", "author": ["M.-R. Amini", "A. Habrard", "L. Ralaivola", "N. Usunier", "editors"], "venue": "Bled, Slovenia,", "citeRegEx": "Amini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2009}, {"title": "Logically Smooth Density Estimation", "author": ["A.R. Barron"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Barron.,? \\Q1985\\E", "shortCiteRegEx": "Barron.", "year": 1985}, {"title": "Minimum complexity density estimation", "author": ["A.R. Barron", "T.M. Cover"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Barron and Cover.,? \\Q1991\\E", "shortCiteRegEx": "Barron and Cover.", "year": 1991}, {"title": "Merging of opinions with increasing information", "author": ["D. Blackwell", "L. Dubins"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Blackwell and Dubins.,? \\Q1962\\E", "shortCiteRegEx": "Blackwell and Dubins.", "year": 1962}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "The Minimum Description Length Principle", "author": ["P.D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald.,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2007}, {"title": "Convergence and loss bounds for Bayesian sequence prediction", "author": ["M. Hutter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Hutter.,? \\Q2003\\E", "shortCiteRegEx": "Hutter.", "year": 2003}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2005\\E", "shortCiteRegEx": "Hutter.", "year": 2005}, {"title": "On universal prediction and Bayesian confirmation", "author": ["M. Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "Machine Learning: Discriminative and Generative", "author": ["T. Jebara"], "venue": null, "citeRegEx": "Jebara.,? \\Q2003\\E", "shortCiteRegEx": "Jebara.", "year": 2003}, {"title": "Weak and strong merging of opinions", "author": ["E. Kalai", "E. Lehrer"], "venue": "Journal of Mathematical Economics,", "citeRegEx": "Kalai and Lehrer.,? \\Q1994\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1994}, {"title": "Discriminative learning can succeed where generative learning fails", "author": ["P. Long", "R. Servedio", "H.U. Simon"], "venue": "Information Processing Letters,", "citeRegEx": "Long et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Long et al\\.", "year": 2007}, {"title": "Probability captures the logic of scientific confirmation", "author": ["P. Maher"], "venue": "Contemporary Debates in Philosophy of Science,", "citeRegEx": "Maher.,? \\Q2004\\E", "shortCiteRegEx": "Maher.", "year": 2004}, {"title": "Asymptotics of discrete MDL for online prediction", "author": ["J. Poland", "M. Hutter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Poland and Hutter.,? \\Q2005\\E", "shortCiteRegEx": "Poland and Hutter.", "year": 2005}, {"title": "Artificial Intelligence. A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig.,? \\Q2003\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2003}, {"title": "Characterizing predictable classes of processes", "author": ["D. Ryabko"], "venue": "In Proc. 25th Conference on Uncertainty in Artificial Intelligence (UAI\u201909),", "citeRegEx": "Ryabko.,? \\Q2009\\E", "shortCiteRegEx": "Ryabko.", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Statistical and Inductive Inference by Minimum Message Length", "author": ["C.S. Wallace"], "venue": null, "citeRegEx": "Wallace.,? \\Q2005\\E", "shortCiteRegEx": "Wallace.", "year": 2005}, {"title": "Best-response multiagent learning in nonstationary environments", "author": ["M. Weinberg", "J.S. Rosenschein"], "venue": "In Proc. 3rd International Joint Conf. on Autonomous Agents & Multi Agent Systems", "citeRegEx": "Weinberg and Rosenschein.,? \\Q2004\\E", "shortCiteRegEx": "Weinberg and Rosenschein.", "year": 2004}], "referenceMentions": [], "year": 2013, "abstractText": "The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.", "creator": "LaTeX with hyperref package"}}}