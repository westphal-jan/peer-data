{"id": "0910.2279", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2009", "title": "Positive Semidefinite Metric Learning with Boosting", "abstract": "The learning of appropriate distance metrics is a critical problem in image classification and retrieval. In this work, we propose a boosting-based technique, termed \\BoostMetric, for learning a Mahalanobis distance metric. One of the primary difficulties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semidefinite. Semidefinite programming is sometimes used to enforce this constraint, but does not scale well. \\BoostMetric is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. \\BoostMetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.", "histories": [["v1", "Tue, 13 Oct 2009 00:54:31 GMT  (545kb,D)", "http://arxiv.org/abs/0910.2279v1", "11 pages, Twenty-Third Annual Conference on Neural Information Processing Systems (NIPS 2009), Vancouver, Canada"]], "COMMENTS": "11 pages, Twenty-Third Annual Conference on Neural Information Processing Systems (NIPS 2009), Vancouver, Canada", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["chunhua shen", "junae kim", "lei wang 0001", "anton van den hengel"], "accepted": true, "id": "0910.2279"}, "pdf": {"name": "0910.2279.pdf", "metadata": {"source": "CRF", "title": "Positive Semidefinite Metric Learning with Boosting", "authors": ["Chunhua Shen", "Junae Kim", "Lei Wang", "Anton van den Hengel"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to understand themselves and what they are doing to change the world. (...) Most of them are able to understand themselves. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world. (...) Most of them are able to change the world."}, {"heading": "2 Algorithms", "text": "In this section we define the mathematical problems (((P0), (P1)) that we want to solve. In order to derive an efficient optimization strategy, we also examine the dual problem ((D1)) from the point of view of convex optimization."}, {"heading": "2.1 Distance Metric Learning", "text": "(As discussed, the Mahalanobis metric is equivalent to a linear transformation of the data by a projection matrix L-RD-D (usually D-RD) before calculating the standard Euclidean distance: dist2ij-L-ai-L-aj-22-LL-LL-J-J-J-J-J-J-J. (1) Although you can learn L directly, as many conventional approaches do, there are non-conventional constraints involved in this constellation that make the problem difficult to solve. Instead, as we will show, a new variable X-LL-J-J-J-J-J-J. This technique has been widely applied in conventional optimization and machine learning, e.g. [12]. If X-I-J-I-I, it reduces to euclidean distance. If X is diagonal, the problem corresponds to learning a metric in which different characteristics are weighted differently."}, {"heading": "2.2 Learning with Exponential Loss", "text": "We derive a general algorithm for exponential loss p.s.d. matrix learning because the logarithmic problem is J = large. (Suppose we want to find a p.s.d. matrix X < 0 so that a bunch of constraints < Ar, X > > 0, r = 1, 2, \u00b7 \u00b7, are as well satisfied as possible. These constraints do not all have to be strictly met. We can define the margin < Ar, X >, 0,. (P0) Note that: (1) We have worked on the logarithmic version of the sum of exponential loss."}, {"heading": "2.3 The Lagrange Dual Problem", "text": "We derive the Lagrange-dual problem that interests us. (P0) We have the problem (P0). (P1) To solve this problem, we write the LagrangianL (w, p). (P1). (P2). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (P3). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5. (5). (5). (5). (5. (5). (5). (5). (5). (5. (5). (5). (5). (5). (5. (5). (5. (5). (5). (5). (5). (5). (5. (5). (5). (5). (5. (5). (5). (5). (5). (5). (5)."}, {"heading": "2.4 Coordinate Descent Optimization", "text": "We show how to derive an AdaBoost-like optimization procedure for our metric learning problem. As in AdaBoost, we have to search for the primary variables wj \u2212 \u2212 all weak learners until iteration j.Algorithm 1 Bisection search for wj.Input: An interval [wl, wu] is known to contain the optimal value of wj and convergence tolerance > 0. repeat1 \u00b7 wj = 0.5 (wl + wu); 2 \u00b7 if l.h.s of (8) > 0 then3 wl = wj; 4else5 wu = wj.6until wu \u2212 root tolerance \u03b5 > 0. repeat1 \u00b7 wj.Optimization for wj.. a Since we are interested in one-a-time coordinate optimization, we keep w1, w2,., wj \u2212 1 fixed for solving the primary problem."}, {"heading": "2.5 Base Learning Algorithm", "text": "In this section we show that the optimization problem (7) can be solved precisely and efficiently by eigenvalue decomposition (EVD). From Z < 0 and rank (Z) = 1 we know that Z has the format: Z = 1 >, and Tr (Z) = 1 means that it is 2 = 1. By naming A = 1, S | r = 1ur Ar, Z 2 = 1, Z 2 = 1, 1 = 1. It is clear that the largest eigenvalue of A, max (A), and the corresponding eigenvector 1, provides the solution to the above problem."}, {"heading": "3 Experiments", "text": "In this section we present experiments on data visualization, classification and image recovery."}, {"heading": "3.1 An Illustrative Example", "text": "In Fig. 1, we show a data visualization problem on an artificial toy data set (concentric circles). The data set has four classes; the first two dimensions follow concentric circles, while the left eight dimensions are all random Gaussian noise. In this experiment, 9000 triplets are generated for training purposes. If the level of noise is large, PCA does not find the first two informative dimensions. LDA also fails because clearly not every class follows a Gaussian deflection and its centers overlap in the same place. The proposed BOOSTMETRIC algorithm finds the informative features. The eigenvalues of X that BOOSTMETRIC has learned are {0.542, 0.414, 0.007, 0, \u00b7 \u00b7 \u00b7 \u00b7, 0}, indicating that BOOSTMETRIC successfully detects the underlying structure of the data."}, {"heading": "3.2 Classification on Benchmark Datasets", "text": "This year it is more than ever before."}, {"heading": "3.3 Visual Object Categorization and Detection", "text": "In fact, the fact is that most of them are able to go to another world, in which they are able to go, rather than to another world, in which they are able, in which they are able, in which they are able to move."}, {"heading": "4 Conclusion", "text": "We have generalized AdaBoost in the sense that the weak learner of BOOSTMETRIC is a matrix rather than a classifier. Our algorithm is simple and efficient. Experiments show its better performance compared to a few current metric learning methods. We are currently combining the idea of online learning into BOOSTMETRIC to handle even larger data sets."}], "references": [{"title": "Discriminant adaptive nearest neighbor classification", "author": ["T. Hastie", "R. Tibshirani"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Distance learning for similarity estimation", "author": ["J. Yu", "J. Amores", "N. Sebe", "P. Radeva", "Q. Tian"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Metric learning using Iwasawa decomposition", "author": ["B. Jian", "B.C. Vemuri"], "venue": "In Proc. IEEE Int. Conf. Comp. Vis.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russell"], "venue": "In Proc. Adv. Neural Inf. Process. Syst. MIT Press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Learning a Mahalanobis metric from equivalence constraints", "author": ["A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Neighbourhood component analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In Proc. Adv. Neural Inf. Process. Syst. MIT Press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In Proc. Adv. Neural Inf. Process. Syst.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "In Proc. Adv. Neural Inf. Process. Syst.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "PSDBoost: Matrix-generation linear programming for positive semidefinite matrices learning", "author": ["C. Shen", "A. Welsh", "L. Wang"], "venue": "Proc. Adv. Neural Inf. Process. Syst.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Theoretical views of boosting and applications", "author": ["R.E. Schapire"], "venue": "In Proc. Int. Conf. Algorithmic Learn. Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Unsupervised learning of image manifolds by semidefinite programming", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Int. J. Comp. Vis.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Linear programming boosting via column generation", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": "Mach. Learn.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "L-BFGS-B: Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "J. Nocedal"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval", "author": ["L. Yang", "R. Jin", "L. Mummert", "R. Sukthankar", "A. Goode", "B. Zheng", "S. Hoi", "M. Satyanarayanan"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. IEEE computer Society Digital Library,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Boosting as a regularized path to a maximum margin classifier", "author": ["S. Rosset", "J. Zhu", "T. Hastie"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "CSDP, a C library for semidefinite programming", "author": ["B. Borchers"], "venue": "Optim. Methods and Softw.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Scale & affine invariant interest point detectors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "Int. J. Comp. Vis.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Comp. Vis.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].", "startOffset": 170, "endOffset": 185}, {"referenceID": 1, "context": "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].", "startOffset": 170, "endOffset": 185}, {"referenceID": 2, "context": "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].", "startOffset": 170, "endOffset": 185}, {"referenceID": 3, "context": "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].", "startOffset": 170, "endOffset": 185}, {"referenceID": 4, "context": "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].", "startOffset": 170, "endOffset": 185}, {"referenceID": 2, "context": "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.", "startOffset": 37, "endOffset": 55}, {"referenceID": 3, "context": "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.", "startOffset": 37, "endOffset": 55}, {"referenceID": 4, "context": "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.", "startOffset": 37, "endOffset": 55}, {"referenceID": 5, "context": "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.", "startOffset": 37, "endOffset": 55}, {"referenceID": 6, "context": "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.", "startOffset": 37, "endOffset": 55}, {"referenceID": 7, "context": "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.", "startOffset": 37, "endOffset": 55}, {"referenceID": 3, "context": "Xing et al [4] firstly proposed to learn a Mahalanobis metric for clustering using convex optimization.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "Neighborhood component analysis (NCA) [6] and large margin nearest neighbor (LMNN) [7] learn a metric by maintaining consistency in data\u2019s neighborhood and keeping a large margin at the boundaries of different classes.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Neighborhood component analysis (NCA) [6] and large margin nearest neighbor (LMNN) [7] learn a metric by maintaining consistency in data\u2019s neighborhood and keeping a large margin at the boundaries of different classes.", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "It has been shown in [7] that LMNN delivers the state-of-the-art performance among most distance metric learning algorithms.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "The work of LMNN [7] and PSDBoost [9] has directly inspired our work.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "The work of LMNN [7] and PSDBoost [9] has directly inspired our work.", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "While the formulation of LMNN looks more similar to support vector machines (SVM\u2019s) and PSDBoost to LPBoost, our algorithm, termed BOOSTMETRIC, largely draws upon AdaBoost [10].", "startOffset": 172, "endOffset": 176}, {"referenceID": 5, "context": "In many cases, it is difficult to find a global optimum in the projection matrix L [6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "Reformulationlinearization is a typical technique in convex optimization to relax and convexify the problem [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 3, "context": ", [4, 7, 12, 8].", "startOffset": 2, "endOffset": 15}, {"referenceID": 6, "context": ", [4, 7, 12, 8].", "startOffset": 2, "endOffset": 15}, {"referenceID": 11, "context": ", [4, 7, 12, 8].", "startOffset": 2, "endOffset": 15}, {"referenceID": 7, "context": ", [4, 7, 12, 8].", "startOffset": 2, "endOffset": 15}, {"referenceID": 6, "context": "Alternative projected (sub-)gradient is adopted in [7, 4, 8].", "startOffset": 51, "endOffset": 60}, {"referenceID": 3, "context": "Alternative projected (sub-)gradient is adopted in [7, 4, 8].", "startOffset": 51, "endOffset": 60}, {"referenceID": 7, "context": "Alternative projected (sub-)gradient is adopted in [7, 4, 8].", "startOffset": 51, "endOffset": 60}, {"referenceID": 8, "context": "PSDBoost [9] converts the particular semidefinite program in metric learning into a sequence of linear programs (LP\u2019s).", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Based on the observation from [9] that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices, we propose BOOSTMETRIC for learning a p.", "startOffset": 30, "endOffset": 33}, {"referenceID": 11, "context": "This technique has been used widely in convex optimization and machine learning such as [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "Weak and strong duality hold under mild conditions [11].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "While it is possible to devise a totally-corrective column generation based optimization procedure for solving our problem as the case of LPBoost [13], we are more interested in considering one-ata-time coordinate-wise descent algorithms, as the case of AdaBoost [10], which has the advantages: (1) computationally efficient and (2) parameter free.", "startOffset": 146, "endOffset": 150}, {"referenceID": 9, "context": "While it is possible to devise a totally-corrective column generation based optimization procedure for solving our problem as the case of LPBoost [13], we are more interested in considering one-ata-time coordinate-wise descent algorithms, as the case of AdaBoost [10], which has the advantages: (1) computationally efficient and (2) parameter free.", "startOffset": 263, "endOffset": 267}, {"referenceID": 13, "context": "Off-the-shelf software like LBFGSB [14] can be used for this purpose.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "Also see [9] for details.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "We have used the same mechanism to generate training triplets as described in [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 14, "context": "LMNN is one of the state-of-the-art according to recent studies such as [15].", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "Experiment setting for LMNN follows [7].", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "LMNN\u2019s results are consistent with those given in [7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "(3) Xing et al [4] and NCA can only handle a few small datasets.", "startOffset": 15, "endOffset": 18}, {"referenceID": 15, "context": "The coordinatewise gradient descent optimization strategy of AdaBoost leads to an `1-norm regularized maximum margin classifier [17].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "Results of NCA and Xing et al [4] on large datasets are not available either because the algorithm does not converge or due to the out-of-memory problem.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "dataset Euclidean Xing et al [4] RCA NCA LMNN BOOSTMETRIC 1 USPS-1 5.", "startOffset": 29, "endOffset": 32}, {"referenceID": 16, "context": "Note that LMNN is much faster than SDP solvers like CSDP [18].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "The proposed BOOSTMETRIC and the LMNN are further compared on four classes of the Caltech101 object recognition database [19], including Motorbikes (798 images), Airplanes (800), Faces (435), and Background-Google (520).", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "For each image, a number of interest regions are identified by the Harris-affine detector [20] and the visual content in each region is characterized by the SIFT descriptor [21].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "For each image, a number of interest regions are identified by the Harris-affine detector [20] and the visual content in each region is characterized by the SIFT descriptor [21].", "startOffset": 173, "endOffset": 177}], "year": 2009, "abstractText": "The learning of appropriate distance metrics is a critical problem in image classification and retrieval. In this work, we propose a boosting-based technique, termed BOOSTMETRIC, for learning a Mahalanobis distance metric. One of the primary difficulties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semidefinite. Semidefinite programming is sometimes used to enforce this constraint, but does not scale well. BOOSTMETRIC is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. BOOSTMETRIC thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.", "creator": "TeX"}}}