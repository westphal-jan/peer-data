{"id": "0910.5461", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2009", "title": "Anomaly Detection with Score functions based on Nearest Neighbor Graphs", "abstract": "We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on $n$-point nominal data. Anomalies are declared whenever the score of a test sample falls below $\\alpha$, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, $\\alpha$, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.", "histories": [["v1", "Wed, 28 Oct 2009 18:46:41 GMT  (73kb)", "http://arxiv.org/abs/0910.5461v1", "10 pages, 10 figures, accepted by NIPS 2009"]], "COMMENTS": "10 pages, 10 figures, accepted by NIPS 2009", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["manqi zhao", "venkatesh saligrama"], "accepted": true, "id": "0910.5461"}, "pdf": {"name": "0910.5461.pdf", "metadata": {"source": "CRF", "title": "Anomaly Detection with Score functions based on Nearest Neighbor Graphs", "authors": ["Manqi Zhao"], "emails": ["mqzhao@bu.edu", "srv@bu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 091 0.54 61v1 [cs.LG] 2 8"}, {"heading": "1 Introduction", "text": "In fact, most of us are able to outdo ourselves, \"he told the German Press Agency in an interview with\" Frankfurter Allgemeine Zeitung \"(Monday).\" It's not as if we are able to change the world, \"he said,\" but it's not as if we are able to change the world. \""}, {"heading": "2 Anomaly Detection Algorithm: Score functions based on K-NNG", "text": "In the simplest case, we assume that the basic algorithms are devoid of any statistical context. Statistical analysis appears in Section 3. Let's define S = {x1, x2, \u00b7 \u00b7 xn} the nominal distribution of the quantity n that belongs to the nominal data. If the test point is an anomaly, it is assumed that it comes from a mixture of the nominal distribution underlying the training data and another known density (see Section 3). Let's use d (x, y) as a distance function that denotes the distance between two points x, y and NS. For simplicity, let's use dij = d (xi, xj)."}, {"heading": "3 Theory: Consistency of LPE", "text": "A statistical framework for the anomalous detection problem is presented in this section. We note that the anomalous detection is equivalent to the threshold p-values for multivariate data. We will then show that the score functions developed in the previous section represent an asymptotically consistent estimate of p-values. Consequently, it will follow that the strategy of declaring an anomalous density f0 (x) supported by the d-dimensional unit cube [0, 1] d, the data belongs to the d-dimensional unit cube [0] d, and the nominal data are sampled from a multivariate density f0 (x) supported by the d-dimensional unit cube [0, 1] d. The anomaly detection can be formulated as a composite hypothesis."}, {"heading": "4 Experiments", "text": "We apply our method to both artificial and real data. Our method allows us to represent the entire ROC curve by varying the thresholds on our scale. To test the sensitivity of K-LPE to parameter changes, we first need to select K-LPE at the benchmark level of artificial data, ranging from 2 to 12. SVA data contains points with their labels (+ 1 or \u2212 1). We can randomly select 109 points with + 1 label and consider them as nominal training data. Test data include 108 + 1 data and 183 \u2212 1 data (Ground Truth) and the algorithms are supposed to predict + 1 data as \"nominal\" and \u2212 1 data as \"anomalous.\" See Figure 2 (a) for configuration of training points and test points. Scores calculated for test sets using Equation 1 are oblivious to true f1 density (\u2212 1)."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a novel non-parametric adaptive algorithm for detecting anomalies, which leads to a computationally efficient solution with verifiable guarantees of optimality. Our algorithm takes a K-next-neighbor diagram as input and generates a score for each test point. Scores are empirical estimates of the volume of minimum volume level sets that contain the test point. While minimal volume level sets provide an optimal characterization for detecting anomalies, these are high-dimensional sets that are generally difficult to calculate reliably in high-dimensional feature spaces. Nevertheless, sufficient statistics for an optimal compromise between false alarms and false alarms is the volume of the MV set itself, which is a real number. By calculating score functions, we avoid the calculation of high-dimensional sets and still ensure optimal control of false alarms and false alarms."}], "references": [{"title": "A linear programming approach to novelty detection", "author": ["C. Campbell", "K.P. Bennett"], "venue": "Advances in Neural Information Processing Systems 13. MIT Press, 2001, pp. 395\u2013401.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Novelty detection: a review \u2013 part 1: statistical approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing, vol. 83, pp. 2481\u20132497, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient algorithms for mining outliers from large data sets", "author": ["R. Ramaswamy", "R. Rastogi", "K. Shim"], "venue": "Proceedings of the ACM SIGMOD Conference, 2000. 9", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Consistency and convergence rates of one-class svms and related algorithms", "author": ["R. Vert", "J. Vert"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 817\u2013854, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Feature extraction for one-class classification", "author": ["D. Tax", "K.R. M\u00fcller"], "venue": "Artificial neural networks and neural information processing, Istanbul, TURQUIE, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimal singl-class classification strategies", "author": ["R. El-Yaniv", "M. Nisenson"], "venue": "Advances in Neural Information Processing Systems 19. MIT Press, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Detection of abrupt changes: theory and applications", "author": ["I.V. Nikiforov", "M. Basseville"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "A new local distance-based outlier detection approach for scattered real-world data", "author": ["K. Zhang", "M. Hutter", "H. Jin"], "venue": "March 2009, arXiv:0903.3257v1[cs.LG].", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R. Williamson"], "venue": "Neural Computation, vol. 13, no. 7, pp. 1443\u20131471, 2001.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "One-class classification: Concept-learning in the absence of counter-examples", "author": ["D. Tax"], "venue": "Ph.D. dissertation, Delft University of Technology, June 2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Robust novelty detection with single-class MPM", "author": ["G.R.G. Lanckriet", "L.E. Ghaoui", "M.I. Jordan"], "venue": "Neural Information Processing Systems Conference, vol. 18, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning minimum volume sets", "author": ["C. Scott", "R.D. Nowak"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 665\u2013704, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometric entropy minimization(GEM) for anomaly detection and localization", "author": ["A.O. Hero"], "venue": "Neural Information Processing Systems Conference, vol. 19, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "A global geometric framework fo nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, vol. 290, pp. 2319\u20132323, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Graph approximations to geodesics on embedded manifolds", "author": ["M. Bernstein", "V.D. Silva", "J.C. Langford", "J.B. Tenenbaum"], "venue": "2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by local linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, pp. 2323\u20132326, 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "Surveys in Combinatorics. Cambridge University Press, 1989, pp. 148\u2013188.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1989}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D.J. Newman"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/$\\sim$mlearn/{MLR}epository.html 10", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "We propose an adaptive non-parametric method for anomaly detection based on score functions that maps data samples to the interval [0, 1].", "startOffset": 131, "endOffset": 137}, {"referenceID": 0, "context": "In statistical hypothesis testing, p-value is any transformation of the feature space to the interval [0, 1] that induces a uniform distribution on the nominal data.", "startOffset": 102, "endOffset": 108}, {"referenceID": 0, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 44, "endOffset": 50}, {"referenceID": 1, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 44, "endOffset": 50}, {"referenceID": 2, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 100, "endOffset": 106}, {"referenceID": 4, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 100, "endOffset": 106}, {"referenceID": 5, "context": "It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the literature.", "startOffset": 139, "endOffset": 142}, {"referenceID": 6, "context": "In parametric approaches [7] the nominal densities are assumed to come from a parameterized family and generalized likelihood ratio tests are used for detecting deviations from nominal.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "(K-NN) anomaly detection approach is presented in [3, 8].", "startOffset": 50, "endOffset": 56}, {"referenceID": 7, "context": "(K-NN) anomaly detection approach is presented in [3, 8].", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "[9] where the basic idea is to map the training data into the kernel space and to separate them from the origin with maximum margin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11].", "startOffset": 119, "endOffset": 122}, {"referenceID": 10, "context": "Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11].", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "Scott and Nowak [12] derive decision regions based on minimum volume (MV) sets, which does provide Type I and Type II error control.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "Related work by Hero [13] based on geometric entropic minimization (GEM) detects outliers by comparing test samples to the most concentrated subset of points in the training sample.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "To overcome these computational limitations [13] proposes heuristic greedy algorithms based on leave-one out K-NN graph, which while inspired by K-MST algorithm is no longer provably optimal.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Our approach is related to these latter techniques, namely, MV sets of [12] and GEM approach of [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "Our approach is related to these latter techniques, namely, MV sets of [12] and GEM approach of [13].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "(1) Like [13] our algorithm scales linearly with dimension and quadratic with data size and can be applied to high dimensional feature spaces.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "(2) Like [12] our algorithm is provably optimal in that it is uniformly most powerful for the specified false alarm level, \u03b1, for the case that the anomaly density is a mixture of the nominal and any other density (not necessarily uniform).", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "(4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "(4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes.", "startOffset": 69, "endOffset": 76}, {"referenceID": 11, "context": "(4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes.", "startOffset": 69, "endOffset": 76}, {"referenceID": 0, "context": "Let S = {x1, x2, \u00b7 \u00b7 \u00b7 , xn} be the nominal training set of size n belonging to the unit cube [0, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "Let d(x, y) be a distance function denoting the distance between any two points x, y \u2208 [0, 1].", "startOffset": 87, "endOffset": 93}, {"referenceID": 13, "context": "The Geodesic Learning algorithm, a subroutine in Isomap [14, 15] can be used to efficiently and consistently estimate the geodesic distances.", "startOffset": 56, "endOffset": 64}, {"referenceID": 14, "context": "The Geodesic Learning algorithm, a subroutine in Isomap [14, 15] can be used to efficiently and consistently estimate the geodesic distances.", "startOffset": 56, "endOffset": 64}, {"referenceID": 15, "context": "This can be accomplished for instance through Mahalanobis distances or as a by product of local linear embedding [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "The score functions map the test data \u03b7 to the interval [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Section 3 establishes that the scores for nominally generated data is asymptotically uniformly distributed in [0, 1].", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "We can see that the curve for the nominal data is approximately uniform in the interval [0, 1] and the curve for the anomaly data has a peak at 0.", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": "Assume that the data belongs to the d-dimensional unit cube [0, 1] and the nominal data is sampled from a multivariate density f0(x) supported on the d-dimensional unit cube [0, 1].", "startOffset": 60, "endOffset": 66}, {"referenceID": 0, "context": "Assume that the data belongs to the d-dimensional unit cube [0, 1] and the nominal data is sampled from a multivariate density f0(x) supported on the d-dimensional unit cube [0, 1].", "startOffset": 174, "endOffset": 180}, {"referenceID": 0, "context": "Suppose test data, \u03b7 comes from a mixture distribution, namely, f(\u03b7) = (1\u2212\u03c0)f0(\u03b7)+\u03c0f1(\u03b7) where f1(\u03b7) is a mixing density supported on [0, 1].", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "Suppose the likelihood ratio f1(x)/f0(x) does not have non-zero flat spots on any open ball in [0, 1].", "startOffset": 95, "endOffset": 101}, {"referenceID": 0, "context": "The above formula can be thought of as a mapping of \u03b7 \u2192 [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Furthermore, the distribution of p(\u03b7) under H0 is uniform on [0, 1].", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "When the mixing density is uniform, namely, f1(\u03b7) = U(\u03b7) where U(\u03b7) is uniform over [0, 1], note that \u03a9\u03b1 = {\u03b7 | p(\u03b7) \u2265 \u03b1} is a density level set at level \u03b1.", "startOffset": 84, "endOffset": 90}, {"referenceID": 11, "context": "It is well known (see [12]) that such a density level set is equivalent to a minimum volume set of level \u03b1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "The minimum volume set at level \u03b1 is known to be the uniformly most powerful decision region for testing H0 : \u03c0 = 0 versus the alternative H1 : \u03c0 > 0 (see [13, 12]).", "startOffset": 155, "endOffset": 163}, {"referenceID": 11, "context": "The minimum volume set at level \u03b1 is known to be the uniformly most powerful decision region for testing H0 : \u03c0 = 0 versus the alternative H1 : \u03c0 > 0 (see [13, 12]).", "startOffset": 155, "endOffset": 163}, {"referenceID": 0, "context": "First, measure theoretic arguments are used to establish p(X) as a random variable over [0, 1] under both nominal and anomalous distributions.", "startOffset": 88, "endOffset": 94}, {"referenceID": 0, "context": ", distributed with nominal density it follows that the random variable p(X) d \u223c U [0, 1].", "startOffset": 82, "endOffset": 88}, {"referenceID": 0, "context": "When X d \u223c f = (1 \u2212 \u03c0)f0 + \u03c0f1 with \u03c0 > 0 the random variable, p(X) d \u223c g where g(\u00b7) is a monotonically decreasing PDF supported on [0, 1].", "startOffset": 132, "endOffset": 138}, {"referenceID": 0, "context": "Let \u03b7 \u2208 [0, 1] be an arbitrary test sample.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "It follows that for a suitable choice K and under the above smoothness conditions, |p\u0302K(\u03b7)\u2212 p(\u03b7)| n\u2192\u221e \u2212\u2192 0 almost surely, \u2200\u03b7 \u2208 [0, 1] For simplicity, we limit ourselves to the case when f1 is uniform.", "startOffset": 127, "endOffset": 133}, {"referenceID": 16, "context": "Instead, we need to use the more generalized concentration-of-measure inequality such as MacDiarmid\u2019s inequality[17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 17, "context": "1 in [18], sup x0,\u00b7\u00b7\u00b7 ,xm,x\u2032i |F (x0, \u00b7 \u00b7 \u00b7 , xi, \u00b7 \u00b7 \u00b7 , xm)\u2212 F (x0, \u00b7 \u00b7 \u00b7 , x \u2032 i, \u00b7 \u00b7 \u00b7 , xn)| \u2264 K\u03b3d/m (9)", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "For comparison we plot the empirical ROC curve of the one-class SVM of [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "K-LPE for Banana Data Figure 2: Performance Robustness of LPE;(a) The configuration of the nominal training points (red \u2018+\u2019) and unlabeled test points (black \u2018 \u2022\u2019) for the banana dataset [19]; (b) Empirical ROC curve of K-LPE on the banana dataset with K = 2, 4, 6, 8, 10, 12 (with n = 400) vs the empirical ROC curve of one class SVM developed in [9].", "startOffset": 348, "endOffset": 351}, {"referenceID": 18, "context": "Next, we ran LPE on three real-world datasets: Wine, Ionosphere[20] and MNIST US Postal Service (USPS) database of handwritten digits.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "(2) For high dimensional data set, the data points are normalized to be within [0, 1] and we use geodesic distance [14](instead of Euclidean distance) as the input to LPE.", "startOffset": 79, "endOffset": 85}, {"referenceID": 13, "context": "(2) For high dimensional data set, the data points are normalized to be within [0, 1] and we use geodesic distance [14](instead of Euclidean distance) as the input to LPE.", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "7% (In contrast FP = 7% and FA = 9% with \u03bd = 5% for OC-SVM as reported in [9]).", "startOffset": 74, "endOffset": 77}], "year": 2009, "abstractText": "We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data. Anomalies are declared whenever the score of a test sample falls below \u03b1, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, \u03b1, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.", "creator": "LaTeX with hyperref package"}}}