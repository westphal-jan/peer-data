{"id": "0910.5761", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2009", "title": "Which graphical models are difficult to learn?", "abstract": "We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).", "histories": [["v1", "Fri, 30 Oct 2009 01:10:44 GMT  (25kb)", "http://arxiv.org/abs/0910.5761v1", null]], "reviews": [], "SUBJECTS": "stat.ML cond-mat.stat-mech cs.LG", "authors": ["andrea montanari", "jose ayres pereira"], "accepted": true, "id": "0910.5761"}, "pdf": {"name": "0910.5761.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jbento@stanford.edu", "montanari@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 091 0.57 61v1 [st at.M L] 30 E"}, {"heading": "1 Introduction and main results", "text": "Considering a graph G = (V = [p], E) and a positive parameter \u03b8 > 0, the ferromagnetic Ising model on G is the paired Markov random parameter G, \u03b8 (x) = 1ZG, \u03b8 (i, j). Although the ferromagnetic Ising model on G is the paired Markov random field \u00b5G, the Ising model becomes a prototype undirected graphical model, with applications in computer vision, clustering and spatial statistics. Its obvious generalization to edge-dependent parameters is not possible. Apart from being one of the most studied models in statistical mechanics, and will be introduced in Section 1.2.2. (Let us emphasize that we follow the statistical mechanics convention of the call (1), an issuing model for each graph G.) In this paper, we are examining the following structural learning problem: subjects in the face of i.i. (i.i.) samples (1)."}, {"heading": "1.1 A toy example: the thresholding algorithm", "text": "To illustrate the interplay between graph structure, sample complexity and interaction strength, it is instructive to look at an example of warming. G is reconstructed by the threshold algorithm, falling below the empirical correlations. (3) THRESHOLDING (samples {x (), thresholds.) 1: Calculate the empirical correlations. (i, j).V \u00b7 V; 2: For each (i, j).V \u00b7 V \u00b7 V: If C (i, j), thresholds. (i, j).E; We will name this algorithm after Thr (\u03c4). Note that its complexity is dominated by the calculation of the empirical correlations. (i, j).V \u00b7 V \u00b7 V \u00b7 V \u00b7 V: If the maximum correlation between the empirical correlations exists."}, {"heading": "1.2 More sophisticated algorithms", "text": "In this section, we characterize \u03c7Alg (G, \u03b8) and nAlg (G, \u03b8) for advanced algorithms. Again, we get very different behaviors of these algorithms depending on far-reaching correlations. Due to space constraints, we focus on two types of algorithms and sketch only the proof of our most challenging result, namely theorem 1.6. In the following, we use \u2202 i to denote the neighborhood of a node i-G (i / \u0445 \u2202 i) and assume the degree to be limited: | \u2202 i | \u2264 \u0445."}, {"heading": "1.2.1 Local Independence Test", "text": "A recurring approach to structural learning processes consists in using the conditional structure of independence encoded by the graph [1, 4, 5, 6], so let's look at the approach of [4], which specializes in the model [1]. Let's fix a vertex r whose neighborhood we want to reconstruct, and let's look at the conditional distribution of xr given value: xr, x x x x x x x x x x x x x x x x x x x x. Each change of xi, i x x x x x, results in a change of this distribution delimited by 0. Let U be a candidate, and assume that the value of xj, j x x x x. U will produce a noticeable change in the limits of Xr, even if we change the remaining values in U and in each W, | W | W | \u2264."}, {"heading": "1.2.2 Regularized Pseudo-Likelihoods", "text": "Another approach to the learning problem is to maximize an appropriate empirical probability function (7, 8, 9, 10, 13), to control the fluctuations caused by the limited number of samples, and select the sparse graphs that contain a regularization term. Indeed, the following probability function is often not considered [7, 8, 10, 11, 12, 13]. As a specific implementation of low complexity of this idea, we consider the pseudolikelihood method of [7]. For each node, the following probability function is considerate (8, 11, 12, 13]. As a specific implementation of low complexity of this idea, we consider the pseudolikelihood method of [7]."}, {"heading": "2 Numerical experiments", "text": "In order to investigate the practical relevance of the above results, we conducted extensive numerical simulations using the regularized logistic regression algorithm Rlr (\u03bb). Among other learning algorithms, Rlr (\u03bb) achieves a good balance of complexity and performance. Samples from the Ising model (1) were generated using Gibbs samples (a.k.a. Glauber dynamics).The mixing time can be very large to predict the evolution time on pentium dual-core processors, and was estimated using the time required for the general sign (this is a fairly conservative estimate at low temperature).In fact, generating the samples {x ()} was the bulk of our computing effort and took about 50 days to predict the CPU time on pentium dual-core processors (we show only a portion of this data here).Note that Rlr (\u03bb) was only tested for comparability (on tree graphs, 7 or weak)."}, {"heading": "3 Proofs", "text": "If M is a matrix and R, P are index sets, then MR P denotes the submatrix with row indices in R and column indices in P. As described above, let r be the vertex whose neighborhood we are trying to reconstruct and define, then we neglect all other parameters below and write the combination of numbers as an abbreviation of Southr, \u00b7.Let z be a subgradient of | | 1 that depends only on the actual parameter values, \u00b7 = < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "3.1 Proof of Theorem 1.6", "text": "Our initial auxiliary results state that if they are small, the probability of loss is low. (...) The probability of success of Rlr (...) is high enough. (...) The probability of success of Rlr (...) is high. (...) The probability of success of Rlr (...) is high. (...) The probability of success of Rlr (...) is high. (...) The probability of success of Rn (...) is high. (...) The probability of success of Rn (...) is high. (...) The next Sp (...) is high. (...) The next Sp (...) is high."}, {"heading": "3.2 Proofs of auxiliary lemmas", "text": "The Evidence. (Lemma 3.1) We will show that the probability that the i component of any subgradient of L (\u03b8; x) in all expressions derived from L.Let z is higher than the subgradients of S > 0 (component-wise) and assume that the subgradients of S > 0 (component-wise) and the subgradients of S > 0 (component-wise) are found in all expressions derived from L.Let z, and assume that the subgradients of S > 1 (component-wise) and the subgradients of S (component-wise) and the subgradients of S (component-wise) and the subgradients of S (component-wise) are found in all expressions derived from L.Let z > 1 (component-wise) and the subgradients of S (component-wise) and the subgradients of S (component-wise)."}, {"heading": "Acknowledgments", "text": "This work was partially supported by a Terman scholarship, the NSF-CAREER prize CCF-0743978 and the NSF scholarship DMS-0806211 as well as a Portuguese doctoral fellowship FCT."}], "references": [{"title": "Learning factor graphs in polynomial time and sample complexity", "author": ["P. Abbeel", "D. Koller", "A. Ng"], "venue": "Journal of Machine Learning Research.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting", "author": ["M. Wainwright"], "venue": "[math.ST],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Information-theoretic limits of selecting binary graphical models in high dimensions", "author": ["N. Santhanam", "M. Wainwright"], "venue": "[cs.IT],", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Reconstruction of Markov Random Fields from Samples: Some Observations and Algorithms\u201d,Proceedings of the 11th international workshop, APPROX 2008, and 12th international workshop", "author": ["G. Bresler", "E. Mossel", "A. Sly"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Consistent estimation of the basic neighborhood structure of Markov random fields", "author": ["Csiszar", "Z. Talata"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Learning Bayesian network structure from massive datasets: The sparse candidate algorithm", "author": ["N. Friedman", "I. Nachman", "D. Peer"], "venue": "In UAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "High-Dimensional Ising Model Selection Using l1-Regularized Logistic Regression", "author": ["P. Ravikumar", "M. Wainwright", "J. Lafferty"], "venue": "[math.ST],", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Inferring graphical model structure using l1regularized pseudolikelihood", "author": ["M.Wainwright", "P. Ravikumar", "J. Lafferty"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods", "author": ["H. H\u00f6fling", "R. Tibshirani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "d\u2019Aspremont, \u201cModel Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data", "author": ["O.Banerjee", "A.L. El Ghaoui"], "venue": "Journal of Machine Learning Research, March 2008,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Model Selection and Estimation in Regression with Grouped Variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Royal. Statist. Soc B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "B\u00fcuhlmann, \u201cHigh dimensional graphs and variable selection with the lasso", "author": ["P.N. Meinshausen"], "venue": "Annals of Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine. Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Critical behavior of the bond-dilute two-dimensional Ising model", "author": ["D. Zobin"], "venue": "Phys. Rev.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1978}, {"title": "Critical Temperatures of Anisotropic Ising Lattices", "author": ["M. Fisher"], "venue": "II. General Upper Bounds\u201d, Phys. Rev", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1967}, {"title": "Ising Models on Locally Tree Like Graphs", "author": ["A. Dembo", "A. Montanari"], "venue": "Ann. Appl. Prob", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The graph learning problem is solvable with unbounded sample complexity, and computational resources [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "General bounds on nAlg(G, \u03b8) have been given in [2, 3], under the assumption of unbounded computational resources.", "startOffset": 48, "endOffset": 54}, {"referenceID": 2, "context": "General bounds on nAlg(G, \u03b8) have been given in [2, 3], under the assumption of unbounded computational resources.", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 3, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 4, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 5, "context": "A recurring approach to structural learning consists in exploiting the conditional independence structure encoded by the graph [1, 4, 5, 6].", "startOffset": 127, "endOffset": 139}, {"referenceID": 3, "context": "Let us consider, to be definite, the approach of [4], specializing it to the model (1).", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "Both theorems that follow are consequences of the analysis of [4].", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "A way out was proposed in [4].", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 7, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 8, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 9, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 12, "context": "A different approach to the learning problem consists in maximizing an appropriate empirical likelihood function [7, 8, 9, 10, 13].", "startOffset": 113, "endOffset": 130}, {"referenceID": 6, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 7, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 8, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 9, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 10, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 11, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 12, "context": "To control the fluctuations caused by the limited number of samples, and select sparse graphs a regularization term is often added [7, 8, 9, 10, 11, 12, 13].", "startOffset": 131, "endOffset": 156}, {"referenceID": 6, "context": "As a specific low complexity implementation of this idea, we consider the l1-regularized pseudolikelihood method of [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "This theorem is proved by noting that for \u03b8 \u2264 K1/\u2206 correlations decay exponentially, which makes all conditions in Theorem 1 of [7] (denoted there by A1 and A2) hold, and then computing the probability of success as a function of n, while strenghtening the error bounds of [7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "This theorem is proved by noting that for \u03b8 \u2264 K1/\u2206 correlations decay exponentially, which makes all conditions in Theorem 1 of [7] (denoted there by A1 and A2) hold, and then computing the probability of success as a function of n, while strenghtening the error bounds of [7].", "startOffset": 273, "endOffset": 276}, {"referenceID": 13, "context": "The analogous result was proven in [14] for model selection using the Lasso.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "Under the restriction \u03bb \u2192 0 the solutions given by Rlr converge to \u03b8 with n [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 13, "context": "It is thus not surprising that, when \u03bb \u2192 0 the incoherence condition introduced for the Lasso in [14] is also relevant for the Ising model.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "Notice that Rlr(\u03bb) had been tested in [7] only on tree graphs G, or in the weakly coupled regime \u03b8 < \u03b8crit.", "startOffset": 38, "endOffset": 41}, {"referenceID": 14, "context": "7 [15, 16].", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": "7 [15, 16].", "startOffset": 2, "endOffset": 10}, {"referenceID": 16, "context": "The proof of this result relies on a local weak convergence result for ferromagnetic Ising models on random graphs proved in [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "By Theorem 1 in [4] we can assume, without loss of generality n > K \u2206log p for some small constant K .", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "Some straightforward manipulations imply (See Lemma 7 from [7]) ||T1,i||\u221e \u2264 \u2206 C2 min ||QSS \u2212QSS ||\u221e , ||T2,i||\u221e \u2264 \u221a \u2206 Cmin ||[QnSCS \u2212Q\u2217SCS ]i||\u221e , ||T3,i||\u221e \u2264 2\u2206 C2 min ||QSS \u2212QSS ||\u221e||[QSCS \u2212QSCS ]i||\u221e , and thus all will be bounded by \u01eb/8 when E holds.", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "It can be proved using [17] and uniform continuity with respect to the \u2018external field\u2019 that non-trivial local expectations with respect to \u03bcG,\u03b8(x) converge to local expectations with respect to \u03bc+T,\u03b8(x), as p \u2192 \u221e.", "startOffset": 23, "endOffset": 27}], "year": 2009, "abstractText": "We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).", "creator": "gnuplot 4.2 patchlevel 2 "}}}