{"id": "0912.3309", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2009", "title": "New Generalization Bounds for Learning Kernels", "abstract": "This paper presents several novel generalization bounds for the problem of learning kernels based on the analysis of the Rademacher complexity of the corresponding hypothesis sets. Our bound for learning kernels with a convex combination of p base kernels has only a log(p) dependency on the number of kernels, p, which is considerably more favorable than the previous best bound given for the same problem. We also give a novel bound for learning with a linear combination of p base kernels with an L_2 regularization whose dependency on p is only in p^{1/4}.", "histories": [["v1", "Thu, 17 Dec 2009 02:29:41 GMT  (11kb)", "http://arxiv.org/abs/0912.3309v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["corinna cortes", "mehryar mohri", "afshin rostamizadeh"], "accepted": true, "id": "0912.3309"}, "pdf": {"name": "0912.3309.pdf", "metadata": {"source": "CRF", "title": "New Generalization Bounds for Learning Kernels", "authors": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "emails": ["corinna@google.com", "mohri@cims.nyu.edu", "rostami@cs.nyu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 091 2.33 09v1 [cs.AI] 17 Dec 200 9"}, {"heading": "1 Introduction", "text": "They can therefore be combined with algorithms such as support vector machines (SVMs), which rather ask the user to commit to a specific kernel that may not be optimal for the task, especially if the user's prior knowledge of the task is poor, learning methods only require him to specify a family of cores, and the learning algorithm then selects both the specific core of that family and the hypothesis that is defined with respect to that kernel."}, {"heading": "2 Preliminaries", "text": "Most learning kernel algorithms are based on a hypothesis derived from convex combinations of a fixed set of kernels K1. (1) Note that linear combinations with potentially negative mix weights have also been considered in literature, e.g. [13], but these combinations do not ensure that the combined kernel PDS. We also consider the hypothesis that H \u2032 p sets on the vector \u00b5 based on an L2 condition and is defined as follows: H \u2032 p = {m \u00b2 i = 1\u03b1iK (xi, \u00b7): K = p \u00b2 ix."}, {"heading": "3 Rademacher complexity bound for Hp", "text": "Theorem em 1: Sample S of size m = Sample S of size m = Sample S of size m = Sample S of size m = Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Sample) Sample S (Hp) Sample S (Sample) Sample S (Sample) Sample S (Hp) Sample S (Sample) Sample S (S) Sample S (Sample) Sample S (S) Sample S (Sample) Sample S (S) Sample S (Hp) Sample S (Sample) Sample S (S) Sample S (Hp) Sample S (Hp) Sample S) Sample S (Sample S) Sample S (Hp) Sample S (Hp) Sample S) Sample S (Hp) Sample S (Hp) Sample S) Sample S (Hp) Sample S (Hp) Sample S) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S) Sample S (Sample) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample S (Hp) Sample) Sample S (Hp) Sample S (Hp) Sample) Sample S (Hp) Sample S (Hp) Sample) Sample S (Sample) Sample S (Sample S (Sample) Sample"}, {"heading": "5 Conclusion", "text": "Our limits are simpler and significantly better than previous limits. Their very low dependence on the number of nuclei seems to indicate the use of a large number of nuclei for this problem. Our experiments with this problem in regression using a large number of nuclei seem to confirm this idea [8]. However, much needs to be done to combine these theoretical results with the somewhat disappointing performance observed in practice in most learning nuclei experiments [7]."}, {"heading": "A Lemma 5", "text": "The following problem is a simple version of Holder's inequality. Lemma 5 Leave q, r > 1 with 1 / q + 1 / r = 1. Then the following result is similar to Holder's inequality: | w \u00b7 \u03a6 (x) | \u2264 (p \u00b2 k = 1% wk \u00b2 q) 1 / q (p \u00b2 k = 1% w \u00b2) 1 / r (14) Proof: Leave q (w) = (p \u00b2 k = 1% wk \u00b2 q) 1 / q and vice versa (p \u00b2 k = 1% wk \u00b2) 1 / q and (p \u00b2 k = 1% p \u00b2 k (x) 1 / r, then | w \u00b2 (x) | w \u00b2 q (w = 1% wk = 1% wk \u00b2 (w) (w = 1% wk = 1%)."}], "references": [{"title": "A DC-programming algorithm for kernel selection", "author": ["Andreas Argyriou", "Raphael Hauser", "Charles Micchelli", "Massimiliano Pontil"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning convex combinations of continuously parameterized basic kernels", "author": ["Andreas Argyriou", "Charles Micchelli", "Massimiliano Pontil"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Exploring large feature spaces with hierarchical multiple kernel learning", "author": ["F. Bach"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard Boser", "Isabelle Guyon", "Vladimir Vapnik"], "venue": "In COLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "On the complexity of learning the kernel matrix", "author": ["Olivier Bousquet", "Daniel J.L. Herrmann"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Invited talk: Can learning kernels help performance", "author": ["Corinna Cortes"], "venue": "In ICML, page 161,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "L2 regularization for learning kernels", "author": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Learning non-linear combinations of kernels", "author": ["Corinna Cortes", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Multi-task feature and kernel selection for SVMs", "author": ["Tony Jebara"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Rademacher processes and bounding the risk of function learning", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "In High Dimensional Probability II,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Gert Lanckriet", "Nello Cristianini", "Peter Bartlett", "Laurent El Ghaoui", "Michael Jordan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Nonstationary kernel combination", "author": ["Darrin P. Lewis", "Tony Jebara", "William Stafford Noble"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Learning the kernel function via regularization", "author": ["Charles Micchelli", "Massimiliano Pontil"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Learning the kernel with hyperkernels", "author": ["Cheng Soon Ong", "Alexander Smola", "Robert Williamson"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Learning with Kernels", "author": ["Bernhard Sch\u00f6lkopf", "Alex Smola"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Kernel Methods for Pattern Analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Learning bounds for support vector machines with learned kernels", "author": ["Nathan Srebro", "Shai Ben-David"], "venue": "In COLT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Statistical Learning Theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "More generality in efficient multiple kernel learning", "author": ["Manik Varma", "Bodla Rakesh Babu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Generalization bounds for learning the kernel problem", "author": ["Yiming Ying", "Colin Campbell"], "venue": "In COLT,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Multiclass multiple kernel learning", "author": ["Alexander Zien", "Cheng Soon Ong"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}], "referenceMentions": [{"referenceID": 15, "context": "1 Introduction Kernel methods are widely used in statistical learning [17, 18].", "startOffset": 70, "endOffset": 78}, {"referenceID": 16, "context": "1 Introduction Kernel methods are widely used in statistical learning [17, 18].", "startOffset": 70, "endOffset": 78}, {"referenceID": 4, "context": "They can be combined with algorithms such as support vector machines (SVMs) [5, 10, 20] or other kernel-based algorithms to form powerful learning techniques.", "startOffset": 76, "endOffset": 87}, {"referenceID": 18, "context": "They can be combined with algorithms such as support vector machines (SVMs) [5, 10, 20] or other kernel-based algorithms to form powerful learning techniques.", "startOffset": 76, "endOffset": 87}, {"referenceID": 11, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 13, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 1, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 0, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 17, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 14, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 12, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 21, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 9, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 2, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 7, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 20, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 8, "context": "There is a large body of literature dealing with various aspects of the problem of learning kernels, including theoretical questions, optimization problems related to this problem, and experimental results [13, 15, 2, 1, 19, 16, 14, 23, 11, 3, 8, 22, 9].", "startOffset": 206, "endOffset": 253}, {"referenceID": 13, "context": "Some of this previous work considers families of Gaussian kernels [15] or hyperkernels [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "Some of this previous work considers families of Gaussian kernels [15] or hyperkernels [16].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "Non-linear combinations of kernels have been recently considered by [21, 3, 9].", "startOffset": 68, "endOffset": 78}, {"referenceID": 2, "context": "Non-linear combinations of kernels have been recently considered by [21, 3, 9].", "startOffset": 68, "endOffset": 78}, {"referenceID": 8, "context": "Non-linear combinations of kernels have been recently considered by [21, 3, 9].", "startOffset": 68, "endOffset": 78}, {"referenceID": 11, "context": "But, the most common family of kernels examined is that of non-negative combinations of some fixed kernels constrained by a trace condition, which can be viewed as an L1 regularization [13], or by an L2 regularization [8].", "startOffset": 185, "endOffset": 189}, {"referenceID": 7, "context": "But, the most common family of kernels examined is that of non-negative combinations of some fixed kernels constrained by a trace condition, which can be viewed as an L1 regularization [13], or by an L2 regularization [8].", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "[13] for the family of convex combinations of p base kernels is similar to that of Bousquet and Herrmann [6] and has the following form: R(h) \u2264 R\u0302\u03c1(h) + O ( 1 \u221a m \u221a maxpk=1 Tr(Kk)max p i=1(\u2016Kk\u2016/Tr(Kk))/\u03c1 ) where R(h) is the generalization error of a hypothesis h, R\u03c1(h) is the fraction of training points with margin less than or equal to \u03c1 and Kk is the kernel matrix associated to the kth base kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[13] for the family of convex combinations of p base kernels is similar to that of Bousquet and Herrmann [6] and has the following form: R(h) \u2264 R\u0302\u03c1(h) + O ( 1 \u221a m \u221a maxpk=1 Tr(Kk)max p i=1(\u2016Kk\u2016/Tr(Kk))/\u03c1 ) where R(h) is the generalization error of a hypothesis h, R\u03c1(h) is the fraction of training points with margin less than or equal to \u03c1 and Kk is the kernel matrix associated to the kth base kernel.", "startOffset": 105, "endOffset": 108}, {"referenceID": 17, "context": "This bound was later shown by Srebro and Ben-David [19] to be always larger than one.", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "[13] for the family of linear combinations of base kernels was also shown by the same authors to be always larger than one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] also presented a multiplicative bound for convex combinations of base kernels that is of the form R(h) \u2264 R\u0302\u03c1(h) + O (\u221a p/\u03c12 m ) .", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Srebro and Ben-David [19] presented a generalization bound based on the pseudo-dimension of the family of kernels that significantly improved on this bound.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "Ying and Campbell [22] also give generalization bounds for learning kernels based on the notion of Rademacher chaos complexity and the pseudo-dimension of the family of kernels used.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "We had previously given a stability bound for an algorithm extending kernel ridge regression to learning kernels that had an additive dependency with respect to p [8] assuming a technical condition of orthogonality on the base kernels.", "startOffset": 163, "endOffset": 166}, {"referenceID": 11, "context": ", [13], however these combinations do not ensure that the combined kernel is PDS.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "Proof: With our definition of the Rademacher complexity, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, the following bound holds for any h \u2208 Hp [12, 4]: R(h) \u2264 R\u0302\u03c1(h) + 2R\u0302S(Hp) + 2 \u221a log 2\u03b4 2m .", "startOffset": 146, "endOffset": 153}, {"referenceID": 3, "context": "Proof: With our definition of the Rademacher complexity, for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4, the following bound holds for any h \u2208 Hp [12, 4]: R(h) \u2264 R\u0302\u03c1(h) + 2R\u0302S(Hp) + 2 \u221a log 2\u03b4 2m .", "startOffset": 146, "endOffset": 153}, {"referenceID": 17, "context": "(9) In comparison, the bound for this problem given by Srebro and Ben-David [19] using the pseudo-dimension has a stronger dependency with respect to p and is more complex:", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "Then, since \u03bck \u2208 [0, 1], this implies \u03bc 4(q\u22121) q k \u2264 \u03bck.", "startOffset": 17, "endOffset": 23}], "year": 2009, "abstractText": "This paper presents several novel generalization bounds for the problem of learning kernels based on the analysis of the Rademacher complexity of the corresponding hypothesis sets. Our bound for learning kernels with a convex combination of p base kernels has only a log p dependency on the number of kernels, p, which is considerably more favorable than the previous best bound given for the same problem. We also give a novel bound for learning with a linear combination of p base kernels with an L2 regularization whose dependency on p is only in p.", "creator": "LaTeX with hyperref package"}}}