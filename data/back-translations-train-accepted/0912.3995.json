{"id": "0912.3995", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2009", "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "abstract": "We consider the problem of optimizing an unknown, noisy function that is expensive to evaluate. We cast this problem as a multiarmed bandit problem where the payoff function is sampled from a Gaussian Process. We resolve an important open problem on deriving regret bounds for this setting. In particular, we analyze an upper confidence algorithm and bound its cumulative regret in terms of the maximal information gain due to sampling, thus connecting Gaussian Process bandits and optimal experimental design. Moreover, we bound the maximal information gain by exploiting known spectral properties of popular classes of kernels and obtain sub-linear regret bounds for our algorithm. In particular, we show that, perhaps surprisingly, the regret bounds for the squared exponential kernel depend only very weakly on the dimensionality of the problem.", "histories": [["v1", "Mon, 21 Dec 2009 00:08:19 GMT  (476kb,D)", "http://arxiv.org/abs/0912.3995v1", "17 pages, 5 figures"], ["v2", "Thu, 4 Feb 2010 06:15:15 GMT  (175kb,D)", "http://arxiv.org/abs/0912.3995v2", null], ["v3", "Sat, 13 Feb 2010 18:24:43 GMT  (175kb,D)", "http://arxiv.org/abs/0912.3995v3", null], ["v4", "Wed, 9 Jun 2010 23:24:13 GMT  (292kb,DS)", "http://arxiv.org/abs/0912.3995v4", null]], "COMMENTS": "17 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["niranjan srinivas", "andreas krause 0001", "sham kakade", "matthias w seeger"], "accepted": true, "id": "0912.3995"}, "pdf": {"name": "0912.3995.pdf", "metadata": {"source": "CRF", "title": "Gaussian Process Bandits without Regret: An Experimental Design Approach", "authors": ["Niranjan Srinivas", "Andreas Krause", "Matthias Seeger"], "emails": ["niranjan@caltech.edu", "krausea@caltech.edu", "sham@tti-c.org", "mseeger@mmci.uni-saarland.de"], "sections": [{"heading": "1 Introduction", "text": "In many real-world problems, it is necessary to optimize a noisy function that is expensive to evaluate. However, recent examples of interest include selecting advertising in the sponsored search to maximize profit in a click-through model [2] and learning optimal control strategies for robots [3]. However, a common approach is to use a probability model to estimate the functional response to the input [4, 5, 6, 7]. A natural choice that has proven effective [3] is the use of Gaussian process models for the functional response. However, the analysis of algorithms for Gaussian process optimization has proved very difficult, and there are no results on convergence rates. A key challenge is that any algorithm for GP optimization must balance exploration against each other to determine the shape of the function - and exploitation - where the function is expected to achieve high values."}, {"heading": "1.1 Related Work", "text": "The algorithm Efficient Global Optimization (EGO) to optimize expensive black box functions with the aim of minimizing the number of function evaluations is proposed in [6] and extended in [14] for noisy black box functions. Convergence of EGO with multivariate Gaussian processes (without an analysis of rates) is established in [15]. In fact, Brochu et al. [13] clearly point to one of the main problems with Bass optimization - it is also often unclear how to handle the trade-off between exploration and exploitation in the utility function. Too much exploration and many iterations can proceed without improvement. Too much exploitation leads to local maximization. Gaussian processes are used to estimate the value function in Reinforcement."}, {"heading": "1.2 Main Contributions", "text": "We consider the problem of adaptive selection of inputs x1, x2,..., xT to an unknown function f, which we have taken from a Gaussian process, so that we consider the problem as a bandit problem with infinitely many arms, in which the dependencies between the arms are modeled by the Gaussian process. We analyze a simple and intuitive algorithm based on the supreme trust and prove that this algorithm achieves sublinear no-repentance for many popular classes of core functions. Our evidence shows a novel general limit for cumulative regret in terms of maximum information gains, thus combining GP optimization and optimal experimental design. This result does not presuppose a structure in decision space D (apart from the existence of a suitable core function). We have bound this maximum information gain for compact decision sets in Rd by exploiting known spectral properties of popular core classes. Furthermore, we show that the limits of regret for the crucial decision based on the capture of the core information are very weak based on the exponential only the exponential one."}, {"heading": "2 Gaussian Processes", "text": "A Gaussian process (c.f., [22]) is a collection of random variables so that each finite subset is distributed according to a multivariate Gaussian distribution. A GP F (x) \u0445 GP (m (x), k (x), x))) (F (x))) rn is complete by its mean functionality (x) = E (x)] and its covariance function is therefore of crucial importance, since it has the smoothing properties of those derived from the GP.An advantage of using Gaussian processes is that well-known formulas exist in closed form [22] for the mean and the variance of the posterior distribution, which allows for exact inference."}, {"heading": "3 GP Optimization and the UCB algorithm", "text": "Our goal is to maximize the sum of the \"rewards\" we receive, i.e., to choose our sampling points to maximize the sum of the functional values obtained. Therefore, this problem is analogous to the classic multi-armed bandit problem, in which we have an arm for every possible decision. [23, 24] Algorithm selects the arm by maximizing an upper trust index; for each arm, this index is calculated by adding the current estimate of the mean of that arm and the weighted standard deviation of that arm: xt = argmax x-argmax x-argmax x-argmax x-arg.t (x)"}, {"heading": "4 Regret bounds", "text": "We will now put a limit on the cumulative regret for GP Bandit Optimization. First, in Section 4.1, we will show that the growth of cumulative regret is limited by the information from sampling. In Section 4.2, we will then tie up the information gain for key examples of core functions."}, {"heading": "4.1 A general bound", "text": "During the discussion, wherever appropriate, the uppercase notation (F, Y) refers to random variables, while the lowercase notation (f, y) refers to its corresponding realization. Let V represent the discretization of the decision set. We observe points = u \u2020 sfV + s, where fV is the realization of the random unknown reward function F at the points in V, i is a Gaussian process of white noise with variance \u03c32 and we are an indicator vector referring to the particular element s of V that we choose to observe. Furthermore, fV is the vector of f values at the discretization points V. We can imagine selecting the set of vectors ui in relation to the selection of the matrix A with ui as columns. Then, we observe YA = A \u2020 FV +, where N (0, \u03c32I) is the variability of the information obtained."}, {"heading": "4.2 Bounding the maximum possible information gain", "text": "It is the key question how to maxA the quantity maxA I (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A)) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) () (A) (A) (A) () (A) (A) (A) (A) (A) () (A) (A) () (A) (A) (A) (A) (A) (A) (A) ("}, {"heading": "5 Bounds for common kernel functions", "text": "We now show that if we opt for discrediting in such a way that n = \u0443 (T \u03c4), where \u03c4 is determined in accordance with the dimension d of the decision set D, the average regret with T disappears asymptotically. We justify this choice and discuss the details at the end of the section. Note also that \u03b2T = O is (log2 (T))."}, {"heading": "5.1 Finite dimensional Bayesian linear regression", "text": "Consider the end-dimensional Bayesian linear regression, with piecemeal Lipschitz continuous base functions \u03c6 (x) \u2020 = (\u03c61 (x), \u03c62 (x),..., \u03c6q (x). As we have seen in section 2, the gram covariance matrix for the sample points x1, x2,..., xn isG = \u03c6 (x1) \u03c6 (x2)... \u03c6 \u2020 (xn) (\u03c6 (x1),? (x2),?? (xn) The maximum number of eigenvalues that G can have is q. Therefore, we have T \u2264 (1 \u2212 1 e) \u2212 1 q = 1 log (1 \u2212 1 e) \u2212 1q log (1 + \u03bbmaxT \u03c32), where \u03bbmax is the maximum eigenvalue of G. It is therefore easy to see that we have a regression of T = O (n)."}, {"heading": "5.2 Squared exponential and Mate\u0301rn kernels", "text": "For the squared exponential kernel, we know that \u00b5s \u2264 b d 2 c \u2212 s 1 / d \u00b2 s \u2265 0, where b < 1 and c > 1 are constants and d is the dimension of the decision space. Therefore, for the eigenvalues of the gram matrix, we actually have remorse \u2264 nb d 2 c \u2212 s 1 / d. Therefore, Equation (4) is implied = m1 \u2212 \u03c32cn b d 2 (ct 1 / d \u2212 1 \u2212 1) Since it is intuitive, the allocations decrease when the eigenvalues decrease. Let N0 be the number of non-zero allocations. We want 0 \u2264 mt for t \u2264 N0 \u2212 rn \u2212 s. Since m1 \u2264 T \u2212 imp \u00b2 s 2cn b d d 2 (ct 1 / d \u2212 1 \u2212 1), we have the solution (5) for."}, {"heading": "5.3 Discretization of the decision set", "text": "We need to make sure that the discretization error disappears asympotically - that is, the optimal point above discretization converges to the optimum via the decision rate. First, we show that if f Lipschitz is continuous, this requirement is fulfilled. If we leave x * = argmaxD f and xV = argmaxV f, then we have | F (x *) \u2212 F (xV) | ig x * * \u2212 xV for a certain period of time. Furthermore, if we choose n = x * (T), we have x * - xV = argmaxV. Therefore, an appropriate choice of this (e.g. d2) is sufficient to ensure that the discretization error disappears asymptomatically quickly (i.e. faster than 1 \u00b0 T) because d is solid and known. Therefore, we just need to prove the Lipschitz continuity of our payout function f."}, {"heading": "6 Experiments", "text": "We are conducting some preliminary experiments to show that our limits accurately capture the growth of cumulative regret. We are discrediting the unit interval D = [0, 1] uniformly in V D, where | V | = n = 1000. We performed the UCB algorithm for T = 1000 (results averaged over 30 random runs) for the square exponential core and the mate exponential core (with \u03bd = 2.5), each using the longitudinal scale parameter 0.01. The sampling variance \u03c32 was set to 0.001 and the precision / confidence delta to 01,000. We calculated the actual cumulative regret and the information gained by the UCB algorithm. We also used the greedy algorithm to calculate the regret conditioned by Theorem 1 and the limit of information gained from Theorem 2."}, {"heading": "7 Conclusions", "text": "In this paper, we demonstrate the first sublinear limits of regret for the UCB GP optimization algorithm for several popular core functions. Our limits of regret are independent of the dimension of the decision set for Bayesian linear regression and have a very weak dependence on the quadratic exponential kernel. For the mate brain kernel, we demand greater suppleness to ensure that asymptotic average regret does not arise as the dimension of the decision room increases, which is very intuitive. Furthermore, our evidence assumes that cumulative regret is limited in terms of maximum information gain without assuming any structure in decision room D, thus creating a natural link between Gaussian process bandit optimization and optimal experimental design. Our preliminary experiments suggest that our limits accurately capture the growth of cumulative regret for the UCB algorithm."}, {"heading": "A Proofs", "text": "Firstly, we demonstrate the corlogized results for the Mate class of kernel in Section 5.2.Proof [Regret bounds for Mate \u2012 rn class of kernels] For the Mate \u2012 rn kernels, we have \u00b5s = O (s \u2212 2\u03bd + dd) Vantage control control control control control control control control control control control control control control control control control control control control control control control control control (s \u2212 2\u03bd + dd) Vantage control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control control over control control control control control control control control control control control control control control control control over control control control control control control control control control control over control control control control control control control control over control control control control control control control control control over control over control control control control control over control control control control over control over control control over control over control over control control control over control over control over control over control over control over control control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control over control"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We consider the problem of optimizing an unknown, noisy function that is ex-<lb>pensive to evaluate. We cast this problem as a multiarmed bandit problem where<lb>the payoff function is sampled from a Gaussian Process. We resolve an important<lb>open problem on deriving regret bounds for this setting. In particular, we ana-<lb>lyze an upper confidence algorithm and bound its cumulative regret in terms of<lb>the maximal information gain due to sampling, thus connecting Gaussian Process<lb>bandits and optimal experimental design. Moreover, we bound the maximal infor-<lb>mation gain by exploiting known spectral properties of popular classes of kernels<lb>and obtain sub-linear regret bounds for our algorithm. In particular, we show that,<lb>perhaps surprisingly, the regret bounds for the squared exponential kernel depend<lb>only very weakly on the dimensionality of the problem.", "creator": "LaTeX with hyperref package"}}}