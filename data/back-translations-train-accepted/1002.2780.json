{"id": "1002.2780", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2010", "title": "Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm", "abstract": "We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.", "histories": [["v1", "Sun, 14 Feb 2010 16:37:04 GMT  (39kb)", "http://arxiv.org/abs/1002.2780v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ruslan salakhutdinov", "nathan srebro"], "accepted": true, "id": "1002.2780"}, "pdf": {"name": "1002.2780.pdf", "metadata": {"source": "META", "title": "Collaborative Filtering in a Non-Uniform World:  Learning with the Weighted Trace Norm", "authors": ["Ruslan Salakhutdinov"], "emails": ["rsalakhu@mit.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 100 2.27 80v1 [cs.LG] 1 4Fe b"}, {"heading": "1. Introduction", "text": "In fact, the current theoretical guarantees for the use of the matrix in the completion of the matrix (Srebro et al., 2005b; Bach, 2008; Abernethy et al., 2009; Salakhutdinov & Mnih, 2008), the current theoretical guarantees for the completion of the matrix all presuppose a uniform distribution of the matrix (Srebro et al., 2005b; Abernethy et al., 2005; Candes & Tao, 2009; Candes & Tao, 2009; Recht, 2009).In a collaborative filtering environment in which the rows of the matrix are penetrated, the matrix is represented by the matrix."}, {"heading": "2. Complexity Control in terms of Matrix Factorizations", "text": "Consider the problem of predicting the entries of an unknown target matrix Y-Rn \u00b7 m based on a random subset S of the observed entries YS. For example, n and m can represent the number of users and the number of movies, and Y can represent a matrix of partially observed weights. Predicting elements of Y can be done by finding a matrix X that minimizes the training error, here measured as a square error, and a measurement c (X) of complexity. That is, either minimized: min X-XS \u2212 YS-2F + \u03bb2F (X) (1) or: minc (X) \u2264 C-XS \u2212 YS-2F, (2) where YS and similar XS \"masks\" the matrix by S: (YS) i, j = {Yi, j if (i, j) \u0445S 0, (3)."}, {"heading": "2.1. Low Rank Factorization", "text": "A basic measure of complexity is the rank of X, which corresponds to the minimum dimensionality k, so that X = U V for some U-Rk \u00b7 n and V-Rk \u00b7 m. Restricting the rank of X directly is one of the most popular approaches for collaborative filtering. The formation of such a model amounts to finding the best rank-k approach to the observed target matrix Y under the given loss function. However, the rank is not convex and difficult to minimize. Nor is it clear whether a strict dimensionality constraint is the best way to measure complexity."}, {"heading": "2.2. Trace-norm Regularization", "text": "Recently, methods have been advocated that regulate the norm of factorization U V and not its dimensionality, and they have been shown to have considerable empirical success (Rennie & Srebro, 2005; Salakhutdinov & Mnih, 2008), which is captured by measuring complexity using the trace standard of X, which can be defined as either the sum of the singular values of X or as (Fazel et al., 2001)."}, {"heading": "2.3. Scaling of the Trace-norm", "text": "It will be useful for us to look at the scaling of the trace standard with the size of the matrix X. (For example, this will allow us to understand the size of the bound matrix. (2) The ranking order in which the number of underlying factors is explained when the data is explained by two factors, then it does not matter how many rows (\"Users\") and columns (\"Movies\") we look at, the data will still rank second. (3) The trace standard is inherently scalable with the size of the matrix. To see this, we note that the trace standard is the spectrum while the Frobenius standard is the spectrum. (4)"}, {"heading": "3. Trace-Norm Under a Non-Uniform Distribution", "text": "In this section, we will analyze regularized learning if the sampling distribution is not uniform. That is, if there is some known or unknown non-uniform distribution D over the entries of the matrix Y (i.e. over the index pairs (i, j), and our sample S is not uniform if we focus on a small subset of the matrix. That is, we have no hope of generalizing rows and columns of Y on which we have zero chance of seeing an observation. Instead, our goal here, as is typically the case when learning under an arbitrary distribution, is to obtain a low average error regarding the same distribution."}, {"heading": "4. Weighted Trace Norm", "text": "The decomposition (11) and the discussion (X) in the previous section show that we have agreed on a uniform distribution. (1) The decomposition (11) and the discussion (2) in the previous section (2). (2) The decomposition (2). (2) The decomposition (2). (2) The decomposition (2). (2) The decomposition (2). (2) The decomposition (2). (2) The decomposition (2). (2). (2) The decomposition (2). (2). (2). (2) The decomposition (2). (2). (3). (3). (3). (3. (3). (3). (3. (3). (3). (3). (3). (3). (3. (3). (3). (3. (3). (3). (3. (3). (3). (3. (3). (3). (3). (3). (3. (3). (3). (3). (3). (3). (3). (2). (2). (2). (2). (2)."}, {"heading": "5. Practical Implementation", "text": "When dealing with large datasets, such as Netflix data, the most practical way to adapt to the enormous q-standard models is by stochastic gradient parentage (Salakhutdinov & Mnih, 2008; Koren, 2008). View the number of observed ratings for user i and movie j (each). The training target (via the index pairs (i, j)) using part-weighted trace standard (q. 12) can be called: number of observed ratings for user i and movie j. (Yij \u2212 i Vj) 2 + (16) + \u03bb2 (p (i) \u03b1ni Ui 2 + q (j) trace standard (q. 12), with U Rk \u00d7 n and V Rk \u00b7 m \u00b7 m (Vj \u00d7 n) very likely. We can achieve this goal by stochastic gradient parentage (j, optimize a random zip) by selecting a training spai (j)."}, {"heading": "6. Experimental results", "text": "The training set contains 100,480,507 ratings from 480,189 randomly selected anonymous users for 17,770 movie titles. Netflix also provides qualification data with 1,408,395 ratings as part of the training data. Couples were selected from the latest ratings for a subset of users of the training data set. Due to the special selection scheme, ratings from users with few ratings are over-represented in the training set relative to the training set. To avoid the problem of dealing with different training and test distributions, we have also created our own validation and test sets, each containing 100,000 ratings randomly selected from the training data set. As a starting point, Netflix provided the test value of its own system trained on the same data, which is 0.9514. This data set is interesting for several reasons. First, it is very large and very sparse (98.8%). Second, the data set contains very consistent ratings with less than 10,000 users who are not rated as movies."}, {"heading": "6.1. Results", "text": "In our first experiment for different values of \u03b1, we fit the parameters U and V using stochastic gradient lineage as in (17) with k = 30. Both U and V were randomly initialized for all models and the regularization parameters \u03bb were selected by cross validation. The results of the weighted trace standard Regu-Larization for different values of \u03b1 are shown in Table 1. Note that the weighted trace standard (\u03b1 = 1) achieved an RMSE of 0.9105 on the Netflix qualification table that significantly exceeds its unweighted counterpart with \u03b1 = 0, resulting in an RMSE of 0.9235. This large performance gap is noteworthy. Table 1 also shows that the weighted trace standard (\u03b1 = 1) is not optimal. Surprisingly, partially weighted trace standards with \u03b1 = 0.9 result in an RMSE of 0.9091, slightly better than the weighted matrix factorization."}, {"heading": "7. Discussion", "text": "Motivated by our analytical analysis, we also proposed a revised version of the weighted trace standard, known as the weighted trace standard, which takes into account the unequal distribution of sampling values. Furthermore, our results for both synthetic and grossly unbalanced Netflix datasets show that the weighted trace standard brings significant improvements in predictive quality. Interestingly, the setting of \u03b1 = 1 in the weighted trace standard target (12) implies that frequent users (movies) are much more heavily regulated than the rare users (movies). From a Bayesian perspective, such regulation is quite unusual, as it effectively states that the effect of the previous generalization is strengthened as we observe more data. However, our analysis and empirical results strongly suggest that such \"unorthodox\" regulation plays an inconsistent role in achieving these results."}, {"heading": "Acknowledgments", "text": "R.S. would like to thank NSERC, Shell and the NTT Communication Sciences Laboratory for their financial support."}], "references": [{"title": "A new approach to collaborative filtering: Operator estimation with spectral regularization", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "J.P. Vert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "Consistency of trace norm minimization", "author": ["F. Bach"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Local minima and convergence in low-rank semidefinite programming", "author": ["S. Burer", "R.D.C. Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "Burer and Monteiro,? \\Q2005\\E", "shortCiteRegEx": "Burer and Monteiro", "year": 2005}, {"title": "Matrix completion with noise", "author": ["E.J. Candes", "Y. Plan"], "venue": "Proceedings of the IEEE (to appear),", "citeRegEx": "Candes and Plan,? \\Q2009\\E", "shortCiteRegEx": "Candes and Plan", "year": 2009}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Candes and Recht,? \\Q2009\\E", "shortCiteRegEx": "Candes and Recht", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Trans. Inform. Theory (to appear),", "citeRegEx": "Candes and Tao,? \\Q2009\\E", "shortCiteRegEx": "Candes and Tao", "year": 2009}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "In Proceedings American Control Conference,", "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Koren", "Yehuda"], "venue": "In ACM SIGKDD,", "citeRegEx": "Koren and Yehuda.,? \\Q2008\\E", "shortCiteRegEx": "Koren and Yehuda.", "year": 2008}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "preprint, available from author\u2019s webpage,", "citeRegEx": "Recht,? \\Q2009\\E", "shortCiteRegEx": "Recht", "year": 2009}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["J.D.M. Rennie", "N. Srebro"], "venue": "In ICML, pp", "citeRegEx": "Rennie and Srebro,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro", "year": 2005}, {"title": "Probabilistic matrix factorization", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Generalization error bounds for collaborative prediction with lowrank matrices", "author": ["N. Srebro", "N. Alon", "T. Jaakkola"], "venue": "In Advances In Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Maximum margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "In Advances In Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Scalable collaborative filtering approaches for large recommender systems", "author": ["Tak\u00e1cs", "G\u00e1bor", "Pil\u00e1szy", "Istv\u00e1n", "N\u00e9meth", "Botty\u00e1n", "Tikk", "Domonkos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tak\u00e1cs et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tak\u00e1cs et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Trace-norm regularization is a popular approach for matrix completion and collaborative filtering, motivated both as a convex surrogate to the rank (Fazel et al., 2001; Candes & Tao, 2009) and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning (Srebro et al.", "startOffset": 148, "endOffset": 188}, {"referenceID": 1, "context": ", 2001; Candes & Tao, 2009) and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning (Srebro et al., 2005b; Bach, 2008; Abernethy et al., 2009; Salakhutdinov & Mnih, 2008).", "startOffset": 139, "endOffset": 225}, {"referenceID": 0, "context": ", 2001; Candes & Tao, 2009) and in terms of a regularized infinite factor model with connections to large-margin norm-regularized learning (Srebro et al., 2005b; Bach, 2008; Abernethy et al., 2009; Salakhutdinov & Mnih, 2008).", "startOffset": 139, "endOffset": 225}, {"referenceID": 8, "context": "Current theoretical guarantees on using the tracenorm for matrix completion all assume a uniform sampling distribution over entries of the matrix (Srebro & Shraibman, 2005; Candes & Tao, 2009; Candes & Recht, 2009; Candes & Tao, 2009; Recht, 2009).", "startOffset": 146, "endOffset": 247}, {"referenceID": 6, "context": "This is captured by measuring complexity in terms of the trace-norm of X , which can be defined equivalently either as the sum of the singular values of X , or as (Fazel et al., 2001):", "startOffset": 163, "endOffset": 183}, {"referenceID": 13, "context": "Strangely, and likely originating as a \u201cbug\u201d in calculating the stochastic gradients by one of the participants, these are the actual SGD steps used by many practitioners on the Netflix dataset (Koren, 2008; Tak\u00e1cs et al., 2009; Salakhutdinov & Mnih, 2008).", "startOffset": 194, "endOffset": 256}, {"referenceID": 8, "context": "Although theoretical guarantees are not the focus of this work, we hope that the weighted tracenorm, and the discussions in Sections 3 and 4, will be helpful in deriving theoretical learning guarantees for non-uniform sampling distributions, both in the form of generalization error bounds as in (Srebro & Shraibman, 2005), and generalizing the compressed-sensing inspired work on recovery of noisy low-rank matrices as in (Candes & Plan, 2009; Recht, 2009).", "startOffset": 423, "endOffset": 457}], "year": 2010, "abstractText": "We show that matrix completion with tracenorm regularization can be significantly hurt when entries of the matrix are sampled nonuniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.", "creator": "LaTeX with hyperref package"}}}