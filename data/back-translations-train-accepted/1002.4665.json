{"id": "1002.4665", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2010", "title": "Syntactic Topic Models", "abstract": "The syntactic topic model (STM) is a Bayesian nonparametric model of language that discovers latent distributions of words (topics) that are both semantically and syntactically coherent. The STM models dependency parsed corpora where sentences are grouped into documents. It assumes that each word is drawn from a latent topic chosen by combining document-level features and the local syntactic context. Each document has a distribution over latent topics, as in topic models, which provides the semantic consistency. Each element in the dependency parse tree also has a distribution over the topics of its children, as in latent-state syntax models, which provides the syntactic consistency. These distributions are convolved so that the topic of each word is likely under both its document and syntactic context. We derive a fast posterior inference algorithm based on variational methods. We report qualitative and quantitative studies on both synthetic data and hand-parsed documents. We show that the STM is a more predictive model of language than current models based only on syntax or only on topics.", "histories": [["v1", "Thu, 25 Feb 2010 00:00:47 GMT  (1485kb,DS)", "http://arxiv.org/abs/1002.4665v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI math.ST stat.TH", "authors": ["jordan l boyd-graber", "david m blei"], "accepted": true, "id": "1002.4665"}, "pdf": {"name": "1002.4665.pdf", "metadata": {"source": "CRF", "title": "Syntactic Topic Models", "authors": ["Jordan Boyd-Graber", "David M. Blei"], "emails": ["jbg@umiacs.umd.edu.", "blei@cs.princeton.edu."], "sections": [{"heading": null, "text": "In the second half of the last decade, we have it in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last century, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the second decade, and in the second half of the last decade."}, {"heading": "1. Background: Topics and Syntax", "text": "Our approach is based on probabilistic topic models, probabilistic syntactical models and Bayean non-parametric methods, which we review here."}, {"heading": "1.1 Probabilistic Topic Models", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "1.2 Probabilistic Syntax Models", "text": "LDA is effective in capturing semantic correlations between words, but it ignores syntactic correlations and connections. The finite tree with the independent child model (FTIC) can be considered as a syntactic complement to LDA (Finkel et al. 2007). As in LDA, this model assumes that observed words are generated by latent states. Instead of looking at words in the context of their common document, the FTIC considers each word in the context of its sentence as determined by its location in a dependency parameter. FTIC embodies a generative process through a collection of sentences with given parameters. It is parameterized by a series of \"syntactic states,\" in which each state is associated with three parameters: a distribution over terms, a set of transition probabilities to other states, and a probability to be selected as root state. Each sentence is generated by traversing the structure of the parameter tree."}, {"heading": "1.3 Random Distributions and Bayesian non-parametric methods", "text": "Many recently developed probability models of language, including those described above, determine distributions as random variables. Therefore, these random distributions are sometimes a history of a parameter, as in traditional Bayesian statistics, or a latent variable within the model. For example, in LDA, the thematic proportions and themes are random distributions; in FTIC, the transition probabilities and the term that generates distributions are random. In this section, we will describe the connection between the pitch-breaking distribution and the dirichlet process, which is a distribution of multinomial parameters, and the distribution of multinomial parameters, a distribution to multinomial vectors with a countable infinite number of components. We will describe the connection between the pitch-breaking distribution and the dirichlet distribution, which is a distribution through arbitrary discrete distributions and a fundamental building block of Bayesian nonparametric methods. These distributions are pivotal to the STA distribution over a dimensional element (M.K) \u2212"}, {"heading": "2. The Syntactic Topic Model", "text": "We assume that the words are documents used in documents (STM), a hierarchical probability model of the language that reflects both the syntax of the language and the themes of the documents. To STM, our observed data are documents, each of which is a collection of dependency trees. (Note: in LDA, the documents are simply word collections.) The main idea is that words emerge from themes that depend on themes of the language and the themes of the documentation. To STM, our observed data are documents, each of which is a collection of dependency trees."}, {"heading": "2.1 Relationships to Other Work", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2.2 Posterior inference with variational methods", "text": "We have described the modeling assumptions behind the STM. How detailed does the STM assume a decomposition of the parsed corpus by a hidden semantic and syntactic structure encoded with latent variables? Given a data set, the central arithmetic challenge for the STM is to calculate the posterior distribution of this hidden structure, since the observed documents and data analysis are driven by the study of this distribution. (The calculation of posterior topics is \"Learning from the Perspective of Bayean Statistics.\" This posterior distribution, as for many hierarchical Bayean models, is not tractable to calculate accurately and we must appeal for an approximation. (The development of algorithms for approximating subordinate or distributions of complex hierarchical models is an active research problem in Bayean statistics and machine learning.) One of the most commonly used approximation techniques for such models is Monte Carlo Markov sample from a chain of 1993 (where a MC sample is distributed)."}, {"heading": "Eq [log p(\u03b2|\u03b1)] + Eq [log p(\u03b8|\u03b1D,\u03b2)] + Eq [log p(\u03c0|\u03b1P ,\u03b2)] + Eq [log p(z|\u03b8,\u03c0)]", "text": "This is tantamount to minimizing the KL divergence between the variational distribution and the posterior plane. Once the variational distribution is used as an approximation to the posterior parameters, the general trading behavior between variant methods and Gibbs sampling is understood as an open research question. Equation 2 optimization proceeds through coordinated ascents, optimization of each variation parameter while the others are fixed. Each pass through the variable parameters increases the ELBO, and we iterate this process until reaching a local optimum. If possible, we find the maximum percentages in closed form. If such updates are not possible, we work on optimization of the various parameters until reaching a local optimum."}, {"heading": "3. Experiments", "text": "We show how the STM works on datasets of increasing complexity. First, we show that the STM captures properties of a simple synthetic dataset that elude both thematic and syntactic models individually. Second, we use a larger real dataset of hand-analyzed sets to show that both thematic and syntactic information is captured by the STM."}, {"heading": "3.1 Topics Learned from Synthetic Data", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.2 Qualitative Description of Topics learned by the STM from Hand-annotated Data", "text": "The same general characteristics, but with greater variation, are represented in real data. We converted Penn Treebank (Marcus et al. 1994), a corpus of manually curated parse trees, into a dependency parse (Johansson and Nugues 2007). Vocabulary was tailored to terms that appeared in at least ten documentaries. Figure 7 shows a subset of topics learned from STM with level 32 abbreviations. Many of the resulting topics illustrate both syntactical and thematic consistency. Some non-specific functional topics emerged (pronouns, possessive pronouns, general verbs, etc.). Many of the nouns categories were more specialized. Thus, Figure 7 also shows clusters of nouns related to media, individuals associated with companies (\"mr,\" \"president,\" \"chairman\"), and abstract nouns related to stock prices (\"shares,\" \"interest\")."}, {"heading": "3.3 Quantitative Results on Synthetic and Hand-annotated Data", "text": "For each position in the parse trees, we estimate the probability of the observed word. We calculate the perplexity as an exponent of the inverse of the average protocol probability per word. The lower the perplexity, the better the model has captured the patterns in the data. We also calculate the perplexity for individual language parts to examine the differences in predictive power between substantive words, such as nouns and verbs, and functional words, such as prepositions and determinators, showing how different algorithms better capture aspects of the context. We expect functional words to be dominated by local context and content words, which are more strongly determined by the themes of the documentation.This trend is evident not only in the synthetic data (Figure 8 (a)), where syntactic models better predict functional categories, such as prepositions, and document models are not responsible for STP patterns, but also for more competitive DP verbs and reverbs."}, {"heading": "4. Conclusion", "text": "In this paper, we examined the common threads linking syntactical and thematic models.More generally, this work serves as an example of how a mixture model can support two different, simultaneous explanations of how the latent class is selected. Although this model uses discrete observations, the variation structure is flexible enough to support other distributions via output. While the primary goal of this work was to show how these two views of the context can be learned simultaneously, there are a number of extensions that could lead to more precise parsers.First, this model could be expanded by integrating a richer syntactic model that not only models the words that appear in a given structure, but also one that models the parcel structure itself, which would allow the model to use large, diverse corpora without relying on an external parser to provide the tree structure.Eliminating the problem between the children for assignment of the individual information between the components would allow it to distinguish between the synthetics."}, {"heading": "Appendix A: Document Likelihood Bound", "text": "In this section we strive to calculate the probability lower limit completely by explicitly extending the expectations in terms of the variable distribution. First, we extend Eq [log-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-"}, {"heading": "Appendix B: Document-specific Variational Updates", "text": "In this section we derive the updates for all document-specific variation parameters except \u03c6n, which are updated according to Equation 3.Since we cannot assume that the point-by-point product of \u03c0k and \u03b8d results in sums to one, we have introduced a lax term \u03c9n into Equation A.1; its update is\u03c9n = \u2211 i = \u2211 i = 1 \u2211 p (n), j \u03b3i\u03bdj, i \u2211 K = 1 \u03b3k \u2211 K = 1 \u03bdj, k. Since we couple both components of the vector, the interaction between these terms in the normalizer prevents us from expressly solving the optimization. Instead, we calculate for each \u03b3d the partial derivative relating to \u03b3d, i for each component of the vector. We then maximize the probability limit for each \u03b3d. In deriving the gradient, the following derivative of the equation is useful: f (x) \u2212 j \u2212 n = N = jj \u00b2 \u00b2 This value results from the probability that is limited for each \u0421d."}, {"heading": "Appendix C: Global Updates", "text": "In this section, we expand the terms of Equation 2, which were not extended in Equation A.2. First, we note that Eq [log GEM (\u03b2; \u03b1)], because the distribution of variations only sets the weight to \u03b2 *, is only log GEM (\u03b2 *; \u03b1). We can return to stick-breaking weights by dividing each \u03b2 * z by the sum of all indices greater than z (remembering that \u03b2 sets the sums to one), Tz + log Tz + log Tz = 1 = 1 \u03b2i = 1 \u03b2i. Using this reformulation, the total probability limit, including Equation A.2 as Ld, is then L = M \u00b2 d Ld + (\u03b1 \u2212 1) log TK \u2212 1 log Tz + log Tz = 1 \u03b2i = 1 \u03b2i = optimization."}], "references": [{"title": "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems", "author": ["E. Antoniak"], "venue": "The Annals of Statistics,", "citeRegEx": "Antoniak.,? \\Q1974\\E", "shortCiteRegEx": "Antoniak.", "year": 1974}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["Blei", "Jordan2005]David M. Blei", "Michael I. Jordan"], "venue": "Journal of Bayesian Analysis,", "citeRegEx": "Blei et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2005}, {"title": "Dynamic topic models", "author": ["Blei", "Lafferty2006]David M. Blei", "John D. Lafferty"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Supervised topic models", "author": ["Blei", "McAuliffe2007]David M. Blei", "Jon D. McAuliffe"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Blei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2007}, {"title": "The nested chinese restaurant process and hierarchical topic models, Oct", "author": ["Blei et al.2007]David M. Blei", "Thomas L. Griffiths", "Michael I. Jordan"], "venue": null, "citeRegEx": "Blei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2007}, {"title": "Bayesian synchronous grammar induction", "author": ["Blunsom et al.2008]Phil Blunsom", "Trevor Cohn", "Miles Osborne"], "venue": "In Proc of NIPS", "citeRegEx": "Blunsom et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2008}, {"title": "A topic model for word sense disambiguation", "author": ["David M. Blei", "Xiaojin Zhu"], "venue": "In Proceedings of Emperical Methods in Natural Language Processing", "citeRegEx": "Boyd.Graber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Boyd.Graber et al\\.", "year": 2007}, {"title": "Bayesian word sense induction", "author": ["Brody", "Lapata2009]Samuel Brody", "Mirella Lapata"], "venue": "In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL", "citeRegEx": "Brody et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brody et al\\.", "year": 2009}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Cohen et al.2008]Shay B. Cohen", "Kevin Gimpel", "Noah A. Smith"], "venue": null, "citeRegEx": "Cohen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Form and content: Dissociating syntax and semantics in sentence comprehension", "author": ["Dapretto", "Bookheimer1999]Mirella Dapretto", "Susan Y. Bookheimer"], "venue": null, "citeRegEx": "Dapretto et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dapretto et al\\.", "year": 1999}, {"title": "Indexing by latent semantic analysis", "author": ["Susan Dumais", "Thomas Landauer", "George Furnas", "Richard Harshman"], "venue": "Journal of the American Society of Information", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Describing disability through individual-level mixture models for multivariate binary data", "author": ["Stephen E. Fienberg", "Cyrille Joutard"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Erosheva et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Erosheva et al\\.", "year": 2007}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["Escobar", "West1995]Michael D. Escobar", "Mike West"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Escobar et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Escobar et al\\.", "year": 1995}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["S. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "Ferguson.,? \\Q1973\\E", "shortCiteRegEx": "Ferguson.", "year": 1973}, {"title": "The infinite tree", "author": ["Trond Grenager", "Christopher D. Manning"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "Finkel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2007}, {"title": "Finding scientific topics", "author": ["Griffiths", "Steyvers2004]Thomas L. Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Integrating topics and syntax", "author": ["Mark Steyvers", "David M. Blei", "Joshua B. Tenenbaum"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Griffiths et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2005}, {"title": "Topics in semantic representation", "author": ["Mark Steyvers", "Joshua Tenenbaum"], "venue": "Psychological Review,", "citeRegEx": "Griffiths et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2007}, {"title": "Hidden topic Markov models", "author": ["Gruber et al.2007]Amit Gruber", "Michael Rosen-Zvi", "Yair Weiss"], "venue": "In Artificial Intelligence and Statistics", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Studying the history of ideas using topic models", "author": ["Hall et al.2008]David Hall", "Daniel Jurafsky", "Christopher D. Manning"], "venue": "In Proceedings of Emperical Methods in Natural Language Processing,", "citeRegEx": "Hall et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2008}, {"title": "A probabilistic model of unsupervised learning for musical-key profiles", "author": ["Hu", "Saul2009]Diane Hu", "Lawrence K. Saul"], "venue": "In International Society for Music Information Retrieval Conference", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Extended constituent-to-dependency conversion for English", "author": ["Johansson", "Nugues2007]Richard Johansson", "Pierre Nugues"], "venue": "In Proceedings of the Nordic Conference on Computational Linguistics", "citeRegEx": "Johansson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2007}, {"title": "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models", "author": ["Johnson et al.2006]Mark Johnson", "Thomas L. Griffiths", "Sharon Goldwater"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Johnson et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2006}, {"title": "An introduction to variational methods for graphical models", "author": ["Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Fast exact inference with a factored model for natural language parsing", "author": ["Klein", "Manning2002]Dan Klein", "Christopher D. Manning"], "venue": "In Advances in Neural Information Processing", "citeRegEx": "Klein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2002}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003]Dan Klein", "Christopher D. Manning"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Accelerated variational Dirichlet process mixtures", "author": ["Max Welling", "Nikos Vlassis"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "Kurihara et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kurihara et al\\.", "year": 2007}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["Li Fei-Fei", "Perona2005]Li Fei-Fei", "Pietro Perona"], "venue": "In CVPR \u201905 - Volume", "citeRegEx": "Fei.Fei et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2005}, {"title": "Structured Bayesian nonparametric models with variational inference (tutorial)", "author": ["Liang", "Klein2007]Percy Liang", "Dan Klein"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Liang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2007}, {"title": "The infinite PCFG using hierarchical Dirichlet processes", "author": ["Liang et al.2007]Percy Liang", "Slav Petrov", "Michael Jordan", "Dan Klein"], "venue": "In Proceedings of Emperical Methods in Natural Language Processing,", "citeRegEx": "Liang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2007}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Beatrice Santorini", "Mary A. Marcinkiewicz"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1994}, {"title": "Mining a digital library for influential authors", "author": ["Mimno", "McCallum2007]David Mimno", "Andrew McCallum"], "venue": "In JCDL \u201907: Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries,", "citeRegEx": "Mimno et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2007}, {"title": "Probabilistic inference using Markov chain Monte Carlo methods", "author": ["M. Neal"], "venue": "Technical Report CRG-TR-93-1,", "citeRegEx": "Neal.,? \\Q1993\\E", "shortCiteRegEx": "Neal.", "year": 1993}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["M. Neal"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Neal.,? \\Q2000\\E", "shortCiteRegEx": "Neal.", "year": 2000}, {"title": "Inference of population structure using multilocus genotype", "author": ["Matthew Stephens", "Peter Donnelly"], "venue": "data. Genetics,", "citeRegEx": "Pritchard et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pritchard et al\\.", "year": 2000}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["Purver et al.2006]Matthew Purver", "Konrad K\u00f6rding", "Thomas L. Griffiths", "Joshua Tenenbaum"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Purver et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Purver et al\\.", "year": 2006}, {"title": "Minimized models for unsupervised part-of-speech tagging", "author": ["Ravi", "Knight2009]Sujith Ravi", "Kevin Knight"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,", "citeRegEx": "Ravi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2009}, {"title": "The interaction of syntax and semantics during sentence processing \u2014 Eye-movements in the analysis of semantically biased sentences", "author": ["Rayner et al.1983]Keith Rayner", "Marcia Carlson", "Lyn Frazier"], "venue": "Journal of Verbal Learning and Verbal Behavior,", "citeRegEx": "Rayner et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Rayner et al\\.", "year": 1983}, {"title": "The author-topic model for authors and documents", "author": ["Thomas L. Griffiths", "Mark Steyvers", "Padhraic Smyth"], "venue": "In Proceedings of Uncertainty in Artificial Intelligence,", "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Hierarchical Dirichlet processes", "author": ["Teh et al.2006]Yee Whye Teh", "Michael I. Jordan", "Matthew J. Beal", "David M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Whye Teh"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Teh.,? \\Q2006\\E", "shortCiteRegEx": "Teh.", "year": 2006}, {"title": "A joint model of text and aspect ratings for sentiment summarization", "author": ["Titov", "McDonald2008]Ivan Titov", "Ryan McDonald"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Titov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2008}, {"title": "A Bayesian LDA-based model for semi-supervised part-of-speech tagging", "author": ["Toutanova", "Johnson2008]Kristina Toutanova", "Mark Johnson"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Toutanova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1\u20132):1\u2013305", "author": ["Wainwright", "Jordan2008]Martin J. Wainwright", "Michael I. Jordan"], "venue": null, "citeRegEx": "Wainwright et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2008}, {"title": "Topic modeling: beyond bag-of-words", "author": ["M. Wallach"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "Wallach.,? \\Q2006\\E", "shortCiteRegEx": "Wallach.", "year": 2006}, {"title": "Continuous time dynamic topic models", "author": ["Wang et al.2008]Chong Wang", "David M. Blei", "David Heckerman"], "venue": "In Proceedings of Uncertainty in Artificial Intelligence", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Fully distributed EM for very large datasets", "author": ["Wolfe et al.2008]Jason Wolfe", "Aria Haghighi", "Dan Klein"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Wolfe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wolfe et al\\.", "year": 2008}, {"title": "TagLDA: Bringing document structure knowledge into topic models", "author": ["Zhu et al.2006]Xiaojin Zhu", "David M. Blei", "John Lafferty"], "venue": "Technical Report TR-1553,", "citeRegEx": "Zhu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 38, "context": "A syntactically correct sentence that is semantically implausible takes longer for people to understand than its semantically plausible counterpart (Rayner et al. 1983).", "startOffset": 148, "endOffset": 168}, {"referenceID": 17, "context": "On the other hand, probabilistic topic models find patterns of words that are thematically related in a large collection of documents (Blei et al. 2003; Griffiths et al. 2007).", "startOffset": 134, "endOffset": 175}, {"referenceID": 40, "context": "In both topic models and syntax models, Bayesian non-parametric methods are used to embed the choice of the number of components into the model (Teh et al. 2006; Finkel et al. 2007).", "startOffset": 144, "endOffset": 181}, {"referenceID": 14, "context": "In both topic models and syntax models, Bayesian non-parametric methods are used to embed the choice of the number of components into the model (Teh et al. 2006; Finkel et al. 2007).", "startOffset": 144, "endOffset": 181}, {"referenceID": 39, "context": ", to authorship (Rosen-Zvi et al. 2004), citation (Mimno and McCallum 2007), sentiment analysis (Blei and McAuliffe 2007; Titov and McDonald 2008), corpus exploration (Hall et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 19, "context": "2004), citation (Mimno and McCallum 2007), sentiment analysis (Blei and McAuliffe 2007; Titov and McDonald 2008), corpus exploration (Hall et al. 2008), part-of-speech labeling (Toutanova and Johnson 2008), discourse segmentation (Purver et al.", "startOffset": 133, "endOffset": 151}, {"referenceID": 36, "context": "2008), part-of-speech labeling (Toutanova and Johnson 2008), discourse segmentation (Purver et al. 2006), word sense induction (Brody and Lapata 2009), and word sense disambiguation (Boyd-Graber et al.", "startOffset": 84, "endOffset": 104}, {"referenceID": 6, "context": "2006), word sense induction (Brody and Lapata 2009), and word sense disambiguation (Boyd-Graber et al. 2007).", "startOffset": 83, "endOffset": 108}, {"referenceID": 35, "context": "Topic models have also been applied to non-language data, such as images (Li Fei-Fei and Perona 2005), population genetics (Pritchard et al. 2000), and music (Hu and Saul 2009).", "startOffset": 123, "endOffset": 146}, {"referenceID": 17, "context": "There are several reviews of topic modeling and related literature (Blei and Lafferty 2009; Griffiths et al. 2007).", "startOffset": 67, "endOffset": 114}, {"referenceID": 11, "context": "In statistics, these kinds of assumptions are called mixed-membership assumptions (Erosheva et al. 2007).", "startOffset": 82, "endOffset": 104}, {"referenceID": 17, "context": "2 The topics tend to correspond to a psychologically plausible interpretation of the themes that pervade the documents (Griffiths et al. 2007).", "startOffset": 119, "endOffset": 142}, {"referenceID": 10, "context": "Topic models represent a fully probabilistic perspective on techniques like latent semantic analysis (LSA) (Deerwester et al. 1990) and probabilistic latent semantic analysis (pLSA) (Hofmann 1999).", "startOffset": 107, "endOffset": 131}, {"referenceID": 14, "context": "The finite tree with independent children model (FTIC) can be seen as the syntactic complement to LDA (Finkel et al. 2007).", "startOffset": 102, "endOffset": 122}, {"referenceID": 14, "context": "The states discovered through posterior inference correlate with part of speech labels (Finkel et al. 2007).", "startOffset": 87, "endOffset": 107}, {"referenceID": 40, "context": "Recent research has extended Bayesian non-parametric methods to build more flexible models where the number of latent components is unbounded and is determined by the data (Teh et al. 2006; Liang and Klein 2007).", "startOffset": 172, "endOffset": 211}, {"referenceID": 8, "context": "These have proved promising in limited-data frameworks; the logistic normal prior has been applied to grammar induction (Cohen et al. 2008) and integer programming has been applied to unsupervised part of speech tagging (Ravi and Knight 2009).", "startOffset": 120, "endOffset": 139}, {"referenceID": 40, "context": "This extension is an application of a hierarchical Dirichlet process (HDP), a model of grouped data where each group arises from a DP whose base measure is itself a draw from a DP (Teh et al. 2006).", "startOffset": 180, "endOffset": 197}, {"referenceID": 14, "context": "This extension is described as the infinite tree with independent children (ITIC) (Finkel et al. 2007).", "startOffset": 82, "endOffset": 102}, {"referenceID": 28, "context": "Other work has applied this non-parametric framework to create language models (Teh 2006), full parsers for Chomsky normal form grammars (Liang et al. 2007), models of lexical acquisition (Goldwater 2007), synchronous grammars (Blunsom et al.", "startOffset": 137, "endOffset": 156}, {"referenceID": 5, "context": "2007), models of lexical acquisition (Goldwater 2007), synchronous grammars (Blunsom et al. 2008), and adaptor grammars for morphological segmentation (Johnson et al.", "startOffset": 76, "endOffset": 97}, {"referenceID": 22, "context": "2008), and adaptor grammars for morphological segmentation (Johnson et al. 2006).", "startOffset": 59, "endOffset": 80}, {"referenceID": 48, "context": "This is the approach taken by TagLDA (Zhu et al. 2006), where each word is associated with a single tag (such as a part of speech), and the model learns a weighting over the vocabulary terms for each tag.", "startOffset": 37, "endOffset": 54}, {"referenceID": 18, "context": "One example is the hidden topic Markov model (Gruber et al. 2007), which finds chains of homogeneous topics within a document.", "startOffset": 45, "endOffset": 65}, {"referenceID": 22, "context": "Some parsing formalisms, such as adaptor grammars (Johnson et al. 2006; Johnson 2009), are broad and expressive enough to also describe topic models.", "startOffset": 50, "endOffset": 85}, {"referenceID": 14, "context": "Gibbs sampling in particular, where the Markov chain is defined by the conditional distribution of each latent variable, has found widespread use in Bayesian non-parametric models and topic models (Neal 1993; Teh 2006; Griffiths and Steyvers 2004; Finkel et al. 2007).", "startOffset": 197, "endOffset": 267}, {"referenceID": 23, "context": "variables that is close to the posterior of interest (Jordan et al. 1999; Wainwright and Jordan 2008).", "startOffset": 53, "endOffset": 101}, {"referenceID": 40, "context": "Variational methods provide effective approximations in topic models and non-parametric Bayesian models (Blei et al. 2003; Blei and Jordan 2005; Teh et al. 2006; Liang et al. 2007; Kurihara et al. 2007).", "startOffset": 104, "endOffset": 202}, {"referenceID": 28, "context": "Variational methods provide effective approximations in topic models and non-parametric Bayesian models (Blei et al. 2003; Blei and Jordan 2005; Teh et al. 2006; Liang et al. 2007; Kurihara et al. 2007).", "startOffset": 104, "endOffset": 202}, {"referenceID": 26, "context": "Variational methods provide effective approximations in topic models and non-parametric Bayesian models (Blei et al. 2003; Blei and Jordan 2005; Teh et al. 2006; Liang et al. 2007; Kurihara et al. 2007).", "startOffset": 104, "endOffset": 202}, {"referenceID": 46, "context": "Non-conjugate pairs appear in the dynamic topic model (Blei and Lafferty 2006; Wang et al. 2008), correlated topic model (Blei et al.", "startOffset": 54, "endOffset": 96}, {"referenceID": 3, "context": "2008), correlated topic model (Blei et al. 2007), and in the STM considered here.", "startOffset": 30, "endOffset": 48}, {"referenceID": 47, "context": "Each document optimization, however, produces expected counts which are summed together; this is similar to the how the the E-step of EM algorithms can be parallelized and summed as input to the M-step (Wolfe et al. 2008).", "startOffset": 202, "endOffset": 221}, {"referenceID": 31, "context": "We converted the Penn Treebank (Marcus et al. 1994), a corpus of manually curated parse trees, into a dependency parse (Johansson and Nugues 2007).", "startOffset": 31, "endOffset": 51}, {"referenceID": 16, "context": "Griffiths et al (Griffiths et al. 2005) observed that nouns, more than other parts of speech, tend to specialize into distinct topics, and this is also evident here.", "startOffset": 16, "endOffset": 39}], "year": 2010, "abstractText": "The syntactic topic model (STM) is a Bayesian nonparametric model of language that discovers latent distributions of words (topics) that are both semantically and syntactically coherent. The STM models dependency parsed corpora where sentences are grouped into documents. It assumes that each word is drawn from a latent topic chosen by combining document-level features and the local syntactic context. Each document has a distribution over latent topics, as in topic models, which provides the semantic consistency. Each element in the dependency parse tree also has a distribution over the topics of its children, as in latent-state syntax models, which provides the syntactic consistency. These distributions are convolved so that the topic of each word is likely under both its document and syntactic context. We derive a fast posterior inference algorithm based on variational methods. We report qualitative and quantitative studies on both synthetic data and hand-parsed documents. We show that the STM is a more predictive model of language than current models based only on syntax or only on topics.", "creator": "LaTeX with hyperref package"}}}