{"id": "1003.0024", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2010", "title": "Asymptotic Analysis of Generative Semi-Supervised Learning", "abstract": "Semisupervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distribution-free analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP.", "histories": [["v1", "Fri, 26 Feb 2010 21:59:02 GMT  (280kb)", "http://arxiv.org/abs/1003.0024v1", "12 pages, 9 figures"]], "COMMENTS": "12 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joshua v dillon", "krishnakumar balasubramanian", "guy lebanon"], "accepted": true, "id": "1003.0024"}, "pdf": {"name": "1003.0024.pdf", "metadata": {"source": "CRF", "title": "Asymptotic Analysis of Generative Semi-Supervised Learning", "authors": ["Joshua V Dillon", "Krishnakumar Balasubramanian"], "emails": ["jvdillon@gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 100 3.00 24v1 [cs.LG] 2 6Semi-Supervised Learning has become a popular framework for improving modeling accuracy while controlling labeling costs. We quantify the asymptotic accuracy of generative semi-supervised learning based on an expansion of stochastic composite probability, complementing the non-distributional analysis by providing an alternative framework for measuring the value associated with different labeling policies and solving the fundamental question of how much data should be labeled and how. We demonstrate our approach with simulation studies as well as with real-world experiments, using naive bayes for text classification and MRFs and CRFs for structured predictions in NLP."}, {"heading": "1 Introduction", "text": "It is particularly useful if the costs of obtaining laboratories and laboratories are different. In particular, if one assumes that unlabeled data is more readily available, the dependence on the amount of unlabeled and unlabeled data is very high."}, {"heading": "2 Related Work", "text": "Perhaps the first study in this area was carried out by Castelli and Cover [3], who examined the convergence of the classification error rate as an example with a label added to an unlabeled dataset taken from a Gaussian mixing model. Nigam et al. [9] proposed a practical SSL framework based on maximizing the probability of the observed data. An edited volume describing recent developments is [4]. However, the goal of quantifying the effect of SSL theoretically has recently gained increased attention. Sinha and Belkin [11] investigated the effect of using unlabeled samples with imperfect models for mixing models. Balcan and Blum [1] and Singh et al. [10] analyze discriminatory SSL using PAC theory and large deviations. Additional analyses were conducted under certain distribution assumptions such as the \"cluster assumption,\" the \"smoothness\" and \"low density.\""}, {"heading": "3 Stochastic SSL Estimators", "text": "Generative SSL [9, 4] estimates a parametric model by maximizing the observed probability using L ratios and U ratios (1) without labeling examples (\u03b8) = L ratios (X (i), Y (i)) + L + U ratios (I). A classic example is the naive Bayes model in [9] in which the equation (X, Y) = labeling capabilities (X | Y), p\u03b8 (Y = y) = Mult (1,., [bond] V) is marginalized. A classic example is the naive Bayes model in [9] in which the equation (X, Y) = labeling capabilities (X | Y) p (Y), p\u03b8 (Y = y) = Mult (1,., [bond] V). However, the framework is general enough to apply to any generative model (X, Y).To analyze the asymptotic behavior of the Maximizers (1), we assume this."}, {"heading": "4 A1: Consistency (Classification)", "text": "Assuming that the data is generated from packing (X, Y), the consistency of the packing exception does not follow the consistency of packing. (Since the consistency of packing does not correspond to the convergence of packing (D = > examples of packing convergence). (D) However, this implies that within the boundary of large data, our estimator would be true. (2) This is not an unexpected conclusion, but for completeness reasons, which we rigorously prove here. Evidence technology will also be applied later when discussing the consistency of SSL estimators for structured predictions.The central idea of the evidence is to cast the generative SSL estimation problem as an extension of stochastic composition probability."}, {"heading": "5 A2: Accuracy (Classification)", "text": "The proposition below states that the distribution of the maximizers of (2) is asymptotically normal and returns their variance, which can be used to characterize the accuracy of proposition 2 (below) and in proposition 4. \u2212 Proposition 2 (below) and in proposition 4 we use Var 2 (H) to denote the variance matrix of a random vector H under Pempel 0. The notation p \u2192, denote convergences in probability and distribution [6] and in proposition 2 (below) are the Var 2 (H), which denotes the variance matrix of a random vector H under Pempel 0. The notation p \u2192, denote convergences in probability and distribution [6] and proposition f (above) are the proposition 0 (below). Proposition 2, which we have the following convergence in the distribution of the maximizers of (2)."}, {"heading": "6 A3: Consistency (Structured)", "text": "In this section, we consider the conditions of this policy to ensure the consistency of the estimate, or in other words, the convergence of the maximizing effect from (4) to (0) as n > probability. We assume that the labeling policy is a probable mix of the deministic sequence marking functions. (3) The policy (3) corresponds to the probability 1 (Y) = 3 (Y) = 3 (Y) = 3 (Y) = {Y1,.,., k with the probabilities 1,.,.,. the policy (3) corresponds to the probability 1 (Y) = 3 (Y) = 3 (Y1,.,.,."}, {"heading": "7 A4: Accuracy (Structured)", "text": "We consider in this section the dependence of estimation accuracy in the structured prediction SSL (4) on n, \u03b80, but perhaps most interesting is that we consider the following convergence in the distribution of the number maximizers of (12) n (2) n (2) n (0) n (0) n (13) as n (1) n (1) k (1) k (1) k (1) k (1) k (1) k (1) k (1) k (1) k (1) k) s (1) k) s (1) k) s (1) k) s (1) k) s (1) k) s (1) s) s (1) s) n (0) s) s (0) s."}, {"heading": "7.1 Conditional Structured Prediction", "text": "So far, our discussion of structured predictions has been limited to generative models such as HMM or Boltzmann chains MRF. However, similar techniques can be used to analyze SSL for conditional models such as CRFs, which are estimated by maximizing conditional probability. The key to extending the results in this paper to CRFs is to express conditional SSL estimates in a form similar to classical conditional MLE asymptotics. We omit further discussion for lack of space, but include some experimental results for CRFs. Figure 3 (left) shows an experiment similar to the conditional estimation experiment in CRF models described in the previous section. The figure shows plexicity per sequence as function n (x-axis) and 1 (y-axis)."}, {"heading": "8 A5: Tradeoff", "text": "Since the illustrations in the preceding sections show that estimation accuracy can be easily loosened with the total number of labels, the Cramer-Rao lower limit states that the highest accuracy is achieved by the maximum probability based on fully observed data. However, assuming that a certain cost factor is associated with the labeling of data, SSL resolves a fundamental accuracy and cost compromise. A decrease in estimation accuracy is acceptable in exchange for reduced labeling costs. Our ability to mathematically characterize the dependence of estimation accuracy on labeling costs leads to a new quantitative formulation of this compromise. Any labeling policy (\u03bb, n in classification and in structured labeling) is associated with a certain estimation accuracy via Propositions 2 and 4 and with a specific labeling cost. The exact method of measuring labeling costs depends on the situation, but we assume that the labeling costs (of the labeling costs) are proportional to the labeling costs (of the samples)."}, {"heading": "9 A6: Practical Algorithms", "text": "Such a resolution is tantamount to answering the basic question of how many labels should be obtained (and, in the case of a structured prediction, which ones too). Resolving the trade-off via (17) - (19) or otherwise, or even simply evaluating asymptotic accuracy tr (\u03a3) requires knowledge of the model parameter \u03b80, which is generally unknown in practice. In this section, we propose a practical two-step algorithm for calculating an estimate that is selected within a certain accuracy and cost compromise. Assuming that we have n unmarked examples, the algorithm starts the first stage by labeling r samples, then estimates the probability via r-labeling and n \u2212 r unmarked samples, and then uses the estimate to obtain a plug-in estimate for asymptotic accuracy."}], "references": [{"title": "Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning", "author": ["S. Ben-David", "T. Lu", "D. Pal"], "venue": "In International Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter", "author": ["V. Castelli", "T.M. Cover"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Statistical and computational tradeoffs in stochastic composite likelihood", "author": ["J. Dillon", "G. Lebanon"], "venue": "In Proc. of the 12th International Conference on Aritficial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A Course in Large Sample Theory", "author": ["T.S. Ferguson"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M.I. Jordan"], "venue": "In Proc. of the International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Equivalence of linear boltzmann chains and hidden markov models", "author": ["D.J.C. MacKay"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Unlabeled data: Now it helps, now it doesnt", "author": ["A. Singh", "R. Nowak", "X. Zhu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "The value of labeled and unlabeled examples when the model is imperfect", "author": ["K. Sinha", "M. Belkin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Our asymptotic derivations are possible by extending the recently proposed stochastic composite likelihood formalism [5] and showing that generative SSL is a special case of that extension.", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "As in [7], the delta method transforms our results from parameter asymptotics to prediction risk asymptotics.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "Perhaps the first study in this area was done by Castelli and Cover [3] who examined the convergence of the classification error rate as a labeled example is added to an unlabeled dataset drawn from a Gaussian mixture model.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "[9] proposed a practical SSL framework based on maximizing the likelihood of the observed data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Sinha and Belkin [11] examined the effect of using unlabeled samples with imperfect models for mixture models.", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "[10] analyze discriminative SSL using PAC theory and large deviation bounds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u201d[4] However, many of these assumptions are criticized in [2].", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "3 Stochastic SSL Estimators Generative SSL [9, 4] estimates a parametric model by maximizing the observed likelihood incorporating L labeled and U unlabeled examples", "startOffset": 43, "endOffset": 49}, {"referenceID": 6, "context": "A classical example is the naive Bayes model in [9] where p\u03b8(X,Y ) = p\u03b8(X |Y )p(Y ), p\u03b8(X |Y = y) = Mult([\u03b8y]1, .", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "The central idea in the proof is to cast the generative SSL estimation problem as an extension of stochastic composite likelihood [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "Our proof follows similar lines to the consistency proof of [5] with the exception that it does not assume independence of the indicator functions Z and (1 \u2212 Z) as is assumed there.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "chapter 16 of [6], hold on S leading to P {", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "The notations p \u2192 , denote convergences in probability and in distribution [6] and \u2207f(\u03b8), \u22072f(\u03b8) are the r \u00d7 1 gradient vector and r \u00d7 r matrix of second order derivatives of f(\u03b8).", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "Figure 1 displays three error measures for the multinomial naive Bayes SSL classifier [9] and the Reuters RCV1 text classification data.", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": ", \u03bbk)) which exposes its similarity to the stochastic composite likelihood function in [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": ", [8]).", "startOffset": 2, "endOffset": 5}], "year": 2013, "abstractText": "Semisupervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distribution-free analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP.", "creator": "LaTeX with hyperref package"}}}