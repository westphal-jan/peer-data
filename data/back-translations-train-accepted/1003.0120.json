{"id": "1003.0120", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2010", "title": "Learning from Logged Implicit Exploration Data", "abstract": "We provide a sound and consistent foundation for the use of \\emph{nonrandom} exploration data in \"contextual bandit\" or \"partially labeled\" settings where only the value of a chosen action is learned.", "histories": [["v1", "Sat, 27 Feb 2010 17:53:46 GMT  (45kb)", "https://arxiv.org/abs/1003.0120v1", null], ["v2", "Mon, 14 Jun 2010 16:06:16 GMT  (45kb)", "http://arxiv.org/abs/1003.0120v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["alexander l strehl", "john langford", "lihong li", "sham kakade"], "accepted": true, "id": "1003.0120"}, "pdf": {"name": "1003.0120.pdf", "metadata": {"source": "META", "title": "Learning from Logged Implicit Exploration Data", "authors": ["Alex Strehl"], "emails": ["astrehl@gmail.com", "jl@yahoo-inc.com", "shamkakade@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 100 3.01 20v2 [cs.LG] 1 4Ju nThe primary challenge in a variety of environments is that the exploration policy that logs \"offline\" data is not explicitly known. Here, previous solutions require either control of actions during the learning process, recorded random exploration, or actions that are randomly and repeatedly selected. Techniques reported here lift these limitations and allow us to learn a policy for selecting specific actions from historical data that has not been randomized or logged. We test our solution empirically based on relatively large real-world data obtained from an online advertising company."}, {"heading": "1. The Problem", "text": "Consider the ad problem, where a search engine company selects an ad to interest the user. Revenue is typically provided to the search engine only when the user clicks on the ad. This problem is of intrinsic economic interest, resulting in a significant share of revenue for several well-known companies such as Google, Yahoo! and Facebook. Furthermore, existing trends imply that this issue is of growing importance. Before discussing the approach we propose, it is important to formalize and generalize the problem, and then consider why more conventional approaches may fail. The hot-start problem for contextual explorationLet X be an arbitrary input space, and A = {1, \u00b7, k} be a set of actions. An instance of the contextual bandit problem is defined by a distribution D over tuples (x, ~ r) in which x, x), x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, etc."}, {"heading": "Approaches that fail", "text": "There are several approaches that seem to solve this problem but turn out to be insufficient: 1. Supervised learning. We could learn a regressor s: X \u00b7 A \u2192 [0, 1], which is trained to predict the reward on observed events that depend on the action a and other information x. From this regressor a policy according to h (x) = argmaxa, A s (x, a) is derived. One flaw of this approach is that the Argmax can extend over a series of decisions that are not included in the training data, and therefore may not generalize at all (or only poorly). This can be verified by considering some extreme cases. Suppose that there are two actions a and b with an action b occurring 102 times. Since the action b occurs only a 10 \u2212 4 fraction of the time, a learning algorithm is forced to make an overwhelming prediction between the anticipated value of ra and a prediction."}, {"heading": "Our Approach", "text": "Our approach, of course, is divided into three ties.1. For each event (x, a, ra), we estimate the probability \u03c0 (a | x) that the logging policy measures using a regression.2. For each event, create a synthetic controlled contextual bandit event by (x, a, ra, 1 / max {\u03c0 (a | x), \u03c4}), where \u03c4 > 0 is any parameter.3. Applying a contextual bandit algorithm to the set of synthetic contextual bandit events is a meaning that indicates how important the current event is for training. At first, the parameter \u03c4 may seem mysterious, but is crucial for numerical stability. 3. applying a contextual bandit algorithm to the set of synthetic contextual bandit events. In our experimental results, a variant of the argmax regressor with two critical modifications is used: (a) We limit the scope of the positive bandit policy to such actions with max probability."}, {"heading": "2. Formal Problem Setup and Assumptions", "text": "Let the learning algorithm obtain a set of T samples, with a function for each t mapping an input from X to a (possibly deterministic) distribution via A. The learning algorithm receives a set of T samples, with each form (x, a, ra) of X \u00b7 A \u00b7 [0, 1] selected, where (x, r) is derived from D as described in Section 1 and the action is selected according to tth policy. We refer to this random process as (x, a, ra) \u0445 (D, \u03c0t (\u00b7 | x)). Likewise, the interaction with the T policy results in a sequence of T samples that we call S \u0445 (D, \u03c0t (\u00b7 | x)) T = 1."}, {"heading": "Offline policy estimator", "text": "Using a data set of the form S = {(xt, at, rt, at)} T t = 1, (1), in which we define the value of a policy by means of an offline estimator for the value of a new policy h. Formally, for a new policy h: X \u2192 A and a data set S, we define the estimator: V-h\u03c0 value (S) = 1 | S value (x, a, r) and SraI (h (x) = a) max {\u03c0 value (a | x), (2), where I (\u00b7) denotes the indicator function. The purpose of the method is to limit the individual terms to the sum up and is comparable to previous methods (Owen & Zhou, 1998)."}, {"heading": "3. Theoretical Results", "text": "We now present our algorithm and the most important theoretical outcomes. The main idea is two-fold: first, we have a policy estimation step where we estimate the (unknown) logging policy (analyzed in subsection 3.1); second, we have a policy optimization step where we use our estimated logging policy (analyzed in subsection 3.2); our main result, Theorem 3.2, provides a generalization - by addressing the question of how both the estimation and optimization errors contribute to total erroneousness. The logging policy can be deterministic, which implies that conventional approaches based on randomization in logging policy are not applicable; next, we show that this is fine when the world is IID and policies vary over its actions. We effectively replace the standard approach of randomization in the algorithm for marginalization in the world. A basic expectation that follows a stoic policy is a stoic one."}, {"heading": "Proof.", "text": "E (x, ~ r) \u0445 D, a \u00b2 p \u00b2 p (\u00b7 | x) raI (h (x) = a) max {\u03c0 (a | x), \u03c4} = E (x, ~ r) \u0445 D \u00b2 a\u03c0 (a | x) raI (h (x) = a) max {\u03c0 (a | x)), \u03c4} = E (x, ~ r) \u0445 D \u00b2 a1T \u00b2 t \u00b2 (a | x) raI (h (x) = a) max (a) \u00b2 (a) \u00b2 (a) \u00b2 (a) max (a) \u00b2 (a) \u00b2 (a) \u00b2 (a) \u00b2 (a) \u00b2 (a) \u00b2 (a) c (c) c (c) c c (c) c c c c c c (c) c c c c (c) c) c (c) c (c) c (c) c (c) c (c) c, c (c) c (c) c, c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c, c (c), c (c), c (c), (c (c), c (c), (c (c), (c), (c (c, x), (c (c), c (c, x), (c (c (c), x), (c (c (c, x), x), x (c (c (c), x, x), x (c (c), x), x (c (c (x), x), x (c (c (x), x (x), x (x), x (a), x (c (a), x), x (c (c (a), x (c (c (c), x), x (c (c (a), x), x (c (c (c), x), x (c (c (c (c), x), x (c (c), x (c (c (c), x), x (c (c (c"}, {"heading": "3.1. Policy Estimation", "text": "In this section, we shall show that we have sufficient accuracy to evaluate new strategies in a suitable choice of numerals and numerals over numerals. (We aggressively use the simplification of the previous paragraph, which indicates that we may present the data as being generated by a rigid statistical estimate. (For a given estimate, we assume that the new numerator is a function. (5) Our first result is that the new numerical value is consistent. (5) In the following Theorem statement, I refer to the indicator function, numerals and numerals of the numerals. (6) The likelihood that the Minutes Policy will take action based on input x and numerals. (5) Our numerals as a definition based on numerals numerals numerals numerals numerals numerals 2 numerals numerals numerals numerals numerals numerals numerals. (6) The likelihood that numerical policy will take action based on input x and numerals numerals numerals numerals numerals numerals numerals. (5) Our numerals as a definition based on numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals 2 numerals on numerals numerals numerals numerals numerals numerals numerals numerals numerals numerals, numerals numerals numerals numerals, numerals numerals, numerals, numerals, numerals, numerals"}, {"heading": "3.2. Policy Optimization", "text": "The previous section demonstrates that we can effectively evaluate a policy h by applying a stochastic policy p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p p = p = p p = p p = p = p p = p = p = p = p = p = p = p = p = p = p = p = p = h = p = h = h = p = p = p = h = p = p = p = p = p = p = p = p = p = p = p = h = p = p = p = p = p = p = p = p = p = p = p = p = p = p = h = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p p p = p p = p p p p p p = p p p p p p = p p p p p p"}, {"heading": "4. Empirical Evaluation", "text": "We received proprietary data from an online advertising company covering a period of about a month. \"The data is composed of a variety of ads, each selecting a single ad for the top or most prominent position where each event represents a visit by a user to a particular website, but these were ignored in our test. Output y is an indicator of whether or not the user clicked on the ad. The total number of ads in the record is approximately 880,000. Training data consists of 35 million events that occur after the events in the training data. The total number of different websites is approximately 3.4 million. We trained a policy h to select an ad based on the current page to maximize the likelihood of clicks."}, {"heading": "4.1. Results", "text": "The measures proposed by the European Commission and the European Commission to reduce the imbalances and imbalances between the individual countries and the individual countries proposed by the European Commission and the European Commission prove to be insufficient: The European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European Commission, the European"}, {"heading": "5. Conclusion", "text": "This problem is obvious for applications from Internet companies that recommend content to users (such as ads, search results, news stories, etc.), but we believe that this may also be of interest for other applications within machine learning. For example, the standard approach to offline policy evaluation could be based on weighted samples (Kearns et al., 2000; Precup et al., 2000). The basic findings presented here could be applied to RL settings, eliminating the need to explicitly know the probability of a chosen measure, allowing an RL agent to learn from external observations of other stakeholders. The main restrictive assumption adopted by the Exploration Scavenging Paper (Langford et al., 2008) is that protocol policy chooses actions independently of input. We have introduced a new method to explore this situation when other measures may be required."}, {"heading": "Auer, Peter, Bianchi, Nicolo\u0300 C., Freund, Yoav, and", "text": "Schapire, Robert E. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32 (1): 48-77, 2002.Hoeffding, Wassily. Probability inequalities for sums of limited random variables. Journal of the American Statistical Association, 58: 13-30, 1963."}, {"heading": "Kearns, Michael, Mansour, Yishay, and Ng, Andrew Y.", "text": "In NIPS, 2000.Langford, John and Zhang, Tong. The epoch-hungry algorithm for multi-armed bandits with ancillary information. In Advances in Neural Information Processing Systems 20, pp. 817-824, 2008."}, {"heading": "Langford, John, Strehl, Alexander L., and Wortman, Jenn.", "text": "Exploration scavenging. In ICML-08: Proceedings of the 25th International Conference on Machine Learning, 2008."}, {"heading": "Owen, Art and Zhou, Yi. Safe and effective importance", "text": "sampling. Journal of the American Statistical Association, 95: 135-143, 1998.Precup, Doina, Sutton, Rich, and Singh, Satinder. Authorization tracks for non-political evaluation. In ICML, 2000."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Bianchi", "Nicol\u00f2 C", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Approximate planning in large pomdps via reusable trajectories", "author": ["Kearns", "Michael", "Mansour", "Yishay", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Kearns et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2000}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["Langford", "John", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Exploration scavenging", "author": ["Langford", "John", "Strehl", "Alexander L", "Wortman", "Jenn"], "venue": "Proceedings of the 25rd international conference on Machine learning,", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Safe and effective importance sampling", "author": ["Owen", "Art", "Zhou", "Yi"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Owen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Owen et al\\.", "year": 1998}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["Precup", "Doina", "Sutton", "Rich", "Singh", "Satinder"], "venue": "In ICML,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Existing approaches to contextual bandits such as EXP4 (Auer et al., 2002) or Epoch Greedy (Langford & Zhang, 2008), require either interaction to gather data or require knowledge of the probability the logging policy chose the action a.", "startOffset": 55, "endOffset": 74}, {"referenceID": 3, "context": "It is possible to recover exploration information from action visitation frequency when a logging policy chooses actions independent of the input x (but possibly dependent on history) (Langford et al., 2008).", "startOffset": 184, "endOffset": 207}], "year": 2010, "abstractText": "We provide a sound and consistent foundation for the use of nonrandom exploration data in \u201ccontextual bandit\u201d or \u201cpartially labeled\u201d settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which \u201coffline\u201d data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on a reasonably sized set of real-world data obtained from an online advertising company.", "creator": "LaTeX with hyperref package"}}}