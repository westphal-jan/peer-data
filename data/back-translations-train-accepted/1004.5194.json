{"id": "1004.5194", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2010", "title": "Clustering processes", "abstract": "The problem of clustering is considered, for the case when each data point is a sample generated by a stationary ergodic process. We propose a very natural asymptotic notion of consistency, and show that simple consistent algorithms exist, under most general non-parametric assumptions. The notion of consistency is as follows: two samples should be put into the same cluster if and only if they were generated by the same distribution. With this notion of consistency, clustering generalizes such classical statistical problems as homogeneity testing and process classification. We show that, for the case of a known number of clusters, consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic (no parametric or Markovian assumptions, no assumptions of independence, neither between nor within the samples). If the number of clusters is unknown, consistency can be achieved under appropriate assumptions on the mixing rates of the processes. (again, no parametric or independence assumptions). In both cases we give examples of simple (at most quadratic in each argument) algorithms which are consistent.", "histories": [["v1", "Thu, 29 Apr 2010 06:38:47 GMT  (16kb)", "http://arxiv.org/abs/1004.5194v1", "in proceedings of ICML 2010. arXiv-admin note: for version 2 of this article please see:arXiv:1005.0826v1"]], "COMMENTS": "in proceedings of ICML 2010. arXiv-admin note: for version 2 of this article please see:arXiv:1005.0826v1", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["daniil ryabko"], "accepted": true, "id": "1004.5194"}, "pdf": {"name": "1004.5194.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar Xiv: 100 4.51 94v1 [cs.LG] 2 9A prThe problem of clustering is taken into account when each data point is a sample generated by a stationary ergodic process. We propose a very natural asymptotic notion of consistency and show that simple consistent algorithms exist, under most general non-parametric assumptions. The concept of consistency is as follows: two samples should be placed in the same cluster if and only if they are generated by the same distribution. By using this concept of consistency, clustering generalizes such classic statistical problems as homogeneity testing and process classification. We show that in the case of a known number of clusters consistency can be achieved under the single assumption that the common distribution of the data is stationary ergodic (no parametric or Markovian assumptions, no assumptions of independence, neither between nor within the samples)."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "2 Preliminaries", "text": "In this thesis we consider the case A = R; extensions to the multidimensional case, as well as to more general spaces, are the distributions to space ((AN), F (AN)). For each k, l and B, l and l, the division of the set k is into k-dimensional cubes with volume hkl = (1 / l) k (the cubes start at 0). Furthermore, we define Bk = (B) k, l and B = 1B k. The division of the set k into k-dimensional cubes starts at 0). We define Bk = (B) k and B = 1B k."}, {"heading": "3 Main results", "text": "The cluster problem can be defined as follows: We get N samples x1,.., xN, where each sample xi is a string ni the length of symbols of A: xi = X i 1.. ni. Each sample is splintered by a stationary ergodic distributions \u03c11,.., \u03c1k different from k. Thus, there is a partitioning I = {I1,..., Ik} of the set {1.. N} into k subsets Ij, j = 1.. k {1.. N} = 1Ij, so that xj, 1 \u2264 j \u2264 N is generated by consistent samples if and only if j \u0441Ij. Partitioning I is called target clustering and the sets Ii, 1 \u2264 i \u2264 i k, are called target clusters. Samples given x1,.., xN and a target cluster I (x)."}, {"heading": "3.1 Known number of clusters", "text": "k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k. k.. k. k. k. k. k. k. k... k. k... k.. k.. k... k. k... k... k.. k.. k.. k.... k.. k.. k.... k.. k.. k.. k.... k. k..... k. k. k... k... k... k.. k.. k. k......... k.... k... k. k.... k.... k.. k........ k...... k. k...... k...... k. k..... k... k.... k.. k... k... k..... k. k.. k...... k.... k..... k. k... k... k.... k..... k."}, {"heading": "3.2 Unknown number of clusters", "text": "So far, we have shown that if the number of clusters is known in advance, consistent clustering is possible on the sole assumption that the common distribution of samples is stationary ergodic again. However, under this assumption, it is impossible to decide whether they were generated by the same or by different distributions, even if we have only two binary weighted samples (even if the distributions come from a smaller class: the set of all B processes). So, if the number of clusters is unknown, we have to settle for less, which means we have to make stronger assumptions about the data. What we need is known for the convergence of frequencies to their expectations."}, {"heading": "T is the partition output by the algorithm, I is the target clustering, \u03b5\u03c1 is a constant that depends only", "text": "Specifically, this means that we select the parameters in such a way that the parameters n = o (1), qn, mn, ln, bn = o (n), qn, mn, l, k (n), k (n), k (n), k (n), k (n), l), bm, ln (n), and, finally, mnlnbn (e), 2 n (n), n (n), n (n), k (n), k (n), 2qn (n), b), as always possible, algorithm 2 is weakly asymptotically consistent (with the number of clusters k unknown).The complexity of algorithms 2 is O (N2mnmax), and all of us, and is limited by O (N2n2max log s \u2212 1)."}, {"heading": "4 Conclusion", "text": "We have proposed a framework for defining the consistency of cluster algorithms when the data comes from a set of samples derived from stationary processes; the main advantage of this framework is its universality: no assumptions need be made about the distribution of data that go beyond stationary and ergodicity; the proposed concept of consistency is so simple and natural that it can be proposed as a basic health check for all cluster algorithms used on sequence-like data; for example, it is easy to see that the k-mean algorithm is consistent with some initializations (e.g., the one used in Algorithm 1) but not with others (e.g., not random); while the algorithms we have presented to demonstrate the existence of consistent cluster methods are compatible and easy to implement, the main value of the established results is theoretical; as mentioned in the introduction, it is possible to replace compression rates with rosy estimates for practical applications only."}], "references": [{"title": "A bit level representation for time series data mining with shape based similarity", "author": ["A. Bagnall", "C. Ratanamahatana", "E. Keogh", "S. Lonardi", "G. Janacek"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Bagnall et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bagnall et al\\.", "year": 2006}, {"title": "Nonparametric Statistics for Stochastic Processes", "author": ["D. Bosq"], "venue": "Estimation and Prediction. Springer,", "citeRegEx": "Bosq,? \\Q1996\\E", "shortCiteRegEx": "Bosq", "year": 1996}, {"title": "Basic properties of strong mixing conditions. A survey and some open questions", "author": ["R.C. Bradley"], "venue": "Probability Surveys,", "citeRegEx": "Bradley,? \\Q2005\\E", "shortCiteRegEx": "Bradley", "year": 2005}, {"title": "Clustering by compression", "author": ["R. Cilibrasi", "P.M.B. Vitanyi"], "venue": "IEEE Trans. Inf. Th.,", "citeRegEx": "Cilibrasi and Vitanyi,? \\Q2005\\E", "shortCiteRegEx": "Cilibrasi and Vitanyi", "year": 2005}, {"title": "Probability, Random Processes, and Ergodic Properties", "author": ["R. Gray"], "venue": null, "citeRegEx": "Gray,? \\Q1988\\E", "shortCiteRegEx": "Gray", "year": 1988}, {"title": "Rates of uniform convergence of empirical means with mixing processes", "author": ["R.L. Karandikara", "M. Vidyasagar"], "venue": "Stat.&Prob. Lett.,", "citeRegEx": "Karandikara and Vidyasagar,? \\Q2002\\E", "shortCiteRegEx": "Karandikara and Vidyasagar", "year": 2002}, {"title": "An impossibility theorem for clustering", "author": ["J. Kleinberg"], "venue": "In NIPS :446\u2013453,", "citeRegEx": "Kleinberg,? \\Q2002\\E", "shortCiteRegEx": "Kleinberg", "year": 2002}, {"title": "Testing Statistical Hypotheses, 2nd ed", "author": ["E. Lehmann"], "venue": null, "citeRegEx": "Lehmann,? \\Q1986\\E", "shortCiteRegEx": "Lehmann", "year": 1986}, {"title": "The planar k-means problem is NP-hard", "author": ["M. Mahajan", "P. Nimbhorkar", "K. Varadarajan"], "venue": "In WALCOM : 274\u2013285,", "citeRegEx": "Mahajan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mahajan et al\\.", "year": 2009}, {"title": "Compression-based methods for nonparametric prediction and estimation of some characteristics of time series", "author": ["B. Ryabko"], "venue": "IEEE Trans. Inf. Th.,", "citeRegEx": "Ryabko,? \\Q2009\\E", "shortCiteRegEx": "Ryabko", "year": 2009}, {"title": "Universal codes as a basis for time series testing", "author": ["B. Ryabko", "J. Astola"], "venue": "Stat. Methodology,", "citeRegEx": "Ryabko and Astola,? \\Q2006\\E", "shortCiteRegEx": "Ryabko and Astola", "year": 2006}, {"title": "Testing composite hypotheses about discrete-valued stationary processes", "author": ["D. Ryabko"], "venue": "In ITW : 291\u2013", "citeRegEx": "Ryabko,? \\Q2010\\E", "shortCiteRegEx": "Ryabko", "year": 2010}, {"title": "Discrimination between B-processes is impossible", "author": ["D. Ryabko"], "venue": "J. Theor. Prob.,", "citeRegEx": "Ryabko,? \\Q2010\\E", "shortCiteRegEx": "Ryabko", "year": 2010}, {"title": "Nonparametric statistical inference for ergodic processes", "author": ["D. Ryabko", "B. Ryabko"], "venue": "IEEE Trans. Inf. Th.,", "citeRegEx": "Ryabko and Ryabko,? \\Q2010\\E", "shortCiteRegEx": "Ryabko and Ryabko", "year": 2010}, {"title": "Clustering sequences with hidden markov models", "author": ["P. Smyth"], "venue": "In NIPS : 648\u2013654", "citeRegEx": "Smyth,? \\Q1997\\E", "shortCiteRegEx": "Smyth", "year": 1997}, {"title": "A uniqueness theorem for clustering", "author": ["R. Zadeh", "S. Ben-David"], "venue": "In UAI,", "citeRegEx": "Zadeh and Ben.David,? \\Q2009\\E", "shortCiteRegEx": "Zadeh and Ben.David", "year": 2009}, {"title": "A unified framework for model-based clustering", "author": ["S. Zhong", "J. Ghosh"], "venue": "JMLR, 4:1001\u20131037,", "citeRegEx": "Zhong and Ghosh,? \\Q2003\\E", "shortCiteRegEx": "Zhong and Ghosh", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "However, even if one assumes the similarity measure known, it is hard to define what a good clustering is Kleinberg (2002); Zadeh & Ben-David (2009).", "startOffset": 106, "endOffset": 123}, {"referenceID": 6, "context": "However, even if one assumes the similarity measure known, it is hard to define what a good clustering is Kleinberg (2002); Zadeh & Ben-David (2009). What is more, even if one assumes the similarity measure to be simply the Euclidean distance (on the plane), and the number of clusters k known, then clustering may still appear intractable for computational reasons.", "startOffset": 106, "endOffset": 149}, {"referenceID": 6, "context": "However, even if one assumes the similarity measure known, it is hard to define what a good clustering is Kleinberg (2002); Zadeh & Ben-David (2009). What is more, even if one assumes the similarity measure to be simply the Euclidean distance (on the plane), and the number of clusters k known, then clustering may still appear intractable for computational reasons. Indeed, in this case finding k centres (points which minimize the cumulative distance from each point in the sample to one of the centres) seems to be a natural goal, but this problem is NPhard Mahajan et al. (2009).", "startOffset": 106, "endOffset": 583}, {"referenceID": 10, "context": "Perhaps the most close approach is mixture models Smyth (1997); Zhong & Ghosh (2003): it is assumed that there are k different distributions that have a particular known form (such as Gaussian, Hidden Markov models, or graphical models) and each one out of N samples is generated independently according to one of these k distributions (with some fixed probability).", "startOffset": 50, "endOffset": 63}, {"referenceID": 10, "context": "Perhaps the most close approach is mixture models Smyth (1997); Zhong & Ghosh (2003): it is assumed that there are k different distributions that have a particular known form (such as Gaussian, Hidden Markov models, or graphical models) and each one out of N samples is generated independently according to one of these k distributions (with some fixed probability).", "startOffset": 50, "endOffset": 85}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions.", "startOffset": 5, "endOffset": 20}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions.", "startOffset": 5, "endOffset": 62}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions. What is important for us here, is that the three-sample problem is easier than the two-sample problem; the reason is that k is known in the latter case but not in the former. Indeed, in Ryabko (2010b) it is shown that in general, for stationary ergodic (binary-valued) processes, there is no solution to the two-sample problem, even in the weakest asymptotic sense.", "startOffset": 5, "endOffset": 307}, {"referenceID": 7, "context": "data Lehmann (1986), but also for Markov chains Gutman (1989), and under certain mixing rates conditions. What is important for us here, is that the three-sample problem is easier than the two-sample problem; the reason is that k is known in the latter case but not in the former. Indeed, in Ryabko (2010b) it is shown that in general, for stationary ergodic (binary-valued) processes, there is no solution to the two-sample problem, even in the weakest asymptotic sense. However, a solution to the threesample problem, for (real-valued) stationary ergodic processes was given in Ryabko & Ryabko (2010). In this work we demonstrate that, if the number k of clusters is known, then there is an asymptotically consistent clustering algorithm, under the only assumption that the joint distribution of data is stationary ergodic.", "startOffset": 5, "endOffset": 603}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a).", "startOffset": 109, "endOffset": 132}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a). Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated.", "startOffset": 109, "endOffset": 148}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a). Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated. For the case of a known number of clusters, the proposed algorithm (which is shown to be consistent) is as follows. (The distance in the algorithms is a suitable empirical estimate of d.) The first sample is assigned to the first cluster. For each j = 2..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign it to the cluster j. Thus we have one point in each of the k clusters. Next, assign each of the remaining points to the cluster that contains the closest points from those k already assigned. For the case of an unknown number of clusters k, the algorithm simply puts those samples together that are not farther away from each other than a certain threshold level, where the threshold is calculated based on the known bound on the mixing rates. In this case, besides the asymptotic result, finite-time bounds on the probability of outputting an incorrect clustering can be obtained. Each of the algorithms is shown to be at most quadratic in each argument. Therefore, we show that for the proposed notion of consistency, there are simple algorithms that are consistent under most general assumptions. While these algorithms can be easily implemented, we have left the problem of trying them out on particular applications, as well as optimizing the parameters, for future research. It may also be suggested that the empirical distributional distance can be replaced by other distances, for which similar theoretical results can be obtained. An interesting direction, that could preserve the theoretical generality, would be to use data compressors. These were used in Ryabko & Astola (2006) for the related problems of hypotheses testing, leading both to theoretical and practical results.", "startOffset": 109, "endOffset": 1923}, {"referenceID": 8, "context": "This distance has proved a useful tool for solving various statistical problems concerning ergodic processes Ryabko & Ryabko (2010); Ryabko (2010a). Although this distance involves infinite summation, we show that its empirical approximations can be easily calculated. For the case of a known number of clusters, the proposed algorithm (which is shown to be consistent) is as follows. (The distance in the algorithms is a suitable empirical estimate of d.) The first sample is assigned to the first cluster. For each j = 2..k, find a point that maximizes the minimal distance to those points already assigned to clusters, and assign it to the cluster j. Thus we have one point in each of the k clusters. Next, assign each of the remaining points to the cluster that contains the closest points from those k already assigned. For the case of an unknown number of clusters k, the algorithm simply puts those samples together that are not farther away from each other than a certain threshold level, where the threshold is calculated based on the known bound on the mixing rates. In this case, besides the asymptotic result, finite-time bounds on the probability of outputting an incorrect clustering can be obtained. Each of the algorithms is shown to be at most quadratic in each argument. Therefore, we show that for the proposed notion of consistency, there are simple algorithms that are consistent under most general assumptions. While these algorithms can be easily implemented, we have left the problem of trying them out on particular applications, as well as optimizing the parameters, for future research. It may also be suggested that the empirical distributional distance can be replaced by other distances, for which similar theoretical results can be obtained. An interesting direction, that could preserve the theoretical generality, would be to use data compressors. These were used in Ryabko & Astola (2006) for the related problems of hypotheses testing, leading both to theoretical and practical results. As far as clustering is concerned, compression-based methods were used (without asymptotic consistency analysis) in Cilibrasi & Vitanyi (2005), and (in a different way) in Bagnall et al.", "startOffset": 109, "endOffset": 2165}, {"referenceID": 0, "context": "As far as clustering is concerned, compression-based methods were used (without asymptotic consistency analysis) in Cilibrasi & Vitanyi (2005), and (in a different way) in Bagnall et al. (2006). Combining our consistency framework with these compression-based methods is a promising direction for further research.", "startOffset": 172, "endOffset": 194}, {"referenceID": 4, "context": "Gray (1988))", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "We refer to Gray (1988) for more information on this metric and its properties.", "startOffset": 12, "endOffset": 24}, {"referenceID": 8, "context": "Indeed, as was shown in Ryabko (2010b), when we have only two binary-valued samples, generated independently by two stationary ergodic distributions, it is impossible to decide whether they have been generated by the same or by different distributions, even in the sense of weak asymptotic consistency (this holds even if the distributions come from a smaller class: the set of all B-processes).", "startOffset": 24, "endOffset": 39}, {"referenceID": 1, "context": "Next we introduce mixing coefficients, mainly following Bosq (1996) in formulations.", "startOffset": 56, "endOffset": 68}, {"referenceID": 2, "context": "Bradley (2005) and references therein.", "startOffset": 0, "endOffset": 15}, {"referenceID": 1, "context": "We use the following bound from Bosq (1996): for any zero-mean random process Y1, Y2, .", "startOffset": 32, "endOffset": 44}, {"referenceID": 1, "context": "Bosq (1996)), so assumptions on the speed of decrease of \u03b2-coefficients are stronger.", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "Bosq (1996)), so assumptions on the speed of decrease of \u03b2-coefficients are stronger. Using the uniform bounds given in Karandikara & Vidyasagar (2002), one can obtain a statement similarto that in Theorem 2, with \u03b1-mixing replaced by \u03b2-mixing, and without the multiplicative factor bn.", "startOffset": 0, "endOffset": 152}, {"referenceID": 9, "context": "As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of Ryabko & Astola (2006); Cilibrasi & Vitanyi (2005); Ryabko (2009).", "startOffset": 219, "endOffset": 242}, {"referenceID": 9, "context": "As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of Ryabko & Astola (2006); Cilibrasi & Vitanyi (2005); Ryabko (2009).", "startOffset": 219, "endOffset": 270}, {"referenceID": 9, "context": "As it was mentioned in the introduction, it can be suggested that for practical applications empirical estimates of the distributional distance can be replaced with distances based on data compression, in the spirit of Ryabko & Astola (2006); Cilibrasi & Vitanyi (2005); Ryabko (2009). Another direction for future research concerns optimal bounds on the speed of convergence: while we show that such bounds can be obtained (of course, only in the case of known mixing rates), finding practical and tight bounds, for different notions of mixing rates, remains open.", "startOffset": 219, "endOffset": 285}], "year": 2010, "abstractText": "The problem of clustering is considered, for the case when each data point is a sample generated by a stationary ergodic process. We propose a very natural asymptotic notion of consistency, and show that simple consistent algorithms exist, under most general non-parametric assumptions. The notion of consistency is as follows: two samples should be put into the same cluster if and only if they were generated by the same distribution. With this notion of consistency, clustering generalizes such classical statistical problems as homogeneity testing and process classification. We show that, for the case of a known number of clusters, consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic (no parametric or Markovian assumptions, no assumptions of independence, neither between nor within the samples). If the number of clusters is unknown, consistency can be achieved under appropriate assumptions on the mixing rates of the processes. In both cases we give examples of simple (at most quadratic in each argument) algorithms which are consistent.", "creator": "LaTeX with hyperref package"}}}