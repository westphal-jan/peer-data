{"id": "1005.0027", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2010", "title": "Learning from Multiple Outlooks", "abstract": "We consider semi-supervised learning from multiple outlooks of the same learning task, that is, learning from different representations of the same type of data. As opposed to learning from multiple views where it is assumed that the exact same instances have multiple representations, we only assume the availability of samples of the same learning task in different domains. We develop an algorithmic framework that is based on mapping the (unlabeled) data followed by adjusting the mapping using the scarcer labeled data. The mapped data from all the outlooks can then be used for a generic classification algorithm. We further provide sample complexity results under the assumption that the different outlooks are inherently low dimension Gaussian mixtures. Experiments with real-world data indicate the performance boost from using multiple outlooks.", "histories": [["v1", "Fri, 30 Apr 2010 21:52:17 GMT  (319kb)", "https://arxiv.org/abs/1005.0027v1", "with full proofs of theorems"], ["v2", "Tue, 14 Jun 2011 06:56:25 GMT  (64kb)", "http://arxiv.org/abs/1005.0027v2", "with full proofs of theorems and all experiments"]], "COMMENTS": "with full proofs of theorems", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maayan harel", "shie mannor"], "accepted": true, "id": "1005.0027"}, "pdf": {"name": "1005.0027.pdf", "metadata": {"source": "META", "title": "Learning from Multiple Outlooks", "authors": ["Maayan Harel"], "emails": ["maayanga@tx.technion.ac.il", "shie@ee.technion.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 100 5.00 27v2 [cs.LG] 1 4Ju n20 11"}, {"heading": "1. Introduction", "text": "It is often the case that a learning task refers to multiple representations that we refer to as outlooks. Examples that belong to different outlooks can have different characteristics and distributions. Furthermore, outlooks are not related by corresponding instances, but only by the common task. Multiple outlooks can be found in many real-world problems, for example in activity detection, when data from different users representing the outlooks is collected by different sensors."}, {"heading": "2. Related work", "text": "The main challenge in these constellations is that the training and test data are drawn from different distributions, and the domain adaptation attempts to solve a common scenario where some changes have been made to the test distribution while the domain naming function remains more or less the same. Some authors illustrate this situation by assuming a single hypothesis that can classify both domains well (Blitzer et al., 2007), while others assume that the backward probability of the target for the domains is the same (Shimodair, 2000; Huang et al., 2007), the latter assumption being also referred to as the co-variable shift problem. Domain adaptation algorithms can be roughly divided into three categories, with one approach being to rebalance the training instances so that they are more similar to the test distribution."}, {"heading": "3. Mapping Two Outlooks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Problem Setting", "text": "The learner receives two outlooks belonging to separate input spaces X1 and X2 of the dimension d1 and d2, respectively, with a common goal Y = {1,..., c}. We assume that all sample pairs of a given outlook j = 1, 2 are drawn independently from each other from an unknown distribution Dj, which is unique for each outlook. Let us use X (1) i and X (2) i to mark the data matrices of class i of the outlook 1 and 2, respectively. We use superscripts to denote the index of the outlook, and subscription to denote the classification class."}, {"heading": "3.2. Multiple Outlook MAPping algorithm", "text": "In this section, we present our most important outlooks on the final outlooks. (MOMAP) We are responsible for the representation of two outlooks. (...) We are responsible for the representation of two outlooks. (...) We are responsible for the representation of two outlooks. (...) We are responsible for the representation of two outlooks. (...) We are responsible for the representation of two outlooks. (...) The goal is to normalize the characteristics of all outlooks on the same area. (Note that this level can be performed using unlabeled data when it is available. (...) Next, we use the labeled outlooks to adjust the two outlooks. The goal of this level is to represent the scaled outlooks by rotation and translation."}, {"heading": "4. Extension to Multiple outlooks", "text": "We present an extension of algorithm 1 to the case of multiple outlooks. Multiple Outlook scenario allows us to use the information available in all outlooks to enable better learning from each one. To this end, we transform all outlooks among each other. In terms of two outlooks, we start by translating the means of each class of all outlooks to zero. In the rotation step, optimal rotations are found by solving min {R (j) i} c \u00b2 i = 1 \u2211 k < j \u00b2 i \u00b2 R (k) i \u2212 R (j) i (j) i \u00b2 2F (4), bearing in mind that algorithm 2 produces an optimal solution with zero errors, since there is always a perfect rotation between two groups of h \u00b2 orthogonal vectors. Therefore, an optimal solution of (4) reaching an objective value of zero must be applied to all outlooks with zero errors, since there is always a perfect rotation between two groups of h \u00b2 orthogonal vectors."}, {"heading": "5. Analysis", "text": "In this section, we give a probabilistic robust interpretation of the rotation process and demonstrate a sample complexity tied to the convergence of the estimated rotation matrix."}, {"heading": "5.1. Probabilistic Interpretation", "text": "In this section, we discuss the effects of adding random noise to the utility matrices on the optimal rotation between two outlooks (problem (2)). We do not assume knowledge of the probability distribution of noise. Instead, we use its limited total value for a certain selected level of confidence. We show that the solution of the utility matrix D (2) i for any class i. Suppose that this uncertainty follows an unknown common distribution. \u2212 This uncertainty can be represented by a random extension of the problem (2). \u2212 A problem (5) Pr."}, {"heading": "5.2. Sample complexity bounds", "text": "Next, we provide a limit to the sample complexity of the rotation step of the algorithm. 1Because problem (2) is separable, the extension is performed for each class individually. We leave the i subscript, which represents the class, from the following derivatives for brevity.2The original rotation problem was actually the square of the Frobenius error. However, the two problems are equivalent, since the square does not change the solution.Assumption 1. (Gaussian mixture) Each outlook is generated by a unique mixture of c Gaussian distributions, where c is the number of classes.The samples of each outlook are realizations of x \u00b2 ci = 1 wifi (x), N (\u00b5i, \u0432i) and \u0445c i = 1 wi = 1. We also assume that the ExxT \u00b2 1 is the 1 for each component."}, {"heading": "6. Experiments", "text": "In this section, we show our activity data detection framework, where different users represent different views. In this application, setting up multiple views provides valuable flexibility in real-world recordings. For example, some users can use a simple sensor configuration for recording, while others can use a complex sensor board with multiple sensors. In addition, this configuration can solve problems of different sampling rates when using different hardware and workloads. In our experiments, we test two settings: domain customization and multi-view setup. To set up domain customization, a shared view of features is used, while setting up multiple views uses a unique feature space for each user."}, {"heading": "6.1. Data set description and feature extraction", "text": "The data set used for the experiments was collected by Subramanya et al. (2006) using a bespoke portable sensor system comprising a 3-axis accelerometer, phototransistors for measuring light, barometric pressure sensors and GPS data. The data consisted of recordings of 6 participants who were asked to perform a variety of activities and record the labels. We used the following labels: Walk, Run, Go Up, Go Down and Go. After removing data with obvious annotation errors, the data consists of about 50 hours of recording, which was roughly evenly divided among the 6 users. For each user, the activities are roughly divided into 40% Walk, 40-50% Stay, 2-3% Walk Up and 2-3% Walk Down. See (Subramanya et al., 2006) for more details about the sensor system and the recordings."}, {"heading": "6.2. Domain Adaptation Setup", "text": "In fact, the fact is that most of them are able to hold their own, and that they are able to hold their own, \"he said in an interview with The New York Times:\" I don't think they will be able to change the world. \""}, {"heading": "6.3. Multiple Outlook Setup", "text": "We conducted three types of experiments for the multiple outlook setup, each with a different feature representation. The experiments \"setup was similar to the previous experiments with some adjustments to the baselines: the SRC, ALL and FEDA baselines were no longer relevant because the properties of the outlook were different. In the first experiment we tested the multiple outlook algorithms on two outlooks in case of different sensors and added noise characteristics. For the mapped outlook we used the acceleration characteristics and print characteristics, and excluded the light characteristics we added with Gaussian random noise (N, 1). The experiment was performed on all pair combinations."}, {"heading": "7. Future Work", "text": "Our proposed approach is a first step in developing the methodology for learning from multiple perspectives, and this approach can be extended to many interesting directions. First, in this paper, we consider only affinity mappings between views, and a natural extension is to consider richer classes of transformations such as piecemeal linear mappings. Furthermore, our approach is stacked in the sense that all data must first be processed and then the classification algorithm can be used. Another expansion of practical interest would be the development of an online version of the proposed approach that takes samples piece by piece and gradually improves mapping. Finally, an important area of application that is of independent interest is natural language processing. Here, the challenge would be to use a language in which labels are abundant in order to better classify them in another language. Here, the main obstacle seems to be the way of representation: language data is often presented as sparse vectors that require a different kind of transformations between views."}, {"heading": "A. Appendix", "text": "The next theorem is the robust counterpart to problem (6); the robust version of the optimization for the two Outlook rotation problems (each component in problem 2). We present the theorem again for clarity: Theorem 1. problem (6) is equivalent to the maximum number of RD (2) - D (1) - D (1) - D (1) - D (1) - D (1) - Proof. we get an explicit expression for maximizing in (6). By definition, the norm can be written in such a way that the maximum number of RD (2) + - D (1) - D (1) - D (2) - D (2) - D (2) - D (2) - D (2) - D (2) - D (2) - D (2) - D (2) - D (2)."}], "references": [{"title": "Learning from Multiple Partially Observed Views\u2013an Application to Multilingual Text Categorization", "author": ["M. Amini", "N. Usunier", "C. Goutte"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Amini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2009}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Learning bounds for domain adaptation", "author": ["J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J. Wortman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "LIBSVM: a library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "Daume\u0301 III, H. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting", "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "Semisupervised alignment of manifolds", "author": ["J. Ham", "D. Lee", "L. Saul"], "venue": "In Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence, Z. Ghahramani and R. Cowell, Eds,", "citeRegEx": "Ham et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2005}, {"title": "Multiple view semi-supervised dimensionality reduction", "author": ["C. Hou", "C. Zhang", "Y. Wu", "F. Nie"], "venue": "Pattern Recognition,", "citeRegEx": "Hou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2010}, {"title": "A general model for multiple view unsupervised learning", "author": ["B. Long", "P.S. Yu", "Z.M. Zhang"], "venue": "In Proceedings of the 8th SIAM International Conference on Data Mining (SDM08),", "citeRegEx": "Long et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Long et al\\.", "year": 2008}, {"title": "Domain adaptation with multiple sources", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Sampling from large matrices: An approach through geometric functional analysis", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Rudelson and Vershynin,? \\Q2007\\E", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2007}, {"title": "Learning with multiple views", "author": ["S. R\u00fcping", "T. Scheffer"], "venue": "In Proceeding of the International Conference on Machine Learning Workshop on Learning with Multiple Views,", "citeRegEx": "R\u00fcping and Scheffer,? \\Q2005\\E", "shortCiteRegEx": "R\u00fcping and Scheffer", "year": 2005}, {"title": "Domain adaptation of conditional probability models via feature subsetting", "author": ["S. Satpal", "S. Sarawagi"], "venue": "In Proceedings of Principles of Data Mining and Knowledge Discovery,", "citeRegEx": "Satpal and Sarawagi,? \\Q2007\\E", "shortCiteRegEx": "Satpal and Sarawagi", "year": 2007}, {"title": "Lectures on stochastic programming: modeling and theory", "author": ["A. Shapiro", "D. Dentcheva", "A. Ruszczy\u0144ski", "A.P. Ruszczy\u0144ski"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "Shapiro et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shapiro et al\\.", "year": 2009}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodair"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodair,? \\Q2000\\E", "shortCiteRegEx": "Shimodair", "year": 2000}, {"title": "Matrix Perturbation Theory", "author": ["Stewart", "G.W", "J.G. Sun"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "Recognizing activities and spatial context using wearable sensors", "author": ["A. Subramanya", "A. Raj", "J. Bilmes", "D. Fox"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence. Citeseer,", "citeRegEx": "Subramanya et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2006}, {"title": "Manifold alignment using Procrustes analysis", "author": ["C. Wang", "S. Mahadevan"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Wang and Mahadevan,? \\Q2008\\E", "shortCiteRegEx": "Wang and Mahadevan", "year": 2008}, {"title": "Manifold alignment without correspondence", "author": ["C. Wang", "S. Mahadevan"], "venue": "In Proceedings of the 21st International Joint Conferences on Artificial Intelligence,", "citeRegEx": "Wang and Mahadevan,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mahadevan", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "Some authors portray this situation by assuming a single hypothesis may classify both domains well (Blitzer et al., 2007), while others assume the target\u2019s posterior probability is equal for the domains (Shimodair, 2000; Huang et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 12, "context": ", 2007), while others assume the target\u2019s posterior probability is equal for the domains (Shimodair, 2000; Huang et al., 2007).", "startOffset": 89, "endOffset": 126}, {"referenceID": 12, "context": "One approach is to reweigh the training instances so they better resemble the test distribution (Shimodair, 2000; Huang et al., 2007).", "startOffset": 96, "endOffset": 133}, {"referenceID": 7, "context": "A different approach is to combine the classifiers learnt in each domain (Mansour et al., 2009).", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "This may be carried out by choosing a subset of features (Satpal & Sarawagi, 2007), combination of features (Daum\u00e9 III, 2007), or by finding some structural correspondence between features in different domains (Blitzer et al., 2006).", "startOffset": 210, "endOffset": 232}, {"referenceID": 6, "context": "One common approach is to map a pattern matrix of each view to a consensus pattern by matching corresponding instances (Long et al., 2008; Hou et al., 2010).", "startOffset": 119, "endOffset": 156}, {"referenceID": 5, "context": "One common approach is to map a pattern matrix of each view to a consensus pattern by matching corresponding instances (Long et al., 2008; Hou et al., 2010).", "startOffset": 119, "endOffset": 156}, {"referenceID": 4, "context": "In manifold alignment we look for a transformation of two data sets with sample pairwise correspondence that minimizes the distance between them, in an unsupervised (Wang & Mahadevan, 2008) or a semi-supervised (Ham et al., 2005) manner.", "startOffset": 211, "endOffset": 229}, {"referenceID": 0, "context": "Amini et al. (2009) considers the case when correspondence is missing for some instances, but assumes the existence of a mapping functions between the views.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Amini et al. (2009) considers the case when correspondence is missing for some instances, but assumes the existence of a mapping functions between the views. Multi-view learning is sometimes referred to as manifold alignment. In manifold alignment we look for a transformation of two data sets with sample pairwise correspondence that minimizes the distance between them, in an unsupervised (Wang & Mahadevan, 2008) or a semi-supervised (Ham et al., 2005) manner. Wang & Mahadevan (2009) present manifold alignment without pairwise correspondence.", "startOffset": 0, "endOffset": 488}, {"referenceID": 11, "context": "However, despite their intuitive probabilistic form, chance constrained problems are generally intractable (Shapiro et al., 2009), thus we approximate Problem (5) as follows.", "startOffset": 107, "endOffset": 129}, {"referenceID": 14, "context": "See (Subramanya et al., 2006) for further details on the sensor system and the recordings.", "startOffset": 4, "endOffset": 29}, {"referenceID": 14, "context": "Data set description and feature extraction The data set used for the experiments was collected by Subramanya et al. (2006) using a customized wearable sensor system.", "startOffset": 99, "endOffset": 124}], "year": 2011, "abstractText": "We propose a novel problem formulation of learning a single task when the data are provided in different feature spaces. Each such space is called an outlook, and is assumed to contain both labeled and unlabeled data. The objective is to take advantage of the data from all the outlooks to better classify each of the outlooks. We devise an algorithm that computes optimal affine mappings from different outlooks to a target outlook by matching moments of the empirical distributions. We further derive a probabilistic interpretation of the resulting algorithm and a sample complexity bound indicating how many samples are needed to adequately find the mapping. We report the results of extensive experiments on activity recognition tasks that show the value of the proposed approach in boosting performance.", "creator": "LaTeX with hyperref package"}}}