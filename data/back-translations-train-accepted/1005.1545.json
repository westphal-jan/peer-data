{"id": "1005.1545", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2010", "title": "Improving Semi-Supervised Support Vector Machines Through Unlabeled Instances Selection", "abstract": "Semi-supervised learning tries to improve performance by using unlabeled data. In some situations, however, its performance may become inferior to that of without using unlabeled data. It is desired to have safe semi-supervised methods which often improve the performance while rarely degenerate the performance. In this paper, we focus on semi-supervised support vector machine and propose the S4VM (Safe Semi-Supervised Support Vector Machine) approach. Our intuition is that we shall use only the unlabeled examples which are very likely to help improve the performance while keeping the unlabeled data which are with high risk to be unexploited. Experimental results on a broad range of data sets over 120 different settings show that our proposed S4VM is highly competitive with TSVM. More important, contrasting to TSVM which degenerates performance in many cases when using unlabeled data, our S4VM never degenerates performance.", "histories": [["v1", "Mon, 10 May 2010 13:49:01 GMT  (664kb)", "http://arxiv.org/abs/1005.1545v1", "20 pages, 4 figures"], ["v2", "Mon, 9 May 2011 13:08:45 GMT  (834kb)", "http://arxiv.org/abs/1005.1545v2", "14 pages, 11 figures"]], "COMMENTS": "20 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yu-feng li", "zhi-hua zhou"], "accepted": true, "id": "1005.1545"}, "pdf": {"name": "1005.1545.pdf", "metadata": {"source": "CRF", "title": "S4VM: Safe Semi-Supervised Support Vector Machine", "authors": ["Yu-Feng Li", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 100 5.15 45v1 [cs.LG] 1 0M aySemi-Supervised Learning tries to improve performance by using unlabeled data. In some situations, however, its performance may be worse than that without unlabeled data. It is desirable to have secure semi-supervised methods that often improve performance while rarely impairing performance. In this essay, we focus on semi-supervised support vector machine and suggest the S4VM (Safe Semi-Supervised Support Vector Machine) approach. Our intuition is that we only use the unlabeled examples that are most likely to help improve performance, while the unlabeled data, which carries a high risk of not being used, remain unused. Experimental results in a wide range of data sets over 120 different settings show that our proposed S4VM is highly competitive with TSVM. More importantly, unlike TM, the SVM performance in many semi-supervised cases is not a semi-supervised method:"}, {"heading": "1. Introduction", "text": "In the last ten years we have received a lot of attention and many algorithms have been proposed [9, 31]. Examples are mainly generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support machines [5, 19]. The corresponding authors are: zhouzh @ nju.edu.cnPreprint submitted for review, 17, 2017While SSL performs well in many situations, it has been found that the use of unlabeled data can degenerate performance."}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Semi-Supervised SVMs (S3VMs)", "text": "Semi-supervised SVMs (S3VMs) [5, 19] are known as a popular type of semi-supervised method that extends the monitored SVM to the unlabeled data. Their goal is to assign class names to the unlabeled data, so that the margin of the resulting monitored SVM is maximized. Intuitively, S3VMs are based on cluster assumptions and favor the decision boundaries that traverse the low-density regions [12]. The accuracy of the S3VM target has been well studied using small data sets [11] and much work has been devoted to coping with the high complexity of solving S3VM, examples being local search [19], semi-definite programming relaxation [6], convex concave procedures [14, 26], mean estimation labeling [21] and many other optimization techniques [10]. However, it has been reported that S3VMs will not work well in many situations [28, no results on S3Ms] which are rare."}, {"heading": "2.2. Other SSL Approaches", "text": "The first is generative methods that extend supervised generative models to SSL by additionally estimating the label of the unlabeled data in such a way that the suitability of the model is maximized, such as the use of EM algorithms [23]; the second is graph-based methods that encrypt both the labeled and unlabeled data through a linked graph and then find class names for the unlabeled data, so that their inconsistencies are minimized both with the supervised data and with the underlying graph structure. Examples of this are Mincut [7], harmonic function [32], local and global consistency [3], diverse regulation, etc. The third is co-training [8] that employs multiple learners and improves each learner by assessing the unlabeled data based on the inconsistency of these learners."}, {"heading": "3. Two Baseline Approaches", "text": "S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-"}, {"heading": "4. The Proposed S4VM Method", "text": "4.1. Deficiencies of S3VM-c and S3VM-pBoth methods S3VM-c and S3VM-p are able to reduce the likelihood of degenerating performance, but both suffer from some deficiencies. S3VM-c needs to identify an appropriate number of clusters that generally work well, this remains an open problem. In addition, S3VM-c may lose some useful unmarked instances by introducing a chart, as it does not take into account a relationship between clusters, such as the unmarked instances in groups 2 and 3. In contrast, S3VM-p does not suffer from the problems of S3VM-c by introducing a chart. However, S3VM-p needs to determine some kind of chart and distance measurement. Furthermore, as stated in [25], it is vulnerable to initialization of label data, while S3VM-c does not have such a problem."}, {"heading": "4.2. S4VM", "text": "To create a secure cluster, there is no need for a hierarchical cluster number in advanced hierarchies. (b), (d), (e), (e), (e), (e), (c), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e), (e)."}, {"heading": "4.3. Time Complexity Analysis", "text": "The temporal complexity of hierarchical clustering is O (N2 lnN) using a priority queue algorithm, where N = l + u is the size of the dataset. In the single-link method, the complexity could be further reduced to O (N2) using the next-best merge array [22]. Calculation of all pi and ni for blank instances costs at most O (N2), whereas the temporal complexity of the S3VM implementation, e.g. TSVM [19], scales at least O (TN2) for non-linear kernels, where T is the number of time that calls standard SVM solvers [14]. Thus, it is clear that the temporal complexity of S4VM is close to that of S3VM."}, {"heading": "5. Empirical Study", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Settings", "text": "We evaluate S4VM using a wide range of data sets, including SSL benchmark data sets used in [9] and 16 UCI data sets. Information on these data sets is in Table 2.Benchmark data sets are g241c, g241d, Digit1, USPS, TEXT and BCI. For each data set, the archive provides 2 data sets, one of which uses 10 labeled examples and the other 100 labeled examples. As for UCI data sets, we will randomly select 10, 50 and 100 examples that can be used as examples and use 2http: / / www.kyb.tuebingen.mpg.de / ssl-book / the remaining data as blank data. The experiments are repeated 30 times and the average accuracy and standard deviations are recorded. Considering that all previous half-verified studies with paired t-test with multiple hold-out repetitions are used as statistically significant tests, we will also use S4VM as these."}, {"heading": "5.2. Comparison Results", "text": "This year, it is only a matter of time before the European Commission will be able to find a solution."}, {"heading": "6. Conclusion", "text": "In this paper, we propose a secure semi-monitored support vector machine (S4VM). Our main intuition is that we should use this untagged data, which is likely to help improve performance while leaving the untagged data at high risk to adjust the classification limit of SVM. Comprehensive experimental results show that S4VM performs extremely competitively with TSVM. More importantly, in many cases, unlike TSVM, performance is not degenerated by S4VM. In this paper, the effectiveness of S4VM is validated by empirical studies in the transductive environment. In the future, we want to analyze the inductive setting that expects important insights from this work to help develop more powerful semi-monitored methods."}], "references": [{"title": "UCI machine learning repository", "author": ["A. Asuncion", "D.J. Newman"], "venue": "School of Information and Computer Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Person identification in webcam images: An application of semi-supervised learning", "author": ["M.F. Balcan", "A. Blum", "P.P. Choi", "J. Lafferty", "B. Pantano", "M.R. Rwebangira", "X. Zhu"], "venue": "In Proceeding of the 22nd International Conference on Machine Learning Workshop on Learning with Partially Classified Training Data,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning", "author": ["S. Ben-David", "T. Lu", "D. P\u00e1l"], "venue": "In Proceedings of the 21th Annual Conference on Learning Theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Semi-supervised support vector machines", "author": ["K. Bennett", "A. Demiriz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Convex methods for transduction", "author": ["T. De Bie", "N. Cristianini"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Learning from Labeled and Unlabeled Data using Graph Mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In Proceedings of the 8th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Combining Labeled and Unlabeled Data with Co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In Proceedings of the 7th annual Conference on Computational Learning Theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Optimization techniques for semi-supervised support vector machines", "author": ["O. Chapelle", "V. Sindhwani", "S.S. Keerthi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Branch and Bound for Semi-Supervised Support Vector Machines", "author": ["O. Chapelle", "G. Tubingen", "V. Sindhwani", "S.S. Keerthi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Semi-supervised learning by low density separation", "author": ["O. Chapelle", "A. Zien"], "venue": "In Proceeding of the 8th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Learning from labeled and unlabeled data: An empirical study across techniques and domains", "author": ["N.V. Chawla", "G. Karakoulas"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Large Scale Transductive SVMs", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Semi-supervised learning of mixture models", "author": ["F.G. Cozman", "I. Cohen", "M.C. Cirelo"], "venue": "In Proceeding of the 20th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Pac generalization bounds for co-training", "author": ["S. Dasgupta", "M.L. Littman", "D. McAllester"], "venue": "In Advances in Neural information processing systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Algorithms for clustering data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1988}, {"title": "Graph construction and b-matching for semi-supervised learning", "author": ["T. Jebara", "J. Wang", "S.F. Chang"], "venue": "In Proceeding of the 26th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceeding of the 16th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Statistical analysis of semi-supervised regression", "author": ["J. Lafferty", "L. Wasserman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Semi-supervised learning using label mean", "author": ["Y.-F. Li", "J.T. Kwok", "Z.-H. Zhou"], "venue": "In Proceeding of the 26th International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schtze"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Text classification from labeled and unlabeled documents using EM", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["A. Singh", "R. Nowak", "X. Zhu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Graph transduction via alternating minimization", "author": ["J. Wang", "T. Jebara", "S.F. Chang"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Large margin semi-supervised learning", "author": ["J. Wang", "X. Shen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Analyzing co-training style algorithms", "author": ["W. Wang", "Z.-H. Zhou"], "venue": "In Proceeding of the 18th European Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "The Value of Unlabeled Data for Classification Problems", "author": ["T. Zhang", "F. Oles"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Scholkopf. Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Semi-supervised learning by disagreement", "author": ["Z.-H. Zhou", "M. Li"], "venue": "Knowledge and Information Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Computer Science, University of Wisconsin- Madison,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "In Proceeding of the 20th International Conference on Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Semi-supervised learning with graphs", "author": ["X. Zhu", "J. Lafferty", "R. Rosenfeld"], "venue": "PhD Thesis,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "Over the past decade, SSL has received much attention and many algorithms have been proposed [9, 31].", "startOffset": 93, "endOffset": 100}, {"referenceID": 30, "context": "Over the past decade, SSL has received much attention and many algorithms have been proposed [9, 31].", "startOffset": 93, "endOffset": 100}, {"referenceID": 22, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 69, "endOffset": 80}, {"referenceID": 28, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 69, "endOffset": 80}, {"referenceID": 2, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 69, "endOffset": 80}, {"referenceID": 7, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 142, "endOffset": 149}, {"referenceID": 18, "context": "Examples mainly include generative methods [23]; graph-based methods [32, 29, 3]; co-training [8] and semi-supervised support vector machines [5, 19].", "startOffset": 142, "endOffset": 149}, {"referenceID": 14, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 27, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 12, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 30, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 19, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 3, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 17, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 29, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 23, "context": "While SSL achieves good performances in many situations, it has been found that using unlabeled data may degenerate the performance [15, 28, 13, 31, 20, 4, 18, 30, 24].", "startOffset": 132, "endOffset": 167}, {"referenceID": 23, "context": "To exclude the high risky unlabeled data, we first study the use of standard clustering technique motivated by the discernibility of density set [24] and label propagation technique motivated by confidence estimation, and then propose our S4VM method.", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "Comprehensive experiments on a broad range of data sets on 120 different settings show that the performance of S4VM is highly competitive with TSVM [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "Semi-Supervised SVMs (S3VMs) [5, 19] are known as a popular type of semi-supervised method which extends supervised SVM for the unlabeled data.", "startOffset": 29, "endOffset": 36}, {"referenceID": 18, "context": "Semi-Supervised SVMs (S3VMs) [5, 19] are known as a popular type of semi-supervised method which extends supervised SVM for the unlabeled data.", "startOffset": 29, "endOffset": 36}, {"referenceID": 11, "context": "Intuitively, S3VMs are built on cluster assumption and favor the decision boundaries that cross the low density regions [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 199, "endOffset": 203}, {"referenceID": 5, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 242, "endOffset": 245}, {"referenceID": 13, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 272, "endOffset": 280}, {"referenceID": 25, "context": "The correctness of the objective of S3VM has been well studied on small data sets [11] and many work have been devoted to cope with the high complexity in solving S3VM, examples include local search [19], semi-definite programming relaxation [6], convex-concave procedure [14, 26], label mean estimation", "startOffset": 272, "endOffset": 280}, {"referenceID": 20, "context": "[21] and many other optimization techniques [10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[21] and many other optimization techniques [10].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "However, it has been reported that S3VMs may not work well in many situations [28, 10] and there is no result on how to make S3VMs rarely degenerate performance.", "startOffset": 78, "endOffset": 86}, {"referenceID": 9, "context": "However, it has been reported that S3VMs may not work well in many situations [28, 10] and there is no result on how to make S3VMs rarely degenerate performance.", "startOffset": 78, "endOffset": 86}, {"referenceID": 22, "context": "The first is generative methods which extend supervised generative models to SSL by additionally estimating the label of unlabeled data such that the fitness of the model is maximized, like utilizing EM algorithm [23].", "startOffset": 213, "endOffset": 217}, {"referenceID": 6, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 31, "endOffset": 34}, {"referenceID": 31, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "Examples mainly include Mincut [7], harmonic function [32], local and global consistency [29], manifold regularization [3], etc.", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "The third is co-training [8] which employs multiple learners and improves each learner by labeling the unlabeled data based on the exploitation of disagreement of these learners.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "By using domain knowledge, it is possible to construct a good graph which leads to good performance [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 32, "context": "One interesting observation by [33] is weighted kNN graphs with a small k tend to perform well empirically.", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "As for co-training, the generalization ability has been studied with plentiful theoretical results based on different assumptions [8, 16, 27].", "startOffset": 130, "endOffset": 141}, {"referenceID": 15, "context": "As for co-training, the generalization ability has been studied with plentiful theoretical results based on different assumptions [8, 16, 27].", "startOffset": 130, "endOffset": 141}, {"referenceID": 26, "context": "As for co-training, the generalization ability has been studied with plentiful theoretical results based on different assumptions [8, 16, 27].", "startOffset": 130, "endOffset": 141}, {"referenceID": 19, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 123, "endOffset": 134}, {"referenceID": 3, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 123, "endOffset": 134}, {"referenceID": 23, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 123, "endOffset": 134}, {"referenceID": 12, "context": "It is also notable that recently, several work have been devoted to discuss the usefulness of unlabeled data theoretically [20, 4, 24] or empirically [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "In fact, such an idea could be interpreted by the analysis in [24] where they show that unlabeled data helps when the component density sets are discernable.", "startOffset": 62, "endOffset": 66}, {"referenceID": 31, "context": "A simple choice to use graph-based label propagation method [32] to estimate the label of unlabeled data and their confidences.", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "Finally, for the data points lying on the middle moon, as stated in [25], label propagation methods does not obtain high confidence of the unlabeled points in groups 4 and 5 due to the unbalanced initial labeled points between the middle and downmost moons, and thus S3VM-p does not exploit these unlabeled points as well.", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "Algorithm 2 S3VM-p Input: D, weight matrix W and parameter \u03b7 1: Train SVM & S3VM 2: Perform label propagation method [32] with W, obtain the predicted label yLabPo(xi) and confidence hi for each unlabeled data xi, i = l + 1, .", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Moreover, as stated in [25], it is sensitive to", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Our solution is benefited from hierarchical clustering [17] which leads to our S4VM method.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "Theorem 1 Based on cluster assumption [9], i.", "startOffset": 38, "endOffset": 41}, {"referenceID": 16, "context": ", singe linkage method [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "For single-link method, the complexity could further reduce to O(N2) with use of next-best-merge array [22].", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": ", TSVM [19], scales at least O(TN2) for non-linear kernel where T is the number of time invoking standard SVM solver [14].", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": ", TSVM [19], scales at least O(TN2) for non-linear kernel where T is the number of time invoking standard SVM solver [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 8, "context": "We evaluate S4VM a broad range of data sets including the SSL benchmark data sets used in [9] and 16 UCI data sets [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "We evaluate S4VM a broad range of data sets including the SSL benchmark data sets used in [9] and 16 UCI data sets [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 18, "context": "S3VM is implemented by TSVM algorithm [19]3.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "For the benchmark data sets, we follow the setup in [9].", "startOffset": 52, "endOffset": 55}], "year": 2017, "abstractText": "Semi-supervised learning tries to improve performance by using unlabeled data. In some situations, however, its performance may become inferior to that of without using unlabeled data. It is desired to have safe semi-supervised methods which often improve the performance while rarely degenerate the performance. In this paper, we focus on semi-supervised support vector machine and propose the S4VM (Safe Semi-Supervised Support Vector Machine) approach. Our intuition is that we shall use only the unlabeled examples which are very likely to help improve the performance while keeping the unlabeled data which are with high risk to be unexploited. Experimental results on a broad range of data sets over 120 different settings show that our proposed S4VM is highly competitive with TSVM. More important, contrasting to TSVM which degenerates performance in many cases when using unlabeled data, our S4VM never degenerates performance.", "creator": "LaTeX with hyperref package"}}}