{"id": "1005.1860", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2010", "title": "Feature Selection Using Regularization in Approximate Linear Programs for Markov Decision Processes", "abstract": "Approximate dynamic programming has been used successfully in a large variety of domains, but it relies on a small set of provided approximation features to calculate solutions reliably. Large and rich sets of features can cause existing algorithms to overfit because of a limited number of samples. We address this shortcoming using $L_1$ regularization in approximate linear programming. Because the proposed method can automatically select the appropriate richness of features, its performance does not degrade with an increasing number of features. These results rely on new and stronger sampling bounds for regularized approximate linear programs. We also propose a computationally efficient homotopy method. The empirical evaluation of the approach shows that the proposed method performs well on simple MDPs and standard benchmark problems.", "histories": [["v1", "Tue, 11 May 2010 15:24:36 GMT  (281kb)", "https://arxiv.org/abs/1005.1860v1", "Technical report corresponding to the ICML2010 submission of the same name"], ["v2", "Thu, 20 May 2010 14:19:17 GMT  (281kb)", "http://arxiv.org/abs/1005.1860v2", "Technical report corresponding to the ICML2010 submission of the same name"]], "COMMENTS": "Technical report corresponding to the ICML2010 submission of the same name", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marek petrik", "gavin taylor", "ronald parr", "shlomo zilberstein"], "accepted": true, "id": "1005.1860"}, "pdf": {"name": "1005.1860.pdf", "metadata": {"source": "META", "title": "Feature Selection Using Regularization in Approximate Linear Programs for Markov Decision Processes", "authors": ["Marek Petrik", "Gavin Taylor"], "emails": ["petrik@cs.umass.edu", "gvtaylor@cs.duke.edu", "parr@cs.duke.edu", "shlomo@cs.umass.edu"], "sections": [{"heading": null, "text": "ar Xiv: 100 5.18 60v2 [cs.AI] 20 May 2"}, {"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Framework and Notation", "text": "In this section, we formally define Markov decision-making processes and linear value functions as approximation. A Markov decision-making process is a tuple (S, A, P, R, N), where S represents the possibly infinite set of states, and A represents the finite set of actions. P: S \u00b7 S \u00b7 S \u00b7 A 7 \u2192 [0, 1] is the transition function, where P (s, s, A) represents the probability of transition to the state s \u00b2 from the given action a. The function r: S \u00b7 A \u00b7 R is the reward function, and \u03b3 is the discount factor. Pa and ra are used to denote the probable transition matrix and the reward vector for action. We want to find a value function v that maps each state s \u00b2 S to the expected total discounted reward for the process. Value functions can be useful in the creation or analysis of a policy."}, {"heading": "3. Approximate Linear Programming", "text": "Approximate Linear Programming (ALP) is an approach for calculating a value approximation for large MDP with a series of characteristics defining a linear space M. (Schweitzer & Seidmann, 1985; de Farias & Van Roy, 2003) The ALP takes the following form: The ALP is a distribution over the initial states and the constraints are for all (s, a), (S, A), (S, A), (S, A), (S, A), (S), (S), (S, A), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S)."}, {"heading": "4. Regularized Approximate Linear Programming", "text": "In this section, we present L1-regulated ALP (RALP) as an approach to automating feature selection and alleviating the need for all constraints in the standard ALP. Adding L1 regulation to ALP allows the user to provide an arbitrary set of features without the risk of overfitting. RALP constraint for base ALP and L1 constraint is defined as min w\u03c1T\u03a6ws.t. The RALP solution approaches the ALP solution. The objective value of (2) as a function of ALP is called a loop. We generally use e = 1 \u2212 1, which is a vector of all but the first position, which is 0; since the first feature is the constant feature, we do not include this problem in the constant (ALP)."}, {"heading": "5. Sampling Bounds", "text": "The purpose of this approach is to show that there are two essential advantages arising from the distribution of funds. (...) The purpose of this approach is to show two essential advantages. (...) The purpose of this approach is to provide two essential advantages. (...) The purpose of this approach is to consider two essential aspects. (...) The purpose of this approach is to consider two essential aspects. (...) The purpose of this approach is to consider two essential aspects. (...) The purpose of this approach is to consider two essential aspects. (...) The purpose of this approach is to find a solution. (...) The purpose of this approach is to consider. (...) The purpose of this approach is to consider. (...) The purpose of this approach is to consider. (...) The purpose of this approach is. (...) The purpose of this approach is. (...) The purpose of this approach is."}, {"heading": "6. Experimental Results", "text": "It is indeed the case that we are able to put ourselves at the top, \"he said in an interview with the German Press Agency.\" We have to put ourselves at the top of the EU, \"he said.\" We have to put ourselves at the top of the EU, \"he said.\" We have to put ourselves at the top of the EU. \""}, {"heading": "7. Conclusion and Related Work", "text": "In this paper, we introduced L1-regulated Approximate Linear Programming and demonstrated its properties for the combined selection of characteristics and the approximation of value functions in amplification learning. RALP is simultaneously concerned with the selection of characteristics, the approximation of value functions, and policy determining problems; our experimental results show that it effectively solves these problems for multiple sample problems, while our limitations explain the impact of the sample on the resulting approximation. There are many additional problems that need to be solved. First, better boundaries need to be created to guide the sample. Our limitations explain the behavior of the RALP approach because it refers to the trade-off between the abundance of characteristics and the number of available samples, but these boundaries can sometimes be quite loose."}, {"heading": "Acknowledgements", "text": "This work was partially supported by DARPA CSSG HR0011-06-1-0027, NSF IIS-0713435, the Air Force Office of Scientific Research under grant number FA9550-08-1-0171, and Duke University Center for Theoretical and Mathematical Sciences. We also thank the anonymous reviewers for their useful comments."}, {"heading": "A. Proofs", "text": "The following Lemmas summarize the basic characteristics of the Bellman operator: V (v + 1): V (v + 1): V (v + 1): V (v + 1): V (v + 1): V (v + 1): V (v + 1): V (v + 1): V (v): V (v): V (v): V (v): V (v): V (v): V (v): V (v): V (v): V (v): V (v): V (v): V): V (v): V (V): V (V): V (v): V (V): V (V): V (V): V (V): V: V (V): V (V): V) (V): V (V): V (V): V (V): V (V)."}, {"heading": "B. Homotopy Continuation Method", "text": "The basic solutions in Simplex, however, depend on the size of the number of variables. As we are interested in solving very large linear programs, this is often not practicable.The homotopy method we propose is insteadFeature Selection using regularization based on basic realizable solutions with a size corresponding to the number of variables. We also use x (i) to name the i element of the vector. We derive the algorithm for a generic linear program, defined as follows: min xcTxs.t. Ax-th series and A-th series as i-column. We derive the algorithm for a generic linear program: min xcTxs.t."}], "references": [{"title": "Dantzig selector homotopy with dynamic measurements", "author": ["Asif", "Salman"], "venue": "In IS&T/SPIE Computational Imaging VII,", "citeRegEx": "Asif and Salman.,? \\Q2009\\E", "shortCiteRegEx": "Asif and Salman.", "year": 2009}, {"title": "Reinforcement Learning: an Introduction", "author": ["Barto", "Andrew G", "Sutton", "Richard S"], "venue": null, "citeRegEx": "Barto et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1998}, {"title": "The Dantzig selector:statistical estimation when p is much larger than n", "author": ["Candes", "Emmanuel", "Tao", "Terence"], "venue": "Annals of Statistics,", "citeRegEx": "Candes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2007}, {"title": "On constraint sampling for the linear programming approach to approximate dynamic programming", "author": ["de Farias", "Daniela Pucci", "Van Roy", "Benjamin"], "venue": "Math. of Operations Res,", "citeRegEx": "Farias et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Farias et al\\.", "year": 2001}, {"title": "A smoothed approximate linear program", "author": ["Desai", "Vijay", "Farias", "Vivek", "Moallemi", "Ciamac"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Desai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Desai et al\\.", "year": 2009}, {"title": "Regularized policy iteration", "author": ["Farahmand", "Amir Massoud", "Ghavamzadeh", "Mohammad", "Szepesvari", "Csaba", "Mannor", "Shie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Farahmand et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2008}, {"title": "DASSO: Connections between the Dantzig selector and lasso", "author": ["James", "Gareth M", "Radchenko", "Peter", "Lv", "Jinchi"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "James et al\\.,? \\Q2009\\E", "shortCiteRegEx": "James et al\\.", "year": 2009}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["Kolter", "J. Zico", "Ng", "Andrew"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "LeastSquares Policy Iteration", "author": ["Lagoudakis", "Michail G", "Parr", "Ronald"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2003}, {"title": "Learning representation and control in markov decision processes: New frontiers", "author": ["Mahadevan", "Sridhar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahadevan and Sridhar.,? \\Q2008\\E", "shortCiteRegEx": "Mahadevan and Sridhar.", "year": 2008}, {"title": "Analyzing feature generation for value-function approximation", "author": ["Parr", "Ronald", "Painter-Wakefield", "Christopher", "Li", "Lihong", "Littman", "Michael"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Parr et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Parr et al\\.", "year": 2007}, {"title": "Constraint Relaxation in Approximate Linear Programs", "author": ["Petrik", "Marek", "Zilberstein", "Shlomo"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Petrik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2009}, {"title": "Learning to drive a bicycle using reinforcement learning and shaping", "author": ["Randl\u00f8v", "Jette", "Alstr\u00f8m", "Preben"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Randl\u00f8v et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Randl\u00f8v et al\\.", "year": 1998}, {"title": "Generalized polynomial approximations in Markovian decision processes", "author": ["Schweitzer", "Paul J", "Seidmann", "Abraham"], "venue": "Journal of mathematical analysis and applications,", "citeRegEx": "Schweitzer et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer et al\\.", "year": 1985}, {"title": "Kernelized Value Function Approximation for Reinforcement Learning", "author": ["Taylor", "Gavin", "Parr", "Ronald"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Linear Programming: Foundations and Extensions", "author": ["Vanderbei", "Robert J"], "venue": "Springer, 2nd edition,", "citeRegEx": "Vanderbei and J.,? \\Q2001\\E", "shortCiteRegEx": "Vanderbei and J.", "year": 2001}, {"title": "An approach to fuzzy control of nonlinear systems: Stability and design issues", "author": ["Wang", "Hua O", "Tanaka", "Kazuo", "Griffin", "Michael F"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "Wang et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Wang et al\\.", "year": 1996}], "referenceMentions": [{"referenceID": 10, "context": "Feature selection, therefore, seeks to automate this process in a way that may preserve the computational simplicity of linear approximation (Parr et al., 2007; Mahadevan, 2008).", "startOffset": 141, "endOffset": 177}, {"referenceID": 4, "context": "ALP has often under-performed ADP methods in practice; this issue has been recently studied and partially remedied (Petrik & Zilberstein, 2009; Desai et al., 2009).", "startOffset": 115, "endOffset": 163}, {"referenceID": 17, "context": "dulum, a standard benchmark problem in reinforcement learning (Wang et al., 1996; Lagoudakis & Parr, 2003).", "startOffset": 62, "endOffset": 106}], "year": 2010, "abstractText": "Approximate dynamic programming has been used successfully in a large variety of domains, but it relies on a small set of provided approximation features to calculate solutions reliably. Large and rich sets of features can cause existing algorithms to overfit because of a limited number of samples. We address this shortcoming using L1 regularization in approximate linear programming. Because the proposed method can automatically select the appropriate richness of features, its performance does not degrade with an increasing number of features. These results rely on new and stronger sampling bounds for regularized approximate linear programs. We also propose a computationally efficient homotopy method. The empirical evaluation of the approach shows that the proposed method performs well on simple MDPs and standard benchmark problems.", "creator": "LaTeX with hyperref package"}}}