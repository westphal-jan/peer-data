{"id": "1005.5581", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2010", "title": "Multi-View Active Learning in the Non-Realizable Case", "abstract": "Many classical bounds on the sample complexity of active learning based on the realizability assumption have been derived, which show that active learning can exponentially improve the sample complexity over passive learning. However, this realizability assumption could not be met in practice and few results on the exponential improvement in the sample complexity in the non-realizable case has been obtained. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case with Tsybakov noise condition under the multi-view setting. We prove that the sample complexity of active learning with unbounded Tsybakov noise can be $\\widetilde{O}(\\log \\frac{1}{\\epsilon})$, contrasting to that polynomial improvement is the best possible achievement with the same noise condition in single-view setting. We also prove that, contrasting to that in previous polynomial bounds the order of $1/\\epsilon$ is related to the Tsybakov noise condition, in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\\widetilde{O}(\\frac{1}{\\epsilon})$, where the order of $1/\\epsilon$ is independent of the Tsybakov noise condition.", "histories": [["v1", "Mon, 31 May 2010 03:59:35 GMT  (36kb)", "https://arxiv.org/abs/1005.5581v1", "24 pages, 1 figure"], ["v2", "Fri, 29 Oct 2010 09:44:44 GMT  (35kb)", "http://arxiv.org/abs/1005.5581v2", "22 pages, 1 figure"]], "COMMENTS": "24 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wei wang 0028", "zhi-hua zhou"], "accepted": true, "id": "1005.5581"}, "pdf": {"name": "1005.5581.pdf", "metadata": {"source": "CRF", "title": "Multi-View Active Learning in the Non-Realizable Case", "authors": ["Wei Wang", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 100 5.55 81v2 [cs.LG] 2 9O ct2 01The sample complexity of active learning under the feasibility assumption has been well studied, but the feasibility assumption rarely applies in practice. In this paper, we theoretically describe the sample complexity of active learning under the feasibility assumption in the non-feasibility case of multiview setting. We also prove that the sample complexity of active learning with unlimited Tsybakov noise can be O-ig in the multiview range (Log 1gig), as opposed to setting from a point of view where polynomial improvement is the best possible performance. We also prove that in the general multiview setting, the sample complexity of active learning with unlimited Tsybakov noise is O-ig (1gig), in which the sequence is 1 / 2 independent of the parameter in Tsybakov noise, in contrast to earlier polynomial boundaries where the parameters in Tsybakov are related to the order 1 / 2 in the Tsybakov noise."}, {"heading": "1. Introduction", "text": "The number of knowledge queried, which is necessary and sufficient to obtain a good space, is known as the example complexity of active learning. Many theoretical limits of the example complexity of active learning have been derived on the basis of the feasibility assumptions (i.e., there is a hypothesis that perfectly separates the data to identify the author). E-mail: zhouzh @ nju.edu.edu.cnPreprint for verification November 1, 2010the hypothesis class) [4, 5, 11, 12, 16] However, the feasibility assumption rarely holds in practice."}, {"heading": "2. Related Work", "text": "In general, the unrealizability of the learning task was caused by the presence of noise, which is a simple improvement for learning the task with any form of noise. [2] suggested the agnostic active learning algorithm A2, proving that its sample complexity depends on the hypothesis class and data distribution, and proved that the sample complexity of the A2 algorithm is more closely linked to the sample complexity of the A2 algorithm. [3] Hanneke [4] defined the discrepancy coefficient, which depends on the hypothesis class and data distribution, and proved that the sample complexity of the A2 algorithm is limited to the sample complexity of the A2 algorithm. [4] Later Dasgupta et al. [13] developed a generalagnostic active learning algorithm that extends the scheme and proves that its sample complexity is O."}, {"heading": "3. Preliminaries", "text": "In the Multi-View setting, the instances are described with several different sets of attributes. For the sake of simplicity, let's consider only the two-view setting in this work. Let's assume that X = X1 \u00b7 X2 is the instance space, X1 and X2 are the two views, Y = {0, 1} is the label space, and D is the distribution over X \u00b7 Y. Let's assume that c = (c1, c2) is the optimal Bayes classifier, where c1 and c2 are the optimal Bayes classifiers in the two views. Let H1 and H2 be the hypothesis class in each view and assume that c1 and c2 are the optimal Bayes."}, {"heading": "4. \u03b1-Expansion in the Non-realizable Case", "text": "S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 - S2 -"}, {"heading": "5. Multi-view Active Learning with Non-degradation Condition", "text": "In this section, we will first consider the multiview learning processes in Table 1 and analyze whether multiview settings can help improve the complexity of active learning in the unrealizable case. In the multiview settings, the classifiers are often combined to make predictions and many strategies can be used to combine them. In this thesis, we will consider the following two combination programs, h + and h \u2212, for both types of binary classification: hi + (x) = 10 otherwise hi \u2212 (x) = 0 if hi1 (x1) = h i 2 (x2) otherwise (4) 5.1. The situation in which S + 1 = S, 2With (4), the error rate of the combined classifiers hi + and h \u2212 satisfy (5) and (6), respektively.R (hi +) \u2212 R (S) \u2212 R (S)."}, {"heading": "6. Multi-view Active Learning without Non-degradation Condition", "text": "In this section, we focus on multi-level active learning in Table 2 and give an analysis with the non-degrading condition that we forego. First, we give theorem 6 for the random complexity of multi-level active learning in Table 2, if we give the two-level learning behavior in Table 1 as a whole (S + 1) with respect to the hypothesis class H1 \u00d7 H2 with respect to definition 1, if s = 2 log 1 logs 1 C2 and mi = 256kC C21 (V + log 1).Theorem 6 for the data distribution D expansion with respect to the hypothesis class H2 \u00d7 H2 with respect to definition 1, if s = 2 log 1 logs 1 C2 and mi = 256kC C21 (V + log 1).Theorem We generate two classifying classes + and h \u2212 s with at least one error rate not greater than R (S)."}, {"heading": "7. Empirical Verification", "text": "In this section, we will empirically check whether setting multiple views can significantly improve the sample complexity of active learning in the unrealizable case. In the experiment, we will use the semi-artificial dataset [20] and the course dataset [6]. The semi-artificial dataset has two artificial views, which are generated by randomly pairing two examples from the same class and contain 800 examples. To control the correlation between the two views, the number of clusters per class can be specified as parameters. In the experiments, we will use 1 cluster, 2 clusters, and 4 clusters each. The course dataset has two natural views: side view (i.e. the text that appears on the page) and link view (i.e. the anchor text attached to hyperlinks pointing to the page) and contains 1,051 examples. We will randomly use 25% data as a test set and use the remaining 75% data to generate the blank dataset U."}, {"heading": "8. Conclusion", "text": "In this paper, we present the first study of active learning in an unrealizable case with multi-view setting. We prove that the sample complexity of active learning can be improved to O-value with multiple views and unlimited Sybakov noise, as opposed to single views where only a polynomic improvement with the same noise state is possible. In general multiple view, we prove that the sample complexity of active learning with unlimited Sybakov noise is O-value (1 / 2), where the sequence of 1 / 2 is independent of the parameter in Sybakov noise, as opposed to previous polynomic boundaries where the sequence of 1 / 2 is related to the parameter in Sybakov noise. In general, the unrealizable learning task can be caused by many types of noise, e.g. misclassification noise and malignant noise. It would be interesting to expand our work to more general noise models."}], "references": [{"title": "editors", "author": ["M. Anthony", "P.L. Bartlett"], "venue": "Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "ICML, pages 65\u201372", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Co-training and expansion: Towards bridging theory and practice", "author": ["M.-F. Balcan", "A. Blum", "K. Yang"], "venue": "In NIPS", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Margin based active learning", "author": ["M.-F. Balcan", "A.Z. Broder", "T. Zhang"], "venue": "COLT, pages 35\u201350", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "The true sample complexity of active learning", "author": ["M.-F. Balcan", "S. Hanneke", "J. Wortman"], "venue": "COLT, pages 45\u201356", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "COLT, pages 92\u2013100", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Upper and lower error bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": "Allerton Conference, pages 225\u2013234", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Minimax bounds for active learning", "author": ["R.M. Castro", "R.D. Nowak"], "venue": "IEEE Transactions on Information Theory, 54(5):2339\u20132353", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Linear classification and selective sampling under low noise conditions", "author": ["G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile"], "venue": "In NIPS", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Improving generalization with active learning", "author": ["D.A. Cohn", "L.E. Atlas", "R.E. Ladner"], "venue": "Machine Learning, 15(2):201\u2013221", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1994}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "In NIPS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A.T. Kalai", "C. Monteleoni"], "venue": "COLT, pages 249\u2013263", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "editors", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "A Probabilistic Theory of Pattern Recognition. Springer, New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, 28(2-3):133\u2013168", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "ICML, pages 353\u2013360", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "COLT", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "ACL, pages 63\u201377", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Active + semi-supervised learning = robust multi-view learning", "author": ["I. Muslea", "S. Minton", "C.A. Knoblock"], "venue": "ICML, pages 435\u2013442", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A. Tsybakov"], "venue": "The Annals of Statistics, 32(1):135\u2013166", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Sufficient conditions for agnostic active learnable", "author": ["L. Wang"], "venue": "In NIPS 22,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "On multi-view active learning and the combination with semisupervised learning", "author": ["W. Wang", "Z.-H. Zhou"], "venue": "ICML, pages 1152\u20131159", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.", "startOffset": 19, "endOffset": 31}, {"referenceID": 12, "context": "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.", "startOffset": 19, "endOffset": 31}, {"referenceID": 15, "context": "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.", "startOffset": 19, "endOffset": 31}, {"referenceID": 3, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 10, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 11, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 13, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 15, "context": "the hypothesis class) [4, 5, 11, 12, 14, 16].", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].", "startOffset": 122, "endOffset": 133}, {"referenceID": 12, "context": ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].", "startOffset": 122, "endOffset": 133}, {"referenceID": 16, "context": ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].", "startOffset": 122, "endOffset": 133}, {"referenceID": 18, "context": "It is worth noting that these bounds obtained in the non-realizable case match the lower bound \u03a9( 2 \u01eb2 ) [19], in the same order as the upper bound O( 1 \u01eb2 ) of passive learning (\u03b7 denotes the generalization error rate of the optimal classifier in the hypothesis class and \u01eb bounds how close to the optimal classifier in the hypothesis class the active learner has to get).", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "Tsybakov noise model [21] is more and more popular in theoretical analysis on the sample complexity of active learning.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "However, existing result [8] shows that obtaining exponential improvement in the sample complexity of active learning with unbounded Tsybakov noise is hard.", "startOffset": 25, "endOffset": 28}, {"referenceID": 22, "context": "Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case.", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "-We define \u03b1-expansion, which extends the definition in [3] and [23] to the non-realizable case, and \u03b2-condition for multi-view setting.", "startOffset": 56, "endOffset": 59}, {"referenceID": 22, "context": "-We define \u03b1-expansion, which extends the definition in [3] and [23] to the non-realizable case, and \u03b2-condition for multi-view setting.", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "[2] proposed the agnostic active learning algorithm A2 and proved that its sample complexity is \u00d4( 2 \u01eb2 ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "2 Hoping to get tighter bound on the sample complexity of the algorithm A2, Hanneke [17] defined the disagreement coefficient \u03b8, which depends on the hypothesis class and the data distribution, and proved that the sample complexity of the algorithm A2 is \u00d4(\u03b82 \u03b7 2 \u01eb2 ).", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "[13] developed a general agnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is \u00d4(\u03b8 \u03b7 2 \u01eb2 ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[13] developed a general agnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is \u00d4(\u03b8 \u03b7 2 \u01eb2 ).", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "Recently, the popular Tsybakov noise model [21] was considered in theoretical analysis on active learning and there have been some bounds on the sample complexity.", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].", "startOffset": 145, "endOffset": 155}, {"referenceID": 6, "context": "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].", "startOffset": 145, "endOffset": 155}, {"referenceID": 17, "context": "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].", "startOffset": 145, "endOffset": 155}, {"referenceID": 3, "context": "[4] assumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the sample complexity of active learning with unbounded Tsybakov noise is O ( \u01eb 2 1+\u03bb ) (\u03bb > 0 depends on Tsybakov noise).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Castro and Nowak [8] showed that the sample complexity of active learning with unbounded Tsybakov noise is \u00d4 ( \u01eb \u2212 2\u03bc\u03c9+d\u22122\u03c9\u22121 \u03bc\u03c9 ) (\u03bc > 1 depends on another form of Tsybakov noise, \u03c9 \u2265 1 depends on the H\u00f6lder smoothness and d is the dimension of the data).", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "[9] assumed that the labels of examples are generated according to a simple linear noise model and indicated that the sample complexity of active learning with unbounded Tsybakov noise is O ( \u01eb \u2212 2(3+\u03bb) (1+\u03bb)(2+\u03bb) ) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity \u00d4 ( \u01eb 2 1+\u03bb ) for active learning with unbounded Tsybakov noise.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity \u00d4 ( \u01eb 2 1+\u03bb ) for active learning with unbounded Tsybakov noise.", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity \u00d4 ( \u01eb 2 1+\u03bb ) for active learning with unbounded Tsybakov noise.", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "For active learning with unbounded Tsybakov noise, Castro and Nowak [8] also proved that at least \u03a9(\u01eb\u2212\u03c1) labels are requested to learn an \u01eb-approximation of the optimal classifier (\u03c1 \u2208 (0, 2) depends on Tsybakov noise).", "startOffset": 68, "endOffset": 71}, {"referenceID": 21, "context": "Wang [22] introduced smooth assumption to active learning with approximate Tsybakov noise and proved that if the classification boundary and the underlying distribution are smooth to \u03be-th order and \u03be > d, the sample complexity of active learning is \u00d4 ( \u01eb 2d \u03be+d ) ; if the boundary and the distribution are infinitely smooth, the sample complexity of active learning is O ( polylog(1\u01eb ) ) .", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "It is well-known [15] that S \u2217 v = {xv : \u03c6v(xv) \u2265 1 2}, where \u03c6v(xv) = P (y = 1|xv).", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv\u2208Xv (|\u03c6v(xv) \u2212 1/2| \u2264 t) \u2264 C0t \u03bb for some finite C0 > 0, \u03bb > 0 and all 0 < t \u2264 1/2, where \u03bb = \u221e corresponds to the best learning situation and the noise is called bounded [8]; while \u03bb = 0 corresponds to the worst situation.", "startOffset": 158, "endOffset": 162}, {"referenceID": 7, "context": "In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv\u2208Xv (|\u03c6v(xv) \u2212 1/2| \u2264 t) \u2264 C0t \u03bb for some finite C0 > 0, \u03bb > 0 and all 0 < t \u2264 1/2, where \u03bb = \u221e corresponds to the best learning situation and the noise is called bounded [8]; while \u03bb = 0 corresponds to the worst situation.", "startOffset": 343, "endOffset": 346}, {"referenceID": 7, "context": "When \u03bb < \u221e, the noise is called unbounded [8].", "startOffset": 42, "endOffset": 45}, {"referenceID": 20, "context": "According to Proposition 1 in [21], it is easy to know that (2) holds.", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "We will use the following lamma [1] which gives the standard sample complexity for non-realizable learning task.", "startOffset": 32, "endOffset": 35}, {"referenceID": 19, "context": "Multi-view active learning first described in [20] focuses on the contention points (i.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "[3] also gave a definition of expansion, Pr(T1\u2295T2) \u2265 \u03b1min [ Pr(T1\u2229T2), P r(T1\u2229T2) ] , for realizable learning task under the assumptions that the learner in each view is never \u201cconfident but wrong\u201d and the learning algorithm is able to learn from positive data only.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In addition, in [3] the instances which are agreed by the two views but are predicted different label by the optimal classifier can be denoted as T1 \u2229 T2.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "So, it can be found that Definition 1 and the definition of expansion in [3] are based on the same intuition that the amount of contention points is no less than a fraction of the amount of instances which are agreed by the two views but are predicted different label by the optimal classifiers.", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "As discussed in [23], we also assume that the learner in Table 1 satisfies the non-degradation condition as the amount of labeled training examples increases, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "In the experiment we use the semi-artificial data set [20] and the course data set [6].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "In the experiment we use the semi-artificial data set [20] and the course data set [6].", "startOffset": 83, "endOffset": 86}], "year": 2010, "abstractText": "The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multiview setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multiview active learning can be \u00d5(log 1\u01eb ), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is \u00d5(1\u01eb ), where the order of 1/\u01eb is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/\u01eb is related to the parameter in Tsybakov noise.", "creator": "LaTeX with hyperref package"}}}