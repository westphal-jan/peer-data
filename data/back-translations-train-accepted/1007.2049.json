{"id": "1007.2049", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2010", "title": "Reinforcement Learning via AIXI Approximation", "abstract": "This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains.", "histories": [["v1", "Tue, 13 Jul 2010 08:48:18 GMT  (34kb)", "http://arxiv.org/abs/1007.2049v1", "8 LaTeX pages, 1 figure"]], "COMMENTS": "8 LaTeX pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joel veness", "kee siong ng", "marcus hutter", "david silver"], "accepted": true, "id": "1007.2049"}, "pdf": {"name": "1007.2049.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["joelv@cse.unsw.edu.au", "keesiong.ng@gmail.com", "marcus.hutter@anu.edu.au", "davidstarsilver@googlemail.com"], "sections": [{"heading": null, "text": "ar Xiv: 100 7,20 49v1 [cs.LG] 1 3Ju l 201 0contents"}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 The Agent Setting 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Bayesian Agents 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Monte Carlo Expectimax Approximation 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Action-Conditional CTW 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Theoretical Results 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Experimental Results 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Related Work 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Limitations 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 Conclusion 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11 Acknowledgements 8", "text": "Keywords: Reinforcement Learning (RL); Context Tree Weighting (CTW); Monte Carlo Tree Search (MCTS); Upper Confidence bounds applied to Trees (UCT); Partially Observable Markov Decision Process (POMDP); Prediction Suffix Trees (PST)."}, {"heading": "1 Introduction", "text": "Consider an active ingredient that exists in an unknown environment; the active ingredient interacts with the environment in cycles; during each cycle, the active ingredient performs an action and receives in return an observation and a reward; the general learning problem of enhancement is to construct an active ingredient that absorbs as much reward as possible from an initially unknown environment over time; the active ingredient [Hut05] is a formal, mathematical solution to the general learning problem of enhancement; it can be divided into two main components: planning and prediction; planning amounts to performing an expectation operation to determine each action; the prediction uses the Bajesian model to predict future observations and rewards on the basis of past experiences, using the largest possible model class that can be expressed on a lathe lathe machine; and AIXI is presented in [Hut05] as optimal in the sense that it quickly learns an accurate model of the unknown environment and uses it to maximize the expected future reward."}, {"heading": "2 The Agent Setting", "text": "This section introduces the notation and terminology we will use to describe the strings of the agent experience, the true underlying environment, and the model of the agent of the environment. < < < < < < < \"The (finite) action, observation, and reward spaces are respectively referred to by A, O, and R as reward.\" We use X to define the perception space O \u00b7 R.Definition 1 A story h is an element of (A \u00b7 X) - (A \u00b7 X) - (A \u00b7 A. Notation: A string x1x2. The length n is referred to as x1: n. The empty string is referred to by L."}, {"heading": "3 Bayesian Agents", "text": "One way to learn an environmental model is to take a Bayesian approach. Instead of committing to a single environmental model, the agent uses a mixture of environmental models. This requires committing to a class of possible environments (the model class), assigning a starting weight to each possible environment (the previous one), and then updating the weight for each model using the Bayes rule (calculating the back level) whenever more experience is gained. However, the above method is similar to Bayesian methods for predicting (individually typed) observations, the key difference in the agent setup is that each prediction now depends on previous agent actions as well. We incorporate these by using the action-based definitions and identities in Section 2.Definition 4."}, {"heading": "4 Monte Carlo Expectimax Approximation", "text": "Note: Since an environmental model subsumes both MDPs and POMDPs, the UCT algorithm effectively extends to a broader class of problem domains. UCT algorithms have proven effective in solving large discount or finite horizon MDPs. It is based on a generative model of the MDP, which, if there is a state action pair (s), produces a subsequent state reward pair (s)."}, {"heading": "5 Action-Conditional CTW", "text": "We present a large mixing model for the use of PTW. (WST95) is an efficient and theoretically well-studied binary prediction algorithm that works well in practice. (10) It is an online model in which the binary sequence is seen so far, M is a prediction suffix tree [RST96], Pr (M) is the previous probability of M, and the summation is across all prediction results. A naive prediction of D (10) takes time O (22 D); using CTW, this calculation only takes O (D) time. (D) In this section we outline how CTW can be extended to the probabilities of PTW."}, {"heading": "6 Theoretical Results", "text": "We are now examining some of their properties. By examining (5) the mixing model (18), we can show that the optimal action for an agent is t in the time after we have had the experience that the maximum action in the range of the AIXI agent max. + m + m x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x."}, {"heading": "7 Experimental Results", "text": "This section evaluates our approximate AIXI agent on a variety of test domains. Cheese Maze, 4x4 Grid, and Extended Tiger domains are taken from the POMDP literature; the TicTacToe domain includes a repeated series of games against an opponent who moves randomly; the Biased RockPaperScissor domain is described in [FMWR07], where the agent repeatedly plays RockPaperScissor against an exploitative opponent; two other challenging domains are included: Kuhn Poker [HSHB05], where the agent plays second against a Nash optimal player; and a partially observable version of Pacman described in [VNHS09]. With the exception of Pacman, each domain has a known optimal solution. Although our domains are modest, the agent requires the environment to learn from scratch, the difficulty of each of these problems increases significantly. Table 1 outlines the preseeding parameters that will be used in each of the experiments, together with the number of observation rooms and the size of the action to be used."}, {"heading": "8 Related Work", "text": "The BLHT algorithm [SH99] is closely related to our work. It uses symbol-level PSTs for learning and an (unspecified) dynamic programming algorithm for controlling. BLHT uses the most likely model for predicting, while we use a blending model that allows for a much stronger convergence result. Another distinction is our use of an Ockham model that has been shown to be asymptomatically optimal when the environment is n-Markov. We implemented the Active LZ test domain, Biased RPS, and compared to their published results. Our agent was able to achieve an optimal level of performance within 106 cycles; in contrast, Active LZ was suboptimal after 108 cycles."}, {"heading": "9 Limitations", "text": "The most important limitation of our current AIXI approach is the restricted model class. Our agent performs poorly when the underlying environment cannot be predicted well by a PST with limited depth. If a large PST model is required for accurate prediction, prohibitive amounts of experience are required. For example, it would be unrealistic to believe that our current AIXI approach could cope with real image or audio data. Identifying efficient and general model classes that better match the AIXI ideal is an important area for future work. Some preliminary ideas are explored in [VNHS09]."}, {"heading": "10 Conclusion", "text": "We have introduced the first mathematically comprehensible approach to the AIXI agent and have shown that it offers a promising approach to the general learning problem of amplification. Investigations to predict multi-alphabet CTW, parallelization of \u03c1UCT, a further expansion of the model class (ideally beyond Markov models with variable order) or more complex rollout strategies for \u03c1UCT are exciting areas for future studies."}, {"heading": "11 Acknowledgements", "text": "This work was supported by the Australian Research Council with funding DP0988049. NICTA is funded by the Australian Government, represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence Programme."}], "references": [{"title": "JMLR", "author": ["Peter Auer. Using confidence bounds for exploitation-exploration trade-offs"], "venue": "3:397\u2013422,", "citeRegEx": "Aue02", "shortCiteRegEx": null, "year": 2002}, {"title": "Universal Reinforcement Learning", "author": ["V. Farias", "C. Moallemi", "T. Weissman", "B. Van Roy"], "venue": "CoRR, abs/0707.3087", "citeRegEx": "FMWR07", "shortCiteRegEx": null, "year": 2007}, {"title": "In AAAI\u201905", "author": ["Bret Hoehn", "Finnegan Southey", "Robert C. Holte", "Valeriy Bulitko. Effective short-term opponent exploitation in simplified poker"], "venue": "pages 783\u2013788,", "citeRegEx": "HSHB05", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability", "author": ["Marcus Hutter"], "venue": "Springer,", "citeRegEx": "Hut05", "shortCiteRegEx": null, "year": 2005}, {"title": "In ECML", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri. Bandit based Monte-Carlo planning"], "venue": "pages 282\u2013293,", "citeRegEx": "KS06", "shortCiteRegEx": null, "year": 2006}, {"title": "PhD thesis", "author": ["Andrew Kachites McCallum. Reinforcement Learning with Selective Perception", "Hidden State"], "venue": "University of Rochester,", "citeRegEx": "McC96", "shortCiteRegEx": null, "year": 1996}, {"title": "Model-based Bayesian Reinforcement Learning in Partially Observable Domains", "author": ["Pascal Poupart", "Nikos Vlassis"], "venue": "ISAIM,", "citeRegEx": "PV08", "shortCiteRegEx": null, "year": 2008}, {"title": "The power of amnesia: Learning probabilistic automata with variable memory length", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine Learning, 25(2):117\u2013150", "citeRegEx": "RST96", "shortCiteRegEx": null, "year": 1996}, {"title": "In NIPS", "author": ["Nobuo Suematsu", "Akira Hayashi. A reinforcement learning algorithm in partially observable environments using short-term memory"], "venue": "pages 1059\u2013 1065,", "citeRegEx": "SH99", "shortCiteRegEx": null, "year": 1999}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "ICML, pages 943\u2013950", "citeRegEx": "Str00", "shortCiteRegEx": null, "year": 2000}, {"title": "CoRR", "author": ["Joel Veness", "Kee Siong Ng", "Marcus Hutter", "David Silver. A Monte Carlo AIXI Approximation"], "venue": "abs/0909.0801,", "citeRegEx": "VNHS09", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D.J. Lizotte", "M.H. Bowling", "D. Schuurmans"], "venue": "ICML, pages 956\u2013963", "citeRegEx": "WLBS05", "shortCiteRegEx": null, "year": 2005}, {"title": "The Context Tree Weighting Method: Basic Properties", "author": ["Frans M.J. Willems", "Yuri M. Shtarkov", "Tjalling J. Tjalkens"], "venue": "IEEE Transactions on Information Theory, 41:653\u2013664,", "citeRegEx": "WST95", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 3, "context": "The AIXI agent [Hut05] is a formal, mathematical solution to the general reinforcement learning problem.", "startOffset": 15, "endOffset": 22}, {"referenceID": 3, "context": "AIXI is shown in [Hut05] to be optimal in the sense that it will rapidly learn an accurate model of the unknown environment and exploit it to maximise its expected future reward.", "startOffset": 17, "endOffset": 24}, {"referenceID": 4, "context": "In particular, we use a generalisation of UCT [KS06] to approximate the expectimax operation, and an agent-specific extension of CTW [WST95], a Bayesian model averaging algorithm for prediction suffix trees, for prediction and learning.", "startOffset": 46, "endOffset": 52}, {"referenceID": 12, "context": "In particular, we use a generalisation of UCT [KS06] to approximate the expectimax operation, and an agent-specific extension of CTW [WST95], a Bayesian model averaging algorithm for prediction suffix trees, for prediction and learning.", "startOffset": 133, "endOffset": 140}, {"referenceID": 3, "context": "Equation 1, called the chronological condition in [Hut05], captures the natural constraint that action an has no effect on observations made before it.", "startOffset": 50, "endOffset": 57}, {"referenceID": 4, "context": "This section introduces \u03c1UCT, a generalisation of the popular UCT algorithm [KS06] that can be used to approximate a finite horizon expectimax operation given an environment model \u03c1.", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "Algorithm 3 describes the UCB [Aue02] policy used to select actions at decision nodes.", "startOffset": 30, "endOffset": 37}, {"referenceID": 12, "context": "Context Tree Weighting (CTW) [WST95] is an efficient and theoretically well-studied binary sequence prediction algorithm that works well in practice.", "startOffset": 29, "endOffset": 36}, {"referenceID": 7, "context": "where y1:t is the binary sequence seen so far, M is a prediction suffix tree [RST96], Pr(M) is the prior probability of M, and the summation is over all prediction suffix trees of bounded depth D.", "startOffset": 77, "endOffset": 84}, {"referenceID": 12, "context": "The following is a straightforward extension of a result due to [WST95].", "startOffset": 64, "endOffset": 71}, {"referenceID": 4, "context": "[KS06] shows that the UCT algorithm is consistent in finite horizon MDPs and derive finite sample bounds on the estimation error due to sampling.", "startOffset": 0, "endOffset": 6}, {"referenceID": 4, "context": "By interpreting histories as Markov states, the general reinforcement learning problem reduces to a finite horizon MDP and the results of [KS06] are now directly applicable.", "startOffset": 138, "endOffset": 144}, {"referenceID": 3, "context": "The next result, adapted from [Hut05], shows that if there is a good model of the (unknown) environment in CD, then Action-Conditional CTW will predict well.", "startOffset": 30, "endOffset": 37}, {"referenceID": 10, "context": "More detail can be found in [VNHS09].", "startOffset": 28, "endOffset": 36}, {"referenceID": 1, "context": "The Biased RockPaperScissor domain is described in [FMWR07], which involves the agent repeatedly playing RockPaperScissor against an exploitable opponent.", "startOffset": 51, "endOffset": 59}, {"referenceID": 2, "context": "Two more challenging domains are included: Kuhn Poker [HSHB05], where the agent plays second against a Nash optimal player and a partially observable version of Pacman described in [VNHS09].", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": "Two more challenging domains are included: Kuhn Poker [HSHB05], where the agent plays second against a Nash optimal player and a partially observable version of Pacman described in [VNHS09].", "startOffset": 181, "endOffset": 189}, {"referenceID": 8, "context": "The BLHT algorithm [SH99] is closely related to our work.", "startOffset": 19, "endOffset": 25}, {"referenceID": 1, "context": "The Active-LZ [FMWR07] algorithm combines a Lempel-Ziv based prediction scheme with dynamic programming for control to produce an agent that is provably asymptotically optimal if the environment is n-Markov.", "startOffset": 14, "endOffset": 22}, {"referenceID": 5, "context": "U-Tree [McC96] is an online agent algorithm that attempts to discover a compact state representation from a raw stream of experience.", "startOffset": 7, "endOffset": 14}, {"referenceID": 5, "context": "Furthermore, the splitting heuristic contains a number of configuration options that can dramatically influence its performance [McC96].", "startOffset": 128, "endOffset": 135}, {"referenceID": 11, "context": "The \u03c1UCT algorithm shares similarities with Bayesian Sparse Sampling [WLBS05]; the key differences are estimating the leaf node values with a rollout function and guiding the search with the UCB policy.", "startOffset": 69, "endOffset": 77}, {"referenceID": 10, "context": "A more comprehensive discussion of related work can be found in [VNHS09].", "startOffset": 64, "endOffset": 72}, {"referenceID": 10, "context": "Some preliminary ideas are explored in [VNHS09].", "startOffset": 39, "endOffset": 47}], "year": 2010, "abstractText": "This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains.", "creator": "LaTeX with hyperref package"}}}