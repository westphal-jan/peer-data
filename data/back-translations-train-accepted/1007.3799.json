{"id": "1007.3799", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2010", "title": "Adapting to the Shifting Intent of Search Queries", "abstract": "Search engines today present results that are often oblivious to abrupt shifts in intent. For example, the query `independence day' usually refers to a US holiday, but the intent of this query abruptly changed during the release of a major film by that name. While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 50% of all search queries. This paper shows that the signals a search engine receives can be used to both determine that a shift in intent has happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions. We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.", "histories": [["v1", "Thu, 22 Jul 2010 04:58:24 GMT  (80kb)", "http://arxiv.org/abs/1007.3799v1", "This is the full version of the paper in NIPS'09"]], "COMMENTS": "This is the full version of the paper in NIPS'09", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["umar syed", "aleksandrs slivkins", "nina mishra"], "accepted": true, "id": "1007.3799"}, "pdf": {"name": "1007.3799.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["usyed@cis.upenn.edu", "slivkins@microsoft.com", "ninam@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 100 7,37 99v1 [cs.LG] 2 2Ju l 2"}, {"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2 Related Work", "text": "While there has been a considerable amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to be learned, not one that shifts over time. Online bandit algorithms (see [7] for background) have been considered in the context of the ranking. For example, Radlinski et al [20] showed how to put together multiple instances of a bandit algorithm to produce a ranking of search results. Pandey et al [19] showed that bandit algorithms can be effective in serving search engine users, and these approaches also assume a stationary inference. Although no existing bandit algorithms are specifically designed for our problem, there are two known algorithms that we compare in this paper. The UCB1 algorithm [2] assumes that fixed click probabilities and persist on most logisms (T)."}, {"heading": "3 Problem Formulation and Preliminaries", "text": "\",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\", \"\", \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \":\" < \"> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >\" > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" > \">\" \">\" > \">\" > \"\" > \"\" > \">\" \"\" \"\" \"\" > \">\" \",\" \"\", \"\" \",\" \"\" \"\" \""}, {"heading": "4 Bandit with Classifier", "text": "Our algorithm is called BWC or \"Bandit with Classifier.\" Ideally, we would like to use a bandit algorithm for the non-event setting, like UCB1, and restart it every time there is an event. Since we don't have an oracle to tell if an event has happened, we use a classifier that looks at the current context and makes a binary prediction. As we mentioned in the introduction, we assume that a priori there are no labeled samples to train such a classifier, so we have to generate the labels ourselves. So, the high-level idea is to restart the bandit algorithm every time the classifier predicts an event, and to use subsequent rounds to generate feedback (labeled samples) to train the classifier. So we have a feedback loop between the bandit algorithm and a classifier that delivers an algorithm, in which this one algorithm is easy to predict, so that the latter is 1.say if they are correct."}, {"heading": "4.1 The meta-algorithm", "text": "This year it is more than ever before."}, {"heading": "4.2 Provable guarantees", "text": "We ask ourselves the question of whether we are in a position to embark on a search for another path that may lead us to another path. \"\" We, \"according to the author,\" we. \"\" We. \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" We. \"\" We. \"\" \"\" We. \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"We.\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"as\" \"\" \"\" as \"as\" as \"as\" as \"as\" as \"as\" as \"as\" as \"as\" as \"as\" as \"as"}, {"heading": "5 Safe Classifier", "text": "In this section, we will show how safe classifiers with low FP complexity cannot be improved, as we will see in the classification. (Let's remember that a classifier is safe if (provided there are only correctly described examples) there can never be a false negative definition of FP complexity motivated by the specification of the BWC algorithm, essentially assuming that all the examples described are false positives. We will first describe a generic classification called SafeCl, which is safe for any concept of FP complexity that applies a certain property of F complexity. In the case that the concept of the class is all dimensional axial parallel hyper-rectangles, we will show that this limit is proportional to d /.And in the case that the concept class is all d-dimensional hyperplanes with margin classes, we will show that this limit cannot be improved exponentially in dependence."}, {"heading": "5.1 Axis-parallel rectangles with margin \u03b4", "text": "A very simple concept is an axis-parallel hyperrectangle. This type of concept can be used uniquely to test whether one of several features is outside its \"normal\" range. This is a particularly well-suited concept class for predicting events that can affect a search engine query, since these events are typically preceded by a major change in a statistic related to the query, such as the volume or the abort rate. Attach the dimension d, and let X Rd be the d-dimensional L standard unit sphere around the origin. A d rectangle in Rd is the cross-product of d non-empty intervals in R. Described d > 0 and a d rectangle R, define a function fR, \u03b4: X \u2212 1, + 1, null} as follows: fR, (x) equal + 1, if L rectangle in Rd."}, {"heading": "5.2 Hyperplanes with margin \u03b4", "text": "We are perhaps the most widespread concept in classification problems. Fix the dimension d, and let X'Rd be the d-dimensional L2-norm unit. (So) It is not as if there would be a function. (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So.). (So. (So.). (So.). (So.). (So.). (So. (So.). (So.). (So.)."}, {"heading": "6 Testable Bandit Algorithms", "text": "In this section we will consider the stochastic n-armed bandit problem. We are looking for (L) test stable algorithms with little regret. TheL will have to be sufficiently large, on the order of what we expect. (5) Unfortunately, UCB1 does not immediately provide a way to best define the t-th round (G +, G \u2212) to guarantee stability (L) -testability. A simple fix is to choose an arm in each of the first L-round to make the best guess, in a simple way, and then we run UCB1. But in the first L-rounds we will regret these algorithms (L), which is very suboptimal compared to R0 (L)."}, {"heading": "7 Upper and Lower Bounds", "text": "In this case, all two events are at least 2L-rounds apart, where L = 2 logT). Considera the BWC algorithms with the parameters L and components classifier and bandit as in section 5 and section 6. Then the regret of the BWC isRBWC (T) n logT n). (logT) n While the linear dependence on n appears large in this range, we note that without additional assumptions must be linear in n. (logT)."}, {"heading": "8 Experiments", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "9 Future Work", "text": "The most immediate open question is whether we could train the classifier more quickly. One idea is to use a more efficient classifier, especially if we can relax the requirement of \"security\" and somehow recover from false negatives. Another idea is to generate labeled samples not only for positive but also for negative predictions, swapping the regret of additional exploration for the benefits of generating additional labeled samples. Finally, it would be desirable to supplement the existing provable worst-case guarantees with stronger ones for contexts in which the contexts are scanned from a \"benevolent\" district. Theoretically, the main drawback of our approach is that we assume the existence of a \"perfect oracle\" - a deterministic Boolean function in contexts that correctly predicts whether a time event has occurred in the current round. It is desirable to extend our results to scenarios that are likely or only approximate contexts."}, {"heading": "A Details for the proof of Theorem 6(ii)", "text": "We can define context sequences {x0t} and {x1t},., {xNt} with the following properties: (1) each sequence {xit} when paired with a problem Ii, defines an eventful bandit problem that is consistent with all our assumptions, and (2) the sequences {x0t} and {xii} match through the first i phase. We know that this sequence exists through the definition of dF. Also, we assume that there is an \"always negative\" context x \u2212 such that f (x \u2212) = 1 for all f-F (this assumption is not necessary, but convenient)."}], "references": [{"title": "Online models for content optimization", "author": ["Deepak Agarwal", "Bee-Chung Chen", "Pradheep Elango", "Nitin Motgi", "Seung-Taek Park", "Raghu Ramakrishnan", "Scott Roy", "Joe Zachariah"], "venue": "In 22nd Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The anatomy of a large-scale hypertextual Web search engine", "author": ["Sergey Brin", "Lawrence Page"], "venue": "Computer Networks and ISDN Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Learning to rank using gradient descent", "author": ["Christopher J.C. Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Gregory N. Hullender"], "venue": "In 22nd Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "In 24th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Learning to order things", "author": ["William W. Cohen", "Robert E. Schapire", "Yoram Singer"], "venue": "J. of Artificial Intelligence Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Integration of news content into web results", "author": ["Fernando Diaz"], "venue": "In 2nd Intl. Conf. on Web Search and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Search engine users", "author": ["D. Fallows"], "venue": "Pew Internet and American Life Project,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Yoav Freund", "Raj Iyer", "Robert E. Schapire", "Yoram Singer"], "venue": "J. of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Online Learning with Prior Knowledge", "author": ["Elad Hazan", "Nimrod Megiddo"], "venue": "In 20th Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims"], "venue": "In 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "In 25th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Bursty and hierarchical structure in streams", "author": ["Jon M. Kleinberg"], "venue": "In 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In 21st Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Bandits for Taxonomies: A Model-based Approach", "author": ["Sandeep Pandey", "Deepak Agarwal", "Deepayan Chakrabarti", "Vanja Josifovski"], "venue": "In SIAM Intl. Conf. on Data Mining (SDM),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Multi-armed Bandit Problems with Dependent Arms", "author": ["Sandeep Pandey", "Deepayan Chakrabarti", "Deepak Agarwal"], "venue": "In 24th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["Filip Radlinski", "Robert Kleinberg", "Thorsten Joachims"], "venue": "In 25th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Convex Analysis", "author": ["R. Tyrrell Rockafellar"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1970}, {"title": "Bandit problems with side observations", "author": ["Chih-Chun Wang", "Sanjeev R. Kulkarni", "H. Vincent Poor"], "venue": "IEEE Trans. on Automatic Control,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "There are studies that suggest that queries likely to be intent-shifting \u2014 such as pop culture, news events, trends, and seasonal topics queries \u2014 constitute roughly half of the search queries that a search engine receives [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 3, "context": "Since traditional ranking features like PageRank [4] change slowly over time, and may be misleading if user intent has shifted very recently, we want to use just the observed click behavior of users to decide which search results to display.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Our bandit subroutine \u2014 a novel version of algorithm UCB1 from [2] which additionally provides high-confidence estimates on the suboptimality of arms \u2014 may be of independent interest.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "The experiments show that if there are no events then the well-studied UCB1 algorithm [2] performs the best.", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 4, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 12, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 7, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 6, "context": "Online bandit algorithms (see [7] for background) have been considered in the context of ranking.", "startOffset": 30, "endOffset": 33}, {"referenceID": 19, "context": "For instance, Radlinski et al [20] showed how to compose several instantiations of a bandit algorithm to produce a ranked list of search results.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "Pandey et al [19] showed that bandit algorithms can be effective in serving advertisements to search engine users.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "The UCB1 algorithm [2] assumes fixed click probabilities and has regret at most O( n \u2206 logT ).", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "S algorithm [3] assumes", "startOffset": 12, "endOffset": 15}, {"referenceID": 21, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 17, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 11, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 16, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 13, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 8, "context": "Diaz [9] used a regularized logistic model to determine when to surface news results for a query.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Agarwal et al [1] used several models, including a dynamic linear growth curve model.", "startOffset": 14, "endOffset": 17}, {"referenceID": 14, "context": "For example, Kleinberg [15] describes a state-based model for inferring stages of burstiness.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "Techniques from [20] may be adopted to find a good list of results.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "For bandit, we build on a standard algorithm UCB1 [2]; as it turns out, making it (L, \u01ebS)-testable requires a significantly extended analysis.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "Therefore, by the intermediate value theorem, there exists x \u2208 X and \u03b8 \u2208 [0, 1] such that x = (1\u2212 \u03b8)xt + \u03b8x and w \u00b7 (x + u) = 0.", "startOffset": 73, "endOffset": 79}, {"referenceID": 20, "context": "Proof of (ii): We will use the well-known separating hyperplane theorem [21]: If nonempty convex sets X,Y \u2208 R do not intersect, then there exist a \u2208 R \\ {0} and b \u2208 R such that a \u00b7 x \u2265 b for all x \u2208 X and a \u00b7 y \u2264 b for all y \u2208 Y (3)", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "A natural candidate would be algorithm UCB1 from [2] which does very well on event-free regret:", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "We will use a slightly modified algorithm UCB1 from [2], with a significantly extended analysis.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "The original regret analysis of UCB1 in [2] carries over to UCB1(\u03b1, t0) so as to guarantee event-free regret (5); we omit the details.", "startOffset": 40, "endOffset": 43}, {"referenceID": 15, "context": "Then in each phase i \u2265 1 the algorithm (essentially) needs to solve a fresh instance of the stochastic bandit problem on n\u22121 arms with time horizon T/k and payoffs 1 2 \u00b1 \u01eb, which implies regret \u03a9(n\u01eb ) log(T/k) [16, 2].", "startOffset": 210, "endOffset": 217}, {"referenceID": 1, "context": "Then in each phase i \u2265 1 the algorithm (essentially) needs to solve a fresh instance of the stochastic bandit problem on n\u22121 arms with time horizon T/k and payoffs 1 2 \u00b1 \u01eb, which implies regret \u03a9(n\u01eb ) log(T/k) [16, 2].", "startOffset": 210, "endOffset": 217}], "year": 2010, "abstractText": "Search engines today present results that are often oblivious to abrupt shifts in intent. For example, the query \u2018independence day\u2019 usually refers to a US holiday, but the intent of this query abruptly changed during the release of a major film by that name. While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 50% of all search queries. This paper shows that the signals a search engine receives can be used to both determine that a shift in intent has happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions. We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.", "creator": "LaTeX with hyperref package"}}}