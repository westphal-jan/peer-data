{"id": "1008.1986", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Aug-2010", "title": "For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia", "abstract": "We report on work in progress on extracting lexical simplifications (e.g., \"collaborate\" -&gt; \"work together\"), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.", "histories": [["v1", "Wed, 11 Aug 2010 20:01:59 GMT  (15kb)", "http://arxiv.org/abs/1008.1986v1", "4 pp; data available atthis http URL"]], "COMMENTS": "4 pp; data available atthis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mark yatskar", "bo pang", "cristian danescu-niculescu-mizil", "lillian lee"], "accepted": true, "id": "1008.1986"}, "pdf": {"name": "1008.1986.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mark Yatskar", "Bo Pang", "Cristian Danescu-Niculescu-Mizil"], "emails": ["my89@cornell.edu,", "bopang@yahoo-inc.com,", "cristian@cs.cornell.edu,", "llee@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar Xiv: 100 8.19 86v1 [cs.CL] 1 1A ug2 010Released by: NAACL 2010 (short essay)"}, {"heading": "1 Introduction", "text": "In fact, different contexts require different styles. Here we are looking at an important dimension of style, namely simplicity. Systems that can rewrite text into simpler versions, however, promise to make information accessible to a wider audience, such as non-native speakers, children, laymen, etc. An important effort to produce such a text is the Simple English Wikipedia (henceforth SimpleEW) 1, a kind of offshoot of the well-known English Wikipedia (henceforth ComplexEW), in which human editors force the simplicity of language by rewriting pseudonyms. The crux of our proposal is to learn lexical simplifications from SimpleEW edit stories, whereby the efforts of 18K pseudonyme pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms pseudonyms"}, {"heading": "2 Method", "text": "The main difficulty in working with these modifications is that they include not only simplifications, but also edits that serve other functions, such as the removal of spam or the correction of grammar or factual content (\"fixes\").2An exception [5] changes the verb time and replaces pronouns. Other lexicon-level work focuses on medical text [4, 2] or frequently uses filtered WordNet synonyms [3].3A type of lexical corruption, such as \"acorn\" \u2192 \"egg grain.\""}, {"heading": "2.1 Edit model", "text": "We assume that there are at least four possible edit operations: (1), (2), (3), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5, 5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5, (5), (5), (5), (5"}, {"heading": "2.2 Metadata-based methods", "text": "In fact, it is the case that most of them are able to hide, and that they are able to hide, able to hide. (Se) D \"n, in which they are able to be able to be able to be able to be able to be able to be able to be able to be able. (Se) n\" D \"n, in which they are able to be able to be able to be able to be able to be able to be able.\" (Se \"n\" n \"n\" n, in which they are able to be able. (Se \"n\" n \"i\" n \"n\" n \"n\" n \"n\" i \"n\" n \"n\" i \"n\" n \"n\" n \"n\" n \"n\" n. \"n\" n. \""}, {"heading": "3.1 Data", "text": "We obtained the revision histories of both SimpleEW (snapshot of November 2009) and ComplexEW (snapshot of January 2008). In total, over 1.5 million revisions were processed for 81,733 SimpleEW articles (only 30% were text changes). For ComplexEW, we processed over 16 million revisions for 1,0407 articles. Extracted lexical editing instances. For each article, we constructed sentences in each pair of adjacent versions using tf-idf scores similar to those of Carnations and Shieber [6] (which led to satisfactory results, as revisions tended to be small changes). From the customized sentence pairs, we obtained the above lexical editing instances A \u2192 a. Since the focus of our study was not on word alignment, we used a simple method that identified the longest different segments (based on word boundaries) between each sentence, except that we did not include the extraction more than one sentence (if we did not match)."}, {"heading": "3.2 Comparison points", "text": "FREQUENT provides the most common lexical editing instances extracted from SimpleEW. Spencer Kelly, the SimpleEW editor, has compiled a list of simple words and simplifications using a combination of dictionaries and manual effort. He provides a list of 17,900 simple words - words that need no further simplification - and a list of 2,000 transformation pairs. We did not use Spencer's phrase as the gold standard because many transformations we thought were reasonable were not on his list. Instead, we measured our match with the list of transformations he compiled (SPLIST). 8Results at http: / / www.cs.cornell.edu / home / llee / data / simple 9http: / / www.spencerwaterbed.com / soft / about.html"}, {"heading": "3.3 Preliminary results", "text": "The top 100 pairs from each system (editing model10 and SIMPL and the two baselines) plus 100 randomly selected pairs from SPLIST were shuffled and presented in random order to three native English speakers and three non-native English speakers (all non-authors); each pair was presented in random orientation (i.e. either as A \u2192 a or as A), and the labels included \"simple,\" \"more complex,\" \"equal,\" \"unrelated\" and \"?\" (\"difficult to judge\"); the first two labels correspond to simplifications for the orientations A \u2192 a and A (each as A. Summarizing the 5 labels into \"simplification,\" \"no simplification\" and \"?\" gives a reasonable match among the 3 native speakers (we = 0.69; 75.5% of the time that all three agreed on the same label)."}, {"heading": "3.4 Future work", "text": "It would be interesting to use our proposed estimates as an initialization for an EM-style iterative re-evaluation; another idea would be to estimate simplification priorities based on a model of inherent lexical complexity; some possible starting points are the number of syllables (used in various readability formulas) or word length. Acknowledgements We would first like to thank Ainur Yessenalina for initial research and helpful comments; we are also R. Barzilay, T. Bruce, C. Callison-Burch, J. Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, & the reviewers for helpful comments; W. Arms and L. Walle for accessing the Cornell Hadoop cluster; J. Cantwell, C. Callison-Burch, J. Cantwell, M. Hwa & A. Owens for annotation software; M. Ulinski for preliminary research; J. Cantwell, J. Yotar, J. verstein, 10J."}], "references": [{"title": "Automatic induction of rules for text simplification", "author": ["R. Chandrasekar", "B. Srinivas"], "venue": "Knowledge-Based Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora", "author": ["L. Del\u00e9ger", "P. Zweigenbaum"], "venue": "Workshop on Building and Using Comparable Corpora,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The use of a psycholinguistic database in the simplification of text for aphasic readers", "author": ["S. Devlin", "J. Tait"], "venue": "In Linguistic Databases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Mining a lexicon of technical terms and lay equivalents", "author": ["N. Elhadad", "K. Sutaria"], "venue": "Workshop on BioNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Text simplification for information-seeking applications", "author": ["B. Beigman Klebanov", "K. Knight", "D. Marcu"], "venue": "OTM Conferences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Towards robust context-sensitive sentence alignment for monolingual corpora", "author": ["R. Nelken", "S.M. Shieber"], "venue": "EACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Mining Wikipedia\u2019s article revision history for training computational linguistics algorithms", "author": ["R. Nelken", "E. Yamangil"], "venue": "WikiAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Extracting lexical reference rules from Wikipedia", "author": ["E. Shnarch", "L. Barak", "I. Dagan"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Syntactic simplification for improving content selection in multidocument summarization", "author": ["A. Siddharthan", "A. Nenkova", "K. McKeown"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Sentence simplification for semantic role labeling", "author": ["D. Vickrey", "D. Koller"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "org formation rules [1, 9, 10].", "startOffset": 20, "endOffset": 30}, {"referenceID": 8, "context": "org formation rules [1, 9, 10].", "startOffset": 20, "endOffset": 30}, {"referenceID": 9, "context": "org formation rules [1, 9, 10].", "startOffset": 20, "endOffset": 30}, {"referenceID": 6, "context": "g, \u201ceggcorns\u201d3 [7] and entailments [8].", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "g, \u201ceggcorns\u201d3 [7] and entailments [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "One exception [5] changes verb tense and replaces pronouns.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3].", "startOffset": 49, "endOffset": 55}, {"referenceID": 1, "context": "Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3].", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "For each article, we aligned sentences in each pair of adjacent versions using tf-idf scores in a way similar to Nelken and Shieber [6] (this produced satisfying results because revisions tended to represent small changes).", "startOffset": 132, "endOffset": 135}], "year": 2010, "abstractText": "We report on work in progress on extracting lexical simplifications (e.g., \u201ccollaborate\u201d \u2192 \u201cwork together\u201d), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list. Published at: NAACL 2010 (short paper)", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}