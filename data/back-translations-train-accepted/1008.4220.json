{"id": "1008.4220", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2010", "title": "Structured sparsity-inducing norms through submodular functions", "abstract": "Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nonincreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lovasz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.", "histories": [["v1", "Wed, 25 Aug 2010 07:28:08 GMT  (123kb)", "https://arxiv.org/abs/1008.4220v1", null], ["v2", "Wed, 22 Sep 2010 03:11:25 GMT  (123kb)", "http://arxiv.org/abs/1008.4220v2", null], ["v3", "Fri, 12 Nov 2010 14:51:23 GMT  (123kb)", "http://arxiv.org/abs/1008.4220v3", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["francis r bach"], "accepted": true, "id": "1008.4220"}, "pdf": {"name": "1008.4220.pdf", "metadata": {"source": "CRF", "title": "Structured sparsity-inducing norms through submodular functions", "authors": ["Francis Bach"], "emails": ["francis.bach@ens.fr"], "sections": [{"heading": null, "text": "ar Xiv: 100 8.42 20v3 [cs.LG]"}, {"heading": "1 Introduction", "text": "It is indeed the case that most of us are able to outdo ourselves. (...) Indeed, it is the case that most of us are able to outdo ourselves. (...) Most of us have not been able to outdo ourselves. (...) Most of us have not been able to outdo ourselves. (...) Most of us have not been able to outdo ourselves. (...) Most of us have not been able to outdo ourselves. (...) \"(...)\" (...) \"Most of us have done it.\" (...) \"(...)\" (...) \"(...).\" (...) \"(We have done it.\" (...). \"(...)\" (... \"(...).\" (... \"(...).\" (...) \"(...\" (...). \"(...)\" (... \"().\" (... \"().\" (We have done it. \"(...).\" (... \"().\" () \"(...\" (). \"(\" We have done it. \"(...).\" () \"(...\" (). \"()\" We have done it. \"(...\" (). \"().\" (... \"We have.\" (). \"().\" We have. \"().\" (... \"().\" We have done. \"().\" (). \"(). (\" We have done. \"().\" (). (We have done. (... \"(). (We have done. (). (). (We have. (). (). (). (We have done. (...\" (). (). (We have done. (). (). (). (We have done. (). (). (We have. (). (We have done. (. (). (We have done. (). (). (. (We have done. (). (). (. (We have done. (). (). (. (). (We have done. (). (). (). (. (. (). (It. (We have. (). (We have done. (). (). (). ("}, {"heading": "2 Review of submodular function theory", "text": "In this paper, we consider a non-decreasing submodular function F (A) defines (A) > function 2V of V = {1,.., p}, i.e., so that: A, B, F (A) + F (B) > F (A), D (A), (A, B), (B, V), A (B), B (A), B (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A), D (A, D), D (A, D, D, D, D (A, D, D, D) (A, D, D, A (A), D (A, D), D (A, D) (A, D), D (A, D), D (A, D), D (A, D), D (A, D), D (A, D), D (A, D), D (A, D, D (A, A, D), D (A, D, D, D) (A, D, D (A, A, D), D (A, A (A, A, A, A, A), D (A (A, A, A), D (A (A, A, A, A, A) (A (A, A, A, A) (A (A, A, A, A (A, A, A, A, A) (A (A, A, A) (A, A, A (A, A) (A (A, A, A) (A, A, A (A, A, A, A, A) (A, A (A, D) (A, A (A, A, A, D) (A (A, A, D) (A, A (A, A, A, D) (A, A (A, A, A) (A, D) (A, A (A, D) (A, A (A, A, A, A (A"}, {"heading": "3 Definition and properties of structured norms", "text": "We have the following properties (see evidence in the appendix) which show that we are actually defining a standard and that it is the desired convex hull: Proposition 1 (convex hull, dual standard) Suppose that the convex hull of the function g: w 7 (soup (w)) is submodular for all singlets, not decreasing and strictly positive. Definitive: Definition: w 7 (| w |). Then: (i) is a norm on Rp, (ii) is the convex hull of the function g: w 7 (soup (w)))) for the unit ball, (iii) the dual norm (see, [18]) for the proxa sphere."}, {"heading": "4 Examples of nondecreasing submodular functions", "text": "We look at three main types of submodular functions with potential applications for regularizing supervised learning. Some existing standards are presented as examples of our framework (Section 4.1, Section 4.3), while other new standards are designed from specific submodular functions (Section 4.2). Further examples of submodular functions, especially with regard to matroids and entropies, can be found in [12, 10, 11] and could also lead to interesting new standards. Quantity lids, which are frequent examples of submodular functions, are subsets of functions defined in Section 4.1 (see e.g. [9])."}, {"heading": "4.1 Norms defined with non-overlapping or overlapping groups", "text": "We consider grouped norms with potentially overlapping groups [1, 2], i.e., we consider them insufficient since they have a non-negative set function (with potentially d (G) = 0 if they are not included in the norm. It is a norm as soon as it is able to comply with the rules, and it corresponds to the non-decreasing submodular function F (A) = 1 (G).1).2 In the case in which they are superseded by the norms, it has shown that the number of norms allowed, submodular functions F (A) = 1 (A).2) The norms of the norms of the norms of the norms of the norms of the norms of the norms of the norms of the norms of the norms of the norms, the norms of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms of the norms of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms of the norms of the norms of the norms of the norms, of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms of the norms of the norms, of the norms of the norms of the norms, of the norms of the norms of the norms, of the norms of the norms, of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms of the norms of the norms of the norms, of the norms, of the norms of the norms of the norms, of the norms of the norms of the norms of the norms of the norms of the"}, {"heading": "4.2 Spectral functions of submatrices", "text": "In view of a positive semidefinitive matrix Q-Rp \u00b7 p and a real weighted function h of R + \u2192 R, tr [h (Q)] can unfortunately be defined as: p i = 1 h (\u03bbi), where \u03bb1,.., \u03bbp are the (non-negative) eigenvalues of Q [21]. Wecan therefore defines the set function F (A) = trh (QAA) for A [V). The functions h (\u03bb) = log (\u03bb + t) for t > 0 lead to submodular functions, since they correspond to the entropies of the Gaussian random variables (see e.g. [12, 9])."}, {"heading": "4.3 Functions of cardinality", "text": "For F (A) = h (| A |), where h is not decreasing, so that h (0) = 0 and concave, then starting from Equation (1), is defined from the ranking statistics of | w | Rp +, i.e., if | w (1) | > | w (2) | > \u00b7 \u00b7 \u00b7 > | w (p) |, then vice versa (w) = \u2211 p = 1 [h (k) \u2212 h (k \u2212 1)] | w (k) |. This includes the sum of q largest elements and could lead to interesting new norms for unstructured variable selection, but this is not pursued here. However, the algorithms and analyses presented in sections 5 and 6 apply to this case."}, {"heading": "5 Convex analysis and optimization", "text": "In this section, we provide algorithmic tools that relate to optimization problems due to regularization by our novel, party-inducing standards. Note that these standards are poledral norms that exhibit an exponential number of wells or areas, and that regular linear programming toolboxes cannot be used. This makes it possible to use subgradient descendents as shown in Figure 4, slow convergence compared to proximal methods. Proximal operators can easily acquire a subgradient in polynomial time as one of the maximizers s's, allowing subgradient descendents as shown in Figure 4, slow convergence compared to proximal operators."}, {"heading": "6 Sparsity-inducing properties", "text": "In this section, we consider a fixed design matrix X-Rn-p and y-Rn as a vector of random reactions. Considering \u03bb > 0, we define w-0 as a minimizer of the cost of the regulated smallest squares: minw-Rp-1 2n-y-Xw-22 + 0 (w). (4) 1The greedy algorithm for finding extreme points of the submodular polyhedron should not be confused with the greedy algorithm (e.g. forward selection) that we use in Section 7.We examine the parity-inducing properties of Equation (6) solutions, i.e. we determine in Section 6.2 which patterns are permissible and in Section 6.3 which sufficient conditions lead to a correct assessment. Like the recent analysis of parity-inducing standards [27], the analysis provided in this section is largely based on the decompatibility properties of our standard."}, {"heading": "6.1 Decomposability", "text": "For a subset J of V, we denote by FJ: 2 J \u2192 R the restriction of F to J, defined by FJ (A) = F (A), and by F J: 2Jc \u2192 R the contraction of F by J, defined by F J (A) = F (A) = F (A). These two functions are submodular and not decreasing once F is (see e.g. [12]). We denote by \"J\" the norm for \"R J\" defined by the submodular function FJ, and \"J\" the pseudonym defined by \"F J\" (as shown in Proposition 4). Note that \"Jc\" (a norm for \"Jc\") is generally different from \"J.\" Moreover, \"J\" (wJ) is actually equal to \"J,\" where w \"J\" and w \"J.\""}, {"heading": "6.2 Sparsity patterns", "text": "In this section, we do not make assumptions about the correct specification of the linear model. We show that probability one can only achieve stable support quantities (see evidence in the appendix). For simplicity, we assume the invertibility of X X, which forbids the high-dimensional situation p > n that we are considering in Section 6.3, but we could consider assumptions similar to those used in [2]. Proposal 5 (Stable thriftiness Patterns) assumes that y-Rn has an absolutely continuous density in relation to the Lebesgue measure and that X X is invertable. Then, the minimizer w-X of Equation (6) is unique and, with probability one, its support Supp (w-V) is a stable proposition."}, {"heading": "6.3 High-dimensional inference", "text": "We now assume that the linear model is well specified and extends the results of [28] for sufficient support conditions and [27] for assessment consistency. As seen in Proposition 4, the norm \u00b2 is degradable and we use this property extensively in this section. We call it \"normal\" (J) = \"normal\" (B \u00b2 J) -F (J) -F (B) \u2212 F (B); after submodularity and monotonicity of \"normal\" (J) is always between \"normal\" and \"normal\" (J). Once J is stable, it is strictly positive (for the \"normal\" norm \"),\" normal \"(J) = 1\" stable. \"In addition, we point to\" normal \"normal.\" \"\""}, {"heading": "7 Experiments", "text": "We look at the regularized least-squares problem of Eq. (6), with data generated as follows: p, n, k, the design matrix X-Rn \u00b7 p is a matrix of the i.i.d. Gaussian components normalized to unit 2-standard columns. A set J of cardinality k is randomly selected and the weights w-Rn J are sampled by a standard multivariate Gaussian distribution and w-Jc = 0. We then take y = Xw-Rn-2, which is based on a standard Gaussian vector (which corresponds to a unit-signal-noise ratio).Proximal methods vs. subgradient lineage."}, {"heading": "8 Conclusions", "text": "We have presented a set of common algorithms and theoretical results, as well as simulations of synthetic examples illustrating the good behavior of these standards. There are several possibilities worth investigating: First, we could follow current practice in sparse methods, for example, by considering related adapted concave penalties to improve parsimonious norms, or by broadening some of the concepts of matrix norms, with potential applications in matrix factorization or multi-task learning (see e.g. [29] for the application of submodular functions to dictionary learning); second, links between submodularity and conciseness could be further explored, especially by considering submodular relaxations of other combinatory functions or studying connections with other polyedral norms such as total variation, which are known to be similarly associated with syetric functions."}, {"heading": "A Properties of the norm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Proposition 1", "text": "(i) positive homogen-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s-s"}, {"heading": "A.2 Proof of Proposition 2", "text": "We have seen in Section 2 that for a set of stable inseparable sets (A-T) {x (A) = F (A)} one side of P is (and these sets are the only ones where this happens) and we arrive at the desired result by considering potentially different characters."}, {"heading": "B Convex optimization results", "text": "Note that the exact subdifferential for the non-zero components of w is quite complicated if w has components of the same order of magnitude. If this is not the case, i.e., | wj1 | > \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 wjk | > 0, where k = | J | is, then the subdifferential for components of the same order of magnitude is reduced to a dot s, so that sjk = F ({j1,.., jk}) \u2212 F ({j1,.., jk \u2212 1}). For further details on the subdifferential for components of other magnitude, see [12].Lemma 1 (decomposition of subdifferential) Let w-Rp, with support J = Supp (w) and with H equal to the smallest stable set that contains J. The subdifferential components (w) for w, can then be broken down as follows: RV = RJ \u00b7 J (J) J (J), J (J), J (J), J (J), J (J), J (J), J (J), J (J, J, J, J, J, J, J, J (J, J, J, J, J, J, J, J, J, J, J, J, J, J (J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J, J)."}, {"heading": "B.1 Proof of Proposition 3", "text": "Following [6], without loss of universality, we assume that z has non-negative components. We have convex duality (applicable here due to Slater's condition): min w-Rp1 2-Rp1 2-Rp1 22-0-0 (w) = min w-Rp max-0 (s) 61 1-2-0-0 (s) 22 + 0-0 (s) 22 + 0-0 (s) 611 2-0 (s) 22-0 (s) 2-0-0 (s) 22, where the (unique) optimal w-value is defined by w-0-0-1-0 (s), which is equivalent to | s-P. Since z has non-negative components, the minimum is limited to | s-0."}, {"heading": "C Sparse estimation", "text": "In this section, we consider a construction X-Rn \u00b7 p as a solid construction and y-Rn as a series of random reactions. Given \u03bb > 0, we define w-squares as minimizing the regularized costs for the smallest squares: min w-Rp12n-y-Xw-22 + f-squares (w). (6)"}, {"heading": "C.1 Proof of Proposition 4", "text": "(i) for s-Rp +, if the components of J-J, s-J (B-J), 6 F-Jc (B-J) and C-Jc, s-J (C-J) -F (J), then A-V, s-J (A-J) + s (A-Jc) 6 F (A-J) + F (A-J) -F (J) 6 F (A) by submodularity. This implies that the desired result is obtained by taking into account the representation of the Lova-sz extension in Equation (1) and the fact that we have just proven that P contains the product of the two submodular polyhedra associated with F-J and FJ. (ii) This results directly from the expression of the Lova-sz extension in Equation (1). In fact, the order within J and those within Jc does not interact."}, {"heading": "C.2 Proof of Proposition 5", "text": "Let Q = 1nX X \u2212 Rp \u00b7 p and r = 1nX J \u2212 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJJ = 1 QJ"}, {"heading": "C.3 Proof of Proposition 6", "text": "Leave q = 1nX-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-yy-y-yy-yy-yy-yy-y-yy-yy-yy-yy-y-y-y-yy-y-y-yy-yy-yy-yy-yy-y-yy-yy-y-y-yy-yy-"}, {"heading": "C.4 Proof of Proposition 7", "text": "As for the proof of proposal 6, we also have for the proof of proposal 6: 1 (x) > J (xJ) + 1: 2 (xJ) + 1: 2 (xJc) + 1: 2 (xJc) > 1: 2 (x). Therefore, if we assume that the degradation property of the standard is used, we follow the proof from [33] by using the degradation property of the standard. We have, according to the optimality of the proposal: 1 2: 1 2 (xJ) + 1: 2 (xJc) + 1: 6 (w) + 1: 6 (w) + 1: 6 (w) + 1: 6 (w) + 1: 6 (w) + 1: 6 (w)."}, {"heading": "C.5 Proof of Proposition 8", "text": "The maximum can be taken from the number of extreme points of the unit ball, which leads to the desired result given in sentence 2."}], "references": [{"title": "Grouped and hierarchical model selection through composite absolute penalties", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics, 37(6A):3468\u20133497", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["R. Jenatton", "J.Y. Audibert", "F. Bach"], "venue": "Technical report, arXiv:0904.3523", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D. Metaxas"], "venue": "Proc. ICML", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Group Lasso with overlaps and graph Lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "Proc. ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Tree-guided group Lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E. Xing"], "venue": "Proc. ICML", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Proc. ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Network flow algorithms for structured sparsity", "author": ["J. Mairal", "R. Jenatton", "G. Obozinski", "F. Bach"], "venue": "Adv. NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Signal reconstruction from noisy random projections", "author": ["J. Haupt", "R. Nowak"], "venue": "IEEE Transactions on Information Theory, 52(9):4036\u20134048", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex analysis and optimization with submodular functions: a tutorial", "author": ["F Bach"], "venue": "Technical Report 00527714, HAL", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "Proc. UAI", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Submodularity cuts and applications", "author": ["Y. Kawahara", "K. Nagano", "K. Tsuda", "J.A. Bilmes"], "venue": "Adv. NIPS", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Elsevier", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Submodular functions", "author": ["J. Edmonds"], "venue": "matroids, and certain polyhedra. In Combinatorial optimization - Eureka, you shrink!, pages 11\u201326. Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Joint support recovery under high-dimensional scaling: Benefits and perils of l1-l\u221e-regularization", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Adv. NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical programming: the state of the art, Bonn, pages 235\u2013257", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1982}, {"title": "Theory of capacities", "author": ["G. Choquet"], "venue": "Ann. Inst. Fourier, 5:131\u2013295", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1954}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematical Programming, 118(2):237\u2013251", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Structured sparse principal component analysis", "author": ["R. Jenatton", "G. Obozinski", "F. Bach"], "venue": "Proc. AISTATS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity and smoothness via the fused Lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "J. Roy. Stat. Soc. B, 67(1):91\u2013108", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Matrix analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge Univ. Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Concavity of certain maps on positive definite matrices and applications to hadamard products", "author": ["T. Ando"], "venue": "Linear Algebra and its Applications, 26:203\u2013241", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1979}, {"title": "Some comments on  Cp", "author": ["C.L. Mallows"], "venue": "Technometrics, 15(4):661\u2013675", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1973}, {"title": "Sparse estimation using general likelihoods and non-factorial priors", "author": ["D. Wipf", "S. Nagarajan"], "venue": "Adv. NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "On total variation minimization and surface evolution using parametric maximum flows", "author": ["A. Chambolle", "J. Darbon"], "venue": "International Journal of Computer Vision, 84(3):288\u2013307", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Adv. NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research, 7:2541\u20132563", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Submodular dictionary selection for sparse representation", "author": ["A. Krause", "V. Cevher"], "venue": "Proc. ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Piecewise linear regularized solution paths", "author": ["S. Rosset", "J. Zhu"], "venue": "Ann. Statist., 35(3):1012\u2013 1030", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P. Bickel", "Y. Ritov", "A. Tsybakov"], "venue": "Annals of Statistics, 37(4):1705\u20131732", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 1, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 5, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": ", [1, 2, 3, 4, 5, 6, 7] and Section 4).", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": "For example, in [4], structured sparsity is used to encode prior knowledge regarding network relationship between genes, while in [6], it is used as an alternative to structured nonparametric Bayesian process based priors for topic models.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "For example, in [4], structured sparsity is used to encode prior knowledge regarding network relationship between genes, while in [6], it is used as an alternative to structured nonparametric Bayesian process based priors for topic models.", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 1, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 3, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 5, "context": "Most of the work based on convex optimization and the design of dedicated sparsity-inducing norms has focused mainly on the specific allowed set of sparsity patterns [1, 2, 4, 6]: if w \u2208 R denotes the predictor we aim to estimate, and Supp(w) denotes its support, then these norms are designed so that", "startOffset": 166, "endOffset": 178}, {"referenceID": 7, "context": "In this paper, we instead follow the approach of [8, 3] and consider specific penalty functions F (Supp(w)) of the support set, which go beyond the cardinality function, but are not limited or designed to only forbid certain sparsity patterns.", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "In this paper, we instead follow the approach of [8, 3] and consider specific penalty functions F (Supp(w)) of the support set, which go beyond the cardinality function, but are not limited or designed to only forbid certain sparsity patterns.", "startOffset": 49, "endOffset": 55}, {"referenceID": 7, "context": ", forward selection) to the problem are considered in [8, 3], we provide convex relaxations to the function w 7\u2192 F (Supp(w)), which extend the traditional link between the l1-norm and the cardinality function.", "startOffset": 54, "endOffset": 60}, {"referenceID": 2, "context": ", forward selection) to the problem are considered in [8, 3], we provide convex relaxations to the function w 7\u2192 F (Supp(w)), which extend the traditional link between the l1-norm and the cardinality function.", "startOffset": 54, "endOffset": 60}, {"referenceID": 8, "context": "Submodular functions may be seen as the set-function equivalent of convex functions, and exhibit many interesting properties that we review in Section 2\u2014see [9] for a tutorial on submodular analysis and [10, 11] for other applications to machine learning.", "startOffset": 157, "endOffset": 160}, {"referenceID": 9, "context": "Submodular functions may be seen as the set-function equivalent of convex functions, and exhibit many interesting properties that we review in Section 2\u2014see [9] for a tutorial on submodular analysis and [10, 11] for other applications to machine learning.", "startOffset": 203, "endOffset": 211}, {"referenceID": 10, "context": "Submodular functions may be seen as the set-function equivalent of convex functions, and exhibit many interesting properties that we review in Section 2\u2014see [9] for a tutorial on submodular analysis and [10, 11] for other applications to machine learning.", "startOffset": 203, "endOffset": 211}, {"referenceID": 0, "context": "\u2212 By selecting specific submodular functions in Section 4, we recover and give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups [1, 2, 7], and we define new norms, in particular ones that can be used as non-factorial priors for supervised learning (Section 4).", "startOffset": 208, "endOffset": 217}, {"referenceID": 1, "context": "\u2212 By selecting specific submodular functions in Section 4, we recover and give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups [1, 2, 7], and we define new norms, in particular ones that can be used as non-factorial priors for supervised learning (Section 4).", "startOffset": 208, "endOffset": 217}, {"referenceID": 6, "context": "\u2212 By selecting specific submodular functions in Section 4, we recover and give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups [1, 2, 7], and we define new norms, in particular ones that can be used as non-factorial priors for supervised learning (Section 4).", "startOffset": 208, "endOffset": 217}, {"referenceID": 2, "context": "These are illustrated on simulation experiments in Section 7, where they outperform related greedy approaches [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 11, "context": "These set-functions are often referred to as polymatroid set-functions [12] or \u03b2-functions [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "These set-functions are often referred to as polymatroid set-functions [12] or \u03b2-functions [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": ", Bk with non empty intersection with A (which will lead to the grouped l1/l\u221e-norm [1, 14]).", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": ", Bk with non empty intersection with A (which will lead to the grouped l1/l\u221e-norm [1, 14]).", "startOffset": 83, "endOffset": 90}, {"referenceID": 14, "context": "Given any set-function F , one can define its Lov\u00e1sz extension [15] (a.", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "Choquet integral [16]) f : Rp+ \u2192 R, as follows: given w \u2208 Rp+, we can order the components of w in decreasing order wj1 > \u00b7 \u00b7 \u00b7 > wjp > 0; the value f(w) is then defined as:", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": ", [15, 12]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 11, "context": ", [15, 12]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": ", minimizing f over {0, 1}p is equivalent to minimizing f over [0, 1] [15, 13].", "startOffset": 63, "endOffset": 69}, {"referenceID": 14, "context": ", minimizing f over {0, 1}p is equivalent to minimizing f over [0, 1] [15, 13].", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": ", minimizing f over {0, 1}p is equivalent to minimizing f over [0, 1] [15, 13].", "startOffset": 70, "endOffset": 78}, {"referenceID": 11, "context": "We denote by P the submodular polyhedron [12], defined as the set of s \u2208 Rp+ such that for all A \u2282 V , s(A) 6 F (A), i.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "One important result in submodular analysis is that if F is a nondecreasing submodular function, then we have a representation of f as a maximum of linear functions [12, 15], i.", "startOffset": 165, "endOffset": 173}, {"referenceID": 14, "context": "One important result in submodular analysis is that if F is a nondecreasing submodular function, then we have a representation of f as a maximum of linear functions [12, 15], i.", "startOffset": 165, "endOffset": 173}, {"referenceID": 12, "context": "Stable sets are also sometimes referred to as flat or closed [13].", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "The set of stable sets is closed by intersection [13], and will correspond to the set of allowed sparsity patterns (see Section 6.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "As shown in [13], the submodular polytope P has full dimension p as soon as F is strictly positive on all singletons, and its faces are exactly the sets {sk = 0} for k \u2208 V and {s(A) = F (A)} for stable and inseparable sets.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Most algorithms for minimizing submodular functions rely on the following strong duality principle [13, 12]:", "startOffset": 99, "endOffset": 107}, {"referenceID": 11, "context": "Most algorithms for minimizing submodular functions rely on the following strong duality principle [13, 12]:", "startOffset": 99, "endOffset": 107}, {"referenceID": 16, "context": "The first type of algorithm leads to strongly polynomial algorithms with best known complexity O(p) [17], while the minimum point algorithm of [12] has no worst-time complexity bounds but is usually much faster in practice [12] and is based on the equivalent problem of finding the minimum-norm point in B(G), i.", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "The first type of algorithm leads to strongly polynomial algorithms with best known complexity O(p) [17], while the minimum point algorithm of [12] has no worst-time complexity bounds but is usually much faster in practice [12] and is based on the equivalent problem of finding the minimum-norm point in B(G), i.", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "The first type of algorithm leads to strongly polynomial algorithms with best known complexity O(p) [17], while the minimum point algorithm of [12] has no worst-time complexity bounds but is usually much faster in practice [12] and is based on the equivalent problem of finding the minimum-norm point in B(G), i.", "startOffset": 223, "endOffset": 227}, {"referenceID": 17, "context": ", [18]) of \u03a9 is equal to \u03a9\u2217(s) = maxA\u2282V \u2016sA\u20161 F (A) = maxA\u2208T \u2016sA\u20161 F (A) .", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "Other examples of submodular functions, in particular in terms of matroids and entropies, may be found in [12, 10, 11] and could also lead to interesting new norms.", "startOffset": 106, "endOffset": 118}, {"referenceID": 9, "context": "Other examples of submodular functions, in particular in terms of matroids and entropies, may be found in [12, 10, 11] and could also lead to interesting new norms.", "startOffset": 106, "endOffset": 118}, {"referenceID": 10, "context": "Other examples of submodular functions, in particular in terms of matroids and entropies, may be found in [12, 10, 11] and could also lead to interesting new norms.", "startOffset": 106, "endOffset": 118}, {"referenceID": 8, "context": ", [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "We consider grouped norms defined with potentially overlapping groups [1, 2], i.", "startOffset": 70, "endOffset": 76}, {"referenceID": 1, "context": "We consider grouped norms defined with potentially overlapping groups [1, 2], i.", "startOffset": 70, "endOffset": 76}, {"referenceID": 1, "context": "In the case where l\u221e-norms are replaced by l2-norms, [2] has shown that the set of allowed sparsity patterns are intersections of complements of groups G with strictly positive weights.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "2 extends the result of [2] to the new case of l\u221e-norms.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond to the set-function F (A) which is the cardinality of the union of ancestors of elements in A.", "startOffset": 54, "endOffset": 63}, {"referenceID": 4, "context": "Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond to the set-function F (A) which is the cardinality of the union of ancestors of elements in A.", "startOffset": 54, "endOffset": 63}, {"referenceID": 5, "context": "Hierarchical norms defined on directed acyclic graphs [1, 5, 6] correspond to the set-function F (A) which is the cardinality of the union of ancestors of elements in A.", "startOffset": 54, "endOffset": 63}, {"referenceID": 4, "context": "These have been applied to bioinformatics [5], computer vision and topic models [6].", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "These have been applied to bioinformatics [5], computer vision and topic models [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "If we assume that the p variables are organized in a 1D, 2D or 3D grid, [2] considers norms based on overlapping groups leading to stable sets equal to rectangular or convex shapes, with applications in computer vision [19].", "startOffset": 72, "endOffset": 75}, {"referenceID": 18, "context": "If we assume that the p variables are organized in a 1D, 2D or 3D grid, [2] considers norms based on overlapping groups leading to stable sets equal to rectangular or convex shapes, with applications in computer vision [19].", "startOffset": 219, "endOffset": 223}, {"referenceID": 1, "context": "This leads to the undesired result, which has been already observed by [2], of adding all variables in one step, rather than gradually, when the regularization parameter decreases in a regularized optimization problem.", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": "fused Lasso penalty [20]), which is a relaxation of the number of jumps in a vector w rather than in its support.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": ", \u03bbp are the (nonnegative) eigenvalues of Q [21].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": ", [12, 9]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": ", [12, 9]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 21, "context": ", [22]), h(\u03bb) = \u03bb for q \u2208 (0, 1] are positive linear combinations of functions that lead to nondecreasing submodular functions.", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": "In a frequentist setting, the Mallows CL penalty [23] depends on the degrees of freedom, of the form trX\u22a4 AXA(X \u22a4 AXA + \u03bbI) \u22121.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "In a Bayesian context however, it is shown by [24] that penalties of the form log det(X\u22a4 AXA + \u03bbI) (which lead to submodular functions) correspond to marginal likelihoods associated to the set A and have good behavior when used within a non-convex framework.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "We do not pursue the extensive evaluation of non-factorial convex priors in this paper but provide in simulations examples with F (A) = tr(X\u22a4 AXA) 1/2 (which is equal to the trace norm of XA [18]).", "startOffset": 191, "endOffset": 195}, {"referenceID": 24, "context": ", [25]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "In this paper, we consider the methods \u201cISTA\u201d and its accelerated variants \u201cFISTA\u201d [25], which are compared in Figure 4.", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "In the case of the l1-norm, this reduces to soft thresholding of z, the following proposition (see proof in the appendix) shows that this is equivalent to a particular algorithm for submodular function minimization, namely the minimum-norm-point algorithm, which has no complexity bound but is empirically faster than algorithms with such bounds [12]: Proposition 3 (Proximal operator) Let z \u2208 R and \u03bb > 0, minimizing 1 2\u2016w \u2212 z\u20162 + \u03bb\u03a9(w) is equivalent to finding the minimum of the submodular function A 7\u2192 \u03bbF (A) \u2212 |z|(A) with the minimum-norm-point algorithm.", "startOffset": 346, "endOffset": 350}, {"referenceID": 25, "context": ", [26]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": ", the l1/l\u221e-norm, tree-structured groups [6], or general overlapping groups [7]).", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": ", the l1/l\u221e-norm, tree-structured groups [6], or general overlapping groups [7]).", "startOffset": 76, "endOffset": 79}, {"referenceID": 26, "context": "Like recent analysis of sparsity-inducing norms [27], the analysis provided in this section relies heavily on decomposability properties of our norm \u03a9.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "3, but we could consider assumptions similar to the ones used in [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 27, "context": "We now assume that the linear model is well-specified and extend results from [28] for sufficient support recovery conditions and from [27] for estimation consistency.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "We now assume that the linear model is well-specified and extend results from [28] for sufficient support recovery conditions and from [27] for estimation consistency.", "startOffset": 135, "endOffset": 139}, {"referenceID": 27, "context": ", Propositions 6 and 8 extend results based on support recovery conditions [28]; while Propositions", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": ", [27]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "We can also get back results for the l1/l\u221e-norm [14].", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "For the submodular function F (A) = |A|1/2 (a simple submodular function beyond the cardinality) we compare three optimization algorithms described in Section 5, subgradient descent and two proximal methods, ISTA and its accelerated version FISTA [25], for p = n = 1000, k = 100 and \u03bb = 0.", "startOffset": 247, "endOffset": 251}, {"referenceID": 7, "context": "We compare three strategies for solving the combinatorial optimization problem minw\u2208Rp 1 2n\u2016y \u2212 Xw\u20162 + \u03bbF (Supp(w)) with F (A) = tr(X\u22a4 AXA) , the approach based on our sparsity-inducing norms, the simpler greedy (forward selection) approach proposed in [8, 3], and by thresholding the ordinary least-squares estimate.", "startOffset": 253, "endOffset": 259}, {"referenceID": 2, "context": "We compare three strategies for solving the combinatorial optimization problem minw\u2208Rp 1 2n\u2016y \u2212 Xw\u20162 + \u03bbF (Supp(w)) with F (A) = tr(X\u22a4 AXA) , the approach based on our sparsity-inducing norms, the simpler greedy (forward selection) approach proposed in [8, 3], and by thresholding the ordinary least-squares estimate.", "startOffset": 253, "endOffset": 259}, {"referenceID": 2, "context": "We now focus on the predictive performance and compare our new norm with F (A) = tr(X\u22a4 AXA) , with greedy approaches [3] and to regularization by l1 or l2 norms.", "startOffset": 117, "endOffset": 120}, {"referenceID": 28, "context": ", [29] for application of submodular functions to dictionary learning).", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": "Second, links between submodularity and sparsity could be studied further, in particular by considering submodular relaxations of other combinatorial functions, or studying links with other polyhedral norms such as the total variation, which are known to be similarly associated with symmetric submodular set-functions such as graph cuts [26].", "startOffset": 338, "endOffset": 342}, {"referenceID": 17, "context": "(ii) We denote by g\u2217 the Fenchel conjugate of g on the domain {w \u2208 R, \u2016w\u2016\u221e 6 1}, and g\u2217\u2217 its bidual [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "= max \u03b4\u2208[0,1]p \u03b4\u22a4|s| \u2212 f(\u03b4) because F \u2212 |s| is submodular.", "startOffset": 8, "endOffset": 13}, {"referenceID": 0, "context": "= max s\u2208Rp min \u03b4\u2208[0,1]p s\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4)", "startOffset": 17, "endOffset": 22}, {"referenceID": 0, "context": "= min \u03b4\u2208[0,1]p max s\u2208Rp s\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4) by strong duality and Slater\u2019s condition [18]", "startOffset": 8, "endOffset": 13}, {"referenceID": 17, "context": "= min \u03b4\u2208[0,1]p max s\u2208Rp s\u22a4w \u2212 \u03b4\u22a4|s|+ f(\u03b4) by strong duality and Slater\u2019s condition [18]", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "= min \u03b4\u2208[0,1]p,\u03b4>|w| f(\u03b4) = f(|w|) because F is nonincreasing.", "startOffset": 8, "endOffset": 13}, {"referenceID": 11, "context": "For more details on the subdifferential for nonzero components, see [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Following [6], without loss of generality, we assume that z has nonnegative components.", "startOffset": 10, "endOffset": 13}, {"referenceID": 25, "context": "Then, following [26], if we add a constant vector with components equal to \u03b1 to z, we may obtain level sets of w\u2217.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "which is a contradiction because of the invertibility of Q and the Schur complement lemma [30] (which implies that the previous quantity must be strictly negative).", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "What remains to be shown is the affine representation of \u0175J when the support is given; it is essentially equivalent to showing that the path is piecewise affine, which is not surprising for a polyhedral norm [31].", "startOffset": 208, "endOffset": 212}, {"referenceID": 31, "context": "Necessary optimality conditions [32] for such the problem in Eq.", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Moreover, by Carath\u00e9odory\u2019s theorem [32], the number k of non-zero \u03b7 may be taken to be less than |J |+ 1.", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "We follow the proof from [33] by using the decomposition property of the norm \u03a9.", "startOffset": 25, "endOffset": 29}], "year": 2010, "abstractText": "Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the l1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov\u00e1sz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.", "creator": "LaTeX with hyperref package"}}}