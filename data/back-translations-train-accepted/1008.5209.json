{"id": "1008.5209", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2010", "title": "Network Flow Algorithms for Structured Sparsity", "abstract": "We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of $\\ell_\\infty$-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.", "histories": [["v1", "Tue, 31 Aug 2010 03:39:49 GMT  (560kb)", "http://arxiv.org/abs/1008.5209v1", "accepted for publication in Adv. Neural Information Processing Systems, 2010"]], "COMMENTS": "accepted for publication in Adv. Neural Information Processing Systems, 2010", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["julien mairal", "rodolphe jenatton", "guillaume obozinski", "francis r bach"], "accepted": true, "id": "1008.5209"}, "pdf": {"name": "1008.5209.pdf", "metadata": {"source": "CRF", "title": "Network Flow Algorithms for Structured Sparsity", "authors": ["Julien Mairal", "Francis Bach", "Rodolphe Jenatton", "Guillaume Obozinski"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 100 8.52 09v1 [cs.LG] 3 1A ug2 010appor t de r r ech er ch eIS SN 0249 -639 9IS RN INR IA / R R-- 7372 --F R + EN GVision, Perception and Multimedia Understanding"}, {"heading": "INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Network Flow Algorithms for Structured Sparsity", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Julien Mairal \u2014 Rodolphe Jenatton \u2014 Guillaume Obozinski \u2014 Francis Bach", "text": "No. 7372August 2010"}, {"heading": "Centre de recherche INRIA Paris \u2013 Rocquencourt Domaine de Voluceau, Rocquencourt, BP 105, 78153 Le Chesnay Cedex", "text": "Telephone: + 33 1 39 63 55 11 - Telephone: + 33 1 39 63 53 30"}, {"heading": "Network Flow Algorithms for Structured Sparsity", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Julien Mairal\u2217\u2020 , Rodolphe Jenatton\u2217\u2020 , Guillaume Obozinski\u2020 , Francis Bach\u2020", "text": "Topic: Vision, Perception and Multimedia Understanding Perception, Cognition, Interaction\u00c9quipe-Projet WillowRapport de recherche n \u00b0 7372 - August 2010 - 23 pages Abstract: We consider a class of learning problems that contain a structured, sparsity-inducing norm, defined as the sum of \u221e norms across groups of variables. While much effort has been put into developing fast optimization methods when the groups are fragmented or embedded in a particular hierarchical structure, here we are dealing with the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to the optimization of the network flow. More specifically, the proximal problem associated with the norm is dual to a square minimum cost-flow problem. We propose an efficient procedure that combines its solution exactly in polynomial time. Our algorithm scales to millions of variables and opens up a minimum range of cost-structured models for applications."}, {"heading": "Algorithmes de Flots pour Parcimonie Structur\u00e9e", "text": "R\u00e9sum\u00e9: Nous consid\u00e9rons une classise de probl\u00e8mes d'apprentissage r\u00e9gularis\u00e9s par une norme induisant de la parcimonie structur\u00e9e, d\u00e9finie comme une somme de normes \u221e sur des groupes de variables. Alors que de nombreux efforts ont \u00e9t\u00e9s mis pour d\u00e9velopper des algorithmes d'optimisation rapides lorsque les groupes sont disjoints ou structur\u00e9s hi\u00e9rarchiquement, nous int\u00e9ressons au cas g\u00e9n\u00e9ral de groupes avec recouvrement. Nous montrons que le probl\u00e8me d'optimisation correspondant est li\u00e9 \u00e0 l'optimimisation de flots sur un r\u00e9seau. Plus pr\u00e9cis\u00e9ment, l'op\u00e9rateur proximal associ\u00e9 \u00e0 la norme que nous consid\u00e9bl\u00e8rons est d'un co\u00fbt quadratique sur un qui \u00e9cuvariables de processives."}, {"heading": "Network Flow Algorithms for Structured Sparsity 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "In such models, linear combinations of small sets of variables are selected to describe the data. Regularisation by the E1 standard has proved to be a powerful tool for solving this combinatorial selection problem, which relies on both a well-developed theory (see [1] and points to it) and efficient algorithms. E1 primarily encourages sparse solutions, regardless of the potential structural relationships (e.g. spatial, temporal or hierarchical) that exist between the variables."}, {"heading": "2 Structured Sparse Models", "text": "In this paper, we will consider convex optimization problems of formin-W-Rp-f (w) + f-Rp (w), (1), where f: Rp-R is a convex, non-smooth, sparsity-inducing regulation function. If one knows a priori that the solutions to this learning problem have few unequal coefficients, it is often chosen as a 1-norm, which leads, for example, to the lasso [13]. If these coefficients are organized in groups, a penalty that explicitly encodes this foreknowledge can improve the predictive performance and / or interpretability of the learned models [12, 14, 15, 16]."}, {"heading": "Network Flow Algorithms for Structured Sparsity 4", "text": "Where G is a group of indices, wj denotes the j-th coordinate of w for j in [1; p], {1,.., p}, the vector wg in R | g | represents the coefficients of w indexed by g in G, and the scalars \u03b7g are positive weights. A sum of 2 norms is also used in literature [7], but the \u221e norm is piecemeal, a property we use in this essay. Note that if G is the set of singlets of [1; p], we get back the 1 norm. If G is a more general division of [1; p], variables are selected in groups and not individually. If the groups overlap, the solution to this task is still a norm and sets groups of variables together to zero [5]. The latter setting was considered first for hierarchies [7, 10, 17] and then extended to general group structures [5]."}, {"heading": "2.1 Proximal Methods", "text": "In a nutshell, proximal methods can be regarded as a natural extension of gradient-based techniques, and they are well suited to minimize the sum f + \u03bb\u0442 of two convex terms, a smooth function f - continuously differentiable with Lipschitz continuous gradient - and a potentially non-smooth function \u03bb\u0442 (see [18] and references to it). In each iteration, the function f is linearized at the current estimate w0, and the so-called proximal problem must be solved: min w, Rpf (w0) + (w \u2212 w0) f (w \u2212 w0) + p, (w) + L 2, w \u2212 w0 \u00b2 2. The square term holds the solution in a neighborhood where the current linear approximation applies, and L > 0 is an upper limit at the Lipschitz constant of f (w0) + p, this problem can be circumscribed asmin w, Rp1 \u00b2 u \u2212 a proximal, + 22, we are connected to the L (3), and the proximal (3) is a variable."}, {"heading": "3 A Quadratic Min-Cost Flow Formulation", "text": "In this section we show that a convex dual of the problem (3) can be reformulated as a quadratic minimum cost-flow problem for general overlapping groups G. We propose an efficient algorithm to solve it precisely, as well as a related algorithm to calculate the dual norm of the problem (3) introduced in [11], in the case where \"is a sum of \u221e norms: 1Note that other types of structured sparse models have also been introduced, either by another norm [6] or by non-convex criteria [8, 9].2For hierarchies, the approach of [11] also applies to the case where\" is a weighted sum of two norms. \"RR n \u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 5", "text": "Lemma 1 (Dual of the proximal problem [11]) Given u in Rp, we consider the Problemmin that in Rp \u00b7 G | 1 2, 2, 3, 4, 5, 5, 5, 5, 5, 6, 6, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, G, 11, 11, 11, 11, 11, 11, G, 11, 11, 11, 11, 11, 11, 11, 11, 11, G, 11, 11, 11, 11, 11, 11, 11, 11, G, 11, 11, G, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, G, 11, 11, 11, 11, 11, G, 11, 11, 11, 11, 11, 11, 11,"}, {"heading": "3.1 Graph Model", "text": "In fact, it is such that it is a way in which most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "Network Flow Algorithms for Structured Sparsity 7", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 Computation of the Proximal Operator", "text": "It has been shown that in both machine learning and operational research, such a projection can be performed in O (p) operations. If the group structure is a tree as shown in Figure 1 (d), strategies developed in the two communities are also similar [11, 19], and solve the problem in O (pd) operations where d is the depth of overlap. Hochbaum and Hong have shown in [19] that square minimum cost flows can be reduced to a specific parametric problem for which an efficient algorithm exists."}, {"heading": "Network Flow Algorithms for Structured Sparsity 8", "text": "This results in an equivalent optimization problem, which can be broken down into two independent problems of smaller size and solved recursively by calling computeFlow (V +, E +) and computeFlow (V \u2212, E \u2212). Note that our algorithm solves a problem of smaller size (4) during the first projection step in line 1 and stops. Formal proof of the correctness of algorithm 1 and further details is provided in Appendix B.The approach of [19, 22] guarantees the same worst-case complexity as a single Maxflow algorithm. However, we have experimentally observed a significant discrepancy between the worst case and empirical complexity for these flow problems, essentially because the empirical costs of each individual Maxflow algorithm are significantly lower than our costs."}, {"heading": "3.3 Computation of the Dual Norm", "text": "The duality gap for problem (1) can be derived from standard fennel duality arguments [29] and it is f (w) + f (w) + f (p) + f (p) + f (p) for w, vice versa (z) for f (c) and f (z). The duality gap for problem (1) can be derived from standard fennel duality arguments [29], and it is f (w) + f (c) for w. To efficiently calculate the duality gap, a practicable duality gap must be found."}, {"heading": "Network Flow Algorithms for Structured Sparsity 9", "text": "In the network problem that is associated with (12), the capacities on the arcs (s, g), g, g, g, and the capacities on the arcs (j, t), j are specified in [1; p]. The solution to the problem (12) boils down to finding the smallest value of the determination, so that a flow exists that the capacities on the arcs that lead to the sink t, saturates. Equation (12) and the subsequent algorithm have been explained in Appendix B. Algorithm 2 as correct. \u2212 4: Return of the dual standard. 1: Inputs: area content: area content Rp, a group of groups G, positive weights (area content) g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, g, v, g, g, v, g, g, g, g, g, g, g, g, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v,"}, {"heading": "4 Applications and Experiments", "text": "Our experiments use the algorithm of [4] based on our proximal operator, where the weights \u03b7g are set to 1. We present this algorithm in Appendix C."}, {"heading": "4.1 Speed Comparison", "text": "We compare our method (ProxFlow) and two generic optimization techniques, namely a subgradient descent (SG) and an inner point method, 5 on a regulated linear regression problem. Both SG and ProxFlow are implemented in C + +. Experiments are carried out on a one- or two-dimensional grid and show local correlations. The following families of groups G using this spatial information are therefore taken into account: (1) each coherent sequence of lengths 3 for the one-dimensional case (DCT), which are naturally organized on one- or two-dimensional grids and exhibit local correlations. The following families of groups G using this spatial information are taken into account: (1) each coherent sequence of lengths 3 for the one-dimensional case (DCT) and (2) all 3 \u00d7 3-square grids. We produce vectors in Rn according to the linear model 0.00.00.00.0- 0.00.00.00.00.00.00.0- 0.00.00.00.00.00.00.0- 0.00.00.00.00.00.00.00.0- 0.00.00.00.00.00.0- 0.00.00.00.00.00.0- 0.00.00.00.00.00.00.0- 0.00.00.00.00.00.00.0- 0.00.00.00.00.0- 0.00.00.00.00.00.0- 0.00.00.00.00.00.0- 0.00.00.00.00.00.00.0- 0.00.00.00.00.0- 0.00.00.00.00.00.00.0- 0.00.00.00.0- 0.00.00.00.00.0- 0.00.00.00.00.0- - 0.00.00.00.00.00.00.0- - 0.00.00.00.00.00.00.0- - - - 0.00.00.00.00.00.0- - - - 0.00.00.00.00.00.00.00.0- - - - - - - - 0.00.00.00.00.00.00.00.00.0- - - - - - - - - - - 0.00.00.00.00.00.00.00.00.0- - - - - - - - - - - - - - - - 0.00.00.00.00.00.00.00.00.0- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 0.0- - - - - - - - - - 0.0- - - - - - - 0.0- - - - - - 0.0- - - - - - 0.0- - - - - - - 0.0"}, {"heading": "Network Flow Algorithms for Structured Sparsity 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 Background Subtraction", "text": "The following [8] is a background subtraction task. Faced with a sequence of images from a fixed camera, we try to segment foreground objects into a new image. If we call this image, which consists of n pixels, y-Rn, then we model y as a sparse linear combination of p other images X-Rn \u00b7 p, plus an error term e in Rn, i.e. y-Xw + e for a sparse vector w in Rp. This approach is reminiscent of [30] in the context of face detection, where e continues to be sparsely used to deal with small events. Xw takes into account background elements present in both y and X, while e contains specific or superficial objects in y. The resulting optimization problem is minw, e-Xw \u2212 e \u00b2 2 + 1 \u00b2 w \u00b2 2 + 1 \u00b2 w \u00b2 1 + x \u00b2 1, with the structural resolution containing 1, 2 \u00b2 x \u00b2 x \u00b2."}, {"heading": "4.3 Multi-Task Learning of Hierarchical Structures", "text": "In [11] Jenatton et al. have recently proposed to use a hierarchically structured norm to learn dictionaries of natural image fields. Following their work, we try to render n signals {y1,.., yn} of dimension m as sparse linear combinations of elements from a dictionary X = [x1,.., xp] in Rm \u00b7 p. This can be expressed for all i in [1; n] as yi \u2248 Xwi, for some sparse vector wi in Rp. In [11] the dictionary elements are embedded in a predefined tree T, over a certain instance of the structured norm that we call \"tree,\" and call G the underlying group of groups. In this case, each signal gives yi a sparse decomposition in the form of a predefined tree of dictionary elements."}, {"heading": "Network Flow Algorithms for Structured Sparsity 11", "text": "The new regulation concept operates on the rows of W and is defined as \"joint (W),\" where W, [w1] | wn] is the matrix of the decomposition coefficients in Rp \u00b7 n. The total penalty on W resulting from the combination of \"tree\" and \"joint\" is itself an instance of the general overlap of groups as defined in Eq (2). To address the problem (6), we use the same optimization scheme as [11], i.e. we switch between dimensions X and W by optimizing one variable with respect to the other."}, {"heading": "Network Flow Algorithms for Structured Sparsity 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Conclusion", "text": "Interestingly, this sheds new light on the links between sparse methods and the literature for optimizing network flows. In particular, the proximal operator for the formulation we are looking at can be considered a square minimum-cost-flow problem, for which we propose an efficient and simple algorithm, allowing the use of accelerated gradient methods.Several experiments show that our algorithm can be applied to a broad class of learning problems that have not been addressed before within sparse methods.RR n \u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 13", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Equivalence to Canonical Graphs", "text": "Formally, the concept of equivalence exists between the graphs with zero cost and infinite possibilities: Lemma 2 (equivalence to canonical graphs) Let G = (V, E, s, t) become the canonical graphs according to a group structure G with weights (\u03b7g) g \"G.\" Let G \"= (V, E,\" s) be a diagram that has the same set of wells, sources and sinks as G. \"We say that G\" is equivalent to G if and only if the following conditions apply: \u2022 Arcs of E \"starting from the source are the same as in E, with the same costs and capacities as G.\" Arcs of E \"going to sink are the same as in E,\" with the same costs and capacities. \u2022 For each arc (g, j) in E, with (g, j) in Vgr \"Vu\" there is a unique path in E. \""}, {"heading": "B Convergence Analysis", "text": "In this section we show the correctness of algorithm 1 for the calculation of the proximal operator and algorithm 2 for the calculation of the dual standard."}, {"heading": "Network Flow Algorithms for Structured Sparsity 14", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Computation of the Proximal Operator", "text": "We prove that our algorithm flow problematic is equivalent and that it finds the optimal solution to the proximal problem while we provide the optimum conditions for the problem (4), [11], (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), \"(5),\" (5), \"(5),\" (5, \"(5),\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5, \"(5),\" (5, \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5"}, {"heading": "Network Flow Algorithms for Structured Sparsity 16", "text": "The algorithm then calculates a maximum flow by using the scalars (Vgr) and (Vgr), and we now have two possible situations: 1. If we (Vgr) and (Vgr) have a maximum flow (V) and (V) a maximum flow (V), we get (V) and (V) a maximum flow (V) and (V) a maximum flow (V). (V) and (V) a maximum flow (V). (V) and (V) a maximum flow (V). (V) and (V) a maximum flow (V). (V) and (V) a maximum flow (V). (V) and (V). (V). (V). (V). (V). (V). (V). (V). (V). (V). (V). (V). (V). (V). (V). (V)."}, {"heading": "Network Flow Algorithms for Structured Sparsity 17", "text": "We will show that it is possible to do the same in Eq (11), so that the combination of these two equations will yield the optimum states of Eq (7) and V (7). (11) This result is therefore relatively intuitive: (s, V +) and (V \u2212 t) is an (s, t) intersection, all arcs between V and V \u2212 are saturated, while there are unsaturated arcs between s and V +; one expects the residuals uj \u2212 j on the V + side while we are rising on the V \u2212 side."}, {"heading": "Network Flow Algorithms for Structured Sparsity 18", "text": "Equivalent graphs. Then it is easy to see that the value given by the max flow and the selected (s, t) section are the same, which is enough to conclude that the algorithm performs the same steps for two equivalent graphs."}, {"heading": "B.2 Computation of the Dual Norm \u2126\u22c6", "text": "Similar to the proximal operator, the calculation of the dual standard, which turns out to be a problem, can solve another network flow problem based on the following variation formula, which is an earlier result of [5]: Lemma 4 (dual formulation of the dual standard, [5]: Lemma 4 (dual formulation of the dual standard, [6]: Lemma 4 (dual formulation of the dual standard, [7]). Let us solve the problem with [7] (7)."}, {"heading": "Network Flow Algorithms for Structured Sparsity 19", "text": "The convergence of the algorithm requires only to show that the cardinality of V in the various calls to the function computeFlow is strictly decreasing. Similar arguments to those used in the proof of Proposition 1 can show that each part of the sections (V +, V \u2212) are not both empty. Therefore, the algorithm requires a finite number of calls to a maximum flow algorithm and converges in a finite and polynomial number of operations. Now, let us prove that the algorithm is correct for a canonical graph. We proceed again with the induction to the number of nodes of the graph. More precisely, let us consider the induction hypothesis H (k) defined as: for all canonical graphs G = (V, s, t) associated with a group structure GV and such that V | \u2264 k, dualNormAux (V = Vgr, E) solves the following optimization problem."}, {"heading": "C Algorithm FISTA with duality gap", "text": "In this section, we describe in detail the algorithm FISTA [4] when applied to solve problem (1), with a duality gap as a stop criterion. Suppose, without loss of universality, we look for models of the form Xw, for a matrix X-Rn \u00b7 p (typically linear models, where X is the data matrix of n observations)."}, {"heading": "Network Flow Algorithms for Structured Sparsity 20", "text": "For the calculation of the duality gap, an assessment of the dual problem is required. (...) In addition, we can consider the difference associated with the method as unique. (...) Based on fennel duality arguments (29), f (...) + f (...) + f (...), for w (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), f (...), w (...), w (...), w (...), w (...). (...), w (...)."}, {"heading": "D Additional Experimental Results", "text": "D.1 Speed comparison of algorithm 1 with parametric max flow algae rithms As shown in [19], min cost flow problems and in particular the dual problem of (3) can be reduced to a specific parametric max flow problem. Therefore, we compare our approach (ProxFlow) with the RR n \u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 21", "text": "The efficient parametric max flow algorithm proposed by Gallo, Grigoriadis and Tarjan [22] and a simplified version of the latter by Babenko and Goldberg in [23]. We refer to these two algorithms as GGT and SIMP, respectively. The benchmark is based on the same data sets already used in the experimental section of the paper, namely: (1) three data sets composed of supercomplete bases of discrete cosine transformations (DCT), each with 104, 105 and 106 variables, and (2) images used for the background subtraction task, composed of 57600 pixels. For GGT and SIMP, we use the paraF software, which is a C + + parametric max flow implementation available at http: / www.avglab.com / andrew / soft.html."}, {"heading": "Network Flow Algorithms for Structured Sparsity 22", "text": "[10] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In Proc. ICML, 2010. [11] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In Proc. ICML, 2010. [12] V. Roth and B. Fischer. The Group-Lasso for generalized linear models: uniqueness of solutions and efficient algorithms. In Proc. ICML, 2008. [13] R. Tibshirani. Regression shrinkage and selection via the Lasso. J. Roy. Stat. Soc. B, 58 (1): 267 - 288, 1996. [14] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Roy. Stat. B, 68: 49-67, 2006. [15] J. Huang and Zint."}, {"heading": "Network Flow Algorithms for Structured Sparsity 23", "text": "[29] J. M. Borwein and A. S. Lewis. Convex analysis and nonlinear optimization: Theory and examples. Springer, 2006. [30] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE T. Pattern. Anal., 31 (2): 210-227, 2009. [31] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar. Collaborative hierarchical sparse modelling. Technical report, 2010. Preprint arXiv: 1003.0400v1. [32] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.RR n \u00b0 7372Centre de recherche INRIA Paris - Rocquencourt Domaine de Voluceau - Rocquencourt - BP 105 - 78153 Le Chesnay Cedex (Centre Nord recherche Centre de recherche MontRIA 4303."}], "references": [{"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P. Bickel", "Y. Ritov", "A. Tsybakov"], "venue": "Ann. Stat.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Ann. Stat.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "Technical report, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imag. Sci.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["R. Jenatton", "J-Y. Audibert", "F. Bach"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Group Lasso with overlap and graph Lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "In Proc. ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Ann. Stat.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "Z. Zhang", "D. Metaxas"], "venue": "In Proc. ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Model-based compressive sensing", "author": ["R.G. Baraniuk", "V. Cevher", "M. Duarte", "C. Hegde"], "venue": "IEEE T. Inform. Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E.P. Xing"], "venue": "In Proc. ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "In Proc. ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The Group-Lasso for generalized linear models: uniqueness of solutions and efficient algorithms", "author": ["V. Roth", "B. Fischer"], "venue": "In Proc. ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Stat. Soc. B,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Roy. Stat. Soc. B,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "The benefit of group sparsity", "author": ["J. Huang", "T. Zhang"], "venue": "Technical report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Stat. Comput.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Exploring large feature spaces with hierarchical multiple kernel learning", "author": ["F. Bach"], "venue": "In Adv. NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Proximal splitting methods in signal processing. In Fixed- Point Algorithms for Inverse Problems in Science and Engineering", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "About strongly polynomial time algorithms for quadratic optimization over submodular constraints", "author": ["D.S. Hochbaum", "S.P. Hong"], "venue": "Math. Program.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Efficient projections onto the l1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proc. ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "An O(n) algorithm for quadratic knapsack problems", "author": ["P. Brucker"], "venue": "Oper. Res. Lett.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1984}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["G. Gallo", "M.E. Grigoriadis", "R.E. Tarjan"], "venue": "SIAM J. Comput.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1989}, {"title": "Experimental evaluation of a parametric flow algorithm", "author": ["M. Babenko", "A.V. Goldberg"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A new approach to the maximum flow problem", "author": ["A.V. Goldberg", "R.E. Tarjan"], "venue": "In Proc. of ACM Symposium on Theory of Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "Maximal flow through a network", "author": ["L.R. Ford", "D.R. Fulkerson"], "venue": "Canadian J. Math.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1956}, {"title": "Linear Network Optimization", "author": ["D.P. Bertsekas"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "On implementing the push-relabel method for the maximum flow problem", "author": ["B.V. Cherkassky", "A.V. Goldberg"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "In Adv. NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Convex analysis and nonlinear optimization: Theory and examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE T. Pattern. Anal.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Collaborative hierarchical sparse modeling", "author": ["P. Sprechmann", "I. Ramirez", "G. Sapiro", "Y.C. Eldar"], "venue": "Technical report,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Centre de recherche INRIA Paris \u2013 Rocquencourt Domaine de Voluceau", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 221, "endOffset": 230}, {"referenceID": 2, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 221, "endOffset": 230}, {"referenceID": 3, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 221, "endOffset": 230}, {"referenceID": 4, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 5, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 6, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 7, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 8, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 5, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 241, "endOffset": 248}, {"referenceID": 9, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 241, "endOffset": 248}, {"referenceID": 10, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 265, "endOffset": 269}, {"referenceID": 7, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 290, "endOffset": 293}, {"referenceID": 2, "context": "Proximal methods have proven to be effective in this context, essentially because of their fast convergence rates and their ability to deal with large problems [3, 4].", "startOffset": 160, "endOffset": 166}, {"referenceID": 3, "context": "Proximal methods have proven to be effective in this context, essentially because of their fast convergence rates and their ability to deal with large problems [3, 4].", "startOffset": 160, "endOffset": 166}, {"referenceID": 11, "context": "While the settings where the penalized groups of variables do not overlap [12] or are embedded in a tree-shaped hierarchy [11] have already been studied, sparsityinducing regularizations of general overlapping groups have, to the best of our knowledge, never been considered within the proximal method framework.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "While the settings where the penalized groups of variables do not overlap [12] or are embedded in a tree-shaped hierarchy [11] have already been studied, sparsityinducing regularizations of general overlapping groups have, to the best of our knowledge, never been considered within the proximal method framework.", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "When one knows a priori that the solutions of this learning problem only have a few non-zero coefficients, \u03a9 is often chosen to be the l1-norm, leading for instance to the Lasso [13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 11, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 13, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 14, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 15, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 6, "context": "A sum of l2-norms is also used in the literature [7], but the l\u221e-norm is piecewise linear, a property that we take advantage of in this paper.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "When the groups overlap, \u03a9 is still a norm and sets groups of variables to zero together [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 61, "endOffset": 72}, {"referenceID": 9, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 61, "endOffset": 72}, {"referenceID": 16, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 61, "endOffset": 72}, {"referenceID": 4, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 10, "context": "Following [11] who tackled the case of hierarchical groups, we propose to approach this problem with proximal methods, which we now introduce.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "1 Proximal Methods In a nutshell, proximal methods can be seen as a natural extension of gradient-based techniques, and they are well suited to minimizing the sum f + \u03bb\u03a9 of two convex terms, a smooth function f \u2014continuously differentiable with Lipschitz-continuous gradient\u2014 and a potentially non-smooth function \u03bb\u03a9 (see [18] and references therein).", "startOffset": 322, "endOffset": 326}, {"referenceID": 2, "context": "Simple proximal method use w as the next iterate, but accelerated variants [3, 4] are also based on the proximal operator and require to solve problem (3) exactly and efficiently to enjoy their fast convergence rates.", "startOffset": 75, "endOffset": 81}, {"referenceID": 3, "context": "Simple proximal method use w as the next iterate, but accelerated variants [3, 4] are also based on the proximal operator and require to solve problem (3) exactly and efficiently to enjoy their fast convergence rates.", "startOffset": 75, "endOffset": 81}, {"referenceID": 17, "context": "(3) is obtained by a soft-thresholding [18].", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "The approach we develop in the rest of this paper extends [11] to the case of general overlapping groups when \u03a9 is a weighted sum of l\u221e-norms, broadening the application of these regularizations to a wider spectrum of problems.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 232, "endOffset": 235}, {"referenceID": 7, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 268, "endOffset": 274}, {"referenceID": 8, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 268, "endOffset": 274}, {"referenceID": 10, "context": "For hierarchies, the approach of [11] applies also to the case of where \u03a9 is a weighted sum of l2-norms.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "Lemma 1 (Dual of the proximal problem [11]) Given u in R, consider the problem", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "(4) derived in [11] show that for all j in [1; p], the signs of the non-zero coefficients \u03be j for g in G are the same as the signs of the entries uj .", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "The graphs (c) and (d) correspond to a special case of tree-structured hierarchy in the sense of [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "2 Computation of the Proximal Operator Quadratic min-cost flow problems have been well studied in the operations research literature [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "It has been shown, both in machine learning [20] and operations research [19, 21], that such a projection can be done in O(p) operations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "It has been shown, both in machine learning [20] and operations research [19, 21], that such a projection can be done in O(p) operations.", "startOffset": 73, "endOffset": 81}, {"referenceID": 20, "context": "It has been shown, both in machine learning [20] and operations research [19, 21], that such a projection can be done in O(p) operations.", "startOffset": 73, "endOffset": 81}, {"referenceID": 10, "context": "When the group structure is a tree as in Figure 1(d), strategies developed in the two communities are also similar [11, 19], and solve the problem in O(pd) operations, where d is the depth of the tree.", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "When the group structure is a tree as in Figure 1(d), strategies developed in the two communities are also similar [11, 19], and solve the problem in O(pd) operations, where d is the depth of the tree.", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "Hochbaum and Hong have shown in [19] that quadratic min-cost flow problems can be reduced to a specific parametric max-flow problem, for which an efficient algorithm exists [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "Hochbaum and Hong have shown in [19] that quadratic min-cost flow problems can be reduced to a specific parametric max-flow problem, for which an efficient algorithm exists [22].", "startOffset": 173, "endOffset": 177}, {"referenceID": 21, "context": "Our method clearly shares some similarities with a simplified version of [22] presented in [23], namely a divide and conquer strategy.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "Our method clearly shares some similarities with a simplified version of [22] presented in [23], namely a divide and conquer strategy.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "Then, the maximum-flow step [24] tries to find a feasible flow such that the vector \u03be\u0304 matches \u03b3.", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "If \u03be\u0304 6= \u03b3, the lower bound cannot be reached, and we construct a minimum (s, t)-cut of the graph [25] that defines two disjoints sets of nodes V + and V ; V + is the part of the graph that can potentially receive more flow from the source, whereas all arcs linking s to V \u2212 are saturated.", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "The properties of a min (s, t)-cut [26] imply that there are no arcs from V + to V \u2212 (arcs inside V have infinite By definition, a parametric max-flow problem consists in solving, for every value of a parameter, a max-flow problem on a graph whose arc capacities depend on this parameter.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "The approach of [19, 22] is guaranteed to have the same worst-case complexity as a single maxflow algorithm.", "startOffset": 16, "endOffset": 24}, {"referenceID": 21, "context": "The approach of [19, 22] is guaranteed to have the same worst-case complexity as a single maxflow algorithm.", "startOffset": 16, "endOffset": 24}, {"referenceID": 23, "context": "\u2022 Efficient max-flow algorithm: We have implemented the \u201cpush-relabel\u201d algorithm of [24] to solve our max-flow problems, using classical heuristics that significantly speed it up in practice (see [24, 27]).", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "\u2022 Efficient max-flow algorithm: We have implemented the \u201cpush-relabel\u201d algorithm of [24] to solve our max-flow problems, using classical heuristics that significantly speed it up in practice (see [24, 27]).", "startOffset": 196, "endOffset": 204}, {"referenceID": 26, "context": "\u2022 Efficient max-flow algorithm: We have implemented the \u201cpush-relabel\u201d algorithm of [24] to solve our max-flow problems, using classical heuristics that significantly speed it up in practice (see [24, 27]).", "startOffset": 196, "endOffset": 204}, {"referenceID": 23, "context": "Our implementation uses the so-called \u201chighest-active vertex selection rule, global and gap heuristics\u201d (see [24, 27]), and has a worst-case complexity of O(|V ||E|) for a graph (V,E, s, t).", "startOffset": 109, "endOffset": 117}, {"referenceID": 26, "context": "Our implementation uses the so-called \u201chighest-active vertex selection rule, global and gap heuristics\u201d (see [24, 27]), and has a worst-case complexity of O(|V ||E|) for a graph (V,E, s, t).", "startOffset": 109, "endOffset": 117}, {"referenceID": 20, "context": "This modified projection step can still be computed in linear time [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "3 Computation of the Dual Norm The dual norm \u03a9 of \u03a9, defined for any vector \u03ba in R by \u03a9(\u03ba) , max\u03a9(z)\u22641 z\u03ba, is a key quantity to study sparsity-inducing regularizations [5, 17, 28].", "startOffset": 168, "endOffset": 179}, {"referenceID": 16, "context": "3 Computation of the Dual Norm The dual norm \u03a9 of \u03a9, defined for any vector \u03ba in R by \u03a9(\u03ba) , max\u03a9(z)\u22641 z\u03ba, is a key quantity to study sparsity-inducing regularizations [5, 17, 28].", "startOffset": 168, "endOffset": 179}, {"referenceID": 27, "context": "3 Computation of the Dual Norm The dual norm \u03a9 of \u03a9, defined for any vector \u03ba in R by \u03a9(\u03ba) , max\u03a9(z)\u22641 z\u03ba, is a key quantity to study sparsity-inducing regularizations [5, 17, 28].", "startOffset": 168, "endOffset": 179}, {"referenceID": 28, "context": "We denote by f the Fenchel conjugate of f [29], defined by f(\u03ba) , supz[z \u03ba\u2212f(z)].", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "The duality gap for problem (1) can be derived from standard Fenchel duality arguments [29] and it is equal to f(w) + \u03bb\u03a9(w) + f(\u2212\u03ba) for w,\u03ba in R with \u03a9(\u03ba) \u2264 \u03bb.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "4 Applications and Experiments Our experiments use the algorithm of [4] based on our proximal operator, with weights \u03b7g set to 1.", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "2 Background Subtraction Following [8], we consider a background subtraction task.", "startOffset": 35, "endOffset": 38}, {"referenceID": 29, "context": "This approach is reminiscent of [30] in the context of face recognition, where e is further made sparse to deal with small occlusions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "3 Multi-Task Learning of Hierarchical Structures In [11], Jenatton et al.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "In [11], the dictionary elements are embedded in a predefined tree T , via a particular instance of the structured norm \u03a9, which we refer to it as \u03a9tree, and call G the underlying set of groups.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Inspired by ideas from multi-task learning [16], we propose to learn the tree structure T by pruning irrelevant parts of a larger initial tree T0.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "In other words, the approach of [11] is extended by the following http://research.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "To address problem (6), we use the same optimization scheme as [11], i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "The task we consider is the denoising of natural image patches, with the same dataset and protocol as [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": ", when \u03a9tree is the l1-norm and \u03bb2 = 0) and the hierarchical dictionary learning of [11] based on predefined trees (i.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "We consider the same hierarchies as in [11], involving between 30 and 400 dictionary elements.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "2 (the pixels have values in [0, 1]); no significant improvements were observed for lower levels of noise.", "startOffset": 29, "endOffset": 35}, {"referenceID": 13, "context": "The simplified case where \u03a9tree and \u03a9joint are the l1- and mixed l1/l2-norms [14] corresponds to [31].", "startOffset": 77, "endOffset": 81}, {"referenceID": 30, "context": "The simplified case where \u03a9tree and \u03a9joint are the l1- and mixed l1/l2-norms [14] corresponds to [31].", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "1 in [26]), saying that there exists a decomposition of \u03c0 as a sum of path flows in E.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "This requires that we introduce the optimality conditions for problem (4) derived in [11], since our convergence proof essentially checks that these conditions are satisfied upon termination of the algorithm.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "Lemma 3 (Optimality conditions of the problem (4), [11]) The primal-dual variables (w, \u03be) are respectively solutions of the primal (3) and dual problems (4) if and only if the dual variable \u03be is feasible for the problem (4) and w = u\u2212 \u2211 g\u2208G \u03be , \u2200g \u2208 G, { w g \u03be g g = \u2016wg\u2016\u221e\u2016\u03be \u20161 and \u2016\u03be \u20161 = \u03bb\u03b7g, or wg = 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "\u2022 There is no flow going from V \u2212 to V + (see properties of the minimum (s, t)-cut [26]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Using the classical max-flow/min-cut theorem [25], we have equality between these two terms.", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "2 Computation of the Dual Norm \u03a9 Similarly to the proximal operator, the computation of dual norm \u03a9 can itself shown to solve another network flow problem, based on the following variational formulation, which extends a previous result from [5]: Lemma 4 (Dual formulation of the dual-norm \u03a9.", "startOffset": 241, "endOffset": 244}, {"referenceID": 31, "context": "This primal problem is convex and satisfies Slater\u2019s conditions for generalized conic inequalities, which implies that strong duality holds [32].", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "j\u2208Vu \u03baj is the value of the max-flow, and the inequality follows from the max-flow/min-cut theorem [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "C Algorithm FISTA with duality gap In this section, we describe in details the algorithm FISTA [4] when applied to solve problem (1), with a duality gap as stopping criterion.", "startOffset": 95, "endOffset": 98}, {"referenceID": 28, "context": "Based on Fenchel duality arguments [29], f(Xw) + \u03bb\u03a9(w) + f(\u2212\u03ba), for w \u2208 R,\u03ba \u2208 R and \u03a9(X\u03ba) \u2264 \u03bb, is a duality gap for (14).", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "Given a primal variable w, a good dual candidate \u03ba can be obtained by looking at the conditions that have to be satisfied by the pair (w,\u03ba) at optimality [29].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "1 Speed comparison of Algorithm 1 with parametric max-flow algorithms As shown in [19], min-cost flow problems, and in particular, the dual problem of (3), can be reduced to a specific parametric max-flow problem.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "efficient parametric max-flow algorithm proposed by Gallo, Grigoriadis, and Tar- jan [22] and a simplified version of the latter proposed by Babenko and Goldberg in [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "efficient parametric max-flow algorithm proposed by Gallo, Grigoriadis, and Tar- jan [22] and a simplified version of the latter proposed by Babenko and Goldberg in [23].", "startOffset": 165, "endOffset": 169}], "year": 2010, "abstractText": "We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of l\u221e-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. Key-words: network flow optimization, convex optimization, sparse methods, proximal algorithms \u2217 Equal contribution. \u2020 INRIA WILLOW Project, Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00e9rieure (INRIA/ENS/CNRS UMR 8548). 23, avenue d\u2019Italie, 75214 Paris. France Algorithmes de Flots pour Parcimonie Structur\u00e9e R\u00e9sum\u00e9 : Nous consid\u00e9rons une classe de probl\u00e8mes d\u2019apprentissage r\u00e9gularis\u00e9s par une norme induisant de la parcimonie structur\u00e9e, d\u00e9finie comme une somme de normes l\u221e sur des groupes de variables. Alors que de nombreux efforts ont \u00e9t\u00e9s mis pour d\u00e9velopper des algorithmes d\u2019optimisation rapides lorsque les groupes sont disjoints ou structur\u00e9s hi\u00e9rarchiquement, nous nous int\u00e9ressons au cas g\u00e9n\u00e9ral de groupes avec recouvrement. Nous montrons que le probl\u00e8me d\u2019optimisation correspondant est li\u00e9 \u00e0 l\u2019optimisation de flots sur un r\u00e9seau. Plus pr\u00e9cis\u00e9ment, l\u2019op\u00e9rateur proximal associ\u00e9 \u00e0 la norme que nous consid\u00e9rons est dual \u00e0 la minimisation d\u2019un co\u00fbt quadratique de flot sur un graphe particulier. Nous proposons une proc\u00e9dure efficace qui calcule cette solution en un temps polynomial. Notre algorithme peut traiter de larges probl\u00e8mes, comportant des millions de variables, et ouvre de nouveaux champs d\u2019applications pour les mod\u00e8les parcimonieux structur\u00e9s. Nous pr\u00e9sentons diverses exp\u00e9riences sur des donn\u00e9es d\u2019images et de vid\u00e9os, qui d\u00e9montrent l\u2019utilit\u00e9 et l\u2019efficacit\u00e9 de notre approche pour r\u00e9soudre de nombreux probl\u00e8mes. Mots-cl\u00e9s : optimisation de flots, optimisation convexe, m\u00e9thodes parcimonieuses, algorithmes proximaux Network Flow Algorithms for Structured Sparsity 3", "creator": "LaTeX with hyperref package"}}}