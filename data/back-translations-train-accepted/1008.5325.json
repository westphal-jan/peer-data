{"id": "1008.5325", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2010", "title": "Inference with Multivariate Heavy-Tails in Linear Models", "abstract": "Heavy-tailed distributions naturally occur in many real life problems. Unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy-tailed distributions.", "histories": [["v1", "Tue, 31 Aug 2010 14:31:57 GMT  (69kb)", "http://arxiv.org/abs/1008.5325v1", "In Neural Information Processing System (NIPS) 2010, Dec. 2010, Vancouver, Canada"], ["v2", "Fri, 5 Nov 2010 15:26:53 GMT  (131kb)", "http://arxiv.org/abs/1008.5325v2", "In Neural Information Processing System (NIPS) 2010, Dec. 2010, Vancouver, Canada"], ["v3", "Mon, 8 Nov 2010 16:14:02 GMT  (101kb)", "http://arxiv.org/abs/1008.5325v3", "In Neural Information Processing System (NIPS) 2010, Dec. 2010, Vancouver, Canada"], ["v4", "Mon, 21 Mar 2011 15:54:54 GMT  (134kb)", "http://arxiv.org/abs/1008.5325v4", "In Neural Information Processing System (NIPS) 2010, Dec. 2010, Vancouver, Canada"]], "COMMENTS": "In Neural Information Processing System (NIPS) 2010, Dec. 2010, Vancouver, Canada", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["danny bickson", "carlos guestrin"], "accepted": true, "id": "1008.5325"}, "pdf": {"name": "1008.5325.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["bickson@cs.cmu.edu", "guestrin@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 100 8.53 25v1 [cs.LG] 3 1Heavy tail distributions naturally occur in many problems of real life. Unfortunately, it is typically not possible to compute conclusions in closed form in graphical models that include such heavy tail distributions. In this thesis, we propose a novel, simple linear graphical model for independent latent random variables, called linear characteristic (LCM), which is defined in the characteristic functional domain. Using stable distributions, a family of heavy tail distributions that is a generalization of Cauchy, Le \u0301 vy, and Gaussian distributions, we show for the first time how to calculate both exact and approximate conclusions in such a linear multivariate graphical model. LCMs are not limited to stable distributions, in fact, LCMs are always defined for any random variable (reactive, continuous, or mixed)."}, {"heading": "1 Introduction", "text": "Heavy distributions occur naturally in many real-life phenomena, for example, in computer networks [16, 8, 10]. Typically, a small set of machines is responsible for a large portion of the network bandwidth consumed. Likewise, a small set of users will generate a large portion of network traffic. Another common feature of communication networks is that network traffic tends to be linear [3, 16]. Linearity is explained by the fact that all incoming traffic at a node is composed of the sum of incoming flows. Recently, several papers suggest using linear multivariate statistical methods to monitor network health, performance analysis, or intrusion detection [9, 10, 8]. Some aspects of network traffic make the task of modeling using a probable graphic model challenging."}, {"heading": "1.1 Related work", "text": "There are three relevant papers in the field of machine learning related to the current work: Convolutionary Factor Graphs (CFG), Copulas and Independent Component Analysis (ICA). Below, we briefly review them and explain why a new graphical model is needed. Convolutional Factor Graphs (CFG) [12, 13] are a graphical model for the representation of linear relations of independent latent random variables. CFG assumes that probability distribution is factorized as a product of potentials, and uses duality to derive a convolution factorization in the characteristic function (CF). In contrast to CFG, we assume that probability distribution is factored as a product of potentials in the cf domain. There are two justifications for our new model: (a) LCMs are always defined for all probability factorizations that are independent."}, {"heading": "2 Stable distribution", "text": "The stable distribution [23] is a family of heavily waisted distributions, with Cauchy, Le \u03b2vy and Gaussian being special cases of this family (see Figure 1).The stable distributions are used in various problem areas, including economics, physics, geology and astronomy [17].The stable distribution is useful because they can model a stable distribution by a tuple of four parameters, which naturally occur in practice. As we will shortly show using our network example, network flows exhibit an empirical distribution that can be modelled remarkably well by stable distributions. We indicate a stable distribution by a tuple of four parameters: S (\u03b1, \u03b2, \u03b3, \u03b4)."}, {"heading": "3 Linear characteristic models", "text": "One of the disadvantages of general stable distributions is that they do not have closed form equations for the pdf or cdf. This fact makes the handling of stable distributions more difficult, which is probably one of the reasons why stable distributions are seldom used in probabilistic graphical models. A new graphical model is needed, as previous approaches such as the CFG or the Copula method cannot be used to calculate closed distributions."}, {"heading": "3.1 Duality of LCM and CFG", "text": "In the face of a linear model (Def. 3.1), the probability of observing p (\u00b7 \u00b7 \u00b7 Y) is given as a fusion of the hidden variables of which it consists [13]: p (yi) = p (x1); \u00b7 \u00b7 * p (xn); \u00b7 \u00b7 \u00b7 j = 1... np (xj); these are binary and multivariate convolution operators respectively. [13] Furthermore, we have shown that the joint probability p (X, \u00b7 \u00b7 Y) of any linear model can be factored as a fusion: p (X, Y) = p (x1, \u00b7 \u00b7, xn, y1, \u00b7, ym) = xi ip (xi, y1, \u00b7 \u00b7 ym). (2) Informally, LCM is the dual representation of (2) in the characteristic functional domain."}, {"heading": "4 Main result: exact and approximate inference in LCM", "text": "Typically, the exact inference in linear models with continuous variables is limited to the well-understood cases of Gaussian, Gaussian mixtures and simple regression problems in exponential families. In this section, we expand on previous results to show how to calculate inferences (both exact and approximate) in the linear model with underlying stable distributions. Due to space constraints, all evidence is deferred to the supplementary material.4Defined in the supplementary material.Algorithm 1: Exact inference in LCM using LCM elimination"}, {"heading": "4.1 Exact inference in LCM", "text": "The conclusion task usually involves calculating the boundary distribution or a conditional distribution of a probability function. For the rest of the discussion we focus on the marginal distribution. The marginal distribution of the node xi is typically calculated by integrating all other nodes: p (xi) \u00b7 X\\ ip (X, Y) dX\\ i, where X\\ i is the set of all nodes that exclude the node i. Unfortunately, when we work with the stable distribution, the above integral is intractable. Instead, we suggest a dual operation called slicing, calculated in the cf domain."}, {"heading": "4.2 Exact inference in stable distributions", "text": "After defining LCM and showing that conclusions can be calculated in the Cf domain, we are finally ready to show how exact conclusions can be calculated in a linear model with underlying stable distributions. 5More detailed explanation of the construction of a graphical model from the linear relation matrix A is found at [1, Chapter 2.3]. Starting point: mij (xj) = 1, [2] Starting point: mij (xj) = 0, [3] Starting point to convergence mij (tj) = i (ti, s1, s1, s1). Starting point: mixi (i), [3] Starting point: mixi (ti), [3] Finally: mixi (ti, s1, \u00b7 \u00b7 \u00b7 sm). Starting point: mij (xj). Starting point: mij (xj). Starting point: xj (xj)."}, {"heading": "4.3 Approximate Inference in LCM", "text": "The cost of the exact conclusion can be expensive. For example, in the related linear model of a multivariate causa (a special case of stable distribution), LCM elimination is due to Gausa type algorithm with an insufficient number of variables. We propose two novel algorithms that are variants of faith propagation but do not always convert. We are moving towards a more efficient approximation of the cost of the exact inference."}, {"heading": "4.4 Approximate inference for stable distributions", "text": "In the case of stable distributions, we derive an approximation algorithm from the CSP update rules, Stable Approx (algorithm 2 (c)), which is derived by replacing the folding and multiplication by scalars (Prop. 2.1 b, a) in the update rules of the CSP algorithm given in Algorithm 2 (a). Similar to belief propagation, our approximate algorithm Stable Approx is no guarantee of convergence in general diagrams containing cycles. We have analyzed the evolutionary dynamics of the update equations for Stable Approx and derived sufficient conditions for convergence. Furthermore, we have analyzed the accuracy of the approximation. Unsurprisingly, the sufficient condition for convergence refers to the properties of the scalar relationship matrix A."}, {"heading": "5 Application: Network flow monitoring", "text": "\"I think it's going to take a lot of time to get to the bottom of what's going on, and I think it's going to take a lot of time to get to the bottom of it,\" he said."}, {"heading": "6 Conclusion and future work", "text": "We have shown for the first time how to perform exact and approximate conclusions in a linear multivariate graphical model when the underlying distributions are stable. We have discussed an application of our design for computerized inference of network flows. We have proposed to adopt ideas from the belief dissemination to calculate efficient conclusions based on the distribution characteristics of disk product operations and integral folding operations. We believe that other problem areas could benefit from this construction and plan to pursue this as future work. We believe that there are several exciting directions for expanding this work. Other distribution families such as geometric stable distributions or wishart can be analyzed in our model. Fourier transformation can be replaced by more general core transformation, resulting in richer models."}, {"heading": "Acknowledgement", "text": "D. Bickson thanks Andrea Pagnani (ISI) for the inspiration to lead this research, John P. Nolan (American University) for publishing parts of his excellent book on stable distribution online, Mark Veillette (Boston University) for publishing his stable distribution code online, Jason K. Johnson (LANL) for helping with convergence analysis, and Sapan Bathia, Marc E. Fiuczynski (Princeton University) for providing the PlanetFlow data. This research was supported by the Army Research Office MURI W911NF0710287."}, {"heading": "7 Supplementary material", "text": "Definition 7.1. Characteristic function. For a scalar random variable X, the characteristic function is defined as the expected value of eitX, where i is the imaginary unit, and t-R is the argument for the characteristic function: X (t) = E [eitX] = \u221e \u2212 eitxdFX (x), where FX (x) is the cumulative distribution function of X. If a random variable X has a probability density function fX, then the characteristic function is its Fourier transformation, X (t) = \u221e \u2212 eitxfX (x) dx."}, {"heading": "7.1 Proof of Theorem 3.3", "text": "Tested. F (p (X, Y)) = F (\u0445 \u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0442\u0438\u0435 i (p (xi, y1, \u00b7 \u00b7, ym)) = \u0441iF (p (xi, y1, \u00b7 \u00b7, ym)) = = \u0441i\u0442 (ti, s1, \u00b7 \u00b7, sm) = \u0432 (t1, \u00b7 \u00b7 \u00b7, tn, s1, \u00b7 \u00b7, sm)."}, {"heading": "7.2 Proof of Theorem 3.4", "text": "Proof. F \u2212 1 (section (t1, \u00b7 \u00b7, tn, s1, \u00b7 \u00b7, sm)) = F \u2212 1 (section (ti, s1, \u00b7 \u00b7, sm)) = 1 (section (ti, s1, \u00b7 \u00b7, sm)) = 1 (section (ti, s1, \u00b7 \u00b7, sm)) = 1 (section (xi, y1, \u00b7 \u00b7, ym) = p (X, Y)."}, {"heading": "7.3 Proof of Theorem 4.2", "text": "Proof. The proof comes from the projection slice theorem (also known as the central slice theorem) [14, p. 349], which is briefly set out here. Let f (x, y) be a multivariate function and F (u, v) its matching Fourier transformation. ThenF {p (x)} = F {p (x, y) dy} = x (x, y) dy] dx = x (x, y) dy] dx = x (x, y) dy = x (x, y) e iux f (x, y) dxdy = F (u, 0).This theorem is naturally extended to several variables. In our case, the proof extends to the number of variables."}, {"heading": "7.4 Proof of Thm. 4.3", "text": "Proof: We use the linear relationship between distributions to extract X: X = A \u2212 1Y. Note that X must be distributed according to the stable distribution, since it is composed of a linear combination of stable variables. To obtain the scale parameter, we (using the linearity of A substituted in Prop. 2.1 (a), (b), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c, c, c (c), c (c), c (c, c), c (c, c (c), c, c (c), c, c (c), c, c (c, c (c), c, c (c), c (c, c, c, c), c, c (c, c, c (c), c, c (c, c), c, c, c, c, c, c (c, c, c), c (c, c, c, c), c (c, c, c, c, c, c, c), c (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "7.5 Proof of Theorem 4.4", "text": "The other algorithms are symmetrical because the slice / convolution operations and integral / convolution operations also maintain the distribution property. We are interested in calculating the posterior marginal probability (xi) = tree (xi) = x-ip (x, y) dX-i-p (x1, \u00b7 \u00b7, xn, y1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, ym) dX-i (7) = F-1 {0-ip (ti, s1, \u00b7, sm) ti = 0}. (8) W.l.g assume Xi is a tree root. Its matching marginal cf operators (ti) can be written as a combination of incoming messages computed by the adjacent subtrees."}, {"heading": "7.6 Proof of Theorem 4.5", "text": "The proof: We start with the calculation of the scale parameters, because it is decoupled from the other parameters."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Heavy-tailed distributions naturally occur in many real life problems. Unfortu-<lb>nately, it is typically not possible to compute inference in closed-form in graphical<lb>models which involve such heavy-tailed distributions.<lb>In this work, we propose a novel simple linear graphical model for independent<lb>latent random variables, called linear characteristic model (LCM), defined in the<lb>characteristic function domain. Using stable distributions, a heavy-tailed family<lb>of distributions which is a generalization of Cauchy, L\u00e9vy and Gaussian distri-<lb>butions, we show for the first time, how to compute both exact and approximate<lb>inference in such a linear multivariate graphical model. LCMs are not limited to<lb>stable distributions, in fact LCMs are always defined for any random variables<lb>(discrete, continuous or a mixture of both).<lb>We provide a realistic problem from the field of computer networks to demon-<lb>strate the applicability of our construction. Other potential application is iterative<lb>decoding of linear channels with non-Gaussian noise.", "creator": "LaTeX with hyperref package"}}}