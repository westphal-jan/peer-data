{"id": "1010.3091", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2010", "title": "Near-Optimal Bayesian Active Learning with Noisy Observations", "abstract": "We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non-uniform cost and their noise is correlated. We also propose EffECXtive, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.", "histories": [["v1", "Fri, 15 Oct 2010 08:20:46 GMT  (438kb,D)", "http://arxiv.org/abs/1010.3091v1", "13 pages"], ["v2", "Mon, 16 Dec 2013 06:42:05 GMT  (441kb,D)", "http://arxiv.org/abs/1010.3091v2", "15 pages. Version 2 contains only one major change, namely an amended proof of Lemma 6"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DS", "authors": ["daniel golovin", "andreas krause 0001", "debajyoti ray"], "accepted": true, "id": "1010.3091"}, "pdf": {"name": "1010.3091.pdf", "metadata": {"source": "CRF", "title": "Near\u2013Optimal Bayesian Active Learning with Noisy Observations", "authors": ["Daniel Golovin", "Andreas Krause"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Bayesian Active Learning in the Noiseless Case", "text": "We assume that the result of each test is a deterrent hypothesis, i.e. that we want to distinguish between a number of hypotheses. (We assume that we assume that the result of each test in X / 2,.) We assume that the result of each test in X / 3,.) We assume that the result of each test in X / 3,. We assume that we have a prior distribution P, which has the common probability P (H, X1, XN) over the hypotheses and test results. In the noiseless case, we assume that the result of each test is a deterrent hypothesis, i.e., for each h H, X1, XN) over the hypotheses and test results."}, {"heading": "5 Experiments", "text": "Several economic theories make claims to explain how people make decisions when payouts are uncertain. Here, we use human subjects to compare four key theories suggested in the literature. Uncertainty of payouts in a given situation is represented by a lottery that is simply a random variable with a range of payouts. L: = \"1,.,.\" For our purposes, a payout is an integer that indicates how many dollars you receive (or lose if the payout is negative). Fix lottery L: \"and leave pi: = P [L =\" i]. The four theories posits distinction functions, with agents who prefer larger utility lotteries. Three of the theories have associated parameters. The expected value theory simply UEV (L) = E [L], and has no parameters."}, {"heading": "6 Conclusions", "text": "In this paper, we looked at the problem of adaptive selection, which noisy tests are to be carried out to identify an unknown hypothesis from a known prior distribution. We examined the problem of equivalence class determination as a means of reducing the case of noisy observations to the classic noiseless case. We introduced EC2, an adaptive greedy algorithm that guarantees to choose the same hypothesis as if it had observed the result of all tests, and which, in all strategies with this guarantee, incurs almost minimum expected costs, in contrast to popular heuristics that are greedy for space reduction, information gains or information values that, as we show, can all be very far from optimal. EC2 works by greedily optimizing a target tailored to distinguish between observations that lead to different decisions. Our limitations are based on the fact that this objective function is adaptively submodular."}, {"heading": "A Additional Proofs", "text": "The objective function f of Eq. (3.1) is strongly adaptive monotone.Proof. We must show that we have for all xA, t / a and possible answer x for all xA (A, H), c), c), c), c), c (A), c), c), c), c), c), c), c), c), c), c), c (A (A), c), c), c), c), c (A (A), c), c), c), c (A (A), c), c), c), c), c), c), c), c (A), c), c), c), c), c), c), c)."}, {"heading": "B A Bad Example for the Info-Gain and Value of Information Criteria", "text": "The same heuristics can be applied to the problem of equivalence class determination by calculating the information gains in relation to the entropy of distribution across classes rather than hypotheses. Let's point out the resulting equivalence class determination policy. Another common heuristic problem for equivalence class determination is that we consider the test maximization of Bayesian decision theory in relation to the value of information (VoI) in relation to the expected reduction in the expected risk of the minimum risk decision, where the risk of expected losses is. Bayesian decision theory will be myoptically selected in \u00a7 4. The VoI criteria to be maximized."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We tackle the fundamental problem of Bayesian active learning with noise, where<lb>we need to adaptively select from a number of expensive tests in order to identify<lb>an unknown hypothesis sampled from a known prior distribution. In the case of<lb>noise\u2013free observations, a greedy algorithm called generalized binary search (GBS)<lb>is known to perform near\u2013optimally. We show that if the observations are noisy,<lb>perhaps surprisingly, GBS can perform very poorly. We develop EC, a novel,<lb>greedy active learning algorithm and prove that it is competitive with the optimal<lb>policy, thus obtaining the first competitiveness guarantees for Bayesian active learn-<lb>ing with noisy observations. Our bounds rely on a recently discovered diminishing<lb>returns property called adaptive submodularity, generalizing the classical notion<lb>of submodular set functions to adaptive policies. Our results hold even if the tests<lb>have non\u2013uniform cost and their noise is correlated. We also propose EFFECX-<lb>TIVE, a particularly fast approximation of EC, and evaluate it on a Bayesian<lb>experimental design problem involving human subjects, intended to tease apart<lb>competing economic theories of how people make decisions under uncertainty.", "creator": "TeX"}}}