{"id": "1010.5511", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2010", "title": "Efficient Minimization of Decomposable Submodular Functions", "abstract": "Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.", "histories": [["v1", "Tue, 26 Oct 2010 20:23:39 GMT  (229kb,DS)", "http://arxiv.org/abs/1010.5511v1", "Expanded version of paper for Neural Information Processing Systems 2010"]], "COMMENTS": "Expanded version of paper for Neural Information Processing Systems 2010", "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["peter stobbe", "andreas krause 0001"], "accepted": true, "id": "1010.5511"}, "pdf": {"name": "1010.5511.pdf", "metadata": {"source": "CRF", "title": "Efficient Minimization of Decomposable Submodular Functions", "authors": ["Peter Stobbe", "Andreas Krause"], "emails": ["stobbe@caltech.edu", "krausea@caltech.edu"], "sections": [{"heading": "1 Introduction", "text": "Many seemingly multimodal optimization problems, such as nonlinear classification, clustering, and dimensionality reduction, can be cast as convex programs. In minimizing a convex loss function, we can be sure that we will efficiently find the optimal solution, even for large problems. Convex optimization is a structural property of continuous optimization problems. Many machine learning problems, such as structural learning, variable selection, MAP inference in discrete graphical models, require the solution of discrete, combinatorial optimization problems. In recent years, another basic problem structure, which has similar beneficial properties, has proven to be very useful in many combinatorial optimization problems that arise in machine learning: submodularity is an intuitively diminishing return property, which states that adding an element to a smaller group helps more than adding it to a larger group."}, {"heading": "2 Background on Submodular Function Minimization", "text": "We are interested in minimizing functions that form some basis from E to real numbers. (D) We are interested in minimizing these functions. (D) We are interested in minimizing functions that relate to real numbers. (D) We are interested in minimizing functions (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "3 The Decomposable Submodular Minimization Problem", "text": "In this paper we consider the problem of minimizing functions as follows: f (A) = c (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA (A) \u00b7 eA, A (A) \u00b7 eA, A (A), A (A), A, A (A), A (A, A, A, A, A (A), A, A, A (A), A, A (A, A, A, A, A (A), A, A, A (A, A, A, A, A, A, A (A), A, A, A, A, A, A, A, A (A, A, A, A, A, A, A), A, A, A, A, A, A (A), A, A, A, A, A, A, A, A (A), A, A, A, A, A, A, A, A), A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, eA), A, A, A, A, eA, eA, eA, eA), A, A, A, A, A, A, eA, eA, eA, A, eA, eA, A, A, A, A"}, {"heading": "4.1 The Smoothed Extension of a Threshold Potential", "text": "The key challenge in our algorithm is to define the Lova \"sz extension of f\" (max.) so that we can fall back on algorithms to accelerate convex minimization (max.). We now show how we can efficiently smooth out the threshold potentials if we can express a large class of submodular functions. For x-0, the Lova \"sz\" extension of the Lova \"w,\" y extension of the section \"y\" (x) = supv \"x-s.t. (x-s) = supv.\" (x-s): x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "4.3 Early Stopping based on Discrete Certificates of Optimality", "text": "In general, we can have an optimum gap: f) f) f) f) f) f) f) f) f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f \"f\" f."}, {"heading": "5 Extension to General Concave Potentials", "text": "To extend our algorithm to work on general concave functions, we note that any concave function can be expressed as an integral of threshold potential functions. This is a simple sequence of integration through parts that we specify in the following Lemma 2 for such concave functions: Lemma 2 for those of C2 ([0, T]),? (x) =? (0) +? (T) x -eA 0 min (x, y) dy,? x [0, T] This means that we have for a general sum of concave potentials as in equation (3): f (A) = c \u00b7 eA +? p p p? p p? p p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p"}, {"heading": "6 Experiments", "text": "In fact, it is such that most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to move, to fight, to fight, to move, to move, to fight, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to move, to fight, to fight, to move, to move, to fight, to move, to fight, to move, to fight, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to fight, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to move, to move, to move, to move, to move"}, {"heading": "7 Conclusion", "text": "We have developed a novel method of efficiently minimizing a large class of submodular functions of practical importance by splitting the function into a sum of threshold potentials, the Lova \u0301 sz extensions of which are suitable for the use of advanced smoothing techniques of convex optimization. This allows us to solve submodular minimization problems with thousands of variables that can be expressed not only by pairing potentials. Thus, we have reached a middle ground between graph-based algorithms that are extremely fast, but only able to handle very specific types of submodular minimization problems, and combinatorial algorithms that require nothing more than submodularity, but are not practical for large-scale problems.Recognition This research has been partially supported by the NSF grant IIS-0953413, a gift from Microsoft Corporation and a research grant from the Okawa Foundation. Thanks to Alex Gittens and Michael McCoy for using their TextonBoost implementation."}, {"heading": "A Submodularity of Decomposable Functions", "text": "Since the sum of the submodular functions is submodular, we only need to prove that the submodularity of f (A) = \u03c6 (w \u00b7 eA), where \u03c6 is an arbitrary concave function on R and w \u2265 0, is defined for all \u03b8 [0, 1] as follows:?? (y + h) + (1 \u2212 \u03b8) x) +? (1 \u2212 \u03b8) (y + h) + \u03b8x)??? (y + h) +? (x) If x \u2264 y and h \u2265 0 are set, then in the above,? (x + h)????? (x + h)? (y + h)? (6)??? (k)??????? (K) +???? (X)?????? (K) +? (K)?? (K)"}, {"heading": "B Reformulation of Set Cover Functions", "text": "A set-cover function can be formulated as a function: f (A) = | \u0441i-ABi | where Bi are subsets of a base set F, and the Bi form a collection of subsets indexed by E. For each k-F, we define the vectors wk-R-E-E-E as follows: wk [i] = {0 k / Bi-1 k-BiWe claim: f (A) = \u2211 k-F min (1, wk \u00b7 eA) The kest term in the sum corresponds to 1 if k-Bi for some i-A and 0 otherwise. The sum of all these terms results in the cardinality of the union of the bi with i-A, which is exactly the set-cover function."}, {"heading": "C Strict Generality of Threshold Potentials", "text": "As mentioned in the text, every concave cardinalityfunction in the sum of several emerging potentials are decomposed. This is effectively the discrete version of Lemma 2:?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "D Reformulation of a Class of Functions", "text": "Another example of decomposed features are the problems in [10] viewed by the following form are: f (A) = c \u00b7 eA + (u \u00b7 eA) f (v \u00b7 eA) f (where u, v not negative weight vectors are and f??? a non-increasing concave function is. We assume that we can vectors ww and concave before selecting to satisfy: f (w?) f???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "E Proof of Lemma 1", "text": "First, we claim that if the hypothesis of the term is correct, the addition of a positive multiple of eA will not change the gradient. That is, the formula for the gradient will not be changed. This means that the gradient will not change. That is, the formula for the gradient will not change. Remember, the formula for adding a positive multiple of eA to x in this formula. Either the effect of eA to x is increased or it remains unchanged; in both cases, the gradient itself remains unchanged. Next, we note the following scale relationship, which results directly from the definition of eA to x."}, {"heading": "F Proof of Lemma 2", "text": "This is a simple calculation: \"T 0 min (x, y) \u03c6\" (y) dy = \"x 0 y\u03c6\" (y) dy + \"T x x\u03c6\" (y) dy = (y\u03c6 \"(y) \u2212 \u03c6 (y),\" x 0 + x\u03c6 \"(y),\" x x x x = x\u03c6 \"(x),\" x x \"(x),\" x \"(x) + \u03c6 (0) + x\u03c6\" (T) \u2212 x\u03c6 \"(x). Intuitively, this is a consequence of the partial integration and the fact that\" X 2 x x 2 min (x, y) = \u2212 \u03b4 (x \u2212 y) (the Dirac delta)."}, {"heading": "G General Smoothed Gradient Formula", "text": "????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}], "references": [{"title": "Structured sparsity-inducing norms through submodular functions", "author": ["F. Bach"], "venue": "Advances in Neural Information Processing Systems ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Nesta: A fast and accurate first-order method for sparse recovery", "author": ["S. Becker", "J. Bobin", "E.J. Candes"], "venue": "Arxiv preprint arXiv 904 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient solutions to relaxations of combinatorial problems with submodular penalties via the Lov\u00e1sz extension and non-smooth convex optimization", "author": ["F.A. Chudak", "K. Nagano"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathematics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "and A", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn"], "venue": "Zisserman, The PASCAL Visual Object Classes Challenge 2009 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A push-relabel framework for submodular function minimization and applications to parametric optimization", "author": ["L. Fleischer", "S. Iwata"], "venue": "Discrete Applied Mathematics 131 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Energy minimization via graph cuts: Settling what is possible", "author": ["D. Freedman", "P. Drineas"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005. CVPR 2005, vol. 2", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "The Minimum-Norm-Point Algorithm Applied to Submodular Function", "author": ["Satoru Fujishige", "Takumi Hayashi", "Shigueo Isotani"], "venue": "Minimization and Linear Programming,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Beyond convexity: Online submodular minimization, Advances in Neural Information Processing Systems", "author": ["Elad Hazan", "Satyen Kale"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Computational geometric approach to submodular function minimization for multiclass queueing systems", "author": ["T. Itoko", "S. Iwata"], "venue": "Integer Programming and Combinatorial Optimization ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "A combinatorial strongly polynomial algorithm for minimizing submodular functions", "author": ["S. Iwata", "L. Fleischer", "S. Fujishige"], "venue": "Journal of the ACM (JACM) 48 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "A simple combinatorial algorithm for submodular function minimization", "author": ["S. Iwata", "J.B. Orlin"], "venue": "Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, Society for Industrial and Applied Mathematics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust Higher Order Potentials for Enforcing Label Consistency", "author": ["Pushmeet Kohli", "Lubor Ladick\u00fd", "Philip H.S. Torr"], "venue": "International Journal of Computer Vision", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "SFO: A Toolbox for Submodular Function Optimization", "author": ["A. Krause"], "venue": "The Journal of Machine Learning Research 11 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical programming: the state of the art, Bonn ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1982}, {"title": "An analysis of the approximations for maximizing submodular set functions", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming 14 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1978}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Minimizing symmetric submodular functions", "author": ["Maurice Queyranne"], "venue": "Mathematical Programming", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}], "referenceMentions": [{"referenceID": 9, "context": "In particular, the minimum of a submodular function can be found in strongly polynomial time [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "Unfortunately, while polynomial-time solvable, exact techniques for submodular minimization require a number of function evaluations on the order of n [12], where n is the number of variables in the problem (e.", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Examples include symmetric functions that can be solved in O(n) evaluations using Queyranne\u2019s algorithm [19], and functions that decompose into attractive, pairwise potentials, that can be solved using graph cutting techniques [7].", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "Examples include symmetric functions that can be solved in O(n) evaluations using Queyranne\u2019s algorithm [19], and functions that decompose into attractive, pairwise potentials, that can be solved using graph cutting techniques [7].", "startOffset": 227, "endOffset": 230}, {"referenceID": 15, "context": "Our algorithm is based on recent techniques of smoothed convex minimization [18] applied to the Lov\u00e1sz extension.", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": ", [16]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "A key result due to Lov\u00e1sz [16] states that each submodular function f has an extension f\u0303 that not only satisfies the above property, but is also convex and efficient to evaluate.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "With the additional assumption that f is nondecreasing, maximizing a submodular function subject to a cardinality constraint |A| \u2264M is \u2018easy\u2019; a greedy algorithm is known to give a near-optimal answer [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 13, "context": "Equation (2) was used to show that submodular minimization can be achieved in polynomial time [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "Recently, Nesterov showed that if knowledge about the structure of a particular non-smooth convex function is available, it can be exploited to achieve a running time of O(1/ ) [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 2, "context": "Connections of this work with submodularity and combinatorial optimization are also explored in [4] and [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "Connections of this work with submodularity and combinatorial optimization are also explored in [4] and [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "In fact, in [2], Bach shows that computing the smoothed Lov\u00e1sz gradient of a general submodular function is equivalent to solving a submodular minimization problem.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "(To apply the smoothing technique of [18], special structural knowledge about the convex function is required, so it is natural that we would need special structural knowledge about the submodular function to leverage those results.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Recent work [14] has shown that classification performance can be improved by adding terms corresponding to such higher order potentials \u03c6j(|Rj\u2229A|) to the objective function where the functions \u03c6j are piecewise linear concave functions, and the regions Rj of various sizes generated from a segmentation algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "Another example of decomposable functions arises in multiclass queuing systems [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "We now wish to use the accelerated gradient descent algorithm of [18] to minimize this function.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Then we simply apply the accelerated gradient descent algorithm of [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "See also [3] for a description.", "startOffset": 9, "endOffset": 12}, {"referenceID": 15, "context": "In summary, as a consequence of the results of [18], we have the following guarantee about SLG:", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "2 (see [9] for more details).", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "We reproduce the experimental setup of [8] designed to compare submodular minimization algorithms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "We compare against the state of the art combinatorial algorithms (LEX2, HYBRID, SFM3, PR [6]) that are guaranteed to find the exact solution in polynomial time, as well as the Minimum Norm algorithm of [8], a practical alternative with unknown running time.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "We compare against the state of the art combinatorial algorithms (LEX2, HYBRID, SFM3, PR [6]) that are guaranteed to find the exact solution in polynomial time, as well as the Minimum Norm algorithm of [8], a practical alternative with unknown running time.", "startOffset": 202, "endOffset": 205}, {"referenceID": 6, "context": "Figures 1(b) and 1(c) compare the running time of SLG against the running times reported in [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "We used an implementation of TextonBoost [20], then trained on and tested subsampled images from [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "In comparison, the MinNorm implementation of the SFO toolbox [15] gave the same result, but took 6900 seconds.", "startOffset": 61, "endOffset": 65}], "year": 2010, "abstractText": "Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.", "creator": "TeX"}}}