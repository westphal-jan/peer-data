{"id": "1011.0041", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2010", "title": "Predictive State Temporal Difference Learning", "abstract": "We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.", "histories": [["v1", "Sat, 30 Oct 2010 03:09:11 GMT  (337kb,D)", "https://arxiv.org/abs/1011.0041v1", null], ["v2", "Tue, 18 Jan 2011 02:04:12 GMT  (337kb,D)", "http://arxiv.org/abs/1011.0041v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["byron boots", "geoffrey j gordon"], "accepted": true, "id": "1011.0041"}, "pdf": {"name": "1011.0041.pdf", "metadata": {"source": "CRF", "title": "Predictive State Temporal Difference Learning", "authors": ["Byron Boots", "Geoffrey J. Gordon"], "emails": ["beb@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": "1 Introduction and Related Work", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Value Function Approximation", "text": "We begin with a discrete time dynamic system with a set of states S, a set of actions A, a distribution over initial states \u03c00, a state transitional function T, a reward function R and a discount factor \u03b3 [0, 1]. We strive for a policy \u03c0, an assignment of states to actions. The concept of a value function is central to enhanced learning: for a given policy \u03c0, the value of the state s is defined as the expected discounted sum of rewards obtained when starting in the states and following the policy \u03c0, J\u03c0 (s) = E [\u2211 \u221e t = 0 \u03b3tR (st) | s0 = s, \u03c0]. It is well known that the value function of the state hs must obey the Bellman equation (s) = R (s) + of the state J\u03c0 (s \u00b2) + (s \u00b2) Pr [s \u00b2 | s \u00b2 s \u00b2 s) | s), that we use the observation (s \u00b2 s), that we can use the transitional function directly, if the state S is sufficient, and if the problem S is small."}, {"heading": "2.1 Least Squares Temporal Difference Learning", "text": "In general, we do not know the transition probabilities Pr [h\u03c0o | h], but we have samples of the state characteristics \u03c6Ht = \u03c6 H (ht), the next state characteristics \u03c6Ht + 1 = \u03c6 H (ht + 1) and the immediate reward Rt = R (ht). We can therefore use the Bellman equation \u03c6H1: k \u00b2 R1: k + \u03b3wT\u03c6H2: k + 1 (4) (Here we have used the notation \u03c6H1: k to mean the matrix whose columns are equal to H = 1... k. We can immediately try to estimate the parameter w by solving the linear system in the sense of the smallest squares: w \u00b2 T = R1: k \u00b2 H2: k \u00b2: k + 1) \u2020, where \u2020 the Moore-Penrose is pseudoinverse."}, {"heading": "3 Predictive Features", "text": "Although LSTD provides a consistent estimate of the value function parameters w, in practice the potential size of the feature vectors can be a problem. If the number of features is large in relation to the number of training parameters, then the estimate of w tends to overmatch. This problem can be mitigated by selecting a few small features that only contain information relevant to approximating the value function. However, with the exception of LARS-TD [18], little has been done on the problem of automatically selecting features for approximating the value function when the system model is unknown; and of course, manual feature selection depends on expert guidance that is not always available. We approach the problem of finding a good set of features from a bottleneck perspective. That is, given a signal from history, in this case a large number of features, we would like to find compression that retains only relevant information for predicting the value function J. As we will see in section 4 of the improvement related to the speculative identification."}, {"heading": "3.1 Tests and Features of the Future", "text": "We must first define precisely the task of predicting the future. Just as a story is an ordered sequence of action-observation pairs executed before time, we define a test of length i to be an ordered sequence of action-observation pairs \u03c4 = a1o1... aioi that can be executed and observed after time t [14]. The prediction for a test after a story h, written result (h), is the probability that we will see the test observations \u03c4O = o1.. oi, given that we can intervene and after time t execute the test actions \u03c4A = a1.. ai:. (h) Pr [\u03c4O | h, do). If Q = {\u03c41,.) is a series of tests that we write Q (h) = (h),."}, {"heading": "3.2 Finding Predictive Features Through a Bottleneck", "text": "In order to find a prediction, we first need to determine what we want to do for the prediction. (Since we are interested in predicting individual characteristics, we will find most of the relevant predictions ourselves, so we could simply try to predict the total future reward, while we do not necessarily expect full reward, unless we have a lot of data that we are able to reduce the variance of future observations, so that we can try to predict the reward of future time steps, while we do not necessarily need to predict total reward, and find much more immediate feedback from future observations, which will hopefully contain information about future rewards, so that they can help us better predict rewards.) Finally, in each specific RL application, we can add problem-specific prediction tasks that draw our attention to relevant information: for a pathology problem, we could try to predict which of several future states we will achieve."}, {"heading": "3.3 Predictive State Temporal Difference Learning", "text": "Now that we have found a predictive compression operator V \u00b2 via equation 10, we can replace the characteristics of the story \u03c6Ht with the compressed characteristics V \u00b2 H \u00b2 in the Bellman recursion, equation 4, resulting in the following approximate Bellman equation: wTV \u00b2 \u03c6H1: k \u00b2 R1: k + \u03b3wTV \u00b2 \u03c6H2: k + 1 (11) The solution of the smallest squares for w is still prone to an error-in-variable problem. The variable \u0445H is still correlated with the true independent variables and not correlated with noise, and so we can use it again as an instrumental variable to neutralize the estimation of w."}, {"heading": "4 Predictive State Representations", "text": "Unlike POMDPs, which represent the state as a distribution over a latent variable, PSRs represent the state as a set of predictions of tests. Formally, a PSR consists of five elements < A, O, Q, s1, F >. A is a finite set of possible actions, and O is a finite set of possible observations. Q is a core set of tests, i.e. a set whose vector of predictions Q (h) is a sufficient statistic to predict the probability of success of all tests. F is the set of functions that embody these predictions. (h) And m1 = Q () is the initial prediction vector vector. In this work, we will limit ourselves to linear PSRs in which all prediction functions are linear."}, {"heading": "4.1 Learning Transformed PSRs", "text": "Let's define a minimum core set of tests for a dynamic system (h) as a linear function of T (h) (h). Then let's define a larger core set of tests (not necessarily minimal, and possibly even with an infinite horizon.) As before, let's write a vector of the characteristics of history at the time t, and write a vector of the characteristics of the future. Since T is a core set of tests, we can calculate any test prediction (h) as a linear function of T (h). And since characteristics of predictions are linear combinations of test predictions, we can define any prediction of T (h) as a linear function of T (h)."}, {"heading": "4.2 Predictive State Temporal Difference Learning (Revisited)", "text": "Finally, we are ready to show that the model-free PSTD learning algorithm introduced in Section 3.3 corresponds to a model-based algorithm that builds on PSR learning. [31] For a fixed policy \u03c0, the value function of a TPSR is a linear function of the state, J\u03c0 (s) = wTDP + wTb, and is the solution of the TPSR Bellman equation [31]: for all b, wTb = bTCE values we use in relation to the PSR values we use in relation to the PSR parameters from equations 17 (a-c), we use getw values, w value, o value, o value, o value, o value, o-value, o-value, o-value, o-value, o DP, o-value, o, o-value, o DP, o-value, o-o, o, o-value, o, o-value, o, o, o-value o, o, o, o-value o, o-o, o, o-value, o-o, o-o, o-value, o-o, o, o-value, o-o, o-value o, o, o-value, o-o, o, o-value, o-o, o-DP, o-value, o-o, o-value, o-o, o, o-value, o-value, o-o, o-o-o, o, o-value, o-value, o-o, o-value, o, o-value, o-o-o, o-o-value, o, o-value, o, o-o-value, o-value, o, o, o-value, o-value, o-value, o, o, o-o-value, o-value, o-P, o-value, o-P, o-value, o, o-value, o-P, o-value, o-value, o, o-value, o-value, o-P, o, o-value, o-value, o-value, o-P, o-value, o, o-value, o, o-value, o-value, o"}, {"heading": "4.3 Insights from Subspace Identification", "text": "In Equation 17 we assumed that the features of history are rich enough to determine the state of the dynamic system completely. Indeed, using the theory developed in [21] it is possible to loosen this assumption and instead assume that the state is correlated only with features of history. In this case we must introduce a new set of covariance matrices, which represent the features of history before and the features of the tests after action a and observation o. We can then estimate the TPSR transition matrices, since B-ao = U-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T (see T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-"}, {"heading": "5 Experimental Results", "text": "We developed several experiments to evaluate the properties of the PSTD learning algorithm. In the first experiment, we look at the comparative advantages of PSTD in relation to LSTD and LARS-TD when applying them to the problem of estimating the value function of a POMDP reduced rank. In the second experiment, we apply PSTD to an optimal stop problem (the pricing of a fictitious financial derivative) and show that PSTD outperforms competing approaches."}, {"heading": "5.1 Estimating the Value Function of a RR-POMDP", "text": "We evaluate the PSTD learning algorithm using a synthetic example derived from [32]. The problem is to find the value function of a policy in a partially observable Markov decision-making process (POMDP). POMDP has 4 latent states, but the transition phase is low: the resulting faith distributions can be represented in a three-dimensional subspace of the original faith simplex. A reward of 1 is given in the first and third latent states, and a reward of 0 in the other two latent states (see Appendix, Section B). The system emits 2 possible observations, mixing information about the latent statutes. We conduct 3 experiments, compare the performance of LSTD, LARS-TD, and PSTD, and PSTD as they are formulated."}, {"heading": "5.2 Pricing A High-dimensional Financial Derivative", "text": "Derivatives are financial contracts with payouts linked to the future prices of basic assets such as stocks, bonds and commodities. \"In some derivatives, the contract holder has no choice, but in more complex cases, the contract holder must make decisions - for example, by exercising the contract early, the contract holder can decide whether to terminate the contract at any time and receive payments based on prevailing market conditions. In these cases, the value of the derivatives depends on how the contract holder acts - and when the exercise is an optimal stop problem: at any time, the contract holder must decide whether to continue the contract or exercise. Such stop problems provide an ideal test bed for policy valuation methods, as we can easily collect a single data set that is sufficient to evaluate any policy: We can then evaluate the\" ongoing \"measures forever. (We can then easily evaluate the\" stop \"measures in any of the resulting states, since the immediate reward of the derivatives is given by the Tsiat, and the next financial state is the one of Roy)."}, {"heading": "6 Conclusion", "text": "In this paper, we address the problem of feature selection for learning time differences. Although known algorithms for temporal differences such as LSTD can provide asymptotically unbiased estimates of value function parameters in linear architectures, they may have difficulty in finite samples: if the number of features is large relative to the number of training samples, they can exhibit large variations in their value function estimates. Therefore, in real-world problems, a considerable amount of time is spent selecting a small number of features, often by trial and error [33, 34]. To address this problem, we present the PSTD algorithm, a new approach to feature selection for TD methods, which shows how insights from system identification can benefit amplification learning. PSTD automatically selects a small group of features relevant to predicting and adjusting the value of the function. It approaches feature selection from a predictive perspective by finding only a small set of features."}, {"heading": "Acknowledgements", "text": "Byron Boots was supported by NSF under grant number EEEC-0540865. Byron Boots and Geoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052."}, {"heading": "A Determining the Compression Operator", "text": "We find a compression operator V-shaped, which optimally predicts test characteristics by the CCA bottleneck defined by U-shaped. Estimation of the smallest squares can be found by minimizing the following lossL (V) standard. We can find V-shaped by taking a derivative of this loss L with respect to V, setting it to zero and solving VL = 1 ktr (((((T1: k \u2212 U-shaped V-shaped H1: k) T) -1 k tr (vT1: k \u2212 V-shaped H1: k) T) -2k tr (vT1: k \u2212 V-shaped H1: k) T-shaped T) T: k-shaped T: k-shaped T: k-shaped T: k-shaped T: T-shaped T-shaped T: T-shaped V-k-shaped T: T-shaped T-k) T: T-shaped U-shaped T: T-shaped T: T-k-k-shaped T: T-shaped T-k) T: T-shaped T: T-k-shaped T: T-shaped T-k) T: T-shaped T: T-k-shaped T: T-k-shaped T-k) T: T-shaped T-shaped T: T-k-shaped T: T-k) T-shaped T: T-k) T-shaped T-shaped T: T-k) T: T-shaped T: T: T-k) T-shaped T-shaped T: T-k) T-shaped T: T-k) T-shaped T: T: T-k) T-shaped T: T-k) T-shaped T: T-k) T-shaped T: T-shaped T: T-k) T-shaped T: T-k-shaped T: T-k) T-shaped T: T-shaped T: T-shaped T-k) T: T-shaped T: T: T-shaped T-k) T-shaped T-K) T-shaped T: T: T-shaped T-shaped T-K) T-shaped T: T: T-K) T: T-shaped T-K) T-shaped T: T: T-shaped T"}, {"heading": "B Experimental Results", "text": "B.1 RR-POMDPThe RR-POMDP parameters are: [m = 4 hidden states, n = 2 observations, k = 3 transition matrix rank].T\u03c0 = 0.7829 0.1036 0.0399 0.07360.1036 0.4237 0.4262 0.04650.0399 0.4262 (0.4380 0.0959 0.0736 0.0465 0.0959 0.7840 O = [1 0 1 00 1 0 1] The discount factor is \u03b3 = 0.9.B.2 Pricing a financial derivative basic functions The first 16 are the basic functions proposed by Van Roy (x); for full description and justification see [33, 34]. The first functions consist of a constant, the reward, the minimum and maximum return, and how long they have occurred: \u03c61 (x) = 1through2 (x) = G (x) \u03c63 (x) \u03c63 (x) = min i = 1,... x (100i) (max = 100x x) x (we are \u2212 through1 x x)."}], "references": [{"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Least-squares temporal difference learning", "author": ["Justin A. Boyan"], "venue": "In Proc. Intl. Conf. Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["Steven J. Bradtke", "Andrew G. Barto"], "venue": "In Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Least-squares policy iteration", "author": ["Michail G. Lagoudakis", "Ronald Parr"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Representation policy iteration", "author": ["Sridhar Mahadevan"], "venue": "In Proceedings of the Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Samuel meets amarel: automating value function approximation using global state space analysis", "author": ["Sridhar Mahadevan"], "venue": "Proceedings of the 20th national conference on Artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Compact spectral bases for value function approximation using kronecker factorization", "author": ["Jeff Johns", "Sridhar Mahadevan", "Chang Wang"], "venue": "Proceedings of the 22nd national conference on Artificial intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Optimal control of Markov decision processes with incomplete state estimation", "author": ["K.J. Astr\u00f6m"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1965}, {"title": "The Optimal Control of Partially Observable Markov Processes", "author": ["E.J. Sondik"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1971}, {"title": "Acting optimally in partially observable stochastic domains", "author": ["Anthony R. Cassandra", "Leslie P. Kaelbling", "Michael R. Littman"], "venue": "In Proc. AAAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Value-directed compression of pomdps", "author": ["Pascal Poupart", "Craig Boutilier"], "venue": "In NIPS, pages 1547\u20131554,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A novel orthogonal nmf-based belief compression for pomdps", "author": ["Xin Li", "William K.W. Cheung", "Jiming Liu", "Zhili Wu"], "venue": "Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Compressing pomdps using locality preserving non-negative matrix factorization", "author": ["Georgios Theocharous", "Sridhar Mahadevan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Predictive representations of state", "author": ["Michael Littman", "Richard Sutton", "Satinder Singh"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Predictive state representations: A new theory for modeling dynamical systems", "author": ["Satinder Singh", "Michael James", "Matthew Rudary"], "venue": "In Proc. UAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Observable operator models for discrete stochastic time series", "author": ["Herbert Jaeger"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Planning in pomdps using multiplicity automata", "author": ["Y. Eyal Even-dar"], "venue": "Proceedings of 21st Conference on Uncertainty in Artificial Intelligence (UAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J. Zico Kolter", "Andrew Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["Ronald Parr", "Lihong Li", "Gavin Taylor", "Christopher Painter-Wakefield", "Michael L. Littman"], "venue": "In ICML \u201908: Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Learning low dimensional predictive representations", "author": ["Matthew Rosencrantz", "Geoffrey J. Gordon", "Sebastian Thrun"], "venue": "In Proc. ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon"], "venue": "In Proceedings of Robotics: Science and Systems VI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Causality: models, reasoning, and inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Learning predictive state representations using non-blind policies", "author": ["Michael Bowling", "Peter McCracken", "Michael James", "James Neufeld", "Dana Wilkinson"], "venue": "In Proc. ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Multivariate Reduced-rank Regression: Theory and Applications", "author": ["Gregory C. Reinsel", "Rajabather Palani Velu"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Matrix Computations", "author": ["Gene H. Golub", "Charles F. Van Loan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "The most predictable criterion", "author": ["Harold Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1935}, {"title": "Dynamic data factorization", "author": ["S. Soatto", "A. Chiuso"], "venue": "Technical report,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Subspace Identification for Linear Systems: Theory, Implementation, Applications", "author": ["P. Van Overschee", "B. De Moor"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1996}, {"title": "Subspace Methods for System Identification", "author": ["Tohru Katayama"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham Kakade", "Tong Zhang"], "venue": "In COLT,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Improving approximate value iteration using memories and predictive state representations", "author": ["Michael R. James", "Ton Wessling", "Nikos A. Vlassis"], "venue": "In AAAI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Reduced-rank hidden Markov models", "author": ["Sajid Siddiqi", "Byron Boots", "Geoffrey J. Gordon"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Optimal stopping of markov processes: Hilbert space theory, approximation algorithms, and an application to pricing high-dimensional financial derivatives", "author": ["John N. Tsitsiklis", "Benjamin Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "A popular family of model-free algorithms called temporal difference (TD) algorithms [1] can then be used to estimate the parameters of the value function.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to find the optimal parameters in a least-squares sense from time-adjacent samples of features.", "startOffset": 35, "endOffset": 44}, {"referenceID": 2, "context": "Least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to find the optimal parameters in a least-squares sense from time-adjacent samples of features.", "startOffset": 35, "endOffset": 44}, {"referenceID": 3, "context": "Least-squares TD (LSTD) algorithms [2, 3, 4] exploit the linearity of the value function to find the optimal parameters in a least-squares sense from time-adjacent samples of features.", "startOffset": 35, "endOffset": 44}, {"referenceID": 4, "context": ", by using a spectral analysis of the state-space transition graph to discover a low-dimensional feature set that preserves the graph structure [5, 6, 7].", "startOffset": 144, "endOffset": 153}, {"referenceID": 5, "context": ", by using a spectral analysis of the state-space transition graph to discover a low-dimensional feature set that preserves the graph structure [5, 6, 7].", "startOffset": 144, "endOffset": 153}, {"referenceID": 6, "context": ", by using a spectral analysis of the state-space transition graph to discover a low-dimensional feature set that preserves the graph structure [5, 6, 7].", "startOffset": 144, "endOffset": 153}, {"referenceID": 7, "context": "Partially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable [8, 9, 10].", "startOffset": 125, "endOffset": 135}, {"referenceID": 8, "context": "Partially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable [8, 9, 10].", "startOffset": 125, "endOffset": 135}, {"referenceID": 9, "context": "Partially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable [8, 9, 10].", "startOffset": 125, "endOffset": 135}, {"referenceID": 10, "context": "Strategies for finding good projections include value-directed compression [11] and non-negative matrix factorization [12, 13].", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "Strategies for finding good projections include value-directed compression [11] and non-negative matrix factorization [12, 13].", "startOffset": 118, "endOffset": 126}, {"referenceID": 12, "context": "Strategies for finding good projections include value-directed compression [11] and non-negative matrix factorization [12, 13].", "startOffset": 118, "endOffset": 126}, {"referenceID": 13, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 81, "endOffset": 89}, {"referenceID": 14, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 81, "endOffset": 89}, {"referenceID": 15, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "The resulting model after compression is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a multiplicity automaton [17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 17, "context": "Kolter and Ng [18] contend with this problem from a sparse feature selection standpoint.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "looked at the problem of value function estimation from the perspective of both model-free and model-based reinforcement learning [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "demonstrated that these two approaches compute exactly the same value function [19], formalizing a fact that has been recognized to some degree before [2].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "demonstrated that these two approaches compute exactly the same value function [19], formalizing a fact that has been recognized to some degree before [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 18, "context": "Instead of learning a linear transition model from features, as in [19], we use subspace identification [20, 21] to learn a PSR from our samples.", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "Instead of learning a linear transition model from features, as in [19], we use subspace identification [20, 21] to learn a PSR from our samples.", "startOffset": 104, "endOffset": 112}, {"referenceID": 20, "context": "Instead of learning a linear transition model from features, as in [19], we use subspace identification [20, 21] to learn a PSR from our samples.", "startOffset": 104, "endOffset": 112}, {"referenceID": 18, "context": "This new approach has a substantial benefit: while the linear feature-to-feature transition model of [19] does not seem to have any common uses outside that paper, PSRs have been proposed numerous times on their own merits (including being invented independently at least three times), and are a strict generalization of POMDPs.", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Therefore our representation is naturally \u201ccompressed\u201d in the sense of [11], speeding up convergence.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "We start from a discrete time dynamical system with a set of states S, a set of actionsA, a distribution over initial states \u03c00, a state transition function T , a reward function R, and a discount factor \u03b3 \u2208 [0, 1].", "startOffset": 208, "endOffset": 214}, {"referenceID": 2, "context": "However, this solution is biased [3], since the independent variables \u03c6t \u2212 \u03b3\u03c6t+1 are noisy samples of the expected difference E[\u03c6H(h) \u2212 \u03b3 \u2211 o\u2208O \u03c6 H(h\u03c0o) Pr[h\u03c0o | h]].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "The quantity \u03c6t T can be viewed as an instrumental variable [3], i.", "startOffset": 60, "endOffset": 63}, {"referenceID": 17, "context": "However, with the exception of LARS-TD [18], there has been little work on the problem of how to select features automatically for value function approximation when the system model is unknown; and of course, manual feature selection depends on not-alwaysavailable expert guidance.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "aioi that can be executed and observed after time t [14].", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "oi, given that we intervene [22] to execute the test actions \u03c4 = a1 .", "startOffset": 28, "endOffset": 32}, {"referenceID": 3, "context": "For example, if \u03c41 and \u03c42 are two tests with \u03c4 1 = \u03c4 A 2 \u2261 \u03c4, The LSTD algorithm can also be theoretically justified as the result of an application of the Bellman operator followed by an orthogonal projection back onto the row space of \u03c6H [4].", "startOffset": 240, "endOffset": 243}, {"referenceID": 22, "context": "(With some additional bookkeeping we could remove this assumption [23], but this bookkeeping would unnecessarily complicate our derivations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "To find our predictive compression, we will use reduced-rank regression [24].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "Then we can find a predictive compression of histories by a singular value decomposition (SVD) of the weighted covariance: write UDV \u2248 \u03a3\u0302T ,HL H (7) for a truncated SVD [25] of the weighted covariance, where U are the left singular vectors, V are the right singular vectors, and D is the diagonal matrix of singular values.", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "2, scaling up future reward by a constant factor results in a value-directed compression\u2014but, unlike previous ways to find value-directed compressions [11], we do not need to know a model of our system ahead of time.", "startOffset": 151, "endOffset": 155}, {"referenceID": 25, "context": "This is equivalent to canonical correlation analysis [26, 27], and the matrix D becomes a diagonal matrix of canonical correlations between futures and histories.", "startOffset": 53, "endOffset": 61}, {"referenceID": 26, "context": "This is equivalent to canonical correlation analysis [26, 27], and the matrix D becomes a diagonal matrix of canonical correlations between futures and histories.", "startOffset": 53, "endOffset": 61}, {"referenceID": 19, "context": "Below we will show an additional benefit: the modelfree algorithm in Equation 13 is, under some circumstances, equivalent to a model-based value function approximation method which uses subspace identification to learn Predictive State Representations [20, 21].", "startOffset": 252, "endOffset": 260}, {"referenceID": 20, "context": "Below we will show an additional benefit: the modelfree algorithm in Equation 13 is, under some circumstances, equivalent to a model-based value function approximation method which uses subspace identification to learn Predictive State Representations [20, 21].", "startOffset": 252, "endOffset": 260}, {"referenceID": 13, "context": "A predictive state representation (PSR) [14] is a compact and complete description of a dynamical system.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Finally, a core set Q for a linear PSR is said to be minimal if the tests in Q are linearly independent [16, 15], i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 14, "context": "Finally, a core set Q for a linear PSR is said to be minimal if the tests in Q are linearly independent [16, 15], i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 0, "context": "In addition to the above PSR parameters, we need a few additional definitions for reinforcement learning: a reward function R(h) = \u03b7Q(h) mapping predictive states to immediate rewards, a discount factor \u03b3 \u2208 [0, 1] which weights the importance of future rewards vs.", "startOffset": 207, "endOffset": 213}, {"referenceID": 19, "context": ") Instead of ordinary PSRs, we will work with transformed PSRs (TPSRs) [20, 21].", "startOffset": 71, "endOffset": 79}, {"referenceID": 20, "context": ") Instead of ordinary PSRs, we will work with transformed PSRs (TPSRs) [20, 21].", "startOffset": 71, "endOffset": 79}, {"referenceID": 27, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 28, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 26, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 29, "context": "In this respect, TPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace identification [28, 29, 27, 30].", "startOffset": 128, "endOffset": 144}, {"referenceID": 30, "context": "For a fixed policy \u03c0, a TPSR\u2019s value function is a linear function of state, J(s) = wb, and is the solution of the TPSR Bellman equation [31]: for all b, wb = b\u03b7 b+ \u03b3 \u2211 o\u2208O w B\u03c0ob, or equivalently, w = b\u03b7 + \u03b3 \u2211", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "PSTD learning is related to value-directed compression of POMDPs [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "If we learn a TPSR from data generated by a POMDP, then the TPSR state is exactly a linear compression of the POMDP state [15, 20].", "startOffset": 122, "endOffset": 130}, {"referenceID": 19, "context": "If we learn a TPSR from data generated by a POMDP, then the TPSR state is exactly a linear compression of the POMDP state [15, 20].", "startOffset": 122, "endOffset": 130}, {"referenceID": 10, "context": "in the sense of Poupart and Boutilier [11].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "In fact, using theory developed in [21], it is possible to relax this assumption and instead assume that state is merely correlated with features of history.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "We can then estimate the TPSR transition matrices as B\u0302ao = \u00db\u03a3\u0302T ,ao,H(\u00db\u03a3\u0302T ,H) (see [21] for proof details).", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "Details on this procedure can be found in [21].", "startOffset": 42, "endOffset": 46}, {"referenceID": 31, "context": "We evaluate the PSTD learning algorithm on a synthetic example derived from [32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "The optimal threshold strategy (sell if price is above a threshold [33]) is in black, LSTD (16 canonical features) is in blue, LSTD (on the full 220 features) is cyan, LARS-TD (feature selection from set of 220) is in green, and PSTD (16 dimensions, compressing 220 features (16 + 204)) is in red.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": ") We consider the financial derivative introduced by Tsitsiklis and Van Roy [33].", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "This process is Markov and ergodic [33, 34]: xt and xt+100 are independent and identically distributed.", "startOffset": 35, "endOffset": 43}, {"referenceID": 32, "context": "For feature selection, we are fortunate: previous researchers have hand-selected a \u201cgood\u201d set of 16 features for this data set through repeated trial and error (see Appendix, Section B and [33, 34]).", "startOffset": 189, "endOffset": 197}, {"referenceID": 32, "context": "We compared PSTD (reducing 220 to 16 features) to LSTD with either the 16 hand-selected features or the full 220 features, as well as to LARS-TD (220 features) and to a simple thresholding strategy [33].", "startOffset": 198, "endOffset": 202}, {"referenceID": 32, "context": "In fact, PSTD performs better than the best previously reported approach [33, 34] by 1.", "startOffset": 73, "endOffset": 81}, {"referenceID": 32, "context": "For this reason, in real-world problems, a substantial amount of time is spent selecting a small set of features, often by trial and error [33, 34].", "startOffset": 139, "endOffset": 147}], "year": 2011, "abstractText": "We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.", "creator": "LaTeX with hyperref package"}}}