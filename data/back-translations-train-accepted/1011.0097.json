{"id": "1011.0097", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2010", "title": "Sparse Inverse Covariance Selection via Alternating Linearization Methods", "abstract": "Gaussian graphical models are of great interest in statistical learning. Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an $\\ell_1$-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem's special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an $\\epsilon$-optimal solution in $O(1/\\epsilon)$ iterations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms.", "histories": [["v1", "Sat, 30 Oct 2010 18:30:43 GMT  (185kb)", "http://arxiv.org/abs/1011.0097v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["katya scheinberg", "shiqian ma", "donald goldfarb"], "accepted": true, "id": "1011.0097"}, "pdf": {"name": "1011.0097.pdf", "metadata": {"source": "CRF", "title": "Sparse Inverse Covariance Selection via Alternating Linearization Methods", "authors": ["Katya Scheinberg"], "emails": ["katyas@lehigh.edu", "sm2756@columbia.edu", "goldfarb@columbia.edu"], "sections": [{"heading": null, "text": "ar Xiv: 101 1.00 97v1 [cs.LG] 3 0O ct"}, {"heading": "1 Introduction", "text": "In multivariate data analysis, the structure of this learning model is a real problem. < / p > p > p > p > p > p > p > p > p > p + p > p > p > p > p + p > p > p > p > p \"p\" p > p > p > p + p > p > p > p > p > p + p > p > p > p + p > p + p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p."}, {"heading": "2 Alternating Linearization Methods", "text": "Here we consider the alternating linearization method (ALM) for solving the following problem: min F (x), f (x), (4), where f and g are both convex functions. An effective method of solving (4) is to \"divide\" f and g by introducing a new variable, i.e. it must (4) rewrite (4), y {f (x) + g (y): x \u2212 y = 0}, (5) and apply an alternating method to them. In the face of a penalty parameter 1 / \u00b5, at k-th iteration, the augmented Lagrangian method minimizes the augmented functionL (x): = f (x) + g (y) \u2212 p: the penalty parameters 1 / \u00b5, at k-th iteration, the augmented Lagrangian method minimizes the augmented functionL (x)."}, {"heading": "3 ALM for SICS", "text": "The SICS problem of f (X) -Sn + F (X) + F (X) + G (X), (12), where f (X) = \u2212 log det (X) + < p (X), and g (X) = f (X), is of the same form as (4). However, in this case neither f (X) nor g (X) the SICS problem is particularly challenging for optimization methods. Nevertheless, we can still use (9) to solve the problem directly. Furthermore, we can apply algorithms 2 and get the complexity in Theorem 2.1 as a consequence. The protocol (X), which implicitly requires X (X) in f (X) and the gradients of f (X) given in f (X)."}, {"heading": "4 Numerical Experiments", "text": "In this section we present numerical results on both synthetic and real problems to demonstrate the efficiency of our SICS ALM algorithms. Our codes for ALM were written in MATLAB. \u2212 All numerical experiments were performed in MATLAB 7.3.0 on a Dell Precision 670 workstation with an Intel Xeon (TM) 3.4GHZ CPU and 6GB of RAM.Da \u2212 k \u2212 g (Y k), on a Dell Precision 670 workstation, so a workable solution to the dual problem (2) is as long as it is positively defined. \u2212 Since the duality gap in k-th iteration is given by: Dgap: = \u2212 log det (Xk) + <."}, {"heading": "4.1 Experiments on synthetic data", "text": "Similar methods were used by Wang et al. in [19] and Li and Toh in [8]. For a given dimension n, we first created a sparse matrix U-Rn \u00b7 n with unequal entries equal to -1 or 1 with equal probability. Then, we calculated S: = (U-U) \u2212 1 as the true covariance matrix. Therefore, S \u2212 1 was sparse. We then drew p = 5n iid vectors, Y1,., Yp, from the Gaussian distribution N (0, S) \u2212 1 using the mvnrnd function in MATLAB and calculated a sample covariance matrix."}, {"heading": "4.2 Experiments on real data", "text": "We tested the ALM using real data from gene expression networks using the five sets of data [8] provided by Kim-Chuan Toh: (1) lymph node status; (2) estrogen receptor; (3) Arabidopsis thaliana; (4) leukemia; (5) hereditary breast cancer. See [8] and the references contained therein for the description of these data sets. Table 2 presents our test results. As proposed in [8], we set \u03c1 = 0.5. From Table 2 we see that ALM is much faster and provides more accurate solutions than PSM and VSM."}, {"heading": "4.3 Solution Sparsity", "text": "In this section, we compare the splitting patterns of the solutions generated by ALM, PSM, and VSM. For ALM, the splitting number of the solution is given by the splitting number Y. Since PSM and VSM solve the dual problem, the primary solution X, obtained by reversing the dual solution W, is never sparse due to floating point errors. Therefore, it is not fair to measure the splitting number of X or an abbreviated version of X. Instead, we measure the splitting number of solutions generated by PSM and VSM by appealing to complementary negligence. Specifically, the (i, j) -th element of the inverse covariance matrix is considered unequal if and only if | Wij \u2212 \u0445ij | = \u03c1. We give results for a random problem (n = 500) and the first real data set in Table 3."}, {"heading": "Acknowledgements", "text": "We thank Professor Kim-Chuan Toh for providing the data set used in section 4.2. The research reported here was partially supported by NSF grants DMS 06-06712 and DMS 10-16571, ONR grants N00014-08-1-1118 and DOE grants DE-FG02-08ER25856."}, {"heading": "5 Appendix", "text": "(124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (124) (12"}], "references": [{"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Convex Analysis and Minimization Algorithms II: Advanced Theory and Bundle Methods", "author": ["J.-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "Model selection and estimation in the Gaussian graphical model", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "High-dimensional graphical model selection using l1regularized logistic regression", "author": ["M. Wainwright", "P. Ravikumar", "J. Lafferty"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian for binary data", "author": ["O. Banerjee", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "An inexact interior point method for l1-regularized sparse covariance selection", "author": ["L. Li", "K.-C. Toh"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc B.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Mining brain region connectivity for alzheimer\u2019s disease study via sparse inverse covariance estimation", "author": ["L. Sun", "R. Patel", "J. Liu", "K. Chen", "T. Wu", "J. Li", "E. Reiman", "J. Ye"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Sinco - a greedy coordinate ascent method for sparse inverse covariance selection problem", "author": ["K. Scheinberg", "I. Rish"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Projected subgradient methods for learning sparse Gaussian", "author": ["J. Duchi", "S. Gould", "D. Koller"], "venue": "Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Smooth minimization for non-smooth functions", "author": ["Y.E. Nesterov"], "venue": "Math. Program. Ser. A,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Introductory lectures on convex optimization", "author": ["Y.E. Nesterov"], "venue": "87:xviii+236,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "First-order methods for sparse covariance selection", "author": ["A. D\u2019Aspremont", "O. Banerjee", "L. El Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and its Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Z. Lu"], "venue": "SIAM J. Optim.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Alternating direction methods for sparse covariance selection", "author": ["X. Yuan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Solving log-determinant optimization problems by a Newton-CG primal proximal point algorithm", "author": ["C. Wang", "D. Sun", "K.-C. Toh"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Augmented Lagrangian methods: applications to the numerical solution of boundary-value problems", "author": ["M. Fortin", "R. Glowinski"], "venue": "North-Holland Pub. Co.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1983}, {"title": "Augmented Lagrangian and Operator-Splitting Methods in Nonlinear Mechanics", "author": ["R. Glowinski", "P. Le Tallec"], "venue": "SIAM, Philadelphia, Pennsylvania,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O(1/k)", "author": ["Y.E. Nesterov"], "venue": "Dokl. Akad. Nauk SSSR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1983}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": "SIAM J. Optim.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Fast alternating linearization methods for minimizing the sum of two convex functions", "author": ["D. Goldfarb", "S. Ma", "K. Scheinberg"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": ", the lack of an edge between i and j denotes the conditional independence of y and y, which corresponds to a zero entry in the inverse covariance matrix \u03a3 ([1]).", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "This problem is NP-hard in general due to the combinatorial nature of the cardinality term \u03c1\u2016X\u20160 ([2]).", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "To get a numerically tractable problem, one can replace the cardinality term \u2016X\u20160 by \u2016X\u20161 := \u2211 i,j |Xij |, the envelope of \u2016X\u20160 over the set {X \u2208 R : \u2016X\u2016\u221e \u2264 1} (see [3]).", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 4, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 5, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 6, "context": ", [4, 5, 6, 7]):", "startOffset": 2, "endOffset": 14}, {"referenceID": 7, "context": "Although an approximate IPM has recently been proposed for the SICS problem [8], most of the methods developed for it are first-order methods.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "[7] proposed a block coordinate descent (BCD) method to solve the dual problem (2).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] is based on the same BCD approach as in [7], but it solves each subproblem as a LASSO problem by yet another coordinate descent (CD) method [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[5] is based on the same BCD approach as in [7], but it solves each subproblem as a LASSO problem by yet another coordinate descent (CD) method [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "[5] is based on the same BCD approach as in [7], but it solves each subproblem as a LASSO problem by yet another coordinate descent (CD) method [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 9, "context": "[10] proposed solving the primal problem (1) by using a BCD method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "They formulate the subproblem as a min-max problem and solve it using a prox method proposed by Nemirovski [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The SINCO method proposed by Scheinberg and Rish [12] is a greedy CD method applied to the primal problem.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Variants of Nesterov\u2019s method [14, 15] have been applied to solve the SICS problem.", "startOffset": 30, "endOffset": 38}, {"referenceID": 14, "context": "Variants of Nesterov\u2019s method [14, 15] have been applied to solve the SICS problem.", "startOffset": 30, "endOffset": 38}, {"referenceID": 15, "context": "[16] applied Nesterov\u2019s optimal first-order method to solve the primal problem (1) after smoothing the nonsmooth l1 term, obtaining an iteration complexity bound of O(1/\u01eb) for an \u01eb-optimal solution, but the implementation in [16] was very slow and did not produce good results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] applied Nesterov\u2019s optimal first-order method to solve the primal problem (1) after smoothing the nonsmooth l1 term, obtaining an iteration complexity bound of O(1/\u01eb) for an \u01eb-optimal solution, but the implementation in [16] was very slow and did not produce good results.", "startOffset": 225, "endOffset": 229}, {"referenceID": 16, "context": "Lu [17] solved the dual problem (2), which is a smooth problem, by Nesterov\u2019s algorithm, and improved the iteration complexity to O(1/ \u221a \u01eb).", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Yuan [18] proposed an alternating direction method based on an augmented Lagrangian framework (see the ADAL method (8) below).", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "in [19] requires a reformulation of the problem that increases the size of the problem making it impractical for solving large-scale problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "The IPM in [8] also requires such a reformulation.", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "Although developed independently, our method is closely related to Yuan\u2019s method [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "[13] and the VSM algorithm proposed by Lu [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[13] and the VSM algorithm proposed by Lu [17].", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 12, "context": "Finally, we present some numerical results on both synthetic and real data in Section 4 and compare ALM with PSM algorithm [13] and VSM algorithm [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 16, "context": "Finally, we present some numerical results on both synthetic and real data in Section 4 and compare ALM with PSM algorithm [13] and VSM algorithm [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": ", [20, 21]):", "startOffset": 2, "endOffset": 10}, {"referenceID": 20, "context": ", [20, 21]):", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "Nesterov [15, 22] proved that one can obtain an optimal iteration complexity bound of O(1/ \u221a \u01eb), using only first-order information.", "startOffset": 9, "endOffset": 17}, {"referenceID": 21, "context": "Nesterov [15, 22] proved that one can obtain an optimal iteration complexity bound of O(1/ \u221a \u01eb), using only first-order information.", "startOffset": 9, "endOffset": 17}, {"referenceID": 22, "context": "This technique has been exploited and extended by Tseng [23], Beck and Teboulle [24], Goldfarb et al.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "This technique has been exploited and extended by Tseng [23], Beck and Teboulle [24], Goldfarb et al.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "[25] and many others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "1 in [17], the optimal solution of (12) X \u03b1I , where \u03b1 = 1 \u2016\u03a3\u0302\u2016+n\u03c1 .", "startOffset": 5, "endOffset": 9}, {"referenceID": 24, "context": "1 can then be applied as discussed in [25].", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "in [13] and implemented by Mark Schmidt 1 and the smoothing method (VSM) 2 proposed by Lu in [17], which are considered to be the state-of-the-art algorithms for solving SICS problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "in [13] and implemented by Mark Schmidt 1 and the smoothing method (VSM) 2 proposed by Lu in [17], which are considered to be the state-of-the-art algorithms for solving SICS problems.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "We randomly created test problems using a procedure proposed by Scheinberg and Rish in [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "in [19] and Li and Toh in [8].", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "in [19] and Li and Toh in [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "We compared ALM with PSM [13] and VSM [17] on these randomly created data with different \u03c1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 16, "context": "We compared ALM with PSM [13] and VSM [17] on these randomly created data with different \u03c1.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "We tested ALM on real data from gene expression networks using the five data sets from [8] provided to us by Kim-Chuan Toh: (1) Lymph node status; (2) Estrogen receptor; (3) Arabidopsis thaliana; (4) Leukemia; (5) Hereditary breast cancer.", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "See [8] and references therein for the descriptions of these data sets.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "As suggested in [8], we set \u03c1 = 0.", "startOffset": 16, "endOffset": 19}], "year": 2010, "abstractText": "Gaussian graphical models are of great interest in statistical learning. Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an l1-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem\u2019s special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an \u01eb-optimal solution in O(1/\u01eb) iterations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms.", "creator": "LaTeX with hyperref package"}}}