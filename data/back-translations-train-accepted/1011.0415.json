{"id": "1011.0415", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2010", "title": "Learning Networks of Stochastic Differential Equations", "abstract": "We consider linear models for stochastic dynamics. To any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. We tackle the problem of learning such a network from observation of the system trajectory over a time interval $T$.", "histories": [["v1", "Mon, 1 Nov 2010 19:09:57 GMT  (39kb)", "http://arxiv.org/abs/1011.0415v1", "This publication is to appear in NIPS 2010"]], "COMMENTS": "This publication is to appear in NIPS 2010", "reviews": [], "SUBJECTS": "math.ST cond-mat.stat-mech cs.IT cs.LG math.IT stat.TH", "authors": ["jos\u00e9 bento", "morteza ibrahimi", "andrea montanari"], "accepted": true, "id": "1011.0415"}, "pdf": {"name": "1011.0415.pdf", "metadata": {"source": "CRF", "title": "Learning Networks of Stochastic Differential Equations", "authors": ["Jos\u00e9 Bento", "Morteza Ibrahimi"], "emails": ["jbento@stanford.edu", "ibrahimi@stanford.edu", "montanari@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 101 1.04 15v1 [m atkeywords: Gaussian processes, model selection and structure learning, graphical models, sparsity and feature selection."}, {"heading": "1 Introduction and main results", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they"}, {"heading": "1.1 Results: Regularized least squares", "text": "We will redefine the relationships with the existing literature in Section 1.3.In the present case, the algorithm will redefine each and every line of the matrix A0. (1) The first line, A0r, is determined by solving the following convex optimization problems. (2) The second line, in which the probability function L is defined, is determined by solving the following convex optimization problems. (2) The second line, in which the probability function L is defined. (3) (3) (3) The second line, in which we specify the transpose of the matrix / vector M. (3) To see that this probability function is actually related to the least quartits, it can be formally written. (3)"}, {"heading": "1.2 Overview of other results", "text": "So far, we have focused on continuity time dynamics. Although this is useful to obtain elegant statements, much of the work is actually devoted to the analysis of the following discrete time dynamics with the parameter \u03b7 > 0: x (t) = x (t \u2212 1) + \u03b7A0x (t \u2212 1) + w (t), t N0. (6) Here x (t) \u0394Rp is the vector collecting the dynamic variables, A0 Rp \u00d7 p indicates the dynamics as above, and {w (t)} t \u2265 0 is a sequence of i.e. normal vectors with covariance \u03b7 Ip \u00d7 p (i.e. with independent variance components). We assume that successive samples {x (t)} 0 \u2264 t \u2264 n are given and ask under which conditions the least square shapes are reconstructed."}, {"heading": "1.3 Related work", "text": "A considerable part of the work has been devoted to the analysis of \"1 regulated smallest squares\" and their variants [6, 7, 8, 9, 10]. The most closely related results are those on high-dimensional consistency in support of restoration [11, 12]. In fact, our evidence follows the line of work developed in this work, with two important challenges. Firstly, the design matrix in our case is produced by stochastic diffusion, and it does not necessarily meet the conditions of inrepresentation used by these works. Secondly, the observations are not corrupted by i.i.d. noise (since successive configurations are correlated) and therefore elementary concentration imbalances are not sufficient. Learning sparse graphic models of regulation is also an issue with significant literature. In the Gaussian case, the graphic LASSO is proposed for the reconstruction of the model from i.i.d. samples."}, {"heading": "2 Illustration of the main results", "text": "It might be difficult to get a clear intuition of theorem 1.1, mainly due to the conditions (a) and (b) that introduce the parameters Cmin and \u03b1. The same difficulty arises with analog results for the high-dimensional consistency of LASSO [11, 12]. In this section we offer concrete illustrations both by numerical simulations and by checking the condition for certain classes of graphs."}, {"heading": "2.1 Learning the laplacian of graphs with bounded degree", "text": "In view of a simple diagram G = (V, E) on the vertex V = [p], its laplactic \u2206 G is the symmetrical p \u00b7 p matrix corresponding to the adjacence matrix of G outside the diagonal, and with the entries \u2206 Gii = \u2212 deg (i) on the diagonal [18]. (Here deg (i) denotes the degree of the vertex i.) It is generally known that \u2206 G is negative semidefinitive, with a eigenvalue equal to 0, the multiplicity of which is the number of connected components of G. The matrix A0 = \u2212 mI + \u2206 G fits into the setting of theorem 1,1 for m > 0. The corresponding model (1,1) describes the overdamped dynamics of a network of masses connected to the origin by springs of unity strength and by a spring strength m."}, {"heading": "2.2 Numerical illustrations", "text": "In this section we present the numerical validation of the proposed method using synthetic data. Results confirm our observations in theorems 1.1 and 3.1, including that the time complexity scales logarithmically with the number of nodes in the network p, giving a constant maximum success. Furthermore, the time complexity is approximately independent of the sampling rate. In Figures 1 and 2, we look at the discrete time setting and generate data as follows. We draw A0 as a random sparse matrix in {0, 1} p with elements that are selected independently of the sampling rate (A0ij = 1) = k / p, k = 5. The process xn0 \u2261 {x (t)} 0 \u2264 t \u2264 n is then generated according to Equation (6). We solve the regularized least square problem (the cost function is explicitly given in equality x. (8) for the discrete time case."}, {"heading": "3 Discrete-time model: Statement of the results", "text": "Consider a system that develops in discrete time according to the model (6), and let xn0 \u2261 {x (t)} 0 \u2264 t \u2264 n be the observed part of the orbital path. Rth line A0r is estimated by minimizing the following convex optimization problem for Ar \u0432 Rpminimizing L (Ar; x; n) max. (8) whereL (Ar; x n 0) \u2261 12\u03b72nn \u2212 1 \u2211 t = 0 {xr (t + 1) - xr (t) \u2212 \u03b7 A \u0445 rx (t) \u00b2 2, (9) Apart from an additive constant, whereby the \u03b7 \u2192 0 limit of this cost function may coincide with the cost function in perpetual time, cf. Eq. In fact, the proof for Theorem 1.1 amounts to a more accurate version of this statement."}, {"heading": "4 Proofs", "text": "In the following, we refer to the matrix whose (t + 1) th column corresponds to the configuration x (t), i.e. X = [x (0), x (1),.., x (n \u2212 1). Furthermore, we write W = [w (1),.., w (n \u2212 1)] for the matrix containing the Gaussian noise alignment. Equivalent is W = \u0445 X \u2212 \u03b7AX. The smallest line W is replaced by Wr.To simplify the notation, we omit the reference to xn0 in the probability function (9) and simply write L (Ar). We define its normalized course and Hessian G by G. To simplify the notation, we will omit the reference to xn0 in the probability function (9) and the reference to L (Ar)."}, {"heading": "4.1 Discrete time", "text": "In this section, we will outline our proof of our main result for discrete time dynamics, i.e. for the validity of these conditions, and finally, we will outline the sketch of the proof. As already mentioned, the evidence strategy, and in particular the following proposal, which provides a compact set of sufficient conditions for the support to be correctly restored, is analogous to that in [12]. Proof for this assertion can be found in the supplementary material. Proposal 4.1. Let \u03b1, Cmin > 0 to min (Q 0 S0, S0) \u2261 Cmin, | | Q0 (S0) C, S0 (Q0) C, S0 (Q0S0, S0) - 1 \u2212 \u03b1."}, {"heading": "4.1.1 Technical lemmas", "text": "In this section we will specify the concentration dilemmas required for the detection of theorem 3.1. These are not trivial because G, Q and Q2 are quadratic functions of dependent random variables (the samples {x (t)} 0 \u2264 t \u2264 n). The evidence for Proposition 4.2, from Proposition 4.3, and Corollary 4.4 can be found in the supplementary material provision. Our first proposition implies concentration of G in the order of 0. Proposition 4.2."}, {"heading": "4.1.2 Outline of the proof of Theorem 3.1", "text": "All we have to do is calculate the probability that the conditions given in sentence 4.1 apply. To set the first condition to G, we assume that the first two conditions (\u03b1, Cmin > 0) of sentence 4.1 apply. (20) We also combine the last two conditions to G, resulting in the following: | | Q [p], S0 \u2212 Q0 [p], S0 | | GOP \u2264 12 Cmin \u221a k, (21) da [p] = S0 (S0) C. We then determine that both the probability of failure of the condition to Q and the probability of failure of the condition to G are limited by Proposition 4.2 if this condition is met (4.11)."}, {"heading": "4.2 Outline of the proof of Theorem 1.1", "text": "To prove theory 1.1, remember that Proposition 4.1 uses the appropriate continuous time expressions for Q (browser) and Q (browser), namelyG (browser) = \u2212 L (A0r) = 1T (T0x (t) dbr (t), Q (2L (A0r) = 1T (T0x) x (t). (22) These are, of course, random variables. To distinguish them from the discrete time version, we assume the notation G (n), Q (n) n for the latter. We claim that these random variables (t) can be coupled (i.e. defined on the same probability range) so that G (n) and Q (n) are almost certainly fixed T. Assuming (5) it is easy to show that (11) the variables for all n > n0 and n0 are a sufficiently large constant (for proof)."}, {"heading": "Acknowledgments", "text": "This work was partially supported by a Terman scholarship, the NSF-CAREER prize CCF-0743978 and the NSF scholarship DMS-0806211 as well as a Portuguese doctoral fellowship FCT."}, {"heading": "A Learning networks of stochastic differential equations: Supplementary materials", "text": "To prove Proposition 4.1, we first establish two technical lemmas.Lemma A.1. For each subset of Q (...), the following decomposition (...) | | p | | p | p (...) is the following decomposition (... | p | p (...). \u2212 p (...) \u2212 p (...) \u2212 p (...) \u2212 p (...) \u2212 p (...) \u2212 p (...) p (...) p \u2212 p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p (...) p) p (...) p (...) p (...) p (...) p (...) p)."}], "references": [], "referenceMentions": [], "year": 2010, "abstractText": "We consider linear models for stochastic dynamics. To any such model can be as-<lb>sociated a network (namely a directed graph) describing which degrees of freedom<lb>interact under the dynamics. We tackle the problem of learning such a network<lb>from observation of the system trajectory over a time interval T .<lb>We analyze the l1-regularized least squares algorithm and, in the setting in which<lb>the underlying network is sparse, we prove performance guarantees that are uni-<lb>form in the sampling rate as long as this is sufficiently high. This result substan-<lb>tiates the notion of a well defined \u2018time complexity\u2019 for the network inference<lb>problem. keywords: Gaussian processes, model selection and structure learning, graphical models, sparsity<lb>and feature selection.", "creator": "LaTeX with hyperref package"}}}