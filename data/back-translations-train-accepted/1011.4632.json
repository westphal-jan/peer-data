{"id": "1011.4632", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2010", "title": "Random Projections for $k$-means Clustering", "abstract": "This paper discusses the topic of dimensionality reduction for $k$-means clustering. We prove that any set of $n$ points in $d$ dimensions (rows in a matrix $A \\in \\RR^{n \\times d}$) can be projected into $t = \\Omega(k / \\eps^2)$ dimensions, for any $\\eps \\in (0,1/3)$, in $O(n d \\lceil \\eps^{-2} k/ \\log(d) \\rceil )$ time, such that with constant probability the optimal $k$-partition of the point set is preserved within a factor of $2+\\eps$. The projection is done by post-multiplying $A$ with a $d \\times t$ random matrix $R$ having entries $+1/\\sqrt{t}$ or $-1/\\sqrt{t}$ with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.", "histories": [["v1", "Sun, 21 Nov 2010 02:37:10 GMT  (52kb)", "http://arxiv.org/abs/1011.4632v1", "Neural Information Processing Systems (NIPS) 2010"]], "COMMENTS": "Neural Information Processing Systems (NIPS) 2010", "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["christos boutsidis", "anastasios zouzias", "petros drineas"], "accepted": true, "id": "1011.4632"}, "pdf": {"name": "1011.4632.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 101 1.46 32v1 [cs.AI] 21 Nov 201 0 \u221a t or \u2212 1 / \u221a t with equal probability. A numerical implementation of our technique and experiments on a large facial image dataset verify the speed and accuracy of our theoretical results."}, {"heading": "1 Introduction", "text": "The results of this work focus on the application of the random projection method (see Section 2.3) to the k-mean clustering problem (see Definition 1). Formally, it is assumed that a set of n points in d dimensions in d dimensions, our goal is to randomly project the points in d dimensions into d dimensions, and then apply a k-mean clustering problem (see Definition 2). Of course, one should be able to compile the projection quickly without significantly distorting the \"clusters\" of the original point (see Algorithm 1)."}, {"heading": "2 Preliminaries", "text": "We start with the formal definition of the k-mean cluster problem using the matrix notation. Later in this section, we describe precisely the approximability framework applied in the k-mean cluster literature and fix the notation. Definition 1. [THE K-MEANS CLUSTERING PROBLEM] Given a number of n points in the d dimensions (lines in a n \u00b7 d matrix A) and a positive integer k denoting the number of clusters, you will find the n \u00b7 k indicator matrix Xopt such thatXopt = arg min X-X-X. [Xopt] A \u2212 XX-M indicator matrices X \u2212 X. The functional F (A, X) is the X-X-X-X algorithm 2F-X denoting the so-called K mean objective function. An n \u00b7 k indicator matrix has exactly one non-zero element per line, the problem j is the point-j denoting the point-member-j."}, {"heading": "2.2 Notation", "text": "Considering a n \u00b7 d matrix A and a whole k with k < min {n, d}, we let Uk-Rn \u00b7 k (resp. Vk-Rd \u00b7 k) be the matrix of the upper k left (resp. Right) singular vectors of A, and let the k-Rk-k be a diagonal matrix containing the topical singular values of A in no increasing order. If we allow an index to take i values in the series {1,..., n}, we write i-k equal A \u2212 Ak, with Ak = UkkV k. With A (i) we denote the i-th row of A. For an index i taking values in the series {1,...) we denote i-k in no increasing order the non-negative singular values of A and the non-negative singular values of A (A) with i (A)."}, {"heading": "2.3 Random Projections", "text": "A classic result of Johnson and Lindenstrauss is that any n-point in d-dimensions - rows in a matrix A-Rn-d - can be projected linearly into t-dimensions (log (n) / \u03b52), while pairs of distances within a factor of 1 \u00b1 \u03b5 can be taken into account by means of a random orthonormal matrix [12]. Subsequent studies simplified the proof for the above result by showing that such a projection can be generated with a d-t-random Gaussian matrix R, i.e. a matrix whose inputs are generally Gaussian random variables with zero mean and variance 1 / \u221a t [11]. Specifically, the following inequality holds with high probability over the randomness of R, (1 \u2212 \u03b5) an indix matrix number, i.e., a matrix number (i) \u2212 A (j) \u2212 algorithm-A means that we get the algorithm-structure A-2, which means that the algorithm-A-structure is true."}, {"heading": "3.1 Running time analysis", "text": "Algorithm 1 reduces the dimensions of A by post-multiplication with a random character matrix R. Interestingly, any \"random projection matrix\" R that respects the properties of Lemma 2 with t = 1 (k / \u03b52) can be used in this step. If R is constructed as in Algorithm 1, one can use the so-called Mailman algorithm for matrix multiplication [15] and calculate the produktAR in O (nd - 2k / log (d) time. In fact, the Mailman algorithm (after pre-processing 1) calculates a matrix vector product of any d-dimensional vector (series of A) and calculates the produktAR in O (d) character matrix. By partitioning the columns of our d \u00b7 t matrix R into t / log (d) blocks, the claim follows: If k = O-time (d), then we get a linear reduction."}, {"heading": "4 Main Theorem", "text": "Theorem 1 is our main result in terms of approximate quality for algorithm 1. Note that if \u03b3 = 1, i.e. if the k mean problem is exactly solved with the inputs A and k, algorithm 1 guarantees a distortion of at most 2 + \u03b5 as advertised. Theorem 1. Let the n \u00b7 d matrix A and the positive integer k < min {n, d} be the inputs of the k mean cluster problem. Leave \u03b5 (0, 1 / 3) and access a \u03b3 approximation k mean algorithm. Start algorithm 1 with inputs."}, {"heading": "A, k, \u03b5, and the \u03b3-approximation algorithm in order to construct an indicator matrix X\u03b3\u0303 . Then with probability at", "text": "The proof for theorem 1 of [19] we must ensure that the matrix R is constructed in algorithm 1 in accordance with definition 1 and lemmas 5 in [19]. Theorem 1.1 of [1] immediately shows that the random character matrix R of the algorithms 1 and 5 in [19].lemmas 5 in [19].Assume that the matrix R is constructed is constructed by using algorithm 1 with the inputs A, k and 1. Singular Values 2 in [19].lemmas 5 in [19].lemmas 2. Assume that the matrix R is constructed."}, {"heading": "5 Experiments", "text": "This section describes an empirical evaluation of algorithm 1 on a facial image collection. We implemented our algorithm in MatLab and compared it with other known dimensionality reduction techniques such as the Local Linear Embedding (LLE) algorithm and the Laplacian values for feature selection. We conducted all the experiments on a Mac machine with a 2.26 Ghz dual-core processor and 4 GB of RAM. Our empirical results are promising and suggest that our algorithm and implementation could be very useful in real-world applications where large-scale data is bundled."}, {"heading": "5.1 An application of Algorithm 1 on a face images collection", "text": "We experiment with a Facebook collection, with an integer k = 40, we initialize the clusters with 91. This collection contains 400 facebooks of the dimensions 64 \u00d7 64, corresponding to 40 different people. These images form 40 groups, each containing exactly 10 different images of the same person. After vectorizing each 2-D image and placing it as a line vector in an appropriate matrix, one can construct a 400-4096 picture-by-pixel matrix A. In this matrix, objects are the facebooks of the ORL collection, while the pixel values of the images are used as a line vector in an appropriate matrix. To apply Lloyd's heuristics to A, we use the function of the matrix, which determines the parameters of the maximum number of repetitions. We also chose a deterrent initialization of the iterative E-M procedure, i.e. whenever we call kputs with matrix A."}, {"heading": "5.2 A note on the mailman algorithm for matrix-matrix and matrix-vector multiplication", "text": "In this section, we compare three different implementations of the third step of algorithm 1. As we discussed in Section 3.1, the Mailman algorithm is asymptotically faster than the naive multiplication of the two matrices A and R. In this section, we want to understand whether this asymptotic behavior of the Mailman algorithm is actually achieved in practical implementation. We compare three different approaches to implementing the third step of our algorithm: the first is the functional time of MatLab (A, R) (MM1); the second takes advantage of the fact that we do not need to explicitly store the entire matrix R and that computing can be done on the fly (column by column) (MM2); the last is the Mailman algorithm [15] (see Section 3.1 for more details). We implemented the last two algorithms in C using the Matrix-M4X technology."}], "references": [{"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "In ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "NP-hardness of Euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "In ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Unsupervised feature selection for the k-means clustering problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Clustering in large graphs and matrices", "author": ["P. Drineas", "A. Frieze", "R. Kannan", "S. Vempala", "V. Vinay"], "venue": "In ACM- SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "An optimal set of discriminant vectors", "author": ["D. Foley", "J. Sammon"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1975}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Result analysis of the NIPS 2003 feature selection challenge", "author": ["I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1984}, {"title": "A simple linear time (1+\u03b5)-approximation algorithm for k-means clustering in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "The Mailman algorithm: A note on matrix-vector multiplication", "author": ["E. Liberty", "S. Zucker"], "venue": "Information Processing Letters,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1982}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, 290:5500,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T. Sarlos"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu"], "venue": "Knowledge and Information Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 8, "context": "We believe that the high dimensionality of modern data will render our algorithm useful and attractive in many practical applications [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Description Dimensions Time Accuracy 1999 [6] SVD - feature extraction k O(ndmin{n, d}) 2 - Folklore RP - feature extraction \u03a9(log(n)/\u03b5) O(nd\u2308\u03b5\u22122 log(n)/ log(d)\u2309) 1 + \u03b5 2009 [5] SVD - feature selection \u03a9(k log(k/\u03b5)/\u03b5) O(ndmin{n, d}) 2 + \u03b5 2010 This paper RP - feature extraction \u03a9(k/\u03b5) O(nd\u2308\u03b5\u22122k/ log(d)\u2309) 2 + \u03b5", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Description Dimensions Time Accuracy 1999 [6] SVD - feature extraction k O(ndmin{n, d}) 2 - Folklore RP - feature extraction \u03a9(log(n)/\u03b5) O(nd\u2308\u03b5\u22122 log(n)/ log(d)\u2309) 1 + \u03b5 2009 [5] SVD - feature selection \u03a9(k log(k/\u03b5)/\u03b5) O(ndmin{n, d}) 2 + \u03b5 2010 This paper RP - feature extraction \u03a9(k/\u03b5) O(nd\u2308\u03b5\u22122k/ log(d)\u2309) 2 + \u03b5", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).", "startOffset": 180, "endOffset": 187}, {"referenceID": 2, "context": "Finding Xopt is an NP-hard problem even for k = 2 [3], thus research has focused on developing approximation algorithms for k-means clustering.", "startOffset": 50, "endOffset": 53}, {"referenceID": 12, "context": "For our discussion, we fix the \u03b3-approximation algorithm to be the one presented in [14], which guarantees \u03b3 = 1+ \u03b5 for any \u03b5 \u2208 (0, 1] with running time O(2(k/\u03b5\u2032)O(1)dn).", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "A classical result of Johnson and Lindenstrauss states that any n-point set in d dimensions - rows in a matrix A \u2208 R - can be linearly projected into t = \u03a9(log(n)/\u03b5) dimensions while preserving pairwise distances within a factor of 1\u00b1\u03b5 using a random orthonormal matrix [12].", "startOffset": 270, "endOffset": 274}, {"referenceID": 10, "context": "Gaussian random variables with zero mean and variance 1/ \u221a t [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Achlioptas proved that even a (rescaled) random sign matrix suffices in order to get the same guarantees as above [1], an approach that we adopt here (see step two in Algorithm 1).", "startOffset": 114, "endOffset": 117}, {"referenceID": 13, "context": "If R is constructed as in Algorithm 1, one can employ the so-called mailman algorithm for matrix multiplication [15] and", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Using, for example, the algorithm of [14] with \u03b3 = 1 + \u03b5 would result in an algorithm that preserves the clustering within a factor of 2 + \u03b5, for any \u03b5 \u2208 (0, 1/3), running in time O(nd\u2308\u03b5\u22122k/ log(d)\u2309 + 2(k/\u03b5)O(1)kn/\u03b52).", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well.", "startOffset": 40, "endOffset": 48}, {"referenceID": 15, "context": "In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well.", "startOffset": 40, "endOffset": 48}, {"referenceID": 17, "context": "The proof of Theorem 1 employs several results from [19] including Lemma 6, 8 and Corollary 11.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab\u2019s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation.", "startOffset": 302, "endOffset": 306}, {"referenceID": 9, "context": "In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab\u2019s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation.", "startOffset": 478, "endOffset": 482}, {"referenceID": 13, "context": "We compare three different approaches for the implementation of the third step of our algorithm: the first is MatLab\u2019s function times(A,R) (MM1); the second exploits the fact that we do not need to explicitly store the whole matrix R, and that the computation can be performed on the fly (column-by-column) (MM2); the last is the mailman algorithm [15] (see Section 3.", "startOffset": 348, "endOffset": 352}, {"referenceID": 13, "context": "We observed that when A is a vector (n = 1), then the mailman algorithm is indeed faster than (MM1) and (MM2) as it is also observed in the numerical experiments of [15].", "startOffset": 165, "endOffset": 169}], "year": 2013, "abstractText": "This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A \u2208 R) can be projected into t = \u03a9(k/\u03b5) dimensions, for any \u03b5 \u2208 (0, 1/3), in O(nd\u2308\u03b5\u22122k/ log(d)\u2309) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + \u03b5. The projection is done by post-multiplying A with a d \u00d7 t random matrix R having entries +1/ \u221a t or \u22121/ \u221a t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.", "creator": "LaTeX with hyperref package"}}}