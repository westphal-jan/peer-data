{"id": "1011.5053", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2010", "title": "Tight Sample Complexity of Large-Margin Learning", "abstract": "We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L_2 regularization: We introduce the \\gamma-adapted-dimension, which is a simple function of the spectrum of a distribution's covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the \\gamma-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions.", "histories": [["v1", "Tue, 23 Nov 2010 10:44:21 GMT  (89kb,S)", "https://arxiv.org/abs/1011.5053v1", "Appearing in Neural Information Processing Systems (NIPS) 2010; This is the full version, including appendix with proofs"], ["v2", "Thu, 5 Apr 2012 16:40:03 GMT  (29kb)", "http://arxiv.org/abs/1011.5053v2", "Appearing in Neural Information Processing Systems (NIPS) 2010; This is the full version, including appendix with proofs; Also with some corrections"]], "COMMENTS": "Appearing in Neural Information Processing Systems (NIPS) 2010; This is the full version, including appendix with proofs", "reviews": [], "SUBJECTS": "cs.LG math.PR math.ST stat.ML stat.TH", "authors": ["sivan sabato", "nathan srebro", "naftali tishby"], "accepted": true, "id": "1011.5053"}, "pdf": {"name": "1011.5053.pdf", "metadata": {"source": "CRF", "title": "Tight Sample Complexity of Large-Margin Learning", "authors": ["Sivan Sabato", "Nathan Srebro", "Naftali Tishby"], "emails": ["sabato@cs.huji.ac.il,", "tishby@cs.huji.ac.il,", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 101 1.50 53v2 [cs.LG] 5 Apr 2We obtain a strict distribution-specific characterization of the sample complexity of the large margin classification with L2 regularization: We introduce the \u03b3-adapted dimension, which is a simple function of the spectrum of the covariance matrix of a distribution, and show distribution-specific upper and lower limits of sample complexity, both regulated by the \u03b3-adapted dimension of the source distribution. We conclude that this new set closely characterizes the true sample complexity of the large margin classification."}, {"heading": "1 Introduction", "text": "In this paper, we address the problem of obtaining a narrow characterization of the sample-like complexity required by a certain learning rule to learn a certain source distribution. (D, Y) Most learning theorists work on a strict characterization of the sample-like complexity required for a large (Euclidean) margin to obtain a low error for a certain distribution. (D, Y) Most learning theory work focuses on the upper distribution level, which is highly likely to achieve a higher distribution rate. (D, E) Evidence that when using a particular learning rule, if the sample size is at least m (D, E), an excessive error of at most two distribution steps can be ensured (in anticipation or with high probability). (D, E) We know that it is necessary for a large margin classification if PD [D, Y] then m (D, E) can be set."}, {"heading": "Related work", "text": "Most work on \"lower limits of sample complexity\" aims to prove that there is a source distribution under a number of assumptions for which at least a certain number of examples are needed to learn with the required error and confidence [4, 5, 6]. However, this kind of lower limit does not say much about the sample complexity of other distributions under the same set of assumptions. As for distribution-specific lower limits, the classical analysis of Vapnik [7, Theorem 16.6] provides not only sufficient but also necessary conditions for the learning ability of a sample class in relation to a specific distribution. The essential condition is that the entropy of the mortgage class in relation to distribution is sublinear within the limit of an infinite sample size. In a sense, this criterion can be regarded as providing a \"lower limit\" of learning capacity for a particular distribution."}, {"heading": "2 Problem setting and definitions", "text": "We are interested in linear separators, parameterized by standard vectors in Bd1, {w, Rd, 2 \u2264 1}. For a predictive value w stands for its error of classification in relation to the distribution D by (w, D), P (X, Y), D [Y < w, X > \u2264 0]. For a predictive value w stands for the loss of w in relation to D by a sample (w, D), P (X, Y), D [Y < w, X > \u2264 0]. The minimum predictive loss in relation to D is represented by a loss in relation to the predictive value w (w, D). For a sample S = (xi, yi) in relation to the predictive and forecast loss in relation to the predictive (D)."}, {"heading": "Sub-Gaussian distributions", "text": "We will characterize the distribution-specific sample complexity using the covariance of X \u0445 DX (sub-Gaussian 1 = sub-Gaussian B), but in order to do so, we must assume that X is not too cumbersome. Otherwise, X may even have an infinite covariance, but can still be learnable, for example, if it has a tiny chance of having an exponentially large norm. We will therefore limit ourselves to sub-Gaussian distributions, which ensures that light tails can be rotated in all directions while a sufficiently rich family of distributions is possible, as we currently see. We also need a more restrictive condition - namely that DX can be rotated to a product distribution over the axes of Rd. A distribution can always be rotated so that its coordinates are uncorrelated. Here, we continue to demand that they be independent, as is natural for any multivariate Gaussian distribution.12 Definition. [See 2.12]"}, {"heading": "3 The \u03b3-adapted-dimension", "text": "As mentioned in the introduction, the sample complexity of margin error minimization in relation to the average standard E [2] is limited by m (3, 3, D) \u2264 O (E [3, 2] / (4, 2]). Alternatively, we can only rely on dimensionality and conclude on dimensionality (4, 5, D) \u2264 O (d / 2) [7]. Thus, although both of these limits are narrow in the worst-case sense, i.e. they are the best limits based only on the standard or only on dimensionality, neither is narrow in a distribution-specific sense: if the average standard is unlimited, an arbitrarily large gap is created between the true m (4, D) and the average standard upper limit. Converses occur when dimensionality is arbitrarily high while the average standard is limited."}, {"heading": "4 A sample complexity upper bound using \u03b3-adapted-dimension", "text": "To establish an upper limit of sample complexity, we will limit the bold-splintering dimension of linear functions by a set (in relation to the \u03b3-matched dimension of the set). Let's remember that the bold-splintering dimension is a classic quantity for proving sample complexity of upper limits: Definition 4.1. Let F have a set of functions f: X \u2192 R, and allow the set [x1] -P (f) -X-X to be smashed by F if it is r1,., rm, R that there is a set for all y ranges. (\u00b1 1) m that there is an f-series of F so that it [m], yi-i (f) -ri) that the coarse-bold-splintering dimension of F is the size of the largest group in X that is smashed by F."}, {"heading": "5 Sample complexity lower bounds using Gram-matrix eigenvalues", "text": "To do this, we will link the ability to learn with the properties of the data distribution. (...) The ability to learn is closely linked to the probability of a sample that needs to be fragmented, as Vapnik's formulations show. (...) For the lower limit, we use the fact presented in Theorem 5.1 that a sample can be fragmented with a relatively high probability. (...) We then refer the fat fragmentation of a sample to the minimal eigenvalue of its gram matrix. (...) This allows us to present a lower binding to the lower limit of sample complexity using a lower value. (...) We refer the fat fragmentation of a sample to the minimal eigenvalue of its gram matrix."}, {"heading": "6 A lower bound for independently sub-Gaussian distributions", "text": "The distribution of this eigenvalue has been studied under various assumptions: the cleanest results are in the case where m, d \u2192 \u221e and md \u2192 \u03b2 < 1, and the coordinates of each example are identically distributed: Theorem 6.1 (Theorem 5.11 in [18]). Let Xi be a series of mi \u00d7 di matrices whose entries are i.e. random variables with mean zero, variance and finite fourth moments."}, {"heading": "7 Summary and consequences", "text": "The result is that the actual complexity of learning each of these distributions is characterized by the actual complexity of the individual distributions. (2) An interesting conclusion can be drawn as an influence of the conditional distribution of labels DY. (2) This result shows that the actual complexity of learning characterizes each of these distributions. (2) We have drawn an interesting conclusion from the fact that the conditional distribution of labels DY | X: Da Eq. (2) we consider for each DY | X, the effects of the direction of the best divider on the sound complexity of learning itself to be highly non-spherical distributions."}, {"heading": "Acknowledgments", "text": "The authors thank Boaz Nadler for many insightful discussions and Karthik Sridharan for pointing this out [14]. Sivan Sabato is supported by the Adams Fellowship Program of the Israeli Academy of Sciences. This work was supported by the NATO SfP scholarship 982480."}, {"heading": "A Proofs for \u201cTight Sample Complexity of Large-Margin Learning\u201d (S. Sabato, N. Srebro and N. Tishby)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Lemma 3.3", "text": "For the other difference, it should first be noted that we EX-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D"}, {"heading": "A.3 Proof of Theorem 4.4", "text": "Proof of the theory 4.4. Allow \u03a3 = Diag (\u03bb1,.., \u03bbd) to be the covariance matrix of DX, at the maximum permissible number of samples (max.) 1 x (max.) 2 x (max.) 2 x (max.) 2 x (max.) 2 x (max.) 2 x (max.) 2 x (max.) 2 x (max.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.) 2 x (.). (.) (. (.) (. (.) 2 x (.). (.) (. (.) (. (.) (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (. (.). (.). (. (.). (. (.). (.). (. (.). (. (.). (. (.). (.). (. (.).). (. (.). (. (.). (.). (.). (. (.).). (. (. (.).). (. (.). (.). (.).). (. (.). (. (.). (.).). (. (. (.). (.). (..................................................... (. (.). (. (. (.). (.). (.). (. (.).). (. (..)..............................................."}, {"heading": "A.4 Proof of Theorem 5.2", "text": "The following problem, which allows the conversion of the representation of the gram matrix into another attribute space while the separation properties remain intact. (For a matrix M, M + +) this means its pseudo-inverse. (M) is invertable then M + = (B) \u2212 1B \u00b2.Lemma A.3. Let X be a m \u00b7 d matrix so that XX \u00b2 is invertable, and Y so that XX \u00b2 = Y \u00b2. (Y) Rm is any real vector, so that Y w = r, then there is a vector w = r, so that there is a vector w that is Xw = r and vice versa, where P = Y \u00b2 Y \u00b2 (Y Y \u00b2) \u2212 1Y \u00b2 is the projection matrix on the sub-space. We have the row of Y.Proof. Denote K = XX \u00b2."}, {"heading": "A.5 Proof of Theorem 6.2", "text": "In the proof of theorem 6.2 we use the fact \u03bbm (XX) = + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +"}, {"heading": "A.6 Lemma A.9", "text": "Let X-Rd be a random vector and let B be a PSD matrix, so that for all u-Rd, E [exp (< u, V >). \u2212 exp (< Bu, u > / 2). Then it is enough to look at diagonal moment matrices: If B is not diagonal, let V-Rd \u00b7 d be an orthogonal matrix, so that V BV \u00b2 is diagonal, and let Y = V \u00b2. We have E [exp (t \u00b2 Y \u00b2)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We obtain a tight distribution-specific characterization of the sample complex-<lb>ity of large-margin classification with L2 regularization: We introduce the<lb>\u03b3-adapted-dimension, which is a simple function of the spectrum of a distribu-<lb>tion\u2019s covariance matrix, and show distribution-specific upper and lower bounds<lb>on the sample complexity, both governed by the \u03b3-adapted-dimension of the<lb>source distribution. We conclude that this new quantity tightly characterizes the<lb>true sample complexity of large-margin classification. The bounds hold for a rich<lb>family of sub-Gaussian distributions.", "creator": "LaTeX with hyperref package"}}}