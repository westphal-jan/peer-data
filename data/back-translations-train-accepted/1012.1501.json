{"id": "1012.1501", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2010", "title": "Shaping Level Sets with Submodular Functions", "abstract": "We consider a class of sparsity-inducing regularization terms based on submodular functions. While earlier work has focused on non-decreasing functions, we explore symmetric submodular functions and their Lovasz extensions. We show that the Lovasz extension may be seen as the convex envelope of a function that depends on level sets: this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets, and not on the supports of the underlying predictors. We provide a unified set of optimization algorithms (such as proximal operators), and theoretical guarantees (allowed level sets and recovery conditions). By selecting specific submodular functions, we give a new interpretation to known norms, such as the total variation; we also define new norms, in particular ones that are based on order statistics, and on noisy cuts in graphs.", "histories": [["v1", "Tue, 7 Dec 2010 13:34:44 GMT  (100kb)", "https://arxiv.org/abs/1012.1501v1", null], ["v2", "Fri, 10 Jun 2011 14:12:14 GMT  (83kb)", "http://arxiv.org/abs/1012.1501v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["francis r bach"], "accepted": true, "id": "1012.1501"}, "pdf": {"name": "1012.1501.pdf", "metadata": {"source": "CRF", "title": "Shaping Level Sets with Submodular Functions", "authors": ["Francis Bach"], "emails": ["francis.bach@ens.fr"], "sections": [{"heading": null, "text": "ar Xiv: 101 2.15 01v2 [cs.LG] 1 0"}, {"heading": "1 Introduction", "text": "In the context of statistics, signal processing, or machine learning, it can take several forms. Classically, in a variable or feature-selection problem, a sparse solution with many zeros is sought, so that the model is either easier to interpret, cheaper to use, or easily available prior knowledge (see, for example, [1, 2, 3] and references in it). In this paper, we instead look at economy-inducing regulation terms that lead to solutions with many of the same values. A classic example is total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and which can be applied to various image-labeling problems [6, 5] or change-point recognition tasks [7, 8, 9]. Another example is the \"Oscar\" penalty, which induces an automatic grouping of features [10]."}, {"heading": "2 Review of Submodular Analysis", "text": "In this section we look at relevant results from submodular analyses, for more details see, for example, [12], and, for a check with evidence from classical convex analyses, see, for example, [11].F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > F (A) > B) > F (A) > B (A) > F (A) > F (A).F) > F (A) > B (A)."}, {"heading": "3 Properties of the Lova\u0301sz Extension", "text": "In this section we deduce the properties of a particular composition, which depends on the individual functions that go beyond convexity and homogeneity. (D) We assume that F (1) is a non-negative, submodular function, equal to zero in F (V) = 0. This directly implies that f (1) is an immutable case [3], our regulators are not norms. However, they are norms at the hyperlevel {w). (A) 6 = 6 = V, F (A) > 0 (which we assume for the rest of this paper).We now show that the Lova-sz expansion is a certain composition that depends on a certain function that depends on all sets. (A) > 0 (which we assume for the rest of this paper).We assume that the Lova expansion depends."}, {"heading": "4 Examples of Submodular Functions", "text": "In this section, we provide examples of submodular functions and their Lova \"sz extensions\" (we are). Some are known (such as intersection functions and total variations), others are new in the context of supervised learning (regular functions), while some have interesting effects in terms of clustering or outlier detection (such as the cartel-based functions). Symmetrication. of any submodular function G, F (A) + G (V) \u2212 G (V) can be defined as being symmetrical. Potentially interesting examples that go beyond the scope of this paper are mutual information, or functions of the submatrices. Cut functions that define non-negative weights d: V \u00b7 V +, we define the intersection F (A) = categorically interesting examples that go beyond the scope of this paper are mutual information, or functions of the submatrices."}, {"heading": "5 Optimization Algorithms", "text": "In this context, we should also mention the fact that the data that have surfaced in recent years in the USA are data that is widespread in the USA and in other countries of the world. (...) In the USA it is also the case that there has been a tightening of laws in recent years. (...) In the USA it is the case that there has been a tightening of laws in recent years. (...) In the USA it is the case that there has been a tightening of laws in the USA. (...) In the USA it is the case that there has been a tightening of laws. (...) In the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA and in the"}, {"heading": "6 Sparsity-inducing Properties", "text": "In this section we consider only the analysis in the context of the orthogonal design matrices, often referred to as the denoting problem, and provide guarantees in connection with the restoration of the level sets, which already lead to interesting results. We begin with the characterization of the permitted level sets, which show that the partial constraints defined in section 3 on the faces of {f) 6 do not arise from random additional groups of variables (see evidence in supplementary material). Proposition 6 (Stable constant sets) Assume z has an absolutely continuous density in relation to the Lebesgue measurement."}, {"heading": "7 Conclusion", "text": "We have presented a family of sparsity-inducing norms dedicated to the inclusion of prior knowledge or structural constraints at the level of linear predictors. We have provided a set of common algorithms and theoretical results, as well as simulations of synthetic examples illustrating the behavior of these norms. There are several possibilities worth investigating: First, we could follow current practice in sparse methods, e.g. by considering related adapted concave penalties to improve sparsity-inducing capabilities, or by expanding some of the concepts for norms of matrices, with possible applications in matrix factorization [24] or multi-task learning [25]."}, {"heading": "Acknowledgements", "text": "This article was partially supported by the Agence Nationale de la Recherche (MGA project), the European Research Council (SIERRA project) and Digiteo (BIOVIZ project)."}, {"heading": "A Proof of Proposition 1", "text": "(1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1.).). (1. (1.).). (1. (1.).). (1.). (1.).). (1.). (1.). (1.). (1.). (1.).). (1. (1.).).). (1. (1.).). (1.).). (1.). (1.). (1.). (1.). (1.). (1.). (1.).). (1.).). (1.).). (1.). (1.). (1.).). (1.).). (1.). (1.).).). (1"}, {"heading": "B Proof of Proposition 2", "text": "The detection of extreme U-points corresponds to the full-dimensional faces of B (F). Starting from sequence 3.4.4 in [12], these facets are exactly those that correspond to sets A with the given conditions. These facets are defined as intersections of {s (A) = F (A)} and {s (V) = F (V)}, which leads to the desired result. Note that this is also a sequence of provost. 3. Note that if F is symmetrical, the second condition is equivalent to V\\ A being inseparable from F."}, {"heading": "C Proof of Proposition 3", "text": "Proof Since the polyhedra U and B (F) are polar to each other [13], the theorem 3.43 in [12] follows, where each of our three assumptions corresponds to a corresponding one in theorem 3.43 from [12]."}, {"heading": "D Proof of Proposition 4", "text": "We start with a problem that follows a constant recovery (we assume a certain thrift pattern and check whether it is actually optimal): Lemma 1 (optimality of the bar for a proximal problem) -1 (M'm) -1 (M'm) -1 (M'm) -1 (M'm) -1 (F) -1 (M'm) -1 (F) -1 (M'm) -1 (F) -1 (F) -1 (M'm) -1 (F) -1 (F's) -1 (M'm) -1) -1 (M'm) -1 (M'm) -1 (M'm) -1 (M'm) -1 (M'm) -1 (M'm) -1 (M'm) -1 (F's) -1) -1 (M'm) -1 (M) -1 (M) -1 (M) -1 (M) -1 M (M) -2 (M) -2 (M (M) -2 (M) -2 (M) (M (M) -2 (M) -2 (M) -2 (M (M) -2 (M) -2 (M) -1 (M (M) -1 (M) -1 (M) -1 (M) -1 (M (M) -1 (M) -1 (M) -1 (M (M) -1 (M) -1 (M) -1 (M (M) -1 (M) -1 (M) -1 (M (M) -1 (M) -1 (M (M) -1 (M) -1 (M) -1 (M) -1 (M (M) -1 (M) -1 (M) -1 (M (M) -1 (M) -1 (M) -1 (M (M) -1 (M) -1 (M (M) -1 (M) -1 (M) -1 (M) -1 (M) -1 (M) -1 (M) -1 (M (M) -1 (M (M) -1 (M (M) -1 (M) -1 (M) -1 (M (M) -1"}, {"heading": "E Proof of Proposition 5", "text": "Proof We use w to denote the unique mininizer of 12, w, z, 22, f, w and the associated dual vector in B, F. The optimal conditions are w, z, s, 0, and f, w, s (again from the optimal conditions for meaningful maxima).We assume that w is unique values v1,..., vm in sets A1,... We define t as tk = character (wk) (wk) + (this is the unique minimizer of 12, w, 22, t, t, 1).The constant quantities of t are Aj, for j, so that | vj, and zero for the union of all Aj, so that | vj, 6, t is achieved by the soft threshold w, which corresponds to the 1-proximal problem."}, {"heading": "F Proof of Proposition 6", "text": "Proof Of Lemma 1 the solution must correspond to a grid D and we only have to show that the vector v = (M M) \u2212 1 (M z \u2212 \u03bbt) has with probability one unambiguous component, which is simple, because it has an absolutely continuous density compared to the Lebesgue measure."}, {"heading": "G Proof of Theorem 1", "text": "The second condition in Eq. (6) is fulfilled by acceptance, while the first condition leads to the sufficient conditions. (M'M) \u2212 1M (M'M) 6 (M'M) 6 (M'M) 6 (M'M) (M'M) 6 (M'M) (M'M) \u2212 1t (M'M) 6 (M'M) 6 (M'M) (M'M) 6 (M'M) 6 (M'M) 6 (M'M) 6 (M'M) -4 (M). (6) The second condition in Eq. (6) is fulfilled by acceptance, while the first condition leads to the sufficient conditions."}], "references": [{"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research, 7:2541\u20132563", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Adv. NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured sparsity-inducing norms through submodular functions", "author": ["F. Bach"], "venue": "Adv. NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparsity and smoothness via the fused Lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "J. Roy. Stat. Soc. B, 67(1):91\u2013108", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "On total variation minimization and surface evolution using parametric maximum flows", "author": ["A. Chambolle", "J. Darbon"], "venue": "International Journal of Computer Vision, 84(3):288\u2013307", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Trans. PAMI, 23(11):1222\u20131239", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Catching change-points with Lasso", "author": ["Z. Harchaoui", "C. L\u00e9vy-Leduc"], "venue": "Adv. NIPS, 20", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast detection of multiple change-points shared by many signals using group LARS", "author": ["J.-P. Vert", "K. Bleakley"], "venue": "Adv. NIPS, 23", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparsistent learning of varying-coefficient models with structural changes", "author": ["M. Kolar", "L. Song", "E. Xing"], "venue": "Adv. NIPS, 22", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Simultaneous regression shrinkage", "author": ["H.D. Bondell", "B.J. Reich"], "venue": "variable selection, and supervised clustering of predictors with oscar. Biometrics, 64(1):115\u2013123", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex analysis and optimization with submodular functions: a tutorial", "author": ["F. Bach"], "venue": "Technical Report 00527714, HAL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Elsevier", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton University Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Clusterpath: an algorithm for clustering using convex fusion penalties", "author": ["T. Hocking", "A. Joulin", "F. Bach", "J.-P. Vert"], "venue": "Proc. ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, 2(1):183\u2013202", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Two algorithms for maximizing a separable concave function over a polymatroid feasible region", "author": ["H. Groenevelt"], "venue": "European Journal of Operational Research, 54(2):227\u2013236", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematical Programming, 118(2):237\u2013251", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Minimizing symmetric submodular functions", "author": ["M. Queyranne"], "venue": "Mathematical Programming, 82(1):3\u201312", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["G. Gallo", "M.D. Grigoriadis", "R.E. Tarjan"], "venue": "SIAM Journal on Computing, 18(1):30\u201355", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "A path algorithm for the fused Lasso signal approximator", "author": ["H. Hoefling"], "venue": "Technical Report 0910.0526v1, arXiv", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research, 11:19\u201360", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Properties and refinements of the fused Lasso", "author": ["A. Rinaldo"], "venue": "Ann. Stat., 37(5):2922\u20132952", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "The TVL1 model: A geometric point of view", "author": ["V. Duval", "J.-F. Aujol", "Y. Gousseau"], "venue": "Multiscale Modeling and Simulation, 8(1):154\u2013189", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "Adv. NIPS 17", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, 73(3):243\u2013272", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2, 3] and references therein).", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": ", [1, 2, 3] and references therein).", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ", [1, 2, 3] and references therein).", "startOffset": 2, "endOffset": 11}, {"referenceID": 3, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 4, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 113, "endOffset": 119}, {"referenceID": 5, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 175, "endOffset": 181}, {"referenceID": 4, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 175, "endOffset": 181}, {"referenceID": 6, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 215, "endOffset": 224}, {"referenceID": 7, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 215, "endOffset": 224}, {"referenceID": 8, "context": "A classical example is the total variation in one or two dimensions, which leads to piecewise constant solutions [4, 5] and can be applied to various image labelling problems [6, 5], or change point detection tasks [7, 8, 9].", "startOffset": 215, "endOffset": 224}, {"referenceID": 9, "context": "Another example is the \u201cOscar\u201d penalty which induces automatic grouping of the features [10].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "In this paper, we follow the approach of [3], who designed sparsity-inducing norms based on non-decreasing submodular functions, as a convex approximation to imposing a specific prior on the supports of the predictors.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": ", [11] and a short review in Section 2)", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "\u2212 We provide unified algorithms in Section 5, such as proximal operators, which are based on a sequence of submodular function minimizations (SFMs), when such SFMs are efficient, or by adapting the generic slower approach of [3] otherwise.", "startOffset": 225, "endOffset": 228}, {"referenceID": 0, "context": "\u2212 We derive unified theoretical guarantees for level set recovery in Section 6, showing that even in the absence of correlation between predictors, level set recovery is not always guaranteed, a situation which is to be contrasted with traditional support recovery situations [1, 3].", "startOffset": 276, "endOffset": 282}, {"referenceID": 2, "context": "\u2212 We derive unified theoretical guarantees for level set recovery in Section 6, showing that even in the absence of correlation between predictors, level set recovery is not always guaranteed, a situation which is to be contrasted with traditional support recovery situations [1, 3].", "startOffset": 276, "endOffset": 282}, {"referenceID": 11, "context": ", [12], and, for a review with proofs derived from classical convex analysis, see, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [11] for this particular formulation).", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "We denote by B(F ) = {s \u2208 R, \u2200A \u2282 V, s(A) 6 F (A), s(V ) = F (V )} the base polyhedron [12], where we use the notation s(A) = \u2211", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "One important result in submodular analysis is that if F is a submodular function, then we have a representation of f as a maximum of linear functions [12, 11], i.", "startOffset": 151, "endOffset": 159}, {"referenceID": 10, "context": "One important result in submodular analysis is that if F is a submodular function, then we have a representation of f as a maximum of linear functions [12, 11], i.", "startOffset": 151, "endOffset": 159}, {"referenceID": 12, "context": ", [13] for definitions and properties of polar sets).", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": ", if A and B are tight, then so are A \u222a B and A \u2229 B [12, 11].", "startOffset": 52, "endOffset": 60}, {"referenceID": 10, "context": ", if A and B are tight, then so are A \u222a B and A \u2229 B [12, 11].", "startOffset": 52, "endOffset": 60}, {"referenceID": 2, "context": "Thus, contrary to the non-decreasing case [3], our regularizers are not norms.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "We now show that the Lov\u00e1sz extension is the convex envelope of a certain combinatorial function which does depend on all levets sets {w > \u03b1} of w \u2208 R (see proof in supplementary material): Proposition 1 (Convex envelope) The Lov\u00e1sz extension f(w) is the convex envelope of the function w 7\u2192 max\u03b1\u2208R F ({w > \u03b1}) on the set [0, 1] + R1V = {w \u2208 R, maxk\u2208V wk \u2212mink\u2208V wk 6 1}.", "startOffset": 322, "endOffset": 328}, {"referenceID": 2, "context": "Note the difference with the result of [3]: we consider here a different set on which we compute the convex envelope ([0, 1]+R1V instead of [\u22121, 1]), and not a function of the support of w, but of all its level sets.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "Note the difference with the result of [3]: we consider here a different set on which we compute the convex envelope ([0, 1]+R1V instead of [\u22121, 1]), and not a function of the support of w, but of all its level sets.", "startOffset": 118, "endOffset": 124}, {"referenceID": 11, "context": "of J , then it has to be in J (see [12] for more details, and an example in Figure 2).", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "Potentially interesting examples which are beyond the scope of this paper are mutual information, or functions of eigenvalues of submatrices [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "In Figure 5 (right plot), we give an example of the usual chain graph, leading to the one-dimensional total variation [4, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 4, "context": "In Figure 5 (right plot), we give an example of the usual chain graph, leading to the one-dimensional total variation [4, 5].", "startOffset": 118, "endOffset": 124}, {"referenceID": 5, "context": "Note that these functions can be extended to cuts in hypergraphs, which may have interesting applications in computer vision [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "By partial minimization, we obtain so-called regular functions [6, 5].", "startOffset": 63, "endOffset": 69}, {"referenceID": 4, "context": "By partial minimization, we obtain so-called regular functions [6, 5].", "startOffset": 63, "endOffset": 69}, {"referenceID": 2, "context": "While these examples do not provide significantly different behaviors for the non-decreasing submodular functions explored by [3] (i.", "startOffset": 126, "endOffset": 129}, {"referenceID": 13, "context": "This function has been extended in [14] by considering situations where each wj is a vector, instead of a scalar, and replacing the absolute value |wi \u2212 wj | by any norm \u2016wi \u2212 wj\u2016, leading to convex formulations for clustering.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "This may have applications to multivariate outlier detection by considering extensions similar to [14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": ", [15]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [15]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "In this paper, we use the method \u201cISTA\u201d and its accelerated variant \u201cFISTA\u201d [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "More precisely, the minimum of A 7\u2192 \u03bbF (A)\u2212 z(A) may be obtained by selecting negative components of the solution of a single proximal problem [12, 11].", "startOffset": 143, "endOffset": 151}, {"referenceID": 10, "context": "More precisely, the minimum of A 7\u2192 \u03bbF (A)\u2212 z(A) may be obtained by selecting negative components of the solution of a single proximal problem [12, 11].", "startOffset": 143, "endOffset": 151}, {"referenceID": 15, "context": "Alternatively, the solution of the proximal problem may be obtained by a sequence of at most p submodular function minimizations of the form A 7\u2192 \u03bbF (A) \u2212 z(A), by a decomposition algorithm adapted from [16], and described in [11].", "startOffset": 203, "endOffset": 207}, {"referenceID": 10, "context": "Alternatively, the solution of the proximal problem may be obtained by a sequence of at most p submodular function minimizations of the form A 7\u2192 \u03bbF (A) \u2212 z(A), by a decomposition algorithm adapted from [16], and described in [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 16, "context": "However, it may be too slow for practical purposes, as the best generic algorithm has complexity O(p) [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": ", [6, 5]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 4, "context": ", [6, 5]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 18, "context": "For proximal methods, we need in fact to solve an instance of a parametric max-flow problem, which may be done using other efficient dedicated algorithms [19, 5] than the decomposition algorithm derived from [16].", "startOffset": 154, "endOffset": 161}, {"referenceID": 4, "context": "For proximal methods, we need in fact to solve an instance of a parametric max-flow problem, which may be done using other efficient dedicated algorithms [19, 5] than the decomposition algorithm derived from [16].", "startOffset": 154, "endOffset": 161}, {"referenceID": 15, "context": "For proximal methods, we need in fact to solve an instance of a parametric max-flow problem, which may be done using other efficient dedicated algorithms [19, 5] than the decomposition algorithm derived from [16].", "startOffset": 208, "endOffset": 212}, {"referenceID": 2, "context": ", beyond cuts and cardinality-based functions), we can follow [3]: since f(w) is expressed as a minimum of linear functions, the problem reduces to the projection on the polytope B(F ), for which we happen to be able to easily maximize linear functions (using the greedy algorithm described in Section 2).", "startOffset": 62, "endOffset": 65}, {"referenceID": 11, "context": "This can be tackled efficiently by the minimum-norm-point algorithm [12], which iterates between orthogonal projections on affine subspaces and the greedy algorithm for the submodular function.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "Note that even in the case of symmetric submodular functions, where more efficient algorithms in O(p) for submodular function minimization (SFM) exist [18], the minimization of functions of the form \u03bbF (A) \u2212 z(A) is provably as hard as general SFM [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 17, "context": "Note that even in the case of symmetric submodular functions, where more efficient algorithms in O(p) for submodular function minimization (SFM) exist [18], the minimization of functions of the form \u03bbF (A) \u2212 z(A) is provably as hard as general SFM [18].", "startOffset": 248, "endOffset": 252}, {"referenceID": 11, "context": "Interestingly, when used for submodular function minimization (SFM), the minimum-norm-point algorithm has no complexity bound but is empirically faster than algorithms with such bounds [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 6, "context": "4 are satisfied by (a) all submodular set-functions that only depend on the cardinality, and (b) by the one-dimensional total variation\u2014we thus recover and extend known results from [7, 20, 14].", "startOffset": 182, "endOffset": 193}, {"referenceID": 19, "context": "4 are satisfied by (a) all submodular set-functions that only depend on the cardinality, and (b) by the one-dimensional total variation\u2014we thus recover and extend known results from [7, 20, 14].", "startOffset": 182, "endOffset": 193}, {"referenceID": 13, "context": "4 are satisfied by (a) all submodular set-functions that only depend on the cardinality, and (b) by the one-dimensional total variation\u2014we thus recover and extend known results from [7, 20, 14].", "startOffset": 182, "endOffset": 193}, {"referenceID": 3, "context": "Following [4], we may add the l1-norm \u2016w\u20161 for additional sparsity of w (on top of shaping its level sets).", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "The following proposition extends the result for the one-dimensional total variation [4, 21] to all submodular functions and their Lov\u00e1sz extensions: Proposition 5 (Proximal problem for l1-penalized problems) The unique minimizer of 1 2\u2016w\u2212 z\u20162+ f(w) +\u03bb\u2016w\u20161 may be obtained by soft-thresholding the minimizers of 12\u2016w\u2212 z\u20162+ f(w).", "startOffset": 85, "endOffset": 92}, {"referenceID": 20, "context": "The following proposition extends the result for the one-dimensional total variation [4, 21] to all submodular functions and their Lov\u00e1sz extensions: Proposition 5 (Proximal problem for l1-penalized problems) The unique minimizer of 1 2\u2016w\u2212 z\u20162+ f(w) +\u03bb\u2016w\u20161 may be obtained by soft-thresholding the minimizers of 12\u2016w\u2212 z\u20162+ f(w).", "startOffset": 85, "endOffset": 92}, {"referenceID": 0, "context": "(3) is the equivalent of the support recovery of the Lasso [1] or its extensions [3].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "(3) is the equivalent of the support recovery of the Lasso [1] or its extensions [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 21, "context": "Note that we could also derive general results when an additional l1-penalty is used, thus extending results from [22].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": ", [23] and the supplementary material).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "This indicates that the noise variance \u03c3 should be small compared to 1/p, which is not satisfactory and would be corrected with the weighting schemes proposed in [14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 23, "context": ", by considering related adapted concave penalties to enhance sparsity-inducing capabilities, or by extending some of the concepts for norms of matrices, with potential applications in matrix factorization [24] or multi-task learning [25].", "startOffset": 206, "endOffset": 210}, {"referenceID": 24, "context": ", by considering related adapted concave penalties to enhance sparsity-inducing capabilities, or by extending some of the concepts for norms of matrices, with potential applications in matrix factorization [24] or multi-task learning [25].", "startOffset": 234, "endOffset": 238}, {"referenceID": 25, "context": ", [26, 27] for definitions and properties of Fenchel conjugates).", "startOffset": 2, "endOffset": 10}, {"referenceID": 26, "context": ", [26, 27] for definitions and properties of Fenchel conjugates).", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": "Let s \u2208 R; we consider the function g : w 7\u2192 max\u03b1\u2208R F ({w > \u03b1}), and we compute its Fenchel conjugate: g(s) def = max w\u2208[0,1]+R1V ws\u2212 g(w),", "startOffset": 120, "endOffset": 125}, {"referenceID": 0, "context": "Moreover, we have, since f is invariant by adding constants and f is submodular, max w\u2208[0,1]+R1V ws\u2212 f(w) = \u03b9s(V )=0(s) + max w\u2208[0,1]p {ws\u2212 f(w)} = \u03b9s(V )=0(s) + max A\u2282V {s(A)\u2212 F (A)} = h(s), where we have used the fact that minimizing a submodular function is equivalent to minimizing its Lov\u00e1sz extension on the unit hypercube.", "startOffset": 87, "endOffset": 92}, {"referenceID": 0, "context": "Moreover, we have, since f is invariant by adding constants and f is submodular, max w\u2208[0,1]+R1V ws\u2212 f(w) = \u03b9s(V )=0(s) + max w\u2208[0,1]p {ws\u2212 f(w)} = \u03b9s(V )=0(s) + max A\u2282V {s(A)\u2212 F (A)} = h(s), where we have used the fact that minimizing a submodular function is equivalent to minimizing its Lov\u00e1sz extension on the unit hypercube.", "startOffset": 128, "endOffset": 133}, {"referenceID": 25, "context": "The result follows from the convexity of f , using the fact the convex envelope is the Fenchel bi-conjugate [26, 27].", "startOffset": 108, "endOffset": 116}, {"referenceID": 26, "context": "The result follows from the convexity of f , using the fact the convex envelope is the Fenchel bi-conjugate [26, 27].", "startOffset": 108, "endOffset": 116}, {"referenceID": 11, "context": "4 in [12], these facets are exactly the ones that correspond to sets A with the given conditions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "C Proof of Proposition 3 Proof Given that the polyhedra U and B(F ) are polar to each other [13], the proposition follows from Theorem 3.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "43 in [12], where each of our three assumptions are equivalent to a corresponding one in Theorem 3.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "43 from [12].", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "(1) are that w\u2212z+\u03bbs = 0, for s \u2208 B(F ) and f(w) = ws (these are obtained from general optimality conditions for functions defined as pointwise maxima [27]).", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "10 in [11], for (b) to be valid, s \u2208 B(F ) simply has to satisfy s(A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai) = F (A1 \u222a \u00b7 \u00b7 \u00b7 \u222a Ai) for all i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Thus, for any set C, we have for \u03bb > \u03bc (which implies \u03bc\u03bb \u2208 [0, 1]),", "startOffset": 59, "endOffset": 65}, {"referenceID": 10, "context": ", it is obtained by grouping some values of w), with no change of ordering [11].", "startOffset": 75, "endOffset": 79}], "year": 2011, "abstractText": "We consider a class of sparsity-inducing regularization terms based on submodular functions. While previous work has focused on non-decreasing functions, we explore symmetric submodular functions and their Lov\u00e1sz extensions. We show that the Lov\u00e1sz extension may be seen as the convex envelope of a function that depends on level sets (i.e., the set of indices whose corresponding components of the underlying predictor are greater than a given constant): this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets, and not only on the supports of the underlying predictors. We provide a unified set of optimization algorithms, such as proximal operators, and theoretical guarantees (allowed level sets and recovery conditions). By selecting specific submodular functions, we give a new interpretation to known norms, such as the total variation; we also define new norms, in particular ones that are based on order statistics with application to clustering and outlier detection, and on noisy cuts in graphs with application to change point detection in the presence of outliers.", "creator": "LaTeX with hyperref package"}}}