{"id": "1102.0836", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2011", "title": "EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning", "abstract": "It is a challenging task to select correlated variables in a high dimensional space. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not explicitly use correlation information embedded in data to select correlated variables. To overcome this limitation, we present a novel Bayesian hybrid model, the \\eig, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We reparameterize the hybrid model in the eigenspace to avoid overfiting and to increase the computational efficiency of its MCMC sampler. Furthermore, we provide an alternative view to the \\eig from a regularization perspective: the \\eig has an adaptive eigenspace-based composite regularizer, which naturally generalizes the $l_{1/2}$ regularizer used by the elastic net. Experiments on synthetic and real data show that the \\eig significantly outperforms the lasso, the elastic net, and the Bayesian lasso in terms of prediction accuracy, especially when the number of training samples is smaller than the number of variables.", "histories": [["v1", "Fri, 4 Feb 2011 04:40:07 GMT  (104kb,D)", "https://arxiv.org/abs/1102.0836v1", null], ["v2", "Tue, 8 Feb 2011 04:03:50 GMT  (104kb,D)", "http://arxiv.org/abs/1102.0836v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuan qi", "feng yan 0003"], "accepted": true, "id": "1102.0836"}, "pdf": {"name": "1102.0836.pdf", "metadata": {"source": "CRF", "title": "EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning", "authors": ["Yuan Qi", "Feng Yan"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2 Background: lasso and elastic net", "text": "We designate n independent and identically distributed samples asD = {(x1, y1),.., (xn, yn)}, where xi is a p-dimensional input characteristic (i.e., explanatory variables) and yi is a scalar label (i.e., answer). We also designate [x1,.., xn] after X and (y1,.., yn) after y. In this paper we consider the problem of binary classification (yi, \u2212 1,}), but our analysis and the proposed models can be extended to regression and other problems. To classify, we use a logistic function as a data probability function: p (y, w, b) = [yi (w Txi + b)))), (1), where the penalties (z) = 11 + exemplary variables (\u2212 z) and w and b are the classification variables."}, {"heading": "3 EigenNet: eigenstructure-guided variable se-", "text": "In this section, we propose to use covariance structures in data to guide the sparse estimation of model parameters. First, consider the following toy examples."}, {"heading": "3.1 Toy examples", "text": "Figure 1 (a) shows samples from two classes. Clearly, the variables x1 and x2 do not correlate. The lasso or the elastic mesh can successfully select the relevant variable x1 to classify the data. For the samples in Figure 1 (b), the variables x1 and x2 are strongly correlated. Despite the strong correlation, the lasso would only select x1 and ignore x2. However, the elastic mesh can select both x1 and x2 if the regulatory weight \u03bb1 is small and \u03bb2 large, so that the elastic mesh behaves like a regulated classifier. However, the elastic mesh does not examine the fact that x1 and x2 are correlated. As the eigenstructure of the data covariance matrix captures correlation information between variables, we are not only focused on regulating the classifier in such a way that it is economical, but also encouraging it to align it with specific eigenvectors (our new meshes), as we are helpful to the eigenfixing task in the 1 and eigenfixing the eigenfixes in the model."}, {"heading": "3.2 Bayesian hybrid of conditional and generative models", "text": "The EigenNet is a hybrid of conditional and generative models. The conditional component allows us to learn the classifier through \"discriminatory\" training; the generative component captures the correlations between variables; and these two models are glued together by a common prior distribution, so that the correlation information is used to guide the estimate of the classifier and the classification task. Our approach is based on the general Bayean framework proposed by Lasserre et al. [2006], and allows to combine conditional and generative models in an elegant way. Specifically, we can use the same probability for the conditional model as (1), p (y | X, b) = i (wTxi + b)). To save the classifier, we can use a laplace v2-v2-vasive model."}, {"heading": "3.3 Reparameterization and constraint in Eigenspace", "text": "In this section, we repair the model in proper space: w = V\u03b1 w = V\u03b2 | (9), where V \u00b2 (v1,.., vm) (m = min {n, p}) and \u03b1 and \u03b2 are the projections of w and w \u00b2 on the eigenvectors. If the number of characteristics is greater than the number of training points, i.e. p > n, it effectively reduces the number of free parameters in the model, avoiding revision. Furthermore, it offers a significant computational advantage if p > > n.Given p (w, w \u00b2) and the relationship between (w \u00b2) and (T \u00b2) when we get an equation."}, {"heading": "4 Alternative view: composite regularization", "text": "In this section, we provide an alternative view of the Own Net by analyzing the boundary between 5 and 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5"}, {"heading": "5 Related work", "text": "EigenNet can be considered an extension of the classical eigenface approaches [Turk & Pentland, 1991, Sirovich & Kirby, 1987].Eigenface approach uses PCA coefficients from samples to train a classifier.Naturally, the large eigenface approaches are often associated with large PCA coefficients, and the classifier is limited in the data subregion if the number of characteristics is less than the number of training specimens.EigenNet essentially extends the eigenface approach by combining generative and conditional models in a Bayesian framework and performing economical learning in an adaptive eigenspace (since the model selects or scales relevant eigenvectors to sj).There are Bayesian versions of the lasso and the elastic mes.The Bayesian lasso [Park et al., 2008] hyper-precedes the eigenspace coefficient and uses a Gibbs sampler to jointly capture both regression weights and the regulation coefficient."}, {"heading": "6 Experimental results", "text": "We evaluate the new sparse Bayesian model, EigenNet, on both synthetic and real data and compare it with three representative, state-of-the-art variable selection methods, including the lasso, the elastic mesh, and the Bayesian lasso modified for classification problems.2 For the lasso and the elastic mesh, we use the software package Glmnet, which pathologically uses cyclic coordinate deviations.2 The original Bayesian lasso was developed for regression and uses Gibbs samples. For the classification tasks we are looking at, we change its Gaussian regression probability to the logistical probability (1) while maintaining its Laplace distributions. We used Markov Chain Monte Carlo instead of the Gibbs sampler to estimate the classifier for the Bayesian lasso. Bayesian approaches are able to estimate all hyperparameters from the data."}, {"heading": "6.1 Visualization of estimated classifiers", "text": "First, we test these methods on synthetic data that contain correlated characteristics. We sample 40 dimensional data points, each of which contains two groups of correlated variables; the correlation coefficient between variables in each group is 0.81 and there are 4 variables in each group; we set the values of the classification weight in one group to 5 and in the other group to -5; we also randomly generate the bias term from a standard Gaussian distribution; we set the number of training points to 80; Figure 4 shows the estimated classifiers and the true classifier; it is not surprising that the elastic mesh identifies more characteristics than the lasso. Interestingly, EigenNet does not suppress many irrelevant characteristics to be exactly 0, but it clearly identifies all relevant ones that dominate the irrelevant ones. To save space, we do not show the estimated classifier by the Bayesian lasso weight. Similar to EigenNet, its classifier contains many, but no small weights."}, {"heading": "6.2 Classification of synthetic data", "text": "We systematically compare these methods with synthetic data sets with correlated characteristics and data sets with independent characteristics. In this first case, we use a similar procedure as in the visualization example: We sample 40 dimensional data points, each of which contains two sets of correlated variables. The correlation coefficient between variables in each group is 0.81 and there are 4 variables in each group. In contrast to the previous example, where the classification weights for the correlated variables are the same, we now set the weights within the same group to have the same character but with different random values. We vary the number of training points ranging from 10 to 80, and test all of these methods. For data sets with independent characteristics, we follow the same procedure, except that the characteristics are sampled independently of each other. We perform the experiments 10 times and show the error rates more than 10 times."}, {"heading": "6.3 Classification of real data", "text": "In addition to the synthetic data, we also test all of these methods for UCI benchmark data sets, two high-dimensional gene expression data sets, leukemia and colorectal cancer, and a spambase data set with relatively smaller dimensions but much more training samples. For the leukemia data set, the task is to distinguish acute myeloid leukemia (AML) from acute lymphoblastic leukemia (ALL).The entire data set consists of 47 and 25 samples of ALL and AML with 7129 characteristics per sample. The data set was randomly divided into 37 training samples and 35 test samples."}, {"heading": "7 Conclusions", "text": "In this paper, we have presented a novel sparse Bayesian hybrid model, EigenNet, which combines a sparse conditional classification model with a generative model for capturing feature correlations and generalizes the elastic mesh by explicitly examining correlations between characteristics. EigenNet achieves significantly improved predictive accuracy on several benchmark datasets compared to several modern methods. We plan to expand our hybrid model by using other probabilistic generative models such as sparse principle component analysis and related projection methods [Guan & Dy, Archambeau & Bach, 2009] and independent component analysis models. Compared to classical PCA models, these models could be used to better select interdependent sparse characteristics."}, {"heading": "Acknowledgement", "text": "Thanks to Jyotishka Datta for his help with the software implementation and to Tommi Jaakkola for the stimulating discussion."}], "references": [{"title": "Sparse probabilistic projections", "author": ["Archambeau", "C\u00e9dric", "Bach", "Francis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Archambeau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Archambeau et al\\.", "year": 2009}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["Fan", "Jianqing", "Li", "Runze"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2001}, {"title": "Principled hybrids of generative and discriminative models", "author": ["Lasserre", "Julia A", "Bishop", "Christopher M", "Minka", "Thomas P"], "venue": "In Proc. of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lasserre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lasserre et al\\.", "year": 2006}, {"title": "The Bayesian Elastic Net", "author": ["Li", "Qing", "Lin", "Nan"], "venue": "Bayesian Analysis,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "The Bayesian Lasso", "author": ["Park", "Trevor", "Casella", "George"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Park et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Park et al\\.", "year": 2008}, {"title": "The matrix cookbook", "author": ["Petersen", "Kaare Brandt", "Pedersen", "Michael Syskind"], "venue": null, "citeRegEx": "Petersen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Petersen et al\\.", "year": 2008}, {"title": "Low-dimensional procedure for the characterization of human faces", "author": ["L. Sirovich", "M. Kirby"], "venue": "J. Opt. Soc. Am. A,", "citeRegEx": "Sirovich and Kirby,? \\Q1987\\E", "shortCiteRegEx": "Sirovich and Kirby", "year": 1987}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani and Robert.,? \\Q1994\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1994}, {"title": "Eigenfaces for recognition", "author": ["Turk", "Matthew", "Pentland", "Alex"], "venue": "J. Cognitive Neuroscience,", "citeRegEx": "Turk et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Turk et al\\.", "year": 1991}, {"title": "Regularization and variable selection via the Elastic Net", "author": ["Zou", "Hui", "Hastie", "Trevor"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlation in a principle Bayesian framework [Lasserre et al., 2006].", "startOffset": 159, "endOffset": 182}, {"referenceID": 2, "context": "Our approach is based on the general Bayesian framework proposed by Lasserre et al. [2006]), which allows one to combine conditional and generative models in an elegant principled way.", "startOffset": 68, "endOffset": 91}, {"referenceID": 4, "context": "Bayesian lasso [Park et al., 2008] puts a hyper-prior on the regularization coefficient and use a Gibbs sampler to jointly sample both regression weights and the regularization coefficient.", "startOffset": 15, "endOffset": 34}], "year": 2013, "abstractText": "It is a challenging task to select correlated variables in a high dimensional space. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not explicitly use correlation information embedded in data to select correlated variables. To overcome this limitation, we present a novel Bayesian hybrid model, the EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We reparameterize the hybrid model in the eigenspace to avoid overfiting and to increase the computational efficiency of its MCMC sampler. Furthermore, we provide an alternative view to the EigenNet from a regularization perspective: the EigenNet has an adaptive eigenspace-based composite regularizer, which naturally generalizes the l1/2 regularizer used by the elastic net. Experiments on synthetic and real data show that the EigenNet significantly outperforms the lasso, the elastic net, and the Bayesian lasso in terms of prediction accuracy, especially when the number of training samples is smaller than the number of variables.", "creator": "LaTeX with hyperref package"}}}