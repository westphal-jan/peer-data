{"id": "1102.4807", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2011", "title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "abstract": "We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are the noisy realizations of the sum of an (appproximately) low rank matrix $\\Theta^\\star$ with a second matrix $\\Gamma^\\star$ endowed with a complementary form of low-dimensional structure. We derive a general theorem that gives upper bounds on the Frobenius norm error for an estimate of the pair $(\\Theta^\\star, \\Gamma^\\star)$ obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. We then specialize this result to two cases that have been studied in the context of robust PCA: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields non-asymptotic Frobenius error bounds for both for deterministic and stochastic noise matrices, and applies to matrices $\\Theta^\\star$ that can be exactly or approximately low rank, and matrices $\\Gamma^\\star$ that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices, we establish matching lower bounds on the minimax error, showing that our results cannot be improved beyond constant factors. The sharpness of our theoretical predictions is confirmed by numerical simulations.", "histories": [["v1", "Wed, 23 Feb 2011 18:02:53 GMT  (368kb)", "https://arxiv.org/abs/1102.4807v1", null], ["v2", "Mon, 18 Jul 2011 02:25:47 GMT  (379kb)", "http://arxiv.org/abs/1102.4807v2", null], ["v3", "Tue, 6 Mar 2012 06:59:59 GMT  (384kb)", "http://arxiv.org/abs/1102.4807v3", "41 pages, 2 figures"]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["alekh agarwal", "sahand negahban", "martin j wainwright"], "accepted": true, "id": "1102.4807"}, "pdf": {"name": "1102.4807.pdf", "metadata": {"source": "CRF", "title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "authors": ["Alekh Agarwal", "Sahand Negahban", "Martin J. Wainwright"], "emails": ["alekh@eecs.berkeley.edu", "sahand@eecs.berkeley.edu", "wainwrig@stat.berkeley.edu"], "sections": [{"heading": null, "text": "We are analyzing a class of estimators based on convex relaxation to solve high-dimensional matrix decomposition problems. Observations are noisy insights into linear transformation X of the sum of a (approximately) low-ranking matrix and a second matrix endowed with a complementary form of low-dimensional structure; this constellation includes many statistical models of interest, including forms of factor analysis, multi-task regression with common structure, and robust covariance estimation. We are deriving a general theorem that contains upper limits to the Frobenius standard errors for estimating the pair (namely by solving a convex optimization problem that combines the nuclear standard with a general decompressible matrix). Our results are based on the introduction of a \"weak matrix\" condition related to a lower than singular vector fission."}, {"heading": "1 Introduction", "text": "eSi rf\u00fc ide nlrrrrrrrreeeeeeeeeeeerrr rf\u00fc eid rf\u00fc rf\u00fc ide rf\u00fc eid rf\u00fc ide rf\u00fc eeisrrlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrsrsrrrrrrsrrsrrrrrrsrrrrsrsrrrrsrsrrrrrrsrrrrsrrrrrsrsrsrrsrsrsrsrsrrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeersrsrsrsrsrsrsrsrsrsrsrsrsrsrrsrsrsrsrsrsrrsrsrsrrsrsrrrsrrrrrrrrrrrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "2 Convex relaxations and matrix decomposition", "text": "In this paper, we consider a family of regulators formed by a combination of the nuclear standard (see e.g. Law et al. [25] and references to it), using a standards-based regulator R: Rd1 \u00b7 d2 \u2192 R + to constrain the structure of the regulatory mechanisms. We provide a general theorem applicable to a class of regulators R that fulfill a certain property of decomposition [19], and then look in detail at some specific decisions of R that have been studied in the past, including the elementary 1 standard and the split (2, 1) standard (see Examples 4 and 5 below)."}, {"heading": "2.1 Some motivating applications", "text": "We begin with some application examples for the general linear observation models with noise (1).Example 1 (factor analysis with sparse noise).In a factor analysis model, the random vectors are Zi (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2, \"(2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2,\" (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2, (2), (2), (2), (2), (, (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (, (2), (2), (2), (2), (2),"}, {"heading": "2.2 Convex relaxation for noisy matrix decomposition", "text": "In the light of the observation model Y = X (land use plan + land use plan) + W, it is natural to consider an estimator based on the solution of the regularised minimum squares (land use plan + land use plan) (land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan = land use plan ="}, {"heading": "2.3 Some examples", "text": "We should look at some examples to provide specific forms of the estimator (7), and the role of additional limitations (1). (1) Suppose that the problem of factor analysis involves non-identical but sparse noise covariance as discussed in Example 1, as well as certain formulations of the robust PCA [7], and model selection in Gauss-Markov include random fields with hidden variables [8]. (2) Given the thriftiness of regulators, an appropriate choice of elementary 1-d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d2 d4 d4 d4 d4 d4 d3 d4 d4 d4 d4 d4 d2 d4 d4 d2 d4 d4 d4 d4 d2 d4 d2 d4 d4 d4 d2 d4 d4 d2 d4 d4 d4 d2 d4 d4 d2 d4 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4 d4 d4 d2 d4 d2 d2 d4 d4 d4 d2 d4 d4 d4 d4 d4 d4 d4 d4 d2 d2 d4 d4 d4 d2 d4 d4 d4 d4 d2 d4 d4 d4 d4 d4 d2 d2 d4 d4 d4 d2 d4 d4 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4 d2 d4 d4 d4 d4 d2 d4 d4 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4 d4 d2 d4 d4"}, {"heading": "3 Main results and their consequences", "text": "In this section, we present our main results and discuss some of their consequences. Our first result relates to the family of convex programs (7), if R belongs to the class of decomposable regularizers, and the loss of the smallest squares associated with the observational model fulfills a specific form of constrained strong convexity [19]. Accordingly, we begin in Section 3.1 by defining the concept of decomposability and then illustrate how the elementary expansion and column (2, 1) norms discussed in Examples 4 and 5 are both cases of decomposable regularizers. In Section 3.2, we define the form of constrained strong convexity appropriate to our attitude. Section 3.3 contains the statement of our main result on the M estimator (7), while Section 3.4 and 3.6 are devoted to its consequences for cases of extraordinarily strong convexity."}, {"heading": "3.1 Decomposable regularizers", "text": "The idea of decomposition is defined in terms of a pair of sub-ranges that (in general) do not require orthogonal complements. Here, we consider a specific case of decomposability that is sufficient to cover the examples of interest in this paper. (In particular: a sub-range M) + R (V) for all U-Rd1 and its orthogonal supplements M). (15) To provide some intuitions, the sub-range M should be thought of as a nominal model subspace; in our results it is chosen so that the matrix within or near M. The orthogonal supplement M represents deviations from the model subspace and equality (15)."}, {"heading": "3.2 Restricted strong convexity", "text": "In the face of a loss function, the general concept of strong convexity involves fixing a square lower limit on the error of the Taylor approximation (1). In our environment, loss is the quadratic function L (1) = 12 | | Y \u2212 X (2) | | 2F (where we use a first order error), so that the Taylor error of the first order in the direction of the matrix from L (2) to L (2) \u2212 L (2) \u2212 L (1) = 2 | | X (2) (4) to X (2). (19) Consequently, strong convexity is equivalent to a lower limit of the form 12 \u0445 X (2)."}, {"heading": "3.3 Results for general regularizers and noise", "text": "We begin by specifying a result for a general observation operator X, a general decompatible regulating unit R, and a general noise matrix W. In later subsections, we specialize this result in certain decisions of the observation operator, regulator, and stochastic noise matrices. In all our results, we measure errors using the square Frobenius standard, which is added to both matrices. (23) With this notation, the following result applies to the observation model Y = X (1) + W, in which the low-level matrix meets the condition. (23) Our upper limit for the square Frobenius error consists of three terms that apply to the observation model Y = X (2)."}, {"heading": "3.4 Results for \u21131-norm regularization", "text": "The question which arises is whether this is a type and manner in which it concerns a manner and manner in which it comes to a condition of the condition which satisfies the condition of the condition of the condition that the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the condition of the"}, {"heading": "3.4.1 Results for stochastic noise matrices", "text": "It is not possible that the rally matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matri"}, {"heading": "3.4.2 Comparison to Hsu et al. [14]", "text": "This latest work focuses on the problem of the decomposition of the matrix with the two minor differences and delivers results for both the silent and the low-noise environment. All of their work focuses on the case of exactly low rank and exactly sparse matrices and deals only with the identity observation operator; in contrast, Theorem 1 in this work provides an upper limit for general matrix pairs and observation operators. Most relevant is the comparison between our two cases with exact ranking limitations to their Theorem 3, the various error limits (in the nuclear and Frobenius norms) for such models with additive noise. These limits are achieved with an estimator similar to our program (10) and in parts of their analysis they set limits on the -standard of the solution. However, this is not done directly with a limitation in this way as in our estimator."}, {"heading": "3.4.3 Results for multi-task regression", "text": "Let us now extend our results to the setting of the multi-task regression, as introduced in Example 2. The observation model is of the form Y = XB + W, where X + Rn \u00b7 d1 is a known design matrix, and we observe the matrix Y = Rn \u00b7 d2. However, our goal is to estimate the regression matrix B + Rd1 \u00b7 d2, which is assumed to have a decomposition of the form B * = Rn \u00b7 d2, with the shared properties between each of the tasks and the matrix models showing disturbances beyond the common structure. Supposing that the matrix is an economical matrix, an appropriate choice of the regulator R is the elementary 1 standard, as in Korollar 2. We use the minimum and maximum or maximum number d2 values of the realized design matrix."}, {"heading": "3.5 An alternative two-step method", "text": "It is possible that a simpler two-step method - namely, based on the first estimation of the entries of the observation matrix Y, and then performing a two-step approximation - can achieve rates similar to the more complex convex relaxation (10). In this section we provide a detailed analysis of such an approach: We also give an example to show that the two-step method is not necessarily good for general observation operators X.In detail let us consider the following two-step estimation: (a) estimation of the sparse component is made by us by solvingo argmin argmin d2. \""}, {"heading": "3.6 Results for \u2016 \u00b7 \u20162,1 regularization", "text": "To keep our presentation relatively short, we will focus here on the case of the identity observation operator X = I.Corollary 5. It is assumed that we solve the convex program (14) with regulation parameters (2), which yield a number of regulation parameters (2) (2). [2] To keep our presentation relatively short, we will focus here on the case of the identity observation operator X = I.Corollary 5. (43) Then there is a universal constant c1, which provides such a constant for each matrix pair (2)."}, {"heading": "3.7 Lower bounds", "text": "In this section, we turn to the complementary question: What are the basic (algorithmically independent) limits of accuracy in the decomposition of the noisy matrix? One way to answer such a question is to analyze statistical minimaxrates. Formally, some families of matrices are considered to have minimax. errors associated with them, given by M (F): = inf (1), [2), [3], [4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, 5, 5, 5, 5, (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, (5), (5), (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, 5), (5), 5, (5), 5), (5), 5), (5, (5), (5), 5), (5, (5), (5, (5), 5), (5), 5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), 5), (5),"}, {"heading": "4 Simulation results", "text": "In this section, we report on simulation results that show the excellent correspondence between our theoretical predictions and the behavior in practice. In all cases, we use square matrices (d = d1 = d2) and a stochastic noise matrix W with i.i.d. N (0, \u03bd 2d2) entries, with which we select the square matrices (d = d1 = d2) in each given rank by randomly selecting the spaces of linear and right singular vectors. We create random spares (elementary or column-wise) matrices by selecting the positions of non-zeros (entries or columns), which we uniformally compute randomly (10)."}, {"heading": "5 Proofs", "text": "In this section we provide the corrections of our main results, along with the corrections of some more technical terms that have been moved to the annexes."}, {"heading": "5.1 Proof of Theorem 1", "text": "For the convenience of the reader, we recall here the two assumptions regarding the regulation parameters: \"Q\" (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\") (\"Q\" (\"Q\") (\"Q\") (\"Q\" Q \") (\" Q \"(\" Q \") (\" Q \"Q\") (Q \"Q\") Q \"(Q\" Q \") (\" Q \"Q\" Q \") (\" Q \"Q\" Q \"(\") (Q \"Q\" Q \") (\" Q \"Q\" Q \") (Q\" Q \") Q\" Q \"(Q\" Q \"Q\") (Q \"Q\" Q \"(Q\") Q \"(Q\" Q \"Q\") Q \"Q\" (Q \"Q\") (Q \"Q\" Q \"Q\" (Q \") Q\" Q \"(Q\") Q \"Q\" Q \"Q\" (Q \") (Q\" Q \"Q\" Q \"Q\") (Q \"Q\" (Q \"Q\" Q \") (Q\" Q \"Q\" Q \"Q\") (\"Q\" Q \"(\" Q \") (Q\" Q \"Q\" Q \") (\" Q \") (Q\" Q \"Q\" Q \"Q\" (\"Q\") (\"Q\" Q \"Q\" Q \") (\" Q \"Q\") (Q \"Q\" Q \"Q\" Q \"Q\" Q \"(\""}, {"heading": "5.2 Proof of Corollaries 2 and 4", "text": "Note that Corollary 2 can be considered a special case of Corollary 4, in which n = d1 and X = Id1 \u00b7 d1. Consequently, we can prove the latter result and then obtain the former result with this specialization. Remember that we specify \u03c3min and \u03c3max the minimum and maximum eigenvalues of X (each), and that \u03bamax = maxj = 1,..., d1 = Xj \u00b2 2 specifies the maximum 2 norm over the columns. (In the special case X = Id1 \u00b7 d2, we have \u03c3min = max = \u03bamax = \u03bamax = 1.) Both corollaries are based on the regulator, R (\u00b7) = approximate 1, and the associated double standard R. (\u00b7) We must verify that the decisions given by (\u03bbd, \u00b5d) meet the requirements of Corollary 1."}, {"heading": "5.3 Proof of Corollary 3", "text": "In this model, the sound matrix is re-centered on Wishart noise - namely W = 1n \u2211 n i = 1 ZiZ T i \u2212 \u03a3, with each Zi \u0445 N (0, \u03a3). If we allow Ui \u0445 N (0, Id \u00b7 d) i.i.i.d. Gaussian random vectors, we have | | W | | | | | \u221a (1 n \u0445 i = 1UiUi \u2212 Id \u00b7 d) \u221a | | | op \u2264 | | | | 1nn \u2211 i = 1UiU T i \u2212 Id \u00b7 d | | | op \u2264 4 | | | \u03a3 | | | op \u221a dn, where the final limit is likely to be greater than 1 \u2212 2 exp (\u2212 c1d), using standard tail boundaries on Gaussian random matrices [10]. Thus, we see that the given choice (36) applies to theorem 1 with a high probability."}, {"heading": "5.4 Proof of Proposition 1", "text": "To begin with, let us recall the state (52) of the regularization parameters, which means that the matrices (2) include all the optimal solutions to the optimization problems (40) and (41) that define the two-step procedure. We redefine the error matrices (2), and the matrices (2) as previously defined in the proof of theorem. Our proof of Proposition 1 is based on two lemmas, the first of which triggers control over the error rate (2) and the second (4)."}, {"heading": "5.5 Proof of Corollary 6", "text": "To confirm the assertion, we must show that the conditions of corollar5 on the regularization pair (\u03bbd, \u00b5d) are highly probable, and that the setting of \u03bbd is the same as corollar2 and is valid by our previous argument. Therefore, to complete the proof, we must set an upper limit on the k-column of the matrix. Let us let Wk be the k-th column of the matrix. Considering that the function Wk 7 \u2192 Wk-2 isLipschitz, measured by the concentration of Gaussian Lipschitz functions [16], we have an upper limit on the k-2-E column."}, {"heading": "5.6 Proof of Corollary 7", "text": "For this model, the sound matrix takes the form W: = 1n \u0445 n i = 1 UiU T i \u2212 \u044b, where Ui \u0445 N (0, \u043a). Since it is positive with r at the most, we can write W = Q {1 n ZiZ T i \u2212 Ir \u00b7 r} QT, where the Q-Rd \u00b7 r matrix is the default Gaussian relation between Q-QQQT and Zi-N (0, Ir \u00b7 r) in dimension r. It follows that on the basis of known results on singular values of wishart matrices [10], we have a high probability of W-standard limits [13], which shows that the specified choice of the device is valid. Thus, the quantity W-2 remains to be bound."}, {"heading": "5.7 Proof of Theorem 2", "text": "Our evidence for the lower limit is based on a standard reduction [12, 31, 30] from an estimate to a reusable hypotheses test problem via a packet of matrix pairs. In particular, given a collection of matrix pairs contained in a family F, we say that in the Frobenius standard it forms a \u03b4 packaging if we are a clear consequence of the inequality of Fano for all unambiguous pairs i, j, j, 1, 2,....., M} that the Minimax error relative to F satisfies the lower limit P [M (F)."}, {"heading": "5.7.1 Lower bounds for elementwise sparsity", "text": "We start by detecting the lower limit (50) for matrix decomposition processes via the family Fsp (r, s, \u03b1)."}, {"heading": "5.7.2 Lower bounds for columnwise sparsity", "text": "The lower limit (51) for the lower limit results from a similar argument. The only modifications are in the package quantity (1). The package for the radius of non-identifiability (1) is the maximum size of the package quantity (2). The package quantity (2) corresponds to the \"bad\" matrix (45) from Example 8, which we call the B value. By constructing the number of package quantity (B value, B value), (1), (1), (B value, B value, B value), (1), (2), (0, 0), (2), each of these matrix pairs (1) belongs to the package quantity Fcol (1, s, s value), so it can be used to establish a lower limit for this quantity. (In addition, there is also a lower limit for the package quantity Fcol (r, s, s, H value)."}, {"heading": "6 Discussion", "text": "In this paper, we analyzed a class of convex relaxations for solving a general class of matrix decomposition problems in which the goal is to restore a matrix pair based on observing a loudly contaminated version of its sum. Since the problem is generally poorly positioned, it is important to impose structure, and this work focuses on the setting in which one matrix is approximately low and the second has a complementary form of low-dimensional structure forced by a decomposable regulator. Specific cases include matrices that are elementally sparse or slit-wise sparse, and the associated matrix decomposition problems have various applications, including robust PCA, robustness in collaborative filtration and model selection in Gauss-Markov Random fields. We have a general, non-asymptotic component that is not related to the Frobenius error of a concentric relaxation based on a combination of a concentric component with a concentric component."}, {"heading": "Acknowledgements", "text": "All three authors were partially supported by the AFOSR-09NL184 scholarship, SN and MJW were partially supported by the NSF-CDI-0941742 scholarship, and AA was partially supported by a Microsoft Research Graduate Fellowship. All three authors would like to thank the Banff International Research Station (BIRS) in Banff, Canada, for the hospitality and work opportunities that stimulated and supported this collaboration."}, {"heading": "A Proof of Lemma 1", "text": "The decomposition described in part (a) has been determined by Recht et al. [25], so that it remains part (b). With the corresponding definitions, part (b) can be restored by using Lemma 1 from Negahban et al. [19]. Their problem applies to optimization problems of the general form constant. The elementary properties of the 1 standard and the nuclear standard are both instances of decompatible regulators. Their Lemma requires that the regulation parameter \u03b3n be chosen to satisfy a property known as decomposition. The elementary properties of the 1 standard and the nuclear standard are both instances of decompatible regulators. Their Lemma requires that the regulation parameter \u03b3n 2 r standard can be chosen."}, {"heading": "B Proof of Lemma 2", "text": "Through the RSC condition (22) we have the possibility of getting involved in the second stage (20) and the third stage (21) of Q and the third stage (21). We now derive a lower limit to the second stage (F) and an upper limit to the second stage (2). Starting with the previous conceptual direction, we consider the second stage (2) to the third stage (F) and the third stage (F) to the third stage (F). We have the third stage (F) to the third stage (F) and the fourth stage (F)."}, {"heading": "C Refinement of achievability results", "text": "In this appendix, we provide refined arguments that result in sharpened forms of corollariums 2 and 6. These refinements result in attainable limits that correspond to the minimum lower limits in Theorem 2 down to constant factors. We note that these refinements are significantly different only if the splitter index s is used as a yardstick for Corollary 2 or as a yardstick (d1d2) for Corollary 6.C.1 Refinement of Corollary 1 if it specializes in the splitter group 1 standard, the sound term as a yardstick (d1d2) for Corollary 2, or as a yardstick (d2) for Corollary 6.C.1 Refinement of Corollars 1. Here, we use a more cautious argument to control this sound term. During the evidence, we assume that the regulation parameters are set in the usual way that we decide against it."}, {"heading": "D Proof of Lemma 4", "text": "Since these two countries are the best and most practicable countries that are able to pay their debts, it is not surprising that they are able to pay their debts. (...) Since they are not able to pay their debts, they do not have to pay their debts. (...) They have to be prepared for the fact that they are not able to pay their debts. (...) They have to be prepared for the fact that they are not able to pay their debts. (...) They have to be prepared for the fact that they are not able to pay their debts. (...) They have to rely on the fact that they are not able to pay their debts. (...) They have to pay their debts."}], "references": [{"title": "An Introduction to Multivariate Statistical Analysis. Wiley Series in Probability and Mathematical Statistics", "author": ["T.W. Anderson"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Covariance regularization by thresholding", "author": ["P.J. Bickel", "E. Levina"], "venue": "Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Zero-shot domain adaptation: A multiview approach", "author": ["J. Blitzer", "D.P. Foster", "Sham M. Kakade"], "venue": "Technical report, Toyota Technological Institute at Chicago,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. Mcdonald", "F. Pereira"], "venue": "In EMNLP Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Robust Principal Component Analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Technical report, Stanford,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Latent variable graphical model selection via convex optimization", "author": ["V. Chandrasekaran", "P.A. Parillo", "A.S. Willsky"], "venue": "Technical report, Massachusetts Institute of Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "Technical report,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Local operator theory, random matrices, and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "In Handbook of Banach Spaces,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Gaussian averages of interpolated bodies and applications to approximate reconstruction", "author": ["Y. Gordon", "A.E. Litvak", "S. Mendelson", "A. Pajor"], "venue": "Journal of Approximation Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Has\u2019minskii. A lower bound on the risks of nonparametric estimates of densities in the uniform metric", "author": ["Z. R"], "venue": "Theory Prob. Appl.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1978}, {"title": "Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Technical report, Univ. Pennsylvania,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On the distribution of the largest eigenvalue in principal components analysis", "author": ["I.M. Johnstone"], "venue": "Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "The Concentration of Measure Phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs. American Mathematical Society,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Lectures on discrete geometry", "author": ["J. Matousek"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Two Proposals for Robust PCA using Semidefinite Programming", "author": ["M. McCoy", "J. Tropp"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "In NIPS Conference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Restricted strong convexity and (weighted) matrix completion: Optimal bounds with noise", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Technical report, UC Berkeley,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain (UCL),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Minimax rates of estimation for high-dimensional linear regression over lq-balls", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Minimax-optimal rates for sparse additive models over kernel classes via convex programming", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1970}, {"title": "Estimation of high-dimensional low-rank matrices", "author": ["A. Rohde", "A. Tsybakov"], "venue": "Technical Report arXiv:0912.5338v2, Universite de Paris,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Empirical Processes in M-Estimation", "author": ["S. van de Geer"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Robust PCA via Outlier Pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "Technical report, University of Texas,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Information-theoretic determination of minimax rates of convergence", "author": ["Y. Yang", "A. Barron"], "venue": "Annals of Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "In Festschrift for Lucien Le Cam, pages 423\u2013435", "author": ["B. Yu. Assouad", "Fano", "Le Cam"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}, {"title": "Dimension reduction and coefficient estimation in multivariate linear regression", "author": ["M. Yuan", "A. Ekici", "Z. Lu", "R. Monteiro"], "venue": "Journal Of The Royal Statistical Society Series B,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}], "referenceMentions": [{"referenceID": 8, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 105, "endOffset": 114}, {"referenceID": 7, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 105, "endOffset": 114}, {"referenceID": 6, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 105, "endOffset": 114}, {"referenceID": 17, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 140, "endOffset": 148}, {"referenceID": 28, "context": "Two particular cases of structure for \u0393\u22c6 that have been considered in past work are elementwise sparsity [9, 8, 7] and column-wise sparsity [18, 29].", "startOffset": 140, "endOffset": 148}, {"referenceID": 8, "context": "Different forms of robust PCA can be formulated in terms of matrix decomposition using the matrix \u0393\u22c6 to model the gross errors [9, 7, 29].", "startOffset": 127, "endOffset": 137}, {"referenceID": 6, "context": "Different forms of robust PCA can be formulated in terms of matrix decomposition using the matrix \u0393\u22c6 to model the gross errors [9, 7, 29].", "startOffset": 127, "endOffset": 137}, {"referenceID": 28, "context": "Different forms of robust PCA can be formulated in terms of matrix decomposition using the matrix \u0393\u22c6 to model the gross errors [9, 7, 29].", "startOffset": 127, "endOffset": 137}, {"referenceID": 7, "context": "The problem of low rank plus sparse matrix decomposition also arises in Gaussian covariance selection with hidden variables [8], in which case the inverse covariance of the observed vector can be decomposed as the sum of a sparse matrix with a low rank matrix.", "startOffset": 124, "endOffset": 127}, {"referenceID": 31, "context": "Matrix decompositions also arise in multi-task regression [32, 21, 27], which involve solving a collection of regression problems, referred to as tasks, over a common set of features.", "startOffset": 58, "endOffset": 70}, {"referenceID": 20, "context": "Matrix decompositions also arise in multi-task regression [32, 21, 27], which involve solving a collection of regression problems, referred to as tasks, over a common set of features.", "startOffset": 58, "endOffset": 70}, {"referenceID": 26, "context": "Matrix decompositions also arise in multi-task regression [32, 21, 27], which involve solving a collection of regression problems, referred to as tasks, over a common set of features.", "startOffset": 58, "endOffset": 70}, {"referenceID": 4, "context": "For some features, one expects their weighting to be preserved across features, which can be modeled by a low-rank constraint, whereas other features are expected to vary across tasks, which can be modeled by a sparse component [5, 2].", "startOffset": 228, "endOffset": 234}, {"referenceID": 1, "context": "For some features, one expects their weighting to be preserved across features, which can be modeled by a low-rank constraint, whereas other features are expected to vary across tasks, which can be modeled by a sparse component [5, 2].", "startOffset": 228, "endOffset": 234}, {"referenceID": 8, "context": "[9] studied the case when \u0393\u22c6 is assumed to sparse, with a relatively small number s \u226a d1d2 of non-zero entries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] analyzed the same model but under an assumption of random sparsity, meaning that the non-zero positions are chosen uniformly at random.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[29] have analyzed a different model, in which the matrix \u0393\u22c6 is assumed to be columnwise sparse, with a relatively small number s \u226a d2 of non-zero columns.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], who derived Frobenius norm error bounds for the case of exact elementwise sparsity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], includes the elementwise l1-norm and columnwise (2, 1)-norm as special cases, as well as various other regularizers used in practice.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "An interesting feature of our analysis is that, in contrast to previous work [9, 29, 7], we do not impose incoherence conditions on the singular vectors of \u0398\u22c6; rather, we control the interaction with a milder condition involving the dual norm of the regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 28, "context": "An interesting feature of our analysis is that, in contrast to previous work [9, 29, 7], we do not impose incoherence conditions on the singular vectors of \u0398\u22c6; rather, we control the interaction with a milder condition involving the dual norm of the regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 6, "context": "An interesting feature of our analysis is that, in contrast to previous work [9, 29, 7], we do not impose incoherence conditions on the singular vectors of \u0398\u22c6; rather, we control the interaction with a milder condition involving the dual norm of the regularizer.", "startOffset": 77, "endOffset": 87}, {"referenceID": 19, "context": "In the special case of elementwise sparsity, this dual norm enforces an upper bound on the \u201cspikiness\u201d of the low-rank component, and has proven useful in the related setting of noisy matrix completion [20].", "startOffset": 202, "endOffset": 206}, {"referenceID": 24, "context": "[25] and references therein), with a norm-based regularizer R : Rd1\u00d7d2 \u2192 R+ used to constrain the structure of \u0393\u22c6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "We provide a general theorem applicable to a class of regularizers R that satisfy a certain decomposability property [19], and then consider in detail a few particular choices of R that have been studied in past work, including the elementwise l1-norm, and the columnwise (2, 1)-norm (see Examples 4 and 5 below).", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "In particular, if the let the observation matrix Y \u2208 Rd\u00d7d be the sample covariance matrix 1 n \u2211n i\u22121 ZiZ T i , then some algebra shows that Y = \u0398 \u22c6 + \u0393\u22c6 + W , where \u0398\u22c6 = LLT is of rank r, and the random matrix W is a re-centered form of Wishart noise [1]\u2014in particular, the zero-mean matrix", "startOffset": 251, "endOffset": 254}, {"referenceID": 1, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 31, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 20, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 26, "context": ", that is, the vectors \u03b2\u2217 j \u2208 Rd2\u2014exhibit some degree of shared structure across tasks [2, 32, 21, 27].", "startOffset": 87, "endOffset": 102}, {"referenceID": 1, "context": "However, many multi-task learning problems exhibit more complicated structure, in which some subset of features are shared across tasks, and some other subset of features vary substantially across tasks [2, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 3, "context": "However, many multi-task learning problems exhibit more complicated structure, in which some subset of features are shared across tasks, and some other subset of features vary substantially across tasks [2, 4].", "startOffset": 203, "endOffset": 209}, {"referenceID": 8, "context": "Indeed, as has been discussed in past work [9, 7, 29], no method can recover the components (\u0398\u22c6,\u0393\u22c6) unless the low-rank component is \u201cincoherent\u201d with the matrix \u0393\u22c6.", "startOffset": 43, "endOffset": 53}, {"referenceID": 6, "context": "Indeed, as has been discussed in past work [9, 7, 29], no method can recover the components (\u0398\u22c6,\u0393\u22c6) unless the low-rank component is \u201cincoherent\u201d with the matrix \u0393\u22c6.", "startOffset": 43, "endOffset": 53}, {"referenceID": 28, "context": "Indeed, as has been discussed in past work [9, 7, 29], no method can recover the components (\u0398\u22c6,\u0393\u22c6) unless the low-rank component is \u201cincoherent\u201d with the matrix \u0393\u22c6.", "startOffset": 43, "endOffset": 53}, {"referenceID": 8, "context": "Past work on both matrix completion and decomposition [9, 7, 29] has ruled out these types of troublesome cases via conditions on the singular vectors of the low-rank component \u0398\u22c6, and used them to derive sufficient conditions for exact recovery in the noiseless setting (see the discussion following Example 4 for more details).", "startOffset": 54, "endOffset": 64}, {"referenceID": 6, "context": "Past work on both matrix completion and decomposition [9, 7, 29] has ruled out these types of troublesome cases via conditions on the singular vectors of the low-rank component \u0398\u22c6, and used them to derive sufficient conditions for exact recovery in the noiseless setting (see the discussion following Example 4 for more details).", "startOffset": 54, "endOffset": 64}, {"referenceID": 28, "context": "Past work on both matrix completion and decomposition [9, 7, 29] has ruled out these types of troublesome cases via conditions on the singular vectors of the low-rank component \u0398\u22c6, and used them to derive sufficient conditions for exact recovery in the noiseless setting (see the discussion following Example 4 for more details).", "startOffset": 54, "endOffset": 64}, {"referenceID": 19, "context": "In this paper, we impose a related but milder condition, previously introduced in our past work on matrix completion [20], with the goal of performing approximate recovery.", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "For instance, see the paper [23] for discussion of non-identifiability in high-dimensional sparse regression.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Motivating applications include the problem of factor analysis with a non-identity but sparse noise covariance, as discussed in Example 1, as well as certain formulations of robust PCA [7], and model selection in Gauss-Markov random fields with hidden variables [8].", "startOffset": 185, "endOffset": 188}, {"referenceID": 7, "context": "Motivating applications include the problem of factor analysis with a non-identity but sparse noise covariance, as discussed in Example 1, as well as certain formulations of robust PCA [7], and model selection in Gauss-Markov random fields with hidden variables [8].", "startOffset": 262, "endOffset": 265}, {"referenceID": 19, "context": "Indeed, this type of spikiness control has proven useful in analysis of nuclear norm relaxations for noisy matrix completion [20].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "2 in the paper [20] for one example).", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Our first result applies to the family of convex programs (7) whenever R belongs to the class of decomposable regularizers, and the least-squares loss associated with the observation model satisfies a specific form of restricted strong convexity [19].", "startOffset": 246, "endOffset": 250}, {"referenceID": 18, "context": "[19], a large class of norms are decomposable with respect to interesting2 subspace pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "2 Restricted strong convexity Given a loss function, the general notion of strong convexity involves establishing a quadratic lower bound on the error in the first-order Taylor approximation [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 25, "context": "[26])", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This guarantee is weaker than the exact recovery results obtained in past work on the noiseless observation model with identity operator [9, 7]; however, these papers imposed incoherence requirements on the singular vectors of the low-rank component \u0398\u22c6 that are more restrictive than the conditions of Theorem 1.", "startOffset": 137, "endOffset": 143}, {"referenceID": 6, "context": "This guarantee is weaker than the exact recovery results obtained in past work on the noiseless observation model with identity operator [9, 7]; however, these papers imposed incoherence requirements on the singular vectors of the low-rank component \u0398\u22c6 that are more restrictive than the conditions of Theorem 1.", "startOffset": 137, "endOffset": 143}, {"referenceID": 8, "context": "We note that the singular vector incoherence conditions, as imposed in past work [9, 7, 14] and used to guarantee exact recovery, would exclude the matrix (33), since its left singular vector is the unit vector e1 \u2208 Rd1 .", "startOffset": 81, "endOffset": 91}, {"referenceID": 6, "context": "We note that the singular vector incoherence conditions, as imposed in past work [9, 7, 14] and used to guarantee exact recovery, would exclude the matrix (33), since its left singular vector is the unit vector e1 \u2208 Rd1 .", "startOffset": 81, "endOffset": 91}, {"referenceID": 13, "context": "We note that the singular vector incoherence conditions, as imposed in past work [9, 7, 14] and used to guarantee exact recovery, would exclude the matrix (33), since its left singular vector is the unit vector e1 \u2208 Rd1 .", "startOffset": 81, "endOffset": 91}, {"referenceID": 14, "context": ", see Johnstone [15]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "[14] This recent work focuses on the problem of matrix decomposition with the \u2016 \u00b7 \u20161-norm, and provides results both for the noiseless and noisy setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29], since their results do not guarantee exact recovery of the pair (\u0398\u22c6,\u0393\u22c6).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "We have implemented the M -estimators based on the convex programs (10) and (14), in particular by adapting first-order optimization methods due to Nesterov [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "Since |||XX|||op \u2264 \u03c32 max, known results on the singular values of Gaussian random matrices [10] imply that", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "where the final bound holds with probability greater than 1 \u2212 2 exp(\u2212c1d), using standard tail bounds on Gaussian random matrices [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 15, "context": "Noting that the function Wk 7\u2192 \u2016Wk\u20162 is Lipschitz, by concentration of measure for Gaussian Lipschitz functions [16], we have", "startOffset": 112, "endOffset": 116}, {"referenceID": 9, "context": "Consequently, by known results on singular values of Wishart matrices [10], we have |||W |||op \u2264 \u221a 8|||\u0398|||op \u221a r n with high probability, showing that the specified choice of \u03bbd is valid.", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "By known matrix norm bounds [13], we have \u2016W\u20162,\u221e \u2264 |||W |||op, so that the claim follows by the previous argument.", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "7 Proof of Theorem 2 Our lower bound proofs are based on a standard reduction [12, 31, 30] from estimation to a multiway hypothesis testing problem over a packing set of matrix pairs.", "startOffset": 78, "endOffset": 90}, {"referenceID": 30, "context": "7 Proof of Theorem 2 Our lower bound proofs are based on a standard reduction [12, 31, 30] from estimation to a multiway hypothesis testing problem over a packing set of matrix pairs.", "startOffset": 78, "endOffset": 90}, {"referenceID": 29, "context": "7 Proof of Theorem 2 Our lower bound proofs are based on a standard reduction [12, 31, 30] from estimation to a multiway hypothesis testing problem over a packing set of matrix pairs.", "startOffset": 78, "endOffset": 90}, {"referenceID": 19, "context": "For the low rank component, we re-state a slightly modified form (adapted to the setting of non-square matrices) of Lemma 2 from the paper [20]: Lemma 5.", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "As for the sparse matrices, the following result is a modification, so as to apply to the matrix setting of interest here, of Lemma 5 from the paper [23]:", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "[24] on minimax rates for kernel classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "By known results on metric entropy of Euclidean balls [17], this function class has logarithmic metric entropy, so that part (b) of the above lemma applies, and yields the stated result.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "This problem is very closely related to matrix completion, a problem for which recent work by Negahban and Wainwright [20] shows that a form of restricted strong convexity holds with high probability.", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "[25], so that it remains to prove part (b).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "By assumption, the regularizer R is decomposable, and as shown in the paper [19], the nuclear norm is also decomposable.", "startOffset": 76, "endOffset": 80}, {"referenceID": 10, "context": "[11] with (q0, q1) = (1, 2), n = d1d2 and t = \u221a s, thereby obtaining", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] with (q0, q1) = (1, 2), n = d1d2 yields", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": ", [28]) can be used to extend this bound to a uniform one over the choice of radii t, so that it applies to the random one t = \u2016\u2206\u0302 \u20161 |||\u2206\u0302\u0393|||F of interest.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "The function W 7\u2192 Z\u0303(s) is a Lipschitz function with parameter \u03bd \u221a d1d2 , so that by concentration of measure for Gaussian Lipschitz functions [16], it satisfies the upper tail bound", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "Now the variable Vk is zero-mean, and sub-Gaussian with parameter \u03bd \u221a d1d2 , again using concentration of measure for Lipschitz functions of Gaussians [16].", "startOffset": 151, "endOffset": 155}, {"referenceID": 10, "context": "[11] with (q0, q1) = (1, 2), n = d2 and t = 4 \u221a s then yields", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "From Lemma 1 in the paper [21], there exists a decomposition \u2206\u0302\u0398 = \u2206\u0302A + \u2206\u0302 \u0398 B such that the rank of \u2206\u0302A upper-bounded by 2 r and |||\u0398|||N \u2212 |||\u0398 + \u2206\u0302A + \u2206\u0302B|||N \u2264 2 d \u2211", "startOffset": 26, "endOffset": 30}], "year": 2012, "abstractText": "We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation X of the sum of an (approximately) low rank matrix \u0398\u22c6 with a second matrix \u0393\u22c6 endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including forms of factor analysis, multi-task regression with shared structure, and robust covariance estimation. We derive a general theorem that gives upper bounds on the Frobenius norm error for an estimate of the pair (\u0398\u22c6,\u0393\u22c6) obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results are based on imposing a \u201cspikiness\u201d condition that is related to but milder than singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields non-asymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices \u0398\u22c6 that can be exactly or approximately low rank, and matrices \u0393\u22c6 that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error, showing that our results cannot be improved beyond constant factors. The sharpness of our theoretical predictions is confirmed by numerical simulations.", "creator": "LaTeX with hyperref package"}}}