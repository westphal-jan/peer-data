{"id": "1103.4601", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2011", "title": "Doubly Robust Policy Evaluation and Learning", "abstract": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. We leverage the strength and overcome the weaknesses of the two approaches by adapting doubly robust estimation techniques to the problems of policy evaluation and optimization. We prove that this approach yields unbiased (and often lower variance) value estimates when we have either a good model of rewards or a good model of past policy. Extensive empirical comparison demonstrate that the doubly robust technique uniformly improves existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice.", "histories": [["v1", "Wed, 23 Mar 2011 19:37:45 GMT  (52kb)", "https://arxiv.org/abs/1103.4601v1", "8 pages and 6 figures"], ["v2", "Fri, 6 May 2011 02:38:18 GMT  (58kb)", "http://arxiv.org/abs/1103.4601v2", "Published at ICML 2011, 8 pages, 6 figures"]], "COMMENTS": "8 pages and 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO stat.AP stat.ML", "authors": ["miroslav dud\u00edk", "john langford", "lihong li"], "accepted": true, "id": "1103.4601"}, "pdf": {"name": "1103.4601.pdf", "metadata": {"source": "CRF", "title": "Doubly Robust Policy Evaluation and Learning", "authors": ["Miroslav Dud\u0131\u0301k", "John Langford", "Lihong Li"], "emails": ["MDUDIK@YAHOO-INC.COM", "JL@YAHOO-INC.COM", "LIHONG@YAHOO-INC.COM"], "sections": [{"heading": null, "text": "ar Xiv: 110 3.46 01v2 [cs.LG] 6 May 2In this paper, we use the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and policy optimization. We demonstrate that this approach delivers accurate assessments when we have either a good (but not necessarily consistent) reward model or a good (but not necessarily consistent) model of past policies. Extensive empirical comparison shows that the doubly robust approach consistently improves over existing techniques, achieving both smaller differences in value evaluation and better policies."}, {"heading": "1. Introduction", "text": "In fact, most of us are able to play by the rules we have set ourselves to make them a reality, \"he said in an interview with The New York Times, saying,\" It's not the first time we've been able to put ourselves in a position to do that. \""}, {"heading": "1.1. Prior Work", "text": "More recently, it has been used in online advertising to estimate the impact of new features on online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010).Previous work has focused on parameter estimation rather than policy evaluation / optimization as discussed here. Furthermore, most previous analyses of doubly robust estimates of asymptotic behavior or are based on various modeling assumptions (e.g. Robins et al. (1994), Lunceford & Davidian (2004) and Kang & Schafer (2007)).Our analysis is not asymptotic and does not make such assumptions. Several other machine learning papers have used ideas that relate to the basic technique discussed here, though not in the same language."}, {"heading": "2. Problem Definition and Approach", "text": "Let us consider X as the entrance area and A = {1,.., k} as a finite space of action. A contextual bandit problem is specified by a deflection D via pairs (x, ~ r), where x, ~ r) is the context and ~ r [0, 1] A. The input data is generated using an unknown (possibly adaptive and randomized) policy as follows: \u2022 The world draws a new example (x, ~ r). Only x is revealed. \u2022 Politics chooses an action a \u00b2 p (a | x, h), where h is the history of previous observations (i.e. the concatenation of all previous contexts, actions and observed rewards). \u2022 Reward ra is revealed. It should be emphasized that other rewards ra \u00b2 with a \u00b2 6 = a are not observed. Note that neither the distribution D nor the policy p is known."}, {"heading": "2.1. Existing Approaches", "text": "The central challenge in estimating political value, given the data described in the previous section, is the fact that we have incomplete information about the reward, so we cannot directly simulate our proposed policy using data set S. There are two common solutions to overcome this limitation: the first method, called direct estimation (DM), is an estimate of the expected reward, depending on context and action; the political value is then estimated by V [ra | x], then the DM estimate is close to V (x); however, if method a (x) is a good approximation of the expected reward, defined as \"a\" (x) = E (x, ~ r) = D [ra | x], then the DM estimate is close to V (x)."}, {"heading": "2.2. Doubly Robust Estimator", "text": "Double robust estimators use both the estimate of the expected reward and the estimate of the probabilities of action p (a | x, h). Here we use a DR estimator of the form first proposed by Cassel et al. (1976) for regression but not previously studied for political learning: V-\u03c0DR = 1 | S | \u2211 (x, h, a, ra) Ps [(ra \u2212 a (x))) I (\u03c0 (x) = a) p (a | x, h) + \u03c0 (x) (x)]]. (1) Informally, the estimator uses pe as the baseline and when data is available, a correction is applied. We will see that our estimator is accurate if at least one of the estimators, pe and p, is correct, hence the name is doubly robust.In practice, it is rare to have an accurate estimate of or p."}, {"heading": "3. Bias Analysis", "text": "Let us call the additive deviation from \"p\" and \"p\" \"h,\" although \"h\" (e.g. \"q\") remains a multiplicative deviation from \"p\" (\"p\") from \"p\" (\"p\") (\"a,\" \"h\"). To eliminate clutter, we use the abbreviations \"a\" for \"a\" (\"x,\" \"a\"), \"a\" for \"a\" (\"o\"), \"o\" (\"o\"), \"o\" o \"(o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o \"o\" o o \"o\" o \"o o\" o \"o\" o o \"o\" o \"o o\" o \"o\" o \"o\" o \"o\" o \"o\" o \""}, {"heading": "If the past policy and the past policy estimate are stationary", "text": "(i.e., independent of h), the expression simplifies to | E [V] \u03c0DR] \u2212 V \u03c0 | = | Ex [\u2206 \u03b4] |.In contrast (for simplicity's sake, we assume stationarity): | E [V] \u03c0DM] \u2212 V \u03c0 | = | Ex [V] \u03c0IPS] \u2212 V \u03c0 | = | Ex [\u03c0 (x) \u03b4 |, the second equality being based on the observation that IPS is a special case of DR for (x) \u0445 0.Generally, none of the estimators dominates the others. However, if one of the two estimators dominates the others, the expected value of the doubly robust estimator will be close to the true value, whereas DM and IPS require a better estimate."}, {"heading": "4. Variance Analysis", "text": "In the previous section, we argued that the expected value of V-shaped effects is positively compared with IPS and DM (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects) (Ex-Effects)"}, {"heading": "5. Experiments", "text": "This section provides empirical evidence for the effectiveness of the DR estimator in comparison to IPS and DM. We consider two classes of problems: classifying multiclass with bandit feedback in public benchmark datasets and estimating the average user visits to an Internet portal."}, {"heading": "5.1. Multiclass Classification with Bandit Feedback", "text": "We begin with a description of how to turn a k-class classification task into a problem of k-armed contextual bandits. This transformation allows us to compare IPS and DR by using public data sets for both policy evaluation and learning."}, {"heading": "5.1.1. DATA SETUP", "text": "In a classification task, we assume that the IID is drawn from a fixed distribution: (x, c) \u0445 D, where x-X is the attribute vector and c-X (1, 2,..., k) is the class name. A typical goal is to find a classifier \u03c0: X 7 \u2192 {1, 2,..., k}, which minimizes the classification error: e (\u03c0) = E (x, c) \u0445 D [I (x) 6 = c)]. Alternatively, we can turn the data point (x, c) into a cost-sensitive classification example (x, l1, l2,..., lk), where la = I (a 6 = c) is the loss for predicting one. Then, a classification problem can be interpreted as a policy of measurement selection, and its classification errors are exactly the expected losses of policy."}, {"heading": "5.1.2. POLICY EVALUATION", "text": "For each set of data, we perform a direct loss minimization (DLM) algorithm by McAllester et al. (2011) in order to obtain a classifier (see Appendix A for details), which represents the guidelines we evaluate using test data; 3. We calculate the classification error based on fully observed test data. This error is treated as a basic truth for comparing various estimates; 4. Finally, we apply the transformation in Section 5.1.1 to the test data to obtain a partial set from which DM, IPS and DR estimates are calculated. Both DM and DR require an estimate of the expected conditional loss as l (x, a) for given (x, a).We use a linear loss model: D, IPS and DR estimates are calculated as a result."}, {"heading": "5.1.3. POLICY OPTIMIZATION", "text": "Since DM is significantly worse on all data sets, as shown in Fig. 1, we focus on the comparison between IPS and DR. Here, we apply the data transformation in Section 5.1.1 to the training data and then learn a classifier based on the losses estimated by IPS and DR. Specifically, we repeat for each data set the following steps 30 times: 1. We randomly divide the data into training data (70%) and test records (30%); 2. We apply the transformation in Section 5.1.1 to the training data to obtain a partially designated set; 3. We then use the IPS and DR estimators to imply non-obvious losses in the training data; 4. Cost-sensitive multiclass classification algorithms are used to learn a classifier from the losses completed by either IPS or DR: the first is DLM (McAllester et al., 2011)."}, {"heading": "5.2. Estimating Average User Visits", "text": "The next problem we are looking at is estimating the average number of user visits to a popular Internet portal."}, {"heading": "6. Conclusions", "text": "Our analysis shows that doubly robust methods tend to provide more reliable and accurate estimates, supported by experiments with both benchmark data and a large, real-world problem. In the future, we expect DR technology to become common practice in improving context-dependent bandit algorithms. As an example, it is interesting to develop a variant of Offset Tree that can benefit from better reward models rather than a rough, constant reward estimate (Beygelzimer & Langford, 2009)."}, {"heading": "Acknowledgements", "text": "We would like to thank Deepak Agarwal for first drawing our attention to the doubly robust technology."}, {"heading": "A. Direct Loss Minimization", "text": "In view of the cost-sensitive classification data of the multiclass (x, l1,., lk), we perform an approximate weight loss of policy loss (or classification error). In the experiments of Section 5.1, the political benefit is determined by k-weight vectors \u03b81,... Given the x-X method, the policy predicts that \u03c0 (x) = argmaxa (1,..., lk) and the current weight proportions, the weight adjustments will be adjusted by the \"better\" version of the direct loss minimization method by McAllester et al. (2011) as follows: In light of the data (x, l1,., lk) and the current weight adjustments, the weight adjustments will be adjusted by the results a1, a1 + x, 2 and 4."}, {"heading": "B. Filter Tree", "text": "The filter tree (Beygelzimer et al., 2008) is a reduction from the cost-sensitive classification to the binary classification. It is entered in the same way as in direct loss minimization, but its output is a binary tree-based predictor where each node of the filter tree uses a binary classifier - in this case the J48 decision tree implemented in Weka 3.6.4 (Hall et al., 2009). Thus, there are two-class decision trees in the node, with the nodes arranged according to a filter tree. Formation in a filter tree is bottom-up, with each trained node filtering the examples observed by its parent until the entire tree is traversed."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The offset tree for learning with partial labels", "author": ["A. Beygelzimer", "J. Langford"], "venue": "In KDD, pp", "citeRegEx": "Beygelzimer and Langford,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer and Langford", "year": 2009}, {"title": "Multiclass classification with filter-trees", "author": ["A. Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": "Unpublished technical report: http:// www.stat.berkeley.edu/\u223cpradeepr/paperz/filter-tree.pdf,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2008}, {"title": "Some results on generalized difference estimation and generalized regression estimation for finite populations", "author": ["C.M. Cassel", "C.E. S\u00e4rndal", "J.H. Wretman"], "venue": null, "citeRegEx": "Cassel et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Cassel et al\\.", "year": 1976}, {"title": "Evaluating online ad campaigns in a pipeline: causal models at scale", "author": ["D. Chan", "R. Ge", "O. Gershony", "T. Hesterberg", "D. Lambert"], "venue": "In KDD,", "citeRegEx": "Chan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2010}, {"title": "Dataset shift in machine learning. In Covariate Shift and Local Learning by Distribution Matching, pp. 131\u2013160", "author": ["A. Gretton", "A.J. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Better algorithms for benign bandits", "author": ["E. Hazan", "S. Kale"], "venue": "In SODA, pp", "citeRegEx": "Hazan and Kale,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2009}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["D.G. Horvitz", "D.J. Thompson"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Horvitz and Thompson,? \\Q1952\\E", "shortCiteRegEx": "Horvitz and Thompson", "year": 1952}, {"title": "Demystifying double robustness: a comparison of alternative strategies for estimating a population mean from incomplete data", "author": ["J.D.Y. Kang", "J.L. Schafer"], "venue": "Statist. Sci.,", "citeRegEx": "Kang and Schafer,? \\Q2007\\E", "shortCiteRegEx": "Kang and Schafer", "year": 2007}, {"title": "More bang for their bucks: assessing new features for online advertisers", "author": ["D. Lambert", "D. Pregibon"], "venue": "In ADKDD,", "citeRegEx": "Lambert and Pregibon,? \\Q2007\\E", "shortCiteRegEx": "Lambert and Pregibon", "year": 2007}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "In NIPS, pp", "citeRegEx": "Langford and Zhang,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang", "year": 2008}, {"title": "Exploration scavenging", "author": ["J. Langford", "A.L. Strehl", "J. Wortman"], "venue": "In ICML, pp", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Stratification and weighting via the propensity score in estimation of causal treatment effects: A comparative study", "author": ["J.K. Lunceford", "M. Davidian"], "venue": "Statistics in Medicine,", "citeRegEx": "Lunceford and Davidian,? \\Q2004\\E", "shortCiteRegEx": "Lunceford and Davidian", "year": 2004}, {"title": "Direct loss minimization for structured prediction", "author": ["D. McAllester", "T. Hazan", "J. Keshet"], "venue": "In NIPS, pp", "citeRegEx": "McAllester et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2011}, {"title": "Semiparametric efficiency in multivariate regression models with missing data", "author": ["J. Robins", "A. Rotnitzky"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Robins and Rotnitzky,? \\Q1995\\E", "shortCiteRegEx": "Robins and Rotnitzky", "year": 1995}, {"title": "Estimation of regression coefficients when some regressors are not always observed", "author": ["J.M. Robins", "A. Rotnitzky", "L.P. Zhao"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Robins et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Robins et al\\.", "year": 1994}, {"title": "Learning from logged implicit exploration data", "author": ["A. Strehl", "J. Langford", "L. Li", "S. Kakade"], "venue": "In NIPS, pp", "citeRegEx": "Strehl et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Both of these problems are instances of contextual bandits (Auer et al., 2002; Langford & Zhang, 2008).", "startOffset": 59, "endOffset": 102}, {"referenceID": 12, "context": "Here, we focus on the offline version: we assume access to historic data, but no ability to gather new data (Langford et al., 2008; Strehl et al., 2011).", "startOffset": 108, "endOffset": 152}, {"referenceID": 17, "context": "Here, we focus on the offline version: we assume access to historic data, but no ability to gather new data (Langford et al., 2008; Strehl et al., 2011).", "startOffset": 108, "endOffset": 152}, {"referenceID": 3, "context": "Doubly robust (or doubly protected) estimation (Cassel et al., 1976; Robins et al., 1994; Robins & Rotnitzky, 1995; Lunceford & Davidian, 2004; Kang & Schafer, 2007) is a statistical approach for estimation from incomplete data with an important property: if either one of the two estimators (in DM and IPS) is correct, then the estimation is unbiased.", "startOffset": 47, "endOffset": 165}, {"referenceID": 16, "context": "Doubly robust (or doubly protected) estimation (Cassel et al., 1976; Robins et al., 1994; Robins & Rotnitzky, 1995; Lunceford & Davidian, 2004; Kang & Schafer, 2007) is a statistical approach for estimation from incomplete data with an important property: if either one of the two estimators (in DM and IPS) is correct, then the estimation is unbiased.", "startOffset": 47, "endOffset": 165}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010).", "startOffset": 119, "endOffset": 164}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)).", "startOffset": 146, "endOffset": 440}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)).", "startOffset": 146, "endOffset": 469}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)).", "startOffset": 146, "endOffset": 496}, {"referenceID": 4, "context": "More recently, it has been used in Internet advertising to estimate the effects of new features for online advertisers (Lambert & Pregibon, 2007; Chan et al., 2010). Previous work focuses on parameter estimation rather than policy evaluation/optimization, as addressed here. Furthermore, most of previous analysis of doubly robust estimation studies asymptotic behavior or relies on various modeling assumptions (e.g., Robins et al. (1994), Lunceford & Davidian (2004), and Kang & Schafer (2007)). Our analysis is non-asymptotic and makes no such assumptions. Several other papers in machine learning have used ideas related to the basic technique discussed here, although not with the same language. For benign bandits, Hazan & Kale (2009) construct algorithms which use reward estimators in order to achieve a worst-case regret that depends on the variance of the bandit rather than time.", "startOffset": 146, "endOffset": 741}, {"referenceID": 17, "context": "It is expected that better evaluation generally leads to better optimization (Strehl et al., 2011).", "startOffset": 77, "endOffset": 98}, {"referenceID": 3, "context": "Here, we use a DR estimator of the form first suggested by Cassel et al. (1976) for regression, but previously not studied for policy learning:", "startOffset": 59, "endOffset": 80}, {"referenceID": 14, "context": "On the training set with fully revealed losses, we run a direct loss minimization (DLM) algorithm of McAllester et al. (2011) to obtain a classifier (see Appendix A for details).", "startOffset": 101, "endOffset": 126}, {"referenceID": 14, "context": "Two cost-sensitive multiclass classification algorithms are used to learn a classifier from the losses completed by either IPS or DR: the first is DLM (McAllester et al., 2011), the other is the Filter Tree reduction of Beygelzimer et al.", "startOffset": 151, "endOffset": 176}, {"referenceID": 2, "context": ", 2011), the other is the Filter Tree reduction of Beygelzimer et al. (2008) applied to a decision tree (see Appendix B for more details);", "startOffset": 51, "endOffset": 77}, {"referenceID": 5, "context": "To define the sampling probabilities pi, we adopted a similar approach as in Gretton et al. (2008). In particular, we obtained the first principal component (denoted x\u0304) of all features {xi}, and projected all data onto x\u0304.", "startOffset": 77, "endOffset": 99}, {"referenceID": 14, "context": "To optimize \u03b8a, we adapt the \u201ctowards-better\u201d version of the direct loss minimization method of McAllester et al. (2011) as follows: given any data (x, l1, .", "startOffset": 96, "endOffset": 121}], "year": 2011, "abstractText": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance. In this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust approach uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice.", "creator": "gnuplot 4.2 patchlevel 2 "}}}