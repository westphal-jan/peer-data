{"id": "1104.2018", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2011", "title": "Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression", "abstract": "Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints. However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient. We also provide an empirical study, demonstrating their feasibility in practice.", "histories": [["v1", "Mon, 11 Apr 2011 18:24:01 GMT  (37kb,D)", "http://arxiv.org/abs/1104.2018v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["sham m kakade", "adam kalai", "varun kanade", "ohad shamir"], "accepted": true, "id": "1104.2018"}, "pdf": {"name": "1104.2018.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression", "authors": ["Sham Kakade", "Adam Tauman Kalai"], "emails": ["skakade@wharton.upenn.edu", "adum@microsoft.com", "vkanade@fas.harvard.edu", "ohadsh@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2 Setting", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,"}, {"heading": "5 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Proof of Thm. 2", "text": "First, we need a property of the LPAV algorithm that is used to find the best one-dimensional non-decreasing 1-Lipschitz function. Formally, this problem can be defined as: Given as input < (1) it is minimal, under the condition that y-i = u (zi) for some non-decreasing 1-Lipschitz function u: [\u2212 W, W] 7 \u2192 [0, 1]. After finding such values, LPAV gets a complete function u by interpolating between the points. Assuming zi are in sorted order, this can be formulated as a square problem with the following constraints: y-i."}, {"heading": "6 Experiments", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcnlrVo"}, {"heading": "6.2 Real World Datasets", "text": "We now turn to describing the results of experiments carried out on several UCI datasets. We chose the following 5 datasets: communities, concrete, homes, Parkinsons, and wine quality.On each dataset, we compared the performance of L-isotron (L-Iso) and GLM-Tron (GLM-t) with isotron and several other algorithms, including standard logistic regression (Log-R), linear regression (Lin-R), and a simple heuristic algorithm (SIM) for individual index models, along the lines of iterative highest probability procedures for this type of problem (e.g. [Cos83]).The algorithm works by iteratively determining the direction w and finding the best transfer function u, and then fixing u and optimizing w by means of gradient descent."}, {"heading": "A Appendix", "text": "The main quandary shows that the error of the current hypothesis is as long as the distance of our predicted direction vector from the ideal direction is w \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212"}, {"heading": "B Proof of Proposition 1", "text": "It is easy to see that the proposition follows from the following uniform convergence guarantee: Theorem 3. With a probability of at least 1 \u2212 Z, for each fixed w \u00b2 W, where d may be infinite (for example, if we use both cores). It is easy to see that the proposition follows from the following uniform convergence guarantee: Theorem 3. With a probability of at least 1 \u2212 Z, for each fixed w \u00b2 W, if we allow the arg min., u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u, u"}], "references": [{"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Distribution-free maximum-likelihood estimator of the binary choice model", "author": ["S. Cosslett"], "venue": null, "citeRegEx": "Cosslett.,? \\Q1983\\E", "shortCiteRegEx": "Cosslett.", "year": 1983}, {"title": "Direct semiparametric estimation of single-index models with discrete covariates", "author": ["J. Horowitz", "W. H\u00e4rdle"], "venue": null, "citeRegEx": "Horowitz and H\u00e4rdle.,? \\Q1994\\E", "shortCiteRegEx": "Horowitz and H\u00e4rdle.", "year": 1994}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["A.T. Kalai", "R. Sastry"], "venue": "In COLT \u201909,", "citeRegEx": "Kalai and Sastry.,? \\Q2009\\E", "shortCiteRegEx": "Kalai and Sastry.", "year": 2009}, {"title": "Improving the sample complexity using global data", "author": ["S. Mendelson"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Mendelson.", "year": 2002}, {"title": "Direct estimation of the index coefficients in a singleindex model", "author": ["A. Juditsky M. Hristache", "V. Spokoiny"], "venue": "Technical Report 3433,", "citeRegEx": "Hristache and Spokoiny.,? \\Q1998\\E", "shortCiteRegEx": "Hristache and Spokoiny.", "year": 1998}, {"title": "Generalized Linear Models (2nd ed.)", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "McCullagh and Nelder.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh and Nelder.", "year": 1989}, {"title": "Isotonic single-index model for high-dimensional database marketing", "author": ["P. Naik", "C. Tsai"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Naik and Tsai.,? \\Q2004\\E", "shortCiteRegEx": "Naik and Tsai.", "year": 2004}, {"title": "The Volume of Convex Bodies and Banach Space Geometry", "author": ["G. Pisier"], "venue": null, "citeRegEx": "Pisier.,? \\Q1999\\E", "shortCiteRegEx": "Pisier.", "year": 1999}, {"title": "Single index convex experts: Efficient estimation via adapted bregman losses", "author": ["P. Ravikumar", "M. Wainwright", "B. Yu"], "venue": "Snowbird Workshop,", "citeRegEx": "Ravikumar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2008}, {"title": "Learning kernel-based halfspaces with the zeroone loss", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Smoothness, low-noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Christianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Christianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Christianini.", "year": 2004}, {"title": "Optimal smoothing in single-index models", "author": ["P. Hall W. H\u00e4rdle", "H. Ichimura"], "venue": "Annals of Statistics,", "citeRegEx": "H\u00e4rdle and Ichimura.,? \\Q1993\\E", "shortCiteRegEx": "H\u00e4rdle and Ichimura.", "year": 1993}, {"title": "Isotonic regression under lipschitz constraint", "author": ["L. Yeganova", "W.J. Wilbur"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Yeganova and Wilbur.,? \\Q2009\\E", "shortCiteRegEx": "Yeganova and Wilbur.", "year": 2009}, {"title": "Risk bounds for isotonic regression", "author": ["C.H. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang.,? \\Q2002\\E", "shortCiteRegEx": "Zhang.", "year": 2002}, {"title": "Covering number bounds for certain regularized function classes", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2002\\E", "shortCiteRegEx": "Zhang.", "year": 2002}], "referenceMentions": [{"referenceID": 3, "context": "Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints.", "startOffset": 0, "endOffset": 24}], "year": 2011, "abstractText": "Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints. However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient. We also provide an empirical study, demonstrating their feasibility in practice.", "creator": "LaTeX with hyperref package"}}}