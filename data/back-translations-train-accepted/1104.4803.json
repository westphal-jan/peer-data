{"id": "1104.4803", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2011", "title": "Clustering Partially Observed Graphs via Convex Optimization", "abstract": "This paper considers the problem of clustering a partially observed unweighted graph -- i.e. one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters.", "histories": [["v1", "Mon, 25 Apr 2011 20:33:55 GMT  (64kb)", "https://arxiv.org/abs/1104.4803v1", null], ["v2", "Thu, 28 Apr 2011 19:08:49 GMT  (1684kb,D)", "http://arxiv.org/abs/1104.4803v2", "Accepted to ICML 2011"], ["v3", "Wed, 20 Feb 2013 21:27:20 GMT  (443kb,D)", "http://arxiv.org/abs/1104.4803v3", "Appeared in ICML 2011"], ["v4", "Thu, 24 Jul 2014 00:44:05 GMT  (442kb,D)", "http://arxiv.org/abs/1104.4803v4", "This is the final version published in Journal of Machine Learning Research (JMLR). Partial results appeared in International Conference on Machine Learning (ICML) 2011"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ali jalali", "yudong chen", "sujay sanghavi", "huan xu"], "accepted": true, "id": "1104.4803"}, "pdf": {"name": "1104.4803.pdf", "metadata": {"source": "CRF", "title": "Clustering Partially Observed Graphs via Convex Optimization", "authors": ["Yudong Chen", "Ali Jalali", "Sujay Sanghavi", "Huan Xu", "Marina Meila"], "emails": ["YDCHEN@UTEXAS.EDU", "ALIJ@MAIL.UTEXAS.EDU", "SANGHAVI@MAIL.UTEXAS.EDU", "MPEXUH@NUS.EDU.SG"], "sections": [{"heading": null, "text": "Our algorithm uses convex optimization; its basis is a reduction of discrepancy minimization to the problem of restoring a (unknown) low-ranking matrix and a (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm using the classic Planted Partition / Stochastic Block model. Our main theorem provides sufficient conditions for the success of our algorithm depending on the minimum cluster size, edge density, and observation probability; in particular, the results characterize the trade-off between the probability of observation and the edge density gap. If there is a constant number of clusters of the same size, our results are optimal up to logarithmic factors. Keywords: graphs, clustering, condensation optimization, and minor decomposition."}, {"heading": "1. Introduction", "text": "This work deals with the following task: Given the partial observation of an undirected, unweighted graph, the nodes divide into fragmented clusters, so that there are dense connections within clusters, and sparse connections between clusters. By partial observation, we mean that for some node pairs we know whether there is an edge or not, and for the other node pairs we do not know - see e.g. Yahoo! -Inc. (2009) for such a project in science and technology. In sponsored search, each cluster is a submarket that represents a specific group of advertisers who spend a large portion of their expenditure on a group of query formulations - see Yahoo! -Inc. for such a project in VLSI and design automation, it is useful to minimize signaling between components (Kernigan and Lin, 1970). In social networks, clusters can represent groups of people with a certain number of clusters."}, {"heading": "1.1 Our Approach", "text": "We focus on a natural formulation that requires no further external input besides the graph itself, based on the minimization of discrepancies that we are now defining. Consider any candidate clustering; this will have (a) observed node pairs that are in different clusters but have an edge between them, and (b) observed node pairs that are in the same cluster but have no edge between them. (a) The total number of node pairs of types (a) and (b) is the number of discrepancies between clustering and the given graph. We focus on the problem of finding the optimal cluster formation - one that minimizes the number of discrepancies. Note that we do not specify the number of clusters beforehand. In the specific case of fully observed graphs, this formulation is exactly the same as the problem of correlation cluster formation first proposed by Bansal et al (2002), the matrix showing that the exact object."}, {"heading": "1.2 Related Work", "text": "Our problem can be interpreted in the general cluster context to mean that the presence of an edge between two points indicates a \"similarity\" and the absence of an edge means \"no similarity.\" The general field of clustering is, of course, extensive, and a detailed overview of all the methods contained therein is outside our scope. Instead, we will focus on the three papers most relevant to this problem: the work on correlation clustering, the planted partition / stochastic block model, and graph clustering with partial observations."}, {"heading": "1.2.1 CORRELATION CLUSTERING", "text": "As I said, for a fully observed graph, our problem is mathematically exactly the same as the correlation formulated in Bansal et al. (2002); in particular, a \"+\" in correlation clustering corresponds to an edge in the graph, a \"-\" around the absence of an edge, and inconsistencies are defined in the same way. Thus, this paper can be considered equivalent to an algorithm, and guarantees correlation clustering under partial observations. Since correlation clustering is NP-hard, much work has been done on the development of alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003). Approximations with convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010)."}, {"heading": "1.2.2 PLANTED PARTITION MODEL", "text": "The planted partition model, also known as the stochastic block model (Condon and Karp, 2001; Holland et al., 1983), assumes that the graph is generated with the probability p and q (where p > q) and is fully observed. The goal is to restore the latent cluster structure. A class of this model with the probability p and q is often used as a benchmark for the average case performance of correlation clusters (see e.g. Mathieu and Schudy, 2010). Our theoretical results are applicable to this model and thus directly comparable to existing work in this area. A detailed comparison is in Table 1. For fully observed graphs, our result provides the best results in terms of both the minimum cluster size and the difference between cluster / inter-cluster densities. We would like to point out that nuclear standardization was used to solve the closely related click problem (Alon, Alon, 1998 and Vasies, 2011)."}, {"heading": "1.2.3 PARTIALLY OBSERVED GRAPHS", "text": "In addition, previous work in Table 1, with the exception of Oymak and Hassibi (2011), does not deal directly with partial observations; a natural course of action is to assume the missing observations with noedge or random edges with symmetric probabilities and then apply one of the results in Table 1. However, this approach yields suboptimal results; in fact, this is explicitly done by Oymak and Hassibi (2011), which require the probability of observing p0 to satisfy p0 & \u221a Kmin n, with n the number of nodes and kmin being the minimum cluster size; in contrast, our approach only requires p0 & nK2min (both right sides must be less than 1, so that the right side of our state is properly smaller and therefore less restrictive.) Shamir and Tishby (2011) deal directly with partial observations and show that p0 & 1n is sufficient to recover two clusters of size."}, {"heading": "2. Main Results", "text": "Our structure for the problem of graph clustering is as follows: We get a partially observed graph of n nodes whose adjacence matrix is A-Rn \u00b7 n, which has ai, j = 1 if there is an edge between the nodes i and j, ai, j = 0 if there is no edge, and ai, j = \"?\" if we do not know. (Here we follow the convention that ai, i = 0 for all i.) Let us determine the number of observed node pairs: ai, j 6 =?}. The goal is to find the optimal cluster formation, i.e. the one that has the minimum number of discrepancies (defined in Section 1.1). For the rest of this section, we present our algorithm for the above task and analyze its performance under the planted partition model with partial observations. We also study the optimism of the performance of our algorithm by deriving a necessary condition for each algorithm to be successful."}, {"heading": "2.1 Algorithm", "text": "It is not only the way in which we represent the cluster of clues and then describe the algorithms, but also the way in which the clues are arranged in the cluster of clues. (In this case, the clue arrangement is a clique.) In this case, the matrix A + I is a low-level matrix in which the number of clues is already ideally clustered - i.e., there is a division of clues so that there is no edge between the clues, and each clue point is a clique. (In this case, the matrix A + I is a low-level matrix with which the clue order is equal.) This can be seen by noting that if we rearrange the rows and columns so that the clues appear together, the result would be a block diagonal, with each block being subjective."}, {"heading": "2.2 Performance Analysis", "text": "For the most important analytical contributions of this paper, we have given the probability 1 \u2212 j = 1, the probability 1 \u2212 j = 1, which minimizes the number of discrepancies among the observed entries. (We have not decreased the number of discrepancies among the observed entries.) Suppose the n-nodes are divided into r-nodes, each with at least one size gap. (Let's show the low matrix corresponding to this cluster (as described above). The adjacency matrix A of the graph is generated as follows: for each pair of nodes (i, j) in the same cluster, ai = 1 \u2212 p0, ai = 1 with the probability p0p, or aij = 0, independent of all others; similarly, for (i, j) in different clusters, ai = 1 \u2212 p0 with the probability, j = 1 with the probability p0p, or aij = 0, independent of all others. (i, j)"}, {"heading": "2.3 Lower Bounds", "text": "We will now discuss the density of Theorem 4. Let us first consider the case in which Kmin = \u0432 (n) is required, which means that there is a constant number of clusters. We will establish a fundamentally lower limit for the density gap 1 \u2212 2\u03c4 and the observation probability p0, which is required for each algorithm to correctly restore the clusters. Theorem 5 Within the framework of the planted partition model with partial observations, we will assume that true clustering is uniformly randomly selected from all possible clusters with the same cluster size K. If K = \u0432 (n) and \u03c4 = 1 \u2212 p > 1 / 100, then for each algorithm to correctly identify the clusters we must select the accuracy with a probability of at least 34, with C > 0 being an absolute constant. Theorem 5 generalizes a similar result in Chaudhuri et al. (2012), which does not take partial observations into account."}, {"heading": "3. Proofs", "text": "In this section we prove theorems 2 and 4. The proof for theorem 5 is moved to appendix B."}, {"heading": "3.1 Proof of Theorem 2", "text": "We first prove theorem 2, which states that if the optimization problem (1) produces a valid matrix, i.e. one that corresponds to clustering of the nodes, this is the disunity-minimizing clustering. Consider the following non-convex optimization problem in B, K, B, K, and K. Since K is a valid clustering, it is positively semidefined and has all the ones along its diagonal. Therefore, it obeys the disunity (K) = track (K) = n. On the other hand, since both K \u2212 I and A are adjazency matrices, the entries of B = I + A \u2212 K in the mix are equal to \u2212 1, 1, or 0 (i.e. it is a disunity matrix)."}, {"heading": "3.2 Proof of Theorem 4", "text": "We now turn to Theorem 4, which provides guarantees as to when the convex program (1) restores true clustering (B *, K *)."}, {"heading": "3.2.1 PROOF OUTLINE AND PRELIMINARIES", "text": "We want to show that it is sufficient to consider an equivalent model for observation and discrepancies, especially if the observation probability and density gap are negligible, which is the interest in this paper.Step 2: We write down the sub-gradient model, which addresses sufficient conditions for the satisfaction of the (B-grading, K-grading) to be the unique optimum of (1). In our case, it is about the existence of a matrix W - the dual certificates - which satisfies certain characteristics. This step is technical - and requires us to deal with the complicated conditions of sub-gradients. It is about showing the existence of a matrix W - which satisfies certain characteristics."}, {"heading": "3.2.2 STEP 1: EQUIVALENT MODEL FOR OBSERVATIONS AND DISAGREEMENTS", "text": "It is easy to show that an increase of p or a decrease of q can only make the probability of success higher, so we proceed without loss of generality from 1 \u2212 p = q = \u043d. Note that the probability of success is entirely determined by the distribution of (b) under the planted partition model with partial observations. The first step is to show that it is sufficient to consider an equivalent model for generation (b), which leads to the same distribution but is easier to handle, in accordance with the spirit of Cande and others (2011, theorems 2.2 and 2.3) and Li (2013, Section 4.1). Specifically, let us consider the following procedure: 1. Allow Ber1 (p0 \u2212 2), and Ber0 (2p), Ber0 (2p + 2p) and Ber0 (2p + 2p) and Ber0 (2p + 2p + 2p), that the original model, 1 \u2212 p0obs and 2), that we include the original model of deobs, we [the etrical, the symmetrical and the upper matrix]."}, {"heading": "3.2.3 STEP 2: SUFFICIENT CONDITIONS FOR OPTIMALITY", "text": "We specify the conditions of the first order, which guarantee that (B *, K *) is the unique optimum of (1) with a high probability. At this point and in the future, we mean with a high probability at least 1 \u2212 cn \u2212 10 for a universal constant c > 0. The following problem follows from Theorem 4.4 in Li (2013) and the discussion thereafter. Lemma 6 (optimality condition) is the only optimal solution for (1) with a high probability, if there is W * Rn \u00b7 n such."}, {"heading": "3.2.4 STEP 3: DUAL CERTIFICATE CONSTRUCTION", "text": "We use a variant of the so-called \"Golfing Scheme\" (Cande-s et al., 2011; Gross, 2011) to construct W (Gross, 2011). Our application of the Golfing Scheme, as well as its analysis, differs from previous work and leads to stronger guarantees. In particular, we can go beyond existing results by allowing the proportion of observed entries and the density gap that is vanishable. By definition in Section 3.2.2, the parameter t is obeyed p0 (1 \u2212 2\u043d). Note that we are considered as generated values if the sentences in Section 3.2 (t) are independent. Here, the parameter t is obeyed p0 (1 \u2212 2\u0442)."}, {"heading": "4. Experimental Results", "text": "We explore the performance of our algorithm by means of simulation as a function of the values of the model parameters (n, Kmin, p0, \u03c4). We see that the performance corresponds well with the theory. In the experiment, each test case is constructed by creating a graph with n nodes, which is divided into clusters of the same size, and then placing a disagreement about each pair of nodes with the probability independently of each other. We then perform algorithm 1, in which the optimization problem (1) is solved with the fast algorithm in Lin et al (2009). We check whether the algorithm succeeds in issuing a solution that corresponds to the underlying true clusters. In the first set of experiments, we fix stages = 0.2 and Kmin = n / 4 and variable p0 and n. For each (p0, n) we repeat the experiment for 5 times and specify the probability of success in the left figure of Figure 2.0."}, {"heading": "5. Conclusion", "text": "We proposed a convex optimization formulation based on a reduction to decaying low-level matrices and sparse matrices to solve the problem of clustering partially observed graphs. We demonstrated that our method is guaranteed to find the optimal (discrepancy minimization) clustering for a wide range of parameters of the planted partition model with partial observations. In particular, our method is more successful than existing methods in this environment in the case of higher noise and / or missing observations. The effectiveness of the proposed method and the scaling of the theoretical results are validated by simulation studies. This work is motivated by graph cluster applications, where collecting similarity data is expensive and it is desirable to use as few observations as possible."}, {"heading": "Acknowledgments", "text": "S. Sanghavi would like to acknowledge the DTRA grant HDTRA1-13-1-0024 and the NSF grant 1302435, 1320175 and 0954059. H. Xu is partially supported by the Singapore Ministry of Education through the AcRF Tier Two grant R-265-000-443-112. The authors thank the anonymous reviewers for their thorough reviews of this work and valuable suggestions for improving the manuscript."}, {"heading": "Appendix A. Technical Lemmas", "text": "In this section we provide several auxiliary lemmas required in the Proof of Theorem 4."}, {"heading": "Appendix B. Proof of Theorem 5", "text": "We use a standardized information technology argument that refers to a related proof by Chaudhuri et al. (2012). Let K be the size of the clusters (which are assumed to be of equal size). For simplicity, let us assume that n / K is an integer. Let F be the set of all possible parts of n nodes in n / K clusters of equal size (n / K). Using Stirling's approximation, we have \u2212 M, \u2212 F | = 1 (n / K)! (n \u2212 K) \u00b7 (K) \u2265 (n 3K) n (1 \u2212 1 K) n (1 \u2212 1 n 1 n 1). Let us give cluster formation Y uniformly according to the random principle of F, and graph A is generated according to the planted partition model of Y with partial observations, using aij =? unobserved pairs. We use PA | Y to determine the distribution of A given Y."}], "references": [{"title": "Finding a large hidden clique in a random graph", "author": ["N. Alon", "M. Krivelevich", "B. Sudakov"], "venue": "In Proceedings of the 9th annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Alon et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1998}, {"title": "Nuclear norm minimization for the planted clique and biclique problems", "author": ["B. Ames", "S. Vavasis"], "venue": "Mathematical Programming,", "citeRegEx": "Ames and Vavasis.,? \\Q2011\\E", "shortCiteRegEx": "Ames and Vavasis.", "year": 2011}, {"title": "Robust hierarchical clustering", "author": ["M.F. Balcan", "P. Gupta"], "venue": "In Proceedings of the Conference on Learning Theory (COLT),", "citeRegEx": "Balcan and Gupta.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Gupta.", "year": 2010}, {"title": "Correlation clustering", "author": ["N. Bansal", "A. Blum", "S. Chawla"], "venue": "In Proceedings of the 43rd Symposium on Foundations of Computer Science,", "citeRegEx": "Bansal et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2002}, {"title": "Max cut for random graphs with a planted partition", "author": ["B. Bollob\u00e1s", "A.D. Scott"], "venue": "Combinatorics, Probability and Computing,", "citeRegEx": "Bollob\u00e1s and Scott.,? \\Q2004\\E", "shortCiteRegEx": "Bollob\u00e1s and Scott.", "year": 2004}, {"title": "Eigenvalues and graph bisection: An average-case analysis", "author": ["R.B. Boppana"], "venue": "In Proceedings of the 28th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Boppana.,? \\Q1987\\E", "shortCiteRegEx": "Boppana.", "year": 1987}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Robust principal component analysis", "author": ["E. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Hill-climbing finds random planted bisections", "author": ["T. Carson", "R. Impagliazzo"], "venue": "In Proceedings of the 12th annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Carson and Impagliazzo.,? \\Q2001\\E", "shortCiteRegEx": "Carson and Impagliazzo.", "year": 2001}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P. Parrilo", "A. Willsky"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Clustering with qualitative information", "author": ["M. Charikar", "V. Guruswami", "A. Wirth"], "venue": "In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "Charikar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2003}, {"title": "Spectral clustering of graphs with general degrees in the extended planted partition model", "author": ["K. Chaudhuri", "F. Chung", "A. Tsiatas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "Low-rank matrix recovery from errors and erasures", "author": ["Y. Chen", "A. Jalali", "S. Sanghavi", "C. Caramanis"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Random Structures and Algorithms,", "citeRegEx": "Condon and Karp.,? \\Q2001\\E", "shortCiteRegEx": "Condon and Karp.", "year": 2001}, {"title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications", "author": ["A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborov\u00e1"], "venue": "Physical Review E,", "citeRegEx": "Decelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Decelle et al\\.", "year": 2011}, {"title": "Correlation clustering with partial information. Approximation, Randomization, and Combinatorial Optimization: Algorithms and Techniques", "author": ["E.D. Demaine", "N. Immorlica"], "venue": null, "citeRegEx": "Demaine and Immorlica.,? \\Q2003\\E", "shortCiteRegEx": "Demaine and Immorlica.", "year": 2003}, {"title": "Correlation clustering in general weighted graphs", "author": ["E.D. Demaine", "D. Emanuel", "A. Fiat", "N. Immorlica"], "venue": "Theoretical Computer Science,", "citeRegEx": "Demaine et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Demaine et al\\.", "year": 2006}, {"title": "Correlation clustering minimizing disagreements on arbitrary weighted graphs", "author": ["D. Emmanuel", "A. Fiat"], "venue": "In Proceedings of the 11th Annual European Symposium on Algorithms,", "citeRegEx": "Emmanuel and Fiat.,? \\Q2003\\E", "shortCiteRegEx": "Emmanuel and Fiat.", "year": 2003}, {"title": "Active clustering: Robust and efficient hierarchical clustering using adaptively selected similarities", "author": ["B. Eriksson", "G. Dasarathy", "A. Singh", "R. Nowak"], "venue": "Arxiv preprint arXiv:1102.3887,", "citeRegEx": "Eriksson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 2011}, {"title": "A database interface for clustering in large spatial databases", "author": ["M. Ester", "H. Kriegel", "X. Xu"], "venue": "In Proceedings of the International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Ester et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ester et al\\.", "year": 1995}, {"title": "Matrix Rank Minimization with Applications", "author": ["M. Fazel"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Fazel.,? \\Q2002\\E", "shortCiteRegEx": "Fazel.", "year": 2002}, {"title": "Heuristics for semirandom graph problems", "author": ["U. Feige", "J. Kilian"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Feige and Kilian.,? \\Q2001\\E", "shortCiteRegEx": "Feige and Kilian.", "year": 2001}, {"title": "Reconstructing many partitions using spectral techniques", "author": ["J. Giesen", "D. Mitsche"], "venue": "In Fundamentals of Computation Theory,", "citeRegEx": "Giesen and Mitsche.,? \\Q2005\\E", "shortCiteRegEx": "Giesen and Mitsche.", "year": 2005}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["D. Gross"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Gross.,? \\Q2011\\E", "shortCiteRegEx": "Gross.", "year": 2011}, {"title": "Stochastic blockmodels: Some first steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social networks,", "citeRegEx": "Holland et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Holland et al\\.", "year": 1983}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Hsu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2011}, {"title": "Spectral clustering with compressed, incomplete and inaccurate measurements", "author": ["B. Hunter", "T. Strohmer"], "venue": "Available at https://www.math.ucdavis.edu/ \u0303strohmer/papers/ 2010/SpectralClustering.pdf,", "citeRegEx": "Hunter and Strohmer.,? \\Q2010\\E", "shortCiteRegEx": "Hunter and Strohmer.", "year": 2010}, {"title": "The metropolis algorithm for graph bisection", "author": ["M. Jerrum", "G.B. Sorkin"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Jerrum and Sorkin.,? \\Q1998\\E", "shortCiteRegEx": "Jerrum and Sorkin.", "year": 1998}, {"title": "An efficient heuristic procedure for partitioning graphs", "author": ["B.W. Kernighan", "S. Lin"], "venue": "Bell System Technical Journal,", "citeRegEx": "Kernighan and Lin.,? \\Q1970\\E", "shortCiteRegEx": "Kernighan and Lin.", "year": 1970}, {"title": "Efficient active algorithms for hierarchical clustering", "author": ["A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "Compressed sensing and matrix completion with constant proportion of corruptions", "author": ["X. Li"], "venue": "Constructive Approximation,", "citeRegEx": "Li.,? \\Q2013\\E", "shortCiteRegEx": "Li.", "year": 2013}, {"title": "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG-09-2215,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Correlation clustering with noisy input", "author": ["C. Mathieu", "W. Schudy"], "venue": "In Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Mathieu and Schudy.,? \\Q2010\\E", "shortCiteRegEx": "Mathieu and Schudy.", "year": 2010}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": "In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "McSherry.,? \\Q2001\\E", "shortCiteRegEx": "McSherry.", "year": 2001}, {"title": "Clustering social networks. In Algorithms and Models for Web-Graph", "author": ["N. Mishra", "I. Stanton R. Schreiber", "R.E. Tarjan"], "venue": null, "citeRegEx": "Mishra et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mishra et al\\.", "year": 2007}, {"title": "Finding dense clusters via low rank + sparse decomposition", "author": ["S. Oymak", "B. Hassibi"], "venue": "Available on arXiv:1104.5186v1,", "citeRegEx": "Oymak and Hassibi.,? \\Q2011\\E", "shortCiteRegEx": "Oymak and Hassibi.", "year": 2011}, {"title": "Spectral clustering and the high-dimensional stochastic blockmodel", "author": ["K. Rohe", "S. Chatterjee", "B. Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "Rohe et al\\.,? \\Q1878\\E", "shortCiteRegEx": "Rohe et al\\.", "year": 1878}, {"title": "Spectral clustering on a budget", "author": ["O. Shamir", "N. Tishby"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Shamir and Tishby.,? \\Q2011\\E", "shortCiteRegEx": "Shamir and Tishby.", "year": 2011}, {"title": "Improved algorithms for the random cluster graph model", "author": ["R. Shamir", "D. Tsur"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Shamir and Tsur.,? \\Q2007\\E", "shortCiteRegEx": "Shamir and Tsur.", "year": 2007}, {"title": "Correlation clustering: maximizing agreements via semidefinite programming", "author": ["C. Swamy"], "venue": "In Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Swamy.,? \\Q2004\\E", "shortCiteRegEx": "Swamy.", "year": 2004}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Arxiv preprint arxiv:1011.3027,", "citeRegEx": "Vershynin.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin.", "year": 2010}, {"title": "Efficient clustering with limited distance information", "author": ["K. Voevodski", "M.F. Balcan", "H. Roglin", "S.H. Teng", "Y. Xia"], "venue": "arXiv preprint arXiv:1009.5168,", "citeRegEx": "Voevodski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2010}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "Luxburg.,? \\Q2007\\E", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "Information-theoretic determination of minimax rates of convergence", "author": ["Y. Yang", "A. Barron"], "venue": "The Annals of Statistics,", "citeRegEx": "Yang and Barron.,? \\Q1999\\E", "shortCiteRegEx": "Yang and Barron.", "year": 1999}], "referenceMentions": [{"referenceID": 28, "context": "In VLSI and design automation, it is useful in minimizing signaling between components (Kernighan and Lin, 1970).", "startOffset": 87, "endOffset": 112}, {"referenceID": 34, "context": "similar interest or background; finding clusters enables better recommendations and link prediction (Mishra et al., 2007).", "startOffset": 100, "endOffset": 121}, {"referenceID": 19, "context": "In the analysis of document databases, clustering the citation graph is often an essential and informative first step (Ester et al., 1995).", "startOffset": 118, "endOffset": 138}, {"referenceID": 7, "context": "Our analysis provides stronger guarantees than are current results on general matrix splitting (Cand\u00e8s et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).", "startOffset": 95, "endOffset": 163}, {"referenceID": 25, "context": "Our analysis provides stronger guarantees than are current results on general matrix splitting (Cand\u00e8s et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).", "startOffset": 95, "endOffset": 163}, {"referenceID": 30, "context": "Our analysis provides stronger guarantees than are current results on general matrix splitting (Cand\u00e8s et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).", "startOffset": 95, "endOffset": 163}, {"referenceID": 12, "context": "Our analysis provides stronger guarantees than are current results on general matrix splitting (Cand\u00e8s et al., 2011; Hsu et al., 2011; Li, 2013; Chen et al., 2013).", "startOffset": 95, "endOffset": 163}, {"referenceID": 3, "context": "For the special case of fully observed graphs, this formulation is exactly the same as the problem of correlation clustering, first proposed by Bansal et al. (2002). They show that exact minimization of the above objective is NP-hard in the worst case\u2014we survey and compare with this and other related work in Section 1.", "startOffset": 144, "endOffset": 165}, {"referenceID": 3, "context": "Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003).", "startOffset": 115, "endOffset": 161}, {"referenceID": 17, "context": "Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003).", "startOffset": 115, "endOffset": 161}, {"referenceID": 10, "context": "Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.", "startOffset": 66, "endOffset": 140}, {"referenceID": 15, "context": "Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.", "startOffset": 66, "endOffset": 140}, {"referenceID": 16, "context": "Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.", "startOffset": 66, "endOffset": 140}, {"referenceID": 39, "context": ", 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.", "startOffset": 27, "endOffset": 66}, {"referenceID": 32, "context": ", 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed.", "startOffset": 27, "endOffset": 66}, {"referenceID": 3, "context": "1 CORRELATION CLUSTERING As mentioned, for a completely observed graph, our problem is mathematically precisely the same as correlation clustering formulated in Bansal et al. (2002); in particular a \u201c+\u201d in correlation clustering corresponds to an edge in the graph, a \u201c-\u201d to the lack of an edge, and disagreements are defined in the same way.", "startOffset": 161, "endOffset": 182}, {"referenceID": 3, "context": "1 CORRELATION CLUSTERING As mentioned, for a completely observed graph, our problem is mathematically precisely the same as correlation clustering formulated in Bansal et al. (2002); in particular a \u201c+\u201d in correlation clustering corresponds to an edge in the graph, a \u201c-\u201d to the lack of an edge, and disagreements are defined in the same way. Thus, this paper can equivalently be considered as an algorithm, and guarantees, for correlation clustering under partial observations. Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003). Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed. We emphasize that we use a different convex relaxation, and we focus on understanding when our convex program yields an optimal clustering without further rounding. We note that Mathieu and Schudy (2010) use a convex formulation with constraints enforcing positive semi-definiteness, triangle inequalities and fixed diagonal entries.", "startOffset": 161, "endOffset": 1104}, {"referenceID": 3, "context": "1 CORRELATION CLUSTERING As mentioned, for a completely observed graph, our problem is mathematically precisely the same as correlation clustering formulated in Bansal et al. (2002); in particular a \u201c+\u201d in correlation clustering corresponds to an edge in the graph, a \u201c-\u201d to the lack of an edge, and disagreements are defined in the same way. Thus, this paper can equivalently be considered as an algorithm, and guarantees, for correlation clustering under partial observations. Since correlation clustering is NP-hard, there has been much work on devising alternative approximation algorithms (Bansal et al., 2002; Emmanuel and Fiat, 2003). Approximations using convex optimization, including LP relaxation (Charikar et al., 2003; Demaine and Immorlica, 2003; Demaine et al., 2006) and SDP relaxation (Swamy, 2004; Mathieu and Schudy, 2010), possibly followed by rounding, have also been developed. We emphasize that we use a different convex relaxation, and we focus on understanding when our convex program yields an optimal clustering without further rounding. We note that Mathieu and Schudy (2010) use a convex formulation with constraints enforcing positive semi-definiteness, triangle inequalities and fixed diagonal entries. For the fully observed case, their relaxation is at least as tight as ours, and since they add more constraints, it is possible that there are instances where their convex program works and ours does not. However, this seems hard to prove/disprove. Indeed, in the full observation setting they consider, their exact recovery guarantee is no better than ours. Moreover, as we argue in the next section, our guarantees are order-wise optimal in some important regimes and thus cannot be improved even with a tighter relaxation. Practically, our method is faster since, to the best of our knowledge, there is no lowcomplexity algorithm to deal with the \u0398(n3) triangle inequality constraints required by Mathieu and Schudy (2010). This means that our method can handle large graphs while their result is practically restricted to small ones (\u223c 100 nodes).", "startOffset": 161, "endOffset": 1960}, {"referenceID": 13, "context": "2 PLANTED PARTITION MODEL The planted partition model, also known as the stochastic block-model (Condon and Karp, 2001; Holland et al., 1983), assumes that the graph is generated with in-cluster edge probability p and inter-cluster edge probability q (where p > q) and fully observed.", "startOffset": 96, "endOffset": 141}, {"referenceID": 24, "context": "2 PLANTED PARTITION MODEL The planted partition model, also known as the stochastic block-model (Condon and Karp, 2001; Holland et al., 1983), assumes that the graph is generated with in-cluster edge probability p and inter-cluster edge probability q (where p > q) and fully observed.", "startOffset": 96, "endOffset": 141}, {"referenceID": 0, "context": "been used to solve the closely related planted clique problem (Alon et al., 1998; Ames and Vavasis, 2011).", "startOffset": 62, "endOffset": 105}, {"referenceID": 1, "context": "been used to solve the closely related planted clique problem (Alon et al., 1998; Ames and Vavasis, 2011).", "startOffset": 62, "endOffset": 105}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 48, "endOffset": 63}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 48, "endOffset": 104}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 48, "endOffset": 145}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 48, "endOffset": 195}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 48, "endOffset": 235}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 48, "endOffset": 267}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 290, "endOffset": 316}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 290, "endOffset": 359}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 290, "endOffset": 403}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al.", "startOffset": 290, "endOffset": 450}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al. (2011) \u03a9\u0303(n3/4) \u03a9\u0303( 3/4 K ) Oymak and Hassibi (2011) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Chaudhuri et al.", "startOffset": 290, "endOffset": 484}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al. (2011) \u03a9\u0303(n3/4) \u03a9\u0303( 3/4 K ) Oymak and Hassibi (2011) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Chaudhuri et al.", "startOffset": 290, "endOffset": 530}, {"referenceID": 4, "context": "Paper Cluster size K Density difference (1\u2212 2\u03c4) Boppana (1987) n/2 \u03a9\u0303( 1 \u221a n ) Jerrum and Sorkin (1998) n/2 \u03a9\u0303( 1 n1/6\u2212 ) Condon and Karp (2001) \u03a9\u0303(n) \u03a9\u0303( 1 n1/2\u2212 ) Carson and Impagliazzo (2001) n/2 \u03a9\u0303( 1 \u221a n ) Feige and Kilian (2001) n/2 \u03a9\u0303( 1 \u221a n ) McSherry (2001) \u03a9\u0303(n2/3) \u03a9\u0303( \u221a n2 K3 ) Bollob\u00e1s and Scott (2004) \u03a9\u0303(n) \u03a9\u0303( \u221a 1 n) Giesen and Mitsche (2005) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Shamir and Tsur (2007) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Mathieu and Schudy (2010) \u03a9\u0303( \u221a n) \u03a9\u0303(1) Rohe et al. (2011) \u03a9\u0303(n3/4) \u03a9\u0303( 3/4 K ) Oymak and Hassibi (2011) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K ) Chaudhuri et al. (2012) \u03a9\u0303( \u221a n) \u03a9\u0303( \u221a n K )", "startOffset": 290, "endOffset": 575}, {"referenceID": 35, "context": "The previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial observations directly.", "startOffset": 44, "endOffset": 69}, {"referenceID": 35, "context": "The previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial observations directly. One natural way to proceed is to impute the missing observations with noedge, or random edges with symmetric probabilities, and then apply any of the results in Table 1. This approach, however, leads to sub-optimal results. Indeed, this is done explicitly by Oymak and Hassibi (2011). They require the probability of observation p0 to satisfy p0 & \u221a Kmin n , where n is the number of nodes and Kmin is the minimum cluster size; in contrast, our approach only needs p0 & n K2 min (both right hand sides have to be less than 1, requiring Kmin & \u221a n, so the right hand side of our condition is order-wise smaller and thus less restrictive.", "startOffset": 44, "endOffset": 401}, {"referenceID": 35, "context": "The previous work listed in Table 1, except Oymak and Hassibi (2011), does not handle partial observations directly. One natural way to proceed is to impute the missing observations with noedge, or random edges with symmetric probabilities, and then apply any of the results in Table 1. This approach, however, leads to sub-optimal results. Indeed, this is done explicitly by Oymak and Hassibi (2011). They require the probability of observation p0 to satisfy p0 & \u221a Kmin n , where n is the number of nodes and Kmin is the minimum cluster size; in contrast, our approach only needs p0 & n K2 min (both right hand sides have to be less than 1, requiring Kmin & \u221a n, so the right hand side of our condition is order-wise smaller and thus less restrictive.) Shamir and Tishby (2011) deal with partial observations directly and shows that p0 & 1 n suffices for recovering two clusters of size \u03a9(n).", "startOffset": 44, "endOffset": 780}, {"referenceID": 2, "context": "For example, Balcan and Gupta (2010), Voevodski et al.", "startOffset": 13, "endOffset": 37}, {"referenceID": 2, "context": "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al.", "startOffset": 13, "endOffset": 62}, {"referenceID": 2, "context": "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012) consider the clustering problem where one samples the rows/columns of the adjacency matrix rather than its entries.", "startOffset": 13, "endOffset": 94}, {"referenceID": 2, "context": "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012) consider the clustering problem where one samples the rows/columns of the adjacency matrix rather than its entries. Hunter and Strohmer (2010) consider partial observations in the features rather than in the similarity graph.", "startOffset": 13, "endOffset": 237}, {"referenceID": 2, "context": "For example, Balcan and Gupta (2010), Voevodski et al. (2010) and Krishnamurthy et al. (2012) consider the clustering problem where one samples the rows/columns of the adjacency matrix rather than its entries. Hunter and Strohmer (2010) consider partial observations in the features rather than in the similarity graph. Eriksson et al. (2011) show that \u03a9\u0303(n) actively selected pairwise similarities are sufficient for recovering a hierarchical clustering structure.", "startOffset": 13, "endOffset": 343}, {"referenceID": 9, "context": "We propose to perform the matrix splitting using convex optimization (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011).", "startOffset": 69, "endOffset": 119}, {"referenceID": 7, "context": "We propose to perform the matrix splitting using convex optimization (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011).", "startOffset": 69, "endOffset": 119}, {"referenceID": 20, "context": "This has been shown to be the tightest convex surrogate for the rank function for matrices with unit spectral norm (Fazel, 2002).", "startOffset": 115, "endOffset": 128}, {"referenceID": 9, "context": "The optimization problem (1) is, in fact, a semidefinite program (SDP) (Chandrasekaran et al., 2011).", "startOffset": 71, "endOffset": 100}, {"referenceID": 9, "context": "The optimization problem (1) is, in fact, a semidefinite program (SDP) (Chandrasekaran et al., 2011). We remark on the above formulation. (a) This formulation does not require specifying the number of clusters; this parameter is effectively learned from the data. The tradeoff parameter \u03bb is artificial and can be easily determined: since any desired K\u2217 has trace exactly equal to n, we simply choose the smallest \u03bb such that the trace of the optimal solution is at least n. This can be done by, e.g., bisection, which is described below. (b) It is possible to obtain tighter convex relaxations by adding more constraints, such as the diagonal entry constraints ki,i = 1, \u2200i, the positive semidefinite constraint K 0, or even the triangular inequalities ki,j + kj,k \u2212 ki,k \u2264 1. Indeed, this is done by Mathieu and Schudy (2010). Note that the guarantees for our formulation (to be presented in the next subsection) automatically imply guarantees for any other tighter relaxations.", "startOffset": 72, "endOffset": 828}, {"referenceID": 31, "context": "Second, our formulation can be solved efficiently using existing Augmented Lagrangian Multiplier methods (Lin et al., 2009).", "startOffset": 105, "endOffset": 123}, {"referenceID": 30, "context": "constraints, Mathieu and Schudy (2010) do not deliver better exact recovery guarantees (cf.", "startOffset": 13, "endOffset": 39}, {"referenceID": 30, "context": "Second, our formulation can be solved efficiently using existing Augmented Lagrangian Multiplier methods (Lin et al., 2009). This is no longer the case with the \u0398(n3) triangle inequality constraints enforced by Mathieu and Schudy (2010), and solving it as a standard SDP is only feasible for small graphs.", "startOffset": 106, "endOffset": 237}, {"referenceID": 30, "context": "To solve the optimization problem (1), we use the fast algorithm developed by Lin et al. (2009), which is tailored for matrix splitting and takes advantage of the sparsity of the observations.", "startOffset": 78, "endOffset": 96}, {"referenceID": 7, "context": "We note that directly applying existing results in the low-rankplus-sparse literature (Cand\u00e8s et al., 2011; Li, 2013) leads to weaker results, where the gap be bounded below by a constant.", "startOffset": 86, "endOffset": 117}, {"referenceID": 30, "context": "We note that directly applying existing results in the low-rankplus-sparse literature (Cand\u00e8s et al., 2011; Li, 2013) leads to weaker results, where the gap be bounded below by a constant.", "startOffset": 86, "endOffset": 117}, {"referenceID": 30, "context": "Table 1), including those in Mathieu and Schudy (2010) and Oymak and Hassibi (2011), which use tighter convex relaxations that are more computationally demanding.", "startOffset": 29, "endOffset": 55}, {"referenceID": 30, "context": "Table 1), including those in Mathieu and Schudy (2010) and Oymak and Hassibi (2011), which use tighter convex relaxations that are more computationally demanding.", "startOffset": 29, "endOffset": 84}, {"referenceID": 11, "context": "Theorem 5 generalizes a similar result in Chaudhuri et al. (2012), which does not consider partial observations.", "startOffset": 42, "endOffset": 66}, {"referenceID": 11, "context": "Theorem 5 generalizes a similar result in Chaudhuri et al. (2012), which does not consider partial observations. The theorem applies to any algorithm regardless of its computational complexity, and characterizes the fundamental tradeoff between p0 and 1 \u2212 2\u03c4 . It shows that when Kmin = \u0398(n), the requirement for 1\u2212 2\u03c4 and p0 in Theorem 4 is optimal up to logarithmic factors, and cannot be significantly improved by using more complicated methods. For the general case with Kmin = O(n), only part of the picture is known. Using non-rigorous arguments, Decelle et al. (2011) show that 1 \u2212 2\u03c4 & \u221a n Kmin is necessary when \u03c4 = \u0398(1) and the graph is fully observed; otherwise recovery is impossible or computationally hard.", "startOffset": 42, "endOffset": 575}, {"referenceID": 9, "context": "Luckily for us, this has been done previously (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011; Li, 2013).", "startOffset": 46, "endOffset": 106}, {"referenceID": 7, "context": "Luckily for us, this has been done previously (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011; Li, 2013).", "startOffset": 46, "endOffset": 106}, {"referenceID": 30, "context": "Luckily for us, this has been done previously (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011; Li, 2013).", "startOffset": 46, "endOffset": 106}, {"referenceID": 9, "context": "The crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011; Li, 2013).", "startOffset": 85, "endOffset": 145}, {"referenceID": 7, "context": "The crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011; Li, 2013).", "startOffset": 85, "endOffset": 145}, {"referenceID": 30, "context": "The crucial Step 3 is where we go beyond the existing literature on matrix splitting (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011; Li, 2013).", "startOffset": 85, "endOffset": 145}, {"referenceID": 30, "context": "4 in Li (2013) and the discussion thereafter.", "startOffset": 5, "endOffset": 15}, {"referenceID": 7, "context": "4 STEP 3: DUAL CERTIFICATE CONSTRUCTION We use a variant of the so-called golfing scheme (Cand\u00e8s et al., 2011; Gross, 2011) to construct W.", "startOffset": 89, "endOffset": 123}, {"referenceID": 23, "context": "4 STEP 3: DUAL CERTIFICATE CONSTRUCTION We use a variant of the so-called golfing scheme (Cand\u00e8s et al., 2011; Gross, 2011) to construct W.", "startOffset": 89, "endOffset": 123}, {"referenceID": 30, "context": "We then run Algorithm 1, where the optimization problem (1) is solved using the fast algorithm in Lin et al. (2009)..", "startOffset": 98, "endOffset": 116}, {"referenceID": 40, "context": "The following version is given by Tropp (2012).", "startOffset": 34, "endOffset": 47}, {"referenceID": 40, "context": "Lemma 7 (Tropp, 2012) Consider a finite sequence {Mi} of independent, random n\u00d7n matrices that satisfy the assumption EMi = 0 and \u2016Mi\u2016 \u2264 D almost surely.", "startOffset": 8, "endOffset": 21}, {"referenceID": 11, "context": "Proof of Theorem 5 We use a standard information theoretical argument, which improves upon a related proof by Chaudhuri et al. (2012). Let K be the size of the clusters (which are assumed to have equal size).", "startOffset": 110, "endOffset": 134}, {"referenceID": 44, "context": "A standard application of Fano\u2019s inequality and the convexity of the mutual information (Yang and Barron, 1999) gives", "startOffset": 88, "endOffset": 111}], "year": 2014, "abstractText": "This paper considers the problem of clustering a partially observed unweighted graph\u2014i.e., one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of \u201cdisagreements\u201d\u2014i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors.", "creator": "LaTeX with hyperref package"}}}