{"id": "1104.5601", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2011", "title": "Mean-Variance Optimization in Markov Decision Processes", "abstract": "We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms.", "histories": [["v1", "Fri, 29 Apr 2011 11:39:40 GMT  (21kb)", "http://arxiv.org/abs/1104.5601v1", "A full version of an ICML 2011 paper"]], "COMMENTS": "A full version of an ICML 2011 paper", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["shie mannor", "john n tsitsiklis"], "accepted": true, "id": "1104.5601"}, "pdf": {"name": "1104.5601.pdf", "metadata": {"source": "CRF", "title": "Mean-Variance Optimization in Markov Decision Processes", "authors": ["Shie Mannor"], "emails": [], "sections": [{"heading": null, "text": "This year, it will only be a matter of time before a solution is found, in which a solution is found."}, {"heading": "II. THE MODEL", "text": "In this section, we define the model, notation, and performance targets that we will examine."}, {"heading": "A. Markov Decision Processes", "text": "We consider a Markov decision-making process (MDP) with finite state, action and reward spaces. An MDP is formally defined by a sextuple M = (T, S, A, R, p, g), where (a) T, a positive integer, is the time horizon; (b) S is a finite collection of states, one of which is called the initial state; (c) A is a collection of finite sets of possible actions, a number determined for each state; (d) R is a finite subset of Q (the set of rational numbers) and is the compilation of possible values of indirect rewards. We leave K = maxr, R |. (e) p: {0,., T \u2212 1} \u00d7 S \u00d7 S \u00b7 A \u2192 Q describes the transition probabilities. In particular pt (s \u2032 | s, a) is the probability that the state is fixed at a certain time."}, {"heading": "B. Policies", "text": "In a deterministic policy \u03c0 = (\u00b50,.., \u00b5T \u2212 1), the action is determined at any time using a figure \u00b5t whose argument is the history Ht = (S0: t, A0: t \u2212 1, R0: t \u2212 1) of the process by allowing At = \u00b5t (Ht). To this end, we assume that there is a sequence of i.e. uniform random variables U0, U1,.., UT \u2212 1 that are independent of anything else. In a randomized policy, the action is also determined in due course by leaving At = \u00b5t (Ht, U0: t)."}, {"heading": "C. Performance Criteria", "text": "Once a policy \u03c0 and a starting state s are established, the cumulative reward WT becomes a well-defined random variable. The yardsticks of interest are their mean and their variance, defined by J\u03c0 = E\u03c0 [WT] or V\u03c0 = Var\u03c0 (WT), respectively. Under our assumptions (finite horizon and limited rewards), it follows 5 that there are finite upper limits of KT or K2T 2, regardless of politics. Given our interest in complexity problems, we will focus on \"decision problems\" that allow for a yes / no answer, with the exception of Section VI. We will define the following problem. Problem MV-MDP: In the face of an MDP M and rational numbers \u03bb, v, is there a policy designed in such a way that JJ and V\u03c0 \u2264 v? It is clear that an algorithm for the problem MV-MDP (\u0438) can be combined with binary search to solve a problem (up to an expected difference in value of the WT)."}, {"heading": "III. COMPARISON OF POLICY CLASSES", "text": "Our first step is to compare the performance of different political classes. We introduce some concepts. Let's say two things: \"I\" and \"I\" are worse than \"I.\" If the political class \"I,\" loosely spoken, can always agree with or exceed the \"performance\" of the political class \"I,\" then it can strictly exceed it. Formally, \"I\" is worse than \"I,\" if the following statement holds true: (i) if (M, c, d) is a \"yes\" instance of MV-MDP, then it is also a \"yes\" instance of MV-MDP. \"Similarly, we say that two political classes\" I \"and\" II \"are equivalent if each\" yes \"(or\" no \") of MV-MDP is\" a \"yes\" instance of MDP. \""}, {"heading": "A. Randomization Improves Performance", "text": "The first observation is that randomization can improve performance, which is not surprising, since we are dealing with two criteria at the same time, and randomization is helpful for limited MDPs (e.g., Altman, 1999).Theorem 1: (a). It is clear that performance cannot deteriorate if randomization is allowed, so it is sufficient to indicate an instance in which randomization improves performance. Consider a one-step action as MDP (T = 1).To begin with, we are 0, and there are two actions available, a and b: the mean and the variance of the resulting reward are both equal to zero and both equal."}, {"heading": "IV. COMPLEXITY RESULTS", "text": "In this context, the question must also be asked to what extent it is indeed a problem in which it is a policy in which there is a reward whose reward is zero. (With regard to the problematic definition of the problem, this means that it is a policy in which the reward variance is zero.) With regard to the problematic definition, this corresponds to the assumption that it is a policy which refers to the reduction of the problem of MV-MDP, in which there is a policy in which the reward variance is zero. (With regard to the problematic definition, which means that the avoidance of KT and V = 0.) The proof of this is a reduction of the SUBSET-SUM problem in which there is a policy in which there is a policy whose reward variance is zero.)"}, {"heading": "V. EXACT ALGORITHMS", "text": "The comparison and complexity results of the two preceding sections indicate that the political classes \u0442t, s, \u0442t, s, w, \u0442t, s, u, and \u0435h are inferior to the class \u0442h, u, and in addition, some of them (\u0442t, s, \u0442t, \u0442t, s, w) seem to have a higher complexity. Therefore, there is no reason to consider them any further. While the MV-MDP (\u0442h, u) problem is NP-hard, there is still the possibility of approximate or pseudopolynomial age algorithms. In this section, we will focus on exact pseudopolynomial age algorithms. Our approach includes an extended state, defined by Xt = (St, Wt). Let X be the set of all possible values of the extended state. Let X be the cardinality of the set S."}, {"heading": "A. State-Action Frequencies", "text": "In this section, we provide some results on the representation of MDPs in relation to a state action frequency. (This means that we set the stage for our subsequent algorithms. (This means that we can only create the conditions for such a policy if we list the state action frequencies in time t = 0, 1,.). (T.Let z\u03c0 be a vector listing all state action frequencies defined above. (It is a vector listing all state action frequencies defined above.) (The following result is known (e.g. Altman, 1999). (It claims that any viable state action frequency can be achieved by a policy that only depends on time)."}, {"heading": "VI. APPROXIMATION ALGORITHMS", "text": "In this section we deal with the optimization counterparts of the problem MV-K-K DP (\u0432h, u). We are interested in the calculation of the two following functions: V-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "B. General Rewards", "text": "If rewards are arbitrary, we can discredit the rewards and get a new MDP. The new MDP is equivalent to one with integer rewards, to which the algorithm of the previous subsection can be applied. This is a legitimate approximation algorithm for the original problem, since, as we will soon show, the function v * () is changed very little if we use a sufficiently fine discrediting. We are given an original MDP M = (T, S, R, p, g) in which the rewards are rational numbers in the interval [\u2212 K], and an approximation parameter is available. We fix a positive number of arguments, a discrediting parameter whose value is specified later. We then construct a new MDP M = (T, S, R, p, g \u00b2) in which the rewards are rounded off."}, {"heading": "C. An Exact Algorithm and its Approximation", "text": "There are two general approaches to the construction of approximation algorithms. (i) You can discredit the problem to get a simpler problem, and then apply an algorithm specifically tailored to the discredited problem. (ii) You can design an exact (but inefficient) algorithm for the original problem, and then roughly implement the algorithm. This approach will work if the approximations are not built excessively over the course of the algorithm. (ii) In this subsection, we will work on the latter approach. We define the polyhedron as the amount of achievable first and second moments of cumulative reward starting from zero. We expand this definition by looking at interim times and arbitrary (intermedial) augmented states."}, {"heading": "VII. CONCLUSIONS", "text": "We have shown that the mean variance optimization problems for MDPs are typically NP-hard, but sometimes admit that the pseudopolynomial approximation algorithms (depending on whether the contribution of the time horizon in infinite horizons can be arbitrarily small in infinite horizons discounted problems (or in \"correct\" stochastic shortcut problems as in Bertsekas (1995))), our approximation algorithms can also provide approximation algorithms for infinite horizons. Two other problems of interest are the search for a policy that has the smallest possible variance, or thelargest possible variance. There is not much we can say here, except for the following: 2If X and Y are subsets of a vector space and a scalar, we will allow it. \""}], "references": [{"title": "Constrained Markov decision processes", "author": ["E. Altman"], "venue": null, "citeRegEx": "Altman,? \\Q1999\\E", "shortCiteRegEx": "Altman", "year": 1999}, {"title": "Bandit processes and dynamic allocation indices", "author": [], "venue": "Journal of the Royal Statistical", "citeRegEx": "Gittins,? \\Q1979\\E", "shortCiteRegEx": "Gittins", "year": 1979}, {"title": "Robust dynamic programming", "author": ["G. Iyengar"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Iyengar,? \\Q2005\\E", "shortCiteRegEx": "Iyengar", "year": 2005}, {"title": "Robust Markov decision processes with uncertain transition matrices", "author": ["A. Press. Nilim", "L. El Ghaoui"], "venue": null, "citeRegEx": "Nilim and Ghaoui,? \\Q2005\\E", "shortCiteRegEx": "Nilim and Ghaoui", "year": 2005}, {"title": "On the approximability of trade-offs and optimal access", "author": ["C.H. Papadimitriou", "M. Yannakakis"], "venue": "Operations Research,", "citeRegEx": "Papadimitriou and Yannakakis,? \\Q2000\\E", "shortCiteRegEx": "Papadimitriou and Yannakakis", "year": 2000}, {"title": "Dynamic coherent risk measures", "author": ["F. Riedel"], "venue": "Stoch. Proc. Appl.,", "citeRegEx": "Riedel,? \\Q2004\\E", "shortCiteRegEx": "Riedel", "year": 2004}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Approximation of dynamic programs \u2013 II", "author": ["W. 231-243. Whitt"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Whitt,? \\Q1979\\E", "shortCiteRegEx": "Whitt", "year": 1979}], "referenceMentions": [{"referenceID": 5, "context": "(Riedel, 2004), problems of this type can be difficult (Le Tallec, 2007), except for some special cases (Iyengar, 2005; Nilim & El Ghaoui, 2005) that can be reduced to Markov games (Shapley, 1953).", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "(Riedel, 2004), problems of this type can be difficult (Le Tallec, 2007), except for some special cases (Iyengar, 2005; Nilim & El Ghaoui, 2005) that can be reduced to Markov games (Shapley, 1953).", "startOffset": 104, "endOffset": 144}, {"referenceID": 0, "context": "Nevertheless, E[W ] and E[W ] are linear functions, and as such can be addressed simultaneously using methods from multicriteria or constrained Markov decision processes (Altman, 1999).", "startOffset": 170, "endOffset": 184}, {"referenceID": 1, "context": "(Riedel, 2004), problems of this type can be difficult (Le Tallec, 2007), except for some special cases (Iyengar, 2005; Nilim & El Ghaoui, 2005) that can be reduced to Markov games (Shapley, 1953). Mean-variance optimization lacks some of the desirable properties of approaches involving coherent risk measures and sometimes leads to counterintuitive policies. Bellman\u2019s principle of optimality does not hold, and as a consequence, a decision maker who has received unexpectedly large rewards in the first stages, may actively seek to incur losses in subsequent stages in order to keep the variance small. Nevertheless, mean-variance optimization is an important approach in financial decision making (e.g., Luenberger, 1997), especially for static (one-stage) problems. Consider, for example, a fund manager who is interested in the 1-year performance of the fund, as measured by the mean and variance of the return. Assuming that the manager is allowed to undertake periodic re-balancing actions in the course of the year, one obtains a Markov decision process with mean-variance criteria. Mean-variance optimization can also be a meaningful objective in various engineering contexts. Consider, for example, an engineering process whereby a certain material is deposited on a surface. Suppose that the primary objective is to maximize the amount deposited, but that there is also an interest in having all manufactured components be similar to each other; this secondary objective can be addressed by keeping the variance of the amount deposited small. We note that expressions for the variance of the discounted reward for stationary policies were developed in Sobel (1982). However, these expressions are quadratic in the underlying transition probabilities, and do not lead to convex optimization problems.", "startOffset": 105, "endOffset": 1677}, {"referenceID": 4, "context": "This is an approximation of the same kind as those considered in Papadimitriou and Yannakakis (2000): it returns a value v\u0302 such that (\u03bb, v\u0302) is an element of the \u201c(\u01eb + \u03bd)-approximate Pareto boundary\u201d of the set PMV .", "startOffset": 65, "endOffset": 101}, {"referenceID": 1, "context": "Of particular interest here are index policies that compute a value (\u201cindex\u201d) for each MDP and select an MDP with maximal index; such policies are often optimal for the classical formulations (see Gittins (1979) and Whittle (1988)).", "startOffset": 197, "endOffset": 212}, {"referenceID": 1, "context": "Of particular interest here are index policies that compute a value (\u201cindex\u201d) for each MDP and select an MDP with maximal index; such policies are often optimal for the classical formulations (see Gittins (1979) and Whittle (1988)).", "startOffset": 197, "endOffset": 231}], "year": 2011, "abstractText": "We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms. keywords: Markov processes; dynamic programming; control; complexity theory.", "creator": "LaTeX with hyperref package"}}}