{"id": "1105.0471", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2011", "title": "Suboptimal Solution Path Algorithm for Support Vector Machine", "abstract": "We consider a suboptimal solution path algorithm for the Support Vector Machine. The solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning. The path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other SVM optimization algorithms. In many machine learning application, however, this strict optimality is often unnecessary, and it adversely affects the computational efficiency. Our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level. It allows us to control the trade-off between the accuracy of the solution and the computational cost. Moreover, We also show that our suboptimal solutions can be interpreted as the solution of a \\emph{perturbed optimization problem} from the original one. We provide some theoretical analyses of our algorithm based on this novel interpretation. The experimental results also demonstrate the effectiveness of our algorithm.", "histories": [["v1", "Tue, 3 May 2011 03:14:14 GMT  (159kb)", "http://arxiv.org/abs/1105.0471v1", "A shorter version of this paper is submitted to ICML 2011"]], "COMMENTS": "A shorter version of this paper is submitted to ICML 2011", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["masayuki karasuyama", "ichiro takeuchi"], "accepted": true, "id": "1105.0471"}, "pdf": {"name": "1105.0471.pdf", "metadata": {"source": "CRF", "title": "Suboptimal Solution Path Algorithm for Support Vector Machine", "authors": ["Masayuki Karasuyama", "Ichiro Takeuchi"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 110 5.04 71v1 [cs.LG] 3 M"}, {"heading": "1 Introduction", "text": "This year, the time has come for us to find a solution that is capable, that we are able, that we are able to find a solution that is able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "2 Solution Path for Support Vector Machine", "text": "In this section we describe the solution path algorithm for regularization parameters of the Support Vector Machine (SVM)."}, {"heading": "2.1 Support Vector Machine", "text": "Suppose we have a set of training data {(xi, yi)} n i = 1, where xi-X-R is the input and yi-i-1, + 1 is the output class name. SVM learns a linear discriminant function f (x) = w-0-0 in a attribute space F, where \u03a6: X-1-F is a map from the input space X into the attribute space F, w-F is a coefficient vector and \u03b10-R is a prejudice. In this paper we consider the optimization problem of the following form: 0-0-i-i-ni = 11-2-w-2-2-2-i-i, (1) s.t. yif (xi-1-i-i, i-i-0-1,."}, {"heading": "2.2 Solution Path Algorithm for SVM", "text": "In this thesis, we consider the solution path with respect to the regularization parameter vector c. To follow the path, we parameterize c in the following form: c (\u03b8) = c (0) + \u03b8d, where c (0) = [C (0) 1,.., C (0) n] is an initial parameter, d = [d1,., dn] 'is a direction of path and storm \u2265 0. We track the change in the optimal solution of SVM when it is from 0.Let {\u03b1 (\u03b1) i} n = 0 the optimal parameters and {f (\u03b8) i} n the outputs f (xi).The KT optimality conditions can be summarized as follows: yif (2001) i \u2265 1, if \u03b1 (\u03b8) i = 0, (3a) i = 0, (3a) yif (\u03b8) i = 1, i = 1, i = 1, i)."}, {"heading": "3 Suboptimal Solution Path", "text": "In this section, we develop a suboptimal solution path algorithm for SVM, where the tolerance to the optimal conditions can be controlled by the user at will. The basic idea is to loosen the KKT optimality conditions and let several data points move simultaneously between the partition \u03c0. Note that this reduces the number of breakpoints and leads to an improvement in computing efficiency: it allows us to control the balance between the accuracy of the solution and the computing costs."}, {"heading": "3.1 Approximate Optimality Conditions", "text": "Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-Asian-"}, {"heading": "3.2 Update Index Sets", "text": "At a breakpoint, our algorithm can handle all data points that violate the strict inequality conditions (4) and not the relaxed conditions (6) (Figure 1 (b)), a situation that can be interpreted as what is called degeneration in parametric programming (Ritter, 1984). Changing several data points simultaneously inevitably leads to \"highly\" degenerated situations that come with many limitations. In degenerated cases, we have a problem called cycling. For example, if we shift two indexes from M to O, both or both of them can immediately move to M. To avoid cycling, we need to design an updated strategy for circumventing cycling. Degeneration can be handled by several approaches known in the parametric literature."}, {"heading": "3.3 Algorithm and Computational Complexity", "text": "Here we summarize our algorithm and analyze its computational complexity. At the k-th breakpoint, however, our algorithm performs the following procedure: Step 1 Solve (10) and calculate \u03b20, \u03b2 and g with (5) step 2. Calculate the next breakpoint \u03b8k + 1 and update it using rank-one updates of an inverse matrix or a cholesky factor from previous iteration with O (| M | 2) calculations. In our case, we need a rank-m update at each breakpoint, where 1 \u2264 m \u2264 B. If we use B as a small constant, the computational costs remain O (| M | 2). Including the other processes in this step, we can update the computational problem at each breakpoint, where 1 \u2264 m \u2264 B. If we set B as a small constant, the computational costs remain as a conventional optimization."}, {"heading": "4 Analysis", "text": "In this section we present some theoretical analyses of our suboptimal solution."}, {"heading": "4.1 Interpretation as Perturbed Problem", "text": "An interesting feature of our approach is that the solutions always maintain the optimality of an optimization problem that differs slightly from the original problem. (The following theorem gives the formulation of the disturbed problem: Theorem 3. \u2212 Any solution \u03b1 (\u03b8) in the suboptimal solution path is the optimal solution of the following optimization problem: max \u03b1 \u2212 12 \u03b1 Q\u03b1 + (1 + p).t. y \u03b1 \u2212 \u03b1 \u2212 q \u2264 c (\u03b8) + q. (11), with the disturbance parameters p, q Rn in \u2212 \u03b511 \u2264 p \u2264 \u03b511 and 0 \u2264 \u03b521, respectiv.Proof. Allow. \u2212 Proof."}, {"heading": "4.2 Error Analysis", "text": "We have shown that the solution of the suboptimal solution path can be interpreted as the optimal solution of the disturbed problem (13). Here we consider how close the optimal solution of the disturbed problem comes to the solution of the original problem in the sense of the optimal objective value. Let us consider D (\u03b1) and D (\u03b1) as the dual objective functions of the original optimization problem (2) and the disturbed problem (11). Let us consider D (\u03b1) as the optimal solution of the disturbed problem. Let us replace this problem with D (\u03b1) + p'qi + (\u2212 Q\u03b1) as the objective + (\u2212 Q\u03b1 + 1 + p) as the optimal solution of the original problem. Let us consider it as the optimal solution of the disturbed problem. Let us replace this problem with \u03b1 = \u03b1 and add this problem 0y (\u03b1 \u2212 \u03b1) as the same way in which we define the difference in which we find the optimal solution."}, {"heading": "5 Experiments", "text": "In this paper, we illustrate the empirical performance of the proposed approach by comparing it with the conventional exact solution path algorithm of 2001. Our task is to compare the solution path from c (0) = 10 \u2212 1 / n \u00b7 1 to c (1) = 0.5 / n \u00b7 1. Since all elements of c (\u03b8) assume the same value in this case, we sometimes refer to this common value as C (3) (i.e., c (\u03b8) = C (1) \u00b7 1) \u00b7 1). The RBF kernel K (xi, xj) = exp (\u2212 xj) 2) is used with g / p, where p is the number of features. To circumvent the possible numerical instability in the solution path, we add small positive constants 10 \u2212 6 to the diagrams of the matrix Q.Let e \u2265 0 be a parameter that controls the degree of the approximations. In this paper, we use e, we set the degree of approximations and approximations = 1."}, {"heading": "6 Conclusion", "text": "In this thesis, we have developed a suboptimal solution path algorithm that tracks the changes of solutions under relaxed optimality conditions. Our algorithm can reduce the number of breakpoints by moving several indices in \u03c0 at a breakpoint. Another interesting feature of our approach is that the suboptimal solutions correspond exactly to the optimal solutions of the disturbed problems from the original SVM optimization problems. Experimental results show that our algorithm efficiently follows the path and exhibits similar patterns of active sets and classification services to the exact path."}, {"heading": "A Proof of Theorem 1", "text": "Here we provide proof for the following theorem: Theorem 1. Let us leave \u03c0 = (O, M, I) the partition at optimal solution \u03b8 and assume that M = [0 y'MyM QM] is not singular. (A.1) As long as \u03c0 is unchanged, {\u03b2i} n i = 0 results at [\u03b20 \u03b2M] = \u2212 M \u2212 1 [y'I QM, I] dI, \u03b2O = 0, \u03b2I = dI. (A.1) Proof proof. As long as \u03c0 is unchanged, I must behave at i O and i i i = 0, i O, \u03b1i = C (\u03b8) i, i I.Therefore, we see that \u03b2O = 0 and \u03b2I = dI. From the definition of M, at optimum, the following linear system QM\u03b1 (\u03b8) M + QM [2001), Ic (2001), Ic (2001), Ic (2001), Ic), I + yMTB (2001) 1.0 = 1.0."}, {"heading": "B Proof of Theorem 2", "text": "Here we give a proof of the theorem 2. We prove first that we have the following problem (B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = B = 0, B = B: 0, B = B = B = B: 0, B = B = B = B: 0, B = B = B: 0, B = B: 0, B = B: 0, B = B: 0, B =: 0, B = B: 0, B = B: 0, B = B: 0, B =: 0, B = B: 0, B: 0: 0, B = B =: 0, B: 0, B = B: 0, B =: 0: 0, B = B: 0, B: 0, B: 0: 0, B = B: 0: 0, B = B: 0: 0, B: 0: 0, B = B: 0, B: 0: 0: 0, B = B: 0: 0, B: 0: 0, B: 0: 0, B: 0, B: 0: 0: 0, B: 0: 0: 0, B =, B: 0: 0, B: 0: 0, B: 0, B: 0: 0: 0: 0, B: 0: 0, B: 0, B =: 0: 0: 0: 0, B: 0: 0, B: 0, B: 0, B: 0: 0, B: 0: 0: 0: 0: 0, B:"}, {"heading": "C Reformulate the Optimization Problem (10)", "text": "We reformulate the Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y, Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y"}], "references": [{"title": "The optimal set and optimal partition approach to linear and quadratic programming", "author": ["A.B. Berkelaar", "K. Roos", "T. Terl\u00e1ky"], "venue": "Advances in Sensitivity Analysis and Parametric Programming,", "citeRegEx": "Berkelaar et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Berkelaar et al\\.", "year": 1997}, {"title": "An algorithm for the solution of the parametric quadratic programming problem", "author": ["M.J. Best"], "venue": "Technical Report 82-24,", "citeRegEx": "Best,? \\Q1982\\E", "shortCiteRegEx": "Best", "year": 1982}, {"title": "New finite pivoting rules for the simplex method", "author": ["R.G. Bland"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bland,? \\Q1977\\E", "shortCiteRegEx": "Bland", "year": 1977}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "L. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Sensitivity analysis for nonlinear programming using penalty methods", "author": ["A.V. Fiacco"], "venue": "Mathematical Programming,", "citeRegEx": "Fiacco,? \\Q1976\\E", "shortCiteRegEx": "Fiacco", "year": 1976}, {"title": "Pathwise coordinate optimization", "author": ["J. Friedman", "T. Hastie", "H. H\u00f6fling", "R. Tibshirani"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "An exponential lower bound on the complexity of regularization", "author": ["B. G\u00e4rtner", "J. Giesen", "M. Jaggi"], "venue": "paths. CoRR,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2009}, {"title": "Approximating parameterized convex optimization problems", "author": ["J. Giesen", "M. Jaggi", "S. Laue"], "venue": "18th European Symposium on Algorithms,", "citeRegEx": "Giesen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Giesen et al\\.", "year": 2010}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods \u2014 Support Vector Learning,", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "On parametric linear and quadratic programming problems", "author": ["K. Ritter"], "venue": "Mathematical Programming: Proceedings of the International Congress on Mathematical Programming,", "citeRegEx": "Ritter,? \\Q1984\\E", "shortCiteRegEx": "Ritter", "year": 1984}], "referenceMentions": [{"referenceID": 4, "context": "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning.", "startOffset": 38, "endOffset": 108}, {"referenceID": 9, "context": "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning.", "startOffset": 38, "endOffset": 108}, {"referenceID": 1, "context": "This technique is originally developed as parametric programming in the optimization community (Best, 1982).", "startOffset": 95, "endOffset": 107}, {"referenceID": 7, "context": "Although some empirical studies suggest that the number of breakpoints grows linearly in the input size, in the worst case, it can grow exponentially (G\u00e4rtner et al., 2009).", "startOffset": 150, "endOffset": 172}, {"referenceID": 9, "context": "In fact, one of the popular SVM optimization algorithm, called sequential minimal optimization (SMO) Platt (1999), is known to produce suboptimal (approximated) solution, where the tolerance to the optimality (degree of approximated) can be specified by users.", "startOffset": 101, "endOffset": 114}, {"referenceID": 8, "context": "1 Giesen et al. (2010) proposed approximated path algorithm with some optimality guarantee that can be applicable to L2-SVM without bias term.", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al., 2004).", "startOffset": 149, "endOffset": 199}, {"referenceID": 5, "context": "This theorem can be viewed as one of the specific forms of the sensitivity theorem Fiacco (1976). It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al.", "startOffset": 83, "endOffset": 97}, {"referenceID": 11, "context": "This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984).", "startOffset": 93, "endOffset": 107}, {"referenceID": 10, "context": "This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984). Here, degeneracy means that multiple constraints hit their boundaries of inequalities simultaneously. Although degenerate situation rarely happens in conventional solution path algorithms, it is not the case in ours. The simultaneous change of multiple data points inevitably brings about \u201chighly\u201d degenerate situations involved with many constraints. In degenerate case, we have a problem called the cycling. For example, if we move two indices i and j from M to O at the breakpoint, then both or either of them may immediately return to M. To avoid the cycling, we need to design an update strategy for \u03c0 that can circumvent cycling. The degeneracy can be handled by several approaches which are known in the parametric programming literature. Ritter (1984) showed that the cycling can be dealt with through the well-known Bland\u2019s minimum index rule in the", "startOffset": 94, "endOffset": 869}, {"referenceID": 2, "context": "linear programming (Bland, 1977).", "startOffset": 19, "endOffset": 32}, {"referenceID": 0, "context": "In this paper, we provide more essential solution to this problem based on (Berkelaar et al., 1997).", "startOffset": 75, "endOffset": 99}], "year": 2013, "abstractText": "We consider a suboptimal solution path algorithm for the Support Vector Machine. The solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning. The path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other SVM optimization algorithms. In many machine learning application, however, this strict optimality is often unnecessary, and it adversely affects the computational efficiency. Our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level. It allows us to control the trade-off between the accuracy of the solution and the computational cost. Moreover, We also show that our suboptimal solutions can be interpreted as the solution of a perturbed optimization problem from the original one. We provide some theoretical analyses of our algorithm based on this novel interpretation. The experimental results also demonstrate the effectiveness of our algorithm.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}