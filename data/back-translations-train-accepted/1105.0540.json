{"id": "1105.0540", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2011", "title": "Pruning nearest neighbor cluster trees", "abstract": "Nearest neighbor (k-NN) graphs are widely used in machine learning and data mining applications, and our aim is to better understand what they reveal about the cluster structure of the unknown underlying distribution of points. Moreover, is it possible to identify spurious structures that might arise due to sampling variability?", "histories": [["v1", "Tue, 3 May 2011 10:34:25 GMT  (85kb)", "https://arxiv.org/abs/1105.0540v1", null], ["v2", "Thu, 5 May 2011 14:13:49 GMT  (85kb)", "http://arxiv.org/abs/1105.0540v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["samory kpotufe", "ulrike von luxburg"], "accepted": true, "id": "1105.0540"}, "pdf": {"name": "1105.0540.pdf", "metadata": {"source": "META", "title": "Pruning nearest neighbor cluster trees", "authors": ["Samory Kpotufe"], "emails": ["SAMORY@TUEBINGEN.MPG.DE", "ULRIKE.LUXBURG@TUEBINGEN.MPG.DE"], "sections": [{"heading": null, "text": "ar Xiv: 110 5.05 40v2 [st at.M L] 5M ay2 01Our first contribution is a statistical analysis that shows how certain sub-graphs of a k-NN diagram form a consistent estimator of the cluster tree of the underlying point distribution. Our second and perhaps most important contribution is the following finite sample guarantee. We carefully work out the trade-off between aggressive and conservative pruning and are able to guarantee the elimination of all false cluster structures at all levels of the tree while ensuring the restoration of outstanding clusters. This is the first such finite sample result in the context of clustering."}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "1.1. Related work", "text": "In fact, most of us are able to move to another world, in which we move to another world."}, {"heading": "2. Preliminaries", "text": "Suppose the finite dataset X = {Xi} ni = 1 is i.i.d. derived from a distribution F over Rd with density function f. We begin with some simple definitions related to k-NN operations. Unless otherwise specified, all spheres denote closed spheres in Rd.Definition 1 (k-NN radii). For x-X, rk, n (x) should denote the radius of the smallest sphere with a center of x x-mass k / n. Definition 2 (k-NN and mutual k-NN graphs). The kNN graph is the one whose vertices are the points in X and where Xi is connected to Xj iff Xi (Xj iff Xi) or Xj \u0445B (Xj)."}, {"heading": "2.1. Cluster tree", "text": "Definition 3 (Connection): We say that A-Rd is connected if for each x-x-x-A there is a continuous 1-1 function P: [0, 1] 7 \u2192 A, where P (0) = x and P (1) = x. \"P in A is called the path between x and x.\" The cluster tree of f is called {G (\u03bb) = > 0, where G (\u03bb) is the CCs of the specified level {x: f (x) \u2265 \u03bb}. Note that {G (\u03bb)} \u03bb > 0 forms an (infinite) tree hierarchy in which for any two components A, A \u00b2, A \u00b2, A \u00b2, A \u00b2, either A \u00b2 is derived from A \u00b2 = \u2205 or one from the other, i.e. A-A \u00b2 or A-A \u00b2 A."}, {"heading": "3. Algorithm", "text": "Definition 4 (k-NN density estimate). Definition of density estimate at the x-Rd level as: fn (x). = kn \u00b7 vol (B (x, rk, n (x))) = kn \u00b7 vdrdk, n (x), where vd is the volume of the standard sphere in Rd.Let Gn be the k-NN or mutual k-NN graph. For \u03bb > 0 define Gn (\u03bb) as a subgraph of Gn containing only depressions in {Xi: fn (Xi) and corresponding edges.The CCs of {Gn (\u03bb)} n = 0 form a tree: let An and A \u2032 n are two such CCs, either A \u00b2 n = \u2205 or one is a descendant of the other, i.e. a subgraph of A \u00b2 n or vice versa."}, {"heading": "4. Results Overview", "text": "We make the following assumptions about the density f. (A.1) F > 0 > A > E = A > F (x) \u2264 F (A.2) f is a finite set of results that determine the conditions under which samples from a connected subset of Rd remain connected in the empirical cluster tree, and samples from two separate subsets of Rd remain separated, even after pruning. Essentially, for k, sufficiently large subsets A remain connected unless k is too large, separate subsets A and A separated by a sufficiently large region of low density (relative to n, k and d), the following two definitions remain separated from each other."}, {"heading": "5. Analysis", "text": "Theorem 1 follows from the lemmas 3 and 6. These two lemmas depend on the events described by lemmas 1, 2 and 4, which have a combined probability of at least 1 \u2212 3\u03b4 for a confidence parameter \u03b4 > 0.Theorem 2 follows from lemmas 5 and 7. These two lemmas also depend on the events described by lemmas 1, 2 and 4, which occur with a combined probability of at least 1 \u2212 3\u03b4."}, {"heading": "5.1. Maintaining Separation", "text": "In this section, we establish conditions under which the points of two separate subsets of Rd remain separated in the empirical tree structure (even after testing).The following is an important problem, which is the estimation error of fn with respect to Xi in sample X. Interestingly, we cannot find this type of finite sampling statement in the literature on k-N2, at least not under our assumptions.The evidence presented as a supplement in sample X. is somewhat involved and begins with an intuition from an asymptotic analysis of (Devroye & Wagner, 1977) combined with a form of Chernoff capability, which appears in (Angluin & Valiant, 1979).Lemma 1. Supply for Satisfaction (A.1) and (A.2).There is C = C (F), so that there are for both types of satisfaction."}, {"heading": "5.1.1. IDENTIFYING MODES", "text": "As a result, we must specify in the definition 7 (ii) that a (r) -salient mode A is contained in a sufficiently large amount of Ak, so that we start near the mode.We have the following UK problem, under which the subsets of Rd are the subsets of X.Lemma 4 (Lemma 5.1 of (Bousquet et al., 2004)). Supply C is a class of subsets of Rd. Let SC (2n) denotes the 2n-shatter coefficients of C. Let Fn denote the empirical distribution over n samples, the i.d of F."}, {"heading": "5.2. Maintaining Connectedness", "text": "In this section we show that the sample points from a connected subset A of Rd in the empirical scale can always be on a certain level before we trim them (hence also after trimming). Similar to (Chaudhuri & Dasgupta, 2010), the path in Gn neara path P in A that connects the two consists of a sequence x1 = x, x2,.. xi = x \u00b2 of sample points centered on the path P in A. (the solid path shown below) The intuition is that P is a high density route near which we can find enough sample points to connect x and x x. xx \u00b2 The balls centered on P must be chosen so that consecutive terms xi + 1 are adjacent in Gn."}, {"heading": "5.2.1. PRUNING OF SPURIOUS BRANCHES", "text": "As a logical consequence of Lemma 6, we can guarantee in Lemma 7 that the pruning method eliminates all wrong branches and thus all wrong clusters. Lemma 7 (pruning). In Lemma 7, let us guarantee that under the assumptions of Lemma 6, the following applies with a probability of at least 1 \u2212 3\u03b4, provided that the branches of A and A \u00b2 n are equal in {G \u00b2 n (\u03bb)}. Let V be the union of the vertices of A and A \u00b2 n and define \u03bb. = infx \u00b2 V f (x). The vertices of A and those of A \u00b2 n are in separate CCs of G (\u03bb). Proof: Let n = minx \u00b2 V fn (x) be the plane in the empirical tree containing An, A \u00b2 n. By Lemma 1 \u2212 f \u2212 f (x) implicit."}, {"heading": "Acknowledgements", "text": "We thank Sanjoy Dasgupta for interesting discussions that contributed to the improvement of the presentation."}, {"heading": "A. Proof of Lemma 1", "text": "Lemma 1 follows Lemma 9 (x). < f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (f) f (n) f (n) f (n) f (f) f (n) f (n) f (n) f (f) f (n) f (n) f (n) f (n) f (f) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n (n) f (n) f (n) f (n) f (n) f (n) f (n (n) f (n) f (n) f (n (n) f (n) f (n (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n (n) f (n (n) f (n) f (f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (f (n) f (n) f (f (n) f (n) f ("}, {"heading": "B. Proof of Lemma 2", "text": "Lemma 2 follows as a sequence on Lemma 10 below.Lemma 10. Let us consider a subset A of Rd (k = k = k = k = k = k = k = k = k = k = k = k = k (k = k = k = k = k = k = k (k = k = k = k).Let us assume that there is r, satisfactory (x x = A).We have P (rk, n (Xi) \u2264 23 / drk (Xi) < 2 \u2212 K (Xi) < 2 \u2212 K (Xi) < 2 \u2212 K (k / 192) \u2264 exp (\u2212 K) < 2 \u2212 K (\u2212 k / 192) \u2264 exp (\u2212 K / 192).LK (\u2212 K / 192).LK (\u2212 K / 192).LK = k = k = k (k = k = k = k = k = k (k = k = k = k = k).LK (k = k = k = k = k (k = k = k = k = k = k = k = k = k).R.R.X (X / 192).K = k = k = k = k = k (k = k = k = k = k = k).K (k = k = k = k = k = k = k = k (k = k = k = k = k = k = k = k = k = k = k = k = k = k)."}], "references": [{"title": "Fast probabilistic algorithms for Hamiltonian circuits and matchings", "author": ["D. Angluin", "L.G. Valiant"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Angluin and Valiant,? \\Q1979\\E", "shortCiteRegEx": "Angluin and Valiant", "year": 1979}, {"title": "Introduction to statistical learning theory", "author": ["O. Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "Lecture Notes in Artificial Intelligence,", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Rates of convergence for the cluster tree", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Chaudhuri and Dasgupta,? \\Q2010\\E", "shortCiteRegEx": "Chaudhuri and Dasgupta", "year": 2010}, {"title": "The strong uniform consistency of nearest neighbor density estimates", "author": ["L.P. Devroye", "T.J. Wagner"], "venue": "The Annals of Statistics,", "citeRegEx": "Devroye and Wagner,? \\Q1977\\E", "shortCiteRegEx": "Devroye and Wagner", "year": 1977}, {"title": "Consistency of single linkage for high-density clusters", "author": ["J.A. Hartigan"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hartigan,? \\Q1981\\E", "shortCiteRegEx": "Hartigan", "year": 1981}, {"title": "Optimal construction of k-nearest neighbor graphs for identifying noisy clusters", "author": ["M. Maier", "M. Hein", "U. von Luxburg"], "venue": "Theoretical Computer Science,", "citeRegEx": "Maier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maier et al\\.", "year": 2009}, {"title": "Fast rates for plug-in estimators of density level", "author": ["P. Rigollet", "R. Vert"], "venue": "sets. Bernouilli,", "citeRegEx": "Rigollet and Vert,? \\Q2009\\E", "shortCiteRegEx": "Rigollet and Vert", "year": 2009}, {"title": "Generalized density clustering", "author": ["A. Rinaldo", "L. Wasserman"], "venue": "Annals of Statistics,", "citeRegEx": "Rinaldo and Wasserman,? \\Q2010\\E", "shortCiteRegEx": "Rinaldo and Wasserman", "year": 2010}, {"title": "Stability of density based clustering", "author": ["A. Rinaldo", "A. Singh", "R. Nugent", "L. Wasserman"], "venue": null, "citeRegEx": "Rinaldo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rinaldo et al\\.", "year": 2010}, {"title": "Adaptive hausdorff estimation of density level sets", "author": ["A. Singh", "C. Scott", "R. Nowak"], "venue": "Annals of Statistics,", "citeRegEx": "Singh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2009}, {"title": "Clustering with confidence: A binning approach", "author": ["W. Stueltze", "R. Nugent"], "venue": "International Federation Classification Societies Conference,", "citeRegEx": "Stueltze and Nugent,? \\Q2009\\E", "shortCiteRegEx": "Stueltze and Nugent", "year": 2009}, {"title": "A generalized single linkage method for estimating the cluster tree of a density", "author": ["W. Stueltze", "R. Nugent"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Stueltze and Nugent,? \\Q2010\\E", "shortCiteRegEx": "Stueltze and Nugent", "year": 2010}, {"title": "Mode analysis: A generalization of nearest neighbor which reduces chaining effects", "author": ["D. Wishart"], "venue": "Numerical Taxonomy,", "citeRegEx": "Wishart,? \\Q1969\\E", "shortCiteRegEx": "Wishart", "year": 1969}, {"title": "A kth nearest neighbor clustering procedure", "author": ["M. Wong", "T. Lane"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Wong and Lane,? \\Q1983\\E", "shortCiteRegEx": "Wong and Lane", "year": 1983}], "referenceMentions": [{"referenceID": 5, "context": "Previous work (Maier et al., 2009) has shown that the connected components (CC) of a given level set of f can be approximated by the CCs of some subgraph of Gn, provided the level set satisfies certain boundary conditions.", "startOffset": 14, "endOffset": 34}, {"referenceID": 4, "context": "Hartigan (Hartigan, 1981).", "startOffset": 9, "endOffset": 25}, {"referenceID": 9, "context": "Many other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al.", "startOffset": 32, "endOffset": 122}, {"referenceID": 5, "context": "Many other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al.", "startOffset": 32, "endOffset": 122}, {"referenceID": 5, "context": ", 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al., 2009) which uses a k-NN graph for level set estimation.", "startOffset": 136, "endOffset": 156}, {"referenceID": 12, "context": "This is similar to an earlier generalization of single-linkage by Wishart (1969) which however was given without a convergence analysis.", "startOffset": 66, "endOffset": 81}, {"referenceID": 8, "context": "A recent archived paper (Rinaldo et al., 2010) also treats the problem of false clusters in cluster tree estimation, but the result is not algorithmic as they only consider the cluster tree of an empirical density estimate, and do not provide a way to compute this cluster tree.", "startOffset": 24, "endOffset": 46}, {"referenceID": 5, "context": "There exist many pruning heuristics in the literature which typically consist of removing small clusters (Maier et al., 2009; Stueltze & Nugent, 2010) using some form of thresholding.", "startOffset": 105, "endOffset": 150}, {"referenceID": 1, "context": "1 of (Bousquet et al., 2004)).", "startOffset": 5, "endOffset": 28}], "year": 2011, "abstractText": "Nearest neighbor (k-NN) graphs are widely used in machine learning and data mining applications, and our aim is to better understand what they reveal about the cluster structure of the unknown underlying distribution of points. Moreover, is it possible to identify spurious structures that might arise due to sampling variability? Our first contribution is a statistical analysis that reveals how certain subgraphs of a k-NN graph form a consistent estimator of the cluster tree of the underlying distribution of points. Our second and perhaps most important contribution is the following finite sample guarantee. We carefully work out the tradeoff between aggressive and conservative pruning and are able to guarantee the removal of all spurious cluster structures at all levels of the tree while at the same time guaranteeing the recovery of salient clusters. This is the first such finite sample result in the context of clustering.", "creator": "LaTeX with hyperref package"}}}