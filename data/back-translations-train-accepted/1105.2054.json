{"id": "1105.2054", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2011", "title": "Generalized Boosting Algorithms for Convex Optimization", "abstract": "Boosting is a popular way to derive powerful learners from simpler hypothesis classes. Following previous work (Mason et al., 1999; Friedman, 2000) on general boosting frameworks, we analyze gradient-based descent algorithms for boosting with respect to any convex objective and introduce a new measure of weak learner performance into this setting which generalizes existing work. We present the first weak to strong learning guarantees for the existing gradient boosting work for smooth convex objectives, and also demonstrate that this work fails for non-smooth objectives. To address this issue, we present new algorithms which extend this boosting approach to arbitrary convex loss functions and give corresponding weak to strong convergence results. In addition, we demonstrate experimental results that support our analysis and demonstrate the need for the new algorithms we present.", "histories": [["v1", "Tue, 10 May 2011 21:02:58 GMT  (82kb,D)", "https://arxiv.org/abs/1105.2054v1", "Extended version of paper presented at the International Conference on Machine Learning, 2011"], ["v2", "Tue, 14 Feb 2012 06:33:18 GMT  (516kb,D)", "http://arxiv.org/abs/1105.2054v2", "Extended version of paper presented at the International Conference on Machine Learning, 2011. 9 pages + appendix with proofs"]], "COMMENTS": "Extended version of paper presented at the International Conference on Machine Learning, 2011", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexander grubb", "drew bagnell"], "accepted": true, "id": "1105.2054"}, "pdf": {"name": "1105.2054.pdf", "metadata": {"source": "META", "title": "Generalized Boosting Algorithms for Convex Optimization", "authors": ["Alexander Grubb", "Andrew Bagnell"], "emails": ["agrubb@cmu.edu", "dbagnell@ri.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "It is a versatile meta-algorithm for combining multiple simple hypotheses or weak learners to form a single complex hypothesis with superior performance.The power of this meta-algorithm lies in its ability to develop hypotheses that use only weak learners who work only marginally better than random.This weak to strong learning guarantee is a key feature of Boosting.To date, much of the work has focused on optimizing the performance of these meta-algorithms, which was published in the Proceedings of the 28th International Conference on Machine Learning, Bellevue, WA, USA, 2011 by the author. Copyright 2011 in relation to specific loss functions and problem settings. AdaBoost algorithms (Freund & Schapire, 1997) are perhaps the best known and most successful of these."}, {"heading": "2. L2 Function Space", "text": "We will propose the function space L2 (X, V, M) as a natural match for this setting, but the subsequent convergence analysis for restricted gradient algorithms can be generalized. < f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f"}, {"heading": "3. Restricted Gradient Descent", "text": "We will now outline the gradient-based view of the increase (Mason et al., 1999; Friedman, 2000) and how it relates to gradient lineage. < p > p > p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p > p > p > p > p > p > p\" p > p > p > p > p > p > p > p > p > p > p \"p > p > p > p > p > p > p > p\" p > p > p > p > p > p > p \"p > p > p > p > p > p > p\" p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p."}, {"heading": "3.1. Relationship to Previous Boosting Work", "text": "Although these projection operations are applicable to all L2 hypotheses, they also have practical interpretations when it comes to specific function classes traditionally used as weak learners to increase learning performance.For a classification-based weak learner with results in {\u2212 1, + 1} and optimization via individual output functions f: X \u2192 R, a projection like in (3) is equivalent to solving the weighted classification problem using examples {xn, sgn ((xn))} Nn = 1 and weights wn = (xn).The projection using the norm minimization in (4) corresponds to solving the regression problem in (h)."}, {"heading": "4. Convergence Analysis", "text": "We will now focus on analyzing the behavior of variants of the restricted gradient descending algorithm presented in algorithm 1 on problems of form: min f-F Remp [f], in which admissible lineages are taken from a specified constraint H-F. In line with previous increasing work, we will specifically consider cases in which the edge requirement in definition 1 is fulfilled for a certain period of time, and seek convergence results in which the empirical target Remp [ft] approaches the optimal training performance minf-F Remp [f]. This work does not attempt to analyze the convergence of the true risk R [f]. While we are looking specifically at L2 functional space, the presented convergence analysis can be extended to optimization across any Hilbert space with limited gradient descend."}, {"heading": "4.1. Smooth Convex Optimization", "text": "A previous result, which is based on two critical characteristics of the objective functionality, is the generalization of the result in the (Ra) A > R > V > S > S > S > S > S > S > S (Ra) S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S."}, {"heading": "4.2. General Convex Optimization", "text": "The convergence analysis for the online case goes beyond the scope of this paper and is not presented here.The convergence results are similar to those for the convergence so far."}, {"heading": "5. Experimental Results", "text": "We present preliminary experimental results for these new algorithms based on three tasks: an imitation learning problem, a ranking naive link, and a series of sample classification problems. The first experimental setup is an optimization problem arising from the Maximum Margin Planning (Ratliff et al., 2009) approach to imitation learning. In this environment, a demonstrated policy is provided as an example behavior, and the goal is to learn a cost function about features of the environment that produce strategies with similar behavior, by optimizing via a convex, non-smooth loss function that minimizes the cost difference between the current behavior and the behavior shown. Previous attempts in the literature have been made to adapt the increase to this setting (Ratliff et al., 2009; Bradley, 2009), similar to the naive algorithm presented here, but with no convergence results for these settings. Figure 1 shows the results of executing all the random algorithms based on a three tasks."}, {"heading": "Acknowledgements", "text": "We would like to thank Kevin Waugh, Daniel Munoz and the ICML auditors for their helpful feedback, which was accomplished through joint participation in the Robotics Consortium sponsored by the US Army Research Laboratory under the Collaborative Technology Alliance Program, Cooperative Agreement W911NF-10-2-0016."}, {"heading": "A. Equivalence of boosting requirements", "text": "(1), (1), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, 2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2)"}, {"heading": "B. Smooth Convergence Results", "text": "At the evidence in this section is assumed that all the norms and internal products related to the distribution P-1-1-2-2-2-3-3-3-3-3-3-3-3-3-3-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-in-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-in-4-4-4-4-4-4-4-4-4 * * * * * * * * * * * * * * *."}, {"heading": "C. General Convergence Results", "text": "For the evidence in this section, it is assumed that all norms and internal products relating to the empirical distribution P + > L = > L = > L = > L = > L = + L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L \u2212 T = L \u2212 T = L \u2212 T = L \u2212 T = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L \u2212 T = L \u2212 T \u2212 T = L \u2212 T = L \u2212 T = L \u2212 T = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L = L"}], "references": [{"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Learning in Modular Systems", "author": ["D.M. Bradley"], "venue": "PhD thesis,", "citeRegEx": "Bradley,? \\Q2009\\E", "shortCiteRegEx": "Bradley", "year": 2009}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "Potential boosters? In Advances in Neural Information", "author": ["Duffy", "Nigel", "Helmbold", "David"], "venue": "Processing Systems", "citeRegEx": "Duffy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Duffy et al\\.", "year": 2000}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman,? \\Q2000\\E", "shortCiteRegEx": "Friedman", "year": 2000}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory, pp", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classifiers", "author": ["L. Mason", "J. Baxter", "P.L. Bartlett", "M. Frean"], "venue": null, "citeRegEx": "Mason et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mason et al\\.", "year": 1999}, {"title": "A theory of multiclass boosting", "author": ["I. Mukherjee", "R.E. Schapire"], "venue": "In Advances in Neural Information Processing Systems 22,", "citeRegEx": "Mukherjee and Schapire,? \\Q2010\\E", "shortCiteRegEx": "Mukherjee and Schapire", "year": 2010}, {"title": "Learning to Search: Structured Prediction Techniques for Imitation Learning", "author": ["N. Ratliff"], "venue": "PhD thesis,", "citeRegEx": "Ratliff,? \\Q2009\\E", "shortCiteRegEx": "Ratliff", "year": 2009}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N. Ratliff", "D. Silver", "J.A. Bagnell"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "On the convergence of leveraging", "author": ["R\u00e4tsch", "Gunnar", "Mika", "Sebastian", "Warmuth", "Manfred K"], "venue": null, "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2002\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2002}, {"title": "The boosting approach to machine learning: An overview", "author": ["R.E. Schapire"], "venue": "In MSRI Workshop on Nonlinear Estimation and Classification,", "citeRegEx": "Schapire,? \\Q2002\\E", "shortCiteRegEx": "Schapire", "year": 2002}, {"title": "A simpler unified analysis of budget perceptrons", "author": ["I. Sutskever"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Sutskever,? \\Q2009\\E", "shortCiteRegEx": "Sutskever", "year": 2009}, {"title": "A general boosting method and its application to learning ranking functions for web search", "author": ["Z. Zheng", "H. Zha", "T. Zhang", "O. Chapelle", "K. Chen", "G. Sun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zheng et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2007}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In Proceedings of the 20th International Conference on Machine Learning,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 8, "context": "Following previous work (Mason et al., 1999; Friedman, 2000) on general boosting frameworks, we analyze gradient-based descent algorithms for boosting with respect to any convex objective and introduce a new measure of weak learner performance into this setting which generalizes existing work.", "startOffset": 24, "endOffset": 60}, {"referenceID": 6, "context": "Following previous work (Mason et al., 1999; Friedman, 2000) on general boosting frameworks, we analyze gradient-based descent algorithms for boosting with respect to any convex objective and introduce a new measure of weak learner performance into this setting which generalizes existing work.", "startOffset": 24, "endOffset": 60}, {"referenceID": 13, "context": "Introduction Boosting (Schapire, 2002) is a versatile meta-algorithm for combining together multiple simple hypotheses, or weak learners, to form a single complex hypothesis with superior performance.", "startOffset": 22, "endOffset": 38}, {"referenceID": 5, "context": "Looking to extend upon the success of AdaBoost, related algorithms have been developed for other domains, such as RankBoost (Freund et al., 2003) and mutliclass extensions to AdaBoost (Mukherjee & Schapire, 2010).", "startOffset": 124, "endOffset": 145}, {"referenceID": 8, "context": "Other previous work on providing general algorithms for boosting has shown that an intuitive link between algorithms like AdaBoost and gradient descent exists (Mason et al., 1999; Friedman, 2000), and that many existing boosting algorithms can be reformulated to fit within this gradient boosting framework.", "startOffset": 159, "endOffset": 195}, {"referenceID": 6, "context": "Other previous work on providing general algorithms for boosting has shown that an intuitive link between algorithms like AdaBoost and gradient descent exists (Mason et al., 1999; Friedman, 2000), and that many existing boosting algorithms can be reformulated to fit within this gradient boosting framework.", "startOffset": 159, "endOffset": 195}, {"referenceID": 5, "context": "Looking to extend upon the success of AdaBoost, related algorithms have been developed for other domains, such as RankBoost (Freund et al., 2003) and mutliclass extensions to AdaBoost (Mukherjee & Schapire, 2010). Each of these algorithms provides both strong theoretical and experimental results for their specific domain, including corresponding weak to strong learning guarantees, but extending boosting to these and other new settings is non-trivial. Recent attempts have been successful at generalizing the boosting approach to certain broader classes of problems, but their focus is also relatively restricted. Mukherjee and Schapire (2010) present a general theory of boosting for multiclass classification problems, but their analysis is restricted to the multiclass setting.", "startOffset": 125, "endOffset": 647}, {"referenceID": 5, "context": "Looking to extend upon the success of AdaBoost, related algorithms have been developed for other domains, such as RankBoost (Freund et al., 2003) and mutliclass extensions to AdaBoost (Mukherjee & Schapire, 2010). Each of these algorithms provides both strong theoretical and experimental results for their specific domain, including corresponding weak to strong learning guarantees, but extending boosting to these and other new settings is non-trivial. Recent attempts have been successful at generalizing the boosting approach to certain broader classes of problems, but their focus is also relatively restricted. Mukherjee and Schapire (2010) present a general theory of boosting for multiclass classification problems, but their analysis is restricted to the multiclass setting. Zheng et al. (2007) give a boosting method which utilizes the second-order Taylor approximation of the objective to optimize smooth, convex losses.", "startOffset": 125, "endOffset": 804}, {"referenceID": 12, "context": "Additionally, convergence rates of these algorithms have been analyzed for the case of smooth convex functionals (R\u00e4tsch et al., 2002) and for specific potential functions used in classification (Duffy & Helmbold, 2000) under the traditional PAC weak learning setting.", "startOffset": 113, "endOffset": 134}, {"referenceID": 8, "context": "Using this foundation, we will present weak to strong learning results for the existing gradient boosting algorithm (Mason et al., 1999; Friedman, 2000) for the special case of smooth convex objectives under our more general setting.", "startOffset": 116, "endOffset": 152}, {"referenceID": 6, "context": "Using this foundation, we will present weak to strong learning results for the existing gradient boosting algorithm (Mason et al., 1999; Friedman, 2000) for the special case of smooth convex objectives under our more general setting.", "startOffset": 116, "endOffset": 152}, {"referenceID": 16, "context": "For convex problems standard gradient descent algorithms are known to provide good convergence results (Zinkevich, 2003; Boyd & Vandenberghe, 2004; Hazan et al., 2006) and are widely applicable.", "startOffset": 103, "endOffset": 167}, {"referenceID": 7, "context": "For convex problems standard gradient descent algorithms are known to provide good convergence results (Zinkevich, 2003; Boyd & Vandenberghe, 2004; Hazan et al., 2006) and are widely applicable.", "startOffset": 103, "endOffset": 167}, {"referenceID": 14, "context": "A related form of gradient descent with gradient errors has previously been studied in the analysis of budgeted learning (Sutskever, 2009), and general results related to gradient projection errors are given in the literature.", "startOffset": 121, "endOffset": 138}, {"referenceID": 6, "context": "In the case of smooth convex functionals, Mason et al. (1999) give a proof of eventual convergence for this previous work, but no rates of convergence are given.", "startOffset": 42, "endOffset": 62}, {"referenceID": 8, "context": "L Function Space Previous work (Mason et al., 1999; Friedman, 2000) has presented the theory underlying function space gradient descent in a variety of ways, but never in a form which is convenient for convergence analysis.", "startOffset": 31, "endOffset": 67}, {"referenceID": 6, "context": "L Function Space Previous work (Mason et al., 1999; Friedman, 2000) has presented the theory underlying function space gradient descent in a variety of ways, but never in a form which is convenient for convergence analysis.", "startOffset": 31, "endOffset": 67}, {"referenceID": 6, "context": ", 1999; Friedman, 2000) has presented the theory underlying function space gradient descent in a variety of ways, but never in a form which is convenient for convergence analysis. Recently, Ratliff (2009) proposed the L function space as a natural match for this setting.", "startOffset": 8, "endOffset": 205}, {"referenceID": 8, "context": "Restricted Gradient Descent We now outline the gradient-based view of boosting (Mason et al., 1999; Friedman, 2000) and how it relates to gradient descent.", "startOffset": 79, "endOffset": 115}, {"referenceID": 6, "context": "Restricted Gradient Descent We now outline the gradient-based view of boosting (Mason et al., 1999; Friedman, 2000) and how it relates to gradient descent.", "startOffset": 79, "endOffset": 115}, {"referenceID": 8, "context": "This is a generalization of the projection operation in Mason et al. (1999) to functions other than classifiers.", "startOffset": 56, "endOffset": 76}, {"referenceID": 8, "context": "The straightforward algorithm (Mason et al., 1999; Friedman, 2000) for peforming restricted gradient descent which uses these projection operations is given in Algorithm 1.", "startOffset": 30, "endOffset": 66}, {"referenceID": 6, "context": "The straightforward algorithm (Mason et al., 1999; Friedman, 2000) for peforming restricted gradient descent which uses these projection operations is given in Algorithm 1.", "startOffset": 30, "endOffset": 66}, {"referenceID": 6, "context": "This projection operation is equivalent to the one given by Friedman (2000). These two projection methods provide relatively simple ways to search over any restriction set for the \u2018best\u2019 descent direction.", "startOffset": 60, "endOffset": 76}, {"referenceID": 12, "context": "(R\u00e4tsch et al., 2002) using results from the optimization literature on coordinate descent.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "We will now give a generalization of the result in (R\u00e4tsch et al., 2002) which uses our more general definition of weak learner edge.", "startOffset": 51, "endOffset": 72}, {"referenceID": 12, "context": "Theorem 3 (Generalization of Theorem 4 in (R\u00e4tsch et al., 2002)).", "startOffset": 42, "endOffset": 63}, {"referenceID": 16, "context": "scent setting (Zinkevich, 2003; Hazan et al., 2006), but with a number of additional error terms due to the gradient projection step.", "startOffset": 14, "endOffset": 51}, {"referenceID": 7, "context": "scent setting (Zinkevich, 2003; Hazan et al., 2006), but with a number of additional error terms due to the gradient projection step.", "startOffset": 14, "endOffset": 51}, {"referenceID": 7, "context": "scent setting (Zinkevich, 2003; Hazan et al., 2006), but with a number of additional error terms due to the gradient projection step. Sutskever (2009) has previously studied the convergence of gradient descent with gradient projection errors using an algorithm similar to Algorithm 1, but the analysis does not focus on the weak to strong learning guarantee we seek.", "startOffset": 32, "endOffset": 151}, {"referenceID": 11, "context": "The first experimental setup is an optimization problem which results from the Maximum Margin Planning (Ratliff et al., 2009) approach to imitation learning.", "startOffset": 103, "endOffset": 125}, {"referenceID": 11, "context": "Previous attempts in the literature have been made to adapt boosting to this setting (Ratliff et al., 2009; Bradley, 2009), similar to the naive algorithm presented here, but no convergence results for this settings are known.", "startOffset": 85, "endOffset": 122}, {"referenceID": 1, "context": "Previous attempts in the literature have been made to adapt boosting to this setting (Ratliff et al., 2009; Bradley, 2009), similar to the naive algorithm presented here, but no convergence results for this settings are known.", "startOffset": 85, "endOffset": 122}, {"referenceID": 16, "context": "First, we start by bounding the potential \u2016ft \u2212 f\u2217\u2016, similar to the potential function arguments in (Zinkevich, 2003; Hazan et al., 2006), but with a different descent step: \u2016ft+1 \u2212 f\u2217\u2016 \u2264 \u2016ft \u2212 \u03b7t(ht)\u2212 f\u2217\u2016 = \u2016ft \u2212 f\u2217\u2016 + \u03b7 t \u2016ht\u2016 2 \u2212 2\u03b7t\u3008ft \u2212 f\u2217, ht \u2212\u2207t\u3009 \u2212 2\u03b7t\u3008ft \u2212 f,\u2207t\u3009 \u3008f\u2217 \u2212 ft,\u2207t\u3009 \u2264 1 2\u03b7t \u2016ft+1 \u2212 f\u2217\u2016 \u2212 1 2\u03b7t \u2016ft \u2212 f\u2217\u2016 \u2212 \u03b7t 2 \u2016ht\u2016 \u2212 \u3008f\u2217 \u2212 ft, ht \u2212\u2207t\u3009", "startOffset": 100, "endOffset": 137}, {"referenceID": 7, "context": "First, we start by bounding the potential \u2016ft \u2212 f\u2217\u2016, similar to the potential function arguments in (Zinkevich, 2003; Hazan et al., 2006), but with a different descent step: \u2016ft+1 \u2212 f\u2217\u2016 \u2264 \u2016ft \u2212 \u03b7t(ht)\u2212 f\u2217\u2016 = \u2016ft \u2212 f\u2217\u2016 + \u03b7 t \u2016ht\u2016 2 \u2212 2\u03b7t\u3008ft \u2212 f\u2217, ht \u2212\u2207t\u3009 \u2212 2\u03b7t\u3008ft \u2212 f,\u2207t\u3009 \u3008f\u2217 \u2212 ft,\u2207t\u3009 \u2264 1 2\u03b7t \u2016ft+1 \u2212 f\u2217\u2016 \u2212 1 2\u03b7t \u2016ft \u2212 f\u2217\u2016 \u2212 \u03b7t 2 \u2016ht\u2016 \u2212 \u3008f\u2217 \u2212 ft, ht \u2212\u2207t\u3009", "startOffset": 100, "endOffset": 137}], "year": 2012, "abstractText": "Boosting is a popular way to derive powerful learners from simpler hypothesis classes. Following previous work (Mason et al., 1999; Friedman, 2000) on general boosting frameworks, we analyze gradient-based descent algorithms for boosting with respect to any convex objective and introduce a new measure of weak learner performance into this setting which generalizes existing work. We present the weak to strong learning guarantees for the existing gradient boosting work for strongly-smooth, strongly-convex objectives under this new measure of performance, and also demonstrate that this work fails for non-smooth objectives. To address this issue, we present new algorithms which extend this boosting approach to arbitrary convex loss functions and give corresponding weak to strong convergence results. In addition, we demonstrate experimental results that support our analysis and demonstrate the need for the new algorithms we present.", "creator": "LaTeX with hyperref package"}}}