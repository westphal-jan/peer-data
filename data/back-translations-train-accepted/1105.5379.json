{"id": "1105.5379", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2011", "title": "Parallel Coordinate Descent for L1-Regularized Loss Minimization", "abstract": "We propose Shotgun, a parallel coordinate descent algorithm for minimizing L1-regularized losses. Though coordinate descent seems inherently sequential, we prove convergence bounds for Shotgun which predict linear speedups, up to a problem-dependent limit. We present a comprehensive empirical study of Shotgun for Lasso and sparse logistic regression. Our theoretical predictions on the potential for parallelism closely match behavior on real data. Shotgun outperforms other published solvers on a range of large problems, proving to be one of the most scalable algorithms for L1.", "histories": [["v1", "Thu, 26 May 2011 19:19:30 GMT  (221kb,D)", "http://arxiv.org/abs/1105.5379v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["joseph k bradley", "aapo kyrola", "danny bickson", "carlos guestrin"], "accepted": true, "id": "1105.5379"}, "pdf": {"name": "1105.5379.pdf", "metadata": {"source": "CRF", "title": "Parallel Coordinate Descent for L1-Regularized Loss Minimization", "authors": ["Joseph K. Bradley", "Aapo Kyrola", "Danny Bickson"], "emails": ["jkbradle@cs.cmu.edu", "akyrola@cs.cmu.edu", "bickson@cs.cmu.edu", "guestrin@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "The fact is that we are able to put ourselves at the top of society, at a time when we are able to change the world, when we are able to change the world, and when we are able to change and change the world. \""}, {"heading": "2.1. Sequential Coordinate Descent", "text": "Shalev-Shwartz and Tewari (2009) analyze Stochastic Coordinate Descent (SCD), a stochastic version1\u03b2 normalization of A does not change the target if for each xj. Algorithm 1 Shooting: Sequential SCDSet x = 0% R2d +. While not converging doChoose j (1,.., 2d) uniformly randomly. Set \u03b4xj \u2190 \u2212 max {\u2212 xj, \u2212 (F (x)) j / \u03b2}. Update xj \u2190 \u2212 xj + \u03b4xj.end whileof Shooting for solving (1). SCD (Alg. 1) random selection of a weight xj for updating per iteration. It calculates the update xj."}, {"heading": "3. Parallel Coordinate Descent", "text": "As the dimensionality d or sample size n increases, even fast sequential algorithms become expensive. To scale larger problems, we turn to the parallel calculation. In this section, we present our most important theoretical contribution: We show that the origin of coordinates can be parallelized by detecting strong convergence boundaries. Algorithm 2 Shotgun: Parallel SCDChoose Number of parallel updates P \u2265 1. Sentence x = 0 - R2d +, while non-convergent doIn parallel steps on P processors, we select j 1,..., 2d} uniformly at random. Sentence 2 - max {\u2212 xj, \u2212 xj, \u2212 F (x)) j / \u03b2}.Update xj \u2190 \u2212 xxj.end whileWe parallelize stochastic shooting and call our algorithm Shotgun (Alg. 2). Shotgun first selects P, the number of weights to be updated."}, {"heading": "3.1. Shotgun Convergence Analysis", "text": "In this section we present our Convergence Result for Shotgun. The result provides a problem-specific measure of the potential for parallelization: the spectral radius of ATA (i.e., the maximum of the magnitude of the parallel updates. We start with generalizing assumptions 2.1 to our parallel setting. The scalars \u03b2 for lasso and logistic regression remain the same as in (6).Assumption 3.1. Let F (x): R2d + \u2212 R be convex function. Assume that there is \u03b2 > 0 such that for all x and parallel updates x exist."}, {"heading": "3.2. Theory vs. Empirical Performance", "text": "We conclude this section by comparing the predictions of Theorem 3.2 on the number of parallel updates P with empirical results for Lasso. We simulated shotgun as in Alg. 2 to eliminate effects from the practical implementation decisions from Sec. 4. We tested two single-pixel camera sets from Duarte et al. (2008) with very different \u03c1, estimating EPt [F (x (T))] by selecting an average of 10 shotgun passes. We used \u03bb = 0.5 for Ball64 singlepixcam to obtain x * with about 27% non-zeros; we used \u03bb = 0.05 for Mug32 singlepixcam to obtain about 20% non-zeros. Fig. 2 records P against the iterations required for EPt [F (T) to get within 0.5% of the optimal F (x) non-zeros."}, {"heading": "3.3. Beyond L1", "text": "Theorems 2.1 and 3.2 generalize beyond L1, because their main requirements (assumptions 2.1, 3.1) apply to a more general class of problems: minF (x) s.t. x \u2265 0, where F (x) is smooth. We discuss shooting and shotgun for sparse regression, since both the method (coordinate descent) and the problem (sparse regression) are probably most useful for high-dimensional settings."}, {"heading": "4. Experimental Results", "text": "We present a comprehensive study of shotgun for the lasso and sparse logistical regression. We compare shotgun with published state-of-the-art solvers on a variety of datasets. We also analyze self-acceleration in detail with respect to Theorem 3.2 and hardware issues."}, {"heading": "4.1. Lasso", "text": "We tested Shooting and Shotgun for the lasso against five published lasso solvers for 35 datasets. We summarize the results here; details are in the addendum. 4.1.1. Implementation: ShotgunOur implementation resulted in several practical improvements to the basic shooting and shotgun algorithms. Following Friedman et al. (2010), we retained a vector axis to avoid a repetition of the calculation. We also used their pioneering optimization scheme: instead of solving directly with the given \u03bb, we solved with an exponentially decreasing sequence \u03bb1, \u03bb2,..., \u03bb. The solution x for \u03bbk will result in warm-start optimization for \u03bbk + 1. This scheme can result in significant acceleration. Although our analysis is intended for synchronous adjustment, our implementation was asynchronous due to the high cost of synchronization. We used atomic comparison and wap operations to update the vector operator."}, {"heading": "4.1.2. Other Algorithms", "text": "L1 LS (Kim et al., 2007) is a log barrier method for inner dots. It uses pre-conditioned conjugate gradients (PCG) to solve Newton steps iteratively and avoid an explicit reversal of Hessian. Implementation is done in Matlab R \u00a9, but the expensive step (PCG) uses very efficient native Matlab calls. In our tests, matrix vector operations were minimized to up to 8 nucles.FPC AS (Wen et al., 2010) uses iterative shrinkage to estimate which elements of x should not be zero, as well as their signs, reducing the target to a smooth, square function, which is then minimized.GPSR BB (Figueiredo et al., 2008) is a gradient projection method that uses line search and scheduling techniques tailored to Lasso.Hard l0 (Blumensath & Davies, 2009)."}, {"heading": "4.1.3. Results", "text": "\"We are very large and meager,\" he said. \"We are very large and meager.\" \"We are very large and meager.\" \"We are very large and meager.\" \"We are very large and meager.\" \"We are very large and meager.\" \"We are very large and meager.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \".\" \"\" \"We.\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \".\" \".\". \".\". \".\". \".\". \".\" \".\". \"\" \".\". \".\" \"\" \".\". \"\". \"\". \"\" \".\" \".\" \"\". \"\" \".\" \"\". \"\". \"\" \".\" \"\". \"\" \".\" \".\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\". \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \".\" \"\" \".\". \"\" \".\". \"\" \".\". \"\" \"\" \".\" \"\". \".\" \"\" \"\" \".\" \"\" \".\". \"\" \"\" \".\" \"\". \"\". \"\" \"\" \"\". \"\". \"\" \".\" \"\" \"\". \"\" \".\" \".\" \".\". \"\" \"\" \"\" \".\" \"\". \".\" \"\" \".\". \"\" \"\" \"\" \"\" \"\". \"\" \"\". \"\". \"\" \"\". \".\". \"\""}, {"heading": "4.2. Sparse Logistic Regression", "text": "For logistical regression, we focus on comparing shotgun with stochastic gradient descent (SGD) variants. SGD methods are of particular interest to us because they are often considered very efficient, especially for learning with many samples; they often have convergence limits that are independent of the number of samples. For a large-scale comparison of different algorithms for sparse logistic regression, we refer the reader to the most recent survey by Yuan et al. (2010). About L1-Logreg (Koh et al., 2007) and CDN (Yuan et al., 2010), our results are qualitatively consistent with their survey. Yuan et al. (2010) do not examine SGD empirically. 4.2.1. Implementation: Shotgun CDNAs Yuan et al. (2010) empirically show that their coordinate descent newton (CDN) method is often orders of magnitude faster than the basic shooting algorithm."}, {"heading": "4.2.2. Other Algorithms", "text": "SGD updates x iteratively in a gradient direction estimated with a sample and scaled by a learning rate. We implemented SGD in C + + following e.g. Zinkevich et al. (2010). We used lazy shrinkage updates (Langford et al., 2009a) to take advantage of scarcity in A. Choosing the learning rates for SGD can be challenging. In our tests, constant rates led to faster convergence than decreasing rates (decreasing rates by 1 / \u221a T). For each test, we tried 14 exponentially increasing rates in [10 \u2212 4, 1] (parallel) and chose the rate with the best training target. We did not use an economical step for SGD.SMIDAS (Shalev-Shwartz & Tewari, 2009) using stochastic mirror deviation, but separating gradients in order to save x. We did not use an economical step for SGD.SMIDAS (Shalev-Shwartz & Tewari, 2009)."}, {"heading": "4.2.3. Results", "text": "The zeta dataset 5 illustrates the regime with nd. It contains 500K samples with 2000 characteristics and is completely dense (in A). SGD performs well and is relatively competitive with shotgun CDN (with P = 8). The rcv1 dataset 6 (Lewis et al., 2004) illustrates the high-dimensional regime (d > n). It has about twice as many characteristics (44504) as samples (18217), with 17% of zeros in A. Shotgun CDN (P = 8) being much faster than SGD, especially in terms of the target. Parallel SGD yielded almost identical results with SGD. Although convergence limits for SMIDAS are comparable to those for SGD, SMIDAS iterations can take much longer due to mirror deviations."}, {"heading": "4.3. Self-Speedup of Shotgun", "text": "To study the self-acceleration of shotgun lasso and shotgun CDN, we used both solvers on our datasets with different \u03bb, using different P (number of parallel updates = number of cores).We recorded runtime for the first time when an algorithm came within 0.5% of the optimal target, as in Shooting.Fig.5 shows results for both acceleration (in time) and acceleration in iterations to convergence. Accelerations in iterations correspond fairly accurately to Theorem 3.2. To understand the boundary factors, we analyzed different accelerations in iterations (about 8 x) not by accelerations in runtime (about 2 x to 4 x).We discovered that accelerations in time were limited by low-level technical problems. To understand the boundary factors, we analyzed different shotgun-like data completion algorithms to find bottlenecks."}, {"heading": "5. Discussion", "text": "Our convergence results for shotgun are the first such results for a parallel coordinate reduction with L1 regularization. Our limitations predict linear speedups up to an interpretable, problem-dependent limit. In experiments, these predictions were consistent with empirical behavior.Extensive comparisons showed that in many datasets, shotgun outperforms state-of-the-art L1 solvers. We believe that shotgun is currently one of the most efficient and scalable solvers for L1-regulated problems.The most exciting extension of this work may be the hybrid of SGD and shotgun discussed in Sec. 4.3.Code, Data, and Benchmark Results: Available at http: / / www.select.cs.cs.cmu.edu / projects."}, {"heading": "Acknowledgments", "text": "Thanks to John Langford, Guy Blelloch, Joseph Gonzalez, Yucheng Low and our reviewers for their feedback. Supported by NSF IIS-0803333, NSF CNS-0721591, ARO MURI W911NF0710287, ARO MURI W911NF0810242."}], "references": [{"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies", "year": 2009}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "Single-pixel", "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems", "author": ["Figueiredo", "M.A.T", "R.D. Nowak", "S.J. Wright"], "venue": "IEEE J. of Sel. Top. in Signal Processing,", "citeRegEx": "Figueiredo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Figueiredo et al\\.", "year": 2008}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Penalized regressions: The bridge versus the lasso", "author": ["W.J. Fu"], "venue": "J. of Comp. and Graphical Statistics,", "citeRegEx": "Fu,? \\Q1998\\E", "shortCiteRegEx": "Fu", "year": 1998}, {"title": "An interior-point method for large-scale `1-regularized least squares", "author": ["S.J. Kim", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "IEEE Journal of Sel. Top. in Signal Processing,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Predicting risk from financial reports with regression", "author": ["S. Kogan", "D. Levin", "B.R. Routledge", "J.S. Sagi", "N.A. Smith"], "venue": "In Human Language Tech.-NAACL,", "citeRegEx": "Kogan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kogan et al\\.", "year": 2009}, {"title": "An interior-point method for large-scale l1-regularized logistic regression", "author": ["K. Koh", "Kim", "S.-J", "S. Boyd"], "venue": "JMLR, 8:1519\u20131555,", "citeRegEx": "Koh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koh et al\\.", "year": 2007}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Slow learners are fast", "author": ["J. Langford", "A.J. Smola", "M. Zinkevich"], "venue": "In NIPS,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "The Cilk++ concurrency platform", "author": ["C.E. Leiserson"], "venue": "In 46th Annual Design Automation Conference. ACM,", "citeRegEx": "Leiserson,? \\Q2009\\E", "shortCiteRegEx": "Leiserson", "year": 2009}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "JMLR, 5:361\u2013397,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Efficient large-scale distributed training of conditional maximum entropy models", "author": ["G. Mann", "R. McDonald", "M. Mohri", "N. Silberman", "D. Walker"], "venue": "In NIPS,", "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Feature selection, l1 vs. l2 regularization and rotational invariance", "author": ["A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Ng,? \\Q2004\\E", "shortCiteRegEx": "Ng", "year": 2004}, {"title": "Stochastic methods for `1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz and Tewari,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz and Tewari", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal Statistical Society,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J.N. Tsitsiklis", "D.P. Bertsekas", "M. Athans"], "venue": null, "citeRegEx": "Tsitsiklis et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1986}, {"title": "Sparco: A testing framework for sparse reconstruction", "author": ["E. van den Berg", "M.P. Friedlander", "G. Hennenfent", "F. Herrmann", "R. Saab", "O. Y\u0131lmaz"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Berg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2009}, {"title": "A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization and continuation", "author": ["Z. Wen", "Yin", "D.W. Goldfarb", "Y. Zhang"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Wen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2010}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "D.R. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Trans. on Signal Processing,", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}, {"title": "Hitting the memory wall: Implications of the obvious", "author": ["W.A. Wulf", "S.A. McKee"], "venue": "ACM SIGARCH Computer Architecture News,", "citeRegEx": "Wulf and McKee,? \\Q1995\\E", "shortCiteRegEx": "Wulf and McKee", "year": 1995}, {"title": "A comparison of optimization methods and software for large-scale l1-reg. linear classification", "author": ["G.X. Yuan", "K.W. Chang", "C.J. Hsieh", "C.J. Lin"], "venue": null, "citeRegEx": "Yuan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2010}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A.J. Smola", "L. Li"], "venue": "In NIPS,", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Introduction Many applications use L1-regularized models such as the Lasso (Tibshirani, 1996) and sparse logistic regression (Ng, 2004).", "startOffset": 75, "endOffset": 93}, {"referenceID": 14, "context": "Introduction Many applications use L1-regularized models such as the Lasso (Tibshirani, 1996) and sparse logistic regression (Ng, 2004).", "startOffset": 125, "endOffset": 135}, {"referenceID": 14, "context": "the number of irrelevant features (Ng, 2004).", "startOffset": 34, "endOffset": 44}, {"referenceID": 5, "context": "These algorithms range from coordinate minimization (Fu, 1998) and stochastic gradient (Shalev-Shwartz & Tewari, 2009) to more complex interior point methods (Kim et al.", "startOffset": 52, "endOffset": 62}, {"referenceID": 6, "context": "These algorithms range from coordinate minimization (Fu, 1998) and stochastic gradient (Shalev-Shwartz & Tewari, 2009) to more complex interior point methods (Kim et al., 2007).", "startOffset": 158, "endOffset": 176}, {"referenceID": 22, "context": "2, theory (Shalev-Shwartz & Tewari, 2009) and extensive empirical results (Yuan et al., 2010) have shown that variants of Shooting are particularly competitive for high-dimensional data.", "startOffset": 74, "endOffset": 93}, {"referenceID": 5, "context": "These algorithms range from coordinate minimization (Fu, 1998) and stochastic gradient (Shalev-Shwartz & Tewari, 2009) to more complex interior point methods (Kim et al., 2007). Coordinate descent, which we call Shooting after Fu (1998), is a simple but very effective algorithm which updates one coordinate per iteration.", "startOffset": 53, "endOffset": 237}, {"referenceID": 13, "context": ", 2009b) and distributed settings (Mann et al., 2009; Zinkevich et al., 2010).", "startOffset": 34, "endOffset": 77}, {"referenceID": 23, "context": ", 2009b) and distributed settings (Mann et al., 2009; Zinkevich et al., 2010).", "startOffset": 34, "endOffset": 77}, {"referenceID": 9, "context": "Recent work analyzes parallel stochastic gradient descent for multicore (Langford et al., 2009b) and distributed settings (Mann et al., 2009; Zinkevich et al., 2010). These methods parallelize over samples. In applications using L1 regularization, though, there are often many more features than samples, so parallelizing over samples may be of limited utility. We therefore take an orthogonal approach and parallelize over features, with a remarkable result: we can parallelize coordinate descent\u2014an algorithm which seems inherently sequential\u2014for L1-regularized losses. In Sec. 3, we propose Shotgun, a simple multicore algorithm which makes P coordinate updates in parallel. We prove strong convergence bounds for Shotgun which predict speedups over Shooting which are linear in P, up to a problem-dependent maximum P\u2217. Moreover, our theory provides an estimate for this ideal P\u2217 which may be easily computed from the data. Parallel coordinate descent was also considered by Tsitsiklis et al. (1986), but for differentiable objectives in the asynchronous setting.", "startOffset": 73, "endOffset": 1003}, {"referenceID": 16, "context": "An instance of (1) is the Lasso (Tibshirani, 1996) (in penalty form), for which Y \u2261 R and F (x) = 1 2 \u2016Ax\u2212 y\u20162 + \u03bb\u2016x\u20161 , (2)", "startOffset": 32, "endOffset": 50}, {"referenceID": 14, "context": "as well as sparse logistic regression (Ng, 2004), for which Y \u2261 {\u22121,+1} and", "startOffset": 38, "endOffset": 48}, {"referenceID": 15, "context": "For analysis, we follow Shalev-Shwartz and Tewari (2009) and transform (1) into an equivalent problem with a twice-differentiable regularizer.", "startOffset": 24, "endOffset": 57}, {"referenceID": 15, "context": "Sequential Coordinate Descent Shalev-Shwartz and Tewari (2009) analyze Stochastic Coordinate Descent (SCD), a stochastic version Normalizing A does not change the objective if a separate, normalized \u03bbj is used for each xj .", "startOffset": 30, "endOffset": 63}, {"referenceID": 15, "context": "To our knowledge, Shalev-Shwartz and Tewari (2009) provide the best known convergence bounds for SCD.", "startOffset": 18, "endOffset": 51}, {"referenceID": 13, "context": "As Shalev-Shwartz and Tewari (2009) argue, Theorem 2.", "startOffset": 3, "endOffset": 36}, {"referenceID": 15, "context": "2): Our proof resembles Shalev-Shwartz and Tewari (2009)\u2019s proof of Theorem 2.", "startOffset": 24, "endOffset": 57}, {"referenceID": 4, "context": "Following Friedman et al. (2010), we maintained a vector Ax to avoid repeated computation.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "We used C++ and the CILK++ library (Leiserson, 2009) for parallelism.", "startOffset": 35, "endOffset": 52}, {"referenceID": 6, "context": "L1 LS (Kim et al., 2007) is a log-barrier interior point method.", "startOffset": 6, "endOffset": 24}, {"referenceID": 19, "context": "FPC AS (Wen et al., 2010) uses iterative shrinkage to estimate which elements of x should be non-zero, as well as their signs.", "startOffset": 7, "endOffset": 25}, {"referenceID": 3, "context": "GPSR BB (Figueiredo et al., 2008) is a gradient projection method which uses line search and termination techniques tailored for the Lasso.", "startOffset": 8, "endOffset": 33}, {"referenceID": 20, "context": "SpaRSA (Wright et al., 2009) is an accelerated iterative shrinkage/thresholding algorithm which solves a sequence of quadratic approximations of the objective.", "startOffset": 7, "endOffset": 28}, {"referenceID": 4, "context": "We also tested published implementations of the classic algorithms GLMNET (Friedman et al., 2010) and LARS (Efron et al.", "startOffset": 74, "endOffset": 97}, {"referenceID": 2, "context": ", 2010) and LARS (Efron et al., 2004).", "startOffset": 17, "endOffset": 37}, {"referenceID": 7, "context": "Large, Sparse Datasets: Very large and sparse problems, including predicting stock volatility from text in financial reports (Kogan et al., 2009).", "startOffset": 125, "endOffset": 145}, {"referenceID": 17, "context": "Sparco: Real-valued datasets of varying sparsity from the Sparco testbed (van den Berg et al., 2009). n \u2208 [12829166], d \u2208 [128, 29166]. Single-Pixel Camera: Dense compressed sensing problems from Duarte et al. (2008). n \u2208 [410, 4770], d \u2208 [1024, 16384].", "startOffset": 82, "endOffset": 217}, {"referenceID": 7, "context": "The largest dataset, whose features are occurrences of bigrams in financial reports (Kogan et al., 2009), has 5 million features and 30K samples.", "startOffset": 84, "endOffset": 104}, {"referenceID": 8, "context": "On L1 logreg (Koh et al., 2007) and CDN (Yuan et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 22, "context": ", 2007) and CDN (Yuan et al., 2010), our results qualitatively matched their survey.", "startOffset": 16, "endOffset": 35}, {"referenceID": 21, "context": "For a large-scale comparison of various algorithms for sparse logistic regression, we refer the reader to the recent survey by Yuan et al. (2010). On L1 logreg (Koh et al.", "startOffset": 127, "endOffset": 146}, {"referenceID": 8, "context": "On L1 logreg (Koh et al., 2007) and CDN (Yuan et al., 2010), our results qualitatively matched their survey. Yuan et al. (2010) do not explore SGD empirically.", "startOffset": 14, "endOffset": 128}, {"referenceID": 22, "context": "As Yuan et al. (2010) show empirically, their Coordinate Descent Newton (CDN) method is often orders of magnitude faster than the basic Shooting algorithm (Alg.", "startOffset": 3, "endOffset": 22}, {"referenceID": 21, "context": ", Zinkevich et al. (2010). We used lazy shrinkage updates (Langford et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 9, "context": "We used lazy shrinkage updates (Langford et al., 2009a) to make use of sparsity in A. Choosing learning rates for SGD can be challenging. In our tests, constant rates led to faster convergence than decaying rates (decaying as 1/ \u221a T ). For each test, we tried 14 exponentially increasing rates in [10\u22124, 1] (in parallel) and chose the rate giving the best training objective. We did not use a sparsifying step for SGD. SMIDAS (Shalev-Shwartz & Tewari, 2009) uses stochastic mirror descent but truncates gradients to sparsify x. We tested their published C++ implementation. Parallel SGD refers to Zinkevich et al. (2010)\u2019s work, which runs SGD in parallel on different subsamples of the data and averages the solutions x.", "startOffset": 32, "endOffset": 621}, {"referenceID": 9, "context": "We used lazy shrinkage updates (Langford et al., 2009a) to make use of sparsity in A. Choosing learning rates for SGD can be challenging. In our tests, constant rates led to faster convergence than decaying rates (decaying as 1/ \u221a T ). For each test, we tried 14 exponentially increasing rates in [10\u22124, 1] (in parallel) and chose the rate giving the best training objective. We did not use a sparsifying step for SGD. SMIDAS (Shalev-Shwartz & Tewari, 2009) uses stochastic mirror descent but truncates gradients to sparsify x. We tested their published C++ implementation. Parallel SGD refers to Zinkevich et al. (2010)\u2019s work, which runs SGD in parallel on different subsamples of the data and averages the solutions x. We tested this method since it is one of the few existing methods for parallel regression, but we note that Zinkevich et al. (2010) did not address L1 regularization in their analysis.", "startOffset": 32, "endOffset": 854}, {"referenceID": 12, "context": "The rcv1 dataset 6 (Lewis et al., 2004) illustrates the high-dimensional regime (d > n).", "startOffset": 19, "endOffset": 39}], "year": 2011, "abstractText": "We propose Shotgun, a parallel coordinate descent algorithm for minimizing L1regularized losses. Though coordinate descent seems inherently sequential, we prove convergence bounds for Shotgun which predict linear speedups, up to a problemdependent limit. We present a comprehensive empirical study of Shotgun for Lasso and sparse logistic regression. Our theoretical predictions on the potential for parallelism closely match behavior on real data. Shotgun outperforms other published solvers on a range of large problems, proving to be one of the most scalable algorithms for L1.", "creator": "TeX"}}}