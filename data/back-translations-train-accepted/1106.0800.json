{"id": "1106.0800", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2011", "title": "Optimal Reinforcement Learning for Gaussian Systems", "abstract": "The exploration-exploitation tradeoff is among the central challenges of reinforcement learning. A hypothetical exact Bayesian learner would provide the optimal solution, but is intractable in general. I show that, however, in the specific case of Gaussian process inference, it is possible to make analytic statements about optimal learning of both rewards and transition dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics. The solution is described by an infinite-dimensional differential equation. For a first impression of how this result may be useful, I also provide an approximate reduction to a finite-dimensional problem, with a numeric solution.", "histories": [["v1", "Sat, 4 Jun 2011 08:14:59 GMT  (2456kb,AD)", "http://arxiv.org/abs/1106.0800v1", null], ["v2", "Wed, 7 Sep 2011 16:11:15 GMT  (37kb,D)", "http://arxiv.org/abs/1106.0800v2", "updated to camera-ready version for publication in NIPS 2011. Note some nontrivial changes to the Equations on page 4"], ["v3", "Fri, 14 Oct 2011 15:01:11 GMT  (39kb,D)", "http://arxiv.org/abs/1106.0800v3", "final pre-conference version of this NIPS 2011 paper. Once again, please note some nontrivial changes to exposition and interpretation of the results, in particular in Equation (9) and Eqs. 11-14. The algorithm and results have remained the same, but their theoretical interpretation has changed"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["philipp hennig"], "accepted": true, "id": "1106.0800"}, "pdf": {"name": "1106.0800.pdf", "metadata": {"source": "CRF", "title": "Optimal Reinforcement Learning for Gaussian Systems", "authors": ["Philipp Hennig"], "emails": ["phennig@tuebingen.mpg.de"], "sections": [{"heading": "1 Introduction \u2014 optimal reinforcement learning", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2 A class of learning problems", "text": "Consider the task of optimal control of an unsafe system, its states (x, t), its future requirements uncertain (RD). Consider the task of optimal control of an unsafe system, its states (x, t), its future operation uncertain (RD = RD = RD). (D) Suppose that the dynamics of the system are divided into a free and a controlled term linearly in the control: dx (t) = [f, t) = [f (x, t) + g (x, t)] dt (x, t) is the control function we are looking for, and f, g are analytical functions. For the following analysis, we must assume that either f or g are known, while the others may be unsure that it is possible to obtain independent samples of both functions."}, {"heading": "3 Optimal control for the learning process", "text": "From a control theory point of view, the optimal solution for exploring all possible forms is formed by the dual control (20) of a common representation of both the physical system and the learning machine to model it. In this section, the equation of the common control problem for the system described in Sec. 2 (3, 4) is interpreted, but not normally as a control problem, which requires a description of the dynamics of the learning algorithm: Let the system sand in phase-space-time = (x), and have the Gaussian process belief GP (q; s) that we (s) have about function q (all derivatives in this section)."}, {"heading": "4 Numerically solving the Hamilton-Jacobi-Bellman equation", "text": "The solution of Equation (14) is in principle a problem of numerical analysis, and a battery of numerical methods can be considered in a similar way. In this section we report on a specific approach, for which I must break with the generality of the previous sections and assume that the nuclei k are given by square exponentials k (a, b) = kSE (a, b) = kSE (a, os, S) = 2 exp (a \u2212 b))) with parameters. We find an approximate solution by a factoring parametric approach: Let the value of any point z in the faith space be given by a set of parameters and some non-linear functionalities, so that their contributions are separated by phase space, meaning and covariance functions: v (z) = x, kr, kr, kr, kr, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp, kp."}, {"heading": "5 Experiments", "text": "I first apply the new method to a simple, one-dimensional environment to show some aspects that may not be obvious, followed by an example application that is compared with other algorithms."}, {"heading": "5.1 Illustrative experiment using an artificial environment", "text": "I constructed a simple example system in a one-dimensional state space by scanning f, q from the model described in section 2 = 10 points, and setting g to the unit function, for simplicity's sake. State space was regularly tiled, in a limited region, with 231 square exponential (\"radial\") basic functions (equation 39), initially all weighing wix = 0. For the information terms, only a single basic function is used for each term (i.e. a single number of parameters, a single \u03c6q, and likewise for all with very large length scales S covering the entire region of interest). We will see that this does not imply a trivial structure for these terms. Five times the number of parameters, i.e. Neval = 1175 eval value sampled, in each step of time, uniformly across the same region. It is not intuitively clear whether each of the points of faith should have their own proof (i.e. the points should also cover the points of faith)."}, {"heading": "6 Conclusion", "text": "To my knowledge, this is the first analytical statement about this trade-off. It decouples the problem of optimal learning in terms of amplification from the difficult prediction of future trajectories and replaces it with the problem of solving a differential equation for which a significant portion of previous work is available. To give some intuition as to how such solutions might work, I have presented a specific approach by reducing the problem to finite parameter estimates of the smallest squares by means of functionalities. The class of systems to which this method is applied is not arbitrarily general, but provides reasonable descriptions of a wide range of physical systems. It even addresses some assumptions of classical learning in terms of amplification, e.g. by allowing dynamics and reward expectations to change over time. There are two arguments for \"structured,\" probable approaches such as the one presented here. The first is the immense complexity of the general problem of amplification, which makes the universality of the theory unlikely to capture, making the theory of its universality."}, {"heading": "Acknowledgments", "text": "I would like to thank Carl E. Rasmussen and Jan Peters for the helpful discussions. This project was financed by a scholarship from the Max Planck Society.Appendix"}, {"heading": "A Details on the supplementary animation", "text": "As in each panel, the current state of the system is shown as a large black diamond with a green border. In each panel, the time is shown on the launch list, the space on the launch list. The third, the \"film dimension,\" shows the development throughout the learning and control process. In each panel, the current state of the system is shown as a large black diamond with a green border. In each panel, the room is on the launch list. The third, the \"film dimension,\" shows the development of the entire learning and control process. In each panel, the current state of the system is shown as a large black diamond with a green border."}, {"heading": "B Comparative experiments: Furuta pendulum", "text": "In this simulation, the pendulum is characterized as two massive rods of mass m1 = 0.5kg (arm) and 2kg (pendulum). However, the exact shapes of the dynamics can be found in Fantoni and Rogelio [19]. I selected the loss function to be seen from the upright position, and the anecular speeds of the arms and pendulums are also able to move. (26), where d = [dtip], the absolute distance of the tip of the pendulum from the upright position, and the anecular speeds of the arms and pendulum. The scaling matrix is D = [1m, 25 / s, 25 / s] 2. The advantage of this form of the loss function is that it roughly has the correct scale (-3.3) to be learned."}, {"heading": "C Mathematical appendix", "text": "In this appendix, the focus is on solving a number of integrals, and the notation should be as clean as possible. To achieve this, we will not treat time as a particular dimension of phase space, and will also introduce some new, more compact notation, which can sometimes come at the cost of a discrepancy between the notations in the two texts, but will make it much easier to analyze the following derivatives. In particular, we will not treat time as a particular dimension of phase space, and instead we will rename the phase coordinates after x x x, K = RD, with a new definition for D as Dappendix = 1.C.1 PreliminariesThe derivatives in this text, at its core, all rely on the Gaussian integral."}], "references": [{"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, 25:275\u2013294", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1933}, {"title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["M.O.G. Duff"], "venue": "PhD thesis, U of Massachusetts, Amherst", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Proceedings of the 23rd International Conference on Machine Learning, pages 697\u2013704", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Model based Bayesian exploration", "author": ["Richard Dearden", "Nir Friedman", "David Andre"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcolm Strens"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "International Conference on Machine Learning, pages 956\u2013963", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th International Conference on Machine Learning. Morgan Kaufmann", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6(1):4\u201322", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, 47(2):235\u2013256", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning, 49 (2):209\u2013232", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "R-max \u2014 a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "Journal of Machine Learning Research, 3:213\u2013231", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Journal of Computer and System Sciences, 74(8):1309\u20131331", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "Proc. ACM Symposium on Theory of Computing, pages 681\u2013690", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Linearly-solvable Markov decision problems", "author": ["E. Todorov"], "venue": "Advances in Neural Information Processing Systems, 19", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to stochastic control theory", "author": ["H.J. Kappen"], "venue": "path integrals and reinforcement learning. In 9th Granada seminar on Computational Physics: Computational and Mathematical Modeling of Cooperative Behavior in Neural Systems., pages 149\u2013181", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal trade-off between exploration and exploitation", "author": ["A. Simpkins", "R. De Callafon", "E. Todorov"], "venue": "American Control Conference, 2008, pages 33\u201338", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-linear Control for Underactuated Mechanical Systems", "author": ["I. Fantoni", "L. Rogelio"], "venue": "Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1973}, {"title": "Dual control theory", "author": ["A.A. Feldbaum"], "venue": "Automation and Remote Control,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1961}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Differential space", "author": ["N. Wiener"], "venue": "Journal of Mathematical Physics, 2:131\u2013174", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1923}, {"title": "An innovations approach to least-squares estimation \u2014 part I: Linear filtering in additive white noise", "author": ["T. Kailath"], "venue": "IEEE Transactions on Automatic Control, 13(6):646\u2013655", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1968}, {"title": "Slice sampling covariance hyperparameters of latent Gaussian models", "author": ["I. Murray", "R.P. Adams"], "venue": "arXiv:1006.0868", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "An algorithm for least-squares estimation of nonlinear parameters", "author": ["D.W. Marquardt"], "venue": "Journal of the Society for Industrial and Applied Mathematics, 11(2):431\u2013441", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1963}, {"title": "Swing-up control of inverted pendulum using pseudo-state feedback", "author": ["K. Furuta", "M. Yamakita", "S. Kobayashi"], "venue": "Journal of Systems and Control Engineering, 206(6):263\u2013269", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["M.P. Deisenroth", "C.E. Rasmussen"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as \u201c -greedy\u201d [1], or \u201cThompson sampling\u201d [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as \u201c -greedy\u201d [1], or \u201cThompson sampling\u201d [2].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "However, at least since a thesis by Duff [3] it has been known that Bayesian inference allows optimal balance between exploration and exploitation.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "The trouble is that this amounts to optimization and integration over a tree of exponential cost in the size of the state space [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 5, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 6, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 7, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 8, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 10, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 11, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 12, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 13, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 14, "context": "But bound-based algorithms can not be extended to continuous spaces without making further assumptions [15], and doing so invalidates the strongest argument in their favour \u2014 that they are free of assumptions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "In a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "In a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems.", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "[18] showed that it is actually possible to solve for the exploration exploitation tradeoff locally, by constructing a linear approximation for the system using a Kalman filter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "(1) still covers numerous physical systems studied in control, for example many mechanical systems, from classics like cart-and-pole to realistic models for helicopters [19].", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "From a control-theoretic standpoint, the optimal solution to the exploration exploitation tradeoff is formed by the dual control [20] of a joint representation of both the physical system and the learning machine used to model it.", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem.", "startOffset": 96, "endOffset": 102}, {"referenceID": 3, "context": "In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem.", "startOffset": 96, "endOffset": 102}, {"referenceID": 20, "context": "For arbitrary points s\u2217 = (x\u2217, t\u2217) \u2208 K, the belief over q(s\u2217) is a Gaussian with mean function \u03bc\u03c4 , and co-variance function \u03a3\u03c4 [21]", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "where d\u03c9 is the Wiener [22] measure.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "(28), reads [18]", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not \u201ccovered\u201d by the notion of optimality of the learning as addressed here.", "startOffset": 141, "endOffset": 149}, {"referenceID": 23, "context": "On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not \u201ccovered\u201d by the notion of optimality of the learning as addressed here.", "startOffset": 141, "endOffset": 149}, {"referenceID": 24, "context": "To solve for w, we simply choose a sufficiently large number of evaluation points zeval to constrain the resulting system of quadratic equations, and then find the least-squares solution wopt by function minimisation, using standard methods, such as Levenberg-Marquardt [25].", "startOffset": 270, "endOffset": 274}, {"referenceID": 25, "context": "For variation, I test the algorithm on its cylindrical version, the pendulum on the rotating arm [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "\u2019s [18] Kalman method and the Gaussian process learning controller (Fig.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "The exact forms of the dynamics can be found in Fantoni and Rogelio [19].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "[18], this method was used only as a tracking controller).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "One such method is that of Deisenroth and Rasmussen [27], which is a purely greedy method, but uses a Gaussian Process forward model for optimization.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "We will use, throughout, kernels k : K \u00d7K_ R in the square exponential (SE) class [21]:", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "We will adopt the notation from Rasmussen and Williams [21], where k(a, b) denotes a row vector \u2208 R1\u00d7dim b and similarly k(a, b) \u2208 R a\u00d7dim .", "startOffset": 55, "endOffset": 59}], "year": 2017, "abstractText": "The exploration-exploitation tradeoff is among the central challenges of reinforcement learning. A hypothetical exact Bayesian learner would provide the optimal solution, but is intractable in general. I show that, however, in the specific case of Gaussian process inference, it is possible to make analytic statements about optimal learning of both rewards and transition dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics. The solution is described by an infinite-dimensional differential equation. For a first impression of how this result may be useful, I also provide an approximate reduction to a finite-dimensional problem, with a numeric solution.", "creator": "TeX"}}}