{"id": "1106.1622", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2011", "title": "Large-Scale Convex Minimization with a Low-Rank Constraint", "abstract": "We address the problem of minimizing a convex function over the space of large matrices with low rank. While this optimization problem is hard in general, we propose an efficient greedy algorithm and derive its formal approximation guarantees. Each iteration of the algorithm involves (approximately) finding the left and right singular vectors corresponding to the largest singular value of a certain matrix, which can be calculated in linear time. This leads to an algorithm which can scale to large matrices arising in several applications such as matrix completion for collaborative filtering and robust low rank matrix approximation.", "histories": [["v1", "Wed, 8 Jun 2011 19:07:09 GMT  (39kb,D)", "http://arxiv.org/abs/1106.1622v1", "ICML 2011"]], "COMMENTS": "ICML 2011", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shai shalev-shwartz", "alon gonen", "ohad shamir"], "accepted": true, "id": "1106.1622"}, "pdf": {"name": "1106.1622.pdf", "metadata": {"source": "META", "title": "Large-Scale Convex Minimization with a Low-Rank Constraint", "authors": ["Shai Shalev-Shwartz", "Alon Gonen"], "emails": ["shais@cs.huji.ac.il", "alongnn@gmail.com", "ohadsh@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Our goal is to approximately solve an optimization problem of form: min A: rank (A) \u2264 r R (A), (1) where R: Rm \u00d7 n \u2192 R is a convex and smooth function, which occurs in many machine learning applications, such as cooperative filtering (Koren et al., 2009), robust low-level matrix approximation (Ke & Kanade, 2005; Croux & Filzmoser, 1998; A. Baccini & Falguerolles, 1996) and multiclass classification (Amit et al., 2007). Ranking A is not convex and therefore generally difficult to solve (this follows from (Natarajan et al., 1997)."}, {"heading": "1.1. Related work", "text": "This year, it has reached the stage where it will be able to take the lead in opening up a wide range of future perspectives."}, {"heading": "2. The GECO algorithm", "text": "In this section we describe our algorithms, which we call Greedy Efficient Component Optimization (or GECO for short). Let us A-Rm \u00b7 n be a matrix, and without loss of generality assume that m-S-S-S-S-S-n. The SVD theorem states that A-R-R-R-R-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S S-S-S-S-S-S-S S S-S-S S S-S-S S-S S S-S S S S-S-S S S S-S-S S S S S-S S-S-S S-S-S-S S-S S S S S S-S S S-S S-S-S-S S S S S-S S S S S-S S S S S-S-S S S S S-S"}, {"heading": "2.1. Variants of GECO", "text": "2.1.1. How does (u, v) GECO (u, v) select the leading singular vectors that represent the maximization of uT-R (A) v over the unified spheres of Rm and Rn? Our analysis in the next section guarantees that this choice leads to a sufficient reduction in the lens function. However, there may be a pair (u, v) that leads to an even greater reduction in the lens value. Choosing such a direction can lead to improved performance. We note that our analysis in the next section is still valid as long as the direction we choose leads to a greater reduction in the lens value relative to the increase we can achieve by using the leading singular vectors. In Section 6 we describe a method that finds better directions."}, {"heading": "2.1.2. Additional replacement steps", "text": "Each GECO iteration increases the rank by 1. In many cases, it is possible to reduce the target by replacing one of the components without increasing the rank. If we verify that this replacement step actually lowers the target (by simply evaluating the target before and after the change), then the analysis we present in the next section will remain valid. We will now describe a simple method of performing a replacement process. We will start by searching for a candidate pair (u, v) and perform steps 5 \u2212 7 of the GECO. Then, we will approach the matrix B by setting its smallest singular value to zero. Let us call this approach. Next, we will verify that R (UB, V T) is strictly smaller than the previous objective value. If so, we will update U, V based on B and achieve that the rank of UV T was not increased while the target was lowered. Otherwise, we will update U based on UB guaranteed and then increase the target by 1, but the analysis says that our rank is increased only when we increase the T)."}, {"heading": "2.1.3. Adding Schatten norm regularization", "text": "In some situations, the ranking constraint is not sufficient to obtain good generalization guarantees, and one can consider objective functions R (A) that contain an additional regularization of the form h (\u03bb (A), where \u03bb (A) is the vector of singular values of A and h is a vector function such as h (x) = x x-2p. If, for example, p = 2, this regularization concept corresponds to the Frobenius norm regularization of A. In general, adding a convex regularization term should not be a problem. A simple trick is to orthonorize the columns of U and V before step 6. Therefore, the singular values of B correspond to the singular values of UBV T. Therefore, we can solve the problem more efficiently in step 6 while regulating B instead of the larger matrix of UBV T."}, {"heading": "2.1.4. Optimizing over diagonal matrices B", "text": "Step 6 of GECO involves solving a problem with i2 variables, where i variables {1,..., r}. If r is small, this is a reasonable amount of computing effort. However, if r is large, steps 6-7 can be expensive. In matrix completion problems, for example, the complexity of step 6 can be scaled with r6. If runtime is important, it is possible to limit B to a diagonal matrix, or in other words, we optimize only the coefficients of the one that corresponds to U and V without changing the support of \u03bb. Thus, in step 6 we solve a problem with i variables, and step 7 is not necessary. It is possible to check whether the analysis we give in the next section still applies to this variant."}, {"heading": "3. Analysis", "text": "In this section we give a competitive analysis for GECO. The first theorem shows that after performing the r-iterations of GECO its solution is not much worse than the solution of all matrices A, whose track orm 3 is defined by a function of r. The second theorem shows that we can compete with additional assumptions with matrices whose rank is r at most. The evidence can be found in the long version of this paper. To formally define the theorems, we must first define a smoothing property of function f. Definition 1 (smoothing) We say that f is smooth if for any matrices and (u, v) if U \u2212 V we havef, v + \u03b7eu, v) + reception f f () if we have such a function f. We say that the trace standard of a matrix is the sum of its singular values.Where eu, v is the whole vector."}, {"heading": "4. Application I: Matrix Completion", "text": "The completion of the matrix is the problem of predicting the entries of an unknown target matrix Y > Rm \u00b7 n on the basis of a random subset of observed entries, E [m] \u00b7 [n]. For example, in the famous Netflix problem, m represents the number of users, n represents the number of movies, and Yi, j is a rating user that I give to the movie j. One approach to learning the matrix Y is to find a low-ranking matrix A that roughly matches Y in the entries of E (in the middle square)."}, {"heading": "4.1. Analysis", "text": "To apply our analysis to matrix completion, we must first define the smoothing parameters. Lemma 1 For matrix completion, the smoothing parameter is at most 2 / | E |.Proof For all u, v and i, j, we can (Ai, j + \u03b7uivj \u2212 Yi, j) 2 as (Ai, j \u2212 Yi, j) 2 + 2 (Ai, j \u2212 Yi, j) 2 as (Ai, j \u2212 2u2i v2i v2j. Taking into account the expectation of (i, j \u2212 Yi, j \u2212 E) we obtain: f (i, j \u2212 Yi, j) 2 + 3 as (Ai, j \u2212 Y."}, {"heading": "5. Application II: Robust Low Rank Matrix Approximation", "text": "A very common problem in data analysis is the search for a low-level matrix A that approaches a given matrix Y, namely the solution of minA: rank (A) \u2264 r (A, Y), where d is a certain discrepancy value. To simplify this, we assume that Y-Rn \u00b7 n. However, if d (A, V) is the normalized Frobenius standard d (A, V) = 1n2 \u00b2 -rank i, j \u2212 Yi, j \u00b2 -2, this problem can be efficiently solved via SVD. However, due to the use of the Frobenius standard, this approach is known to be sensitive to outliers. One way to make the process more robust is to replace the Frobenius standard with a less sensitive standard, such as the l1 standard d (A, V) = 1 n2 \u00b2 standard that we know."}, {"heading": "6. Experiments", "text": "We evaluated GECO for the problem of matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-"}, {"heading": "7. Discussion", "text": "GECO is an efficient, greedy approach to minimizing a convex function that is subjected to rank constraints. One of the main advantages of GECO is that in each of its iterations few (precise O (log (n))) iterations of the power method are performed, and therefore GECO scales are applied to large matrices. In the future, we intend to apply GECO to other applications such as multi-class classification and the learning of fast square classifiers."}, {"heading": "Acknowledgements", "text": "This work arose from fruitful discussions with Tomer Baba, Barak Cohen, Harel Livyatan and Oded Schwarz. The work is supported by the Israeli Science Foundation with grant number 598-10."}, {"heading": "A. Proofs", "text": "To prove the theory, we need the following key dilemma, which generalizes a result that is in (ShalevShwartz et al., 2010).Lemma 3 Suppose it is a vector that is based on me. Suppose there are two subsets of U \u00b7 V. Let us have a minimizer of f (\u03bb) over all vectors with support in I and let it be a vector that is based on me. Suppose that f (\u03bb) > f (\u03bb) > f (\u03bb), let us denote s = all subsets of U \u00b7 1, and let it come to a vector that is based on me. Let us then assume that there is such a vector."}], "references": [{"title": "A l1-norm pca and a heuristic approach", "author": ["A. Baccini", "P. Besse", "A. Falguerolles"], "venue": "Ordinal and Symbolic Data Analysis,", "citeRegEx": "Baccini et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Baccini et al\\.", "year": 1996}, {"title": "Uncovering shared structures in multiclass classification", "author": ["Amit", "Yonatan", "Fink", "Michael", "Srebro", "Nathan", "Ullman", "Shimon"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.F. Cai", "E.J. Candes", "Z. Shen"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2008}, {"title": "Matrix completion with noise", "author": ["E.J. Candes", "Y. Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Cand\u00e8s and Recht,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht", "year": 2009}, {"title": "Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm", "author": ["K.L. Clarkson"], "venue": "In Proceedings of the nineteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Clarkson,? \\Q2008\\E", "shortCiteRegEx": "Clarkson", "year": 2008}, {"title": "Robust factorization of a data matrix", "author": ["C. Croux", "P. Filzmoser"], "venue": "In COMPASTAT, Proceedings in Computational Statistics,", "citeRegEx": "Croux and Filzmoser,? \\Q1998\\E", "shortCiteRegEx": "Croux and Filzmoser", "year": 1998}, {"title": "Subspace pursuit for compressive sensing: Closing the gap between performance and complexity", "author": ["W. Dai", "O. Milenkovic"], "venue": null, "citeRegEx": "Dai and Milenkovic,? \\Q2008\\E", "shortCiteRegEx": "Dai and Milenkovic", "year": 2008}, {"title": "Greedy adaptive approximation", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation,", "citeRegEx": "Davis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1997}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "In American Control Conference,", "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "An algorithm for quadratic programming", "author": ["M. Frank", "P. Wolfe"], "venue": "Naval Res. Logist. Quart.,", "citeRegEx": "Frank and Wolfe,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe", "year": 1956}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["Hazan", "Elad"], "venue": "In Proceedings of the 8th Latin American conference on Theoretical informatics,", "citeRegEx": "Hazan and Elad.,? \\Q2008\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2008}, {"title": "A simple algorithm for nuclear norm regularized problems", "author": ["M. Jaggi", "M. Sulovsk\u1ef3"], "venue": "In ICML,", "citeRegEx": "Jaggi and Sulovsk\u1ef3,? \\Q2010\\E", "shortCiteRegEx": "Jaggi and Sulovsk\u1ef3", "year": 2010}, {"title": "Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming", "author": ["Q. Ke", "T. Kanade"], "venue": "In CVPR,", "citeRegEx": "Ke and Kanade,? \\Q2005\\E", "shortCiteRegEx": "Ke and Kanade", "year": 2005}, {"title": "Optspace: A gradient descent algorithm on the grassman manifold for matrix completion", "author": ["R.H. Keshavan", "S. Oh"], "venue": "Arxiv preprint arXiv:0910.5260", "citeRegEx": "Keshavan and Oh,? \\Q2009\\E", "shortCiteRegEx": "Keshavan and Oh", "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert M", "Volinsky", "Chris"], "venue": "IEEE Computer,", "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Estimating the largest eigenvalue by the power and Lanczos algorithms with a random start", "author": ["J. Kuczy\u0144ski", "H. Wo\u017aniakowski"], "venue": "SIAM journal on matrix analysis and applications,", "citeRegEx": "Kuczy\u0144ski and Wo\u017aniakowski,? \\Q1992\\E", "shortCiteRegEx": "Kuczy\u0144ski and Wo\u017aniakowski", "year": 1992}, {"title": "Admira: Atomic decomposition for minimum rank approximation", "author": ["K. Lee", "Y. Bresler"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Lee and Bresler,? \\Q2010\\E", "shortCiteRegEx": "Lee and Bresler", "year": 2010}, {"title": "Matching pursuits with timefrequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mallat and Zhang,? \\Q1993\\E", "shortCiteRegEx": "Mallat and Zhang", "year": 1993}, {"title": "Sparse approximate solutions to linear systems", "author": ["B. Natarajan"], "venue": "SIAM J. Computing,", "citeRegEx": "Natarajan,? \\Q1995\\E", "shortCiteRegEx": "Natarajan", "year": 1995}, {"title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Needell and Tropp,? \\Q2009\\E", "shortCiteRegEx": "Needell and Tropp", "year": 2009}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["YC Pati", "R. Rezaiifar", "Krishnaprasad", "PS"], "venue": "In Signals, Systems and Computers,", "citeRegEx": "Pati et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pati et al\\.", "year": 1993}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": null, "citeRegEx": "Recht et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2007}, {"title": "Collaborative filtering with the trace norm: Learning, bounding, and transducing", "author": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["Shalev-Shwartz", "Shai", "Zhang", "Tong", "Srebro", "Nathan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Generalization error bounds for collaborative prediction with low-rank matrices", "author": ["N. Srebro", "N. Alon", "T. Jaakkola"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Lemma 3 Assume that f is \u03b2-smooth. Let I, \u012a be two subsets of U \u00d7 V. Let \u03bb be a minimizer of f(\u03bb) over all vectors with support in I and let \u03bb\u0304 be a vector supported on \u012a", "author": ["Shwartz"], "venue": null, "citeRegEx": "Shwartz,? \\Q2010\\E", "shortCiteRegEx": "Shwartz", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "This problem arises in many machine learning applications such as collaborating filtering (Koren et al., 2009), robust low rank matrix approximation (Ke & Kanade, 2005; Croux & Filzmoser, 1998; A.", "startOffset": 90, "endOffset": 110}, {"referenceID": 1, "context": "Baccini & Falguerolles, 1996), and multiclass classification (Amit et al., 2007).", "startOffset": 61, "endOffset": 80}, {"referenceID": 20, "context": "The rank constraint on A is non-convex and therefore it is generally NP-hard to solve Equation (1) (this follows from (Natarajan, 1995; Davis et al., 1997)).", "startOffset": 118, "endOffset": 155}, {"referenceID": 8, "context": "The rank constraint on A is non-convex and therefore it is generally NP-hard to solve Equation (1) (this follows from (Natarajan, 1995; Davis et al., 1997)).", "startOffset": 118, "endOffset": 155}, {"referenceID": 2, "context": "See for example (Cai et al., 2008; Candes & Plan, 2010; Cand\u00e8s & Recht, 2009; Keshavan et al., 2010; Keshavan & Oh, 2009).", "startOffset": 16, "endOffset": 121}, {"referenceID": 15, "context": "See for example (Cai et al., 2008; Candes & Plan, 2010; Cand\u00e8s & Recht, 2009; Keshavan et al., 2010; Keshavan & Oh, 2009).", "startOffset": 16, "endOffset": 121}, {"referenceID": 25, "context": ", 2002) algorithms), and in particular we extend the fully corrective forward greedy selection algorithm given in (Shalev-Shwartz et al., 2010)).", "startOffset": 114, "endOffset": 143}, {"referenceID": 5, "context": "The algorithm we propose is also related to Hazan\u2019s algorithm (Hazan, 2008) for solving PSD problems, which in turns relies on Frank-Wolfe algorithm (Frank & Wolfe, 1956) (see Clarkson (Clarkson, 2008)), as well as to the follow-up paper of (Jaggi & Sulovsk\u1ef3, 2010), which applies Hazan\u2019s algorithm for optimizing with trace-norm constraints.", "startOffset": 185, "endOffset": 201}, {"referenceID": 25, "context": "These differences between the approaches are analogous to the difference between Frank-Wolfe algorithm and fully corrective greedy selection, for minimizing over sparse vectors, as discussed in (Shalev-Shwartz et al., 2010).", "startOffset": 194, "endOffset": 223}, {"referenceID": 26, "context": "(Srebro et al., 2005)), it is possible to show that for any matrix A of rank at most r we have that with high probability", "startOffset": 0, "endOffset": 21}], "year": 2011, "abstractText": "We address the problem of minimizing a convex function over the space of large matrices with low rank. While this optimization problem is hard in general, we propose an efficient greedy algorithm and derive its formal approximation guarantees. Each iteration of the algorithm involves (approximately) finding the left and right singular vectors corresponding to the largest singular value of a certain matrix, which can be calculated in linear time. This leads to an algorithm which can scale to large matrices arising in several applications such as matrix completion for collaborative filtering and robust low rank matrix approximation.", "creator": "LaTeX with hyperref package"}}}