{"id": "1106.1887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "Learning the Dependence Graph of Time Series with Latent Factors", "abstract": "This paper considers the problem of learning, from samples, the dependency structure of a system of linear stochastic differential equations, when some of the variables are latent. In particular, we observe the time evolution of some variables, and never observe other variables; from this, we would like to find the dependency structure between the observed variables -- separating out the spurious interactions caused by the (marginalizing out of the) latent variables' time series. We develop a new method, based on convex optimization, to do so in the case when the number of latent variables is smaller than the number of observed ones. For the case when the dependency structure between the observed variables is sparse, we theoretically establish a high-dimensional scaling result for structure recovery. We verify our theoretical result with both synthetic and real data (from the stock market).", "histories": [["v1", "Thu, 9 Jun 2011 19:34:29 GMT  (33kb)", "http://arxiv.org/abs/1106.1887v1", null], ["v2", "Sat, 25 Feb 2012 02:03:33 GMT  (1654kb,D)", "http://arxiv.org/abs/1106.1887v2", null], ["v3", "Fri, 16 Mar 2012 16:18:16 GMT  (1656kb,D)", "http://arxiv.org/abs/1106.1887v3", null], ["v4", "Tue, 1 May 2012 04:30:11 GMT  (1656kb,D)", "http://arxiv.org/abs/1106.1887v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ali jalali", "sujay sanghavi"], "accepted": true, "id": "1106.1887"}, "pdf": {"name": "1106.1887.pdf", "metadata": {"source": "CRF", "title": "Learning with Latent Factors in Time Series", "authors": ["Ali Jalali", "Sujay Sanghavi"], "emails": ["alij@mail.utexas.edu", "sanghavi@mail.utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 6.18 87v1 [cs.LG] 9 Jun 2"}, {"heading": "1 Introduction", "text": "Linear stochastic dynamic systems are classical processes that are often used to model time series data in a huge number of domains: financial data [8], biological networks of species [17] or genes [1], chemical reactions [11, 12], control systems with noise [27], etc. An important task in several of these domains is to learn the model from data [25]; this is often the first step in data interpretation, and predicting future values or the effect of perturbations. Often, one is interested in learning the dependency structure [15]; i.e., identifying for each variable which variable it directly interacts. For stock market data, this can show which other stocks most directly influence a particular stock. We look at model structure learning in a particularly challenging but widespread setting: where (the time series of) some state variables are observed, and others are not observed / latent."}, {"heading": "2 Problem Setting and Main Idea", "text": "This paper looks at the problem of structural learning in linear stochastic dynamic systems, in an environment in which only a subset of the time series Q = > Q is observed and others are ignored / latent. In particular, we look at a system with the fastest vectors x (t) and u (t).Q = = unique Q = > Q (considered in total max.) Q (considered in total max. Q + x (considered in total)."}, {"heading": "2.1 Main Idea", "text": "Let us consider the discrete time system (2) in a constant state and suppose for a moment that we have ignored the fact that there may be latent time series; in this case we would be back in the classical frame for which the (population version of) probability isL (A) = 1 2\u03b72E [\u0442 x (i + 1) \u2212 x (i) \u2212 \u03b7Ax (i) \u2022 22].Lemma 1. For x (\u00b7) generated by (2), the optimum A: = maxA L (A) is obtained from A = A + B * R (Q *) \u2212 1.Thus, the optimal A * is a sum of the original A * (which we want to restore) and the matrix B * R * (Q *) \u2212 1, which captures the fake interactions obtained from the latent time series. Note that the matrix B * (Q *) \u2212 1 is at most equal to the latent time series."}, {"heading": "2.2 Identifiability", "text": "In addition to determining the effect of the latent time series, the above finding also allows us to quantify the assumptions we would have to make to ensure identification. In particular, the true model should be such that A * is uniquely identifiable by B * R * (Q *) \u2212 1. We opt for models that have a local-global structure in which (a) each of the observed time series xi (t) interacts with only a few other observed series, while (b) each of the latent series interacts with a (relatively) large number of observed series. In the example of the equity market, this would model the case where the latent series corresponds to macroeconomic factors such as currencies or oil prices that affect a lot of stock prices."}, {"heading": "2.3 Algorithm", "text": "It should be remembered that our task is to restore the matrix A * in view of the observations of the x (\u00b7) process. We saw that the maximum probability estimate (in the case of the population) was the sum of A * and a low-ranking matrix; we subsequently assumed that A * is sparse. In view of this development, it is natural to use the maximum probability as a loss function for the sum of a low-ranking and low-ranking matrix, and separately suitable regulators for each of the components. Therefore, we propose for the continuous time system observed until the time T, the solution (A *, L *) = Argmin A, L12T * Tt = 0 (A + L) x (t) x (T) T (A + L) T (T) T (t) T (T) T (T) T (T) + Gate (T) + A (t) + (t), A (A (A) + la), A (A (A), A (A), A (A), A (A, A, A (A), A (A, A (A), A (A, A, A (A), A (A, A, A, A (A), A (A, A, A (A), A (A, A), A (A, A (A), A (A), A (A, A (A), A (A, A (A), A (A), A (A), A (A), A (A, A (A), A, A (A), A (A), A (A (A), A (A), A (A), A (A), A (A (A), A (A), A (A (A), A (A), A (A), A (A), A (A), A (A), A (A (A), A (A), A (A), A (A), A (A)."}, {"heading": "2.4 High-dimensional setting", "text": "Note that the actual degree of freedom between the observed variables is smaller than that occupied by the ambient dimension p. In fact, we will be interested in restoring A * with a number of samples n, which is potentially much smaller than p (for small s and d). In the particular case, if we are in a steady state and L = 0 (i.e. large), restoring each series of A * is comparable to a LASSO [24] problem (the sparse vector recovery from loud linear measurements), where Q * is the covariance of the design matrix. We therefore demand Q \u00b2 to satisfy incoherence conditions similar to those in LASSO (see e.g. [26] for the necessity of such conditions). (A3) Incoherence: To control the effect of the irrelevant (non-latent) variables on the set of relevant variables, we demand: = 1 \u2212 maxk \u00b2 SK \u00b2 (row SK \u00b2), where SK \u00b2 is the SK \u00b2."}, {"heading": "2.5 Related Work", "text": "Extensive work is being done on the sparse and low decomposition of matrices for both silent [6, 4, 7] and loud [28, 3] cases, as well as on graphic model learning [22, 14, 20], all by convex optimization. Recently, Chandrasekaran et. al. [5] investigated the problem of learning latent variables in graphic models by sparse and low decomposition. In most of these results, the focus is on restoring the low component, where, as in our work, we try to restore the sparse component. Furthermore, the graphic models assume that the samples are independent and we certainly cannot make this assumption. Learning graphic models for time series has recently been examined in [2]. However, they do not take latent time series into account and therefore use LASSO only to obtain a sparse graphic model. Our method can handle latent time series if they exist."}, {"heading": "3 Main Results", "text": "In this section, we present our main results for both Continuous and Discrete Time Systems. We begin by making some assumptions about regulators and sample complexity (4). (4) Regulators: We must impose some assumptions on regulators in order to be able to guarantee our results. (4) We impose the following assumptions on regulators: \u2022 (4-1). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4)............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "4 Proof Outline", "text": "In this section, we first present some notations and definitions, and then provide a three-step proof technique to prove the most important theorems for the discrete time system. \u2212 Proof of the continuous time system is provided by a coupling problem that is included in the appendix. \u2212 Predetermined break knowledge: For each matrix A-Rp \u00b7 p, define Supp (A) \u00b7 p (A-Rp) \u00b7 p, define Supp (A-Rp) \u00b7 p, define Supp (A-Rp) \u00b7 p, whose support is the subset of the matrix A-Rp. Orthogonal projection on the subspace can be defined so that (K) j equals M (k) j, if (j) j, k) j, Supp (A-Rp) and zero else."}, {"heading": "5 Experimental Results", "text": "In this section we illustrate numerically the power of our theoretical result for both synthetic and real data collected on the stock exchange."}, {"heading": "5.1 Synthetic Data", "text": "We generate a sparse matrix A * with values taken from a standard normal distribution, and we generate B *, C * and D * with i.i.d. entries drawn from a standard normal distribution scaled by a factor r (to ensure that amine is statistically large enough). To make the matrix A * negatively unambiguous (i.e. stable), we place a sufficiently large negative value on the diagonal. We simulate the discrete time system for different values of \u03b7 = C\u03b7 2\u03c3max (A *) for the stability parameter C\u03b7 (0, 1) and solve (4) by means of accelerated proximal gradient method [18]. Note that the value of \u03b7 alone is not a good criterion for assessing the performance of these algorithms, since the rate of changes in the dynamic system depends on A \u00b2. For this reason, we use C\u043c for performance analysis."}, {"heading": "5.2 Stock Market Data", "text": "We take the closing prices at the end of the day for 50 different companies during the period from May 17, 2010 to May 13, 2011 (255 business days). These companies (including Amazon, eBay, Pepsi, etc.) are consumer goods companies that are traded either on the NASDAQ or on the NYSE in USD. The data is collected from the Google Finance website. Our goal is to monitor stock prices for a period of time and use them for the entire days of the next month with small errors. We apply our algorithm to this data and try to learn the model from the data for n (consecutive) days and then calculate the average square error in the prediction of the following month (25 business days).We randomly select an initial day n0 between day 1 and day 255 \u2212 25 \u2212 n. Then we learn the model using the data from day n0 to day n0 + n (the average square ratio of n days).Then we test our data on the consecutive 25 days."}, {"heading": "A Proof of Lemma 1", "text": "Proof. Ignoring the term \u0435x (n + 1) \u2212 x (n) \u0445 22, the minimization of L (A) at this infinite sample size corresponds to tomin A E [x (n) TATAx (n) \u2212 2\u03b7 (x (n + 1) \u2212 x (n)) TAx (n) = min A E [trace (Ax (n) x (n) TAT) \u2212 2 (A \u0445 x (n) + B \u0445 u (t)) T Ax (n)] = min Atrace (AQ \u0445 AT) \u2212 2trace (A \u0445 Q \u0445 AT) \u2212 2trace (B \u0445 R \u0445 AT) = min Atrace (((A \u2212 2 (A \u0445 + B \u0445 R (Q \u0445) \u2212 1))) Q \u0445 AT).Here we ignored the term w (n) due to the fact that it is zero mean and independent of x (n) and u (n)."}, {"heading": "B Proof of Lemma 2", "text": "General notation: For a matrix X-Ra \u00b7 b, we use X (1),.., X (a) to specify rows, X1,. \u2212 Z to specify columns, and X (1),. \u2212 Z, the matrix XS1S2,. \u2212 Z, the matrix XS1S2, represents the submatrix of X, consisting of rows and columns corresponding to the index sets S1 and S2.SDL. \u2212 Z, the matrix XS1S2,. \u2212 Z representing the submatrix of X, consisting of rows and columns. \u2212 Z corresponding to the index sets S1 and S2.SDL. \u2212 Z,."}, {"heading": "C Auxiliary Optimality Lemmas", "text": "Lemma 4 (Convex Optimality) = PT (PT) and PT (PT) = PT (PT)."}, {"heading": "D Concentration Results", "text": "In this section we will illustrate the concentration results used in the work. Before specifying the results, we will introduce some useful inequalities that are used to obtain the results. According to the dynamics of the system, we have [x (i) u (i) u (a) i (x) u (0) u (0) u (1) i \u2212 l (l) u (i).Lemma 7. For each S (1, 2,., p) with | S (s) s and the sample complexity np (n) i (n) i (n) i (n) i (n) i (n)."}, {"heading": "E Proof of the Continuous Time Theorem", "text": "The proof: Denote X (t) = [x (t) u (t)] T and letQ (n) T (n) is sufficient (see proof of theorem 1.1 in [2] for more details) to show that for a given continuous time system there is a discrete time system with Q (n) and W (n), so that Q (n) \u2212 \u2192 Q (n) \u2212 Q (n) \u2212 W (n) for a given continuous time system can almost certainly satisfy Q (n) \u2212 \u2192 Q (n) the convergence. Let Q \u0445 be the matrix that represents the continuous time of Lyapunov's equation of stability A."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper considers the problem of learning, from samples, the depen-<lb>dency structure of a system of linear stochastic differential equations,<lb>when some of the variables are latent. In particular, we observe the time<lb>evolution of some variables, and never observe other variables; from this,<lb>we would like to find the dependency structure between the observed vari-<lb>ables \u2013 separating out the spurious interactions caused by the (marginal-<lb>izing out of the) latent variables\u2019 time series. We develop a new method,<lb>based on convex optimization, to do so in the case when the number of<lb>latent variables is smaller than the number of observed ones. For the case<lb>when the dependency structure between the observed variables is sparse,<lb>we theoretically establish a high-dimensional scaling result for structure re-<lb>covery. We verify our theoretical result with both synthetic and real data<lb>(from the stock market).", "creator": "LaTeX with hyperref package"}}}