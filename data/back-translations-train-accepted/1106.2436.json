{"id": "1106.2436", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2011", "title": "From Bandits to Experts: On the Value of Side-Observations", "abstract": "We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node $i$ is linked to node $j$ if sampling $i$ provides information on the reward of $j$. This setting naturally interpolates between the well-known \"experts\" setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, as well as partially-matching lower bounds. The regret depends on non-trivial graph theoretic properties of the information feedback structure, and reveals an interesting trade-off between regret optimality and computational efficiency.", "histories": [["v1", "Mon, 13 Jun 2011 13:11:33 GMT  (84kb,D)", "http://arxiv.org/abs/1106.2436v1", null], ["v2", "Tue, 14 Jun 2011 22:33:57 GMT  (85kb,D)", "http://arxiv.org/abs/1106.2436v2", null], ["v3", "Tue, 25 Oct 2011 15:55:47 GMT  (87kb,D)", "http://arxiv.org/abs/1106.2436v3", "Presented at the NIPS 2011 conference"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shie mannor", "ohad shamir"], "accepted": true, "id": "1106.2436"}, "pdf": {"name": "1106.2436.pdf", "metadata": {"source": "CRF", "title": "From Bandits to Experts: On the Value of Side-Observations", "authors": ["Shie Mannor", "Ohad Shamir"], "emails": ["shie@ee.technion.ac.il", "ohad@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to put ourselves in a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they are able to put themselves into another world, in which they are able to move, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which"}, {"heading": "1.1 Related Work", "text": "The standard problem of multi-armed bandits, however, does not assume any relationship between actions. Several studies examined alternative models in which actions are endowed with a richer structure. However, the feedback structure in the vast majority of these works is the same as that of standard multi-armed bandits. Examples are [12], where the rewards of actions are drawn from a statistical distribution, with correlations between actions; and [1, 9], where it is assumed that the rewards of some parts of actions satisfy a certain Lipschitz continuity characteristic in terms of distance between actions. In terms of other approaches, the combinatory bandit framework [8] considers an attitude somewhat similar to ours by selecting and observing the rewards of some parts of actions. However, it is decisively assumed that the reward that is achieved is the sum of the rewards of all actions in the subset. In other words, there is no separation between the rewards obtained and the information about them."}, {"heading": "2 Problem Setting", "text": "Leave [k] = {1,.., k} and [T] = {1,., T}. We consider a series of actions 1, 2,.., k. Selecting an action i in turn t results in us receiving a reward gi (t) which we will accept without loss of generality to be limited in [0, 1]. Our goal is to minimize regret in relation to the best individual action in the future, namelymax i T = 1 gi (t) \u2212 T = 1 git (t). For simplicity, we will focus on a finite horizon setting (where the number of actions T is known), on regret in relation to the best individual action, namelymax i i t = 1 gi (t) \u2212 T = 1 git (t)."}, {"heading": "3 The ExpBan Algorithm", "text": "We start by introducing the ExpBan algorithm (see algorithm 1 above), which builds on existing algorithms to deal with our setting, in the particular case where the graph structure remains fixed during the rounds - namely Gt = G for all. The idea of the algorithm is to divide the actions into c cliques so that selecting an action in a clique reveals unbiased estimates of the rewards of all other actions in the clique. By executing a standard expert algorithm (such as the exponentially weighted forecaster - see [7, Chapter 2]), we can get a slight regret about any action in that clique. We then treat any such expert algorithm as a meta action and run a standard bandit algorithm (such as the EXP3 [4]) over these c-meta actions. We call this algorithm ExpBan because it combines an expert algorithm with a bandit algorithm."}, {"heading": "4 The ELP Algorithm", "text": "Turning now to the ELP algorithm (which stands for \"exponentially weighted algorithm with linear programming\"), like all multi-armed bandit algorithms, it is based on a compromise between exploration and exploitation. However, unlike standard algorithms, the exploration component is not uniform across the actions, but is carefully selected to reflect the graph structure in each round. In fact, the optimal choice of exploration requires us to solve a simple linear program, hence the name of the algorithm. Below, we present the pseudo-code and a few theorems that limit the expected regrets of the algorithm to appropriate parameter selections. The proofs of the theorems are shifted to the complementary material. The first theorem concerns the symmetrical observation case, with the choice of the algorithm i providing information about the action j, then the choice of the action j."}, {"heading": "4.1 Undirected Graphs", "text": "Theorem 2: Suppose that for all t Gt an undirected graph (i.e., it is an undirected graph (i.e., it is an undirected graph (i.e., i-Nj (t)))) is guaranteed if and only if j-Ni (t) is bound. Suppose we run algorithm 2 using some \u03b2 (0, 1 / 2bk), and select {si (t)} i-Nj (t) = argmax (t) i si si (t) \u2265 0, \u2211 i (t) = 1 min j-Nj (t)."}, {"heading": "4.2 Directed Graphs", "text": "So far, we have assumed that the graphs we are dealing with are all undirected. However, it is a natural extension of this setting to assume a directed graph, where the selection of an action i can give us information about the reward of action j, but not vice versa. It is easy to see that the ExpBan algorithm would still work in this setting, with the same guarantee. For the ELP algorithm, we can give the following guarantee: Theorem 3. Under the conditions of Thm. 2 (with the ease that the graphs Gt might be directed), however, it applies to each set action j that T \u2211 t = 1 gj (t) \u2212 E [T \u2211 t = 1 git (t)]]. 5\u03b2b2 T \u0445 t = 1 (Gt), + log (k) \u03b2. (4), where the set action j (t) is."}, {"heading": "5 Lower Bound", "text": "The following theorem provides a lower limit for regret in relation to the independence number \u03b1 (G), for a constant chart Gt = G.Theorem 4. SupposeGt = G for all t, and that actions that are not associated with G do not get any side observations between them. Then, there is an (accidental) strategy of the opponent, so that for each T \u2265 374\u03b1 (G) 3 and each learning strategy the expected regret is at least 0.06 \u221a (G) T. Proof this is provided in the supplementary material. The intuition of the evidence is that if the chart G has independent vertices, an opponent can make this problem as difficult as a standard problem with several armed bandits played on \u03b1 (G) actions. Using a known lower limit for armed bandits on n actions, our result follows n actions. If the diagram is constantly undirected, this lower limit corresponds to the regret upper limit for the ELM-2 factors (Thm)."}, {"heading": "6 Examples", "text": "Here we briefly discuss some concrete examples of graph G and show how the repentance performance of our algorithms depends on their structure. An interesting problem is the potential gap between the performance of our algorithms, due to the independence of the graph number \u03b1 (G) and clique partition number \u0432 (G). First, we consider the case in which there is a single action, so that the selection of this action reveals the rewards of all other actions. In contrast, the selection of the other actions only shows their own reward. At first glance, it may seem that such a \"super-action,\" which reveals everything that is happening in the current round, should help us improve our regrets. However, the independence number \u03b1 (G) of such a graph is slightly higher than k \u2212 1. Based on our lower limit, we see that this \"super-action\" is actually unhelpful (up to negligible factors). Second, we consider the case in which the actions are equipped with a metric distance function, and the edge (j) is only constant in distance (G)."}, {"heading": "7 Empirical Performance Gap between ExpBan and ELP", "text": "In this section, we show that the gap between the performance of the ExpBan algorithm and the ELP algorithm can be real and not just an artifact of our analyses.To show this, we performed the following simple experiment: We created a random terdo \ufffd s - re \ufffd nyi graph over 300 nodes, each pair of nodes being independently associated with the probability of p. Selecting an action leads to observing the rewards of neighboring actions in the graph. The reward of each action in each round was randomly and independently selected to be 1 with the probability of 1 / 2 and 0 with the probability of 1 / 2, with the exception of a single node whose reward is 1 with a higher probability of 3 / 4. We then implemented the ExpBan and ELP algorithms in this setting, for T = 30, 000. For comparison, we implemented the standard expo 3 multiarmed bandit algorithm [4], which does not use side observations."}, {"heading": "8 Discussion", "text": "In this paper, we initiated a study on a large family of online learning problems with secondary observations. In particular, we examined the broad regime that interpolates between the experts and the bandits who set online learning. We provided algorithms, as well as upper and lower limits of achievable regret. Regret seems to have an interesting and non-trivial dependence on the information feedback structure. In addition, the results lead to something that appears like a trade-off between the information-theoretical achievable regret and what can be achieved with mathematically efficient methods, at least from a worst-case perspective. This contributes to an ongoing line of work that examines the relationship between the information-theoretical and the computational aspects of learning (e.g. [13, 14]). Intuitively, this can be seen by considering the extreme cases - for a complete graph of nodes that are both numbers equal, and for an empty graph of k-nodes that are both."}, {"heading": "A Supplementary Material: Proofs", "text": "Suppose we share the actions in c cliquesC1 = 1 + 1 + 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 + 1 = 1 + 1 = 1 + 1 = 1 + 1 + 1 = 1 + 1 = 1 + 1 + 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 + 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 + 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 + 1 + 1 + 1 = 1 = 1 + 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 + 1 = 1 = 1 = 1 + 1 + 1 = 1 + 1 = 1 = 1 = 1 + 1 = 1 = 1 + 1 ="}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We consider an adversarial online learning setting where a decision maker can choose an action in every stage<lb>of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on<lb>the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded<lb>as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting<lb>naturally interpolates between the well-known \u201cexperts\u201d setting, where the decision maker can view all rewards, and<lb>the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop<lb>practical algorithms with provable regret guarantees, as well as partially-matching lower bounds. The regret depends<lb>on non-trivial graph theoretic properties of the information feedback structure, and reveals an interesting trade-off<lb>between regret optimality and computational efficiency.", "creator": "LaTeX with hyperref package"}}}