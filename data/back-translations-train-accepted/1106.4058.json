{"id": "1106.4058", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2011", "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "abstract": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (", "histories": [["v1", "Mon, 20 Jun 2011 23:23:11 GMT  (97kb,AD)", "http://arxiv.org/abs/1106.4058v1", "11 pages, to be presented at EMNLP 2011, to be published in Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing"]], "COMMENTS": "11 pages, to be presented at EMNLP 2011, to be published in Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing", "reviews": [], "SUBJECTS": "cs.CL math.CT", "authors": ["edward grefenstette", "mehrnoosh sadrzadeh"], "accepted": true, "id": "1106.4058"}, "pdf": {"name": "1106.4058.pdf", "metadata": {"source": "CRF", "title": "Experimental Support for a Categorical Compositional Distributional Model of Meaning", "authors": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh"], "emails": ["edward.grefenstette@cs.ox.ac.uk", "mehrs@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "We are, of course, good at understanding ambiguous words in a context and building the meaning of a sentence from the meaning of its components, but while humans do so, the machines cannot deliver. Search engines like Google either resort to terms used to process terms like syntax and lexical relationships - or they use superficial models of lexical semantics to retrieve pages of terms from quantity (Manning et al., 2008). However, such models fail when it comes to processing the semantics of phrases and tendencies. Discovering the process of assigning meaning in natural language is one of the most difficult and fundamental questions in linguistics and computer science, and the results increase our understanding of cognition and intelligence and help us automate language-related tasks."}, {"heading": "2 Two Orthogonal Semantic Models", "text": "In fact, most people who live in a country in which they live are not in a position to integrate themselves, but in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "3 A Hybrid Logico-Distributional Model", "text": "For example, while semantic compositional mechanisms for set-theoretical constructions are understood as a common good, there are no obvious corresponding methods for vector spaces. To solve this problem, we use the abstract placement of category theory to transform the grammatical structure of a sentence into a morphism compatible with the higher level of the logical structure of vector spaces. The pragmatic consequence of this abstract idea is as follows: There is a meaning vector for each word, e.g. cats, \u2212 like, and \u2212 like milk. The logical recipe tells us to apply the meaning of the verb to the meanings of subjects and objects, but how can a vector be applied to other vectors that are suggested above that one must have different levels of meaning for words of different types, similar to logical models in which verbal relations and nouns are atomic sets."}, {"heading": "4 Building Matrices for Relational Words", "text": "In this section, we introduce a general scheme to form matrices for a relational word that is based on any foundations. \"We have a vector space with a base.\" \"We have a vector space.\" \"We have a vector space.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We.. \"\" We.. \"\" We. \"\" We. \"\" We. \"\" We. \"We.\" We. \"\" We. \"\" We. \"We..\" We. \"\" We.. \"We.\" \"We.\" \"We.\" We. \"\" We.. \"We.\" \"We..\" We. \"\" We.. \"\" We. \"\" We. \"\" We. \"\" \"We.\" \"We..\" We. \"\" \"We..\" \"We.\" \"\" We.. \"\" We. \"\" \"We..\" \"We..\" \"\" \"We.\" \"\" \"We...\" We... \"We.\" \"We.\" \"\" \"We..\" \"We..\" We..... \"We.\" We.. \"We....\" We. \"\" We.. \"\" We.... \"We..\" We. \"We..\" We.. \"\" \"\" We. \"We..\" We.. \"\" \"\" We. \"We....\" We.. \"We.\" We.. \"We..\" We... \"We.\" We. \"We...\" We. \"We...\" We. \"We..\" We. \"We...\" We. \""}, {"heading": "5 Computing Sentence Vectors", "text": "In fact, it is as if it were a reactionary act, capable of retaliating and retaliating. (...) In fact, it is as if it were a reactionary action. (...) It is as if it were an attempt. (...) It is as if it were a reactionary action. (...) It is as if it were a reactionary action. (...) It is as if it were a reactionary action. (...) It is as if it were a reactionary action. (...) It is as if it were a reactionary action. (...) It is as if it were a reactionary action. (...)"}, {"heading": "6 Evaluation", "text": "In (Grefenstette et al., 2011) it is proposed that the simplified model we have presented and expanded here could be evaluated in the same way as lexical semantic models by presenting a new evaluation task that extends the experimental methodology of Mitchell and Lapata. (2008) In this section we briefly describe the evaluation of our model prior to this dataset. Subsequently, we present a new evaluation task that expands the experimental methodology of Mitchell and Lapata. (2008) to include transitive verb-centric propositions to those discussed by Mitchell and Lapata. (2008) within this new experiment The first experiment described by Mitchell and Lapata (2008) is ambiguous."}, {"heading": "7 Discussion", "text": "In this paper we have described an implementation of the categorical model of meaning (Coecke et al., 2010), which combines formal logic and empirical frameworks of distribution into a unified semantic model, based on the construction of matrices for words with relative types (adjectives, verbs) and vectors for words with atomic types (nouns), based on data from the BNC. We then show how to apply verbals to their subject / object to calculate the meaning of intransitive and transitive sentences. (2008) Other work uses matrices for model meanings (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases. Our approach easily applies to combinations containing combinations of adjectives, subjectives, verbs, verbs and adverses."}, {"heading": "8 Future Work", "text": "The treatment of functional words such as \"the,\" \"who\" and logical words such as quantifiers and subjunctivities will be left to future work, which will build on our setting in addition to the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005). Preparatory work to integrate the two was presented by Preller (2007) and more recently by Preller and Sadrzadeh (2009). As mentioned by one of the reviewers, our pregroup approach to grammar flattens out the sentence representation by applying the verb simultaneously to its subject and object; while in other approaches such as CCG it is first applied to the object to produce a verb phrase which is then applied to the subject to produce the sentence. The advantages and disadvantages of this method and comparisons with other systems, in particular CCG, represent ongoing work."}, {"heading": "9 Acknowledgement", "text": "We thank P. Blunsom, S. Clark, B. Coecke, S. Pulman and the anonymous EMNLP reviewers for discussions and comments. M. Sadrzadeh is grateful for the support of the EPSRC scholarship EP / F042728 / 1."}], "references": [{"title": "Combining Symbolic and Distributional Models of Meaning", "author": ["S. Clark", "S. Pulman."], "venue": "Proceedings of AAAI Spring Symposium on Quantum Interaction. AAAI Press.", "citeRegEx": "Clark and Pulman.,? 2007", "shortCiteRegEx": "Clark and Pulman.", "year": 2007}, {"title": "Categories for the Practicing Physicist", "author": ["B. Coecke", "E. Paquette."], "venue": "New Structures for Physics, 167271. B. Coecke (ed.). Lecture Notes in Physics 813. Springer.", "citeRegEx": "Coecke and Paquette.,? 2011", "shortCiteRegEx": "Coecke and Paquette.", "year": 2011}, {"title": "Mathematical Foundations for Distributed Compositional Model of Meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark."], "venue": "Lambek Festschrift. Linguistic Analysis 36, 345\u2013384. J. van Benthem, M. Moortgat and W. Buszkowski (eds.).", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "From Distributional to Semantic Similarity", "author": ["J. Curran."], "venue": "PhD Thesis, University of Edinburgh.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "A Structured Vector Space Model for Word Meaning in Context", "author": ["K. Erk", "S. Pad\u00f3."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 897\u2013906.", "citeRegEx": "Erk and Pad\u00f3.,? 2004", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2004}, {"title": "\u00dcber Sinn und Bedeutung", "author": ["G. Frege"], "venue": "Zeitschrift f\u00fcr Philosophie und philosophische Kritik 100.", "citeRegEx": "Frege,? 1892", "shortCiteRegEx": "Frege", "year": 1892}, {"title": "A synopsis of linguistic theory 19301955", "author": ["J.R. Firth."], "venue": "Studies in Linguistic Analysis.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Concrete Compositional Sentence Spaces for a Compositional Distributional Model of Meaning", "author": ["E. Grefenstette", "M. Sadrzadeh", "S. Clark", "B. Coecke", "S. Pulman."], "venue": "International Conference on Computational Semantics (IWCS\u201911). Oxford.", "citeRegEx": "Grefenstette et al\\.,? 2011", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["G. Grefenstette."], "venue": "Kluwer.", "citeRegEx": "Grefenstette.,? 1994", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "A Regression Model of AdjectiveNoun Compositionality in Distributional Semantics", "author": ["E. Guevara."], "venue": "Proceedings of the ACL GEMS Workshop.", "citeRegEx": "Guevara.,? 2010", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "A Cycling Cancellation-Automaton for Sentence Well-Formedness", "author": ["Z.S. Harris."], "venue": "International Computation Centre Bulletin 5, 69\u201394.", "citeRegEx": "Harris.,? 1966", "shortCiteRegEx": "Harris.", "year": 1966}, {"title": "Word Grammar", "author": ["R. Hudson."], "venue": "Blackwell.", "citeRegEx": "Hudson.,? 1984", "shortCiteRegEx": "Hudson.", "year": 1984}, {"title": "From Word to Sentence", "author": ["J. Lambek."], "venue": "Polimetrica, Milan.", "citeRegEx": "Lambek.,? 2008", "shortCiteRegEx": "Lambek.", "year": 2008}, {"title": "A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T. Landauer", "S. Dumais."], "venue": "Psychological review.", "citeRegEx": "Landauer and Dumais.,? 2008", "shortCiteRegEx": "Landauer and Dumais.", "year": 2008}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze."], "venue": "Cambridge University Press.", "citeRegEx": "Manning et al\\.,? 2008", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, 236\u2013244.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "English as a formal language", "author": ["R. Montague."], "venue": "Formal Philosophy, 189\u2013223.", "citeRegEx": "Montague.,? 1974", "shortCiteRegEx": "Montague.", "year": 1974}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["J. Nivre."], "venue": "Proceedings of the 8th International Workshop on Parsing Technologies (IWPT).", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Word Segmentation: The role of distributional cues", "author": ["J. Saffron", "E. Newport", "R. Asling."], "venue": "Journal of Memory and Language 35, 606\u2013621.", "citeRegEx": "Saffron et al\\.,? 1999", "shortCiteRegEx": "Saffron et al\\.", "year": 1999}, {"title": "Automatic Word Sense Discrimination", "author": ["H. Schuetze."], "venue": "Computational Linguistics 24, 97\u2013123.", "citeRegEx": "Schuetze.,? 1998", "shortCiteRegEx": "Schuetze.", "year": 1998}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems", "author": ["P. Smolensky."], "venue": "Computational Linguistics 46, 1\u2013 2, 159\u2013216.", "citeRegEx": "Smolensky.,? 1990", "shortCiteRegEx": "Smolensky.", "year": 1990}, {"title": "The Syntactic Process", "author": ["M. Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Geometry and Meaning", "author": ["D. Widdows."], "venue": "University of Chicago Press.", "citeRegEx": "Widdows.,? 2005", "shortCiteRegEx": "Widdows.", "year": 2005}, {"title": "Philosophical Investigations", "author": ["L. Wittgenstein."], "venue": "Blackwell.", "citeRegEx": "Wittgenstein.,? 1953", "shortCiteRegEx": "Wittgenstein.", "year": 1953}], "referenceMentions": [{"referenceID": 2, "context": "We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it.", "startOffset": 47, "endOffset": 68}, {"referenceID": 2, "context": "We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.", "startOffset": 47, "endOffset": 344}, {"referenceID": 14, "context": "Search engines such as Google either fall back on bag of words models\u2014ignoring syntax and lexical relations\u2014or exploit superficial models of lexical semantics to retrieve pages with terms related to those in the query (Manning et al., 2008).", "startOffset": 218, "endOffset": 240}, {"referenceID": 16, "context": "Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question.", "startOffset": 38, "endOffset": 68}, {"referenceID": 12, "context": "Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question.", "startOffset": 38, "endOffset": 68}, {"referenceID": 6, "context": "Compositional type-logical approaches (Montague, 1974; Lambek, 2008) and distributional models of lexical semantics (Schutze, 1998; Firth, 1957) have provided two partial orthogonal solutions to the question.", "startOffset": 116, "endOffset": 144}, {"referenceID": 5, "context": "Compositional formal semantic models stem from classical ideas from mathematical logic, mainly Frege\u2019s principle that the meaning of a sentence is a function of the meaning of its parts (Frege, 1892).", "startOffset": 186, "endOffset": 199}, {"referenceID": 23, "context": "Distributional models are more recent and can be related to Wittgenstein\u2019s later philosophy of \u2018meaning is use\u2019, whereby meanings of words can be determined from their context (Wittgenstein, 1953).", "startOffset": 176, "endOffset": 196}, {"referenceID": 8, "context": "The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al.", "startOffset": 105, "endOffset": 139}, {"referenceID": 3, "context": "The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al.", "startOffset": 105, "endOffset": 139}, {"referenceID": 14, "context": "The distributional models have found their way into real world applications such as thesaurus extraction (Grefenstette, 1994; Curran, 2004) or automated essay marking (Landauer, 1997), and have connections to semantically motivated information retrieval (Manning et al., 2008).", "startOffset": 254, "endOffset": 276}, {"referenceID": 2, "context": "Recently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category ar X iv :1 10 6.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": "A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011). In this paper we implement it by training the model over the entire BNC.", "startOffset": 88, "endOffset": 115}, {"referenceID": 15, "context": "The implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs.", "startOffset": 61, "endOffset": 88}, {"referenceID": 15, "context": "The implementation is evaluated against the task provided by Mitchell and Lapata (2008) for disambiguating intransitive verbs, as well as a similar new experiment for transitive verbs. Our model improves on the best method evaluated in Mitchell and Lapata (2008) and offers promising results for the transitive case, demonstrating its scalability in comparison to that of other models.", "startOffset": 61, "endOffset": 263}, {"referenceID": 15, "context": "Common operations discussed in (Mitchell and Lapata, 2008) such as vector addition (+) and componentwise multiplication ( , cf.", "startOffset": 31, "endOffset": 58}, {"referenceID": 21, "context": "via cosine measure or k-means clustering), as discussed in Widdows (2005). The principal drawback of such models is their non-compositional nature: they ignore grammatical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words.", "startOffset": 59, "endOffset": 74}, {"referenceID": 20, "context": "\u00a74 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007).", "startOffset": 52, "endOffset": 69}, {"referenceID": 0, "context": "\u00a74 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007). However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable.", "startOffset": 133, "endOffset": 157}, {"referenceID": 2, "context": "To solve this problem, Coecke et al. (2010) use the abstract setting of category theory to turn the grammatical structure of a sentence into a morphism compatible with the higher level logical structure of vector spaces.", "startOffset": 23, "endOffset": 44}, {"referenceID": 12, "context": "Pregroup Grammars The aforementioned linear maps turn out to be the grammatical reductions of a type-logic called a Lambek pregroup grammar (Lambek, 2008)2.", "startOffset": 140, "endOffset": 154}, {"referenceID": 1, "context": "Pregroups and vector spaces share the same high level mathematical structure, referred to as a compact closed category, for a proof and details of this claim see Coecke et al. (2010); for a friendly introduction to category theory, see Coecke and Paquette (2011).", "startOffset": 162, "endOffset": 183}, {"referenceID": 1, "context": "(2010); for a friendly introduction to category theory, see Coecke and Paquette (2011). One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors.", "startOffset": 60, "endOffset": 87}, {"referenceID": 2, "context": "Syntax-guided Semantic Composition According to Coecke et al. (2010) and based on a general completeness theorem between compact categories, wire diagrams, and vector spaces, the meaning of sentences can be canonically reduced to linear algebraic formulae.", "startOffset": 48, "endOffset": 69}, {"referenceID": 15, "context": "The basis vectors of N are in principle all the words from the corpus, however in practice and following Mitchell and Lapata (2008) we had to restrict these to a subset of the most occurring words.", "startOffset": 105, "endOffset": 132}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008).", "startOffset": 3, "endOffset": 30}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset.", "startOffset": 4, "endOffset": 297}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset. Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008) to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new experiment.", "startOffset": 4, "endOffset": 503}, {"referenceID": 7, "context": "In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008). In this section, we briefly describe the evaluation of our model against this dataset. Following this, we present a new evaluation task extending the experimental methodology of Mitchell and Lapata (2008) to transitive verb-centric sentences, and compare our model to those discussed by Mitchell and Lapata (2008) within this new experiment.", "startOffset": 4, "endOffset": 612}, {"referenceID": 15, "context": "First Dataset Description The first experiment, described in detail by Mitchell and Lapata (2008), evaluates how well compositional models disambiguate ambiguous words given the context of a potentially disambiguating noun.", "startOffset": 71, "endOffset": 98}, {"referenceID": 15, "context": "The correlation of the model\u2019s similarity judgements with the human judgements is also calculated using Spearman\u2019s \u03c1, a metric which is deemed to be more scrupulous, and ultimately that by which models should be ranked, by Mitchell and Lapata (2008). The mean for each model is on a [0, 1] scale, except for UpperBound which is on the same [1, 7] scale the annotators used.", "startOffset": 223, "endOffset": 250}, {"referenceID": 15, "context": "Other Models The other models we compare ours to are those evaluated by Mitchell and Lapata (2008). We provide a selection of the results", "startOffset": 72, "endOffset": 99}, {"referenceID": 15, "context": "We invite the reader to consult (Mitchell and Lapata, 2008) for the description of Kintsch\u2019s additive model and parametric choices.", "startOffset": 32, "endOffset": 59}, {"referenceID": 15, "context": "Model Parameters To provide the most accurate comparison with the existing multiplicative model, and exploiting the aforementioned feature that the categorical model can be built \u201con top of\u201d existing lexical distributional models, we used the parameters described by Mitchell and Lapata (2008) to reproduce the vectors evaluated in the original experiment as our noun vectors.", "startOffset": 267, "endOffset": 294}, {"referenceID": 15, "context": "The multiplicative model presented here is what is qualified as best in (Mitchell and Lapata, 2008).", "startOffset": 72, "endOffset": 99}, {"referenceID": 15, "context": "Table 3: Selected model means for High and Low similarity items and correlation coefficients with human judgements, first experiment (Mitchell and Lapata, 2008).", "startOffset": 133, "endOffset": 160}, {"referenceID": 15, "context": "Second Dataset Description The second dataset6, developed by the authors, follows the format of the (Mitchell and Lapata, 2008) dataset used for the first experiment, with the exception that the target and landmark verbs are transitive, and an object noun is provided in addition to the subject noun, hence forming a small transitive sentence.", "startOffset": 100, "endOffset": 127}, {"referenceID": 15, "context": "The dataset comprises 200 entries consisting of sentence pairs (hence a total of 400 sentences) constructed by following the procedure outlined in \u00a74 of (Mitchell and Lapata, 2008), using transitive verbs from CELEX7.", "startOffset": 153, "endOffset": 180}, {"referenceID": 15, "context": "Model Parameters As in the first experiment, the lexical vectors from (Mitchell and Lapata, 2008) were used for the other models evaluated (additive, multiplicative and baseline)8 and for the noun vec-", "startOffset": 70, "endOffset": 97}, {"referenceID": 2, "context": "In this paper, we described an implementation of the categorical model of meaning (Coecke et al., 2010), which combines the formal logical and the empirical distributional frameworks into a unified semantic model.", "startOffset": 82, "endOffset": 103}, {"referenceID": 9, "context": "Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases.", "startOffset": 42, "endOffset": 86}, {"referenceID": 15, "context": "We evaluated our model in two ways: first against the word disambiguation task of Mitchell and Lapata (2008) for intransitive verbs, and then against a similar new experiment for transitive verbs, which we developed.", "startOffset": 82, "endOffset": 109}, {"referenceID": 15, "context": "This should not surprise us given that the context is so small and our method becomes similar to the multiplicative model of Mitchell and Lapata (2008). However, our approach is sensitive to grammatical structure, leading us to develop a second experiment taking this into account and differentiating it from models with commutative composition operations.", "startOffset": 125, "endOffset": 152}, {"referenceID": 2, "context": "This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005).", "startOffset": 52, "endOffset": 73}, {"referenceID": 2, "context": "This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005). It is not yet entirely clear how existing set-theoretic approaches, for example that of discourse representation and generalised quantifiers, apply to our setting.", "startOffset": 52, "endOffset": 127}, {"referenceID": 2, "context": "This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005). It is not yet entirely clear how existing set-theoretic approaches, for example that of discourse representation and generalised quantifiers, apply to our setting. Preliminary work on integration of the two has been presented by Preller (2007) and more recently also by Preller and Sadrzadeh ( 2009).", "startOffset": 52, "endOffset": 372}], "year": 2015, "abstractText": "Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.", "creator": "TeX"}}}