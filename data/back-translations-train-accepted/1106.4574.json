{"id": "1106.4574", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods", "abstract": "Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice.", "histories": [["v1", "Wed, 22 Jun 2011 20:59:20 GMT  (37kb,D)", "http://arxiv.org/abs/1106.4574v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew cotter", "ohad shamir", "nati srebro", "karthik sridharan"], "accepted": true, "id": "1106.4574"}, "pdf": {"name": "1106.4574.pdf", "metadata": {"source": "CRF", "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods", "authors": ["Andrew Cotter", "Ohad Shamir", "Karthik Sridharan"], "emails": ["cotter@ttic.edu", "ohadsh@microsoft.com", "nati@ttic.edu", "karthik@ttic.edu"], "sections": [{"heading": null, "text": "We consider a stochastic optimization problem of formmin w & W L (w), where L (w) = Ez (w, z)], and the optimization is based on an empirical sample of instances z1,.., zm. We focus on objectives (w, z) that are not negative, convex, and smooth in their initial reasoning (i.e., we have a Lipschitz continuous gradient).The classic learning application is when z = (x, y) and \"(w, y) is a loss of prediction. In recent years, there has been much interest in the development of efficient first-class stochastic optimization methods for these problems, such as stochastic mirror descents [2, 6] and stochastic dual averages [9, 16]. These methods are characterized by incremental updates based on subgradients (w, zi) of individual instances and enjoy the benefits of highly scalable and heritable algorithms."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization<lb>problems. We study how such algorithms can be improved using accelerated gradient methods. We<lb>provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to<lb>obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this<lb>deficiency, enjoys a uniformly superior guarantee and works well in practice.", "creator": "LaTeX with hyperref package"}}}