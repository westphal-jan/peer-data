{"id": "1107.1283", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2011", "title": "Spectral Methods for Learning Multivariate Latent Tree Structure", "abstract": "This work considers the problem of learning the structure of a broad class of multivariate latent variable tree models, which include a variety of continuous and discrete models (including the widely used linear-Gaussian models, hidden Markov models, and Markov evolutionary trees). The setting is one where we only have samples from certain observed variables in the tree and our goal is to estimate the tree structure (i.e., the graph of how the underlying hidden variables are connected to the observed variables). We provide the Spectral Recursive Grouping algorithm, an efficient and simple bottom-up procedure for recovering the tree structure from independent samples of the observed variables. Our finite sample size bounds for exact recovery of the tree structure elucidate certain natural dependencies on underlying statistical and structural properties of the underlying joint distribution. Furthermore, our sample complexity guarantees have no explicit dependence on the dimensionality of the observed variables, making the algorithm applicable to many high-dimensional settings. At the heart of our algorithm is a spectral quartet test for determining the relative topology of a quartet of variables, which only utilizes certain second order statistics and is based on the determinants of certain cross-covariance matrices.", "histories": [["v1", "Thu, 7 Jul 2011 02:33:31 GMT  (367kb,D)", "https://arxiv.org/abs/1107.1283v1", null], ["v2", "Tue, 8 Nov 2011 15:42:32 GMT  (172kb,D)", "http://arxiv.org/abs/1107.1283v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["animashree anandkumar", "kamalika chaudhuri", "daniel j hsu", "sham m kakade", "le song", "tong zhang 0001"], "accepted": true, "id": "1107.1283"}, "pdf": {"name": "1107.1283.pdf", "metadata": {"source": "CRF", "title": "Spectral Methods for Learning Multivariate Latent Tree Structure", "authors": ["Animashree Anandkumar", "Kamalika Chaudhuri", "Daniel Hsu", "Sham M. Kakade", "Le Song", "Tong Zhang"], "emails": ["a.anandkumar@uci.edu,", "kamalika@cs.ucsd.edu,", "dahsu@microsoft.com,", "skakade@microsoft.com,", "lesong@cs.cmu.edu,", "tzhang@stat.rutgers.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Latent variable tree models", "text": "Let T be a contiguous, directional graphical tree model with leaves vobs: = {x1, x2,.., xn} and internal nodes vhid: = {h1, h2,.., hm}, so that each node has at most one parent. The leaves are called observed variables and the internal nodes hidden variables. Note that all nodes in this work generally correspond to multivariate random vectors; we will misuse terminology and still refer to these random vectors as random variables. For each h-vhid, ChildrenT (h) VT should represent the children of h in T. Each observed variable x-Vhid is modeled as a random vector in Rd, and each hidden variable h-Vhid as a random vector in Rk. Common distribution across all variables VT: = Vobs-Vhid is assumed as conditional independence properties specified by the tree structure."}, {"heading": "2.2 Structural and distributional assumptions", "text": "The class of linear tree distribution models used in practice comprises linear and discrete tree distribution models and the assumption that it is a hybrid combination of the two models widely used in practice. (The class of linear tree models used in practice comprises a variety of continuous and discrete tree distributions (as well as hybrid forms). (The class of linear tree distribution models used in practice comprises a variety of continuous and discrete tree distributions (as well as hybrid combinations of the two, such as Gaussian hybrid models). (The class of linear tree models used in practice comprises linear and discrete tree distribution models). (The class of linear tree distribution models used in practice comprises a variety of continuous and discrete tree distributions (as hybrid combinations of the two, such as Gaussian hybrid models)."}, {"heading": "3 Spectral quartet tests", "text": "This section describes the core of our learning algorithm, a spectral quartet test that determines the topology of the sub-tree induced by four observed variables {z1, z2, z3, z4}. There are four possibilities for the induced sub-tree, as shown in Figure 1. Our quartet test either returns the correct induced sub-tree among the possibilities in Figure 1 (a) - (c); or it returns the corresponding results to indicate abstinence. If the test does not provide guarantees for the induced sub-tree topology, if it returns a sub-tree, the output is guaranteed to be the correct induced sub-tree (with high probability). The proposed quartet test is described in Algorithm 1 (SpectralQuartet Test). Notation [a] + stands for max {0, a} and [t] for the correct sub-tree types."}, {"heading": "3.1 Properties of the spectral quartet test", "text": "With exact second moments (E [z1z > 4]) detk (E [z2z > 3]) detk (E [z1z > 3]) detk (E [2]) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) detk (E) dek (tk) (E) dek (E) (tdek) (E) (E) dek (E) (E) dek) (E)."}, {"heading": "4 The Spectral Recursive Grouping algorithm", "text": "The spectral variables are introduced as parents, and their children are removed by the parents when they leave parenthood. (...) The algorithms in the working world always correspond to the roots of the recursive subtrees of T discovered by the algorithms of the algorithms. (...) Since these subtrees are rooted, they naturally induce parent-child relationships, but these may differ from those detected by the boundary guidelines in T.) In each iteration, the algorithm determines which variables are to combine in the working group. (...) When the variables are combined, a new hidden variable is introduced as a parent, and their children are removed. (...) The algorithms determine which variables are to combine in the working group. (...) When the variables are combined, a new hidden variable is added to the working group, and the children are removed. (...) The algorithms determine which variables are to combine in the working group."}, {"heading": "Acknowledgements", "text": "Part of this work was completed during his DH residency at the Wharton School of the University of Pennsylvania and Rutgers University. AA was partially supported by UCI facility funds and the AFOSR Award FA9550-10-1-0310."}, {"heading": "A Sample-based confidence intervals for singular values", "text": "We show how to set trust limits for the individual values of \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"i,\" \"\" i, \"\" i, \"\" i, \"\" i, \"i,\" i, \"\" i. \"We show how to set\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i.\""}, {"heading": "B Analysis of the spectral quartet test", "text": "For each hidden variable h (gq) Vhid, let DescendantsT (h) > log = > log = > log = > log = > log = > log (v) > v (v) > Vhid so that the (directed) path from h to g then g1 \u2192 g2 \u2192 g2 \u00b7 \u00b7 \u00b7 gq = g, define A (g | h). (v) \u2212 v \u2212 Vhid [\u2212 v] so that the (directed) path from h to g gq \u2212 gq \u2212 1) \u00b7 \u00b7 A (g1 | h).p (g1 | h).p (v), for each x (g | h) Vobs so that the (guided) path from h to x h \u2192 g1 \u00b7 g2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 gq \u2192 gq \u2192 x, define C (x | h)."}, {"heading": "C Analysis of Spectral Recursive Grouping", "text": "C.1 OverviewHere is a sketch of the argument for Theorem 1.1. First, we have a condition for a 1 \u2212 \u03b7 probability event via the iid samples from the Vobs distribution in which the empirical matrices of the second moment are sufficiently close to the true matrices of the second moment maintained by the algorithm) that cause the mergeable subroutine to be true. (Lemma 11), as well as those that cause them, are false (Lemma 12).3 We use the above characterizations to show that the main loop of the algorithm maintains the loop invariants so that when the entire tree structure is completely terminated (Lemma 13)."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>This work considers the problem of learning the structure of multivariate linear tree models, which<lb>include a variety of directed tree graphical models with continuous, discrete, and mixed latent variables<lb>such as linear-Gaussian models, hidden Markov models, Gaussian mixture models, and Markov evolu-<lb>tionary trees. The setting is one where we only have samples from certain observed variables in the tree,<lb>and our goal is to estimate the tree structure (i.e., the graph of how the underlying hidden variables are<lb>connected to each other and to the observed variables). We propose the Spectral Recursive Grouping al-<lb>gorithm, an efficient and simple bottom-up procedure for recovering the tree structure from independent<lb>samples of the observed variables. Our finite sample size bounds for exact recovery of the tree structure<lb>reveal certain natural dependencies on underlying statistical and structural properties of the underlying<lb>joint distribution. Furthermore, our sample complexity guarantees have no explicit dependence on the<lb>dimensionality of the observed variables, making the algorithm applicable to many high-dimensional set-<lb>tings. At the heart of our algorithm is a spectral quartet test for determining the relative topology of a<lb>quartet of variables from second-order statistics.", "creator": "LaTeX with hyperref package"}}}