{"id": "1107.3258", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jul-2011", "title": "On Learning Discrete Graphical Models using Greedy Methods", "abstract": "In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d^2 log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of \\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.", "histories": [["v1", "Sat, 16 Jul 2011 22:04:13 GMT  (39kb)", "http://arxiv.org/abs/1107.3258v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.ML stat.TH", "authors": ["ali jalali", "christopher c johnson", "pradeep ravikumar"], "accepted": true, "id": "1107.3258"}, "pdf": {"name": "1107.3258.pdf", "metadata": {"source": "CRF", "title": "On Learning Discrete Graphical Models Using Greedy Methods", "authors": ["Ali Jalali"], "emails": ["alij@mail.utexas.edu", "cjohnson@cs.utexas.edu", "pradeepr@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 7,32 58"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2 Review, Setup and Notation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Markov Random Fields", "text": "Let X = (X1,.., Xp) be a random vector, where each variable Xi takes values in a discrete set X of cardinality m. Let G = (V,.., Xp) denote a graph with p-nodes corresponding to the p-variables {X1,...., Xp}. A paired Markov random field over X = (X1,.., Xp) is then specified successir by smart and paired functions: X 7 \u2192 R for all r-V and succeeds: X \u00b7 X \u00b7 X \u00b7 7 \u2192 R for all (r, t). E: P (x)."}, {"heading": "2.2 Graphical Model Selection", "text": "Leave Q: x (1), x (n), x (2), x (3), x (3), x (3), x (3), x (3), x (3), x (4), x (4), x (4), x (5), x (4), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x, x (5), x (5), x (5), x (5), x (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), x (5), x, x, x (5), x, x (5), x (5), x, x (5), x (5), x (5), x (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), 5 (5), x (5), x (5), x (5), x (5), x (5), x, x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5, x (5), x (5), x (5), x (5), x (5), x (5, x (5), x (5), x (5), x (5), x (5), x (5, x (5), x (5), x (5), x (5), x (5), x (5, x (5), x (5), x (5), x (5), x (5), x (5), x (5), x (5,"}, {"heading": "3 Greedy Algorithm for General Losses", "text": "Consider a random variable Z with an empty set of variables S (0) and gradually add (2)."}, {"heading": "3.1 Lemmas for Theorem 1", "text": "We list the simple lemmas that characterize the solution that is achieved when the algorithm ends and on which the proof of Theorem 1 is based. Algorithm 2 Greedy forward-backward algorithm for paired discrete graphical model LearningInput: Data D: = {x (1),., x (n), Stopping Threshold, Backward Step Factor, Output: Estimated Edges E: Estimated Edges for r, V do Run Algorithm 1 with L (\u00b7), described by (4) to get the forward algorithm and its support. Output E: Estimated Edges E: (r, t) Lemma 1 (Stopping Forward Step). If algorithm 1 is stopped with the parameters that are based on S, stopped."}, {"heading": "4 Greedy Algorithm for Pairwise Graphical Models", "text": "Suppose we get a set of n > i.d. samples D: = {x (1),., x (n), drawn from a pair Ising model as in (2), with parameters. \u2212 n Furthermore, it will be useful to specify the maximum node degree in diagram E (1). As we will show, our model selection performance depends crucially on this parameter. \u2212 n We then suggest algorithm 2 to estimate the underlying graphic model from the n samples D.Theorem 2 (Pairwise Sparsistency). We run algorithm 2 with holding threshold S 1 d log pn, where d is the maximum node degree in the graphic model, and the true parameters are satisfactory."}, {"heading": "5 Experimental Results", "text": "We now present experimental results that illustrate the power of algorithm 2 and support our theoretical guarantees. We simulated the structure learning of several different graph structures and compared the learning rates of our method with that of a standard diagram for 1-logistic regression as outlined in [20]. We conducted experiments with 3 different graph structures: (a) chain (line diagram), (b) 4-nearest neighbor (grid diagram) and (c) star diagram. For each experiment, we performed a pair binary output model in which each circle number rt = \u00b1 1 is random. For each graph type, we generated a series of n samples x (1),... x (n) using Gibbs sample. We then tried to learn the structure of the model using both algorithms 2 and its 1-logistic regression. We then compared the actual graph structure of the graph structure with the templated graph structure."}, {"heading": "A Auxiliary Lemmas for Theorem 1", "text": "In this section, we will prove the lemmas used in the proof of Theorem 1. Note that when the algorithm ends, the forward step does not pass through. (6) The next problem shows that this has the consequence that the deviation in loss between the estimated parameters is greater than the actual parameters. (6) When the algorithm stops with the parameters supported on S, we have the consequence that the deviation in loss between the estimated parameters is greater than the actual parameters. (6) When the algorithm stops, we will stop the parameters supported on S. (7) Let us stop the deviation based on S. (4) - L (5) \u2212 L (5). \u2212 L (5). \u2212 S."}, {"heading": "B Lemmas on the Stopping Size", "text": "Lemma 8: / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>In this paper, we address the problem of learning the structure of a pairwise<lb>graphical model from samples in a high-dimensional setting. Our first main re-<lb>sult studies the sparsistency, or consistency in sparsity pattern recovery, properties<lb>of a forward-backward greedy algorithm as applied to general statistical models.<lb>As a special case, we then apply this algorithm to learn the structure of a discrete<lb>graphical model via neighborhood estimation. As a corollary of our general result,<lb>we derive sufficient conditions on the number of samples n, the maximum node-<lb>degree d and the problem size p, as well as other conditions on the model param-<lb>eters, so that the algorithm recovers all the edges with high probability. Our result<lb>guarantees graph selection for samples scaling as n = \u03a9(d log(p)), in contrast<lb>to existing convex-optimization based algorithms that require a sample complexity<lb>of \u03a9(d log(p)). Further, the greedy algorithm only requires a restricted strong<lb>convexity condition which is typically milder than irrepresentability assumptions.<lb>We corroborate these results using numerical simulations at the end.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}