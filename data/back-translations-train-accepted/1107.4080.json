{"id": "1107.4080", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2011", "title": "On the Universality of Online Mirror Descent", "abstract": "We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee.", "histories": [["v1", "Wed, 20 Jul 2011 19:34:00 GMT  (32kb,D)", "http://arxiv.org/abs/1107.4080v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nati srebro", "karthik sridharan", "ambuj tewari"], "accepted": true, "id": "1107.4080"}, "pdf": {"name": "1107.4080.pdf", "metadata": {"source": "CRF", "title": "On the Universality of Online Mirror Descent", "authors": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "emails": ["nati@ttic.edu", "karthik@ttic.edu", "ambuj@cs.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "The question of the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in which we behave in the way in which we behave in which we behave in the way in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we behave in which we behave in which we behave in the way in which we behave in which we behave in which we behave in which we have in which we behave in which we behave in which we behave in which we behave in which we have in which we behave in which we have in which we have in which we behave in which we have in which we have in which we have in which we behave in which we behave in"}, {"heading": "2 Online Convex Learning Problem", "text": "In fact, it is as if most people are able to understand themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to survive in the world. (...) It is as if they are able to survive in the world. \"(...) It is so.\" (...) It is as if they are able to surpass themselves. (...) It is as if they are able to survive in the world. (...) It is as if they are in the world. (...) It is as if they are in the world. (...) It is as if they are in the world. (...)"}, {"heading": "3 Mirror Descent and Uniform Convexity", "text": "A key tool in analysis is the notion of a strong convexity, or more generally of a uniform convexity. (W) A key tool in analysis is the notion of a strong convexity, or more generally of a uniform convexity. (W) A key tool in analysis is the notion of a uniform convexity. (W) A key tool in analysis is the notion of a uniform convexity. (W) A key tool in analysis. (W) A key tool in analysis. (W) A key tool in analysis. (W) A key tool in analysis. (W) A key tool in analysis. (W) A key tool in analysis. (W) A key tool in analysis. (W) A key tool in analysis. (W) A key tool in analysis. (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W)."}, {"heading": "4 Martingale Type and Value", "text": "In this section, we expand the classical notion of Banach space (see, for example, [16]) in a way that refers to the pair (W?, X). Before proceeding with the definitions, we would like to introduce a few necessary notations. First, we will represent the infinite sequence of characters drawn randomly. (i.e.) Each i has the same probability of being + 1 or \u2212 1. (xn) n We represent a sequence of mappings in which each xn: {\u00b1 1} N to represent an infinite sequence of characters. (i.e.) We commit ourselves to the misuse of notation and usage xn () to represent xn (1)."}, {"heading": "5 Uniform Convexity and Martingale Type", "text": "The classical concept of the Martingale type plays a central role in the study of the geometry of q spaces. In [16] it was shown that a Banach space has the Martingale type p (the classical concept) if and only if there are uniformly convex functions with certain properties in this space (w.r.t. the norm of this Banach space).In this section we expand this result and show how the Martingale type of a pair (W?, X) is related to the existence of certain uniformly convex functions. In particular, the following sentence shows that the concept of the Martingale type of a pair (W?, X) is equivalent to the existence of a non-negative function, which is uniformly convex w.r.t. the norm converts X? onW. Lemma 7. If there is a constant p for any p (1, 2), so that there is a constant C > 0 p, so that for all consequences of the conversion p."}, {"heading": "6 Optimality of Mirror Descent", "text": "In Section 3, we saw that if we can find an appropriate uniform convex function that we use in the mirror descent algorithm, we can guarantee a reduction in regret, but the open question was when can we find such a function and what is the rate that we can calculate for the existence of certain universal convex functions. We can now combine these results to show that the mirror descent algorithm is a universal online learning algorithm for convex learning problems. Specifically, we show that whenever a problem is learnable online, the mirror descent algorithm can guarantee a near optimal rate: Theorem 9. If we can find Vn (W, X) and V \u2212 1 q for some constant V > 0 and some q2), Vn (W, X) and V \u2212 1 q for all."}, {"heading": "7 Examples", "text": "We show that we have to get involved in several online learning problems (see also: \"P-Dual Couples\"), to which we engage in literature, in order to consider the case in which the question is concerned, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question is a question, whether it is a question, whether it is a question, whether it is a question is a question, whether it is a question, whether it is a question, whether it is a question"}, {"heading": "8 Conclusion and Discussion", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}], "references": [{"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["J. Abernethy", "P.L. Bartlett", "A. Rakhlin", "A. Tewari"], "venue": "In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Sharp uniform convexity and smoothness inequalities for trace norms", "author": ["Keith Ball", "Eric A. Carlen", "Elliott H. Lieb"], "venue": "Invent. Math.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Operations Research Letters,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "The perceptron: A model for brain functioning", "author": ["H.D. Block"], "venue": "Reviews of Modern Physics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1962}, {"title": "Sparse and low-rank matrix decompositions", "author": ["V. Chandrasekaran", "S. Sanghavi", "P. Parrilo", "A. Willsky"], "venue": "In IFAC Symposium on System Identification,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "A Dirty Model for Multi-task Learning", "author": ["Ali Jalali", "Pradeep Ravikumar", "Sujay Sanghavi", "Chao Ruan"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Stochastic approximation approach to stochastic programming", "author": ["A. Juditsky", "G. Lan", "A. Nemirovski", "A. Shapiro"], "venue": "SIAM J. Optim,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization", "author": ["Sham M. Kakade", "Shai Shalev-shwartz", "Ambuj Tewari"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["N. Littlestone"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "On cesaro\u2019s convergence of the gradient descent method for finding saddle points of convex-concave functions", "author": ["A. Nemirovski", "D. Yudin"], "venue": "Doklady Akademii Nauk SSSR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1978}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A. Nemirovski", "D. Yudin"], "venue": "Nauka Publishers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1978}, {"title": "Martingales with values in uniformly convex spaces. Israel", "author": ["G. Pisier"], "venue": "Journal of Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1975}, {"title": "Martingales in banach spaces (in connection with type and cotype)", "author": ["G. Pisier"], "venue": "Winter School/IHP Graduate Course,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Online learning: Random averages, combinatorial parameters, and learnability", "author": ["A. Rakhlin", "K. Sridharan", "A. Tewari"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Convex repeated games and fenchel duality", "author": ["S. Shalev-Shwartz", "Y. Singer"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Maximum-margin matrix factorization", "author": ["Nathan Srebro", "Jason D.M. Rennie", "Tommi S. Jaakola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Rank, trace-norm and max-norm", "author": ["Nathan Srebro", "Adi Shraibman"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Stochastic optimization for machine learning", "author": ["Nathan Srebro", "Ambuj Tewari"], "venue": "In ICML 2010, tutorial,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Convex games in Banach spaces", "author": ["K. Sridharan", "A. Tewari"], "venue": "In Proceedings of the 23nd Annual Conference on Learning Theory,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Randomized online pca algorithms with regret bounds that are logarithmic in the dimension", "author": ["Manfred K. Warmuth", "Dima Kuzmin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Mirror Descent is a first-order optimization procedure which generalizes the classic Gradient Descent procedure to non-Euclidean geometries by relying on a \u201cdistance generating function\u201d specific to the geometry (the squared `2norm in the case of standard Gradient Descent) [14, 4].", "startOffset": 289, "endOffset": 296}, {"referenceID": 2, "context": "1 Introduction Mirror Descent is a first-order optimization procedure which generalizes the classic Gradient Descent procedure to non-Euclidean geometries by relying on a \u201cdistance generating function\u201d specific to the geometry (the squared `2norm in the case of standard Gradient Descent) [14, 4].", "startOffset": 289, "endOffset": 296}, {"referenceID": 7, "context": "Mirror Descent is also applicable, and has been analyzed, in a stochastic optimization setting [9] and in an online setting, where it can ensure bounded online regret [20].", "startOffset": 95, "endOffset": 98}, {"referenceID": 18, "context": "Mirror Descent is also applicable, and has been analyzed, in a stochastic optimization setting [9] and in an online setting, where it can ensure bounded online regret [20].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "the Perceptron algorithm [5] and Online Gradient Descent [27]), or in the simplex (`1 geometry), using an entropic distance generating function (Winnow [13] and Multiplicative Weights / Online Exponentiated Gradient algorithm [11]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 24, "context": "the Perceptron algorithm [5] and Online Gradient Descent [27]), or in the simplex (`1 geometry), using an entropic distance generating function (Winnow [13] and Multiplicative Weights / Online Exponentiated Gradient algorithm [11]).", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "the Perceptron algorithm [5] and Online Gradient Descent [27]), or in the simplex (`1 geometry), using an entropic distance generating function (Winnow [13] and Multiplicative Weights / Online Exponentiated Gradient algorithm [11]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 9, "context": "the Perceptron algorithm [5] and Online Gradient Descent [27]), or in the simplex (`1 geometry), using an entropic distance generating function (Winnow [13] and Multiplicative Weights / Online Exponentiated Gradient algorithm [11]).", "startOffset": 226, "endOffset": 230}, {"referenceID": 8, "context": "More recently, the Online Mirror Descent framework has been applied, with appropriate distance generating functions derived for a variety of new learning problems like multi-task learning and other matrix learning problems [10], online PCA [26] etc.", "startOffset": 223, "endOffset": 227}, {"referenceID": 23, "context": "More recently, the Online Mirror Descent framework has been applied, with appropriate distance generating functions derived for a variety of new learning problems like multi-task learning and other matrix learning problems [10], online PCA [26] etc.", "startOffset": 240, "endOffset": 244}, {"referenceID": 22, "context": "We then extend the notion of a martingale type of a Banach space to be sensitive to both the constraint set and the data domain, and building on results of [24], we relate the value of the online learning repeated game to this generalized notion of martingale type (Section 4).", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "Finally, again building on and generalizing the work of [16], we show how having appropriate martingale type guarantees the existence of a good uniformly convex function (Section 5), that in turn establishes the desired nearly-optimal guarantee on Online Mirror Descent (Section 6).", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "We mainly build on the analysis of [24], who related the value of the online game to the notion of martingale type of a Banach space and uniform convexity when the constraint set and data domain are dual to each other.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "Mirror Descent was initially introduced as a first order deterministic optimization procedure, with an `p constraint and a matching `q Lipschitz assumption (1 \u2264 p \u2264 2, 1/q + 1/p = 1), was shown to be optimal in terms of the number of exact gradient evaluations [15].", "startOffset": 261, "endOffset": 265}, {"referenceID": 22, "context": "Sridharan and Tewri [24] generalized the optimality of online Mirror Descent (w.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "The second equality is shown in [18].", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "Further, for any p \u2208 [1, 2] we us also define : Vp := inf { V \u2223\u2223\u2223 \u2200n \u2208 N,Vn(W,X ) \u2264 V n\u2212(1\u2212 1 p )} (3) Most prior work on online learning and optimization considers the case whenW is the unit ball of some Banach space, and X is the unit ball of the dual space, i.", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "\u2016 \u00b7 \u2016 inW \u2282 B: \u2200w,w\u2032\u2208W\u2200\u03b1\u2208[0,1] \u03a8 (\u03b1w + (1\u2212 \u03b1)w\u2032) \u2264 \u03b1\u03a8(w) + (1\u2212 \u03b1)\u03a8(w\u2032)\u2212 \u03b1(1\u2212\u03b1) q \u2016w \u2212w \u2032\u2016 It is important to emphasize that in the definition above, the norm \u2016.", "startOffset": 25, "endOffset": 30}, {"referenceID": 0, "context": "Similarly to the value of the game, for any p \u2208 [1, 2], we define:", "startOffset": 48, "endOffset": 54}, {"referenceID": 22, "context": "4 Martingale Type and Value In [24], it was shown that the concept of the Martingale type (also sometimes called the Haar type) of a Banach space and optimal rates for online convex optimization problem, where X andW are duals of each other, are closely related.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "In this section we extend the classic notion of Martingale type of a Banach space (see for instance [16]) to one that accounts for the pair (W,X ).", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "We point the reader to [16, 17] for more details.", "startOffset": 23, "endOffset": 31}, {"referenceID": 15, "context": "We point the reader to [16, 17] for more details.", "startOffset": 23, "endOffset": 31}, {"referenceID": 0, "context": "Further, for any p \u2208 [1, 2] we also define,", "startOffset": 21, "endOffset": 27}, {"referenceID": 22, "context": "The results of [24, 18] showing that a Martingale type implies low regret, actually apply also for \u201cnon-matching\u201d W and X and, in our notation, imply that Vp \u2264 2Cp.", "startOffset": 15, "endOffset": 23}, {"referenceID": 16, "context": "The results of [24, 18] showing that a Martingale type implies low regret, actually apply also for \u201cnon-matching\u201d W and X and, in our notation, imply that Vp \u2264 2Cp.", "startOffset": 15, "endOffset": 23}, {"referenceID": 22, "context": "Specifically we have the following theorem from [24, 18] : Theorem 4.", "startOffset": 48, "endOffset": 56}, {"referenceID": 16, "context": "Specifically we have the following theorem from [24, 18] : Theorem 4.", "startOffset": 48, "endOffset": 56}, {"referenceID": 22, "context": "[24, 18] For anyW \u2208 B and any X \u2208 B and any n \u2265 1,", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[24, 18] For anyW \u2208 B and any X \u2208 B and any n \u2265 1,", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "We then extend the result of Pisier in [16] to the \u201cnon-matching\u201d setting combining it with the above theorem to finally get : Lemma 5.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "For any p \u2208 [1, 2] and any p\u2032 < p : Cp\u2032 \u2264 1104 Vp (p\u2212p\u2032)2", "startOffset": 12, "endOffset": 18}, {"referenceID": 14, "context": "In [16], it was shown that a Banach space has Martingale type p (the classical notion) if and only if uniformly convex functions with certain properties exist on that space (w.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "For any p \u2208 [1, 2], Dp \u2264 Cp.", "startOffset": 12, "endOffset": 18}, {"referenceID": 15, "context": "First we note that though the function \u03a8q in the construction (9) need not be such that (q\u03a8q(w)) is a norm, with a simple modification as noted in [17] we can make it a norm.", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "p\u2032 < p, Cp\u2032 \u2264 Vp \u2264 MDp \u2264 Dp \u2264 Cp Lemma 5 (extending Pisier\u2019s result [16]) Definition of Vp (Generalized MD guarantee) Lemma 2 Construction of \u03a8, Lemma 11 (extending Pisier\u2019s result [16])", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "p\u2032 < p, Cp\u2032 \u2264 Vp \u2264 MDp \u2264 Dp \u2264 Cp Lemma 5 (extending Pisier\u2019s result [16]) Definition of Vp (Generalized MD guarantee) Lemma 2 Construction of \u03a8, Lemma 11 (extending Pisier\u2019s result [16])", "startOffset": 181, "endOffset": 185}, {"referenceID": 1, "context": "Ball et al [3] tightly calculate the constants of strong convexity of squared `p norms, establishing the tightness of D2 when p1 = p2.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "These results again follow using similar arguments as `p case and tight constants for strong convexity parameters of the Schatten norm from [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 19, "context": "Max Norm Max-norm has been proposed as a convex matrix regularizer for application such as matrix completion [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "As noted in [22] the max-norm ball is equivalent, up to a factor two, to the convex hull of all rank one sign matrices.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "This matches the stochastic (PAC) learning guarantee [22], and is the first guarantee we are aware of for the max norm matrix completion problem in the online setting.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "al [6] (here one can use the interpolation norm of second type to interpolate between trace norm and element wise `1 norm).", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "al [8].", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Similarly for the [8] case X could be either matrices with bounded entries or some other natural assumption that suits the problem.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "Furthermore, we might also find other properties of w desirable, such as sparsity, which would bias us toward alternative methods [12, 7].", "startOffset": 130, "endOffset": 137}, {"referenceID": 5, "context": "Furthermore, we might also find other properties of w desirable, such as sparsity, which would bias us toward alternative methods [12, 7].", "startOffset": 130, "endOffset": 137}, {"referenceID": 16, "context": "Furthermore, we know that in terms of worst-case behavior, both in the stochastic and in the online setting, for convex cost functions, the value is unchained when the convex hull of a non-convex constraint set [18].", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "In the total variation regularization problem, W is the set of all functions on the interval [0, 1] with total variation bounded by 1 which is in fact a Banach space.", "startOffset": 93, "endOffset": 99}, {"referenceID": 0, "context": "It only consists of evaluations of the functions inW at points on interval [0, 1] and one can consider a supervised learning problem where the goal is to use the set of all functions with bounded variations to predict targets which take on values in [\u22121, 1] .", "startOffset": 75, "endOffset": 81}, {"referenceID": 7, "context": "We would like to consider the question of whether Mirror Descent is optimal for stochastic convex optimization, or equivalently convex statistical learning, setting [9, 19, 23] in general.", "startOffset": 165, "endOffset": 176}, {"referenceID": 17, "context": "We would like to consider the question of whether Mirror Descent is optimal for stochastic convex optimization, or equivalently convex statistical learning, setting [9, 19, 23] in general.", "startOffset": 165, "endOffset": 176}, {"referenceID": 21, "context": "We would like to consider the question of whether Mirror Descent is optimal for stochastic convex optimization, or equivalently convex statistical learning, setting [9, 19, 23] in general.", "startOffset": 165, "endOffset": 176}, {"referenceID": 0, "context": "References [1] J.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[3] Keith Ball, Eric A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] H.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] V.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Ali Jalali, Pradeep Ravikumar, Sujay Sanghavi, and Chao Ruan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Sham M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Nathan Srebro, Jason D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Nathan Srebro and Adi Shraibman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Nathan Srebro and Ambuj Tewari.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] Manfred K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "They use similar techniques as in [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "We restate below a proposition from Pisier\u2019s note (in [17]) Proposition 16 (Proposition 8.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "53 of [17]).", "startOffset": 6, "endOffset": 10}], "year": 2011, "abstractText": "We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee.", "creator": "LaTeX with hyperref package"}}}