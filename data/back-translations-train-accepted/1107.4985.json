{"id": "1107.4985", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2011", "title": "Variational Gaussian Process Dynamical Systems", "abstract": "High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.", "histories": [["v1", "Mon, 25 Jul 2011 15:54:05 GMT  (7958kb,AD)", "http://arxiv.org/abs/1107.4985v1", "16 pages, 19 figures"]], "COMMENTS": "16 pages, 19 figures", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.CV math.PR", "authors": ["andreas c damianou", "michalis k titsias", "neil d lawrence"], "accepted": true, "id": "1107.4985"}, "pdf": {"name": "1107.4985.pdf", "metadata": {"source": "CRF", "title": "Variational Gaussian Process Dynamical Systems", "authors": ["Andreas Damianou", "Michalis K. Titsias"], "emails": ["andreas.damianou@sheffield.ac.uk", "mtitsias@cs.man.ac.uk", "neil@dcs.shef.ac.uk"], "sections": [{"heading": null, "text": "In this paper we present the variational Gaussian process dynamic system. Our work builds on recent variation approximations of latent variable models for Gaussian processes to enable a nonlinear dimensionality reduction while simultaneously learning a dynamic pre-stage in latent space. It also allows the automatic determination of the appropriate dimensionality of latent space. We demonstrate the model using a dataset for human motion sensing and a series of high-resolution video sequences."}, {"heading": "1 Introduction", "text": "A standard approach is the simultaneous application of a nonlinear dimension reduction to the data, while latent space is governed by a nonlinear temporal history. The main difficulty for such approaches is that the analytical marginalization of latent space is typically insoluble. Markov chain Monte Carlo approaches can also be problematic, since latent trajectories are strongly correlated, requiring efficient sampling. A promising approach for these time series has been the extension of the latent variable model of the Gaussian process [1, 2] with a dynamic advance for latent space and the search for a maximum a posteriori solution for the latent points [3, 4, 5]. Ko and Fox [6] extend these models for fully Bayesian filtering in a robotic environment. We refer to this class of dynamic models based on the GP-LVM as a Gaussian process of dynamic systems."}, {"heading": "2 The Model", "text": "We assume that a set of data we have provided represents only a very limited layer in the generation of the data, and that the Dth function then depends on the generation of the data, which is then produced by xn = x (tn). (xn) Depending on how the data is generated, the Dth function is then derived from xn = x (tn). (xn) Depending on how the data is generated, we will map the data from xn = x (tn) to xn (xn). (1) Where fd (x) is a latent mapping of the low dimensions of the observation space and \u03b2 is the inverse variance of white noise. We do not want to make strong assumptions about the functionality of the functions we assume from the low dimensions of the observation space. (1) We assume that we assume the inverse variance of white noise."}, {"heading": "2.1 Variational Bayesian training", "text": "The key difficulties with the Bayesian approach are problematic because the previous density p (X) q (F) q (X) q (X) q (X) c (X) c (X) c (X) c (X) c (X) c (X) c (X) c (X) c (X) c (X) c (X) c (X) c (c) c (X) c (X) c (c) c (X) c (X) c (X) c (X) c (X) c (X) c (X) c (c) c (X) c (c) c (X) c (X) c (c) c (X) c (X) c (c) c (c) c (c) c (X) c c (c) c c (c) c (X) c c c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c (c) c (c) c (c) c (c (c) c (c) c (X) c (c (c) c (c) c (c) c (c (c) c (c) c (c) c (c (X) c (c) c (c (c) c (c (c) c (X) c (c (c (c) c (c (c) c (c) c (c) c (X) c (c (c (c) c) c (X) c (c (c (c (c) c) c (c (X) c (c (c) c (X) c c c (c c c c c c (c) c (X) c c c c (X) c c c c c c c (X) c c c c (X) c c c c (X) c c c c (X) c c c c (X) c c (X (X) c c c c) c (X) c) c (X (X) c) c (X (X (X) c c c c) c) c (X X"}, {"heading": "2.2 Reparametrization and Optimization", "text": "The optimization includes the model parameters \u03b8 = (\u03b2, \u03b8f, \u03b8t), the variation parameters {\u00b5q, Sq} Qq = 1 of q (X) and the inducing points3 X.The optimization of the variation parameters appears challenging due to their large number and the correlations between them. However, by repairing our variation parameters O (N2) according to the framework described in [12], we can obtain a series of O (N) less correlated variation parameters. Specifically, we first take the derivatives of the limit of variation (14) w.r.t. Sq and \u00b5q and set them to zero to find the stationary points Sq = (K \u2212 1t + qqq) \u2212 1 and \u00b5q = Kt\u00b5 \u00b2 q, (15) first taking the derivatives of the limit of variation (14) w.r.t. Sq = \u2212 2\u0445 v (q) and \u00b5q = amorphous parameters, Sq = (\u2212 1q + 1qqt) and can generate the original dimension \u2212 \u00b5q."}, {"heading": "2.3 Learning from Multiple Sequences", "text": "Our goal is to model multivariate time series. A given dataset may consist of a group of independently observed sequences of varying lengths (e.g., when capturing human motion data several steps away from a subject).For example, let the dataset be a group of independent S sequences (Y (1),..., Y (S)).We would like our model to capture the underlying commonality of these data by allowing a different latent function for each of the independent sequences, so that X (s) is the set of latent variables corresponding to the sequence s. These sets are a priori assumed to be independent because they correspond to separate sequences, e.g. p (X (1), X (2),..., X (S) = 1 p (X (s)))))), abandoning conditioning for simplicity, since this factorization results in a block diagonal structure for the time coance Kt."}, {"heading": "3 Predictions", "text": "Our algorithm models the temporal evolution of a dynamic system. It should be able to generate completely new sequences or to reconstruct missing observations from partially observed data. To generate novel sequence data, the model needs a time vector t * * as input and calculates a density p (Y * | Y, t, t *) from it. To reconstruct partially observed data, the timestamp information is additionally accompanied by a partially observed sequence Y p * x Dp from the entire Y * = (Y p *, Y m *), using p and m indices indicating the presence (i.e. observed) or missing dimensions of Y *, so that p \u00b2 m = {1.,.,., D} can be reconstructed. We reconstruct the missing dimensions by calculating the Bayean prediction distribution p (Y m \u00b2 | Y p \u00b2, Y *, T *, T). Predictable densities can also be used as estimates for the next generation of Bayesian time, as we always do during the classification."}, {"heading": "3.1 Predictions Given Only the Test Time Points", "text": "In order to approximate the expected density, we must use the underlying latent functional values F * q = q q = q q q q (the noise-free version of Y *) and the latent variables X * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "3.2 Predictions Given the Test Time Points and Partially Observed Outputs", "text": "The expression for the predictive poems p (Y m * | Y p *, Y) is similar to (16), p (Y m * | Y p *, Y) = p (Y m * | Fm *) p (Fm * | X *, Y *, Y) p (X * | Y p *, Y) dFm * dX *, (24) and is analytically insoluble. To get an approximation, we must first apply the variational inference and the approximate p (X * | Y p *, Y) with a Gaussian distribution. This requires the optimization of a new variable lower limit that takes into account the contribution of the partially observed data Y p *. This lower limit approaches the true limit probability p (Y p *, Y) and has exactly analogous form with the lower limit Y. Furthermore, the variable optimization requires the definition of the variable distribution q (X *, X *), which is inaccurate vis-\u00e0-vis the X (incorrect) and (X) * (p)."}, {"heading": "4 Handling Very High Dimensional Datasets", "text": "Our frame of variation avoids the typical cubic complexity of Gaussian processes, which allow relatively large training sets (thousands of time points, N). In addition, the model scales only linearly with the number of dimensions D. Specifically, the number of dimensions only plays a role when calculating with the data matrix Y. In the final form of the lower limit (and thus in all derived quantities, such as gradients), this matrix appears only in the form Y >, which can be pre-calculated, which means that in N D Y > we can calculate only once and then replace Y with the SVD (or cholesky decomposition) of Y Y >. In this way, we can work with a N \u00d7 N instead of a N \u00b7 D matrix. Practically, this allows us to work with data sets containing millions of characteristics. In our experiments, we directly model the pixels of videos in HD quality and use this trick."}, {"heading": "5 Experiments", "text": "We are looking at two different types of high-dimensional time series, a dataset for recording human movements, consisting of different walks and high-resolution video sequences. The experiments are designed to explore the different properties of the model and to evaluate its performance in different tasks (prediction, reconstruction, generation of data).The Matlab source code for repeating the following experiments is available online at http: / / staffwww.dcs.shef.ac.uk / people / N.Lawrence / vargplvm /."}, {"heading": "5.1 Human Motion Capture Data", "text": "We followed [14, 15] in taking into account movement data from walks and runs taken from subjects 35 in the CMU Motion Capture Database. We treated each movement as an independent sequence, constructing and pre-processing the data set as described in [15], resulting in 2,613 separate 59-dimensional frames, divided into 31 training sequences with an average length of 84 frames each. As explained in Section 2.3, the model is trained jointly on both runs, i.e. the algorithm learns a common latent space for these movements. At the test date, we are investigating the model's ability to reconstruct test data from a previously unseen sequence containing partial information for the test objectives. This is tested once by providing only those dimensions that correspond to the subject's body, and once by providing those that correspond to the legs. We are comparing the results in [15], which use MAP approximations for the dimensions of the next-day versus the dimensions."}, {"heading": "5.2 Modeling Raw High Dimensional Video Sequences", "text": "In fact, it is a pure fabric, capable of changing and transforming the world."}, {"heading": "6 Discussion and Future Work", "text": "We have introduced a fully Bayesian approach to modeling dynamic systems by probable nonlinear dimensionality reduction. Marginalizing latent space and reconstructing data using Gaussian processes leads to a very general model for capturing complex nonlinear correlations even in very high-dimensional data, without having to perform data preprocessing or exhaustive search for the definition of the structure and parameters of the model.The effectiveness of our method has been demonstrated in two tasks: first, in modeling human motion capture data, and second, in reconstructing and generating raw, very high-dimensional video sequences.A promising future direction would be to improve our formulation with domain-specific knowledge encoded, for example, in more complex covariance functions, or in the way that data is pre-processed. Thus, we can obtain application-oriented methods used for tasks in areas such as robotics, computer vision, and finance."}, {"heading": "Acknowledgments", "text": "The research was supported in part by the University of Sheffield Moody Foundation and the Greek State Scholarships Foundation (IKY) and we thank Colin Litster and Fit Fur Life for allowing us to use their video files as data sets."}, {"heading": "A Derivation of the variational bound", "text": "We want to approximate the marginal probability: p (Y) = q (Q) = q (Q) = q (Q) (Q) = q (Q) (Q) (Q) (Q) = q (Q) = Q (Q) (Q) = q (Q) = q (Q) (Q) = Q (Q) (D) (Q) = Q (A) = Q (A) = Q (Q) = Q (Q) = Q (A) = Q (Q) = Q (A) = Q (Q) (A) = Q (Q) = Q (Q) = Q (Q) = Q (Q) = Q (Q) = Q (Q) = Q) = Q (A) = Q (Q) = Q) = Q (A) = Q (Q) = B = B (Q) (Q) (Q) (Q) = B) (Q) (B) (Q) (Q) = D) (Q) = Q (Q) = B (B) (Q) (Q) = Q) = Q (Q) = Q (B)."}, {"heading": "B Derivatives of the variational bound", "text": "Before naming the expressions for the derivatives of the boundary of variation (30), it should be remembered that the variation parameters (1) and Sq (for all qs) have been converted into a square diagonal matrix and vice versa. In view of the above parameters, the sentence of the parameters to be optimized (1), (2), (2), (3), (3), (4), (4), (4), (4), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, 5, 5, (5), (5), (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 (5), 5, (5), 5, (5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 (5), 5 (5), 5, (5, 5, 5, 5, 5, 5, 5, 5 (5), (5, 5), 5, 5 (5, 5, 5, 5, 5, 5, 5), 5 (5, 5, 5, (5, 5, 5, 5, 5), 5 (5, 5), 5 (5, 5, 5 (5), (5, 5), (5), (5, (5), 5, (5), (5), 5, (5, 5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), 5, (5), (5), (5,"}, {"heading": "C Predictions", "text": "In order to approximate the prospective density, we must insert the underlying latent function values (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q). (Q = Q = Q = Q = K). (Z = K = K). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z. (Z). (Z). (Z). (Z. (Z). (Z. (Z). (Z). (Z. (Z).). (Z. (Z. (Z). (Z. (Z). (Z). (Z. (Z. (Z). (Z.). (Z). (Z. (Z). (Z). (Z. (Z). (Z). (Z). (Z. (Z). (Z). (Z). (Z). (Z. (Z). (Z. (Z. (Z. (Z. (Z). (Z). (Z). (Z). (Z). (Z).). (Z."}], "references": [{"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["N.D. Lawrence"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 1783\u20131816, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1816}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "In NIPS, p. 2004, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Gaussian process dynamical models", "author": ["J.M. Wang", "D.J. Fleet", "A. Hertzmann"], "venue": "In NIPS, pp. 1441\u2013 1448, MIT Press, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Gaussian process dynamical models for human motion", "author": ["J.M. Wang", "D.J. Fleet", "A. Hertzmann"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, pp. 283\u2013298, Feb. 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Hierarchical Gaussian process latent variable models", "author": ["N.D. Lawrence"], "venue": "In International Conference in Machine Learning, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models", "author": ["J. Ko", "D. Fox"], "venue": "Auton. Robots, vol. 27, pp. 75\u201390, July 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["M. Titsias"], "venue": "JMLR W&CP, vol. 5, pp. 567\u2013574, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian Gaussian process latent variable model", "author": ["M. Titsias", "N.D. Lawrence"], "venue": "Journal of Machine Learning Research - Proceedings Track, vol. 9, pp. 844\u2013851, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Introduction to Gaussian processes", "author": ["D.J.C. MacKay"], "venue": "Neural Networks and Machine Learning (C. M. Bishop, ed.), NATO ASI Series, pp. 133\u2013166, Kluwer Academic Press, 1998.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Pattern Recognition and Machine Learning (Information", "author": ["C.M. Bishop"], "venue": "Science and Statistics). Springer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "The variational Gaussian approximation revisited", "author": ["M. Opper", "C. Archambeau"], "venue": "Neural Computation, vol. 21, no. 3, pp. 786\u2013792, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Gaussian process priors with uncertain inputs - application to multiple-step ahead time series forecasting", "author": ["A. Girard", "C.E. Rasmussen", "J. Qui\u00f1onero-Candela", "R. Murray-Smith"], "venue": "Neural Information Processing Systems, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling human motion using binary latent variables", "author": ["G.W. Taylor", "G.E. Hinton", "S. Roweis"], "venue": "Advances in Neural Information Processing Systems, p. 2007, MIT Press, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 106, "endOffset": 112}, {"referenceID": 2, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 226, "endOffset": 235}, {"referenceID": 3, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 226, "endOffset": 235}, {"referenceID": 4, "context": "One promising approach to these time series has been to extend the Gaussian process latent variable model [1, 2] with a dynamical prior for the latent space and seek a maximum a posteriori (MAP) solution for the latent points [3, 4, 5].", "startOffset": 226, "endOffset": 235}, {"referenceID": 5, "context": "Ko and Fox [6] further extend these models for fully Bayesian filtering in a robotics setting.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In this paper we build on recent developments in variational approximations for Gaussian processes [7, 8] to introduce a variational Gaussian process dynamical system (VGPDS) where latent variables are approximately marginalized through optimization of a rigorous lower bound on the marginal likelihood.", "startOffset": 99, "endOffset": 105}, {"referenceID": 7, "context": "In this paper we build on recent developments in variational approximations for Gaussian processes [7, 8] to introduce a variational Gaussian process dynamical system (VGPDS) where latent variables are approximately marginalized through optimization of a rigorous lower bound on the marginal likelihood.", "startOffset": 99, "endOffset": 105}, {"referenceID": 8, "context": "1 Instead we would like to infer them in a fully Bayesian non-parametric fashion using Gaussian processes [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "In our experiments, we will focus on the squared exponential covariance function (RBF), the Matern 3/2 which is only once differentiable, and a periodic covariance function [9, 10] which can be used when data exhibit strong periodicity.", "startOffset": 173, "endOffset": 180}, {"referenceID": 9, "context": "In our experiments, we will focus on the squared exponential covariance function (RBF), the Matern 3/2 which is only once differentiable, and a periodic covariance function [9, 10] which can be used when data exhibit strong periodicity.", "startOffset": 173, "endOffset": 180}, {"referenceID": 7, "context": "This, as in the variational Bayesian formulation of the GP-LVM [8], enables an automatic relevance determination procedure (ARD), i.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "[5, 3]) marginalise out only F and seek a MAP solution for X .", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[5, 3]) marginalise out only F and seek a MAP solution for X .", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "In the next section we describe how efficient variational approximations can be applied to marginalize X by extending the framework of [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": "Following a standard procedure [11], we introduce a variational distribution q(\u0398) and compute the Jensen\u2019s lower bound Fv on the logarithm of (9),", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "As shown in [8], this intractability is removed by applying the \u201cdata augmentation\u201d principle.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "By dropping X\u0303 from our expressions, we write the augmented GP prior analytically (see [9]) as p(fd|ud, X) = N ( fd|KNMK MMud,KNN \u2212KNMK \u22121 MMKMN ) .", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "(12) A key result in [8] is that a tractable lower bound (computed analogously to (10)) can be obtained through the variational density", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Titsias and Lawrence [8] assume full independence for q(X) and the variational covariances are diagonal matrices.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Therefore, the first term in (14) has the same analytical solution as the one derived in [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 11, "context": "However, by reparametrizing our O ( N ) variational parameters according to the framework described in [12] we can obtain a set of O(N) less correlated variational parameters.", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "where p(x\u2217,q|xq) is a Gaussian found from the conditional GP prior (see [9]) and q(X) is also Gaussian.", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "However, following the same argument as in [9, 13], we can calculate analytically its mean and covariance: E(F\u2217) = B\u03a81 (22) Cov(F\u2217) = B> ( \u03a82 \u2212\u03a81(\u03a81) ) B + \u03a80I \u2212 Tr [( K\u22121 MM \u2212 (KMM + \u03b2\u03a82) \u22121 ) \u03a82 ] I, (23)", "startOffset": 43, "endOffset": 50}, {"referenceID": 12, "context": "However, following the same argument as in [9, 13], we can calculate analytically its mean and covariance: E(F\u2217) = B\u03a81 (22) Cov(F\u2217) = B> ( \u03a82 \u2212\u03a81(\u03a81) ) B + \u03a80I \u2212 Tr [( K\u22121 MM \u2212 (KMM + \u03b2\u03a82) \u22121 ) \u03a82 ] I, (23)", "startOffset": 43, "endOffset": 50}, {"referenceID": 13, "context": "1 Human Motion Capture Data We followed [14, 15] in considering motion capture data of walks and runs taken from subject 35 in the CMU motion capture database.", "startOffset": 40, "endOffset": 48}, {"referenceID": 13, "context": "We can also indirectly compare with the binary latent variable model (BLV) of [14] which used a slightly different data preprocessing.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "We assess the performance using the cumulative error per joint in the scaled space defined in [14] and by the root mean square error in the angle space suggested by [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "CL / CB are the leg and body datasets as preprocessed in [14], L and B the corresponding datasets from [15].", "startOffset": 57, "endOffset": 61}], "year": 2011, "abstractText": "High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.", "creator": "TeX"}}}