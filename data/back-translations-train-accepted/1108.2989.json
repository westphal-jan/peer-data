{"id": "1108.2989", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2011", "title": "A Theory of Multiclass Boosting", "abstract": "Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the \"correct\" requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.", "histories": [["v1", "Mon, 15 Aug 2011 13:26:26 GMT  (2070kb,D)", "http://arxiv.org/abs/1108.2989v1", "A preliminary version appeared in NIPS 2010"]], "COMMENTS": "A preliminary version appeared in NIPS 2010", "reviews": [], "SUBJECTS": "stat.ML cs.AI", "authors": ["indraneel mukherjee", "robert e schapire"], "accepted": true, "id": "1108.2989"}, "pdf": {"name": "1108.2989.pdf", "metadata": {"source": "CRF", "title": "A Theory of Multiclass Boosting", "authors": ["Indraneel Mukherjee", "Robert E. Schapire"], "emails": ["imukherj@cs.princeton.edu", "schapire@cs.princeton.edu"], "sections": [{"heading": null, "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}], "references": [{"title": "Optimal stragies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["Erin L. Allwein", "Robert E. Schapire", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2000}, {"title": "AdaBoost is consistent", "author": ["Peter L. Bartlett", "Mikhail Traskin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Traskin.,? \\Q2007\\E", "shortCiteRegEx": "Bartlett and Traskin.", "year": 2007}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Error-correcting tournaments", "author": ["Alina Beygelzimer", "John Langford", "Pradeep Ravikumar"], "venue": "In Algorithmic Learning Theory: 20th International Conference,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Solving multiclass learning problems via errorcorrecting output codes", "author": ["Thomas G. Dietterich", "Ghulum Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Multiclass boosting for weak classifiers", "author": ["G\u00fcnther Eibl", "Karl-Peter Pfeiffer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Eibl and Pfeiffer.,? \\Q2005\\E", "shortCiteRegEx": "Eibl and Pfeiffer.", "year": 2005}, {"title": "An adaptive version of the boost by majority algorithm", "author": ["Yoav Freund"], "venue": "Machine Learning,", "citeRegEx": "Freund.,? \\Q2001\\E", "shortCiteRegEx": "Freund.", "year": 2001}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Yoav Freund"], "venue": "Information and Computation,", "citeRegEx": "Freund.,? \\Q1995\\E", "shortCiteRegEx": "Freund.", "year": 1995}, {"title": "Continuous drifting games", "author": ["Yoav Freund", "Manfred Opper"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Opper.,? \\Q2002\\E", "shortCiteRegEx": "Freund and Opper.", "year": 2002}, {"title": "Experiments with a new boosting algorithm", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "In Machine Learning: Proceedings of the Thirteenth International Conference,", "citeRegEx": "Freund and Schapire.,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1996}, {"title": "Game theory, on-line prediction and boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "In Proceedings of the Ninth Annual Conference on Computational Learning Theory,", "citeRegEx": "Freund and Schapire.,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1996}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Classification by pairwise coupling", "author": ["Trevor Hastie", "Robert Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Hastie and Tibshirani.,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Tibshirani.", "year": 1998}, {"title": "The rate of convergence", "author": ["Indraneel Mukherjee", "Cynthia Rudin", "Robert E. Schapire"], "venue": "Theoretical Computer Science,", "citeRegEx": "Mukherjee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2010}, {"title": "The strength of weak learnability", "author": ["Robert E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Schapire.,? \\Q2005\\E", "shortCiteRegEx": "Schapire.", "year": 2005}, {"title": "Improved boosting algorithms using confidence-rated", "author": ["Robert E. Schapire", "Yoram Singer"], "venue": "categorization. Machine Learning,", "citeRegEx": "Schapire and Singer.,? \\Q2000\\E", "shortCiteRegEx": "Schapire and Singer.", "year": 2000}, {"title": "Boosting the margin", "author": ["Robert E. Schapire", "Yoav Freund", "Peter Bartlett", "Wee Sun Lee"], "venue": "predictions. Machine Learning,", "citeRegEx": "Schapire et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1999}, {"title": "On the Consistency of Multiclass Classification", "author": ["Ambuj Tewari", "Peter L. Bartlett"], "venue": null, "citeRegEx": "Tewari and Bartlett.,? \\Q1998\\E", "shortCiteRegEx": "Tewari and Bartlett.", "year": 1998}, {"title": "Statistical behavior and consistency of classification methods based on convex", "author": ["Tong Zhang"], "venue": "ods. Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Zhang.", "year": 2007}, {"title": "Multi-class AdaBoost", "author": ["Zhu", "Hui Zou", "Saharon Rosset", "Trevor Hastie"], "venue": "Annals of Statistics,", "citeRegEx": "Zhu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 8, "context": "Specifically, it is known that the Boost-bymajority (Freund, 1995) algorithm is optimal in a certain sense, and that AdaBoost (Freund and Schapire, 1997) is a practical approximation.", "startOffset": 52, "endOffset": 66}, {"referenceID": 12, "context": "Specifically, it is known that the Boost-bymajority (Freund, 1995) algorithm is optimal in a certain sense, and that AdaBoost (Freund and Schapire, 1997) is a practical approximation.", "startOffset": 126, "endOffset": 153}, {"referenceID": 1, "context": "The most common approaches so far have relied on reductions to binary classification (Allwein et al., 2000), but it is hardly clear that the weak-learning conditions implicitly assumed by such reductions are the most appropriate.", "startOffset": 85, "endOffset": 107}, {"referenceID": 1, "context": "The most common approaches so far have relied on reductions to binary classification (Allwein et al., 2000), but it is hardly clear that the weak-learning conditions implicitly assumed by such reductions are the most appropriate. The purpose of a weak-learning condition is to clarify the goal of the weak-learner, thus aiding in its design, while providing a specific minimal guarantee on performance that can be exploited by a boosting algorithm. These considerations may significantly impact learning and generalization because knowing the correct weak-learning conditions might allow the use of simpler weak classifiers, which in turn can help prevent overfitting. Furthermore, boosting algorithms that more efficiently and effectively minimize training error may prevent underfitting, which can also be important. In this paper, we create a broad and general framework for studying multiclass boosting that formalizes the interaction between the boosting algorithm and the weak-learner. Unlike much, but not all, of the previous work on multiclass boosting, we focus specifically on the most natural, and perhaps weakest, case in which the weak classifiers are genuine classifiers in the sense of predicting a single multiclass label for each instance. Our new framework allows us to express a range of weak-learning conditions, both new ones and most of the ones that had previously been assumed (often only implicitly). Within this formalism, we can also now finally make precise what is meant by correct weak-learning conditions that are neither too weak nor too strong. We focus particularly on a family of novel weak-learning conditions that have an especially appealing form: like the binary conditions, they require performance that is only slightly better than random guessing, though with respect to performance measures that are more general than ordinary classification error. We introduce a whole family of such conditions since there are many ways of randomly guessing on more than two labels, a key difference between the binary and multiclass settings. Although these conditions impose seemingly mild demands on the weak-learner, we show that each one of them is powerful enough to guarantee boostability, meaning that some combination of the weak classifiers has high accuracy. And while no individual member of the family is necessary for boostability, we also show that the entire family taken together is necessary in the sense that for every boostable learning problem, there exists one member of the family that is satisfied. Thus, we have identified a family of conditions which, as a whole, is necessary and sufficient for multiclass boosting. Moreover, we can combine the entire family into a single weak-learning condition that is necessary and sufficient by taking a kind of union, or logical or, of all the members. This combined condition can also be expressed in our framework. With this understanding, we are able to characterize previously studied weak-learning conditions. In particular, the condition implicitly used by AdaBoost.MH (Schapire and Singer, 1999), which is based on a one-against-all reduction to binary, turns out to be strictly stronger than necessary for boostability. This also applies to AdaBoost.M1 (Freund and Schapire, 1996a), the most direct generalization of AdaBoost to multiclass, whose conditions can be shown to be equivalent to those of AdaBoost.MH in our setting. On the other hand, the condition implicit to the SAMME algorithm by Zhu et al. (2009) is too weak in the sense that even when the condition is satisfied, no boosting algorithm can guarantee to drive down the training error.", "startOffset": 86, "endOffset": 3517}, {"referenceID": 9, "context": "However, using the powerful machinery of drifting games (Freund and Opper, 2002; Schapire, 2001), we are able to compute the optimal strategy for the games arising out of each weak-learning condition in the family described above.", "startOffset": 56, "endOffset": 96}, {"referenceID": 0, "context": "These optimal strategies have a natural interpretation in terms of random walks, a phenomenon that has been observed in other settings (Abernethy et al., 2008; Freund, 1995).", "startOffset": 135, "endOffset": 173}, {"referenceID": 8, "context": "These optimal strategies have a natural interpretation in terms of random walks, a phenomenon that has been observed in other settings (Abernethy et al., 2008; Freund, 1995).", "startOffset": 135, "endOffset": 173}, {"referenceID": 12, "context": "algorithms (Schapire et al., 1998; Freund and Schapire, 1997; Koltchinskii and Panchenko, 2002).", "startOffset": 11, "endOffset": 95}, {"referenceID": 2, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset.", "startOffset": 42, "endOffset": 70}, {"referenceID": 12, "context": "The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997).", "startOffset": 116, "endOffset": 143}, {"referenceID": 12, "context": "M2 (Freund and Schapire, 1997), as well as AdaBoost.", "startOffset": 3, "endOffset": 30}, {"referenceID": 1, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 4, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 5, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 13, "context": "There are also more general approaches that can be applied to boosting including (Allwein et al., 2000; Beygelzimer et al., 2009; Dietterich and Bakiri, 1995; Hastie and Tibshirani, 1998).", "startOffset": 81, "endOffset": 187}, {"referenceID": 8, "context": "The first one (Freund and Schapire, 1996b; R\u00e4tsch and Warmuth, 2005) views the weak-learning condition as a minimax game, while drifting games (Schapire, 2001; Freund, 1995) were designed to analyze the most efficient boosting algorithms.", "startOffset": 143, "endOffset": 173}, {"referenceID": 9, "context": "These games have been further analyzed in the multiclass and continuous time setting in (Freund and Opper, 2002).", "startOffset": 88, "endOffset": 112}, {"referenceID": 2, "context": ", 1998; Freund and Schapire, 1997; Koltchinskii and Panchenko, 2002). Consistency in the multiclass classification setting has been studied by Tewari and Bartlett (2007) and has been shown to be trickier than binary classification consistency.", "startOffset": 8, "endOffset": 170}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997).", "startOffset": 43, "endOffset": 940}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997).", "startOffset": 43, "endOffset": 958}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997). Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 (Freund and Schapire, 1997), as well as AdaBoost.MH and AdaBoost.MR (Schapire and Singer, 1999). Other approaches include the work by Eibl and Pfeiffer (2005); Zhu et al.", "startOffset": 43, "endOffset": 1251}, {"referenceID": 1, "context": "Nonetheless, by following the approach in (Bartlett and Traskin, 2007) for showing consistency in the binary setting, we are able to extend the empirical consistency guarantees to general consistency guarantees in the multiclass setting: we show that under certain conditions and with sufficient data, our adaptive algorithm approaches the Bayes-optimum error on the test dataset. We present experiments aimed at testing the efficacy of the adaptive algorithm when working with a very weak weak-learner to check that the conditions we have identified are indeed weaker than others that had previously been used. We find that our new adaptive strategy achieves low test error compared to other multiclass boosting algorithms which usually heavily underfit. This validates the potential practical benefit of a better theoretical understanding of multiclass boosting. Previous work. The first boosting algorithms were given by Schapire (1990) and Freund (1995), followed by their AdaBoost algorithm (Freund and Schapire, 1997). Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 (Freund and Schapire, 1997), as well as AdaBoost.MH and AdaBoost.MR (Schapire and Singer, 1999). Other approaches include the work by Eibl and Pfeiffer (2005); Zhu et al. (2009). There are also more general approaches that can be applied to boosting including (Allwein et al.", "startOffset": 43, "endOffset": 1270}, {"referenceID": 12, "context": "M1 (Freund and Schapire, 1997) measures the performance of weak classifiers using ordinary error.", "startOffset": 3, "endOffset": 30}, {"referenceID": 18, "context": "The SAMME algorithm of Zhu et al. (2009) requires the weak classifiers to achieve less error than uniform random guessing for multiple labels; in our language, their weak-learning condition is (C,U\u03b3), as shown in Section 3, where CSAM consists of cost matrices whose rows are of the form (0, t, t, .", "startOffset": 23, "endOffset": 41}, {"referenceID": 8, "context": "Our algorithms are derived from the very general drifting games framework (Schapire, 2001) for solving boosting games, which in turn was inspired by Freund\u2019s Boost-by-majority algorithm (Freund, 1995), which we review next.", "startOffset": 186, "endOffset": 200}, {"referenceID": 15, "context": "However, Schapire (2001) observed that the payoffs can be very well approximated by certain potential functions.", "startOffset": 9, "endOffset": 25}, {"referenceID": 15, "context": "When the weak learning condition being used is (C,B), Schapire (2001) proposed a Booster strategy, called the OS strategy, which always chooses the weight \u03b1t = 1, and uses the potential functions to construct a cost matrix Ct as follows.", "startOffset": 54, "endOffset": 70}, {"referenceID": 15, "context": "When the weak learning condition being used is (C,B), Schapire (2001) proposed a Booster strategy, called the OS strategy, which always chooses the weight \u03b1t = 1, and uses the potential functions to construct a cost matrix Ct as follows. Each row Ct(i) of the matrix achieves the minimum of the right hand side of (24) with b replaced by B(i), t replaced by T \u2212 t, and s replaced by current state st(i): Ct(i) = argmin c\u2208C0 k max l=1 { \u03c6 B(i) T\u2212t\u22121 (s + el)\u2212 (c(l)\u2212 \u3008c,B(i)\u3009) } . (25) The following theorem, proved in the appendix, provides a guarantee for the loss suffered by the OS algorithm, and also shows that it is the game-theoretically optimum strategy when the number of examples is large. Similar results have been proved by Schapire (2001), but our theorem holds much more generally, and also achieves tighter lower bounds.", "startOffset": 54, "endOffset": 752}, {"referenceID": 15, "context": "Using a different initial distribution, Schapire and Singer (1999) achieve the slightly better bound \u221a (k \u2212 1)e\u2212T\u03b32/2.", "startOffset": 40, "endOffset": 67}, {"referenceID": 7, "context": "Now, there is no lower bound on how small the edge \u03b3 may get, and, anticipating the worst, it makes sense to choose an infinitesimal \u03b3, in the spirit of (Freund, 2001).", "startOffset": 153, "endOffset": 167}, {"referenceID": 12, "context": "M2 (Freund and Schapire, 1997) or AdaBoost.", "startOffset": 3, "endOffset": 30}, {"referenceID": 18, "context": "Consistency for multiclass classification in the probabilistic setting has been studied by Tewari and Bartlett (2007), who show that, unlike in the binary setting, many natural approaches fail to achieve consistency.", "startOffset": 91, "endOffset": 118}, {"referenceID": 2, "context": "MM and binary AdaBoost, and then leveraging almost identical known consistency results for AdaBoost (Bartlett and Traskin, 2007).", "startOffset": 100, "endOffset": 128}, {"referenceID": 6, "context": "1 in (Schapire and Freund, 2012), which in turn is based on the work by Zhang (2004) and Bartlett et al.", "startOffset": 19, "endOffset": 85}, {"referenceID": 3, "context": "1 in (Schapire and Freund, 2012), which in turn is based on the work by Zhang (2004) and Bartlett et al. (2006). Let p(x) = Pr(x\u2032,y\u2032)\u223cD (x \u2032 = x) denote the the marginalized probability of drawing example x from D, and let py = Pr(x\u2032,y\u2032)\u223cD [y \u2032 = y|x\u2032 = x] denote the conditional probability of drawing label y given we have drawn example x.", "startOffset": 89, "endOffset": 112}, {"referenceID": 7, "context": "3 in (Schapire and Freund, 2012), and so we omit it. When k = 2, AdaBoost.MM is identical to AdaBoost. For Theorem 29 to hold for AdaBoost, the richness assumption (85) is necessary, since there are examples due to Long and Servedio (2010) showing that the theorem may not hold when that assumption is violated.", "startOffset": 19, "endOffset": 240}, {"referenceID": 16, "context": "MH using stumps (Schapire and Singer, 2000), and compared it against our method MM with a naive greedy tree-searching weak-learner Greedy.", "startOffset": 16, "endOffset": 43}, {"referenceID": 7, "context": "A very similar reduction was carried out by Freund and Schapire (1997). Borrowing their terminology, the transformed dataset roughly consists of mislabel triples (x, y, l) where y is the", "startOffset": 44, "endOffset": 71}], "year": 2011, "abstractText": "Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the \u201ccorrect\u201d requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.", "creator": "TeX"}}}