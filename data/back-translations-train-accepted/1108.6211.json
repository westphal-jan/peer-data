{"id": "1108.6211", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2011", "title": "Transfer from Multiple MDPs", "abstract": "Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them into the training set used to solve a given target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.", "histories": [["v1", "Wed, 31 Aug 2011 12:46:11 GMT  (70kb)", "https://arxiv.org/abs/1108.6211v1", "2011"], ["v2", "Thu, 1 Sep 2011 09:19:00 GMT  (70kb)", "http://arxiv.org/abs/1108.6211v2", "2011"]], "COMMENTS": "2011", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["alessandro lazaric", "marcello restelli"], "accepted": true, "id": "1108.6211"}, "pdf": {"name": "1108.6211.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["alessandro.lazaric@inria.fr", "restelli@elet.polimi.it"], "sections": [{"heading": null, "text": "ar Xiv: 110 8.62 11v2 [cs.AI] 1"}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Preliminaries", "text": "We define a discounted Markov Decision Process (MDP) as a tuple M = < X, A, R, P, \u03b3 >, where the state space X is a limited closed subset of Euclidean space, A is a finite space of action, the deterministic 1 reward function R: X \u00b7 A \u2192 R is uniformly limited by Rmax, the transition core P is such that for all x x-X and an x-X, P (\u00b7 x, a) is a distribution over X, and vice versa (0, 1) is a discount factor. We denote by S (X \u00b7 A) the amount of probability measures above X \u00b7 A and by B (X \u00b7 A); Vmax = Rmax1 \u2212 B the space of measurable functions with the domain X \u00b7 A. We define the optimal function."}, {"heading": "3 All-Sample Transfer Algorithm", "text": "We first consider the case when the source samples according to Def. 1 are generated beforehand and the designer does not have access to the source tasks. We examine the algorithm referred to as All-Sample Transfer (AST) (Fig. 1), which is simply FQI with a linear spacing F on the entire training set {(Xl, Al, Yl, Rl)} Ll = 1. With each iteration k, considering the result of the previous iteration Q-k-1 = T (Q-k-1)), the algorithm returns: Q-k = argmin f-F1LL-l = 1 (f (Xl, Al) \u2212 (Rl + \u03b3maxa \u00b2 A-Q-k-1 (Yl, a-1))) 2. (1) In the case of linear spaces, the minimization problem is solved in a closed form, as in Fig. 1. In the following we report on a finite sample analysis of the performance of AST and then we examine each [similar to Iteration 11]."}, {"heading": "3.1 Single Iteration Finite-Sample Analysis", "text": "We define the average MDP M\u03bb as the average of the M MDPs in the hand. We define their reward function Q = Q = = 32 Q and their transition core P\u03bb as the weighted average of the reward functions and transition cores of the basic MDPs with weights determined by the proportions of the multinomial distribution. The resulting average Bellman operator is (T \u03bbQ) (x, a) = (M \u2211 m = 1\u03bbmT mQ) = 1\u03bbmT mQ) (x, a) = R (x, a) + \u03b3 Xmax a \u2032 Q (y, a) P (dy x, a). In the random task design, the average MDP plays a crucial role, since the implicit objective function of minimizing empirical loss in Eq. 1 is actually T-Q."}, {"heading": "3.2 Propagation Finite-Sample Analysis", "text": "We are now examining how the previous error spreads through iterations. Let us be the evaluation norm (i.e., in general we differ from the Sampling distribution). We report two assumptions first. \u2212 Assumption 1: [11] In view of the fact that c (p) = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4 Best Average Transfer Algorithm", "text": "As discussed in the previous section, the transmission error plays a decisive role in the comparison with the individual task \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"Q\" = \"=\" Q \"=\" Q \"=\" Q \"=\" Q \"=\" Q \"=\" Q \"=\" Q \"=\" Q \"=\" = \"Q\" = \"=\" Q \"=\" Q \"=\" = \"Q\" = \"=\" Q \"=\" = \"Q\" = \"=\" Q \"=\" = \"Q\" = \"=\" = \"Q\" = \"=\" = \"Q\" = \"=\" = \"Q\" = \"=\" Q \"=\" = \"Q\" = \"=\" Q \"=\" = \"Q\" = \"=\" = \"Q\" = \"=\" = \"Q\" = \"=\" = \"Q\" = \"=\" = \"Q\" = \"=\" Q \"=\" = \"=\" Q \"=\" = \"=\" Q \"=\" = \"=\" Q \"=\" = \"=\" = \"Q\" = \"=\" = \"=\" Q = \"=\" = \"=\" = \"Q\" = \"=\" = \"=\" = \"Q =\" = \"=\" = \"=\" = \"=\" Q = \"=\" = \"=\" = \"Q =\" = \"=\" = \"Q =\" = \"=\" = \"=\" Q = \"=\" = \"=\" = \"=\" Q = \"=\" = \"=\" = \"Q =\" = \"=\" = \"=\" = \"Q =\" = \"=\" Q = \"=\" = \"=\" = \"Q =\" = \"=\" = \"=\" Q = \"=\" = \"=\" = \"=\" = \"Q =\" = \"=\" Q = \"=\" = \"=\" = \"=\" Q = \"Q =\" = \"=\" = \"=\" = \"Q =\" = \"=\" = \"=\" = \"=\" = \"=\""}, {"heading": "5 Best Transfer Trade-off Algorithm", "text": "The previous algorithm has proved successful in estimating the combination of source tasks that are more similar to the Bellman operator of the target task. (\u03b2 \u03b2 \u03b2) However, BAT relies on the implicit assumption that L-samples can always be generated from each source task 3 and it cannot be applied to the case where the number of source samples is limited. Here, we are looking at the more difficult case where the designer still has access to the source tasks but only a limited number of samples is available in each of them. In this case, an adaptive transfer algorithm should resolve a compromise between selecting as many samples as possible to reduce the estimate error and correctly selecting the proportion of source samples to control the transfer error. Solving this compromise cannot provide trivial results where source tasks similar to the target task but with a few samples are removed in favor of a pool of tasks whose average size roughly approximates the target task but can provide a larger number of samples."}, {"heading": "6 Experiments", "text": "In this section, we report and discuss preliminary experimental results of the transfer algorithms introduced in the previous sections. [11] The main goal is to illustrate the functioning of the algorithms and compare their results with theoretical results. Thus, we focus on a simple problem and leave more challenging problems for future work.We consider a continuous extension of the 50-state variant of the chain-walking problem described in [6] the state space by a continuous state variable x and two actions: one that3M = 1 for task, then the algorithm would provide all L training samples from the task m.Table 1: Parameters for the first set of taskstasks p l for RewardM1 0.9 + 1 0.1 actions."}, {"heading": "7 Conclusions", "text": "In this paper, we formalized and studied the sample transfer problem. We first derived a finitesample analysis of the performance of a simple transfer algorithm, which includes in the training all source samples used to solve a specific target task. At best, this is the first theoretical result for a transfer algorithm in RL that shows the potential benefits of transferring via single task learning. Then, in the case where the designer has direct access to the source tasks, we introduced an adaptive algorithm that selects the ratio of source tasks to minimize bias due to the use of source samples. Finally, we considered a more difficult setting in which the number of samples available in each source task is limited and a compromise between the amount of transferred samples and the similarity between source and target tasks needs to be resolved. For this setting, we proposed a principal transfer of adaptive algorithms. Finally, we report on a detailed experimental analysis of a simple problem that confirms and supports theoretical finalizations."}, {"heading": "A Additional Notation", "text": "In addition to the notation introduced in section 2, we present additional symbols used in the proofs. We define two empirical norms for functions and vectors. In view of a series of N pairs (Xn, An), we define the empirical norm as | | f | | \u00b5 as | f | | 2\u00b5 = 1NN = 1NN = 1f (Xn, An) 2.Similarly, for a vector y = RN, we define the empirical norm | | y | | 2N = 1NN = 1y2n.For a series of N pairs {(Xn, An)} Nn = 1, we allow a vector y-RN (X1, A1);."}, {"heading": "B Fitted Q-iteration with Linear Spaces", "text": "Although adapted iterative methods have already been analyzed in detail in [11] and [1], to the best of our knowledge no explicit limits of finite samples are available for linear space FQI. Since FQI solves an explicit regression problem with each iteration, the derivation is largely a simple application of regression limits for linear spaces and square loss. Here we only report on the result and the proof of the single iteration error for the so-called fixed and random design settings of samples. In Algorithm 6 we report on the structure of the algorithm."}, {"heading": "B.1 Fixed Samples Design", "text": "Similar to the analysis of LSTD in [7], we first derive the fixed design error (i.e., the performance is evaluated exactly on the states in the training set). - Theorem 4: Let F = (xn, an). - Errors are a d-dimensional linear space. - Let [xn, an, an, Yn, Rn) Nn = 1 be the training in which [xn, an, an) Nn = 1 is an arbitrary sequence of state action pairs, Yn \u00b2 P (xn, an) and Rn = R (xn, an). Faced with a function Q \u00b2 B (X \u00d7 A, Vmax), let q \u00b2 RN be the vector whose components are qn = (T Q) (xn, an) and q \u00b2 the solution of a single iteration of the adapted value variation."}, {"heading": "B.2 Random Samples Design", "text": "While in the previous section we analyzed the performance of FQI on exactly the same state / action pairs in the training set, we now focus on generalization (i.e. predictive) performance on the entire state / action set. Let Q be any function that is satisfactory to f\u03b1 and F, where q is the vector defined in the previous section. Then, we derive the following theorem. Theorem 5. Let F = {\u03c6 (\u00b7, \u00b7) \u03b1, \u03b1 Rd} be a d-dimensional linear space. Let {(Xn, An, Yn, Rn)} Nn = 1 be the training set in which (Xn, An) iid \u0445 \u00b5, Yn \u0445 P (\u00b7 Xn, An) and Rn = R (Xn, An). Let Q be the solution of a single iteration of the adjusted value setting with probability in the next case \u2212 1."}, {"heading": "Q\u0302 satisfies", "text": "The proof is essentially based on the application of the concentration inequalities for linear spaces to the function limited in Theorem 4.Let f\u03b1 \"F,\" the f\u03b1 \"Q\" (Xn) = \"q\" n, i.e. the approximation error to the approximation error point, where the approximation error can be rewritten to the approximation error to the approximation error point (Q \"Q\") n, i.e. to the approximation error to the approximation error point (T \"Q\") n, where the approximation error can be rewritten to the approximation error to the approximation error to the approximation point (T \"Q\") n, to the approximation error to the approximation point (T \"Q\") n, to the approximation error to the approximation error to the approximation point (T \"Q\" 2 \") n, to the approximation error to the approximation point (T\" Q \") n, to the approximation error to the approximation error to the approximation point (T\" Q \") n."}, {"heading": "C Analysis of AST", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C.1 Proof of Theorem 1", "text": "Since the detection follows similar steps to the detection of theorem 5, we will only discuss the specified patterns here. We define the vector p = 1,.., L, pl = \u2211 M m = 1 I {Ml = m} (Rml + \u03b3maxa \u2032 Q (Y ml, a \u00b2)). The target vector q \u00b2 RL is the image of function Q by the average optimum Bellman operator. In fact, by defining ql = (T \u00b2 Q) (Xl, Al) we obtain a zero mean noise vector. l = pl \u2212 ql, so that E [\u043al] = 0 and | \u0445l | 2Vmax. 44The expectation is achieved both by the random realization of the reward Rml and by the next state Y m l and taskindex Ml.The statement of the theorem simply follows by decompiling the prediction error of Q \u00b2 as an approximation error."}, {"heading": "C.2 Proof of Theorem 2", "text": "The main structure of the evidence is exactly the same as in [11]. The main differences are due to the use of linear spaces and the transmission error. If you follow the passages in the proof of theorem 2 in [11], you get all the terms in the statement of theorem 1 that are affected by the maximization in the course of iterations. The approximation term corresponds max k min f-f-2Q-k-k-k-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-f-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-"}, {"heading": "D Analysis of BAT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Proof of Theorem 3", "text": "2. Let {(Xs, As, R1s,..., RMs)} Ss = 1 be a training set in which (Xs, As) iid \u0445 \u00b5 and Rms = Rm (Xs, As) and for each state-action pair and for each task m, T the next states Y ms, t \u0445 Pm (\u00b7 | Xs, As) with t = 1,... are available. For each fixed limited function Q-B (X \u00b7 A; Vmax) results by minimizing equation equation."}, {"heading": "Proof. [Lemma 2]", "text": "The sketch of the proof is as follows: For any state action pair Xs, As, we define: E-proxy (Xs, As) = R1s \u2212 M-proxy (Xs, As) = 1 (max a) Q-1 (Y 1s, t, a) \u2212 M-proxy m = 2\u03bbm max a \"Q\" k \u2212 1 (Y ms, t, a \"), andE-proxy (Xs, As) = (T1Q, A) (Xs, As) \u2212 M \u00b2 k \u2212 1 (Y ms, As). Consequently, we prove: E \u2212 proxy (x, a) 2\" and E-proxy (Bells = 1S, As) = E-proxy (Xs, As)."}, {"heading": "E Additional Experimental Analysis", "text": "In this section we provide additional experimental results related to the BTT algorithm."}, {"heading": "E.1 Analysis of parameters \u03b2", "text": "In order to have a better understanding of how BTT calculates between the necessity of sampling and the risk of introducing a large transfer error, we show in Figure 8 the values of the parameters \u03b2 (representing the percentage of samples transferred from each task) as being optimized by the BTT algorithm at each FQI iteration. The tasks that are taken into account are target task M1 and source tasks M6, M7, M8, M9, each with 5000 samples available. Figure 8 compares the values of \u03b2 in two scenarios: If the available target samples are 100 (left side) and 10,000 (right side). Obviously, BTT always utilizes all target samples (\u03b21 = 1). If only a few target samples are available, BTT transfers high percentages of samples from the source tasks. Specifically, it transfers all available samples from task M9 in each iteration, and also the percentage of samples that are taken at a constant (almost 0.7)."}, {"heading": "E.2 Analysis of parameter \u03c4", "text": "The trade-off achieved by the BTT algorithm is multiplied by the parameter \u03c4 by the estimation error. In Figure 11, we analyze the effect of \u03c4 on learning performance. Different values of the target parameter have been tried (\u03c4 = 0.25, 0.50, 0.75, 1.0) when both 5000 samples (left range) and 10,000 samples (right range) are available for each target task. We can see that BTT is relatively robust in the choice of the target parameter. The main differences occur when a small number of target samples are available. In this case, low values of \u03c4 make the BTT more concerned about the transfer error and, as a result, it tends to avoid transferring source samples even when target samples are insufficient. On the other hand, at high values of Celsius, BTT is pushed to use more source samples, and this can have a negative effect on performance when multiple target tasks are available and no combination of source tasks provides good target parameters."}], "references": [{"title": "Fitted Q-iteration in continuous actionspace MDPs", "author": ["Andras Antos", "R\u00e9mi Munos", "Csaba Szepesvari"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Vaughan"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Learning from multiple sources", "author": ["Koby Crammer", "Michael Kearns", "Jennifer Wortman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "A distribution-free theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzy\u017cak", "H. Walk"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Finite-sample analysis of LSTD", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "Technical Report inria-00482189,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Transfer of samples in batch reinforcement learning", "author": ["A. Lazaric", "M. Restelli", "A. Bonarini"], "venue": "In Proceedings of the Twenty-Fifth Annual International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Knowledge Transfer in Reinforcement Learning", "author": ["Alessandro Lazaric"], "venue": "PhD thesis, Poltecnico di Milano,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of the 22nd Conference on Learning Theory (COLT\u201909),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Finite time bounds for fitted value iteration", "author": ["R. Munos", "Cs. Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Transferring instances for model-based reinforcement learning", "author": ["Matthew E. Taylor", "Nicholas K. Jong", "Peter Stone"], "venue": "In Proceedings of the European Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "The objective of transfer in reinforcement learning (RL) [12] is to speed-up RL algorithms by reusing knowledge (e.", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "A wide range of scenarios and methods for transfer in RL have been studied in the last decade (see [14, 9] for a thorough survey).", "startOffset": 99, "endOffset": 106}, {"referenceID": 12, "context": "This approach has been already investigated in the case of transfer between tasks with different state-action spaces in [13], where the source samples are used to build a model of the target task whenever the number of target samples is not large enough.", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "A more sophisticated sample-transfer method is proposed in [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "In [2] and [10] different distance measures are proposed and are shown to be connected to the performance of the transferred solution.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In [2] and [10] different distance measures are proposed and are shown to be connected to the performance of the transferred solution.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "The case of transfer of samples from multiple source tasks is studied in [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "We introduce three sample-transfer algorithms based on fitted Qiteration [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 10, "context": "Similar to [11], we first study the prediction error in each iteration and we then propagate it through iterations.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "[11] Given \u03bc, \u03bd, p \u2265 1, and an arbitrary sequence of policies {\u03c0p}p\u22651, we assume that the future-state distribution \u03bcP1 \u03c01 \u00b7 \u00b7 \u00b7 P1 \u03c0p is absolutely continuous w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "This term is referred to as the inherent Bellman error [11] of the space F and it is related to how well the Bellman images of functions in F can be approximated by F itself.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "This term also displays interesting similarities with the notion of discrepancy introduced in [10] in domain adaptation.", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": ", Rs,M )}s=1 be an auxiliary training set where (Xs, As) \u223c \u03bc and Rs,m = We refer to [11] for a thorough explanation of the concentrability terms.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "It is interesting to notice that similar estimation errors appear in FVI [11] where the optimal Bellman operator is approximated by Monte-Carlo estimation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "In [8] a method to compute the similarity between MDPs is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In particular, the method in [8] tries to identify source tasks which are individually similar to the target task, while the transfer error minimized in BAT considers the average MDP obtained by the transfer process.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "do Compute \u03b2\u0302 = argmin\u03b2\u2208[0,1]M \u00ca\u03b2 + c \u221a d \u2211 M m=1 \u03b2mNm Run one iteration of AST (Fig.", "startOffset": 24, "endOffset": 29}, {"referenceID": 0, "context": "Let \u03b2 \u2208 [0, 1] be a weight vector, where \u03b2m is the fraction of samples from task m used in the transfer process.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "\u03b2\u0302 = arg min \u03b2\u2208[0,1]M ( \u00ca\u03b2(Q\u0303) + \u03c4 \u221a d \u2211M m=1 \u03b2mNm ) , (6)", "startOffset": 15, "endOffset": 20}, {"referenceID": 5, "context": "We consider a continuous extension of the 50-state variant of the chain walk problem proposed in [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M2 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M2 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M3 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M3 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 +5 in [\u221211,\u22129] \u222a [9, 11] M4 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +5 in [\u221211,\u22129] \u222a [9, 11] M4 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 3, "context": "1 +1 in [\u22126,\u22124] \u222a [4, 6] M5 0.", "startOffset": 18, "endOffset": 24}, {"referenceID": 5, "context": "1 +1 in [\u22126,\u22124] \u222a [4, 6] M5 0.", "startOffset": 18, "endOffset": 24}, {"referenceID": 3, "context": "1 \u22121 in [\u22126,\u22124] \u222a [4, 6] Table 2: Parameters for the second set of tasks", "startOffset": 18, "endOffset": 24}, {"referenceID": 5, "context": "1 \u22121 in [\u22126,\u22124] \u222a [4, 6] Table 2: Parameters for the second set of tasks", "startOffset": 18, "endOffset": 24}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M6 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M6 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M7 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M7 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M8 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 +1 in [\u221211,\u22129] \u222a [9, 11] M8 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M9 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "1 \u22125 in [\u221211,\u22129] \u222a [9, 11] M9 0.", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "5 +5 in [\u221211,\u22129] \u222a [9, 11]", "startOffset": 19, "endOffset": 26}, {"referenceID": 10, "context": "5 +5 in [\u221211,\u22129] \u222a [9, 11]", "startOffset": 19, "endOffset": 26}, {"referenceID": 8, "context": "The reward function provides +1 when the system state reaches the regions [\u221211,\u22129] and [9, 11] and 0 elsewhere.", "startOffset": 87, "endOffset": 94}, {"referenceID": 10, "context": "The reward function provides +1 when the system state reaches the regions [\u221211,\u22129] and [9, 11] and 0 elsewhere.", "startOffset": 87, "endOffset": 94}], "year": 2011, "abstractText": "Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.", "creator": "gnuplot 4.4 patchlevel 0"}}}