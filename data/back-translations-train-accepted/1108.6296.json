{"id": "1108.6296", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2011", "title": "Infinite Tucker Decomposition: Nonparametric Bayesian Models for Multiway Data Analysis", "abstract": "Tensor decomposition is a powerful tool for multiway data analysis. Many popular tensor decomposition approaches---such as the Tucker decomposition and CANDECOMP/PARAFAC (CP)---conduct multi-linear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g. missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose a tensor-variate latent $t$ process model, InfTucker, for robust multiway data analysis: it conducts robust Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, it handles both continuous and binary data in a probabilistic framework. Unlike previous nonparametric Bayesian models on matrices and tensors, our latent $t$-process model focuses on multiway analysis and uses nonlinear covariance functions. To efficiently learn InfTucker from data, we develop a novel variational inference technique on tensors. Compared with classical implementation, the new technique reduces both time and space complexities by several orders of magnitude. This technique can be easily adopted in other contexts (e.g., multitask learning) where we encounter tensor-variate $t$ processes or Gaussian processes. Our experimental results on chemometrics and social network datasets demonstrate that the new InfTucker model achieves significantly higher prediction accuracy than several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD), Weighted CP, and nonnegative tensor decomposition.", "histories": [["v1", "Wed, 31 Aug 2011 17:36:26 GMT  (35kb)", "http://arxiv.org/abs/1108.6296v1", null], ["v2", "Sat, 14 Jan 2012 16:11:56 GMT  (46kb,D)", "http://arxiv.org/abs/1108.6296v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["zenglin xu", "feng yan 0003", "yuan qi"], "accepted": true, "id": "1108.6296"}, "pdf": {"name": "1108.6296.pdf", "metadata": {"source": "CRF", "title": "InfTucker: t-Process based Infinite Tensor Decomposition", "authors": ["Zenglin Xu"], "emails": ["xu218@purdue.edu", "yan12@purdue.edu", "alanqi@cs.purdue.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 8,62 96v1 ["}, {"heading": "1 Introduction", "text": "However, these models face serious challenges in modeling complex multiway interactions. First, the interactions between entities in each mode are coupled with each other and are highly nonlinear. Classical multilinear models cannot capture these complicated relationships."}, {"heading": "2 Preliminary", "text": "The notations. In this paper, we denote scalars by lowercase letters (e.g. a), vectors by bold lowercase letters (e.g. a), matrices by bold uppercase letters (e.g. A), and tensors by calligraphic uppercase letters (e.g. A). The calligraphic uppercase letters are also used for probability distributions (e.g. K) (e.g. N (\u00b5, \u03a3). We use uij to match the (i, j) entry of a matrix U, yi to the i = (i1,.., iK) entry of a tensor Y. U V denotes the Kronecker product of the two matrices there. We define the vectorization process denoted by vec (Y) to match the tensor entries in a K k = 1,."}, {"heading": "3 Robust infinite Tucker decomposition by t processes", "text": "In this section, we present the robust infinite Tucker decomposition based on latent t processes. We extend the classic Tucker decomposition in three aspects: i) flexible noise models for continuous and binary observations; ii) an infinite core tensor for modelling complex interactions; and iii) latent t process, which makes the model robust to outsiders. 1In contrast to the usual column-by-column Vec operation, our definition of vec () is sequenced on matrices, which avoids the use of transpose in many equations during this paper. Specifically, we assume that the observed tensor Y is evaluated from a latent reevaluated tensor M via a probable noise model p (Y | M) = i p (yi | mi). We perform tucker decomposition of M with a core tensor W of infinite size."}, {"heading": "3.1 Tensor-variate t processes", "text": "Before formally defining the tensor-variate-t process, we designate the domain of mode k of Uk, where the K-covariance functions of \u03a3 (k): Uk \u00b7 Uk \u2192 R, the covariance matrices of a tucker tensor S \u2212 1 2 = [(\u03a3 (1))) \u2212 1,., (\u03a3 (K) \u2212 2 and n = Kk = 1 nk. The norm of the tensor process A is defined as a 2 i. Then we define tensor-variate tProcesses as follows. Definition 1 (tensor-variate t distributions and tenor-variate t processes) Given the K position, Uk = 1,.,., K, let b: U1 \u00d7., UK \u2192 R be the mean function. M = {f (u (u (1)."}, {"heading": "3.2 Noise models", "text": "We use a noise model p (Y | M) to link the infinite Tucker decomposition with the tensor observation Y.Probit model: In this case, each entry of the observation is binary; that is, yi {0, 1}. A probit function p (yi | mi) = \u03a6 (mi) yi (1 \u2212 \u03a6 (mi) 1 \u2212 yi models the binary observation. Note that \u03a6 (\u00b7) is the normal cumulative distribution function.Gaussian model: We use a Gaussian probability model p (yi | mi) = N (yi | mi, \u03c32) to model the really evaluated observation. Missing values: We allow missing values in the observation. Let O denote the indices of the observed entries in Y. Then we have p (YO | MO) as a probability model. Other noise models include modified Probit models for ordinary regression and zero classification for this [zero] classification."}, {"heading": "4 Inference", "text": "Given the observed tensor Y, our goal is to estimate the component matrices U (k) by maximizing the marginal probability p (Y | {U (k)} Kk = 1) p ({U (k)} Kk = 1). However, integrating M into the above equation is difficult. Therefore, we resort to approximate inference; more specifically, we develop a variation expectation maximization algorithm. In the following paragraphs, we first present the inference algorithms for both noise models and then describe an efficient algebraic approach to significantly reduce computational complexity."}, {"heading": "4.1 Variational EM for binary classification", "text": "We follow the data augmentation scheme of Albert and Chib [2] to break down the q-q model into p (yi-mi) = p (yi-i | zi) p (zi-mi) dzi. We have the indicator function, we know that a t distribution can be converted into a normal distribution related to a gamma distribution, such as T (M-Z), 0, (k) Kk = N (M | 0). It is well known that a t distribution can be converted into a normal distribution."}, {"heading": "4.2 Regression", "text": "Conclusion: The conclusion for the regression case follows the same format as the case for the binary classification. The only changes are: 1) Replacing Eq [Z] with Y and skipping the update of q (Z). 2) The variable EM algorithm is applied only to the observed entries. Prediction: In the case of a shortfall index i = (i1,..., iK) the prediction distribution is isp (yi | YO) \u2248 p (yi | mi) p (mi | M, \u03b7) q (\u03b7) dMd\u03b7The above integral is insoluble, so we replace the integral q (\u03b7) d\u03b7 with the mode of its approximate posterior distribution rate."}, {"heading": "4.3 Efficient Algorithms", "text": "(K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K"}, {"heading": "5 Related Works", "text": "The InfTucker model extends the probabilistic PCA (PPCA) [20] and the Gaussian Process Latent Variable Models (GPLVMs) [11]: while PPCA and GPLVM model interactions of one mode of a matrix and ignore the common interactions of two modes, InfTucker does so. Our model is related to previous tensor-variable GPs Yu et al. [23], Yu and Chu [22]. The main difference lies in the fact that they used linear covariance functions to reduce computational complexities and deal with matrix variant data for online recommendations and link predictions. Our model is closely related to the probable Tucker 3 (pTucker) model [5]; in fact, InfTucker reduces computational complexity to pTucker as a special case criterion convergence."}, {"heading": "6 Experiments", "text": "To evaluate the proposed InfTucker degradation methods, we perform two sets of experiments: one for setting with Gaussian noise and the other for probit noise. For both sets, we compare InfTucker with the following conventional and state-of-the-art tensor decomposition methods: CP, Tucker decomposition (TD), Nonnegative CP (NCP), High Order SVD (HOSVD), and weighted CP (WCP), all of which are implemented in the Tensor toolbox2. Experiment on Gaussian Noise Model: We employ four continuous chemometrics datasets3, namely amino, brot, flow injection, suger. Their dimensions are 5 x 201 x 61, 10 x 100 x 89, and 268 x 571 x 7, respectively, all of the above datasets are standardized."}, {"heading": "7 Conclusion", "text": "We proposed a new non-parametric Bayesian tensor decomposition framework, InfTucker, in which the observation tensor is modelled as a draw from a tensor variable t process. This framework is able to flexibly model the interdependence between multimodes by performing tensor decomposition in infinite attribute space. It can also handle multiple types of data, such as incomplete data and binary data, using various approaches to noise modeling, while most traditional methods rely on estimates with the least square. We also proposed an efficient variational approach to estimating component matrices as a significant contribution. Experimental results on chemometry and social network datasets show that the new InfTucker model achieves significantly higher predictive accuracy than several state-of-the-art tensor decomposition approaches."}, {"heading": "A Proof of Theorem 2", "text": "The proof of convergence (K) results from convergence (20).Convergence (K) results from convergence (K), where convergence (K) r (i, j) r (i, j) r (i, j) r (i, j) r (K) r (K) r (K) r (K) r (K) r (K)) r (K) r (K) r (K) r (n))) (20) This is how convergence (K) results. \u2212 \u2212 \u2212 Convergence (20) results in convergence (r) r (K) r (i, j) r (K) r (K) r (K) r (K) r (K) r (K) r (K) r (K) r (K) r (K) (K). \u2212 Convergence (20) results in convergence (K)."}, {"heading": "C Original figures", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Scalable tensor factorizations for incomplete data", "author": ["E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. M\u00f8rup"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Bayesian analysis of binary and polychotomous response data", "author": ["James H Albert", "Siddhartha Chib"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Multi-task Gaussian process prediction", "author": ["Edwin Bonilla", "Kian Ming Chai", "Chris Williams"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Probabilistic models for incomplete multi-dimensional arrays", "author": ["Wei Chu", "Zoubin Ghahramani"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Foundations of the PARAFAC procedure: Model and conditions for an\u201dexplanatory\u201dmulti-mode factor analysis", "author": ["R.A. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "Hierarchical multilinear models for multiway data", "author": ["Peter D. Hoff"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Nonnegative Tucker decomposition", "author": ["Yong-Deok Kim", "Seungjin Choi"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Tensor decompositions and applications", "author": ["Tamara G. Kolda", "Brett W. Bader"], "venue": "SIAM Review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A multilinear singular value decomposition", "author": ["Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle"], "venue": "SIAM J. Matrix Anal. Appl,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "The Gaussian process latent variable model", "author": ["Neil Lawrence"], "venue": "Technical Report CS-06-03,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Semi-supervised learning via gaussian processes", "author": ["Neil D. Lawrence", "Michael I. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization", "author": ["Xi Chen"], "venue": "In Proceedings of SDM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Multi-HDP: A non-parametric Bayesian model for tensor factorization", "author": ["Ian Porteous", "Evgeniy Bart", "Max Welling"], "venue": "In Proc. AAAI Conference on Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Graphical Model Structure Learning with L1-Regularization", "author": ["Mark Schmidt"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Non-negative tensor factorization with applications to statistics and computer vision", "author": ["Amnon Shashua", "Tamir Hazan"], "venue": "In Proceedings of the 22nd ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Beyond streams and graphs: Dynamic tensor analysis", "author": ["Jimeng Sun", "Dacheng Tao", "Christos Faloutsos"], "venue": "In KDD,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Multivis: Content-based social network exploration through multi-way visual analysis", "author": ["Jimeng Sun", "Spiros Papadimitriou", "Ching-Yung Lin", "Nan Cao", "Shixia Liu", "Weihong Qian"], "venue": "In SDM\u201909,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Probabilistic principal component analysis", "author": ["Michael E. Tipping", "Christopher M. Bishop"], "venue": "Journal of The Royal Statistical Society Series B-statistical Methodology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["Ledyard Tucker"], "venue": "Psychometrika, 31:279\u2013311,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1966}, {"title": "Gaussian process models for link analysis and transfer learning", "author": ["Kai Yu", "Wei Chu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Stochastic relational models for discriminative link prediction", "author": ["Kai Yu", "Wei Chu", "Shipeng Yu", "Volker Tresp", "Zhao Xu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Spatio-temporal compressive sensing and internet traffic matrices", "author": ["Yin Zhang", "Matthew Roughan", "Walter Willinger", "Lili Qiu"], "venue": "In Proceedings of the ACM SIGCOMM 2009 conference on Data communication", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}], "referenceMentions": [{"referenceID": 19, "context": "Traditional multiway factor models\u2014 such as the Tucker decomposition [21] and CANDECOMP/PARAFAC (CP) [6]\u2014have been widely applied in many areas (e.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Traditional multiway factor models\u2014 such as the Tucker decomposition [21] and CANDECOMP/PARAFAC (CP) [6]\u2014have been widely applied in many areas (e.", "startOffset": 101, "endOffset": 104}, {"referenceID": 22, "context": ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).", "startOffset": 82, "endOffset": 94}, {"referenceID": 17, "context": ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).", "startOffset": 82, "endOffset": 94}, {"referenceID": 6, "context": "Compared with the tensor-variate Gaussian process approaches [7, 23], the t-processbased InfTucker is more robust (not sensitive to data outliers).", "startOffset": 61, "endOffset": 68}, {"referenceID": 21, "context": "Compared with the tensor-variate Gaussian process approaches [7, 23], the t-processbased InfTucker is more robust (not sensitive to data outliers).", "startOffset": 61, "endOffset": 68}, {"referenceID": 4, "context": "Recently several nonparametric Bayesian models on matrices and tensors [5, 23, 4] have been developed, but they are either designed for different tasks (e.", "startOffset": 71, "endOffset": 81}, {"referenceID": 21, "context": "Recently several nonparametric Bayesian models on matrices and tensors [5, 23, 4] have been developed, but they are either designed for different tasks (e.", "startOffset": 71, "endOffset": 81}, {"referenceID": 3, "context": "Recently several nonparametric Bayesian models on matrices and tensors [5, 23, 4] have been developed, but they are either designed for different tasks (e.", "startOffset": 71, "endOffset": 81}, {"referenceID": 9, "context": "several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD) [10], Weighted CP [1] and nonnegative tensor decomposition [17].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD) [10], Weighted CP [1] and nonnegative tensor decomposition [17].", "startOffset": 129, "endOffset": 132}, {"referenceID": 15, "context": "several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD) [10], Weighted CP [1] and nonnegative tensor decomposition [17].", "startOffset": 170, "endOffset": 174}, {"referenceID": 8, "context": "As in Kolda and Bader [9], We collectively denote the group of K matrices as a Tucker tensor with a identity core U = [ U, .", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "The alternating least square (ALS) method has been used to solve both Tucker decomposition and CP [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "Other noise models include modified probit models for ordinal regression and multiclass classification [2], null category noise models for semi-supervised classification [12].", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "Other noise models include modified probit models for ordinal regression and multiclass classification [2], null category noise models for semi-supervised classification [12].", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "1 Variational EM for binary classification We follow the data augmentation scheme by Albert and Chib [2] to decompose the probit model into p(yi|mi) = \u222b", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": ", q(Z), in (10) at a time, while having all the other approximate distributions fixed [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 14, "context": "With an l1 penalty on f(U), we choose a projected scaled subgradient L-BFGS algorithm for optimization\u2014due to its excellent performance [16].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "The InfTucker model extends Probabilistic PCA (PPCA) [20] and Gaussian process latent variable models (GPLVMs) [11]: while PPCA and GPLVM model interactions of one mode of a matrix and ignore the joint interactions of two modes, InfTucker does.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "The InfTucker model extends Probabilistic PCA (PPCA) [20] and Gaussian process latent variable models (GPLVMs) [11]: while PPCA and GPLVM model interactions of one mode of a matrix and ignore the joint interactions of two modes, InfTucker does.", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "[23], Yu and Chu [22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23], Yu and Chu [22].", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "Our model is closely related to the probabilistic Tucker-3 (pTucker) model [5]; actually InfTucker reduces to pTucker as a special case as \u03bd \u2192 \u221e.", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "Also, Hoff [7] proposed a hierarchical Bayesian extension to CANDECOMP/PARAFAC that captures the interaction of component matrices.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "Unlike both [5]\u2019s and Hoff [7]\u2019s approach, ours handles non-Gaussian noise and uses nonlinear covariance functions to model complex interactions.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Unlike both [5]\u2019s and Hoff [7]\u2019s approach, ours handles non-Gaussian noise and uses nonlinear covariance functions to model complex interactions.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "In addition, [7] used a Gibbs sampler for inference\u2014its convergence incurs high computational cost and makes this approach infeasible for tensors with moderate and large sizes.", "startOffset": 13, "endOffset": 16}, {"referenceID": 15, "context": "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 7, "context": "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 12, "context": "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 13, "context": "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].", "startOffset": 209, "endOffset": 224}, {"referenceID": 0, "context": "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].", "startOffset": 265, "endOffset": 268}, {"referenceID": 3, "context": "Finally, note that the inference technique described in Section 4 can be adopted for Gaussian process or t-process multi-task learning [4, 25].", "startOffset": 135, "endOffset": 142}], "year": 2017, "abstractText": "Tensor decomposition is a powerful tool for multiway data analysis. Many popular tensor decomposition approaches\u2014such as the Tucker decomposition and CANDECOMP/PARAFAC (CP)\u2014conduct multi-linear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g. missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose a tensor-variate latent t process model, InfTucker , for robust multiway data analysis: it conducts robust Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, it handles both continuous and binary data in a probabilistic framework. Unlike previous nonparametric Bayesian models on matrices and tensors, our latent t-process model focuses on multiway analysis and uses nonlinear covariance functions. To efficiently learn InfTucker from data, we develop a novel variational inference technique on tensors. Compared with classical implementation, the new technique reduces both time and space complexities by several orders of magnitude. This technique can be easily adopted in other contexts (e.g. , multitask learning) where we encounter tensor-variate t processes or Gaussian processes. Our experimental results on chemometrics and social network datasets demonstrate that the new InfTucker model achieves significantly higher prediction accuracy than several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD), Weighted CP, and nonnegative tensor decomposition.", "creator": "LaTeX with hyperref package"}}}