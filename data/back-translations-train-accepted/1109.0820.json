{"id": "1109.0820", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2011", "title": "ShareBoost: Efficient multiclass learning with feature sharing", "abstract": "Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of ShareBoost and evaluate its success relatively to other state-of-the-art approaches.", "histories": [["v1", "Mon, 5 Sep 2011 07:52:17 GMT  (156kb,D)", "http://arxiv.org/abs/1109.0820v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV stat.ML", "authors": ["shai shalev-shwartz", "yonatan wexler", "amnon shashua"], "accepted": true, "id": "1109.0820"}, "pdf": {"name": "1109.0820.pdf", "metadata": {"source": "META", "title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing", "authors": ["Shai Shalev-Shwartz", "Yonatan Wexler"], "emails": ["shais@cs.huji.ac.il", "yonatan.wexler@orcam.com", "shashua@cs.huji.ac.il"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of us are able to play by the rules we have set ourselves to change the world, \"he said in an interview with The New York Times,\" It's not as if we are able to change the world. \""}, {"heading": "1.1. Formal problem statement", "text": "We assume that x is the set of possible classes of Y = 1. Our goal is to learn a multicultural dictatorship, which is an object of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1. Our goal is to learn a multicultural dictatorship, which is an object of Y = 1. We assume that x is the set of possible classes of Y = 1. We assume that x is the set of possible classes of Y = 1."}, {"heading": "1.2. Related Work", "text": "In fact, most of them are able to survive on their own, while others are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...)"}, {"heading": "2. The ShareBoost Algorithm", "text": "ShareBoost is a forward-looking, greedy selection approach to solving eqn. (4) q = qy (4) q = qy (4). Normally, in a greedy approach, we update the weight of a feature at a time. \u2212 Since W is a matrix of W, we have L (W) a matrix of the partial derivatives of L. Denote is maximized by the method of L (W) the r'th column of L (W), i.e. the vector (W) is a matrix of L (W) is a matrix of the partial derivatives of L. Denote to L (W). \u00b7 rL (W) the r'th column of L (W), i.e. the vector (W) is L (W). \u2212 rnd: L (W) sp."}, {"heading": "3. Variants of ShareBoost", "text": "We will now describe several variants of ShareBoost. The analysis we present in Section 5 can also be easily customized for these variants."}, {"heading": "3.1. Modifying the Greedy Choice Rule", "text": "ShareBoost chooses the feature r that maximizes the \"1 standard of the r-th column of the gradient matrix. Our analysis shows that this choice leads to a sufficient reduction in the objective function. However, one can easily develop other ways to choose a feature that can potentially lead to an even greater reduction in the lens. For example, we can choose a feature r that minimizes L (W) over the matrices W with the support of I-R. This leads to the maximum possible reduction in the objective function in the current iteration. Of course, the runtime of the choice of r will now be much longer. Some intermediate options are to choose r that minimizes min \u03b1-RW + \u03b1-RR (W), or r that minimizes the minimum w-RkW + we \u2020 r, where e \u2020 r is the row vector with the exception of 1 in the r position."}, {"heading": "3.2. Selecting a Group of Features at a Time", "text": "In some situations, attributes can be divided into groups where the runtime for calculating a single attribute in each group is almost the same as the runtime for calculating all attributes in the group. In such cases, it makes sense to select attribute groups for each iteration of ShareBoost. This can easily be done by simply selecting attribute group J, which provides maximum dimensions."}, {"heading": "3.3. Adding Regularization", "text": "Our analysis implies that if | S | is much higher than O-value (Tk), ShareBoost will not miss. If this is not the case, we can integrate regulation into ShareBoost's goal to prevent overmatch. A simple way is to add a Frobenius standardization term of the form \u03bb i, jW 2 i, j to the objective function L (W), where \u03bb is a regulation parameter. It is easy to verify that it is a smooth and convex function, and therefore we can easily adapt ShareBoost to this regulated goal. It is also possible to rely on other standards such as the \"1 standard\" or the \"1 mixed standard.\" However, there is a technical peculiarity due to the fact that these standards are not smooth. We can overcome this problem by defining smooth approximations to these standards."}, {"heading": "4. Non-Linear Prediction Rules", "text": "The basic idea is similar to the approach of boosting and SVM. That is, we construct a nonlinear predictor by first mapping the original characteristics into a higher dimensional space and then learning a linear predictor in that space that corresponds to a nonlinear predictor above the original attribute space. To illustrate this idea, we present two concrete mappings: the first is the method of decision stub, which is widely used by boosting algorithms; the second approach shows how ShareBoost can be used to learn piecemeal linear predictors and is inspired by the super vector construction recently described in (Zhou et al., 2010)."}, {"heading": "4.1. ShareBoost with Decision Stumps", "text": "To construct a nonlinear predictor, we can convert any object v into a feature vector x that contains all possible decision stumps. Of course, the dimensionality of x is very large (in fact, it may even be infinite), and the calculation of step 4 of ShareBoost can take forever. Fortunately, a simple trick leads to an efficient solution. First, for each i, all stump characteristics that match i can get at most m + 1 values on a training set of size m. Therefore, if we sort the values of vi over the m examples in the training set, we can efficiently implement the value of the right side of eqn. (7) for all possible values of 2001 in the total time of O (m)."}, {"heading": "4.2. Learning Piece-wise Linear Predictors with ShareBoost", "text": "In order to motivate our next construction, let us first consider a simple one-dimensional functional problem. In view of the examples q (x1, yi),.., (xm, ym), we would like to find a function f: R \u2192 R so that f (xi) \u2248 yi forall i. The class of piecemeal linear functions can be a good candidate for the approximation function f. See, for example, a figure in Fig. 1. In fact, it is easy to verify that all smooth functions can be approximated by piecemeal linear functions (see, for example, the discussion in (Zhou et al., 2010)). In general, we can asf (v) = q \u00b2 j = 1 [vj \u00b2 < rj] (< uj, v > + bj), where q is the number of parts (uj, bj), multi-model."}, {"heading": "5. Analysis", "text": "In this section we provide formal guarantees for the ShareBoost algorithm. Evidence is moved to the appendix. First, we show that if the algorithm has managed to find a matrix W with a small number of non-zero columns and a small training error, then the generalization error of W. The limit given below is in relation to the 0 \u2212 1 loss. A related limit specified in relation to the convex loss function is described in (Zhang, 2004). Theorem 1 Let us assume that the ShareBoost algorithm runs for T iterations and let W be its output matrix. Then, with a probability of at least 1 \u2212 \u043c above the selection of the training set S, we have this P (x, y) solution."}, {"heading": "6. Feature Sharing \u2014 Illustrative Examples", "text": "In this section, we will present illustrative examples that show that whenever a strong feature ShareBoost is possible, ShareBoost will find it (>), while there can be no solutions with a small number of attributes. (\u2212) In analyzing the examples below, we will use the following simple sequence of Theorem 1. Assuming that there is a matrix W (W), so that L (W?) all W? entries are in [\u2212 c], and the number of attributes that ShareBoost requires (as well as mixed standards) and the number of attributes needed by \"2 or.\" Consider a series of examples so that each example has an exponential gap between the attributes that ShareBoost requires (as mixed standards) and the number of attributes required by \"2 or\" 1."}, {"heading": "7. Experiments", "text": "In this section, we demonstrate the merits (and pitfalls) of ShareBoost by comparing it with alternative algorithms in various scenarios; the first experiment illustrates the characteristics that ShareBoost has in common; we conduct experiments with an OCR dataset and show a slight increase in the number of features as the number of classes increases from 2 to 36; the second experiment compares ShareBoost with the mixed norm regulation and the JointBoost algorithm of (Torralba et al., 2007); we follow the same experimental setup as in (Duchi & Singer, 2009); the main finding is that ShareBoost outperforms the mixed norm regulation method when the output predictor has to be very economical, while mixed norm regulation can be better in the regime of fairly dense predictors; and we show that ShareBoost is both faster and more precise than JointBoost; the third and last experiment is on the handwritten MT, where we demonstrate the highest accuracy."}, {"heading": "7.1. Feature Sharing", "text": "To demonstrate this feature of ShareBoost, we experimented with the Char74k dataset, which consists of figures and letters. We trained ShareBoost on the number of classes that vary from 2 classes to the 36 classes that correspond to the 10 digits and 26 uppercase letters. We calculated how many features were needed to achieve a certain fixed accuracy depending on the number of classes. Section 7.4 describes the attribute space. We compared ShareBoost with the 1-for-rest approach, where we trained each binary classifier using the same mechanism that ShareBoost uses."}, {"heading": "7.2. Comparing ShareBoost to Mixed-Norms Regularization", "text": "Our next experiment compares ShareBoost with the use of the regularization of mixed standards (see eqn. (5)) as surrogates for the non-convex splitting constraint. See Section 1.2 for the description of the approach. To make the comparison fair, we used the same experimental setup as in (Duchi & Singer, 2009) (using the code provided by). We calculated the entire regularization path for the regularization of mixed standards by running the algorithm of (Duchi & Singer, 2009) with many values of the regularization parameter. In Fig. 3, we plot the results on three UCI datasets: StatLog, Pendigits, and Isolet. The number of classes for the datasets is 7,10,26. The original dimensionality of these datasets is not very high, and therefore, after that (Dumalchi & Singer, 2009), we expanded the functions by adding all products over pairing characteristics."}, {"heading": "7.3. Comparing ShareBoost to JointBoost", "text": "As in the previous experiment, we followed the setup as in (Duchi & Singer, 2009) and performed JointBoost of (Torralba et al., 2007) using their published code with additional implementation of BFS heuristics to trim the 2k space of all class subgroups as described in their papers. Fig.3 (below) shows the results. Here, we used stump features for both algorithms, as these are needed for JointBoost. As can be seen, ShareBoost reduces the error much faster and is therefore preferable if the target is a rather sparse solution. As in the previous experiment, we observe that ShareBoost begins to overmatch when more features are allowed."}, {"heading": "7.4. MNIST Handwritten Digits Dataset", "text": "The goal of this experiment is to show that ShareBoost is able to describe the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the way he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner and he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner he behaves in the manner and he behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaves in the manner he has behaved in the manner he has behaves in the manner and he has behaved in the manner he has behaves in the manner and he has behaved in the manner he has behaved in the manner and he has behaved in the manner and he has behaved in the manner he has behaves in the manner he has behaved in the manner in the manner in the manner and has behaves in the manner and has behaves in the manner and has behaves in the manner and has behaves in the manner and has behaves"}, {"heading": "7.5. Comparing ShareBoost to kernel-based SVM", "text": "In the experiments on the MNIST dataset mentioned above, each attribute is the maximum response to the reduction of the SVM value, weighted by a spatial mask. One might wonder whether the stellar performance of ShareBoost is perhaps due to the error rate of the SVM value. In this section, we remove doubts by using a piecemeal linear predictor, as described in Section 4.2, for generic properties. We show that ShareBoost comes close to the error rate of the SVM value, while it requires only 230 anchor points, far below the number of support vectors required by kernel SVM."}, {"heading": "8. Acknowledgements", "text": "We would like to thank Itay Erlich and Zohar BarYehuda for their dedicated contribution to the introduction of ShareBoost."}, {"heading": "A. Proofs", "text": "The proof of the theorem 1The proof is based on an analysis of the Natarajan dimension = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "In NIPS, pp", "citeRegEx": "Argyriou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2006}, {"title": "Consistency of the group lasso and multiple kernel learning", "author": ["F.R. Bach"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE PAMI,", "citeRegEx": "Belongie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Belongie et al\\.", "year": 2002}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Decoding by linear programming", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "Candes and Tao,? \\Q2005\\E", "shortCiteRegEx": "Candes and Tao", "year": 2005}, {"title": "Deep big simple neural nets excel on handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L. Maria", "J. Schmidhuber"], "venue": null, "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer,? \\Q2003\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2003}, {"title": "Multiclass learnability and the erm principle", "author": ["A. Daniely", "S. Sabato", "S. Ben-David", "S. ShalevShwartz"], "venue": "In COLT,", "citeRegEx": "Daniely et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2011}, {"title": "Greedy adaptive approximation", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation,", "citeRegEx": "Davis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1997}, {"title": "Training invariant support vector machines", "author": ["D. Decoste", "S. Bernhard"], "venue": "Mach. Learn.,", "citeRegEx": "Decoste and Bernhard,? \\Q2002\\E", "shortCiteRegEx": "Decoste and Bernhard", "year": 2002}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "Technical Report, Stanford University,", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Boosting with structural sparsity", "author": ["J. Duchi", "Y. Singer"], "venue": "In Proc. ICML, pp", "citeRegEx": "Duchi and Singer,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer", "year": 2009}, {"title": "Online multiclass learning by interclass hypothesis sharing", "author": ["M. Fink", "S. Shalev-Shwartz", "Y. Singer", "S. Ullman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Fink et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fink et al\\.", "year": 2006}, {"title": "A short introduction to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. of Japanese Society for AI,", "citeRegEx": "Freund and Schapire,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani,? \\Q1995\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1995}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Hsu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2010}, {"title": "The benefit of group sparsity", "author": ["J. Huang", "T. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Huang and Zhang,? \\Q2010\\E", "shortCiteRegEx": "Huang and Zhang", "year": 2010}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D.N. Metaxas"], "venue": "In ICML,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P.L. Bartlett", "Ghaoui", "L. El", "M.I. Jordan"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y.L. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fast group sparse classification", "author": ["A. Majumdar", "R.K. Ward"], "venue": "Electrical and Computer Engineering, Canadian Journal of,", "citeRegEx": "Majumdar and Ward,? \\Q2009\\E", "shortCiteRegEx": "Majumdar and Ward", "year": 2009}, {"title": "Sparse approximate solutions to linear systems", "author": ["B. Natarajan"], "venue": "SIAM J. Computing,", "citeRegEx": "Natarajan,? \\Q1995\\E", "shortCiteRegEx": "Natarajan", "year": 1995}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Y. Nesterov", "I.U.E. Nesterov"], "venue": null, "citeRegEx": "Nesterov and Nesterov,? \\Q2004\\E", "shortCiteRegEx": "Nesterov and Nesterov", "year": 2004}, {"title": "Transfer learning for image classification with sparse prototype representations", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Quattoni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2008}, {"title": "An efficient projection for l 1,infinity regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "In ICML, pp", "citeRegEx": "Quattoni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["S. Shalev-Shwartz", "T. Zhang", "N. Srebro"], "venue": "Siam Journal on Optimization,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "Dave", "Platt", "John C"], "venue": "Document Analysis and Recognition,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Learning to learn: Introduction", "author": ["S. Thrun"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Thrun,? \\Q1996\\E", "shortCiteRegEx": "Thrun", "year": 1996}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc B.,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Sharing visual features for multiclass and multiview object detection", "author": ["A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "Torralba et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2007}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Tropp and Gilbert,? \\Q2007\\E", "shortCiteRegEx": "Tropp and Gilbert", "year": 2007}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["E. Xing", "A.Y. Ng", "M. Jordan", "S. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Class-size independent generalization analysis of some discriminative multi-category classification", "author": ["T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Image classification using super-vector coding of local image descriptors", "author": ["X. Zhou", "K. Yu", "T. Zhang", "T. Huang"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 23, "context": "While, in general, finding the most accurate sparse predictor is known to be NP hard (Natarajan, 1995; Davis et al., 1997), two main approaches have been proposed for overcoming the hardness result.", "startOffset": 85, "endOffset": 122}, {"referenceID": 9, "context": "While, in general, finding the most accurate sparse predictor is known to be NP hard (Natarajan, 1995; Davis et al., 1997), two main approaches have been proposed for overcoming the hardness result.", "startOffset": 85, "endOffset": 122}, {"referenceID": 32, "context": "the Lasso algorithm (Tibshirani, 1996) and the compressed sensing literature (Candes & Tao, 2005; Donoho, 2006)).", "startOffset": 20, "endOffset": 38}, {"referenceID": 11, "context": "the Lasso algorithm (Tibshirani, 1996) and the compressed sensing literature (Candes & Tao, 2005; Donoho, 2006)).", "startOffset": 77, "endOffset": 111}, {"referenceID": 26, "context": "An alternative approach, which has recently been studied in (Quattoni et al., 2009; Duchi & Singer, 2009), generalizes the `1 norm based approach, and relies on mixednorms.", "startOffset": 60, "endOffset": 105}, {"referenceID": 36, "context": "This logistic multiclass loss function `(W, (x, y)) has several nice properties \u2014 see for example (Zhang, 2004).", "startOffset": 98, "endOffset": 111}, {"referenceID": 23, "context": "(4) is NP-hard (Natarajan, 1995; Davis et al., 1997).", "startOffset": 15, "endOffset": 52}, {"referenceID": 9, "context": "(4) is NP-hard (Natarajan, 1995; Davis et al., 1997).", "startOffset": 15, "endOffset": 52}, {"referenceID": 30, "context": "(Taskar et al., 2003)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 36, "context": "While this approach can yield predictors with a rather mild dependency of the required features on k (see for example the analysis in (Zhang, 2004; Taskar et al., 2003; Fink et al., 2006)), it relies on a-priori assumptions on the structure of X and Y.", "startOffset": 134, "endOffset": 187}, {"referenceID": 30, "context": "While this approach can yield predictors with a rather mild dependency of the required features on k (see for example the analysis in (Zhang, 2004; Taskar et al., 2003; Fink et al., 2006)), it relies on a-priori assumptions on the structure of X and Y.", "startOffset": 134, "endOffset": 187}, {"referenceID": 13, "context": "While this approach can yield predictors with a rather mild dependency of the required features on k (see for example the analysis in (Zhang, 2004; Taskar et al., 2003; Fink et al., 2006)), it relies on a-priori assumptions on the structure of X and Y.", "startOffset": 134, "endOffset": 187}, {"referenceID": 26, "context": "However, as pointed out in (Quattoni et al., 2009), these regularizers might yield a matrix with many nonzeros columns, and hence, will lead to a predictor that uses many features.", "startOffset": 27, "endOffset": 50}, {"referenceID": 20, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 1, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 2, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 26, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 28, "context": "ShareBoost generalizes the fully corrective greedy selection procedure given in (Shalev-Shwartz et al., 2010) to the case of selection of groups of variables, and our analysis follows similar techniques.", "startOffset": 80, "endOffset": 109}, {"referenceID": 19, "context": "Obtaining group sparsity by greedy methods has been also recently studied in (Huang et al., 2009; Majumdar & Ward, 2009), and indeed, ShareBoost shares similarities with these works.", "startOffset": 77, "endOffset": 120}, {"referenceID": 19, "context": "We differ from (Huang et al., 2009) in that our analysis does not impose strong assumptions (e.", "startOffset": 15, "endOffset": 35}, {"referenceID": 19, "context": "In (Huang et al., 2009), a ratio between difference in objective and different in costs is used.", "startOffset": 3, "endOffset": 23}, {"referenceID": 33, "context": "Another related method is the JointBoost algorithm (Torralba et al., 2007).", "startOffset": 51, "endOffset": 74}, {"referenceID": 33, "context": "While the original presentation in (Torralba et al., 2007) seems rather different than the type of predictors we describe in eqn.", "startOffset": 35, "endOffset": 58}, {"referenceID": 33, "context": "In practice, (Torralba et al., 2007) relies on heuristics for finding C on each boosting step.", "startOffset": 13, "endOffset": 36}, {"referenceID": 31, "context": "Finally, we mention that feature sharing is merely one way for transferring information across classes (Thrun, 1996) and several alternative ways have been proposed in the literature such as target embedding (Hsu et al.", "startOffset": 103, "endOffset": 116}, {"referenceID": 17, "context": "Finally, we mention that feature sharing is merely one way for transferring information across classes (Thrun, 1996) and several alternative ways have been proposed in the literature such as target embedding (Hsu et al., 2010; Bengio et al., 2011), shared hidden structure (LeCun et al.", "startOffset": 208, "endOffset": 247}, {"referenceID": 4, "context": "Finally, we mention that feature sharing is merely one way for transferring information across classes (Thrun, 1996) and several alternative ways have been proposed in the literature such as target embedding (Hsu et al., 2010; Bengio et al., 2011), shared hidden structure (LeCun et al.", "startOffset": 208, "endOffset": 247}, {"referenceID": 21, "context": ", 2011), shared hidden structure (LeCun et al., 1998; Amit et al., 2007), shared prototypes (Quattoni et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 0, "context": ", 2011), shared hidden structure (LeCun et al., 1998; Amit et al., 2007), shared prototypes (Quattoni et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 25, "context": ", 2007), shared prototypes (Quattoni et al., 2008), or sharing underlying metric (Xing et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 35, "context": ", 2008), or sharing underlying metric (Xing et al., 2003).", "startOffset": 38, "endOffset": 57}, {"referenceID": 37, "context": "The second approach shows how to use ShareBoost for learning piece-wise linear predictors and is inspired by the super-vectors construction recently described in (Zhou et al., 2010).", "startOffset": 162, "endOffset": 181}, {"referenceID": 37, "context": "In fact, it is easy to verify that all smooth functions can be approximated by piece-wise linear functions (see for example the discussion in (Zhou et al., 2010)).", "startOffset": 142, "endOffset": 161}, {"referenceID": 36, "context": "A related bound, which is given in terms of the convex loss function, is described in (Zhang, 2004).", "startOffset": 86, "endOffset": 99}, {"referenceID": 33, "context": "The second experiment compares ShareBoost to mixed-norm regularization and to the JointBoost algorithm of (Torralba et al., 2007).", "startOffset": 106, "endOffset": 129}, {"referenceID": 33, "context": "Here we compare ShareBoost to the JointBoost algorithm of (Torralba et al., 2007).", "startOffset": 58, "endOffset": 81}, {"referenceID": 33, "context": "As in the previous experiment, we followed the experimental setup as in (Duchi & Singer, 2009) and ran JointBoost of (Torralba et al., 2007) using their published code with additional implementation of the BFS heuristic for pruning the 2 space of all class-subsets as described in their paper.", "startOffset": 117, "endOffset": 140}, {"referenceID": 3, "context": "For example, the Shape Context similarity measure introduced by (Belongie et al., 2002) uses a Bipartite matching algorithm between descriptors computed along 100 points in each image.", "startOffset": 64, "endOffset": 87}, {"referenceID": 6, "context": "The top MNIST performer (Ciresan et al., 2010) uses a feed-forward Neural-Net with 7.", "startOffset": 24, "endOffset": 46}, {"referenceID": 29, "context": "During training, geometrically distorted versions of the original examples were generated in order to expand the training set following (Simard et al., 2003) who introduced a warping scheme for that purpose.", "startOffset": 136, "endOffset": 157}, {"referenceID": 29, "context": "The set was expanded with 5 deformed versions of each input, using the method in (Simard et al., 2003).", "startOffset": 81, "endOffset": 102}], "year": 2011, "abstractText": "Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of ShareBoost and evaluate its success relatively to other state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}