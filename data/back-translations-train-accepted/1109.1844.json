{"id": "1109.1844", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2011", "title": "Weighted Clustering", "abstract": "In this paper we investigate clustering in the weighted setting, in which every data point is assigned a real valued weight. We conduct a theoretical analysis on the influence of weighted data on standard clustering algorithms in each of the partitional and hierarchical settings, characterising the precise conditions under which such algorithms react to weights, and classifying clustering methods into three broad categories: weight-responsive, weight-considering, and weight-robust. Our analysis raises several interesting questions and can be directly mapped to the classical unweighted setting.", "histories": [["v1", "Thu, 8 Sep 2011 20:53:54 GMT  (12kb)", "https://arxiv.org/abs/1109.1844v1", null], ["v2", "Tue, 4 Oct 2016 08:33:09 GMT  (44kb,D)", "http://arxiv.org/abs/1109.1844v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["margareta ackerman", "shai ben-david", "simina br\u00e2nzei", "david loker"], "accepted": true, "id": "1109.1844"}, "pdf": {"name": "1109.1844.pdf", "metadata": {"source": "CRF", "title": "Weighted Clustering", "authors": ["Margareta Ackerman", "Shai Ben-David", "David Loker"], "emails": ["mackerman@fsu.edu", "shai@cs.uwaterloo.ca", "simina.branzei@gmail.com", "dloker@cs.uwaterloo.ca"], "sections": [{"heading": null, "text": "So far, these properties have focused on the advantages of classic linkage-based algorithms, without recognizing when other cluster paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that outline the differences between common cluster paradigms, clearly and formally demonstrating the advantages of centered approaches for some applications. these properties address how sensitive algorithms are to changes in element frequencies that we detect in a generalized environment in which each element is associated with a real weight. \u043d E-mail: mackerman @ fsu.edu \u2020 E-mail: shai @ cs.uwaterloo.ca \u2021 E-mail: simina.branzei @ gmail.com \u00a7 E-mail: dloker @ cs.uwaterloo.caar Xiv: 110 9.18 44v2 [cs.LG] 4 O"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "1.1 Related Work", "text": "The weighted cluster structure was briefly considered in the early 1970s, but has not evolved to this day. [9] Several characteristics of cluster algorithms have been introduced, including the permissibility of \"point proportions,\" which require that the results of an algorithm should not change when these points are duplicated; they then observe that a few algorithms allow a point ratio. Furthermore, cluster algorithms can exhibit a much broader range of behaviors on weighted data than merely satisfying or not satisfying the permissibility of points. We are conducting the first comprehensive analysis of clusters on weighted data by characterizing the exact conditions under which algorithms react to weighted properties. [21] A formalization of cluster analyses consisting of eleven axioms has been proposed."}, {"heading": "2 Preliminaries", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "3 Basic Categories", "text": "Different cluster algorithms react radically differently to weighted data. In this section, we introduce a formal categorization of cluster algorithms based on their response to weights. This categorization identifies fundamental differences between cluster paradigms while highlighting when some of the more empirically successful methods should be used. These simple properties can help clusters select suitable methods by simply considering how an appropriate algorithm should respond to element duplication. After presenting the three categories, we show a classification of some known cluster methods according to their response to weight, summarized in Table 1."}, {"heading": "3.1 Weight Robust Algorithms", "text": "Weight robustness requires that the output of the algorithm be unaffected by changes in the element weights (or the number of occurrences of each point in the unweighted setting).This category is closely related to the \"permissibility of the point ratio\" by [9].Definition 3 (weight robust (partial).A partitional algorithm is weight robust if for all (X, d) and 1 < k < | X | range (A (X, d, k)) | = 1. Definition in the hierarchical setting is analogous. Definition 4 (weight robust (hierarchical).A hierarchical algorithm is more weight robust if for all (X, d), | range (A (X, d)) | range in which the weights are ductive."}, {"heading": "3.2 Weight Sensitive Algorithms", "text": "We present the definition of \"weight sensitive\" algorithms. Definition 5 (Weight Sensitive (Partitional). A partial algorithm is weight sensitive if it applies to all (X, D) and 1 < K < K (A, D). The definition is analogous to hierarchical algorithms. Definition 6 (Weight Sensitive (Hierarchy). A hierarchical algorithm is weight sensitive if it applies to all (X, D)."}, {"heading": "3.3 Weight Considering Algorithms", "text": "Definition 7 (Weight Considering (Partitional)). A partitional algorithm A is weighted if it exists (X, d) and 1 < k < K < K (A, d, k). Definition 8 (Weight Considering) and 1 < K (Weight Considering). Definition 8 (Weight Considering) and 1 (Weight Considering). Definition 8 (Weight Considering). Definition 8 (Weight Considering) and 1 < K (Weight Considering). Definition A is weighted if it exists (X, d, k)."}, {"heading": "4 Partitional Methods", "text": "In this section, we show that partitional cluster algorithms react to weights in different ways. Many popular partitional cluster paradigms, including k mean, k median, and min sum, are weight sensitive. It is easy to see that methods such as min diameter and k center are weighted. We begin by analyzing the behavior of a spectral objective function ratio that shows interesting behavior on weighted data by responding to weight, unless the data is highly structured."}, {"heading": "4.1 Ratio-Cut Clustering", "text": "We study the behavior of a spectral objective function that responds to weights, initially a few."}, {"heading": "5 Hierarchical Algorithms", "text": "We show that Ward's method ([20]), a successful linkage-based algorithm, as well as popular hierarchical methods, are weight-sensitive. On the other hand, it is easy to see that the linkage-based algorithms simple linkage and full linkage are both weight-stable, as observed in [9]. Average linkage, another popular linkage-based method, exhibits more differentiated behavior on weighted data. If a cluster fulfills a reasonable notion of clustering capability, average linkage recognizes this clustering independently of weights. On the other hand, this algorithm responds to weights on all other clustering. We find that the notion of clustering capability required for average linkage is much weaker than the notion of clustering capability used to characterize the behavior of weighted data."}, {"heading": "5.1 Average Linkage", "text": "In fact, most people who are able to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "5.2 Ward\u2019s Method", "text": "Ward's method is a highly effective cluster algorithm ([20]) that merges the clusters that yield the minimum increase to the square-sum error (the k-means-objective function) at each step. Let ctr (X, d, w) contain the center of gravity of the mass of the dataset (w [X], d), then the link function for Ward's method is' Ward (X1, X2, d, w) = w (X1) \u00b7 w (X2) \u00b7 d (ctr (X1, d, w), ctr (X2, d)) 2w (X1) + w (X2), where X1 and X2 are root subsets (clusters) of X.Theorem 5.2. Ward's method is weight-sensitive. Proof. Consider any dataset (X, d) and any cluster output of C (X, d)."}, {"heading": "5.3 Divisive Algorithms", "text": "The class of dividing cluster algorithms is a well-known family of hierarchical algorithms that construct the dendrogram using a top-down approach. This family of algorithms includes the popular bisecting k mean algorithm. We show that a class of algorithms that contains the k mean consists of weight-sensitive methods. Considering a node x in the dendrogram (T, M), let C (x) designate the cluster represented by the node x. That is, C (x) = {M (y) | y is a leaf and a descendant of x}.Formally, a P dividing algorithm is a hierarchical cluster algorithm that uses a partitional cluster algorithm P to divide the dataset recursively into two clusters until only individual elements remain. Formally, a P dividing algorithm is known as xx.Definition (12-dividing a cluster algorithm that is a data algorithm that divides two)."}, {"heading": "6 Heuristic Approaches", "text": "We have seen how weights affect different algorithms that optimize different clustering objectives. As the optimization of a clustering target is usually NP-hard, heuristics are used in practice. In this section, we will look at several common heuristic clustering approaches and show how they respond to weightings.We note that there are many algorithms that aim to find high-quality partitions for popular objective functions.For the k-mean lens alone, many different algorithms have been proposed, most of which provide different initializations for Lloyd's method.For example, [18] we have examined a dozen different initializations. There are many other algorithms based on the objective functions of k-means, some of which are most noteworthy randomizing on k-means + + ([4]) and tree-schmoys initialization ([10])."}, {"heading": "6.1 Partitioning Around Medoids (PAM)", "text": "Unlike the Lloyd method and k averages, PAM is a heuristic for copy-based objective functions such as k-medoide, which selects data points as centers (hence there is no need to calculate mass centers), so this approach can be applied to any data, not just to normalized vector spaces. Medoide (PAM) partitioning receives an initial set of k centers, T, and changes T iteratively to find a \"better\" set of k centers by sucking centers for other points in the dataset and calculating the cost function: [13]. Note that our results in this section are independent of how the initial k centers are selected (x, c) \u00b7 w (x). Each iteration performs a swap when better costs are possible, and stops when no changes are made [13]."}, {"heading": "6.2 Llyod method", "text": "The Lloyd algorithm can be combined with various approaches for seeding the initial centers. In this section, we begin by calculating the centers of mass of the resulting clusters by adding up the elements in each cluster and dividing by the number of elements in that partition, and assigning each element to its next new center. Continue until no change is made in an iteration.Lloyd method, dissimilarity (or distance) to the center can be both the \"1 norm and the square procedure. First, we consider the case when the k-initial centers are selected in a deterministic manner."}, {"heading": "7 Conclusion", "text": "We have studied the behavior of cluster algorithms based on weighted data and presented three basic categories that describe how such algorithms respond to weights, classifying several known algorithms according to these categories. Our results are summarized in Table 1. We note that all of our results can be applied directly to the default setting by mapping each integer point to the same number of unweighted duplicates. Our results can be used to select a cluster algorithm. In the asset allocation application discussed in the introduction, for example, where weights are of primary importance, a weight-sensitive algorithm is appropriate. Other applications may require weight-weighing algorithms. This can happen when weights (i.e. the number of duplicates) should not be ignored, but it is still desirable to identify rare instances that represent small but well-shaped outlier clusters. For example, this applies to patient data about potential causes of a rare disease that is critical."}], "references": [{"title": "Discerning linkage-based algorithms among hierarchical clustering methods", "author": ["M. Ackerman", "S. Ben-David"], "venue": "In IJCAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Characterization of linkage-based clustering", "author": ["M. Ackerman", "S. Ben-David", "D. Loker"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Towards property-based classification of clustering paradigms", "author": ["M. Ackerman", "S. Ben-David", "D. Loker"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In SODA,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["M.F. Balcan", "A. Blum", "S. Vempala"], "venue": "In STOC,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A uniqueness theorem for clustering", "author": ["R. Bosagh-Zadeh", "S. Ben-David"], "venue": "In UAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "How the initialization affects the stability of the k-means algorithm", "author": ["S\u00e9bastien Bubeck", "Marina Meila", "Ulrike von Luxburg"], "venue": "arXiv preprint arXiv:0907.5494,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Characterization, stability and convergence of hierarchical clustering methods", "author": ["Gunnar Carlsson", "Facundo M\u00e9moli"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Admissible clustering procedures", "author": ["L. Fisher", "J. Van Ness"], "venue": "Biometrika, 58:91\u2013104,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1971}, {"title": "A best possible heuristic for the k-center problem", "author": ["Dorit S Hochbaum", "David B Shmoys"], "venue": "Mathematics of operations research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Hierarchical clustering and the concept of space distortion", "author": ["Lawrence Hubert", "James Schultz"], "venue": "British Journal of Mathematical and Statistical Psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1975}, {"title": "The construction of hierarchic and non-hierarchic classifications", "author": ["Nicholas Jardine", "Robin Sibson"], "venue": "The Computer Journal,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1968}, {"title": "Partitioning Around Medoids (Program PAM)", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "An impossibility theorem for clustering", "author": ["J. Kleinberg"], "venue": "Proceedings of International Conferences on Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In FOCS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "P-complete approximation problems", "author": ["Sartaj Sahni", "Teofilo Gonzalez"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1976}, {"title": "K-means clustering: a half-century synthesis", "author": ["D. Steinley"], "venue": "British Journal of Mathematical and Statistical Psychology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Initializing k-means batch clustering: a critical evaluation of several techniques", "author": ["Douglas Steinley", "Michael J Brusco"], "venue": "Journal of Classification,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "J. Stat. Comput.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["Joe H Ward Jr."], "venue": "Journal of the American statistical association,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1963}, {"title": "A formalization of cluster analysis", "author": ["W.E. Wright"], "venue": "J. Pattern Recogn.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1973}], "referenceMentions": [{"referenceID": 1, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 5, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 2, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 0, "context": "Even the fairy basic problem of which algorithm to select for a given application (known as \u201cthe user\u2019s dilemma\u201d) is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods ([2, 6, 3, 1]).", "startOffset": 233, "endOffset": 245}, {"referenceID": 8, "context": "\u201cThe user\u2019s dilemma,\u201d has been tackled since the 70s ([9, 21]), yet we still do not have an adequate solution.", "startOffset": 54, "endOffset": 61}, {"referenceID": 20, "context": "\u201cThe user\u2019s dilemma,\u201d has been tackled since the 70s ([9, 21]), yet we still do not have an adequate solution.", "startOffset": 54, "endOffset": 61}, {"referenceID": 8, "context": "A formal approach to this problem (see, for example, [9, 6, 3]) proposes that we rely on succinct mathematical properties that reveal fundamental differences in the input-output behaviour of different clustering algorithms.", "startOffset": 53, "endOffset": 62}, {"referenceID": 5, "context": "A formal approach to this problem (see, for example, [9, 6, 3]) proposes that we rely on succinct mathematical properties that reveal fundamental differences in the input-output behaviour of different clustering algorithms.", "startOffset": 53, "endOffset": 62}, {"referenceID": 2, "context": "A formal approach to this problem (see, for example, [9, 6, 3]) proposes that we rely on succinct mathematical properties that reveal fundamental differences in the input-output behaviour of different clustering algorithms.", "startOffset": 53, "endOffset": 62}, {"referenceID": 16, "context": "According to these properties, there is never a reason to choose, say, algorithms based on the k-means objective ([17]), which often performs well in practice.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "For example, k-means can be reconfigured to output a dendrogram using Ward\u2019s method and Bisecting k-mean, and classical hierarchical methods can be terminated using a variety of termination conditions ([14]) to obtain a single partition instead of a dendrogram.", "startOffset": 202, "endOffset": 206}, {"referenceID": 8, "context": "[9] introduced several properties of clustering algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "In addition, [21] proposed a formalization of cluster analysis consisting of eleven axioms.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 5, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 2, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 13, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 169, "endOffset": 182}, {"referenceID": 0, "context": "Like earlier work, recent work on simple properties capturing differences in the input-output behaviour of clustering methods also focuses on the unweighed partitional ([2, 6, 3, 14]) and hierarchical settings ([1]).", "startOffset": 211, "endOffset": 214}, {"referenceID": 11, "context": "Particularly well-studied is the single-linkage algorithm, for which there are multiple property-based characterizations, showing that single-linkage is the unique algorithm that satisfies several sets of properties ([12, 6, 8]).", "startOffset": 217, "endOffset": 227}, {"referenceID": 5, "context": "Particularly well-studied is the single-linkage algorithm, for which there are multiple property-based characterizations, showing that single-linkage is the unique algorithm that satisfies several sets of properties ([12, 6, 8]).", "startOffset": 217, "endOffset": 227}, {"referenceID": 7, "context": "Particularly well-studied is the single-linkage algorithm, for which there are multiple property-based characterizations, showing that single-linkage is the unique algorithm that satisfies several sets of properties ([12, 6, 8]).", "startOffset": 217, "endOffset": 227}, {"referenceID": 1, "context": "More recently, the entire family of linkage-based algorithms was characterized ([2, 1]), differentiating those algorithms from other clustering paradigms by presenting some of the advantages of those methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 0, "context": "More recently, the entire family of linkage-based algorithms was characterized ([2, 1]), differentiating those algorithms from other clustering paradigms by presenting some of the advantages of those methods.", "startOffset": 80, "endOffset": 86}, {"referenceID": 2, "context": "In addition, previous property-based taxonomies in this line of work highlight the advantages of linkage-based methods ([3, 9]), and some early work focuses on properties that distinguish among linkage-based algorithms ([11]).", "startOffset": 120, "endOffset": 126}, {"referenceID": 8, "context": "In addition, previous property-based taxonomies in this line of work highlight the advantages of linkage-based methods ([3, 9]), and some early work focuses on properties that distinguish among linkage-based algorithms ([11]).", "startOffset": 120, "endOffset": 126}, {"referenceID": 10, "context": "In addition, previous property-based taxonomies in this line of work highlight the advantages of linkage-based methods ([3, 9]), and some early work focuses on properties that distinguish among linkage-based algorithms ([11]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 8, "context": "This category is closely related to \u201cpoint proportion admissibility\u201d by [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Indeed, when a similar property was proposed by [9], it was presented as a desirable characteristic.", "startOffset": 48, "endOffset": 51}, {"referenceID": 18, "context": "We investigate the behaviour of a spectral objective function, ratio-cut ([19]), on weighted data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "Many popular partitional clustering paradigms, including k-means (see [17] for a detailed exposition of this popular objective function and related algorithms), k-median, and the min-sum objective ([16]), are weight sensitive.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Many popular partitional clustering paradigms, including k-means (see [17] for a detailed exposition of this popular objective function and related algorithms), k-median, and the min-sum objective ([16]), are weight sensitive.", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "As shown by [15], the k-means objective function is equivalent to \u2211 x,y\u2208Ci d(x, y) 2 \u00b7 w(x) \u00b7 w(y) w(Ci) .", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "We show that Ward\u2019s method ([20]), a successful linkage-based algorithm, as well as popular divisive hierarchical methods, are weight sensitive.", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "On the other hand, it is easy to see that the linkage-based algorithms single-linkage and complete-linkage are both weight robust, as was observed in [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Ward\u2019s method is a highly effective clustering algorithm ([20]), which, at every step, merges the clusters that will yield the minimal increase to the sum-of-squares error (the k-means objective function).", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "For example, [18] studied a dozen different initializations.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "There are many other algorithms based on the k-means objective functions, some of the most notable being k-means++ ([4]) and the Hochbaum-Schmoys initialization ([10]) studied, for instance, by [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 9, "context": "There are many other algorithms based on the k-means objective functions, some of the most notable being k-means++ ([4]) and the Hochbaum-Schmoys initialization ([10]) studied, for instance, by [7].", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "There are many other algorithms based on the k-means objective functions, some of the most notable being k-means++ ([4]) and the Hochbaum-Schmoys initialization ([10]) studied, for instance, by [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 12, "context": "Each iteration performs a swap only if a better cost is possible, and it stops when no changes are made [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 2, "context": "For example, one deterministic seeding approach involves selecting the k-furthest centers (see, for example, [3]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "The k-means++ algorithm, introduced by Arthur and Vassilvitskii ([4]) is the Lloyd algorithm with a randomized initialization method that aims to place the initial centers far apart from each other.", "startOffset": 65, "endOffset": 68}], "year": 2016, "abstractText": "One of the most prominent challenges in clustering is \u201cthe user\u2019s dilemma,\u201d which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this problem relies on the identification of succinct, user-friendly properties that formally capture when certain clustering methods are preferred over others. Until now these properties focused on advantages of classical Linkage-Based algorithms, failing to identify when other clustering paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that delineate the differences between common clustering paradigms, which clearly and formally demonstrates advantages of centerbased approaches for some applications. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. \u2217E-mail: mackerman@fsu.edu \u2020E-mail: shai@cs.uwaterloo.ca \u2021E-mail: simina.branzei@gmail.com \u00a7E-mail: dloker@cs.uwaterloo.ca 1 ar X iv :1 10 9. 18 44 v2 [ cs .L G ] 4 O ct 2 01 6", "creator": "LaTeX with hyperref package"}}}