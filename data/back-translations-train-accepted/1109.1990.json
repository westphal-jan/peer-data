{"id": "1109.1990", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2011", "title": "Trace Lasso: a trace norm regularization for correlated designs", "abstract": "Using the $\\ell_1$-norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm, which is a convex surrogate of the rank, of the selected covariates as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net.", "histories": [["v1", "Fri, 9 Sep 2011 13:01:41 GMT  (129kb,D)", "http://arxiv.org/abs/1109.1990v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["edouard grave", "guillaume obozinski", "francis r bach"], "accepted": true, "id": "1109.1990"}, "pdf": {"name": "1109.1990.pdf", "metadata": {"source": "CRF", "title": "Trace Lasso: a trace norm regularization for correlated designs", "authors": ["Edouard Grave", "Guillaume Obozinski"], "emails": ["edouard.grave@inria.fr", "guillaume.obozinski@inria.fr", "francis.bach@inria.fr"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Definition and properties of the trace Lasso", "text": "We look at the problem of predicting y-R with a vector x-Rp, assuming a linear approach = w > x + \u03b5, where \u03b5 (Gaussian) noise is mean 0 with variance \u03c32. A widely used method of estimating the parameter vector w is punished empirical risk minimization w, while f is a regulatory term used to punish complex models. This second term helps to avoid overadjustment, especially in which we have many more parameters than observation, i.e., n p."}, {"heading": "2.1 Related work", "text": "The first regression, known as Tikhonov [16] or the second regression [17] is the fact that it does not perform the variable selection and is therefore not in the sparse high-dimensional settings. It is, of course, able to replace the linear models by the number of variables used."}, {"heading": "2.2 The ridge, the Lasso and the trace Lasso", "text": "In fact, the fact is that most of us are able to go in search of a solution that is capable, that is in a position, that is in a position, that is in a position, that is in a position, that is in a position, that is in a position, that is in a position to find a solution, that is in a position, that is in a position to find a solution, that is in a position, that is in a position to find a solution, that is in a position to find a solution, that is in a position to find a solution, that is in a position to find a solution."}, {"heading": "2.3 A new family of penalty functions", "text": "In this section, we present a new family of punitive measures, inspired by the Trace Lasso, which allows us to write the \"1-norm,\" the \"1-norm,\" the \"1-norm,\" the \"1-norm,\" the \"1-norm,\" the \"1-norm,\" the \"1-norm,\" the \"1-norm,\" the \"2-norm,\" the \"1-norm,\" the \"2-norm,\" the \"2-norm,\" the \"the\" 1-norm, \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the., \"the.\" the. \"the.\" the. \"the.\" the., \"the.\" the. \"the.\" the. \"the.,\" the. \"the.\" the. \"the.\" the. S., \"the.,\" the. the. \"the.\" the. \"the.\" the. \"the.\" the., \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. S., \"the.\" the., the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the.\" the. \"the\" the. \"the.\" the. \"the\" the. \"the.\" the \"the.\" the. \"the.\" the. \"the\" the. \"the.\" the. \"the.\" the \"the.\" the. \"the.\" the. \"the\" the \"the. S., the\" the. \"the\" the. \"the.\" the. \"the\" the. \"the.\" the \"the\" the. \"the.\" the \"the."}, {"heading": "2.4 Dual norm", "text": "The dual standard is an important quantity both for the optimization and for the theoretical analysis of the estimator. Unfortunately, in general, we are not able to obtain a closed form of expression of the dual standard for the family of standards just introduced. However, we can obtain a limit that is exact for some specific cases: Proposal 4. The dual standard, defined by the sentence \"P (u) = max\" P (v) \u2264 1 \"u > v,\" can be limited by: rule \"P (u) \u2264\" P \"Diag (v), the inequality being due to the fact that the operator standard\" P \"\u00b7 P\" = 1 is \"the dual standard of the track standard.\" The definition of the dual standard then results in the result."}, {"heading": "3 Optimization algorithm", "text": "In this section we present an algorithm for estimating the parameter vector w = > problem if the loss function is equal to the square loss: \"(y, w > x) = 12 (y \u2212 w > x) 2 and the penalty is the trace lasso. It is easy to extend this algorithm to the family of norms indexed by P. We consider the problem to be ismin w1 \u00b2 y \u2212 Xw \u00b2 and the rate of convergence of subgradient lineage is quite slow. Instead, we consider an iteratively reweighted method of the least squares. First, we must introduce a well-known formulation for the trace standard [22]."}, {"heading": "3.1 Choice of \u03bb", "text": "In fact, we know that the vector 0 is a solution if and only if we need to reduce the inequalities on the dual norm that we received in the previous section. By using the inequalities on the dual norm that we received in the previous section, we get the equation \"X > y\" (X > y) \u2264 \"X\" op \"X > y.\" Therefore, it is a good choice to start with \"X > op\" X > y. \""}, {"heading": "4 Approximation around the Lasso", "text": "In this section, we calculate the approximation of the second order of our standard by the special case corresponding to the lasso. We remember that when P = I-Rp \u00b7 p is used, our standard corresponds to the \"1 standard.\" We add a minor error (\"1-Sp\") to the identity matrix, and use prop.6 of Appendix A, we obtain the following second order approximation: \"I +\" Diag (w), \"W\" (\"1 + diag\"), \"W\" (\"W\"), \"W\" (\"W\"), \"W\" (\"W\"), \"W\" (\"W\"), \"W\" (\"W\"), \"W\" (\"W\"), \"W\" (\"W\"), \"W\" W \"(\" W \"),\" W \"W\" (\"W\" W \"),\" W \"W\" W \"(\" W \")."}, {"heading": "5 Experiments", "text": "In this section we perform synthetic experiments to illustrate the behavior of the trace lasso and other classical penalties when there are highly correlated covariates in the design matrix. For all experiments we have p = 1024 covariates and n = 256 observations. Indeed, the support S of w is equal to {1,..., k}, where k is the size of the support. For i in the support of w we have wi = 2 (bi \u2212 1 / 2), where each bi is drawn independently of an even distribution to [0, 1]. The observations xi are drawn by a multivariate Gauss with mean 0 and covariance matrix. For the first experiment we bet on identity, for the second experiment the block diagonal with blocks equal to 0.2I + 0.811 > according to the clusters of eight variables, finally for the third experiment we set the coronals which are elos."}, {"heading": "6 Conclusion", "text": "We are introducing a new tightening function, the trace lasso, which exploits the correlation between covariates to add strong convexity exactly where it is needed, as opposed to, say, the elastic mesh, which blindly adds a square \"two-standard term\" in all directions. We are using synthetic data to show that this adaptive behaviour leads to better estimation performance. In the future, we want to show that the trace lasso behaves similarly when a specific standard using prior knowledge such as the lasso group can be used, and that its performance does not deteriorate too much, which provides theoretical guarantees for such adaptability. Finally, we will seek the application of this estimator to inverse problems such as the design matrix, where the design matrix has a strong correlation structure."}, {"heading": "Acknowledgements", "text": "This paper was partially supported by the European Research Council (SIERRA project ERC239993)."}, {"heading": "A Perturbation of the trace norm", "text": "We follow the technique, which in [25] is used to achieve the annexation of the spurnjyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy"}, {"heading": "B Proof of proposition 1", "text": "In this section we prove that if the loss function is strong in relation to your second argument, then the solution of empirical risk minimization is unique."}, {"heading": "C Proof of proposition 3", "text": "For the first inequality we have the following formula: \"P Diag (w) 2\" = \"P Diag (w).\" For the second inequality we have the formula \"P Diag (w)\" = \"max\" M \"op\" \u2264 \"1tr\" (M > P Diag (w)) = \"max\" M \"op\" \u2264 \"1 diag\" (M > P) > w \"max\" M \"op\" \u2264 \"1 p\" i = \"M\" (i) > P (i) | | wi \"\u2264\" w \"1. The first equality is the fact that the dual standard of the trace standard is the operator standard, and the second inequality uses the fact that all matrices of the operator standard have less than a column of\" 2 norm smaller than one."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>Using the `1-norm to regularize the estimation of the parameter vector of a linear model<lb>leads to an unstable estimator when covariates are highly correlated. In this paper, we in-<lb>troduce a new penalty function which takes into account the correlation of the design matrix<lb>to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm, which<lb>is a convex surrogate of the rank, of the selected covariates as the criterion of model com-<lb>plexity. We analyze the properties of our norm, describe an optimization algorithm based on<lb>reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing<lb>that it is more adapted to strong correlations than competing methods such as the elastic net.", "creator": "LaTeX with hyperref package"}}}