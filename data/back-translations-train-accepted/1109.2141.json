{"id": "1109.2141", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2011", "title": "Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms", "abstract": "The paper studies machine learning problems where each example is described using a set of Boolean features and where hypotheses are represented by linear threshold elements. One method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features. This can be done explicitly or where possible by using a kernel function. Focusing on the well known Perceptron and Winnow algorithms, the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions. We then consider the question of whether kernel functions can analogously be used to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in this setting. However, we prove that it is computationally hard to simulate Winnows behavior for learning DNF over such a feature set. This implies that the kernel functions which correspond to running Winnow for this problem are not efficiently computable, and that there is no general construction that can run Winnow with kernels.", "histories": [["v1", "Fri, 9 Sep 2011 20:31:05 GMT  (220kb)", "http://arxiv.org/abs/1109.2141v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r khardon", "d roth", "r a servedio"], "accepted": true, "id": "1109.2141"}, "pdf": {"name": "1109.2141.pdf", "metadata": {"source": "CRF", "title": "Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms", "authors": ["Roni Khardon", "Dan Roth", "Rocco A. Servedio"], "emails": ["roni@cs.tufts.edu", "danr@cs.uiuc.edu", "rocco@cs.columbia.edu"], "sections": [{"heading": null, "text": "First, we show that these cores can be used to run the Perceptron algorithm efficiently over a trait space of exponentially many conjunctions; however, we also show that when using such cores, the Perceptron algorithm can demonstrably make an exponential number of errors, even when learning simple functions. We then consider the question of whether core functions can be used analogously to run the Winnow algorithm with multiplicative updates over an extended trait space of exponentially many conjunctions. Known upper limits imply that the Winnow algorithm can learn disjunctive normal formulas (DNF) with a polynomic error bound in this setting. However, we prove that it is computationally difficult to simulate Winnow's behavior when learning DNF over such a trait set of features, implying that the core functions corresponding to Winnow for this problem are not efficient, that Winnow is not compatible, and that there are no constructions now."}, {"heading": "1. Introduction", "text": "The problem of classifying objects into one of two classes that are \"positive\" and \"negative\" is often investigated in machine learning, and the task of machine learning is to extract such a classifier from predetermined, pre-classified examples - the problem of learning from data. If each example is represented by a set of n numerical features, an examplec \u00a9 2005 AI Access Foundation is created. All rights reserved can be regarded as a point in euclidean space. A common representation for classifiers in this case is a hyperplane of dimension (n \u2212 1) that divides the range of examples into two ranges of positive and negative examples. Such a representation is known as a linear threshold function, and many learning algorithms that are thus represented, implies, implies, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicates, implicitly, implicitly, implicitly, implies, and many learning algorithms that are thus represented, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly, implicitly."}, {"heading": "1.1 Background: On-Line Learning with Perceptron and Winnow", "text": "Before describing our results, let's remember some necessary background information about the online learning model (Littlestone, 1988) and the perceptron and winnow algorithms. In an instance space X of possible examples, a concept is a mapping of instances into one of two (or more) classes. A concept class C 2X is simply a set of concepts. In online learning, a concept class C is defined from the outset and an opponent can select a concept c-C. The learning is then modeled as a repetitive game in which the opponent chooses an example x-X in each iteration, the student makes a guess for the value c (x) and then gets the correct value. We count an error for each iteration in which the value is not correctly predicted. A learning algorithm learns a concept class C with an error-bound M if a positive M attribute is used for each choice c-C and any (arbitrarily long) sequence of examples."}, {"heading": "1.1.1 Perceptron", "text": "If the prediction is 1 and the label is \u2212 1 (false positive prediction), then the vector w is set to w \u2212 x, while the prediction is -1 and the label is 1 (false negative), then w is set to w + x. No changes are made to w if the prediction is correct. Many variants of this basic algorithm have been suggested and studied, and in particular, a non-zero threshold and a learning rate that controls the size of the update to w. Some of these will be further described in Section 3.The famous Perceptron Convergence Theorem (Rosenblatt, 1958; Block, 1962; Novikoff, 1963) limits the number of errors the perceptron algorithm can make: Theorem 1 < Let i > 1, < < 1."}, {"heading": "R and yi \u2208 {\u22121, 1} for all i. Let u \u2208 \u211c", "text": "N, \u0441 > 0 should be such that yi (u \u00b7 x i) \u2265 \u044b for all i. Then Perceptron makes a maximum of R 2, u-2 errors in this sample sequence."}, {"heading": "1.1.2 Winnow", "text": "The Winnow algorithm (Littlestone, 1988) has a very similar structure. Winnow maintains a hypothesis vector w-N, which is initially w = (1,..., 1). Winnow is parameterized by a doctoral factor \u03b1 > 1 and a threshold \u03b8 > 0; after receiving an example x-0, 1} N Winnow says w-x according to the threshold function. If the prediction is 1 and the label is \u2212 1 then for all i the value of xi = 1 is set to wi / \u03b1; this is a demotion step. If the prediction is \u2212 1 and the label is 1 then for all i the value of xi = 1 is set to \u03b1wi; this is a promotion step. No change is made to w if the prediction is correct. For our purposes, the following error limit is implicit in Littstone's work (1988) of interest: Theorem 2 Let the target function be a k-literal monotone prediction (if the prediction is x\u03b1)."}, {"heading": "1.2 Our Results", "text": "We are interested in the computational power and convergence of the individual examples (n). (n) We are interested in these algorithms growing beyond the extended functional spaces of the conjunctions. (n) We are interested in these algorithms growing beyond the extended functional spaces of the conjunctions. (n) We are not able to expand the functional spaces of the conjunctions and thus extend the learning capabilities of Perceptron and Winnow. (n) We refer to these extended algorithms as Kernel Perceptron and Kernel Winnow.Our first result is that an exponential number of conjunctive features has an exponential number of conjunctive features. (see Theorem 3) There is an algorithm that simulates the perception of the three-dimensional characteristics of all constellations.) In view of a sequence of t-labeled examples in {0, 1} n take the predictions and updates for each example. (n) We are interested in the fundamental features of all the constellations of all the constellations."}, {"heading": "2. Kernel Perceptron with Many Features", "text": "It is well known that the hypothesis w of the Perceptron algorithm is a linear combination of the previous examples on which mistakes were made (Christianini & Shaw-Taylor, 2000). Specifically, if we use L (v), 1), 1), the designation of the example v (v), then we have that w = [M L (v) v, where M is the set of examples on which the algorithm made an error. Thus, the prediction of Perceptron is to x, 1), w \u00b7 x = (v), x (v), x (v), x (n), x (x), x), the prediction of Perceptron is to x, x (x), the conversion into an extended feature space like the space of all conjunctions. To run the Perceptron algorithm over the extended space, we must use x (x), x (x), x (x), x), x (x), x (where x) is the value."}, {"heading": "3. Kernel Perceptron with Many Mistakes", "text": "In this section, we describe a simple monotonous DNF target function and a sequence of examples described that causes the monotonous monomialisms to make exponentially many errors.For x, y, 1} n, we write the number of 1s in x and, as described above, the number of bit positions we have. < p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, y, p, p, y, p, y, p, y, p, y, p, y, p, y, y, y, p, y, y, p, y, y, p, y, p, y, y, y, p, y, y, p, y, y, p, y, y, and 1} n, p, p, p, y, y, y, y, y, y, y, y, y, p, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, y, p, p, p, y, p, y, p, p, p, y, p, p, y, p, p, y, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, and, p, p, p, p, p, p, p, p, p, p, and, p, p, p, p, p, p, p, p, p, p, and, p, p, p, p, p, p, p, and, p, p, p, p, p, p, and, p, p, p, p, p, p, p, p, and, p, p, p, p,"}, {"heading": "3.1 A Negative Result for the PAC Model", "text": "In this model, each example x is independent of a fixed probability distribution D, and therefore, with a high probability, the learner must construct a hypothesis h that has a high accuracy with respect to the target concept c under distribution D. See the Kearns-Vazirani text (1994) for a detailed discussion of the PAC learning model. Let D be the probability distribution above {0, 1} n, which has a high accuracy with respect to the target concept c under example 0n, weight 1 / 4 with respect to example 1n, and weight 12 1 / 9600 with respect to each of the en / 9600 examples x1,.Let D be the probability distribution above {0, 1} n, the example examples 1 / 4 with respect to example 0n, weight 1 / 4 with respect to example 1n, and weight 12 1 / 9600 with respect to each of the en / 9600 examples x1,.Theorem 8 If the Kernel Perceptron is performed using a random polynomial sample, we will error the probability with at least 16 (1)."}, {"heading": "4. Computational Hardness of Kernel Winnow", "text": "px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-px-"}, {"heading": "5. Conclusion", "text": "In fact, most of them are able to survive by themselves if they are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...)"}, {"heading": "Acknowledgments", "text": "This work was done partly during Kharton's time at the University of Edinburgh and partly during Servedio at Harvard University. The authors are grateful for the financial support of the EPSRC Grant GR / N03167, the NSF Grant IIS-0099446 and a Research Semester Fellowship Award from Tufts University (Khardon), the NSF Scholarships ITR-IIS00-85836, ITR-IIS-0085980 and IIS-9984168 (Roth), as well as the NSF Grant CCR-98-77049 and the NSF Mathematical Sciences Postdoctoral Fellowship (Servedio)."}], "references": [{"title": "Negative results for equivalence queries", "author": ["D. Angluin"], "venue": "Machine Learning, 2, 121\u2013150. Block, H. (1962). The perceptron: a model for brain functioning. Reviews of Modern", "citeRegEx": "Angluin,? 1990", "shortCiteRegEx": "Angluin", "year": 1990}, {"title": "The SNoW learning architecture", "author": ["A. Carlson", "C. Cumby", "J. Rosen", "D. Roth"], "venue": null, "citeRegEx": "Carlson et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 1999}, {"title": "UIUCDCS-R-99-2101, UIUC Computer Science", "author": ["Tech. rep"], "venue": null, "citeRegEx": "rep.,? \\Q2004\\E", "shortCiteRegEx": "rep.", "year": 2004}, {"title": "On kernel methods for relational learning", "author": ["C. Cambridge Press. Cumby", "D. Roth"], "venue": null, "citeRegEx": "Cumby and Roth,? \\Q2003\\E", "shortCiteRegEx": "Cumby and Roth", "year": 2003}, {"title": "Efficiency versus convergence of Boolean kernels for on-line learning algorithms", "author": ["R. Khardon", "D. Roth", "R. Servedio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Khardon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Khardon et al\\.", "year": 2002}, {"title": "Maximum margin algorithms with Boolean kernels", "author": ["R. Khardon", "R. Servedio"], "venue": "In Proceedings of the Sixteenth Annual Conference on Computational Learning Theory,", "citeRegEx": "Khardon and Servedio,? \\Q2003\\E", "shortCiteRegEx": "Khardon and Servedio", "year": 2003}, {"title": "Introduction to Coding Theory", "author": ["J.V. Lint"], "venue": "Springer-Verlag.", "citeRegEx": "Lint,? 1992", "shortCiteRegEx": "Lint", "year": 1992}, {"title": "Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm", "author": ["N. Littlestone"], "venue": "Machine Learning, 2, 285\u2013318.", "citeRegEx": "Littlestone,? 1988", "shortCiteRegEx": "Littlestone", "year": 1988}, {"title": "Efficient learning with virtual threshold gates", "author": ["W. Maass", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Maass and Warmuth,? \\Q1998\\E", "shortCiteRegEx": "Maass and Warmuth", "year": 1998}, {"title": "Perceptrons: an introduction to computational geometry", "author": ["M. Minsky", "S. Papert"], "venue": null, "citeRegEx": "Minsky and Papert,? \\Q1968\\E", "shortCiteRegEx": "Minsky and Papert", "year": 1968}, {"title": "On convergence proofs for perceptrons", "author": ["A. Novikoff"], "venue": "Proceeding of the Symposium on the Mathematical Theory of Automata, Vol. 12, pp. 615\u2013622.", "citeRegEx": "Novikoff,? 1963", "shortCiteRegEx": "Novikoff", "year": 1963}, {"title": "Computational Complexity", "author": ["C. Papadimitriou"], "venue": "Addison-Wesley.", "citeRegEx": "Papadimitriou,? 1994", "shortCiteRegEx": "Papadimitriou", "year": 1994}, {"title": "The Perceptron: a probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65, 386\u2013407.", "citeRegEx": "Rosenblatt,? 1958", "shortCiteRegEx": "Rosenblatt", "year": 1958}, {"title": "Learning to resolve natural language ambiguities: A unified approach", "author": ["D. Roth"], "venue": "Proc. of the American Association of Artificial Intelligence, pp. 806\u2013813.", "citeRegEx": "Roth,? 1998", "shortCiteRegEx": "Roth", "year": 1998}, {"title": "Learning of Boolean functions using support vector machines", "author": ["K. Sadohara"], "venue": "Proc. of the Conference on Algorithmic Learning Theory, pp. 106\u2013118. Springer. LNAI 2225.", "citeRegEx": "Sadohara,? 2001", "shortCiteRegEx": "Sadohara", "year": 2001}, {"title": "Path kernels and multiplicative updates", "author": ["E. Takimoto", "M. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth", "year": 2003}, {"title": "The complexity of enumeration and reliability problems", "author": ["L.G. Valiant"], "venue": "SIAM Journal of Computing, 8, 410\u2013421.", "citeRegEx": "Valiant,? 1979", "shortCiteRegEx": "Valiant", "year": 1979}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Communications of the ACM, 27 (11), 1134\u20131142.", "citeRegEx": "Valiant,? 1984", "shortCiteRegEx": "Valiant", "year": 1984}, {"title": "Kernels from matching operations", "author": ["C. Watkins"], "venue": "Tech. rep. CSD-TR-98-07, Computer Science Department, Royal Holloway, University of London. 356", "citeRegEx": "Watkins,? 1999", "shortCiteRegEx": "Watkins", "year": 1999}], "referenceMentions": [{"referenceID": 12, "context": "Of particular interest in this paper are the well known Perceptron (Rosenblatt, 1958; Block, 1962; Novikoff, 1963) and Winnow (Littlestone, 1988) algorithms that have been intensively studied in the literature.", "startOffset": 67, "endOffset": 114}, {"referenceID": 10, "context": "Of particular interest in this paper are the well known Perceptron (Rosenblatt, 1958; Block, 1962; Novikoff, 1963) and Winnow (Littlestone, 1988) algorithms that have been intensively studied in the literature.", "startOffset": 67, "endOffset": 114}, {"referenceID": 7, "context": "Of particular interest in this paper are the well known Perceptron (Rosenblatt, 1958; Block, 1962; Novikoff, 1963) and Winnow (Littlestone, 1988) algorithms that have been intensively studied in the literature.", "startOffset": 126, "endOffset": 145}, {"referenceID": 13, "context": "As one example, the SNoW system (Roth, 1998; Carlson, Cumby, Rosen, & Roth, 1999) has successfully applied variations of Perceptron and Winnow to problems in natural language processing.", "startOffset": 32, "endOffset": 81}, {"referenceID": 2, "context": "A common representation for classifiers in this case is a hyperplane of dimension (n \u2212 1) which splits the domain of examples into two areas of positive and negative examples. Such a representation is known as a linear threshold function, and many learning algorithms that output a hypothesis represented in this manner have been developed, analyzed, implemented, and applied in practice. Of particular interest in this paper are the well known Perceptron (Rosenblatt, 1958; Block, 1962; Novikoff, 1963) and Winnow (Littlestone, 1988) algorithms that have been intensively studied in the literature. It is also well known that the expressiveness of linear threshold functions is quite limited (Minsky & Papert, 1968). Despite this fact, both Perceptron and Winnow have been applied successfully in recent years to several large scale real world classification problems. As one example, the SNoW system (Roth, 1998; Carlson, Cumby, Rosen, & Roth, 1999) has successfully applied variations of Perceptron and Winnow to problems in natural language processing. The SNoW system extracts basic Boolean features x1, . . . , xn from labeled pieces of text data in order to represent the examples, thus the features have numerical values restricted to {0, 1}. There are several ways to enhance the set of basic features x1, . . . , xn for Perceptron or Winnow. One idea is to expand the set of basic features x1, . . . , xn using conjunctions such as (x1\u2227x3\u2227x4) and use these expanded higher-dimensional examples, in which each conjunction plays the role of a basic feature, as the examples for Perceptron or Winnow. This is in fact the approach which the SNoW system takes running Perceptron or Winnow over a space of restricted conjunctions of these basic features. This idea is closely related to the use of kernel methods, see e.g. the book of Cristianini and Shawe-Taylor (2000), where a feature expansion is done implicitly through the kernel function.", "startOffset": 9, "endOffset": 1875}, {"referenceID": 7, "context": "Before describing our results, we recall some necessary background on the on-line learning model (Littlestone, 1988) and the Perceptron and Winnow algorithms.", "startOffset": 97, "endOffset": 116}, {"referenceID": 12, "context": "The famous Perceptron Convergence Theorem (Rosenblatt, 1958; Block, 1962; Novikoff, 1963) bounds the number of mistakes which the Perceptron algorithm can make:", "startOffset": 42, "endOffset": 89}, {"referenceID": 10, "context": "The famous Perceptron Convergence Theorem (Rosenblatt, 1958; Block, 1962; Novikoff, 1963) bounds the number of mistakes which the Perceptron algorithm can make:", "startOffset": 42, "endOffset": 89}, {"referenceID": 7, "context": "The Winnow algorithm (Littlestone, 1988) has a very similar structure.", "startOffset": 21, "endOffset": 40}, {"referenceID": 7, "context": "The Winnow algorithm (Littlestone, 1988) has a very similar structure. Winnow maintains a hypothesis vector w \u2208 RN which is initially w = (1, . . . , 1). Winnow is parameterized by a promotion factor \u03b1 > 1 and a threshold \u03b8 > 0; upon receiving an example x \u2208 {0, 1}N Winnow predicts according to the threshold function w \u00b7x \u2265 \u03b8. If the prediction is 1 and the label is \u22121 then for all i such that xi = 1 the value of wi is set to wi/\u03b1; this is a demotion step. If the prediction is \u22121 and the label is 1 then for all i such that xi = 1 the value of wi is set to \u03b1wi; this is a promotion step. No change is made to w if the prediction is correct. For our purposes the following mistake bound, implicit in Littlestone\u2019s work (1988), is of interest:", "startOffset": 22, "endOffset": 730}, {"referenceID": 18, "context": "Our first result (cf. also the papers of Sadohara, 1991; Watkins, 1999; and Kowalczyk et al., 2001) uses kernel functions to show that it is possible to efficiently run the kernel Perceptron algorithm over an exponential number of conjunctive features.", "startOffset": 17, "endOffset": 99}, {"referenceID": 17, "context": "We also give a variant of this result showing that kernel Perceptron fails in the Probably Approximately Correct (PAC) learning model (Valiant, 1984) as well.", "startOffset": 134, "endOffset": 149}, {"referenceID": 0, "context": "Angluin (1990) proved that DNF expressions cannot be learned efficiently using equivalence queries whose hypotheses are themselves DNF expressions.", "startOffset": 0, "endOffset": 15}, {"referenceID": 14, "context": "This kernel has been obtained independently by Sadohara (2001). To express all monotone monomials as in (2) we take K(x, y) = 2|x\u2229y| where |x \u2229 y| is the number of active features common to both x and y, i.", "startOffset": 47, "endOffset": 63}, {"referenceID": 2, "context": "This kernel is reported also by Watkins (1999). For case (4) we have K(x, y) = \u2211k l=0 (|x\u2229y| l )", "startOffset": 15, "endOffset": 47}, {"referenceID": 6, "context": "5 of the book by Van Lint (1992), there can be at most 20.", "startOffset": 21, "endOffset": 33}, {"referenceID": 17, "context": "The proof above can be adapted to give a negative result for kernel Perceptron in the PAC learning model (Valiant, 1984).", "startOffset": 105, "endOffset": 120}, {"referenceID": 16, "context": "The proof above can be adapted to give a negative result for kernel Perceptron in the PAC learning model (Valiant, 1984). In this model each example x is independently drawn from a fixed probability distribution D and with high probability the learner must construct a hypothesis h which has high accuracy relative to the target concept c under distribution D. See the Kearns-Vazirani text (1994) for a detailed discussion of the PAC learning model.", "startOffset": 106, "endOffset": 397}, {"referenceID": 16, "context": "The following problem is #P-hard (Valiant, 1979):", "startOffset": 33, "endOffset": 48}, {"referenceID": 11, "context": "See the book of Papadimitriou (1994) or the paper of Valiant (1979) for details on #P.", "startOffset": 16, "endOffset": 37}, {"referenceID": 11, "context": "See the book of Papadimitriou (1994) or the paper of Valiant (1979) for details on #P.", "startOffset": 16, "endOffset": 68}, {"referenceID": 13, "context": "In the paper (Cumby & Roth, 2003) a kernel for expressions in description logic (generalizing the monomials kernel) is developed and successfully applied for natural language and molecular problems. Takimoto and Warmuth (2003) study the use of multiplicative update algorithms other than Winnow (such as weighted majority) and obtain some positive results by restricting the type of loss function used to be additive over base features.", "startOffset": 22, "endOffset": 227}, {"referenceID": 13, "context": "In the paper (Cumby & Roth, 2003) a kernel for expressions in description logic (generalizing the monomials kernel) is developed and successfully applied for natural language and molecular problems. Takimoto and Warmuth (2003) study the use of multiplicative update algorithms other than Winnow (such as weighted majority) and obtain some positive results by restricting the type of loss function used to be additive over base features. Chawla et al. (2004) have studied Monte Carlo estimation approaches to approximately simulate the Winnow algorithm\u2019s performance when run over a space of exponentially many features.", "startOffset": 22, "endOffset": 458}], "year": 2011, "abstractText": "The paper studies machine learning problems where each example is described using a set of Boolean features and where hypotheses are represented by linear threshold elements. One method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features. This can be done explicitly or where possible by using a kernel function. Focusing on the well known Perceptron and Winnow algorithms, the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions. We then consider the question of whether kernel functions can analogously be used to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in this setting. However, we prove that it is computationally hard to simulate Winnow\u2019s behavior for learning DNF over such a feature set. This implies that the kernel functions which correspond to running Winnow for this problem are not efficiently computable, and that there is no general construction that can run Winnow with kernels.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}