{"id": "1109.3843", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2011", "title": "Fast approximation of matrix coherence and statistical leverage", "abstract": "The \\emph{statistical leverage scores} of a matrix $A$ are the squared row-norms of the matrix containing its (top) left singular vectors and the \\emph{coherence} is the largest leverage score. These quantities have been of interest in recently-popular problems such as matrix completion and Nystr\\\"{o}m-based low-rank matrix approximation; in large-scale statistical data analysis applications more generally; and since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary $n \\times d$ matrix $A$, with $n \\gg d$, and that returns as output relative-error approximations to \\emph{all} $n$ of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of $n$ and $d$) in $O(n d \\log n)$ time, as opposed to the $O(nd^2)$ time required by the na\\\"{i}ve algorithm that involves computing an orthogonal basis for the range of $A$. Our analysis may be viewed in terms of computing a relative-error approximation to an \\emph{under}constrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with $n \\approx d$, and the extension to streaming environments.", "histories": [["v1", "Sun, 18 Sep 2011 04:38:12 GMT  (60kb)", "https://arxiv.org/abs/1109.3843v1", "28 pages"], ["v2", "Wed, 5 Dec 2012 00:13:53 GMT  (35kb)", "http://arxiv.org/abs/1109.3843v2", "29 pages; conference version is in ICML; journal version is in JMLR"]], "COMMENTS": "28 pages", "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["michael w mahoney", "petros drineas", "malik magdon-ismail", "david p woodruff"], "accepted": true, "id": "1109.3843"}, "pdf": {"name": "1109.3843.pdf", "metadata": {"source": "CRF", "title": "Fast approximation of matrix coherence and statistical leverage", "authors": ["Petros Drineas", "Michael W. Mahoney", "David P. Woodruff"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 110 9.38 43v2 [cs.DS] 5D ec2 012"}, {"heading": "1 Introduction", "text": "The concept of statistical leverage also measures the extent to which the singular vectors of a matrix are correlated with the standard basis, and as such it has recently found usefulness in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21]. A related term is that of matrix coherence, which has been of interest for recently popular problems such as matrix completion and nystro-m-based low-rank matrix approximation [47, 46]. More precisely defined below, the statistical leverage values can be calculated as squared Euclidean norms of matrix rows containing the uppermost left singular vectors and the coherence of the matrix."}, {"heading": "1.1 Overview and definitions", "text": "In fact, we are going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position to be able to put ourselves in a position to be able to do that. \""}, {"heading": "1.2 Our main result", "text": "Our main result is a randomized algorithm for calculating relative error rates to each statistical leverage score, as well as an additive error assumption to all the large error rates resulting from the range of this matrix. (See Algorithm 1 in Section 3) This will amount to constructing a \"randomized sketch\" of the input matrix and then calculating the Euclidean standards of this sketch. (See Algorithm 1 in Section 3) (see Algorithm 2 in Section 3).The following theorem provides our key quality approximation and runtime results for algorithm 1. Let's get a complete approximation to the large cross-leverage scores (see Algorithm 2 of Section 3). (See Algorithm 2 of Section 3).The following theorem provides our key quality approximation and runtime results for algorithms."}, {"heading": "1.3 Significance and related work", "text": "This year, it has got to the point where there is only one person who is able to try to find a solution, to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, to find a solution."}, {"heading": "1.4 Empirical discussion of our algorithms", "text": "Although the most important contribution of our paper is to provide a rigorous theoretical understanding of the rapid leverage of the approach, our paper analyzes the theoretical performance of what is supposed to be a practical algorithm. Thus, one might wonder about the empirical performance of our algorithms - for example, whether hidden constants render the algorithms useless for the data of real size. This depends greatly on the quality of the numerical implementation, whether one is interested in \"large\" or more general matrices, etc. We will consider empirical and numerical aspects of these algorithms in the coming papers, for example, that we provide a brief summary of several numerical problems for the reader interested in these questions."}, {"heading": "1.5 Outline", "text": "In Section 2, we will give a brief overview of relevant notations and concepts from linear algebra. Then, in Sections 3 and 4, we will present our main results: Section 3 will contain our main algorithm and Section 4 the proof of our main theorem. Section 5 will then describe extensions of our main result to general \"bold\" matrices, i.e. to those with n \u2248. Section 6 will conclude with a description of the relationship of our main result to another related statistical leverage estimate, an application of our main algorithm to the problem of underlimited approximation of the smallest squares and extensions of our main algorithm to streaming environments."}, {"heading": "2 Preliminaries on linear algebra and fast random projections", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Basic linear algebra and notation", "text": "Let [n] the set of integers [1, 2,.., n] for each matrix A-Rn \u00b7 d, let A-Rn, let A-Rn, let the i-th row of A denote a row vector, and let A (j), j [d] denote the j-th column of A as a column vector. Let's denote the j-th column of A-Rn as a column vector. (Let's denote the j-th column of A-Rn as a column vector. (Let's denote the j-th column of A-Rn as a column vector.) Let's denote the spectral norm of A. Relative, for each vector x-R n, its euclidean norm (or V2 norm) is the square root of the sum of the squares of its elements."}, {"heading": "2.2 The Fast Johnson-Lindenstrauss Transform (FJLT)", "text": "Considering that the number of points x1,.. xn with xi-JLT, is a projection of the points in Rr, so that (1 \u2212) xi-JLT with probability 1 / 6 and otherwise zero (with probability 2 / 3) [1]. To construct such a matrix with high probability, simply select each entry in Rr independently, equal to \u00b1 3 / r with probability 1 / 6 and otherwise zero (with probability 2 / 3) [1]. Let JLT be a matrix drawn from such a distribution over r \u00b7 d matrices. 4 Then the following problem persists: Lemma 1 (theorem 1,1 of [1]). Leave x1,., xn, a matrix drawn from the matrix."}, {"heading": "2.3 The Subsampled Randomized Hadamard Transform (SRHT)", "text": "One can use a Randomized Hadamard Transform (RHT) to construct a high probability FJLT. (5) It should be remembered that in the development of o (nd2) randomized algorithms for the general overloaded LS problem (22) and its variants for a high-quality numerical implementation of such randomized algorithms (44, 6), RHT has been crucial for the development of o (nd2) randomized algorithms for the general overloaded LS problem (22) and their variants to enable the numerical implementation of such randomized algorithms (44, 6). (unnormized) n matrix of the Hadamard transform H'n is recursively defined by H'n. (H'n H'n H'n H'n H'n \u2212 H'n), with H'n \"n normalized matrix."}, {"heading": "3 Our main algorithmic results", "text": "In this section we will describe our main results for calculating relative error approximations to each statistical leverage score (see algorithm 1) and additive error approximations to all large cross-leverage scores (see algorithm 2) of any matrix A, Rn, and d with n. Both algorithms use a \"randomized sketch\" of A in the form A, where \"1\" is an A-FJLT and \"2 is an A-JLT. We begin with a description of the basic ideas at a high level."}, {"heading": "3.1 Outline of our basic approach", "text": "We remember that our first goal is to make the first equality from the second to the third from the third to the fourth from the fourth to the fourth from the fourth to the fourth from the fourth to the fourth from the fourth to the fourth from the fourth from the fourth to the fourth from the fourth from the fourth to the fourth from the fourth from the fourth to the fifth from the fourth to the fifth from the fourth to the fifth from the fifth from the fifth to the ninth from the ninth from the ninth from the ninth to the ninth from the ninth from the ninth from the ninth from the ninth from the ninth from the ninth to the ninth from the ninth from the ninth from the ninth to the ninth from the ninth from the ninth to the ninth from the ninth from the ninth."}, {"heading": "3.2 Approximating all the statistical leverage scores", "text": "The first major result is \"A.\" (A) \"A.\" (A) \"A.\" (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A). (A)."}, {"heading": "3.3 Approximating the large cross-leverage scores", "text": "By combining Lemmas 6 and 7 (in Section 4.2 below) with the triangular inequality, you immediately get the following estimation.Lemma 5. Let's be either the sketch matrix constructed by Algorithm 1, i.e. the pairs with additive errors are additive-erroneous approximations to the leverage values and crossing effects. That is, if you would be interested in getting an approximation to all the crossing effects. < U (i), U (j) > < B (i) > < B (i) > avoiding points with additive errors."}, {"heading": "4 Proofs of our main theorems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Sketch of the proof of Theorems 1 and 2", "text": "We begin by making a sketch of the proof of theorems 1 and 2. Detailed proof will be provided in the next two sections."}, {"heading": "4.2 Proof of Theorem 1", "text": "We condition all of our analyses on the events that have led to a result that leads to a result that leads to a result. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4.3 Proof of Theorem 2", "text": "We shall first estimate the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh these pairs normal.i. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma. (...) We shall weigh the number of such irs.Lemma."}, {"heading": "5 Extending our algorithm to general matrices", "text": "In this section, we will describe an important extension of our main results, namely the calculation of the statistical leverage values in relation to the best rank-k approach to a general matrix A. Specifically, we will consider the estimate of the leverage values for the case of the general \"fat\" matrices, namely the input matrices A and D, where both rankings are large, e.g. if the numerical rank of A is small (e.g. if the leverage values of all rankings are exactly the same n \u00b7 n matrix).The problem becomes interesting if you specify a ranking parameter k \u00b2 min {n}, which can occur when the numerical rank of A is small (e.g. in some scientific computing applications, more than 99% of the spectral standard of A can be detected."}, {"heading": "5.1 Leverage Scores for Spectral Norm Approximators", "text": "The next problem argues that there is a matrix of rank X that ranks sufficiently close to A (especially that it is a member of the SVP with a constant probability) and can also be written as X = BY, where there is a matrix of rank X."}, {"heading": "5.2 Leverage Scores for Frobenius Norm Approximators.", "text": "The next problem is that there is a matrix X that ranks k = 1 QUQTA = 1 QUQTA = 1 QUQTA = 1 QUQTA = 1 QUQTA = 1 QTA = 1 QTA = 1 QTA = 1 QUQTA = 1 QUQTA = 1 QTA = 1 QUQTA = 1 QUQTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA = 1 QUTA ="}, {"heading": "6 Discussion", "text": "We conclude with a discussion of our main results in a broader context: understanding the relationship between our main algorithm and a related statistical leverage estimator; applying our main algorithm to solve unlimited least-squares problems; and implementing variants of the basic algorithm in streaming environments."}, {"heading": "6.1 A related estimator for the leverage scores", "text": "Magdon-Ismail in [31] introduced the following algorithm for estimating the statistical leverage values: as input a n \u00b7 d matrix A, with n \u0445 d, the algorithm proceeds as follows. \u2022 Calculation of an estimate A, where the O (n lnd ln2 n) \u00b7 n matrix A is an SRHT or another FJLT. \u2022 Specifically, the proof can be found in term 33 in section A.3; note that for our purposes here we essentially use 10 k + 1 procedures. \u2022 Calculation X = (A) \u2020. \u2022 For t = 1,. n, Calculation of the estimate w = AT (t) X (t) and sentence wt = max {d ln2 n, w \u00b2 t}. \u2022 Return the quantities p = wi / n i. \""}, {"heading": "6.2 An application to under-constrained least-squares problems", "text": "It is well known that we can solve this problem exactly in O (n2d) time and that the minimum solution to the 2-standard question is given by xopt = A \u00b2 -2-standard solution. In this section we will argue that algorithm 6 is a simple, precise estimator x x opt for xopt. In words, algorithm 6 samples a small number of columns from A (note that the columns of the A-standard correspond to the variables in our sublimited problem) and the sampled columns are used to determine which columns will be included in the sample."}, {"heading": "6.3 Extension to streaming environments", "text": "In this section, we look at the estimation of the leverage scores and associated statistics, if the input data is so large that the data can be considered as a data stream. (...) In this context, it is possible that the data depend only logarithmically on the high dimension n and the polynomial distribution of the data. (...) When discussing the bits of space, we assume that the entries from A to O (log n) bit integers can be discredited, although all our results can be generalized to arbitrary words. (...) The general strategy behind our algorithms is as follows. (...) Since the data flows from, compute TA, for an adequate linear sketrix T, and compute. (...)"}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences, 66(4):671\u2013687", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "Proceedings of the 38th Annual ACM Symposium on Theory of Computing, pages 557\u2013563", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "The fast Johnson-Lindenstrauss transform and approximate nearest neighbors", "author": ["N. Ailon", "B. Chazelle"], "venue": "SIAM Journal on Computing, 39(1):302\u2013322", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast dimension reduction using Rademacher series on dual BCH codes", "author": ["N. Ailon", "E. Liberty"], "venue": "Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1\u20139", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Streaming algorithms from precision sampling", "author": ["A. Andoni", "R. Krauthgamer", "K. Onak"], "venue": "Technical report. Preprint: arXiv:1011.1263 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Blendenpik: Supercharging LAPACK\u2019s leastsquares solver", "author": ["H. Avron", "P. Maymounkov", "S. Toledo"], "venue": "SIAM Journal on Scientific Computing, 32:1217\u20131236", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Low cost high performance uncertainty quantification", "author": ["C. Bekas", "A. Curioni", "I. Fedulova"], "venue": "Proceedings of the 2nd Workshop on High Performance Computational Finance, page Article No.: 8", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "An estimator for the diagonal of a matrix", "author": ["C. Bekas", "E. Kokiopoulou", "Y. Saad"], "venue": "Applied Numerical Mathematics, 57:1214\u20131229", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Power and centrality: A family of measures", "author": ["P. Bonacich"], "venue": "The American Journal of Sociology, 92(5):1170\u20131182", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["C. Boutsidis", "P. Drineas", "M. Magdon-Ismail"], "venue": "Technical report. Preprint: arXiv:1103.0995 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["C. Boutsidis", "P. Drineas", "M. Magdon-Ismail"], "venue": "Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science, pages 305\u2013314", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 968\u2013977", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Technical report. Preprint: arXiv:0805.4471 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Influential observations", "author": ["S. Chatterjee", "A.S. Hadi"], "venue": "high leverage points, and outliers in linear regression. Statistical Science, 1(3):379\u2013393", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1986}, {"title": "Sensitivity Analysis in Linear Regression", "author": ["S. Chatterjee", "A.S. Hadi"], "venue": "John Wiley & Sons, New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Regression Analysis by Example", "author": ["S. Chatterjee", "A.S. Hadi", "B. Price"], "venue": "John Wiley & Sons, New York", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "A sparse Johnson-Lindenstrauss transform", "author": ["A. Dasgupta", "R. Kumar", "T. Sarl\u00f3s"], "venue": "Proceedings of the 42nd Annual ACM Symposium on Theory of Computing, pages 341\u2013350", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM Journal on Computing, 36:132\u2013157", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Inferring geographic coordinates of origin for Europeans using small panels of ancestry informative markers", "author": ["P. Drineas", "J. Lewis", "P. Paschou"], "venue": "PLoS ONE, 5(8):e11892", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Sampling algorithms for l2 regression and applications", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1127\u20131136", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications, 30:844\u2013881", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Faster least squares approximation", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan", "T. Sarl\u00f3s"], "venue": "Numerische Mathematik, 117(2):219\u2013249", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimating hybrid frequency moments of data streams", "author": ["S. Ganguly", "M. Bansal", "S. Dube"], "venue": "Proceedings of the 2nd Annual International Workshop on Frontiers in Algorithmics, pages 55\u201366", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "In preparation", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press, Baltimore", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J.A. Tropp"], "venue": "SIAM Review, 53(2):217\u2013288", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Sketching and streaming entropy via approximation theory", "author": ["N.J.A. Harvey", "J. Nelson", "K. Onak"], "venue": "Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science, pages 489\u2013498", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "The hat matrix in regression and ANOVA", "author": ["D.C. Hoaglin", "R.E. Welsch"], "venue": "The American Statistician, 32(1):17\u201322", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1978}, {"title": "Effective resistance of Gromovhyperbolic graphs: Application to asymptotic sensor network problems", "author": ["E.A. Jonckheere", "M. Lou", "J. Hespanha", "P. Barooah"], "venue": "Proceedings of the 46th IEEE Conference on Decision and Control, pages 1453\u20131458", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Row sampling for matrix algorithms via a non-commutative Bernstein bound", "author": ["M. Magdon-Ismail"], "venue": "Technical report. Preprint: arXiv:1008.0587 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Randomized algorithms for matrices and data", "author": ["M.W. Mahoney"], "venue": "Foundations and Trends in Machine Learning. NOW Publishers, Boston", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proc. Natl. Acad. Sci. USA, 106:697\u2013702", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "LSRN: A parallel iterative solver for strongly over- or under-determined systems", "author": ["X. Meng", "M.A. Saunders", "M.W. Mahoney"], "venue": "Technical report. Preprint: arXiv:1109.5981 ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "1-pass relative-error lp-sampling with applications", "author": ["M. Monemizadeh", "D.P. Woodruff"], "venue": "Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1143\u20131160", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Data Streams: Algorithms and Applications", "author": ["S. Muthukrishnan"], "venue": "Foundations and Trends in Theoretical Computer Science. Now Publishers Inc, Boston", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}, {"title": "editor", "author": ["M.Z. Nashed"], "venue": "Generalized Inverses and Applications. Academic Press, New York", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1976}, {"title": "A measure of betweenness centrality based on random walks", "author": ["M.E.J. Newman"], "venue": "Social Networks, 27:39\u201354", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Latent semantic indexing: a probabilistic analysis", "author": ["C.H. Papadimitriou", "P. Raghavan", "H. Tamaki", "S. Vempala"], "venue": "Journal of Computer and System Sciences, 61(2):217\u2013235", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "Ancestry informative markers for finescale individual assignment to worldwide populations", "author": ["P. Paschou", "J. Lewis", "A. Javed", "P. Drineas"], "venue": "Journal of Medical Genetics, page doi:10.1136/jmg.2010.078212", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "PCA-correlated SNPs for structure identification in worldwide human populations", "author": ["P. Paschou", "E. Ziv", "E.G. Burchard", "S. Choudhry", "W. Rodriguez-Cintron", "M.W. Mahoney", "P. Drineas"], "venue": "PLoS Genetics, 3:1672\u20131686", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Mining knowledge-sharing sites for viral marketing", "author": ["M. Richardson", "P. Domingos"], "venue": "Proceedings of the 8th Annual ACM SIGKDD Conference, pages 61\u201370", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "A randomized algorithm for principal component analysis", "author": ["V. Rokhlin", "A. Szlam", "M. Tygert"], "venue": "SIAM Journal on Matrix Analysis and Applications, 31(3):1100\u20131124", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast randomized algorithm for overdetermined linear leastsquares regression", "author": ["V. Rokhlin", "M. Tygert"], "venue": "Proc. Natl. Acad. Sci. USA, 105(36):13212\u201313217", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T. Sarl\u00f3s"], "venue": "Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science, pages 143\u2013152", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2006}, {"title": "Matrix coherence and the Nystr\u00f6m method", "author": ["A. Talwalkar", "A. Rostamizadeh"], "venue": "Proceedings of the 26th Conference in Uncertainty in Artificial Intelligence", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient computing of regression diagnostics", "author": ["P.F. Velleman", "R.E. Welsch"], "venue": "The American Statistician, 35(4):234\u2013242", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1981}], "referenceMentions": [{"referenceID": 45, "context": "1 Introduction The concept of statistical leverage measures the extent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21].", "startOffset": 278, "endOffset": 290}, {"referenceID": 31, "context": "1 Introduction The concept of statistical leverage measures the extent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21].", "startOffset": 278, "endOffset": 290}, {"referenceID": 20, "context": "1 Introduction The concept of statistical leverage measures the extent to which the singular vectors of a matrix are correlated with the standard basis and as such it has found usefulness recently in large-scale data analysis and in the analysis of randomized matrix algorithms [47, 33, 21].", "startOffset": 278, "endOffset": 290}, {"referenceID": 12, "context": "A related notion is that of matrix coherence, which has been of interest in recently popular problems such as matrix completion and Nystr\u00f6m-based low-rank matrix approximation [13, 46].", "startOffset": 176, "endOffset": 184}, {"referenceID": 44, "context": "A related notion is that of matrix coherence, which has been of interest in recently popular problems such as matrix completion and Nystr\u00f6m-based low-rank matrix approximation [13, 46].", "startOffset": 176, "endOffset": 184}, {"referenceID": 27, "context": "Statistical leverage scores have a long history in statistical data analysis, where they have been used for outlier detection in regression diagnostics [29, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 13, "context": "Statistical leverage scores have a long history in statistical data analysis, where they have been used for outlier detection in regression diagnostics [29, 14].", "startOffset": 152, "endOffset": 160}, {"referenceID": 20, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 31, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 11, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 19, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 43, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 21, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 238, "endOffset": 262}, {"referenceID": 30, "context": "Statistical leverage scores have also proved crucial recently in the development of improved worst-case randomized matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists [21, 33, 12, 20, 45, 22]; see [32] for a detailed discussion.", "startOffset": 268, "endOffset": 272}, {"referenceID": 24, "context": ", the basis provided by the Singular Value Decomposition (SVD) or a basis provided by a QR decomposition [26], and then use that basis to compute diagonal elements of the projection matrix onto the span of that basis.", "startOffset": 105, "endOffset": 109}, {"referenceID": 43, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 174, "endOffset": 182}, {"referenceID": 21, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 174, "endOffset": 182}, {"referenceID": 37, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 43, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 20, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 31, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 11, "context": ", either rapidly approximated or rapidly uniformized at the preprocessing step) in developing fast randomized algorithms for matrix problems such as least-squares regression [45, 22] and low-rank matrix approximation [39, 45, 21, 33, 12].", "startOffset": 217, "endOffset": 237}, {"referenceID": 30, "context": "See [32] for a detailed discussion.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "As an example, the CUR decomposition of [21, 33] essentially computes pi = li/k, for all i \u2208 {1, .", "startOffset": 40, "endOffset": 48}, {"referenceID": 31, "context": "As an example, the CUR decomposition of [21, 33] essentially computes pi = li/k, for all i \u2208 {1, .", "startOffset": 40, "endOffset": 48}, {"referenceID": 1, "context": "On the other hand, the computational bottleneck for random projection algorithms is the application of the random projection, which is sped up by using variants of the Fast Johnson-Lindenstrauss Transform [2, 3].", "startOffset": 205, "endOffset": 211}, {"referenceID": 2, "context": "On the other hand, the computational bottleneck for random projection algorithms is the application of the random projection, which is sped up by using variants of the Fast Johnson-Lindenstrauss Transform [2, 3].", "startOffset": 205, "endOffset": 211}, {"referenceID": 19, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 49, "endOffset": 61}, {"referenceID": 20, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 49, "endOffset": 61}, {"referenceID": 31, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 49, "endOffset": 61}, {"referenceID": 43, "context": "In particular, the random sampling algorithms of [20, 21, 33] for least-squares approximation and low-rank matrix approximation now run in time that is essentially the same as the best corresponding random projection algorithm for those problems [45].", "startOffset": 246, "endOffset": 250}, {"referenceID": 42, "context": "Recently, high-quality numerical implementations of variants of the basic randomized matrix algorithms have proven superior to traditional deterministic algorithms [44, 43, 6].", "startOffset": 164, "endOffset": 175}, {"referenceID": 41, "context": "Recently, high-quality numerical implementations of variants of the basic randomized matrix algorithms have proven superior to traditional deterministic algorithms [44, 43, 6].", "startOffset": 164, "endOffset": 175}, {"referenceID": 5, "context": "Recently, high-quality numerical implementations of variants of the basic randomized matrix algorithms have proven superior to traditional deterministic algorithms [44, 43, 6].", "startOffset": 164, "endOffset": 175}, {"referenceID": 7, "context": "More generally, density functional theory [8] and uncertainty quantification [7] are two scientific computing areas where computing the diagonal elements of functions (such as a projection or inverse) of very large input matrices is common.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "More generally, density functional theory [8] and uncertainty quantification [7] are two scientific computing areas where computing the diagonal elements of functions (such as a projection or inverse) of very large input matrices is common.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "For example, in the former case, \u201cheuristic\u201d methods based on using Chebychev polynomials have been used in numerical linear algebra to compute the diagonal elements of the projector [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 27, "context": "The statistical leverage scores and the scores relative to the best rank-k approximation to A are equal to the diagonal elements of the socalled \u201chat matrix\u201d [29, 15].", "startOffset": 158, "endOffset": 166}, {"referenceID": 14, "context": "The statistical leverage scores and the scores relative to the best rank-k approximation to A are equal to the diagonal elements of the socalled \u201chat matrix\u201d [29, 15].", "startOffset": 158, "endOffset": 166}, {"referenceID": 27, "context": "As such, they have a natural statistical interpretation in terms of the \u201cleverage\u201d or \u201cinfluence\u201d associated with each of the data points [29, 14, 15].", "startOffset": 138, "endOffset": 150}, {"referenceID": 13, "context": "As such, they have a natural statistical interpretation in terms of the \u201cleverage\u201d or \u201cinfluence\u201d associated with each of the data points [29, 14, 15].", "startOffset": 138, "endOffset": 150}, {"referenceID": 14, "context": "As such, they have a natural statistical interpretation in terms of the \u201cleverage\u201d or \u201cinfluence\u201d associated with each of the data points [29, 14, 15].", "startOffset": 138, "endOffset": 150}, {"referenceID": 27, "context": "In the context of regression problems, the i leverage score quantifies the leverage or influence of the i constraint/row of A on the solution of the overconstrained least squares optimization problem minx \u2016Ax\u2212 b\u20162 and the (i, j)-th cross leverage score quantifies how much influence or leverage the i data point has on the j least-squares fit (see [29, 14, 15] for details).", "startOffset": 348, "endOffset": 360}, {"referenceID": 13, "context": "In the context of regression problems, the i leverage score quantifies the leverage or influence of the i constraint/row of A on the solution of the overconstrained least squares optimization problem minx \u2016Ax\u2212 b\u20162 and the (i, j)-th cross leverage score quantifies how much influence or leverage the i data point has on the j least-squares fit (see [29, 14, 15] for details).", "startOffset": 348, "endOffset": 360}, {"referenceID": 14, "context": "In the context of regression problems, the i leverage score quantifies the leverage or influence of the i constraint/row of A on the solution of the overconstrained least squares optimization problem minx \u2016Ax\u2212 b\u20162 and the (i, j)-th cross leverage score quantifies how much influence or leverage the i data point has on the j least-squares fit (see [29, 14, 15] for details).", "startOffset": 348, "endOffset": 360}, {"referenceID": 45, "context": "Historically, these quantities have been widely-used for outlier identification in diagnostic regression analysis [47, 16].", "startOffset": 114, "endOffset": 122}, {"referenceID": 15, "context": "Historically, these quantities have been widely-used for outlier identification in diagnostic regression analysis [47, 16].", "startOffset": 114, "endOffset": 122}, {"referenceID": 8, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 40, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 36, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "[9, 42, 38, 30, 32].", "startOffset": 0, "endOffset": 19}, {"referenceID": 39, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 31, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 18, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 38, "context": "In genetics, dense matrices of size thousands by hundreds of thousands (a size scale at which even traditional deterministic QR algorithms fail to run) constructed from DNA Single Nucleotide Polymorphisms (SNP) data are increasingly common, and the statistical leverage scores can correlate strongly with other metrics of genetic interest [41, 33, 19, 40].", "startOffset": 339, "endOffset": 355}, {"referenceID": 39, "context": "Our main result will permit the computation of these scores and related quantities for significantly larger SNP data sets than has been possible previously [41, 19, 40, 24].", "startOffset": 156, "endOffset": 172}, {"referenceID": 18, "context": "Our main result will permit the computation of these scores and related quantities for significantly larger SNP data sets than has been possible previously [41, 19, 40, 24].", "startOffset": 156, "endOffset": 172}, {"referenceID": 38, "context": "Our main result will permit the computation of these scores and related quantities for significantly larger SNP data sets than has been possible previously [41, 19, 40, 24].", "startOffset": 156, "endOffset": 172}, {"referenceID": 23, "context": ", [25].", "startOffset": 2, "endOffset": 6}, {"referenceID": 5, "context": "The state of the art here is the Blendenpik algorithm of [6] and the LSRN algorithm of [34].", "startOffset": 57, "endOffset": 60}, {"referenceID": 32, "context": "The state of the art here is the Blendenpik algorithm of [6] and the LSRN algorithm of [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "that random projection algorithms should be incorporated into future versions of Lapack\u201d [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 35, "context": "The Moore-Penrose pseudoinverse of A is the d\u00d7n matrix defined by A\u2020 = V \u03a3\u22121UT [37].", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "The SVD is important for a number of reasons [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "3/r with probability 1/6 each and zero otherwise (with probability 2/3) [1].", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "1 of [1]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "For our main results, we will also need a stronger requirement than the simple \u01eb-JLT and so we will use a version of the Fast Johnson-Lindenstrauss Transform (FJLT), which was originally introduced in [2, 3].", "startOffset": 201, "endOffset": 207}, {"referenceID": 2, "context": "For our main results, we will also need a stronger requirement than the simple \u01eb-JLT and so we will use a version of the Fast Johnson-Lindenstrauss Transform (FJLT), which was originally introduced in [2, 3].", "startOffset": 201, "endOffset": 207}, {"referenceID": 19, "context": "The next lemma follows from the definition of an \u01eb-FJLT, and its proof can be found in [20, 22].", "startOffset": 87, "endOffset": 95}, {"referenceID": 21, "context": "The next lemma follows from the definition of an \u01eb-FJLT, and its proof can be found in [20, 22].", "startOffset": 87, "endOffset": 95}, {"referenceID": 21, "context": "Note that the RHT has also been crucial in the development of o(nd) randomized algorithms for the general overconstrained LS problem [22] and its variants have been used to provide high-quality numerical implementations of such randomized algorithms [44, 6].", "startOffset": 133, "endOffset": 137}, {"referenceID": 42, "context": "Note that the RHT has also been crucial in the development of o(nd) randomized algorithms for the general overconstrained LS problem [22] and its variants have been used to provide high-quality numerical implementations of such randomized algorithms [44, 6].", "startOffset": 250, "endOffset": 257}, {"referenceID": 5, "context": "Note that the RHT has also been crucial in the development of o(nd) randomized algorithms for the general overconstrained LS problem [22] and its variants have been used to provide high-quality numerical implementations of such randomized algorithms [44, 6].", "startOffset": 250, "endOffset": 257}, {"referenceID": 5, "context": "(Variants of this basic construction that relax this assumption and that are more appropriate for numerical implementation have been described and evaluated in [6].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "Third, if we only need to access r elements in the transformed vector, then those r elements can be computed in O(n log2 r) time [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 17, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 19, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 20, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 21, "context": "Using the sampling matrix formalism described previously [18, 20, 21, 22], we will represent the operation of randomly sampling r rows of an n\u00d7 d matrix A using an r \u00d7 n linear sampling operator S .", "startOffset": 57, "endOffset": 73}, {"referenceID": 21, "context": "We summarize this discussion in the following lemma (which is essentially a combination of Lemmas 3 and 4 from [22], restated to fit our notation).", "startOffset": 111, "endOffset": 115}, {"referenceID": 21, "context": "This is where the SRHT enters, since it preserves important structures of A, in particular its rank, by first rotating A to a random basis and then uniformly sampling rows from the rotated matrix (see [22] for more details).", "startOffset": 201, "endOffset": 205}, {"referenceID": 42, "context": "This preprocessing is reminiscent of how [44, 6] preprocessed the input to provide numerical implementations of the fast relative-error algorithm [22] for approximate LS approximation.", "startOffset": 41, "endOffset": 48}, {"referenceID": 5, "context": "This preprocessing is reminiscent of how [44, 6] preprocessed the input to provide numerical implementations of the fast relative-error algorithm [22] for approximate LS approximation.", "startOffset": 41, "endOffset": 48}, {"referenceID": 21, "context": "This preprocessing is reminiscent of how [44, 6] preprocessed the input to provide numerical implementations of the fast relative-error algorithm [22] for approximate LS approximation.", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 112, "endOffset": 116}, {"referenceID": 39, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 218, "endOffset": 226}, {"referenceID": 31, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 218, "endOffset": 226}, {"referenceID": 25, "context": ", we might be seeking a low-rank approximation to the matrix with respect to the spectral [21] or the Frobenius [12] norm, or we might be seeking useful features or data points in downstream data analysis applications [41, 33], or we might be seeking to develop high-quality numerical implementations of low-rank matrix approximation algorithms [27], etc.", "startOffset": 345, "endOffset": 349}, {"referenceID": 25, "context": "essentially proven in [27], but see also [43] for computational details; we will use the version of the lemma that appeared in [10].", "startOffset": 22, "endOffset": 26}, {"referenceID": 41, "context": "essentially proven in [27], but see also [43] for computational details; we will use the version of the lemma that appeared in [10].", "startOffset": 41, "endOffset": 45}, {"referenceID": 9, "context": "essentially proven in [27], but see also [43] for computational details; we will use the version of the lemma that appeared in [10].", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "(See also the conference version [11], but in the remainder we refer to the technical report version [10] for consistency of numbering.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "(See also the conference version [11], but in the remainder we refer to the technical report version [10] for consistency of numbering.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": ") Note that for our purposes in this section, the computation of Y is not relevant and we defer the reader to [27, 10] for details.", "startOffset": 110, "endOffset": 118}, {"referenceID": 9, "context": ") Note that for our purposes in this section, the computation of Y is not relevant and we defer the reader to [27, 10] for details.", "startOffset": 110, "endOffset": 118}, {"referenceID": 9, "context": "This version of the above lemma is proven in [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "Then, the above lemma is proven in [10].", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": "1 A related estimator for the leverage scores Magdon-Ismail in [31] presented the following algorithm to estimate the statistical leverage scores: given as input an n\u00d7 d matrix A, with n \u226b d, the algorithm proceeds as follows.", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": "[31] argued that the output p\u0303i achieves an O(ln 2 n) approximation to all of the (normalized) statistical leverage scores of A in roughly O(nd2/ ln n) time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "1 of [22] to get12 that", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "1 of [22] with A = V T and note that \u2225 V T \u2225", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "The resulting algorithm could be theoretically analyzed following the lines of [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 34, "context": "3 Extension to streaming environments In this section, we consider the estimation of the leverage scores and of related statistics when the input data set is so large that an appropriate way to view the data is as a data stream [36].", "startOffset": 228, "endOffset": 232}], "year": 2012, "abstractText": "The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr\u00f6mbased low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n\u00d7 d matrix A, with n \u226b d, and that returns as output relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O(nd) time required by the n\u00e4\u0131ve algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n \u2248 d, and the extension to streaming environments.", "creator": "LaTeX with hyperref package"}}}