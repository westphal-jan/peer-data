{"id": "1109.5647", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2011", "title": "Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization", "abstract": "Stochastic gradient descent (SGD) with averaging is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be at most O(\\log(T)/T). However, recent results showed that using a different algorithm, one can get an optimal O(1/T) rate. This might lead one to believe that SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the convergence rate of SGD with averaging in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T) rate. However, for non-smooth problems, the convergence rate might really be \\Omega(\\log(T)/T), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T) step, and no significant change of the algorithm is necessary.", "histories": [["v1", "Mon, 26 Sep 2011 17:24:52 GMT  (13kb)", "http://arxiv.org/abs/1109.5647v1", null], ["v2", "Wed, 28 Sep 2011 18:43:20 GMT  (42kb,D)", "http://arxiv.org/abs/1109.5647v2", "New version with experimental results and a few other modifications"], ["v3", "Sun, 2 Oct 2011 23:14:46 GMT  (42kb,D)", "http://arxiv.org/abs/1109.5647v3", "Fixed some typos"], ["v4", "Mon, 14 May 2012 19:06:53 GMT  (207kb,D)", "http://arxiv.org/abs/1109.5647v4", "Full version of the ICML 2012 paper"], ["v5", "Tue, 3 Jul 2012 22:06:45 GMT  (199kb,D)", "http://arxiv.org/abs/1109.5647v5", "Full version of the ICML 2012 paper"], ["v6", "Thu, 5 Jul 2012 14:08:50 GMT  (207kb,D)", "http://arxiv.org/abs/1109.5647v6", "Full version of the ICML 2012 paper"], ["v7", "Sun, 9 Dec 2012 21:19:27 GMT  (199kb,D)", "http://arxiv.org/abs/1109.5647v7", "Updated version which fixes a bug in the proof of lemma 1 and modifies the step size choice. As a result, constants are changed throughout the paper"]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["alexander rakhlin", "ohad shamir", "karthik sridharan"], "accepted": true, "id": "1109.5647"}, "pdf": {"name": "1109.5647.pdf", "metadata": {"source": "CRF", "title": "Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization", "authors": ["Ohad Shamir"], "emails": ["ohadsh@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 110 9.56 47v1 [cs.LG] 2 6SE p"}, {"heading": "1 Introduction", "text": "This is one of the simplest and most popular first-order methods for solving convex learning problems. Given a convex loss function and a training series of T examples, SGD can be used to obtain a sequence of T predictors whose average has a generalization error that converges (with T) in the class of predictors we consider optimal. The common framework for analyzing such first-order algorithms is about stochastic optimization, where our goal is to optimize an unknown convex function F, since only unbiased estimates of F's subgradients are available (see Sec. 2 for a more precise definition).An important special case is when F is strongly convex (intuitively, can be constrained by a lower quadratic function). Such functions occur, for example, in support vector machines and other regulated learning algorithms."}, {"heading": "2 Preliminaries", "text": "We assume that we provide a minimum of analysis. < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p \"p > p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" > \"p\" p \">\" p \"p\" p \">\" p \"p\" p \"p\" p \">\" p \"p\" p \"p\" p \">\" p \"p\" p \"p\" p \"p\" > \"p\" p \"p\" p \"p\" > \"p\" p \"p\" p \"p\" > \"p\" p \"p\" p \"p\" p \"p\" > \"p\" p \"p\" p"}, {"heading": "3 Smooth Functions", "text": "We begin by considering the case in which the expected function F (\u00b7) is both strongly convex and smooth. < W (1 / T) for the last predictor wT of SGD. \u2212 W (1 / 2) for the last predictor wD. \u2212 W (1 / 2) for the last predictor wD. \u2212 W (1 / 2) for the last predictor wD. \u2212 W (1 / 2) for the last predictor wD. \u2212 W (1 / 2) for the last predictor wD. \u2212 W (1 / 2) for the last predictor wD. \u2212 W (1 / 2) for the last predictor wD. \u2212 W (2 / 2) for the second pick. \u2212 W (2 / 2) for the second pick. \u2212 W (2 / 2) for the second pick. \u2212 W (2 / 2) for the second pick. \u2212 W (2 / 2) for the second pick. \u2212 W (2 / 2) for the second pick."}, {"heading": "4 Non-Smooth Functions", "text": "Let us now turn to the more general case where the function F may not be smooth (i.e. there is no constant that fulfills Eq (2). - In the context of learning, this can happen when we try to learn a predictor in relation to a non-smooth loss function, like the hinge loss.As discussed above, SGD shows with an algorithm rate different from SGD that one can obtain a rate of O (1 / T) even in the non-smooth case. This could lead us to the assumption that a rate of O (1 / T) is possible in the non-smooth case, and that the O (log (T) / T) analysis is simply not smooth."}, {"heading": "6 Discussion", "text": "In this paper, we analyzed the behavior of SGD using averages for strongly convex stochastic optimization problems. We showed that this simple and well-known algorithm always works optimally when the underlying function is also smooth, but can be suboptimal for non-smooth problems. However, a simple modification of the average step is sufficient to restore the optimal rate, and a more sophisticated algorithm is not necessary. There are still some open questions. For example, all our results relate to limits that are held in expectation, and it remains to be seen whether high probability limits can be reached at a similar rate. In addition, we are currently conducting experiments to complement our theoretical results and examine the practical implications of our results."}, {"heading": "A Technical Lemmas", "text": "3. Let a > 1, b \u00b2 0 and x1 \u00b2 [0, D] be any constant. Let x2, x3,.. be a non-negative sequence, the + 1 \u2264 (1 \u2212 a) xt + b \u00b2. Then applies to all t, xt \u2264 max (D, b / (a \u2212 1). Let m = max (D, b / (a \u2212 1). The proof is by simple induction. The assertion clearly applies to t = 1. Now we assume that xt \u2264 mt. Then it is sufficient to show that (1 \u2212 a) m + b \u00b2 t2 \u2264 m + 1.This can be simplified to b (t + 1) \u2264 m (a \u2212 1) t + a).which is clearly considered m \u00b2."}], "references": [{"title": "Robust stochastic approximation approach to stochastic programming", "author": ["G. Lan A. Nemirovski", "A. Juditsky", "A. Shapiro"], "venue": "SIAM J. Optim.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["F. Bach", "E. Moulines"], "venue": "In NIPS (to appear),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Beyond the regret minimization barrier: An optimal algorithm for stochastic stronglyconvex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "In COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Primal-dual subgradient methods for minimizing uniformly convex functions. Technical Report (August 2010), available at http://hal.archives-ouvertes.fr/docs/00/50/89/33/ PDF/Strong-hal.pdf", "author": ["A. Juditsky", "Y. Nesterov"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["H. Kushner", "G. Yin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Pegasos: primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "This rate is obtained using the analysis of the algorithm in the harder setting of online learning [3], with the O(log(T )/T ) rate obtained via an online-to-batch conversion (see [4] for more details).", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "This rate is obtained using the analysis of the algorithm in the harder setting of online learning [3], with the O(log(T )/T ) rate obtained via an online-to-batch conversion (see [4] for more details).", "startOffset": 180, "endOffset": 183}, {"referenceID": 3, "context": "Surprisingly, a recent paper by Hazan and Kale [4] showed that in fact, an O(log(T )/T ) is not the best that one can achieve for strongly convex stochastic problems.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "A very similar algorithm was also presented in [5].", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "Indeed, the whole motivation of [4] was that the standard online analysis is too loose to analyze the stochastic setting appropriately.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Following the paradigm of [4], we analyze the algorithm directly in the stochastic setting, and avoid an online analysis with an online-to-batch conversion.", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "In terms of related work, we note that the performance of SGD in a stochastic setting has been extensively researched in stochastic approximation theory (see for instance [6]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "We also note that a finite-sample analysis of SGD in the stochastic setting was recently presented in [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "For example, smoothness assumptions do not cover the application of SGD to support vector machines (as in [7]), since it uses a non-smooth loss function.", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "For stochastic optimization of \u03bb-strongly functions, the standard analysis (through online learning) focuses on the step size \u03b7t being exactly 1/\u03bbt [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "Note that this is a somewhat weaker assumption than [4], which required that \u2016\u011dt\u2016 \u2264 G with probability 1, since we focus only on bounds which hold in expectation.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "These types of assumptions are common in the literature, and we note that our assumption can be shown to hold if W is a bounded domain, or alternatively, if w1 is initialized not too far from w and F satisfies certain technical conditions (see for instance the proof of Theorem 1 in [7]).", "startOffset": 283, "endOffset": 286}, {"referenceID": 0, "context": "We note that this result is well-known in the stochastic approximation literature (see for instance [1] and references therein), and we include a proof for completeness.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "Moreover, [4] showed that for using a different algorithm than SGD, one can obtain a rate of O(1/T ) even in the non-smooth case.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "There exists a stochastic 1-strongly convex problem over the domain W = [0, 1], with a global minimum w at 0 and E[\u2016\u011dt\u2016] \u2264 d + 5, with the following property: If SGD is initialized at any point in W , and ran with \u03b7t = c/t, then for any T \u2265 T0 + 1, where T0 = max{2, c/2}, we have E[F (w\u0304T )\u2212 F (w)] \u2265 c 16T T\u22121", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "The SGD iterate can be written separately for the first coordinate as wt+1,1 = \u03a0[0,1] ((1 \u2212 \u03b7t)wt,1 \u2212 \u03b7tZt) (3) Fix some t \u2265 T0, and suppose first that Zt \u2264 \u22121/2.", "startOffset": 80, "endOffset": 85}, {"referenceID": 0, "context": "Conditioned on this event, we have wt+1,1 \u2265 \u03a0[0,1] (", "startOffset": 45, "endOffset": 50}, {"referenceID": 0, "context": "\u2265 \u03a0[0,1](\u03b7t/2) \u2265 \u03b7t/2, since t \u2265 T0 implies \u03b7t = c/t \u2264 2.", "startOffset": 3, "endOffset": 8}, {"referenceID": 3, "context": "To get the optimal O(1/T ) rate for any F , we might turn to the algorithms of [4] and [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "To get the optimal O(1/T ) rate for any F , we might turn to the algorithms of [4] and [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "This applies to any relevant step size c/\u03bbt, and matches the optimal guarantees in [4] up to constant factors.", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "However, this is shown for standard SGD, as opposed to the more specialized algorithm of [4].", "startOffset": 89, "endOffset": 92}], "year": 2017, "abstractText": "Stochastic gradient descent (SGD) with averaging is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be at most O(log(T )/T ). However, recent results showed that using a different algorithm, one can get an optimal O(1/T ) rate. This might lead one to believe that SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the convergence rate of SGD with averaging in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T ) rate. However, for non-smooth problems, the convergence rate might really be \u03a9(log(T )/T ), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T ) step, and no significant change of the algorithm is necessary.", "creator": "LaTeX with hyperref package"}}}