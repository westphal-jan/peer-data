{"id": "1110.1394", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2011", "title": "Learning Sentence-internal Temporal Relations", "abstract": "In this paper we propose a data intensive approach for inferring sentence-internal temporal relations. Temporal inference is relevant for practical NLP applications which either extract or synthesize temporal information (e.g., summarisation, question answering). Our method bypasses the need for manual coding by exploiting the presence of markers like after\", which overtly signal a temporal relation. We first show that models trained on main and subordinate clauses connected with a temporal marker achieve good performance on a pseudo-disambiguation task simulating temporal inference (during testing the temporal marker is treated as unseen and the models must select the right marker from a set of possible candidates). Secondly, we assess whether the proposed approach holds promise for the semi-automatic creation of temporal annotations. Specifically, we use a model trained on noisy and approximate data (i.e., main and subordinate clauses) to predict intra-sentential relations present in TimeBank, a corpus annotated rich temporal information. Our experiments compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements. We evaluate performance against gold standard corpora and also against human subjects.", "histories": [["v1", "Thu, 6 Oct 2011 20:55:54 GMT  (827kb)", "http://arxiv.org/abs/1110.1394v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["m lapata", "a lascarides"], "accepted": true, "id": "1110.1394"}, "pdf": {"name": "1110.1394.pdf", "metadata": {"source": "CRF", "title": "Learning Sentence-internal Temporal Relations", "authors": ["Mirella Lapata", "Alex Lascarides"], "emails": ["MLAP@INF.ED.AC.UK", "ALEX@INF.ED.AC.UK"], "sections": [{"heading": null, "text": "Our experiments focus on two tasks that are relevant to applications that either extract or synthesize temporal information (e.g. summarizing, answering questions). Our first task focuses on interpretation: using a subordinate clause and a main clause, we identify the temporal relationship between them. The second task is a fusion task: when we specify two clauses and a temporal relationship between them, we decide which of them contains the temporal marker (i.e., identify the subordinate clause and the main clause). We compare and compare several probable models that differ in their feature space, linguistic assumptions and data requirements. We evaluate performance against a gold standard corpus and also against human subjects performing the same tasks. The best model achieves a 69.1% F score by determining the temporal relationship between two clauses and a 93.4% F score in distinguishing between the main clause and the known sub-clause."}, {"heading": "1. Introduction", "text": "The processing of temporal phenomena has recently attracted a lot of attention, in part because of their increasing importance for potential applications (e.g. 2001).In multi-page summaries, information contained in the summary is often extracted from different documents and synthesized into a meaningful text. However, knowledge of the temporal order of events is important in determining which contents should be communicated (interpretation) and in correctly merging and presenting information in the summary (generation).In fact, ignoring temporal relationships either in the phase of gathering information or in the summary generation is potentially results in a summary that is misleading in terms of the temporal information in the original documents. In the search for answers, information about the temporal properties of events (e.g. when did X resign or how events relate to each other (e.g. Did X resign before Y?) is an important first step toward the automatic handling of temporal phenomena?), the analysis and identification of temporal expressions is the absolute date-specific expressions."}, {"heading": "2. Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "3. Problem Formulation", "text": "iSe's, he says, \"is that we are able to hide ourselves.\" iSe \"iSe aht,\" he says, \"is that we are able to be able to be.\""}, {"heading": "4. Parameter Estimation", "text": "We can estimate the parameters of our models from a large corpus. In their simplest form, we first identify clauses in a hypotactic relationship, i.e. principal clauses that make up the subordinate proposition. Next, in the training phase, we estimate the probabilities P (ahM; iijt j) and P (ahS; iijt j) for the disjunctive model by simply counting the appearance of the characters ahM; ii and ahS; ii with the marker t (i.e. f (ahM; ii; t j)) and (f (ahS; iijt j). Essentially, in this model, we assume that the corpus is representative of the way different temporal markers are used in English."}, {"heading": "4.1 Data Extraction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which"}, {"heading": "4.2 Model Features", "text": "In fact, the fact is that most of them are able to go to another world, to go to another world, to not find themselves, to not find themselves."}, {"heading": "5. Experiment 1: Sentence Interpretation", "text": "We have recalled that we received 83,810 main and sub-level pairs for the human study evaluated in Experiment 3. We have performed parameter adjustments on a developmental basis; all of our results are reported on the invisible test set unless otherwise stated. We compare the performance of the conjunctive and disjunctive models, evaluating the effects of the characteristics on the time interpretation task. Furthermore, we compare the performance of the two proposed models against a base model that includes a word-based feature space (see (7), where P (ahM; iiiijt j) and P (iyy)."}, {"heading": "6. Experiment 2: Sentence Fusion", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "7. Experiment 3: Human Evaluation", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able to live as in another world, in which they are able to live as in another world, as in another world in which they are able to live and live."}, {"heading": "8. General Discussion", "text": "uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuzuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu"}, {"heading": "Acknowledgments", "text": "The authors would like to thank the EPSRC (grant numbers GR / R40036 / 01 and GR / T04540 / 01) and Regina Barzilay and Frank Keller for their helpful comments and suggestions."}, {"heading": "Appendix A. Experimental Materials for Human Evaluation", "text": "The phrases were taken from the BLLIP corpus given in section 4.1.30LEARNING TEMPORAL RELATIONS31LAPATA & LASCARIDES343.3 K 6.7 K 10 K 13.4K 16.8K 20.1K 23.4K 26.8K 30.1K 33.5K 36.8K 40.2K 43.6K 46.9K 50.3K 53.6K 56.9K 60.3KNumber of cases in training data4045505570808590A ccur accuracy (%) Word-based Baseline Conjunctive Model Disjunctive Model Disjunctive Model4550560657A ccur (%).Alex etcabiliacy Number of cases in training data40455055707070808590A ccur accuracy (%) of the University of S\u00f3raldo-based Baseline Conjunctive Model Disjunctive Model Disjunctive Model4550560657A ccur (%).Alex etcabiliacy Number of cases in training data40455055707070708085590A ccur accuracy (%) of the University of S\u00f3raldo-based Baseline Conjunctive Model Disjunctive Modelative Model4550560657A ccur (%) Alex (%) 70ukelelc70ukelel470kelelkel470808080elel445elUK @"}, {"heading": "1 Introduction", "text": "The only question is whether this is a way in which people put themselves and themselves in a situation in which they maneuver themselves into one situation, in which they maneuver themselves into another situation, in which they maneuver themselves into another, in which they are drawn into another, in which they are drawn into another, in which they are drawn into another - and are drawn into another, in which they are drawn into another, in which they are drawn into another, in which they are drawn into another, in which they are drawn into - and are drawn into - and thrown into it."}, {"heading": "2 The Model", "text": "In view of a main clause and a minor clause attached to it, it is our task to derive the temporal linkage of the two clauses (SM, t j, SS). < S (S, S, S, S, S, S) derive. < S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S"}, {"heading": "3 Parameter Estimation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data Extraction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2 Model Features", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "4 Experiment 1: Interpretation and Fusion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Method", "text": "We received 83,810 main and secondary sets, which were randomly divided into training (80%), development (10%) and test data (10%). Eighty randomly selected pairs from the test data were reserved for the human study reported in Experiment 2. We performed parameter tuning on the development set; all our results are reported via the invisible test set, unless otherwise stated."}, {"heading": "4.2 Results", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "5 Experiment 2: Human Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2 Results", "text": "Interpretation Fusion K% K% H-H.410 45.0.490 70.0 H-G.421 46.9.522 79.2 E-H.390 44.3.468 70.0 E-G.413 47.5.489 75.0 We calculate pairs of matches and indicate the mean. In Table 6, H refers to the subjects, G to the gold standard and E to the ensemble. As shown in Table 6, there is less agreement among the subjects on the interpretation problem than in the statutory merger problem. This is expected because some of the markers are semantically similar and in some cases more than one marker is compatible with the meaning of the two clauses. Also, note that neither the model nor the subjects have access to the context whose markers must be deduced (we discuss this further in Section 6). Additional analyses of the interpretation data for gold show that the majority of the discrepancies arise as once for clauses."}, {"heading": "6 Discussion", "text": "In this paper, we have proposed a data-intensive approach to deduce the temporal relationships of events. We have introduced a model that learns temporal relations from sentences in which temporal information is explicitly given via time markers, and this model can then be used in cases where open temporal markers are absent. We have also evaluated our model against a sentence fusion task, which is relevant for applications such as summarizing or answering questions in which sentence fragments must be combined into a fluent sentence. For the merger task, our model determines the appropriate sequence between a time marker and two clauses. We have experimented with a variety of linguistically motivated features and demonstrated that it is possible to extract semantic information from companies even when it is not commented semantically. We have achieved an accuracy of 70.7% on the interpretation task and 97.4% on the fusion task. This performance is a significant improvement over the baseline and compares favorably with human performance on the same tasks."}, {"heading": "Acknowledgments", "text": "The authors are supported by the EPSRC grant number GR / R40036. Thanks to Regina Barzilay and Frank Keller for helpful comments and suggestions."}], "references": [{"title": "Natural Language Understanding", "author": ["J. Allen"], "venue": "Benjamin Cummins.", "citeRegEx": "Allen,? 1995", "shortCiteRegEx": "Allen", "year": 1995}, {"title": "Logics of Conversation", "author": ["N. Asher", "A. Lascarides"], "venue": null, "citeRegEx": "Asher and Lascarides,? \\Q2003\\E", "shortCiteRegEx": "Asher and Lascarides", "year": 2003}, {"title": "Probabilistic head-driven parsing for discourse structure", "author": ["J. Baldridge", "A. Lascarides"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL)", "citeRegEx": "Baldridge and Lascarides,? \\Q2005\\E", "shortCiteRegEx": "Baldridge and Lascarides", "year": 2005}, {"title": "Information Fusion for Multi-Document Summarization: Praphrasing and Generation", "author": ["R. Barzilay"], "venue": "Ph.D. thesis, Columbia University.", "citeRegEx": "Barzilay,? 2003", "shortCiteRegEx": "Barzilay", "year": 2003}, {"title": "Timeml-compliant text analysis for temporal reasoning", "author": ["B. Boguraev", "R.K. Ando"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Boguraev and Ando,? \\Q2005\\E", "shortCiteRegEx": "Boguraev and Ando", "year": 2005}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, 2(24), 123\u2013140.", "citeRegEx": "Breiman,? 1996a", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Stacked regressions", "author": ["L. Breiman"], "venue": "Machine Learning, 3(24), 49\u201364.", "citeRegEx": "Breiman,? 1996b", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory", "author": ["L. Carlson", "D. Marcu", "M. Okurowski"], "venue": "In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue,", "citeRegEx": "Carlson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2001}, {"title": "Estimating probabilities: a crucial task in machine learning", "author": ["B. Cestnik"], "venue": "Proceedings of the 16th European Conference on Artificial Intelligence, pp. 147\u2013149, Stockholm, Sweden.", "citeRegEx": "Cestnik,? 1990", "shortCiteRegEx": "Cestnik", "year": 1990}, {"title": "A maximum-entropy-inspired parser", "author": ["E. Charniak"], "venue": "Proceedings of the 1st Conference of the North American Chapter of the Assocation for Computational Linguistics, pp. 132\u2013139, Seattle, WA.", "citeRegEx": "Charniak,? 2000", "shortCiteRegEx": "Charniak", "year": 2000}, {"title": "Human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks", "author": ["K.J. Cherkauer"], "venue": "In Working Notes of the AAAI Workshop on Integrating Multiple Learned Models, pp. 15\u201321, Portland, OR.", "citeRegEx": "Cherkauer,? 1996", "shortCiteRegEx": "Cherkauer", "year": 1996}, {"title": "Supersense tagging of unknown words in wordnet", "author": ["M. Ciaramita", "M. Johnson"], "venue": "In Proceedings of the 8th,", "citeRegEx": "Ciaramita and Johnson,? \\Q2003\\E", "shortCiteRegEx": "Ciaramita and Johnson", "year": 2003}, {"title": "Machine learning research: Four current directions", "author": ["T.G. Dietterich"], "venue": "AI Magazine, 18(4), 97\u2013136.", "citeRegEx": "Dietterich,? 1997", "shortCiteRegEx": "Dietterich", "year": 1997}, {"title": "Selecting tnese aspect and connective words in language generation", "author": ["B. Dorr", "T. Gaasterland"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Dorr and Gaasterland,? \\Q1995\\E", "shortCiteRegEx": "Dorr and Gaasterland", "year": 1995}, {"title": "The effects of aspectual class on the temporal sturcture of discourse: Semantics or pragmatics", "author": ["D. Dowty"], "venue": "Linguistics and Philosophy, 9(1), 37\u201361.", "citeRegEx": "Dowty,? 1986", "shortCiteRegEx": "Dowty", "year": 1986}, {"title": "WordNet: An Electronic Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Tides temporal annotation guidelines", "author": ["L. Ferro", "I. Mani", "B. Sundheim", "G. Wilson"], "venue": "Tech. rep., The MITRE Corporation", "citeRegEx": "Ferro et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ferro et al\\.", "year": 2000}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Proceedings of the 13th International Conference on Machine Learning,", "citeRegEx": "Freund and Shapire,? \\Q1996\\E", "shortCiteRegEx": "Freund and Shapire", "year": 1996}, {"title": "A framework for resolution of time in natural language", "author": ["B. Han", "A. Lavie"], "venue": "ACM Transactions on Asian Language Information Processing (TALIP),", "citeRegEx": "Han and Lavie,? \\Q2004\\E", "shortCiteRegEx": "Han and Lavie", "year": 2004}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions in Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hansen and Salamon,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon", "year": 1990}, {"title": "Algorithms for analyzing the temporal structure of discourse", "author": ["J. Hitzeman", "M. Moens", "C. Grover"], "venue": "In Proceedings of the 7th Meeting of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Hitzeman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hitzeman et al\\.", "year": 1995}, {"title": "Interpretation as abduction", "author": ["J.R. Hobbs", "M. Stickel", "D. Appelt", "P. Martin"], "venue": "Artificial Intelligence,", "citeRegEx": "Hobbs et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hobbs et al\\.", "year": 1993}, {"title": "Tense trees as the finite structure of discourse", "author": ["C. Hwang", "L. Schubert"], "venue": "In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Hwang and Schubert,? \\Q1992\\E", "shortCiteRegEx": "Hwang and Schubert", "year": 1992}, {"title": "From Discourse to the Lexicon: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory", "author": ["H. Kamp", "U. Reyle"], "venue": null, "citeRegEx": "Kamp and Reyle,? \\Q1993\\E", "shortCiteRegEx": "Kamp and Reyle", "year": 1993}, {"title": "From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht", "author": ["H. Kamp", "U. Reyle"], "venue": null, "citeRegEx": "Kamp and Reyle,? \\Q1993\\E", "shortCiteRegEx": "Kamp and Reyle", "year": 1993}, {"title": "The annotation of temporal information in natural language sentences", "author": ["G. Katz", "F. Arosio"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Katz and Arosio,? \\Q2001\\E", "shortCiteRegEx": "Katz and Arosio", "year": 2001}, {"title": "Coherence, Reference and the Theory of Grammar", "author": ["A. Kehler"], "venue": "CSLI Publications, Cambridge University Press.", "citeRegEx": "Kehler,? 2002", "shortCiteRegEx": "Kehler", "year": 2002}, {"title": "Using subcategorisation to resolve verb class ambiguity", "author": ["M. Lapata", "C. Brew"], "venue": "In Proceedings of Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Lapata and Brew,? \\Q1999\\E", "shortCiteRegEx": "Lapata and Brew", "year": 1999}, {"title": "Temporal interpretation, discourse relations and commonsense entailment", "author": ["A. Lascarides", "N. Asher"], "venue": "Linguistics and Philosophy,", "citeRegEx": "Lascarides and Asher,? \\Q1993\\E", "shortCiteRegEx": "Lascarides and Asher", "year": 1993}, {"title": "English Verb Classes and Alternations", "author": ["B. Levin"], "venue": "Chicago University Press.", "citeRegEx": "Levin,? 1995", "shortCiteRegEx": "Levin", "year": 1995}, {"title": "Temporally anchoring and ordering events in news", "author": ["I. Mani", "B. Schiffman"], "venue": null, "citeRegEx": "Mani and Schiffman,? \\Q2005\\E", "shortCiteRegEx": "Mani and Schiffman", "year": 2005}, {"title": "Inferring temporal ordering of events in news", "author": ["I. Mani", "B. Schiffman", "J. Zhang"], "venue": "In Proceedings of the 1st Human Language Technology Conference and Annual Meeting", "citeRegEx": "Mani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2003}, {"title": "A formal and computational synthesis of grosz and sidner\u2019s and mann and thompson\u2019s theories", "author": ["D. Marcu"], "venue": "Workshop on the Levels of Representation of Discourse, pp. 101\u2013 107, Edinburgh.", "citeRegEx": "Marcu,? 1999", "shortCiteRegEx": "Marcu", "year": 1999}, {"title": "An unsupervised approach to recognizing discourse relations", "author": ["D. Marcu", "A. Echihabi"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Marcu and Echihabi,? \\Q2002\\E", "shortCiteRegEx": "Marcu and Echihabi", "year": 2002}, {"title": "Temporal ontology and temporal reference", "author": ["M. Moens", "M.J. Steedman"], "venue": "Computational Linguistics,", "citeRegEx": "Moens and Steedman,? \\Q1988\\E", "shortCiteRegEx": "Moens and Steedman", "year": 1988}, {"title": "The specification of timeml", "author": ["J. Pustejovsky", "B. Ingria", "R. Sauri", "J. Castano", "J. Littman", "R. Gaizauskas", "A. Setzer"], "venue": "The Language of Time: A reader,", "citeRegEx": "Pustejovsky et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2004}, {"title": "Arda summer workshop on graphical annotation toolkit for timeml", "author": ["J. Pustejovsky", "I. Mani", "L. Belanger", "B. Boguraev", "B. Knippen", "J. Litman", "A. Rumshisky", "A. See", "S. Symonen", "J. van Guilder", "L. van Guilder", "M. Verhagen"], "venue": "Tech. rep.", "citeRegEx": "Pustejovsky et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "A Comprehensive Grammar of the English Language", "author": ["R. Quirk", "S. Greenbaum", "G. Leech", "J. Svartvik"], "venue": null, "citeRegEx": "Quirk et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 1985}, {"title": "From temporal expressions to temporal information: Smeantic tagging of news messages", "author": ["F. Schilder", "C. Habel"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Schilder and Habel,? \\Q2001\\E", "shortCiteRegEx": "Schilder and Habel", "year": 2001}, {"title": "A pilot study on annotating temporal relations in text", "author": ["A. Setzer", "R. Gaizauskas"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Setzer and Gaizauskas,? \\Q2001\\E", "shortCiteRegEx": "Setzer and Gaizauskas", "year": 2001}, {"title": "Non Parametric Statistics for the Behavioral Sciences", "author": ["S. Siegel", "N.J. Castellan"], "venue": null, "citeRegEx": "Siegel and Castellan,? \\Q1988\\E", "shortCiteRegEx": "Siegel and Castellan", "year": 1988}, {"title": "Sentence level discourse parsing using syntactic and lexical information", "author": ["R. Soricut", "D. Marcu"], "venue": "In Proceedings of the 1st Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Soricut and Marcu,? \\Q2003\\E", "shortCiteRegEx": "Soricut and Marcu", "year": 2003}, {"title": "Exploiting linguistic cues to classify rhetorical relations", "author": ["C. Sporleder", "A. Lascarides"], "venue": "In Proceedings of the 16th Irish Conference on Artificial Intelligence and Cognitive Science", "citeRegEx": "Sporleder and Lascarides,? \\Q2005\\E", "shortCiteRegEx": "Sporleder and Lascarides", "year": 2005}, {"title": "Improving accuracy in wordclass tagging through combination of machine learning systems", "author": ["H. van Halteren", "J. Zavrel", "W. Daelemans"], "venue": "Computational Linguistics,", "citeRegEx": "Halteren et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Halteren et al\\.", "year": 2001}, {"title": "An empirical approach to temporal reference resolution", "author": ["J.M. Wiebe", "T.P. O\u2019Hara", "T. \u00d6hrstr\u00f6m Sandgren", "K.J. McKeever"], "venue": "Journal of Artifical Intelligence Research,", "citeRegEx": "Wiebe et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 1998}, {"title": "A multilingual approach to annotating and extracting temporal information", "author": ["G. Wilson", "I. Mani", "B. Sundheim", "L. Ferro"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Wilson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2001}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks, 5, 241\u2013259.", "citeRegEx": "Wolpert,? 1992", "shortCiteRegEx": "Wolpert", "year": 1992}, {"title": "Line has cut 1,900 jobs since it acquired the core assets of the Mulwaukee Road trail line", "author": ["38 Soo"], "venue": null, "citeRegEx": "Soo,? \\Q1985\\E", "shortCiteRegEx": "Soo", "year": 1985}], "referenceMentions": [{"referenceID": 0, "context": "An additional challenge to this task poses the nature of temporal information itself which is often implicit (i.e., not overtly verbalised) and must be inferred using both linguistic and non-linguistic knowledge. Consider the examples in (1) taken from Katz and Arosio (2001). Native speakers can infer that John first met and then kissed the girl; that he left the party after kissing the girl and then walked home; and that the events of talking to her and asking her for her name temporally overlap (and occurred before he left the party).", "startOffset": 16, "endOffset": 276}, {"referenceID": 35, "context": "Pustejovsky et al. (2003) found evidence that this annotation task is sufficiently complex that human annotators can realistically identify only a small number of the temporal relations that hold in reality; i.", "startOffset": 0, "endOffset": 26}, {"referenceID": 21, "context": "This involves complex reasoning over a variety of rich information sources, including representations of domain knowledge and detailed logical forms of the clauses (e.g., Dowty, 1986; Hwang & Schubert, 1992; Hobbs et al., 1993; Lascarides & Asher, 1993; Kamp & Reyle, 1993a; Kehler, 2002).", "startOffset": 164, "endOffset": 288}, {"referenceID": 26, "context": "This involves complex reasoning over a variety of rich information sources, including representations of domain knowledge and detailed logical forms of the clauses (e.g., Dowty, 1986; Hwang & Schubert, 1992; Hobbs et al., 1993; Lascarides & Asher, 1993; Kamp & Reyle, 1993a; Kehler, 2002).", "startOffset": 164, "endOffset": 288}, {"referenceID": 0, "context": "Allen (1995), Hitzeman et al.", "startOffset": 0, "endOffset": 13}, {"referenceID": 0, "context": "Allen (1995), Hitzeman et al. (1995), and Han and Lavie (2004) propose more computationally tractable approaches to inferring temporal information from text, by hand-crafting algorithms which integrate shallow versions of the knowledge sources that are exploited in the above theoretical literature (e.", "startOffset": 0, "endOffset": 37}, {"referenceID": 0, "context": "Allen (1995), Hitzeman et al. (1995), and Han and Lavie (2004) propose more computationally tractable approaches to inferring temporal information from text, by hand-crafting algorithms which integrate shallow versions of the knowledge sources that are exploited in the above theoretical literature (e.", "startOffset": 0, "endOffset": 63}, {"referenceID": 30, "context": "(2003) and Mani and Schiffman (2005) demonstrate that TimeML-compliant annotations are useful for learning a model of temporal relations in news text.", "startOffset": 11, "endOffset": 37}, {"referenceID": 4, "context": "Boguraev and Ando (2005) use semi-supervised learning for recognising events and inferring temporal relations (between two events or between an event and a time expression).", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier.", "startOffset": 95, "endOffset": 117}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For example, for text spans connected with RESULT, it is implied by the semantics of this relation, that the events in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides, 2005). But there is currently no automatic mapping from these discourse structures to their temporal consequences; so although there is potential for eventually using linguistic resources labelled with discourse structure to acquire a model of temporal relations, that potential cannot be presently realised. Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi (2002) whose approach bypasses altogether the need for manual coding in a supervised learning setting.", "startOffset": 96, "endOffset": 1004}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For example, for text spans connected with RESULT, it is implied by the semantics of this relation, that the events in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides, 2005). But there is currently no automatic mapping from these discourse structures to their temporal consequences; so although there is potential for eventually using linguistic resources labelled with discourse structure to acquire a model of temporal relations, that potential cannot be presently realised. Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi (2002) whose approach bypasses altogether the need for manual coding in a supervised learning setting. A key insight in their work is that rhetorical relations (e.g., EXPLANATION and CONTRAST) are sometimes signalled by an unambiguous discourse connective (e.g., because for EXPLANATION and but for CONTRAST). They extract sentences containing such unambiguous markers from a corpus, and then (automatically) identify the text spans connected by the marker, remove the marker and replace it with the rhetorical relation it signals. A Naive Bayes classifier is trained on this automatically labelled data. The model is designed to be maximally simple and employs solely word bigrams as features. Specifically, bigrams are constructed over the cartesian product of words occurring in the two text spans and it is assumed that word pairs are conditionally independent. Marcu and Echihabi demonstrate that such a knowledge-lean approach performs well, achieving an accuracy of 49.70% when distinguishing six relations (over a baseline of 16.67%). However, since the model relies exlusively on word-co-occurrences, an extremely large training corpus (in the order of 40 M sentences) is required to avoid sparse data (see Sporleder and Lascarides (2005) for more detailed discussion).", "startOffset": 96, "endOffset": 2245}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For example, for text spans connected with RESULT, it is implied by the semantics of this relation, that the events in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides, 2005). But there is currently no automatic mapping from these discourse structures to their temporal consequences; so although there is potential for eventually using linguistic resources labelled with discourse structure to acquire a model of temporal relations, that potential cannot be presently realised. Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi (2002) whose approach bypasses altogether the need for manual coding in a supervised learning setting. A key insight in their work is that rhetorical relations (e.g., EXPLANATION and CONTRAST) are sometimes signalled by an unambiguous discourse connective (e.g., because for EXPLANATION and but for CONTRAST). They extract sentences containing such unambiguous markers from a corpus, and then (automatically) identify the text spans connected by the marker, remove the marker and replace it with the rhetorical relation it signals. A Naive Bayes classifier is trained on this automatically labelled data. The model is designed to be maximally simple and employs solely word bigrams as features. Specifically, bigrams are constructed over the cartesian product of words occurring in the two text spans and it is assumed that word pairs are conditionally independent. Marcu and Echihabi demonstrate that such a knowledge-lean approach performs well, achieving an accuracy of 49.70% when distinguishing six relations (over a baseline of 16.67%). However, since the model relies exlusively on word-co-occurrences, an extremely large training corpus (in the order of 40 M sentences) is required to avoid sparse data (see Sporleder and Lascarides (2005) for more detailed discussion). In a sense, when considering the complexity of various models used to infer temporal and discourse relations, Marcu and Echihabi\u2019s (2002) model lies at the simple extreme of the spectrum, whereas the semantics and inference-based approaches to discourse interpretation (e.", "startOffset": 96, "endOffset": 2414}, {"referenceID": 32, "context": "ularly interested in assessing the performance of models on smaller training sets than those used by Marcu and Echihabi (2002); such models will be useful for classifiers that are trained on data sets", "startOffset": 101, "endOffset": 127}, {"referenceID": 30, "context": "Our work differs from Mani et al. (2003) and Boguraev and Ando (2005) in that we do not", "startOffset": 22, "endOffset": 41}, {"referenceID": 4, "context": "(2003) and Boguraev and Ando (2005) in that we do not", "startOffset": 11, "endOffset": 36}, {"referenceID": 32, "context": "We share with Marcu and Echihabi (2002) the use of data with overt markers as a proxy for hand coded temporal relations.", "startOffset": 14, "endOffset": 40}, {"referenceID": 32, "context": "different from theirs, our work departs from Marcu and Echihabi (2002) in three further important ways.", "startOffset": 45, "endOffset": 71}, {"referenceID": 21, "context": ", Hobbs et al., 1993). Many rules typically draw on the relationships between the verbs in both clauses, or the nouns in both clauses, and so on. Both the disjunctive and conjunctive models are different from Marcu and Echihabi\u2019s (2002) model in several respects.", "startOffset": 2, "endOffset": 237}, {"referenceID": 8, "context": "Features with zero counts are smoothed in both models; we adopt the m-estimate with uniform priors, with m equal to the size of the feature space (Cestnik, 1990).", "startOffset": 146, "endOffset": 161}, {"referenceID": 9, "context": "The latter is a Treebank-style, machine-parsed version of the Wall Street Journal (WSJ, years 1987\u201389) which was produced using Charniak\u2019s (2000) parser.", "startOffset": 128, "endOffset": 146}, {"referenceID": 9, "context": "The latter is a Treebank-style, machine-parsed version of the Wall Street Journal (WSJ, years 1987\u201389) which was produced using Charniak\u2019s (2000) parser. Our study focused on the following (confusion) set of temporal markers: fafter, before, while, when, as, once, until, sinceg. We initially compiled a list of all temporal markers discussed in Quirk, Greenbaum, Leech, and Svartvik (1985) and eliminated markers with frequency less than 10 per million in our corpus.", "startOffset": 128, "endOffset": 391}, {"referenceID": 13, "context": "These constraints are perhaps best illustrated in the system of Dorr and Gaasterland (1995) who examine how inherent (i.", "startOffset": 64, "endOffset": 92}, {"referenceID": 1, "context": "Asher and Lascarides (2003) argue that many of the rules for inferring temporal relations should be specified in terms of the semantic class of the verbs, as opposed to the verb forms themselves, so as to maximise the linguistic generalisations captured by a model of temporal relations.", "startOffset": 0, "endOffset": 28}, {"referenceID": 15, "context": "Accordingly, we use two well-known semantic classifications for obtaining some degree of generalisation over the extracted verb occurrences, namely WordNet (Fellbaum, 1998) and the verb classification proposed by Levin (1995).", "startOffset": 156, "endOffset": 172}, {"referenceID": 14, "context": "Accordingly, we use two well-known semantic classifications for obtaining some degree of generalisation over the extracted verb occurrences, namely WordNet (Fellbaum, 1998) and the verb classification proposed by Levin (1995). Verbs in WordNet are classified in 15 broad semantic domains (e.", "startOffset": 157, "endOffset": 226}, {"referenceID": 14, "context": "Accordingly, we use two well-known semantic classifications for obtaining some degree of generalisation over the extracted verb occurrences, namely WordNet (Fellbaum, 1998) and the verb classification proposed by Levin (1995). Verbs in WordNet are classified in 15 broad semantic domains (e.g., verbs of change, verbs of cognition, etc.) often referred to as supersenses (Ciaramita & Johnson, 2003). We therefore mapped the verbs occurring in main and subordinate clauses to WordNet supersenses. (feature VW). Semantically ambiguous verbs will correspond to more than one semantic class. We resolve ambiguity heuristically by always defaulting to the verb\u2019s prime sense (as indicated in WordNet) and selecting its corresponding supersense. In cases where a verb is not listed in WordNet we default to its lemmatised form. Levin (1995) focuses on the relation between verbs and their arguments and hypothesises that verbs which behave similarly with respect to the expression and interpretation of their arguments share certain meaning components and can therefore be organised into semantically coherent classes (200 in total).", "startOffset": 157, "endOffset": 835}, {"referenceID": 1, "context": "Asher and Lascarides (2003) argue that these classes provide important information for identifying semantic relationships between clauses.", "startOffset": 0, "endOffset": 28}, {"referenceID": 1, "context": "Asher and Lascarides (2003) argue that these classes provide important information for identifying semantic relationships between clauses. Verbs in our data were mapped into their corresponding Levin classes (feature VL); polysemous verbs were disambiguated by the method proposed in Lapata and Brew (1999).3 Again, for verbs not included in Levin, the lemmatised verb form is used.", "startOffset": 0, "endOffset": 307}, {"referenceID": 1, "context": "Noun Identity (N) It is not only verbs, but also nouns that can provide important information about the semantic relation between two clauses; Asher and Lascarides (2003) discuss an example in which having the noun meal in one sentence and salmon in the other serves to trigger inferences that the events are in a part-whole relation (eating the salmon was part of the meal).", "startOffset": 143, "endOffset": 171}, {"referenceID": 1, "context": "Noun Class (NW) As with verbs, Asher and Lascarides (2003) argue in favour of symbolic rules for inferring temporal relations that utilise the semantic classes of nouns wherever possible, so as to maximise the linguistic generalisations that are captured.", "startOffset": 31, "endOffset": 59}, {"referenceID": 27, "context": "Lapata and Brew (1999) develop a simple probabilistic model which determines for a given polysemous verb and its frame its most likely meaning overall (i.", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "Lapata and Brew (1999) develop a simple probabilistic model which determines for a given polysemous verb and its frame its most likely meaning overall (i.e., across a corpus), without relying on the availability of a disambiguated corpus. Their model combines linguistic knowledge in the form of Levin (1995) classes and frame frequencies acquired from a parsed corpus.", "startOffset": 0, "endOffset": 309}, {"referenceID": 20, "context": "As the rules for inferring temporal relations in Hobbs et al. (1993) and Asher and Lascarides (2003) attest, the predicate argument structure of clauses is crucial to making the correct temporal inferences in many cases.", "startOffset": 49, "endOffset": 69}, {"referenceID": 1, "context": "(1993) and Asher and Lascarides (2003) attest, the predicate argument structure of clauses is crucial to making the correct temporal inferences in many cases.", "startOffset": 11, "endOffset": 39}, {"referenceID": 32, "context": "This model resembles Marcu and Echihabi\u2019s (2002)\u2019s model in that it does not make use of the linguistically motivated features presented in the previous section; all that is needed for estimating its parameters", "startOffset": 21, "endOffset": 49}, {"referenceID": 40, "context": "features and the word-based model also shows, in line with the findings in Sporleder and Lascarides (2005), that linguistic abstractions are useful in overcoming sparse data.", "startOffset": 75, "endOffset": 107}, {"referenceID": 32, "context": "This result is in agreement with Marcu and Echihabi (2002) who employ a very large corpus (1 billion words) for training their word-based model.", "startOffset": 33, "endOffset": 59}, {"referenceID": 12, "context": "uncorrelated (Dietterich, 1997).", "startOffset": 13, "endOffset": 31}, {"referenceID": 5, "context": "Multiple classifiers can be generated either by using subsamples of the training data (Breiman, 1996a; Freund & Shapire, 1996) or by manipulating the set of input features available to the component learners (Cherkauer, 1996).", "startOffset": 86, "endOffset": 126}, {"referenceID": 10, "context": "Multiple classifiers can be generated either by using subsamples of the training data (Breiman, 1996a; Freund & Shapire, 1996) or by manipulating the set of input features available to the component learners (Cherkauer, 1996).", "startOffset": 208, "endOffset": 225}, {"referenceID": 46, "context": "A more sophisticated combination method is stacking where a learner is trained to predict the correct output class when given as input the outputs of the ensemble classifiers (Wolpert, 1992; Breiman, 1996b; van Halteren et al., 2001).", "startOffset": 175, "endOffset": 233}, {"referenceID": 6, "context": "A more sophisticated combination method is stacking where a learner is trained to predict the correct output class when given as input the outputs of the ensemble classifiers (Wolpert, 1992; Breiman, 1996b; van Halteren et al., 2001).", "startOffset": 175, "endOffset": 233}, {"referenceID": 1, "context": "Asher and Lascarides\u2019 (2003) symbolic theory of discourse interpretation also emphasises the importance of lexical information in inferring", "startOffset": 0, "endOffset": 29}, {"referenceID": 26, "context": ", of the two clauses) as well as complex inferences over real world knowledge (e.g., Hobbs et al., 1993; Lascarides & Asher, 1993; Kehler, 2002).", "startOffset": 78, "endOffset": 144}, {"referenceID": 20, "context": ", Hobbs et al., 1993; Lascarides & Asher, 1993; Kehler, 2002). Our best model achieved an F-score of 69.1% on the interpretation task and 93.4% on the fusion task. This performance is a significant improvement over the baseline and compares favourably with human performance on the same tasks. Our experiments further revealed that not only lexical but also syntactic information is important for both tasks. This result is in agreement with Soricut and Marcu (2003) who find that syntax trees encode sufficient information to enable accurate derivation of discourse relations.", "startOffset": 2, "endOffset": 467}, {"referenceID": 4, "context": "The approach presented in this paper can be also combined with the annotations present in the TimeML corpus in a semi-supervised setting similar to Boguraev and Ando (2005) to yield", "startOffset": 148, "endOffset": 173}], "year": 2011, "abstractText": "In this paper we propose a data intensive approach for inferring sentence-internal temporal relations. Our approach bypasses the need for manual coding by exploiting the presence of temporal markers like after, which overtly signal a temporal relation. Our experiments concentrate on two tasks relevant for applications which either extract or synthesise temporal information (e.g., summarisation, question answering). Our first task focuses on interpretation: given a subordinate clause and main clause, identify the temporal relation between them. The second is a fusion task: given two clauses and a temporal relation between them, decide which one contained the temporal marker (i.e., identify the subordinate and main clause). We compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements. We evaluate performance against a gold standard corpus and also against human subjects performing the same tasks. The best model achieves 69.1% F-score in inferring the temporal relation between two clauses and 93.4% F-score in distinguishing the main vs. the subordinate clause, assuming that the temporal relation is known.", "creator": "LaTeX with hyperref package"}}}