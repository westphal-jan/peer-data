{"id": "1110.3907", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2011", "title": "AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem", "abstract": "LogitBoost and its later improvement, ABC-LogitBoost, are both successful multi-class boosting algorithms for classification. In this paper, we explicitly formulate the tree building at each LogitBoost iteration as constrained quadratic optimization. Both LogitBoost and ABC-LogtiBoost adopt approximated solver to such quadratic subproblem. We then propose an intuitively more natural solver, i.e. the block coordinate descent algorithm, and demonstrate that it leads to higher classification accuracy and faster convergence rate on a number of public datasets. This new LogitBoost behaves as if it combines many one-vs-one binary classifiers adaptively, hence the name AOSO-LogitBoost(Adaptive One-vs-One LogitBoost)", "histories": [["v1", "Tue, 18 Oct 2011 08:26:59 GMT  (209kb)", "https://arxiv.org/abs/1110.3907v1", null], ["v2", "Thu, 17 May 2012 19:43:06 GMT  (463kb)", "http://arxiv.org/abs/1110.3907v2", "11 pages. Extended version of an ICML2012 paper with the same title"], ["v3", "Wed, 4 Jul 2012 07:14:17 GMT  (278kb)", "http://arxiv.org/abs/1110.3907v3", "8-pages camera ready version for ICML2012"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.CV", "authors": ["peng sun", "mark d reid", "jie zhou"], "accepted": true, "id": "1110.3907"}, "pdf": {"name": "1110.3907.pdf", "metadata": {"source": "META", "title": "AOSO-LogitBoost: Adaptive One-Vs-One LogitBoost for Multi-Class Problem", "authors": ["Peng Sun", "Mark D. Reid", "Jie Zhou"], "emails": ["sunp08@mails.tsinghua.edu.cn", "mark.reid@anu.edu.au", "jzhou@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 111 0.39 07v3 [st at.M L] 4In this paper, we propose techniques to address the two main difficulties of the LogitBoost setting: 1) we adopt a vector tree (i.e., each node value is a vector) that forces a sum-to-zero constraint, and 2) we use an adaptive block coordinate lineage that uses dense Hessian to calculate tree-split gains and node values. Higher classification accuracy and faster convergence rates are observed for a number of public datasets compared to both the original and ABC LogitBoost implementation."}, {"heading": "1. Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2. The Algorithm", "text": "We start with the basic setting for the LogitBoost algorithm. ForK class classification (K \u2265 2), we consider N sample training {xi, yi} N i = 1, where xi denotes a attribute value, and yi class {1,.., K} a class label. Class probabilities caused by x (p1,..., pK) T, are learned from the training set. For a test example with known x and unknown y, we predict a class label by applying the Bayes rule: y = argmaxk pk, k = 1,.., K.Instead of learning the class probability directly, we learn our \"proxy\" F = (F1,., FK) T, which is given by the so-called Logit link function: pk = exp (Fk), executed K = 1 exp (Fj) x exp (1) exp (Fj) (1) with the constraint Kk = 1 Fk = 0 (Friedman."}, {"heading": "2.1. Vector Tree Model", "text": "The f (x) in (5) is typically represented by K scalar regression trees (e.g. in LogitBoost (Friedman et al., 1998) or a single vector tree (e.g. in the Real AdaBoost.MH implementation in (Ke'gl & BusaFekete, 2009).) In this paper, we adopt a single vector tree. We also limit that it is a binary tree (i.e., only binary splits on internal nodes are allowed) and that the split must be vertical to the coordinate axis, as in (Friedman et al., 1998) or (Li, 2010a). Formally, f (x) = J \u2211 j = 1tjI (x-Rj) (6), where {Rj} J = 1 describes how the attribute space is partitioned, while tj-R K is a sum-to-zero constraint associated with a vector."}, {"heading": "2.2. Tree Building", "text": "The solution (5) with the tree model (6) is equivalent to determining the parameters {tj, Rj} J = 1 at the m-th iteration. In this section we will show how this problem can be reduced to solving a collection of convex optimization problems for which we can apply any numerical method."}, {"heading": "2.3. Properties of Approximated Node Loss", "text": "To minimize (10), we give some properties for (10) that should be taken into account in the search for a solver. We start with the sum-to-zero constraint. The probability estimate pk in Logit loss (3) does not have to be zero and sum-to-one, which is ensured by the link function (1). Such a constraint, in turn, means that pk remains unchanged by adding any constant to each component in F. Consequently, the single example loss (3) is invariant to move it along an All-1 vector 1. That is, L (yi, F i + c1) = L (yi, F i), where c is any real constant (note that 1 happens to be the orthogonal addition to the space defined by sum-to-zero constraint). This property also applies to the approximate loss of property (10) (=)."}, {"heading": "2.4. Block Coordinate Descent", "text": "For the variable t in (10), we select only two (the least possible number due to the sum-to-zero constraint) coordinates, i.e. one class pair, to update them while the others remain fixed. Suppose we have chosen the rth and the s-th coordinate (how to do this, will be moved to the next subsection). Let us let tr = t and ts = \u2212 t select the free variables (so that tr + ts = 0) and tk = 0 for k 6 = r, k 6 = s. If we select these in (10) one-dimensional square problems related to the scalar variable t: loss (t) = gT t + 12 ht2 (16), where the gradient and Hessian collapse to scalars are: g = \u2212 x I (ri, r \u2212 pi, r \u2212 pi \u2212 h) and gpi \u2212 i \u2212 s (approximate) are: g = 2p, i \u2212 s, i (s, i, s, \u2212 k, \u2212 k = 1, k \u2212 (k), k = 1 \u2212 p (p \u2212 i) and p \u2212 1 (p \u2212 p), so that we are \u2212 18 for the pair."}, {"heading": "2.5. Class Pair Selection", "text": "In (Bottou & Lin, 2007) two methods are proposed for selecting (r, s): One is based on an approximation of the first order; allow tr and ts to be the free variables and the rest to be fixed at 0; for a t with a sufficiently small fixed length, allow tr = female and ts = \u2212 female, the first order being roughly constant enough; (10) The approximation of the first order of (10) is: loss (t) + gT t = loss (0) \u2212 female (\u2212 gr \u2212 (\u2212 gs)) (23) Consequently, the indexes r, s leading to (23) are: r = argmax k {\u2212 gk} s = argmin k k {\u2212 gk} s = loss (0) \u2212 gk}. (24) Another method that can be derived similarly takes into account the information of the second order: r = argmax k {\u2212 gk} s = argmax {(gr \u2212 gk} {gk}."}, {"heading": "3. Comparison to (ABC-)LogitBoost", "text": "In this section we compare the derivatives of LogitBoost and ABC-LogitBoost and give an intuition for observed behavior in the experiments in Section 4."}, {"heading": "3.1. ABC-LogitBoost", "text": "To solve (5) with a sum-to-zero constraint, ABCLogitBoost uses K \u2212 1 independent trees: fk = {\u2211 j tjkI (x-Rjk) k 6 = b \u2212 \u2211 l 6 = b fl k = b. (26) Algorithm 1 AOSO-LogitBoost. v is shrink factor that controls the learning rate. (1) Fik = 0, k = 1,.., K, i = 1,.., N 2: for m = 1 to do M 3: pi, k = exp (Fi, k) \u2211 K = 1 exp (Fi, j), k = 1,., K, i = 1,.., N.4: Obtain {Rmj} J j = 1 by recursive region partition.Node gain is called (22), where the class pair (r, s) is selected."}, {"heading": "3.2. LogitBoost", "text": "In the original LogitBoost (Friedman et al., 1998), the Hessian matrix (14) is approached diagonally, thus expressing the f in (5) by K uncoupled scale address: fk = jtjkI (x-Rjk), k = 1, 2,.., K (27) with the gradient and the Hessian for calculating the node value and the node division increase given by: gk = \u2212 \u2211 i-I (ri, k \u2212 pi, k), hk = \u2212 \u2211 i-Ipi, k (1 \u2212 pi, k). (28) Here, we use the subscript k for g and h to build the k-th tree independently of the other K \u2212 1 trees (i.e. the sum-to-zero constraint is dropped). Although this simplifies mathematics, such an aggressive approximation harms both the classification accuracy and the convergence rate shown in Li's 2009 experiment."}, {"heading": "4. Experiments", "text": "In this section, we compare AOSO-LogitBoost with ABC-LogitBoost, which exceeds the original LogitBoost in Li's experiments (Li, 2010a; 2009). We test AOSO on all data sets used in (Li, 2010a; 2009), as shown in Table 1. In the upper section are UCI data sets and in the lower section are Mnist data sets with many variations (see (Li, 2010b) for detailed descriptions. To exhaust the learning capability of (ABC-) LogitBoost, Li will stop upgrading if either the training matches (i.e., the loss (2) approaches 0, implemented as \u2264 10 \u2212 16) or a maximum number of iterations, M, is reached. Test errors at the last iteration are simply reported, as no obvious matching of Boost is observed."}, {"heading": "4.1. Classification Errors", "text": "In Table 3, we summarize the results for all records. In (Li, 2010a), Li reported that ABC-LogitBoost is insensitive to J on all records except Poker25kT13Code and data at http: / / ivg.au.tsinghua.edu.cn / index.php? n = People.ABC is insensitive to J, v on all records except Poker25kT2. Therefore, Li simply summarizes the ABC classification errors at J = 20 and v = 0.1, except that errors are reported on Poker25kT1 and Poker25kT2 using the other test set as a validation set (PengSunand Poker25kT2)."}, {"heading": "4.2. Convergence Rate", "text": "Remember that when either the maximum number of iterations is reached or it converges (i.e. the loss (2) \u2264 10 \u2212 16), we terminate the boosting process. The fewer trees are added when boosting stops, the faster the convergence is, and the lower the time cost of training or testing. We compare AOSO with ABC in terms of the number of trees added when boosting stops for ABC results available in (Li, 2010a; 2009). Note that the simple comparison of the number of boosting iterations is unfair to AOSO, since each iteration adds only one tree in AOSO and K \u2212 1 in ABC. Results are in Table 4 and Table 5. Except when J-v is too small or particularly difficult datasets where both ABC and AOSO achieve maximum iterations, we found that trees needed in AOSO typically only 50% to 80% of test errors are found in this version of AOSrams compared to AB2 trees in A.O and AB2 diagrams."}, {"heading": "5. Conclusions", "text": "We present an improved LogitBoost, AOSOLogitBoost, for multi-class classification. Compared to ABC-LogitBoost, our experiments suggest that our adaptive method of selecting class pairs leads to lower classification errors and faster convergence rates."}, {"heading": "Acknowledgments", "text": "This work was supported by the National Natural Science Foundation of China (61020106004, 61021063, 61005023) and the National Key Technology F & D Program (2009BAH40B03). NICTA is funded by the Australian government, represented by the Ministry of Broadband, Communications and the Digital Economy, and the ARC by the ICT Centre of Excellence Program."}], "references": [{"title": "Support vector machine solvers", "author": ["L. Bottou", "Lin", "C.-J"], "venue": "Large Scale Kernel Machines,", "citeRegEx": "Bottou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2007}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "In Computational learning theory,", "citeRegEx": "Freund and Schapire,? \\Q1995\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1995}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J. Friedman"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Additive logistic regression: a statistical view of boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1998}, {"title": "Boosting products of base classifiers", "author": ["B. K\u00e9gl", "R. Busa-Fekete"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "K\u00e9gl and Busa.Fekete,? \\Q2009\\E", "shortCiteRegEx": "K\u00e9gl and Busa.Fekete", "year": 2009}, {"title": "Adaptive base class boost for multi-class classification", "author": ["P. Li"], "venue": "Arxiv preprint arXiv:0811.1250,", "citeRegEx": "Li,? \\Q2008\\E", "shortCiteRegEx": "Li", "year": 2008}, {"title": "Abc-logitboost for multi-class classification", "author": ["P. Li"], "venue": "Arxiv preprint arXiv:0908.4144,", "citeRegEx": "Li,? \\Q2009\\E", "shortCiteRegEx": "Li", "year": 2009}, {"title": "Robust logitboost and adaptive base class (abc) logitboost", "author": ["P. Li"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Li,? \\Q2010\\E", "shortCiteRegEx": "Li", "year": 2010}, {"title": "An empirical evaluation of four algorithms for multiclass classification: Mart, abc-mart, robust logitboost, and abc-logitboost", "author": ["P. Li"], "venue": "Arxiv preprint arXiv:1001.1020,", "citeRegEx": "Li,? \\Q2010\\E", "shortCiteRegEx": "Li", "year": 2010}, {"title": "Fast abc-boost for multi-class classification", "author": ["P. Li"], "venue": "Arxiv preprint arXiv:1006.5051,", "citeRegEx": "Li,? \\Q2010\\E", "shortCiteRegEx": "Li", "year": 2010}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R. Schapire", "Y. Singer"], "venue": "Machine learning,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "New multicategory boosting algorithms based on multicategory fisher-consistent losses", "author": ["H. Zou", "J. Zhu", "T. Hastie"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Zou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "Among those popular variants, we are particularly focusing on LogitBoost (Friedman et al., 1998) in this paper.", "startOffset": 73, "endOffset": 96}, {"referenceID": 3, "context": "Originally, LogitBoost is motivated by statistical view (Friedman et al., 1998), where boosting algorithms consists of three key components: the loss, the function model, and the optimization algorithm.", "startOffset": 56, "endOffset": 79}, {"referenceID": 2, "context": "Among those popular variants, we are particularly focusing on LogitBoost (Friedman et al., 1998) in this paper. Originally, LogitBoost is motivated by statistical view (Friedman et al., 1998), where boosting algorithms consists of three key components: the loss, the function model, and the optimization algorithm. In the case of LogitBoost, these are the Logit loss, the use of additive tree models, and a stage-wise optimization, respectively. There are two important factors in the LogitBoost setting. Firstly, the posterior class probability estimate must be normalised so as to sum to one in order to use the Logit loss. This leads to a coupled classifier output, i.e., the sum-to-zero classifier output. Secondly, a dense Hessian matrix arises when deriving the tree node split gain and node value fitting. It is challenging to design a tractable optimization algorithm that fully handles both these factors. Consequently, some simplification and/or approximation is needed. Friedman et al. (1998) proposes a \u201cone scalar regression tree for one class\u201d strategy.", "startOffset": 74, "endOffset": 1004}, {"referenceID": 5, "context": "gence rate (Li, 2008; 2010a).", "startOffset": 11, "endOffset": 28}, {"referenceID": 3, "context": "with the constraint \u2211K k=1 Fk = 0 (Friedman et al., 1998).", "startOffset": 34, "endOffset": 57}, {"referenceID": 3, "context": ", in LogitBoost (Friedman et al., 1998) or the Real AdaBoost.", "startOffset": 16, "endOffset": 39}, {"referenceID": 3, "context": "MH implementation in (Friedman et al., 1998)) or a single vector tree (e.", "startOffset": 21, "endOffset": 44}, {"referenceID": 3, "context": ", only binary splits on internal node are allowed) and the split must be vertical to coordinate axis, as in (Friedman et al., 1998) or (Li, 2010a).", "startOffset": 108, "endOffset": 131}, {"referenceID": 2, "context": ", in Friedmans\u2019s MART (Friedman, 2001), leads to decreased classification accuracy.", "startOffset": 22, "endOffset": 38}, {"referenceID": 11, "context": "(Zou et al., 2008)).", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "In (Li, 2010c), b is selected only every several iterations, while in (Li, 2008), b is, intuitively, set to be the class that leads to largest loss reduction at last iteration.", "startOffset": 70, "endOffset": 80}, {"referenceID": 3, "context": "In the original LogitBoost (Friedman et al., 1998), the Hessian matrix (14) is approximated diagonally.", "startOffset": 27, "endOffset": 50}, {"referenceID": 6, "context": "Although this simplifies the mathematics, such an aggressive approximation turns out to harm both classification accuracy and convergence rate, as shown in Li\u2019s experiments (Li, 2009).", "startOffset": 173, "endOffset": 183}, {"referenceID": 6, "context": "Dash \u201d-\u201d means unavailable in (Li, 2010a)(Li, 2009).", "startOffset": 41, "endOffset": 51}, {"referenceID": 6, "context": "In the right panel of Table 3 we provide the comparison for the best results achieved over all J-v combinations when the corresponding results for ABC are available in (Li, 2010a) or (Li, 2009).", "startOffset": 183, "endOffset": 193}], "year": 2012, "abstractText": "This paper presents an improvement to model learning when using multi-class LogitBoost for classification. Motivated by the statistical view, LogitBoost can be seen as additive tree regression. Two important factors in this setting are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the dense Hessian matrices that arise when computing tree node split gain and node value fittings. In general, this setting is too complicated for a tractable model learning algorithm. However, too aggressive simplification of the setting may lead to degraded performance. For example, the original LogitBoost is outperformed by ABC-LogitBoost due to the latter\u2019s more careful treatment of the above two factors. In this paper we propose techniques to address the two main difficulties of the LogitBoost setting: 1) we adopt a vector tree (i.e., each node value is vector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block coordinate descent that exploits the dense Hessian when computing tree split gain and node values. Higher classification accuracy and faster convergence rates are observed for a range of public data sets when compared to both the original and the ABC-LogitBoost implementations. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "LaTeX with hyperref package"}}}