{"id": "1110.5102", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2011", "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models", "abstract": "Scene understanding includes many related sub-tasks, such as scene categorization, depth estimation, object detection, etc. Each of these sub-tasks is often notoriously hard, and state-of-the-art classifiers already exist for many of them. These classifiers operate on the same raw image and provide correlated outputs. It is desirable to have an algorithm that can capture such correlation without requiring any changes to the inner workings of any classifier.", "histories": [["v1", "Mon, 24 Oct 2011 00:31:00 GMT  (6019kb,D)", "http://arxiv.org/abs/1110.5102v1", "14 pages, 11 figures"]], "COMMENTS": "14 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["congcong li", "adarsh kowdle", "ashutosh saxena", "tsuhan chen"], "accepted": true, "id": "1110.5102"}, "pdf": {"name": "1110.5102.pdf", "metadata": {"source": "CRF", "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models", "authors": ["Congcong Li", "Tsuhan Chen"], "emails": ["apk64}@cornell.edu,", "tsuhan@ece.cornell.edu"], "sections": [{"heading": null, "text": "Index Terms - Scene Understanding, Classification, Machine Learning, Robotics"}, {"heading": "1 INTRODUCTION", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to find its way into the future."}, {"heading": "2 OVERVIEW OF SCENE UNDERSTANDING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Holistic Scene Understanding", "text": "When we look at an image of a scene, as in Figure 1, we are often interested in answering several different questions: What objects are in the image? How far are things? What is going on in the scene? What kind of scene is it? And so on. These are just a few examples of questions in the area of scene understanding; and there could be even more. In the past, the emphasis was on tackling each task in isolation, with the goal of each task being to create a label Yi-Si for the ith subtask. If we consider a depth estimation (see Figure 1), then the label Y1-S1 = 100-100 + for continuous depth values in a 100-100 output. For scene categorization, we have Y2-S2 = {1,., K} for K-scene classes. If we have n-subtasks, then we would have an output as: Y = {Y1,.,., Yn} and S2."}, {"heading": "2.2 Related Work", "text": "This year, it has come to the point where it is only a matter of time before a decision is reached in which a decision is reached."}, {"heading": "3 FEEDBACK ENABLED CASCADED CLASSIFICATION MODELS", "text": "In the field of scene understanding, many independent research on each of the vision sub-tasks has led to excellent classifiers. These independent classifiers are typically trained on different or heterogeneous data sets because there is a lack of basic truths for all sub-tasks. In addition, each of these classifiers has its own learning and follow-up methods. Our goal is to consider each of these data sets as a \"black box,\" making it easy to combine them. We describe what we understand by \"black box classifiers.\" Black box classifier. A black box classifier, as the name suggests, is a classifier for which operations (such as learning and inference algorithms) are available, but their inner workings are not known. We assume that 4 will be extracted in light of the training data set X, functions that transfer i-data to the i-function."}, {"heading": "3.1 Our Model", "text": "Our model is constructed in the form of a two-layer cascade, as shown in Figure 2. The first layer consists of an instantiation of each of the black box classifiers with the image properties as input. The second layer is a repeated instantiation of each of the classifiers with the first layer. Instead, we consider them to be repeated instantiation just because they are used for the same classification function. Note: We consider n related subtasks as classifiers, i, 2,... We describe the notations used in this paper only because they are used for the same classification task. Note: We consider n related subtasks as classifiers, i, 2,.. Note: The notations used in this paper, Yi (X) properties that correspond to the classification."}, {"heading": "3.2 Inference Algorithm", "text": "Using the learned parameters \u03b8i for the first level of the classifiers and \u03c9i for the second level of the classifiers, we first infer the first layer outputs Zi and then the second layer outputs Yi. More formally, we perform the following conclusions: Z-i = Optimization of the Zif-iinfer (\u0441i (X), Zi; \u2082 i) (3) Y-i = Optimization of the Yif-i inferences ([\u03c6i (X) Z-i], Yi (4) The inference algorithm is given in Algorithm 1. this method allows us to use the internal inference function (Equation 2) of the black box classifiers without knowing their inner workings. Note that the complexity here is no more than the constant of the complexity of the inferences in the original classifiers. This method allows us to use the internal inference function (Equation 2) of the black box classifiers without paying attention to the inner complexity of the classifiers, which is greater than the original inferences."}, {"heading": "3.3 Learning Algorithm", "text": "During the training phase, we are able to learn the model through an integrated grading. (We assume that each layer is independent and that each layer produces the best performance independently (without taking into account other layers), and therefore use the basic truth labels for Z even to train the classifiers in the first layer.) On the other hand, we want to optimize the final outputs as much as possible. (w.r.t.) Instead, we focus on error modes that would arise in the second shift (Y1, Y2, Yn)."}, {"heading": "3.4 Probabilistic Interpretation", "text": "Our algorithm can be explained by a calculation of probabilities in which the goal is to maximize the probability of the results of all tasks (1). (1) Therefore, the goal of the proposed model shown in Figure 2 is the Maximizelog Method (1). (8) To introduce the hidden values of Zis, we extend the Eq.8 as follows, using the dependencies represented by the directed model in Figure 2. (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (3). (3). (3). (3). (3). (3. (3). (3). (3. (3). (3). (3). (3). (3. (3). (3). (3). (3). (3). (3). (3). (3). (3. (3). (3). (3). (3). (3). (3. (3). (3). (3). (3). (3. (3). (3). (3). (3). (3. (3). (3). (3. (3). (3. (3). (3). (3). (3). (3). (3. (3). (3). (3. (3. (3). (3). (3. (3). (3).). (3. (3. (3). (3). (3.). (3.). (3. (3).). (3.). (3.). (3.). (3.). (3. (3.). (3.).). (3. (3.). (3.). (3.).).). (3. (3"}, {"heading": "4 TRAINING WITH HETEROGENEOUS DATASETS", "text": "In this section, we will show our formulation for this general case where we use \"i\" as a data set that only has labels for the ith task. These modifications also allow us to develop different variants of the model that are described in Section 4.1. \"Z-Step-Forward\" Step: Based on the feedback step, we can have \"Zi\" for all data, i.e. we can use all datasets to label the \"ith task.\" These modifications also allow us to develop different variants of the model that are described in Section 4.1. \"Z-Step-Forward\" Step: Based on the feedback step, we can have \"Zi\" for all data. Therefore, we use all datasets to learn each of the first layers of \"i points.\" If the internal learning function of the black box classifier is additive to the data points, then we have \"i.\""}, {"heading": "4.1 FECCM: Different Instantiations", "text": "The \u03c0j parameters allow us to formulate three different instances of our model. \u2022 Unified FECCM: In this instance, our goal is to achieve improvements in all tasks with a set of parameters. \u2022 We want to balance the data from different datasets (i.e. with different task names). \u2022 Towards this goal, \u03c0j is set to be inversely proportional to the amount of data in the dataset of the jth task. Therefore, the unified FECCM balances the amount of data in different datasets, based on Equation 23. \u2022 One-target FECCM: In this instance, we set \u03c0j = 1 if j = k, and \u03c0j = 0 otherwise. This is an extreme setting to favor the specific task k. In this case, the retraining of the first layer classifier is only the feedback from the classifier on the second layer, i.e. we only use the datasets with labels for the task."}, {"heading": "5 SCENE UNDERSTANDING: IMPLEMENTATION", "text": "In this section, we will describe the implementation of our instance of FE-CCM for scene understanding (i = 5) and geometry (i = 5). Each of the classifiers for the sub-tasks described below is our \"base model,\" which is presented in Table 1. In some sub-tasks, our base model will be even easier than the predicted sub-tasks (which are often manually matched for the specific sub-tasks), but even if we use the base models in our FE-CCM scene, our model will still exceed the depth of the respective sub-tasks (on the same standard or datasets) to explain the implementation details for the different tasks, we will use the index of the tasks we consider to be 6 tasks for our scene understanding: scene categorization (i = 1), depth assessment (i = 2), event categorization (i = 3), object detection (i = 5) and geometrization (i = 5)."}, {"heading": "6 EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experimental Setting", "text": "The proposed FE-CCM model is a unified model that collectively optimizes for all sub-tasks. We believe that it is a powerful algorithm, while independent efforts on each sub-task have led to state-of-the-art algorithms that require complicated modeling for that specific sub-task. In our experiment, the proposed approach is a unified model that can beat the performance of the state of the art in each sub-task and that can be applied seamlessly in different application areas. We evaluate our proposed method of combining six tasks, which is in Section 5. In our experiment, the formation of the FE-CCM takes 4-5 iterations. For each of the sub-tasks in each of the domains, we evaluate our performance using the standard data set for that sub-task (and compare it with the specially designed state-of-the-art algorithm for that data set)."}, {"heading": "6.2 Datasets", "text": "The data sets used are mentioned in Section 5, and the number of test images in each data set is shown in Table 1. For each data set, we use the same number of training images as the state-of-the-art algorithm (for comparison).We perform a 6-fold cross-validation of the entire model with 5 out of 6 subtasks to evaluate the performance of each task. We do not perform cross-validation in object recognition as is standard for the PASCAL 2006 [63] data set (1277 train images and 2686 test images, respectively)."}, {"heading": "6.3 Results", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they live, in"}, {"heading": "6.4 Discussion", "text": "FE-CCM allows each classifier in the second layer to learn what information from the other first layer subtasks are useful, and this can be seen automatically in the learned weights for the second layer. We offer a visualization of weights for the six visual tasks in Figure 8 (a). We see that the model agrees with our intuitions that large weights are assigned to the outputs of the same task (see also the large weights assigned to the diagonals in the categorization tasks), although the detection of lightness is an exception that depends more on their original properties (not shown here) and the geometric labeling of the output of images. We also observe that the weights of the scene are sparse. This is an advantage of our approach, since the algorithm automatically maps which outputs from the first level classifier are useful to achieve the best performance."}, {"heading": "7 ROBOTIC APPLICATIONS", "text": "In order to demonstrate the applicability of our FE-CCM to different areas of scene understanding, we have also used the proposed method in several robot applications."}, {"heading": "7.1 Robotic Grasping", "text": "Faced with an image and a deep map (Figure 9), the goal of the learning algorithm in a rampaging robot is to select a point to capture the object (this place is referred to as a grassroots point, [66]. It turns out that different categories of objects require different capture strategies. In previous work, we have not used object categories to capture. In this work, we use our FE-CCM map to combine object classification and capture. Implementation: We work with the labeled synthetic dataset al. [66] the object categories comprise 6 object categories and also include an aligned pixel plane depth map for each image. The six object categories include symmetrical objects."}, {"heading": "7.2 Object-finding Robot", "text": "Considering an image, the goal of an object finding robot is to find a desired object in a crowded room. As we have already discussed, some types of scenes such as the living room are more likely to contain objects (e.g. shoes) than other types of scenes such as the kitchen. Similarly, office scenes are more likely to include TV monitors than kitchen scenes. Furthermore, it is intuitive that shoes appear on the supporting surface such as the floor rather than on the vertical surface such as the wall. Therefore, in this work, we use our FE-CCM to combine object recognition with indoor object categorization and geometric labeling. Implementation: For scene categorization, we use the indoor scene subsets in the Cal Scene Dataset [68] and13 classify an image into one of the four categories: bedroom, living room, kitchen and office."}, {"heading": "8 CONCLUSIONS", "text": "We propose a method for combining existing classifiers for different but related scenic understanding tasks. We consider the individual classifiers merely a \"black box\" (i.e. we do not need to know the inner workings of the classifier) and suggest learning techniques to combine them (and therefore do not need to know how to combine the tasks).Our method introduces feedback into the training process from the later stage to the earlier, so that a later classifier can provide information to the earlier classifiers on what error modes they focus on or what can be ignored without hurting the collective performance.Our extensive experiments show that our unified model (a single FE-CCM trained for all sub-tasks) significantly improves performance in all sub-tasks considered by the respective state-of-the-art classifiers. We show that this was the result of our feedback process. The classifier actually learns meaningful relationships between tasks automatically. We believe that this is a small step towards a holistic scenario understanding."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Anish Nahar, Matthew Cong, TP Wong, Norris Xu and Colin Ponce for their help with the robot experiments and Daphne Koller for useful discussions."}], "references": [{"title": "A hierarchical field framework for unified context-based classification", "author": ["S. Kumar", "M. Hebert"], "venue": "ICCV, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A.Y. Ng"], "venue": "PAMI, vol. 30, no. 5, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Closing the loop on scene interpretation", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "CVPR, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework", "author": ["L.-J. Li", "R. Socher", "L. Fei-Fei"], "venue": "CVPR, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Depth from familiar objects: A hierarchical model for 3d scenes", "author": ["E.B. Sudderth", "A. Torralba", "W.T. Freeman", "A.S. Willsky"], "venue": "CVPR, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint parsing and semantic role labeling", "author": ["C. Sutton", "A. McCallum"], "venue": "CoNLL, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "From appearance to contextbased recognition: Dense labeling in small images", "author": ["D. Parikh", "C. Zitnick", "T. Chen"], "venue": "CVPR, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Object detection via boundary structure segmentation", "author": ["A. Toshev", "B. Taskar", "K. Daniilidis"], "venue": "CVPR, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Monocular human motion capture with a mixture of regressors", "author": ["A. Agarwal", "B. Triggs"], "venue": "CVPR Workshop on Vision for HCI, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Depth estimation using monocular and stereo cues", "author": ["A. Saxena", "J. Schulte", "A.Y. Ng"], "venue": "IJCAI, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Cascaded classification models: Combining models for holistic scene understanding.", "author": ["G. Heitz", "S. Gould", "A. Saxena", "D. Koller"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Neural network ensembles", "author": ["L. Hansen", "P. Salamon"], "venue": "PAMI, vol. 12, no. 10, pp. 993\u20131001, 1990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Cascaded neural networks based image classifier", "author": ["Y. Freund", "R.E. Schapire"], "venue": "ICASSP, 1993.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "EuroCOLT, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "On the design of cascades of boosted ensembles for face detection", "author": ["S.C. Brubaker", "J. Wu", "J. Sun", "M.D. Mullin", "J.M. Rehg"], "venue": "IJCV, vol. 77, no. 1-3, pp. 65\u201386, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "IJCV, vol. 57, no. 2, pp. 137\u2013154, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Mutual boosting for contextual inference", "author": ["M. Fink", "P. Perona"], "venue": "In NIPS, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Contextual models for object detection using boosted random fields", "author": ["A. Torralba", "K. Murphy", "W. Freeman"], "venue": "In NIPS, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Auto-context and its application to high-level vision tasks", "author": ["Z. Tu"], "venue": "CVPR, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Feedback enabled cascaded classication models for scene understanding", "author": ["C. Li", "A. Kowdle", "A. Saxena", "T. Chen"], "venue": "NIPS, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "A generic model to compose vision modules for holistic scene understanding", "author": ["A. Kowdle", "C. Li", "A. Saxena", "T. Chen"], "venue": "ECCV Workshop on Parts and Attributes, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "On combining classifiers", "author": ["J. Kittler", "M. Hatef", "R.P. Duin", "J. Matas"], "venue": "PAMI, vol. 20, pp. 226\u2013239, 1998.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "\u03b8-mrf:capturing spatial and semantic structure in the parameters for scene understanding", "author": ["C. Li", "A. Saxena", "T. Chen"], "venue": "NIPS, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 1817\u20131853, December 2005.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1817}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims"], "venue": "ICML, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "NIPS, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "Neural Information Processing Systems (NIPS), 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Conditional random fields for object recognition", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "In NIPS, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning structural svms with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "ICML, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Contextual priming for object detection", "author": ["A. Torralba"], "venue": "Int. J. Comput. Vision, vol. 53, no. 2, pp. 169\u2013191, 2003.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Depth estimation from image structure", "author": ["A. Torralba", "A. Oliva"], "venue": "IEEE PAMI, vol. 24, pp. 1226\u20131238, September 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Putting objects in perspective", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "IJCV, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiresolution models for object detection", "author": ["D. Park", "D. Ramanan", "C. Fowlkes"], "venue": "ECCV, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning spatial context: Using stuff to find things", "author": ["G. Heitz", "D. Koller"], "venue": "ECCV, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Context by region ancestry", "author": ["J. Lim", "P. Arbel andez", "C. Gu", "J. Malik"], "venue": "ICCV, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "A hierarchical field framework for unified  14 context-based classification", "author": ["S. Kumar", "M. Hebert"], "venue": "ICCV, 2005.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "PAMI, 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Objects in context", "author": ["A. Rabinovich", "A. Vedaldi", "C. Galleguillos", "E. Wiewiora", "S. Belongie"], "venue": "ICCV, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "From appearance to contextbased recognition: Dense labeling in small images", "author": ["D. Parikh", "C. Zitnick", "T. Chen"], "venue": "CVPR, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative models for multi-class object layout", "author": ["C. Desai", "D. Ramanan", "C. Fowlkes"], "venue": "ICCV, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling mutual context of object and human pose in human-object interaction activities", "author": ["B. Yao", "L. Fei-Fei"], "venue": "CVPR, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiclass object localization by combining local contextual interactions", "author": ["C. Galleguillos", "B. McFee", "S. Belongie", "G. Lanckriet"], "venue": "CVPR, 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical study of context in object detection", "author": ["S. Divvala", "D. Hoiem", "J. Hays", "A. Efros", "M. Hebert"], "venue": "CVPR, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Object localization with global and local context kernels", "author": ["M. Blaschko", "C. Lampert"], "venue": "BMVC, 2009.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "What, where and who? classifying event by scene and object recognition", "author": ["L. Li", "L. Fei-Fei"], "venue": "ICCV, 2007.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Scaling learning algorithms towards ai", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Large-Scale Kernel Machines, 2007.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, pp. 41\u201375, 1997.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade, G. Orr and M. K., Eds. Springer, 1998.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1998}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxena", "H. Lee", "A. Ng"], "venue": "NIPS, 2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "N. Comp, 2006.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2006}, {"title": "Deconvolutional networks", "author": ["M. Zeiler", "D. Krishnan", "G. Taylor", "R. Fergus"], "venue": "CVPR, 2010.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J of Royal Stat. Soc., Series B, vol. 39, no. 1, pp. 1\u201338, 1977.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1977}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": "Learning in graphical models, vol. 89, pp. 355\u2013368, 1998.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1998}, {"title": "Variational gaussian process classifiers", "author": ["M. Gibbs", "D. Mackay"], "venue": "Neural Networks, IEEE Trans, 2000.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2000}, {"title": "Discriminative sparse image models for class-specific edge detection and image interpretation", "author": ["J. Mairal", "M. Leordeanu", "F. Bach", "M. Hebert", "J. Ponce"], "venue": "ECCV, 2008.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, vol. 42, pp. 145\u2013175, 2001.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2001}, {"title": "MIT outdoor scene dataset", "author": ["A. Torralba", "A. Oliva"], "venue": "http://people.csail.mit.edu/torralba/code/spatialenvelope/index.html.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 0}, {"title": "3-d depth reconstruction from a single still image", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "IJCV, vol. 76, 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning depth from single monocular images", "author": ["A. Saxena", "S. Chung", "A. Ng"], "venue": "NIPS, 2005.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2005}, {"title": "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.", "author": ["A. Torralba", "A. Oliva", "M.S. Castelhano", "J.M. Henderson"], "venue": "Psychol Rev,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2006}, {"title": "Frequencytuned Salient Region Detection", "author": ["R. Achanta", "S. Hemami", "F. Estrada", "S. Susstrunk"], "venue": "CVPR, 2009.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "The PASCAL VOC2006 Results.", "author": ["M. Everingham", "A. Zisserman", "C.K.I. Williams", "L. Van Gool"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2006}, {"title": "Discriminatively trained deformable part models, release 3", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "http://people.cs.uchicago.edu/\u223cpff/latent-release3/.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 0}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR, 2005.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2005}, {"title": "Robotic grasping of novel objects", "author": ["A. Saxena", "J. Driemeyer", "J. Kearns", "A.Y. Ng"], "venue": "NIPS, 2006.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2006}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "IJRR, vol. 27, no. 2, pp. 157\u2013173, 2008.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2008}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR, 2005.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2005}, {"title": "Recovering the spatial layout of cluttered rooms", "author": ["V. Hedau", "D. Hoiem", "D. Forsyth"], "venue": "ICCV, 2009.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2009}, {"title": "The PASCAL Visual Object Classes  Challenge 2007 (VOC2007) Results", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 1, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 3, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 4, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 5, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 6, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 7, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 8, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 9, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 10, "context": "[11] recently developed a framework for scene understanding called Cascaded Classification Models (CCM) treating each classifier as a \u2018black-box\u2019.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The idea of cascading layers of classifiers to aid a task was first introduced with neural networks as multi-level perceptrons where, the output of the first layer of perceptrons is passed on as input to the next layer [12\u201314].", "startOffset": 219, "endOffset": 226}, {"referenceID": 12, "context": "The idea of cascading layers of classifiers to aid a task was first introduced with neural networks as multi-level perceptrons where, the output of the first layer of perceptrons is passed on as input to the next layer [12\u201314].", "startOffset": 219, "endOffset": 226}, {"referenceID": 13, "context": "The idea of cascading layers of classifiers to aid a task was first introduced with neural networks as multi-level perceptrons where, the output of the first layer of perceptrons is passed on as input to the next layer [12\u201314].", "startOffset": 219, "endOffset": 226}, {"referenceID": 14, "context": "The idea of improving classification performance by combining outputs of many classifiers is used in methods such as Boosting [15], where many weak learners are combined to obtain a more accurate classifier; this has been applied to tasks such as face detection [16, 17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "The idea of improving classification performance by combining outputs of many classifiers is used in methods such as Boosting [15], where many weak learners are combined to obtain a more accurate classifier; this has been applied to tasks such as face detection [16, 17].", "startOffset": 262, "endOffset": 270}, {"referenceID": 16, "context": "The idea of improving classification performance by combining outputs of many classifiers is used in methods such as Boosting [15], where many weak learners are combined to obtain a more accurate classifier; this has been applied to tasks such as face detection [16, 17].", "startOffset": 262, "endOffset": 270}, {"referenceID": 17, "context": "To incorporate contextual information, Fink and Perona [18] exploited local dependencies between objects in a boosting framework, but did not allow for multiple rounds of communication between objects.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "[19] introduced Boosted Random Fields to model object dependency, which used boosting to learn the graph structure and local evidence of a conditional random field.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Tu [20] proposed a more general framework which used pixel-level label maps to learn a contextual model through a cascaded classifier approach.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "However, in our CCM framework [21, 22], the focus is on capturing contextual interactions between labels of different types.", "startOffset": 30, "endOffset": 38}, {"referenceID": 21, "context": "However, in our CCM framework [21, 22], the focus is on capturing contextual interactions between labels of different types.", "startOffset": 30, "endOffset": 38}, {"referenceID": 19, "context": "Furthermore, compared to the feed-forward only cascade method in [20], our model with feedback not only iteratively refines the contextual interactions, but also refines the individual classifiers to provide helpful context.", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": ", in biometrics, data from voice recognition and face recognition is combined [23].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Kumar and Hebert [1] developed a large MRF-based probabilistic model to link multi-class segmentation and object detection.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "[24] modeled mulitple interactions within tasks and across tasks by defining a MRF over parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Sutton and McCallum [6] combined a parsing model with a semantic role labeling model into a unified probabilistic framework that solved both simultaneously.", "startOffset": 20, "endOffset": 23}, {"referenceID": 24, "context": "Ando and Zhang [25] proposed a general framework for learning predictive functional structures from multiple tasks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": ", [26\u201328]) can also be a viable option for the setting of combining multiple tasks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 26, "context": ", [26\u201328]) can also be a viable option for the setting of combining multiple tasks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 27, "context": ", [26\u201328]) can also be a viable option for the setting of combining multiple tasks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 28, "context": "hidden conditional random field [29], latent structured SVM [30]), which can be potentially applied to multi-task settings with disjoint datasets.", "startOffset": 32, "endOffset": 36}, {"referenceID": 29, "context": "hidden conditional random field [29], latent structured SVM [30]), which can be potentially applied to multi-task settings with disjoint datasets.", "startOffset": 60, "endOffset": 64}, {"referenceID": 8, "context": ", [7\u2013 9]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "manually designed the terms in an MRF to combine depth estimation with object detection [2] and stereo cues [10].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "manually designed the terms in an MRF to combine depth estimation with object detection [2] and stereo cues [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "[5] used object recognition to help 3D structure estimation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31, 32] used the statistics of low-level features across the entire scene to prime object detection or help depth estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[31, 32] used the statistics of low-level features across the entire scene to prime object detection or help depth estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[33] used 3D scene information to provide priors on potential object locations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] used the ground plane estimation as contextual information for pedestrian detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 92, "endOffset": 99}, {"referenceID": 35, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 92, "endOffset": 99}, {"referenceID": 36, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 92, "endOffset": 99}, {"referenceID": 37, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 38, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 39, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 40, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 41, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 42, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 126, "endOffset": 133}, {"referenceID": 43, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 126, "endOffset": 133}, {"referenceID": 44, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 126, "endOffset": 133}, {"referenceID": 2, "context": "[3] proposed an innovative but ad-hoc system that combined boundary detection and surface labeling by sharing some lowlevel information between the classifiers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4, 46] combined image classification, annotation and segmentation with a hierarchical graphical model.", "startOffset": 0, "endOffset": 7}, {"referenceID": 45, "context": "[4, 46] combined image classification, annotation and segmentation with a hierarchical graphical model.", "startOffset": 0, "endOffset": 7}, {"referenceID": 46, "context": "There is also a large body of work in the areas of deep learning, and we refer the reader to Bengio and LeCun [47] for a nice overview of deep learning architectures and Caruana [48] for multitask learning with shared representation.", "startOffset": 110, "endOffset": 114}, {"referenceID": 47, "context": "There is also a large body of work in the areas of deep learning, and we refer the reader to Bengio and LeCun [47] for a nice overview of deep learning architectures and Caruana [48] for multitask learning with shared representation.", "startOffset": 178, "endOffset": 182}, {"referenceID": 48, "context": "While efficient back-propagation methods like [49] have been commonly used in learning a multilayer network, it is not as easy to apply to our case where each node is a complex classifier.", "startOffset": 46, "endOffset": 50}, {"referenceID": 49, "context": ", [50\u201352]) are different from our work in that, those works focus on one particular task (same labels) by building different classifier architectures, as compared to our setting of different tasks with different labels.", "startOffset": 2, "endOffset": 9}, {"referenceID": 50, "context": ", [50\u201352]) are different from our work in that, those works focus on one particular task (same labels) by building different classifier architectures, as compared to our setting of different tasks with different labels.", "startOffset": 2, "endOffset": 9}, {"referenceID": 51, "context": ", [50\u201352]) are different from our work in that, those works focus on one particular task (same labels) by building different classifier architectures, as compared to our setting of different tasks with different labels.", "startOffset": 2, "endOffset": 9}, {"referenceID": 50, "context": "[51] used unsupervised learning to obtain an initial configuration of the parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] assume that each layer is independent and that each layer produces the best output independently (without consideration for other layers), and therefore use the ground-truth labels for Z even for training the classifiers in the first layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Training with this initialization, our cascade is equivalent to CCM in [11], where the classifiers (and the parameters) in the first layer are similar to the original state-of-the-art classifier and the classifiers in the second layer use the outputs of the first layer in addition to the original features as input.", "startOffset": 71, "endOffset": 75}, {"referenceID": 52, "context": "Motivated by the Expectation Maximization algorithm [53], we iterate between the two steps as described in the following.", "startOffset": 52, "endOffset": 56}, {"referenceID": 53, "context": "This can be considered as a special variant of the general EM framework (hard EM, [54]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 54, "context": ", approximating it as a Gaussian, [55]) to get:", "startOffset": 34, "endOffset": 38}, {"referenceID": 55, "context": "We do this by introducing the l1 sparsity in the parameters [56].", "startOffset": 60, "endOffset": 64}, {"referenceID": 56, "context": "[57] tall building, inside city, street, highway, coast, open country, mountain and forest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "We evaluate the performance by measuring the rate of incorrectly assigning a scene label to an image on the MIT outdoor scene dataset [57].", "startOffset": 134, "endOffset": 138}, {"referenceID": 56, "context": "The feature inputs for the first-layer scene classifier \u03a81 \u2208 R is the GIST feature [57], extracted at 4 \u00d7 4 regions of the image, on 4 scales and 8 orientations.", "startOffset": 83, "endOffset": 87}, {"referenceID": 57, "context": "We use an RBF-Kernel SVM classifier [58], as the firstlayer scene classifier, and a multi-class logistic classifier for the second layer.", "startOffset": 36, "endOffset": 40}, {"referenceID": 58, "context": "We evaluate the estimation performance by computing the root mean square error of the estimated depth with respect to ground truth laser scan depth using the Make3D Range Image dataset [59, 60].", "startOffset": 185, "endOffset": 193}, {"referenceID": 59, "context": "We evaluate the estimation performance by computing the root mean square error of the estimated depth with respect to ground truth laser scan depth using the Make3D Range Image dataset [59, 60].", "startOffset": 185, "endOffset": 193}, {"referenceID": 58, "context": "We uniformly divide each image into 55 \u00d7 305 patches as [59].", "startOffset": 56, "endOffset": 60}, {"referenceID": 58, "context": "[59].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46]: bocce, badminton, polo, rowing, snowboarding, croquet, sailing and rock-climbing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "The feature inputs for the first-layer event classifier \u03a83 \u2208 R is a 43-dimensional feature vector, which includes the top 30 PCA projections of the 512dimensional GIST features [61], the 12-dimension global", "startOffset": 177, "endOffset": 181}, {"referenceID": 61, "context": "[62] for our experiments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62] and a bias term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "We use the train-set and test-set of PASCAL 2006 [63] for our experiments.", "startOffset": 49, "endOffset": 53}, {"referenceID": 63, "context": "[64].", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "The feature inputs for the first-layer object detection classifier \u03a85 \u2208 R are the HOG features extracted based on the candidate window as [65] plus the detection score from the part-based detector [64].", "startOffset": 138, "endOffset": 142}, {"referenceID": 63, "context": "The feature inputs for the first-layer object detection classifier \u03a85 \u2208 R are the HOG features extracted based on the candidate window as [65] plus the detection score from the part-based detector [64].", "startOffset": 197, "endOffset": 201}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "We use the dataset and the algorithm by [33] as the firstlayer geometric labeling module.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "[11], which we re-implement for six sub-tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 22, "endOffset": 26}, {"referenceID": 58, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 34, "endOffset": 38}, {"referenceID": 57, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 48, "endOffset": 52}, {"referenceID": 61, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "[38] (base)", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "CCM [11] 73.", "startOffset": 4, "endOffset": 8}, {"referenceID": 62, "context": "We do not do cross-validation on object detection as it is standard on the PASCAL 2006 [63] dataset (1277 train and 2686 test images respectively).", "startOffset": 87, "endOffset": 91}, {"referenceID": 45, "context": "The state-of-the-art classifiers improve on the base model by explicitly hand-designing the task specific probabilistic model [46, 59] or by using adhoc methods to implicitly use information from other tasks [33].", "startOffset": 126, "endOffset": 134}, {"referenceID": 58, "context": "The state-of-the-art classifiers improve on the base model by explicitly hand-designing the task specific probabilistic model [46, 59] or by using adhoc methods to implicitly use information from other tasks [33].", "startOffset": 126, "endOffset": 134}, {"referenceID": 32, "context": "The state-of-the-art classifiers improve on the base model by explicitly hand-designing the task specific probabilistic model [46, 59] or by using adhoc methods to implicitly use information from other tasks [33].", "startOffset": 208, "endOffset": 212}, {"referenceID": 58, "context": "The state-of-the-art method for depth estimation in [59] follows a slightly different testing procedure.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "5 CCM [11] 50.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "In order to analyze this, we consider the two tasks of scene recognition and object detection on the DS1 dataset in [11], which contains ground-truth labels for both the tasks.", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "We then combined four tasks: event categorization, scene categorization, depth estimation, and saliency detection, and got improvements in all these sub-tasks [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 65, "context": "1 Robotic Grasping Given an image and a depthmap (Figure 9), the goal of the learning algorithm in a grasping robot is to select a point to grasp the object (this location is called the grasp point, [66]).", "startOffset": 199, "endOffset": 203}, {"referenceID": 65, "context": "[66, 67] did not use object category information for grasping.", "startOffset": 0, "endOffset": 8}, {"referenceID": 66, "context": "[66, 67] did not use object category information for grasping.", "startOffset": 0, "endOffset": 8}, {"referenceID": 65, "context": "[66] which spans 6 object categories and also includes an aligned pixel level depth map for each image, as shown in Figure 9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 65, "context": "For grasp point detection, we compute image and depthmap features at each point in the image (using codes given by [66]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 65, "context": "Results: We evaluate our algorithm on a dataset published in [66], and perform cross-validation to evaluate the performance on each task.", "startOffset": 61, "endOffset": 65}, {"referenceID": 67, "context": "Implementation: For scene categorization, we use the indoor scene subsets in the Cal-Scene Dataset [68] and", "startOffset": 99, "endOffset": 103}, {"referenceID": 68, "context": "For geometric labeling, we use the Indoor Layout Data [69] and assign each pixel to one of three geometry classes: ground, wall and ceiling.", "startOffset": 54, "endOffset": 58}, {"referenceID": 69, "context": "For object detection, we use the PASCAL 2007 Dataset [70] and our own shoe dataset to learn detectors for four object categories: shoe, dining table, tv-monitor, and sofa.", "startOffset": 53, "endOffset": 57}, {"referenceID": 37, "context": "We first use the part-based object detection algorithm in [38] to create candidate windows, and then use the same classifiers as described in Section 5.", "startOffset": 58, "endOffset": 62}], "year": 2011, "abstractText": "Scene understanding includes many related sub-tasks, such as scene categorization, depth estimation, object detection, etc. Each of these sub-tasks is often notoriously hard, and state-of-the-art classifiers already exist for many of them. These classifiers operate on the same raw image and provide correlated outputs. It is desirable to have an algorithm that can capture such correlation without requiring any changes to the inner workings of any classifier. We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that jointly optimizes all the sub-tasks, while requiring only a \u2018black-box\u2019 interface to the original classifier for each sub-task. We use a two-layer cascade of classifiers, which are repeated instantiations of the original ones, with the output of the first layer fed into the second layer as input. Our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about which error modes to focus on. We show that our method significantly improves performance in all the sub-tasks in the domain of scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection. Our method also improves performance in two robotic applications: an object-grasping robot and an object-finding robot.", "creator": "LaTeX with hyperref package"}}}