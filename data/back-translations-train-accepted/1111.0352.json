{"id": "1111.0352", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2011", "title": "Revisiting k-means: New Algorithms via Bayesian Nonparametrics", "abstract": "One of the many benefits of Bayesian nonparametric processes such as the Dirichlet process is that they can be used for modeling infinite mixture models, thus providing a flexible answer to the question of how many clusters exist in a data set. For the most part, such flexibility is currently lacking in techniques based on hard clustering, such as k-means, graph cuts, and Bregman hard clustering. For finite mixture models, there is a precise connection between k-means and mixtures of Gaussians, obtained by an appropriate limiting argument. In this paper, we apply a similar technique to an infinite mixture arising from the Dirichlet process (DP). We show that a Gibbs sampling algorithm for DP mixtures approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like objective that includes a penalty term based on the number of clusters. We generalize our analysis to the case of clustering multiple related data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We discuss additional extensions that further highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that requires O(|E|) time per iteration and automatically determines the number of clusters in a graph.", "histories": [["v1", "Wed, 2 Nov 2011 00:09:18 GMT  (148kb,D)", "http://arxiv.org/abs/1111.0352v1", "18 pages"], ["v2", "Thu, 14 Jun 2012 15:05:55 GMT  (101kb,D)", "http://arxiv.org/abs/1111.0352v2", "14 pages. Updated based on the corresponding ICML paper"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["brian kulis", "michael i jordan"], "accepted": true, "id": "1111.0352"}, "pdf": {"name": "1111.0352.pdf", "metadata": {"source": "CRF", "title": "Revisiting k-means: New Algorithms via Bayesian Nonparametrics", "authors": ["Brian Kulis", "Michael I. Jordan"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Most cluster methods assume a fixed known value for k, which in many cases is simply not known a priori. Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which apply various model selection criteria. In the case of mixing models, one possible solution is to employ ideas from Bayesian nonparametrics, to expand the mixing models so that they \"automatically detect the number of clusters in a given dataset; a canonical example is the Dirichlet process (DP) mixing models; one possible solution is to apply ideas from Bayesian nonparametrics to extend the mixing models.\""}, {"heading": "2 Background", "text": "We will start with a brief discussion of the relevant models taken into account in this paper. In particular, we will give an overview of mixtures of Gaussian, k-averages, graph sections and DP mixtures. We will introduce the models and briefly discuss some algorithmic details and known connections between the models."}, {"heading": "2.1 Gaussian Mixture Models", "text": "In a (finite) Gaussian mixing model, we assume that the data is derived from the following distribution: p (x) = k \u2211 c = 1 \u03c0cN (x | \u00b5c, \u044bc), where k is the fixed number of components, \u03c0c is the mixing coefficients, and \u00b5c and \u0442c are the means or covariances of the k Gaussian distributions. In the non-Bayean setting, we can use the Expectation Maximization Algorithm to perform the maximum probability based on a series of observations x1,..., xn. In short, we initialize the mean values \u00b5c, covariances \u0445c and the mixing coefficients \u03c0c. Then we switch between the E-step and the M-step. In the E-step, using the current parameter values, we calculate the following quantities, which for all i = 1,..., n and for all c = 1,..., k: \u03b3 \u2212 zic \u2212 n (xi \u2212 zic) and we can sign the newness step."}, {"heading": "2.2 k-Means", "text": "A related model for clustering is provided by the k \u2212 mean objective function, a goal for the discovery of a hard cluster of data. Faced with a set of data points x1,..., xn, the k \u2212 mean objective function, cluster '1,...,' k tries to find the following objective function to minimize: min {'c} kc = 1 p = 1 p \u00b2 x x \u00b2 c \u00b2 x \u2212 c \u00b2 2 where \u00b5c = 1 | 'c \u00b2 x \u00b2 \u00b5c x.The most common method for minimizing this objective function is simply called k-mean algorithm. It initializes the algorithm with some hard clusters of data along with the cluster mean of the corresponding clusters. Then the algorithm switches between the reallocation of points to clusters and the recalculation of averages. Specifically, for the re-allocation of c averages, the square euclidean distance from each point to each cluster c is calculated and finds out the minimum: \""}, {"heading": "2.3 Graph Cuts", "text": "The k-mean lens and algorithm can be generalized in different ways. For example, it is unnecessary to apply k-mean in the core space. Suppose that the data was mapped via any function, so that the data points in the attribute space are generalized as follows: p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p (c), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p), p \"p\" p \"p\" p \"p\" p, \"p\" p, \"p,\" p \"p,\" p, \"p\" p, \"p,\" p \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p, p,\" p, p, p, p, \"p, p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p, p,\" p, p \"p,\" p, \"p\" p, \"p,\" p, \"p\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p"}, {"heading": "2.4 Dirichlet Process Mixture Models", "text": "We can describe the distribution over the cluster indicators as follows: The distribution over the cluster indicators follows a discrete distribution, so that a Bayesian extension of the mixing model arises by first establishing a symmetrical dirichlet before the mix coefficient. If we continue to assume that the Gaussian covariances are fixed on me and that the means are drawn from any previous distribution G0, we get the following Bayesian model: \u00b51,..., \u00b5k, G0, p, Dir (k) z1, zn, discretion (...) x1, xn, N, I).We denote \u03c0 = 1,..., one way to look at the DP mix is to take a border over the model."}, {"heading": "3 Hard Clustering via Dirichlet Processes", "text": "In the following sections, we will derive hardclustering algorithms based on DP mixing models. We will analyze the properties of the resulting algorithms and show connections to existing hardclustering algorithms, especially k-means."}, {"heading": "3.1 The Infinite Limit of the Gibbs Sampler", "text": "We now want to see what happens to these probabilities in order to do this."}, {"heading": "3.2 Underlying Objective and the AIC", "text": "With the procedure from the previous section in hand we can now analyze its properties. A first question is what the underlying cluster objective corresponds to, which corresponds to this k-medium-like algorithm. In this section we show that the monoton algorithm reduces the following target at each iteration monoton, where one iteration is defined as a full loop through all data points to update all cluster assignments and means: min 'c} kc = 1 x x x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "4 Clustering with Multiple Data Sets", "text": "One of the most useful enhancements to the standard DP mixing model is when we put another DP layer over the basic dimension. Specifically, we have the basic dimension G0 distributed even by a basic dimension H Dirichlet method. The result is that, faced with a collection of datasets, each of which is a DP mixture, the basic dimension of which is the common H Dirichlet process, we can cluster each dataset while ensuring that the clusters share a certain structure across the datasets. We will not describe the sampling algorithm for the Hierarchical Dirichlet Process (HDP) in detail, but refer the reader to [20] for a detailed introduction to the HDP model and a description of the reference techniques."}, {"heading": "4.1 Overview of the HDP", "text": "To set the stage, we assume that we have D records, 1,..., j,..., D. Denote xij is data point i from record j, and there should be nj data points from each record j. The basic idea is that we cluster the data points from each record locally, but that some clusters are shared across records. Each record j has a set of local cluster indicators given by zij, so that zij = c when the data point i in record j is mapped to the local cluster Sjc. Each local cluster Sjc is mapped to a global cluster, means \u00b5p. Local clusters across multiple records can be mapped to a single global cluster meaning."}, {"heading": "4.2 The Hard HDP", "text": "We can extend the asymptotic argument we used for the HDP algorithm: we will summarize the resulting algorithms; the derivative is analogous to the derivative of the individual DP model. We will have a threshold that determines when we need to introduce a new cluster. Algorithms work as follows: for each data point we will need two parameters (according to the above-mentioned HDP model): Let the \"local\" threshold be parameter, namely the \"global\" threshold. The algorithms work as follows: for each data point xij, we calculate the distance to each global cluster."}, {"heading": "5 Further Extensions", "text": "We are now discussing two additional extensions of the proposed goal: a spectral relaxation for the proposed hard clustering method and a normalized intersection algorithm that takes time O (| E |) per iteration but does not fix the number of clusters in the graph."}, {"heading": "5.1 Spectral Meets Nonparametric", "text": "In this section we will show that the DP-k mean is objectively related to the number of eigenvalues greater than the eigenvalues of the kernel matrix; these eigenvectors form the globally optimal \"relaxed\" cluster indicator indicator [24]. Similarly, in this section we will show the following extension: the globally optimal solution of the DP-k mean objective function is achieved by calculating the eigenvectors of the kernel matrix corresponding to the eigenvectors of the kernel matrix."}, {"heading": "5.2 Graph Clustering", "text": "In this section we will illustrate a practical application of our techniques by detailing the resulting normalized intersection problems. We will give a result that has proven itself in [6] for the normalized intersection problems.Theorem 5.3. Let J (K) be the diagonal degree matrix A. Let D be the diagonal degree matrix A. Let D be the diagonal degree matrix corresponding to A (D = diag) K and (diagonal) weight matrix W, and let NC (A) be the normalized objective with adjacency matrix A. Let D be the diagonal degree matrix. Let D is the diagonal degree matrix corresponding to A (Ae). Then the following relationship continues: J (K) = \u03c3n + tr (D \u2212 1 / 2AD \u2212 1) \u2212 1 / 2) \u2212 (B) k + NC (A) when we K = sp."}, {"heading": "6 A Preliminary Experiment", "text": "We provide some preliminary results on a simple synthetic dataset to explore some of the properties of our basic algorithm. We have generated a dataset of 100 points from three Gaussian distributions, as shown in the most left figure in Figure 2. The rest of this figure shows different clusters obtained by different choice of \u03bb. In Figure 3 we record the average number of clusters obtained for different choice of \u03bb; for this graph, 100 runs of the algorithm (with different permutations of the data indices) are executed for each choice of \u03bb. Interestingly, the algorithm shows remarkable robustness in determining the \"true\" number of clusters in this simple dataset: for \u03bb between.1575 and.22, the algorithm practically always returns 3 clusters."}, {"heading": "7 Conclusions and Open Problems", "text": "This paper outlines connections that arise between DP blending models and hard clustering algorithms, and develops new algorithms for hard clustering in the case of an unknown number of clusters. Our analysis is only a first step, and we believe that there are several possibilities for future work: \u2022 Can we generalize asymptotic analysis beyond the use of the Gibbs sampler? One can derive the k-mean lens from the complete data log probability of the Gaussian blending model; we suspect that an analogous result applies to the DP blend. \u2022 In the case of standard k-means (as well as standardized cut, etc.) there is significant work in improving the basic global algorithm. [5] Considered on the inclusion of local movements, suggesting that the DP blends move from one cluster to another, so that the k-mean lens lens lens lens lens lens lens lens lens is considered to be lowered. [5] Considered on the inclusion of local movements, these mixes could be considered mixing processes, so that the k-mean-lens lens lens lens lens lens lens lens lens process is considered to be improved. [5] Considered on the inclusion of local movements, suggesting that the DP mixtures move from one cluster to another, so that the k-mixing process may be considered to be mixing in the k-mean lens lens lens lens lens lens lens lens lens, etc.)."}], "references": [{"title": "Clustering with Bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "Journal of Machine Learning Research, 6:1705\u20131749", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "MDL principle for robust vector quantisation", "author": ["H. Bischof", "A. Leonardis", "A. Selb"], "venue": "Pattern Analysis and Applications, 2(1):59\u201372", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["D. Blei", "M. Jordan"], "venue": "Journal of Bayesian Analysis, 1(1):121\u2013144", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral k-way ratio cut partitioning", "author": ["P. Chan", "M. Schlag", "J. Zien"], "venue": "IEEE Trans. CAD-Integrated Circuits and Systems, 13:1088\u20131096", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Iterative clustering of high dimensional text data augmented by local search", "author": ["I.S. Dhillon", "Y. Guan", "J. Kogan"], "venue": "Proceedings of the 2nd IEEE International Conference on Data Mining, pages 131\u2013138", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Weighted graph cuts without eigenvectors: A multilevel approach", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(11):1944\u20131957", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "How many clusters? Which clustering method? Answers via model-based cluster analysis", "author": ["C. Fraley", "A.E. Raftery"], "venue": "Computer Journal, 41(8):578\u2013588", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning the k in k-means", "author": ["G. Hamerly", "C. Elkan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Bayesian Nonparametrics: Principles and Practice", "author": ["N. Hjort", "C. Holmes", "P. Mueller", "S. Walker"], "venue": "Cambridge University Press, Cambridge, UK", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["H. Ishwaran", "L.F. James"], "venue": "Journal of the American Statistical Association, 96(453):161\u2013173", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model", "author": ["S. Jain", "R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics, 13(1):158\u2013182", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "The application of cluster analysis in strategic management research: an analysis and critique", "author": ["D.J. Ketchen", "C . L. Shook"], "venue": "Strategic Management Journal,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "A permutation-augmented sampler for DP mixture models", "author": ["P. Liang", "M. Jordan", "B. Taskar"], "venue": "International Conference on Machine Learning", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Selecting variables for k-means cluster analysis by using a genetic algorithm that optimizes the silhouettes", "author": ["R. Lleti", "M.C. Ortiz", "L.A. Sarabia", "M.S. Sanchez"], "venue": "Analytica Chimica Acta, 515:87\u2013100", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge University Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics, 9:249\u2013265", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Combinatorial Stochastic Processes", "author": ["J. Pitman"], "venue": "Springer-Verlag", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888\u2013905", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Finding the number of clusters in a data set: An information theoretic approach", "author": ["C.A. Sugar", "G.M. James"], "venue": "Journal of the American Statistical Association, 98:750\u2013763", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association, 101(476):1566\u20131581", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "Journal of the Royal Statistical Society Series B, 63:411\u2013423", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Hierarchical priors and mixture models", "author": ["M. West", "P. M\u00fcller", "M.D. Escobar"], "venue": "with application in regression and density estimation. In P. R. Freeman and A. F. M. Smith, editors, Aspects of Uncertainty, pages 363\u2013386. John Wiley", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Multiclass spectral clustering", "author": ["S.X. Yu", "J. Shi"], "venue": "Proceedings of the International Conference on Computer Vision", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Spectral relaxation for k-means clustering", "author": ["H. Zha", "X. He", "C. Ding", "H. Simon", "M. Gu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 1, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 6, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 7, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 11, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 13, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 18, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 20, "context": "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.", "startOffset": 114, "endOffset": 139}, {"referenceID": 15, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 214, "endOffset": 222}, {"referenceID": 21, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 214, "endOffset": 222}, {"referenceID": 15, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 254, "endOffset": 258}, {"referenceID": 10, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 2, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 310, "endOffset": 313}, {"referenceID": 12, "context": "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].", "startOffset": 350, "endOffset": 354}, {"referenceID": 14, "context": "While this objective appears in the k-means literature, as an extension motivated by the Akaike Information Criterion (AIC) [15], it does not appear that any algorithms have been developed for optimization of the objective, nor are there any existing connections made to Bayesian nonparametrics.", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "We can take a step further and extend our analysis to the hierarchical Dirichlet process (HDP), a model for clustering multiple related data sets [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "This connection leads to kernel k-means algorithms for monotonically decreasing the cut values, and which scale efficiently to large graphs; the work in [6] developed a technique for graph clustering based on this connection that outperforms spectral clustering approaches to graph clustering, and scales to significantly larger graphs.", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].", "startOffset": 108, "endOffset": 119}, {"referenceID": 17, "context": "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].", "startOffset": 108, "endOffset": 119}, {"referenceID": 22, "context": "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].", "startOffset": 108, "endOffset": 119}, {"referenceID": 5, "context": "A surprising fact is that both the normalized cut and ratio cut objective functions can be expressed exactly as the weighted kernel k-means function for appropriate choice of weights and kernels [6].", "startOffset": 195, "endOffset": 198}, {"referenceID": 8, "context": "4 Dirichlet Process Mixture Models Finally, we briefly review DP mixture models [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 21, "context": "[22] and further discussed by Neal [16], Algorithm 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[22] and further discussed by Neal [16], Algorithm 2.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "[15] describe the above penalized k-means objective function with a motivation arising from the AIC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "We will not describe the sampling algorithm for the hierarchical Dirichlet process (HDP) in detail, but refer the reader to [20] for a detailed introduction to the HDP model and a description of inference techniques.", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "In particular, for the k-means objective, one computes the eigenvectors corresponding to the k largest eigenvalues of the kernel matrix over the data; these eigenvectors form the globally optimal \u201crelaxed\u201d cluster indicator matrix [24].", "startOffset": 231, "endOffset": 235}, {"referenceID": 5, "context": "We state a result proven in [6] for standard normalized cut.", "startOffset": 28, "endOffset": 31}, {"referenceID": 4, "context": "For instance, [5] looked at incorporating local moves, which finds points to move from one cluster to another such that the k-means objective is lowered.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.", "startOffset": 21, "endOffset": 29}, {"referenceID": 16, "context": "Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.", "startOffset": 21, "endOffset": 29}, {"referenceID": 0, "context": "\u2022 The work of [1] considers a more general connection between mixture models and k-means-type algorithms.", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "One of the many benefits of Bayesian nonparametric processes such as the Dirichlet process is that they can be used for modeling infinite mixture models, thus providing a flexible answer to the question of how many clusters exist in a data set. For the most part, such flexibility is currently lacking in techniques based on hard clustering, such as k-means, graph cuts, and Bregman hard clustering. For finite mixture models, there is a precise connection between k-means and mixtures of Gaussians, obtained by an appropriate limiting argument. In this paper, we apply a similar technique to an infinite mixture arising from the Dirichlet process (DP). We show that a Gibbs sampling algorithm for DP mixtures approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like objective that includes a penalty term based on the number of clusters. We generalize our analysis to the case of clustering multiple related data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We discuss additional extensions that further highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that requires O(|E|) time per iteration and automatically determines the number of clusters in a graph.", "creator": "LaTeX with hyperref package"}}}