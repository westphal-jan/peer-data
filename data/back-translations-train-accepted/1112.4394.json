{"id": "1112.4394", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2011", "title": "Additive Gaussian Processes", "abstract": "We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.", "histories": [["v1", "Mon, 19 Dec 2011 16:22:09 GMT  (3861kb,D)", "http://arxiv.org/abs/1112.4394v1", "Appearing in Neural Information Processing Systems 2011"]], "COMMENTS": "Appearing in Neural Information Processing Systems 2011", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["david k duvenaud", "hannes nickisch", "carl edward rasmussen"], "accepted": true, "id": "1112.4394"}, "pdf": {"name": "1112.4394.pdf", "metadata": {"source": "CRF", "title": "Additive Gaussian Processes", "authors": ["David Duvenaud", "Hannes Nickisch"], "emails": ["dkd23@cam.ac.uk", "hn@tue.mpg.de", "cer54@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Most statistical regression models used today are of the form: g (y) = f (x1) + f (x2) + \u00b7 \u00b7 + f (xD). Popular examples include logistic regression, linear regression, and generalized linear models [1]. This family of functions, known as Generalized Additive Models (GAM) [2], is generally easy to adjust and interpret. Some extensions of this family, such as smoothing splines ANOVA [3], add terms depending on more than one variable. However, such models tend to be intractable and difficult to fit as the number of terms increases. At the other end of the spectrum are kernel-based models that typically depend on the response to all input variables at the same time. These have the form: y = f (x1, x2, xD)."}, {"heading": "2 Gaussian Process Models", "text": "Gaussian processes are a flexible and traceable precursor of functions, useful for solving regression and classification tasks. [5] The type of structure that can be captured by a GP model is mainly determined by its core: the covariance function. One of the greatest difficulties in defining a Gaussian process model is the selection of a core that can represent the structure present in the data. In small to medium-sized datasets, the core has a major impact on modelling efficiency. Figure 1 compares an additive core of the first order with a core of the second order for two-dimensional functions. We see that a GP with a first-order additive core is an example of a GAM: every function derived from this model is a sum of orthogonal one-dimensional functions. Compared to functions from the higher-order GP, such functions from the GP have a greater range."}, {"heading": "3 Additive Kernels", "text": "We now give a precise definition of the additive cores. First, we assign a sum to each dimension (1. D) = a one-dimensional base quantity ki (xi, x \"i), then we define the additive kernel of first order, second order and third order as: kadd1 (x, x\") = \u03c321 D = 1 ki (xi, x \"i) (1) kadd2 (x, x\" i) = \u03c322 D \u2211 i = 1 D \u2211 j = i + 1 ki (xi, x \"i) kj (xj, x\" j) (2) kaddn (x, x \"i) = \u03c32n \u2211 1 \u2264 i1 < i2 <... < in \u2264 D [n \u00b2 d = 1kid (xid, x\" id \"id)] (3), where D is the dimension of our input space (xx2\") and \u03c32n is the variance assigned to all n \u00b2 d \u00b2 interactions."}, {"heading": "3.1 Parameterization", "text": "The only design choice necessary for specifying an additive core is to select a one-dimensional base core for each input dimension. All parameters (such as length scales) of the base cores can be learned as usual by maximizing the marginal probability of the training data. In addition to the hyperparameters of each dimensional core, additive cores are equipped with a series of D hyperparameters that determine how much variance we assign to each interaction order. These \"order variance\" hyperparameters have a useful interpretation: The hyperparameter for the variance of the Dth order variance controls how much of the target function variance comes from interactions of the Dth order. Table 1 shows examples of normalized order variance hyperparameters learned on real datasets. On different datasets, the dominant of the interaction variants estimated by the additive model can vary greatly. An additive GP with all of its function comes from the 1st order, which is a hyperquivalent."}, {"heading": "3.2 Interpretability", "text": "As Plate [6] notes, one of the main advantages of additive models such as GAM is their interpretability. Plate also points out that one can compare high-order and low-order interactions with predictive accuracy by allowing interpretability. In the case where the hyperparameters indicate that most of the variance of a function can be explained by low-order interactions, it is useful and easy to draw the corresponding low-order functions, as in Figure 2."}, {"heading": "3.3 Efficient Evaluation of Additive Kernels", "text": "In this section we show how to evaluate the sum of all terms in O (D2). The additive core of the n th order corresponds to the n elementary symmetric polynomial [7] [8], which we call en. Example: If x has 4 input dimensions (D = 4), and if we zi = ki (xi, x \u2032 i), thenkadd1 (x \u2032), thenkadd1 (x \u2032) = e1 (z1, z2, z3, z4) = zzard \u2212 z3 + z4 kadd2 (x \u2032) = e2 (z1, z3, z4)."}, {"heading": "3.4 Computation", "text": "The calculation cost of evaluating the gram matrix of a product core (such as the SE kernel) is O (N2D), while the cost of evaluating the gram matrix of the additive core O (N2DR), where R is the maximum permitted level of interaction (up to D), can be significant in higher dimensions, even compared to the fixed O (N3) cost of reversing the gram matrix. However, as our experiments show, typically only the first few interaction orders are important for modeling a particular function; therefore, if you are computer-dependent, you can simply limit the maximum degree of interaction without losing much accuracy. Additive Gauss processes are particularly attractive in practice, as their application requires only the specification of the base core. All other aspects of GP inference remain the same. All experiments in this paper were performed using the standard PML toolbox1; all experiments are available to the author."}, {"heading": "4 Related Work", "text": "Plate [6] constructs a kind of additive GP, but only using the terms first order and third order. This model is motivated by the desire to reconcile the interpretative ability of first order models with the flexibility of models of complete order. Our experiments show that often the middle degree of interaction contributes most of the variance.A related functional ANOVA-GP model [9] breaks down the middle function into a weighted sum of GP. However, the effect of a certain gradient of interaction cannot be quantified by this approach. Also, from a computational point of view, the Gibbs sampling approach used in [9] is disadvantageous. Christoudias et al. [10] has previously shown how mixtures of nuclei can be learned through gradient descent in the Gaussian process framework. They refer to this approach as Bayesian localized multiple kernel learning. However, their approach learns a mixture via a small group of solid nuclei, while our method learns about all possible products through this mixture."}, {"heading": "4.1 Hierarchical Kernel Learning", "text": "Bach [4] uses a regulated optimisation framework to learn a weighted sum over an exponential number of nuclei that can be calculated in polynomial time.The subsets of nuclei taken into account by this method are limited to being a shell of nucleus.3 In view of the nucleus of each dimension and a predefined weighting across all terms, HKL performs the model selection by searching through hulls of interaction terms. In [4] Bach also determines the relative weighting between interaction orders with a single term \u03b1, with the sum of all terms according to: ka (x, x) = v2D = 1 (1 + \u03b1kd (xd, x \u2032 d))) (8) having a computational complexity O (D). However, this formulation forces the weight of all third terms of the order according to: ka (x) = v2D = d = 1 (1 + \u03b1kd (xd, x \u2032 d) (x8) (O)."}, {"heading": "4.2 ANOVA Procedures", "text": "Vapnik [11] introduces the support vector ANOVA decomposition, which has the same shape as our additive core. However, they recommend approximating the sum of all D commands with only one term of \"reasonable order,\" presumably due to the difficulty of determining the hyperparameters of an SVM. Stitson et al. [12], which favorably compares the support vector ANOVA decomposition with polynomial and spline cores. Again, they allowed only one order to be active, and set hyperparameters by cross-validation. A closely related procedure from the statistical literature is the smoothing splines ANOVA (SS-ANOVA) [3]. An SS-ANOVA model is estimated as a weighted sum of splines along each dimension, plus a sum of splines across all pairs of dimensions, all trebles, etc., with each individual interaction parameter having a separate weight parameter. Since the number of terms to be taken into account in the second order of the ANVA rule can only be considered in the first and second order of magnitude."}, {"heading": "4.3 Non-local Interactions", "text": "By far the most popular nuclei for regression and classification tasks based on continuous data are the quadratic exponential (Gaussian) nucleus and the mate \u0301 rn kernel. These nuclei depend only on the scaled euclidean distance between two points, both of which have the shape: k (x, x \u2032) = f (\u2211 Dd = 1 (xd \u2212 x \u2032 d) 2 / l2d). Bengio et al. [13] argue that models based on quadratic exponential nuclei are particularly vulnerable to the curse of dimensionality. They stress that the locality of the nuclei means that these models cannot capture non-local structure. They argue that many functions that are important to us have such a structure. Methods that are based exclusively on local nuclei require training examples on all combinations of relevant input factors. Additive nuclei have a much more complex structure and allow an extrapolation based on a strong input structure between the two input parts, forming an input mass distribution between the two."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Synthetic Data", "text": "Since additive GPs can detect a non-local structure in the data, they are particularly well suited for problems where local interpolation fails. Figure 5 shows a dataset demonstrating this property of additive GPs, consisting of data obtained from the sum of two axial sine functions; the training set is limited to a small, L-shaped range; the test set contains a peak far removed from the training types; the additive GP has restored both original sine functions (shown in green) and correctly concluded that most of the variance in the function is due to first-order interactions; the ability of additive GPs to detect far-reaching structures suggests that this model may be well suited to deal with problems of covariant shift."}, {"heading": "5.2 Experimental Setup", "text": "Based on a diverse collection of data sets, we compared five different models. In the following result tables, GP Additives refers to a GP who uses the additive kernel with square Exp base cores. For speed reasons, we have the maximum order of interaction in the additive cores to 10. GP-GAM refers to an additive GP model with only first order interactions. GP Squared-Exp is a GP model with square exponential ARD kernel. HKL4 was executed with the kernel of all subsets, which corresponds to the same set of cores that the additive GP considers with a square-exp base kernel. In addition to the learned kernel hyperparameters, we adapt hyperparameters to the standard method to maximize the probability of training sets. In the classification, experiments were performed using L-BFGS [14] for 500 iterations, which allows five random restarts to match hyper-learning averages."}, {"heading": "5.3 Results", "text": "Tables 2, 3, 4 and 5 show the mean performance over 10 tensile test splits. Since HKL does not specify a noise model, it could not be included in the probability comparisons. The model with the best performance on each data set is presented in bold, along with all other models that did not significantly differ in the context of a paired t test. The additive model never performs significantly worse than any other model and sometimes performs significantly better than all other models. The4Code for HKL is available at http: / / www.di.ens.fr / fbach / hkl / difference between all methods larger in the case of regression experiments. The performance of HKL is consistent with the results in [4] and performs competitively, but slightly worse than SE-GPS. The additive GP performs best on data sets that are well explained by low order of interaction, and approximately as well as the SE-GP model on data sets, which is explained by high orders of GP, where GP is a local advantage (see table)."}, {"heading": "6 Conclusion", "text": "We present additive Gaussian processes: a simple family of models that generalizes two widely used classes of models. Additive GPs also introduce a tractable new type of structure into the GP framework. Our experiments suggest that such an additive structure exists in real data sets, allowing our model to function better than standard GP models. If there is no such structure, our model can also restore arbitrarily flexible models. In addition to improving modeling efficiency, additive GP also improves the interpretability of the model: the order variance hyperparameters indicate what types of structure are present in our model. Compared to HKL, the only other tractable method capable of detecting the same structure types, our method benefits from the ability to learn individual kernel hyperparameters, as well as from the weighting of different orders of interaction. Our experiments show that additive GPs are a state-of-the-art regression model."}, {"heading": "Acknowledgments", "text": "The authors thank John J. Chew and Guillaume Obozonksi for their helpful comments."}], "references": [{"title": "Generalized linear models", "author": ["J.A. Nelder", "R.W.M. Wedderburn"], "venue": "Journal of the Royal Statistical Society. Series A (General),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1972}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Spline models for observational data", "author": ["G. Wahba"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "High-dimensional non-linear variable selection through hierarchical kernel learning", "author": ["Francis Bach"], "venue": "CoRR, abs/0909.0844,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "CKI Williams"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Accuracy versus interpretability in flexible modeling: Implementing a tradeoff using Gaussian process models", "author": ["T.A. Plate"], "venue": "Behaviormetrika,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Symmetric functions and Hall polynomials", "author": ["I.G. Macdonald"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Enumerative combinatorics", "author": ["R.P. Stanley"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Bayesian functional anova modeling using Gaussian process prior distributions", "author": ["C.G. Kaufman", "S.R. Sain"], "venue": "Bayesian Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Bayesian localized multiple kernel learning", "author": ["M. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "Technical report,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Statistical learning theory, volume 2", "author": ["V.N. Vapnik"], "venue": "Wiley New York,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Support vector regression with ANOVA decomposition kernels. Advances in kernel methods: Support vector learning, pages", "author": ["M. Stitson", "A. Gammerman", "V. Vapnik", "V. Vovk", "C. Watkins", "J. Weston"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "The curse of highly variable functions for local kernel machines", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Updating quasi-newton matrices with limited storage", "author": ["J. Nocedal"], "venue": "Mathematics of computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1980}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Popular examples include logistic regression, linear regression, and Generalized Linear Models [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "This family of functions, known as Generalized Additive Models (GAM) [2], are typically easy to fit and interpret.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "Some extensions of this family, such as smoothing-splines ANOVA [3], add terms depending on more than one variable.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "We note that a similar breakthrough has recently been made, called Hierarchical Kernel Learning (HKL) [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "Finally, on real datasets, HKL is outperformed by the standard SE-GP [4].", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "Gaussian processes are a flexible and tractable prior over functions, useful for solving regression and classification tasks [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "2 Interpretability As noted by Plate [6], one of the chief advantages of additive models such as GAM is their interpretability.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "The nth order additive kernel corresponds to the nth elementary symmetric polynomial [7] [8], which we denote en.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "The nth order additive kernel corresponds to the nth elementary symmetric polynomial [7] [8], which we denote en.", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "2 4 Related Work Plate [6] constructs a form of additive GP, but using only the first-order and Dth order terms.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "A related functional ANOVA GP model [9] decomposes the mean function into a weighted sum of GPs.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Also, computationally, the Gibbs sampling approach used in [9] is disadvantageous.", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "[10] previously showed how mixtures of kernels can be learnt by gradient descent in the Gaussian process framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "1 Hierarchical Kernel Learning Bach [4] uses a regularized optimization framework to learn a weighted sum over an exponential number of kernels which can be computed in polynomial time.", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "In [4], Bach also fixes the relative weighting between orders of interaction with a single term \u03b1, computing the sum over all orders by: ka(x,x \u2032) = v D D \u220f", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "For details, see [4].", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "of [4] is that hyperparameters are hard to set other than by cross-validation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "2 ANOVA Procedures Vapnik [11] introduces the support vector ANOVA decomposition, which has the same form as our additive kernel.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "[12] performed experiments which favourably compared the support vector ANOVA decomposition to polynomial and spline kernels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "A closely related procedure from the statistics literature is smoothing-splines ANOVA (SS-ANOVA) [3].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "[13] argue that models based on squared-exponential kernels are particularily susceptible to the curse of dimensionality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "For all GP models, we fit hyperparameters by the standard method of maximizing training-set marginal likelihood, using L-BFGS [14] for 500 iterations, allowing five random restarts.", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "In the classification experiments, GP inference was done using Expectation Propagation [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "The performance of HKL is consistent with the results in [4], performing competitively but slightly worse than SE-GP.", "startOffset": 57, "endOffset": 60}], "year": 2011, "abstractText": "We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.", "creator": "LaTeX with hyperref package"}}}