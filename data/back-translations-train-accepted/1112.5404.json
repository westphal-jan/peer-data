{"id": "1112.5404", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2011", "title": "Similarity-based Learning via Data Driven Embeddings", "abstract": "We consider the problem of classification using similarity/distance functions over data. Specifically, we propose a framework for defining the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions. Our framework unifies and generalizes the frameworks proposed by [Balcan-Blum ICML 2006] and [Wang et al ICML 2007]. An attractive feature of our framework is its adaptability to data - we do not promote a fixed notion of goodness but rather let data dictate it. We show, by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems. We propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria. We then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection. We demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark UCI datasets on which our method consistently outperforms existing approaches by a significant margin.", "histories": [["v1", "Thu, 22 Dec 2011 18:08:27 GMT  (113kb,S)", "http://arxiv.org/abs/1112.5404v1", "To appear in the proceedings of NIPS 2011, 14 pages"]], "COMMENTS": "To appear in the proceedings of NIPS 2011, 14 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["purushottam kar", "prateek jain 0002"], "accepted": true, "id": "1112.5404"}, "pdf": {"name": "1112.5404.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["purushot@cse.iitk.ac.in", "prajain@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 111 2.54 04v1 [cs.LG] 2 2D ec"}, {"heading": "1 Introduction", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country and a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "2 Methodology", "text": "Let D be a fixed but unknown distribution over the labeled input domain X and let's not get down to an average similarity to similar values. Let D be a fixed but unknown distribution over the labeled input domain X and let it work similarly, given a (potentially non-PSD) notion of a similarity function over D. Now, it seems unlikely that learning a reasonable similarity is unlikely if the given similarity function does not have an inherent \"goodness\" property. Intuitively, the goodness of a similarity function should be at hand for the classification task. For PSD kernels, the notion of goodness is defined in terms of the margin offered in the RKHS. However, a more fundamental requirement is that the similarity function should retain affinities under similarly labeled points - that is, a good similarity function should not be based on an average similarity, but on an average similarity to higher values."}, {"heading": "2.1 Learning the transfer function", "text": "In this section, we present the results that allow us to learn an almost optimal transfer function from a family of transfer functions."}, {"heading": "2.2 Working with surrogate loss functions", "text": "The formulation of a good similarity function indicates a simple learning algorithm that involves the construction of an embedding of the domain in a land-marked space on which the existence of a large classifier with a low misclassification rate is guaranteed. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "2.3 Selecting informative landmarks", "text": "Remember that generalization guarantees that in the previous section we are relying on random selection of landmark pairs from a fixed distribution across the domain. However, in practice, a completely random selection may require selecting a large number of landmarks, leading to inefficient classification in terms of training and testing times. Therefore, we propose a landmark pair selection that we call DSELECT (see Algorithm 1). heuristic generalization naturally generalizes to multi-class problems and can also be applied to the classification model of Balcan-Blum, which uses landmark singletons instead of pairs. At the core of our heuristic selection is a novel notion of diversity under Landmark-1 (see Algorithm 1). Assuming that K is a standardized core core, we call a number of points S-X different when the average landmark singlet is used instead of pairs."}, {"heading": "3 Empirical results", "text": "In this section, we examine empirically the performance of our proposed methods on a variety of benchmark data sets. We refer to the algorithmic formulation presented in [1] as BBS and its augmentation using DSELECT as BBS + D. We refer to the formulation presented in [2] as DBOOST. We refer to our transfer function, which is based on the formulation as FTUNE and its augmentation using DSELECT as FTUNE + D. In multi-class classification scenarios, we will use a one-on-all formulation that allows us to further exploit the transfer function by learning separate transfer functions per class (i.e. per one-on-all problem). We refer to our formulation using a single (or multiple) transfer function as FTUNE + D-S (resp. FTUNE + D-M-M)."}, {"heading": "3.1 Similarity learning datasets", "text": "First, we conduct experiments with a few similarity learning datasets [5]; these datasets provide a (non-PSD compliant) similarity matrix along with class names. For each dataset, we randomly select 70% of the data for training, 10% for validation, and the remaining 10% for testing purposes. We then apply our FTUNE-S, FTUNE + D-S, BBS + D methods along with BBS, and DBOOST with different numbers of landmark pairs. Note that we do not apply our FTUNE-M method to these datasets because it greatly exceeds these datasets, as they are usually small in size. First, we compare the accuracy achieved by FTUNE + D-S with existing methods, with Table 1 comparing the accuracy achieved by our FTUNE + D-S method with the BBS and DBOOST datasets across different datasets, with different parentheses in parentheses."}, {"heading": "3.2 UCI benchmark datasets", "text": "We have carried out experiments with FTUNE and FTUNE + D (FTUNE with a single transfer function) and FTUNE-M (FTUNE with one transfer function per class), but the latter has brought no benefit. Due to space constraints, we delete them from our presentation and show only results for FTUNE-S (FTUNE with a single transfer function) and FTUNE-M. Similar to [2], we use the Gaussian core function as a similarity function for evaluating our method. We give the \"width parameter\" in the Gaussian core as the mean of all paired training distances, a standard heuristics. For all data sets, we randomly select 50% data for training, 20% for validation, and the remaining data for testing. We report accuracy values averaged over 20 runs for each method, with varying numbers of limit paths. Table 2 compares the accuracy achieved by our FTUNE-S and FTUNE-large limits with most of the other methods we use on a BOBS basis."}, {"heading": "3.3 Discussion", "text": "We find that FTUNE, because it chooses its output by validation, is prone to overadjustments to small datasets, but at the same time is able to increase performance to large datasets. We observe a similar trend in our experiments - for smaller datasets (such as those in Table 1 with average dataset size 660), FTUNE overadjustments and performance worse than BBS and DBOOST. But even in these cases, DSELECT (intuitively) eliminates redundancies in boundaries, allowing FTUNE to regain the best transfer function. In contrast, for larger datasets such as those in Table 2 (average size 13200), FTUNE itself is able to restore better transmission functions than the base methods, and thus both FTUNE-S and FTUNE-M perform significantly better than the baseline."}, {"heading": "Acknowledgments", "text": "We thank the authors of [2] for providing the C + + code for their implementation. P. K. is supported by Microsoft Corporation and Microsoft Research India as part of a Microsoft Research India Ph.D. Fellowship Award. Most of this work was done during a visit by P. K. to Microsoft Research Labs India, Bangalore."}, {"heading": "A Comparison with the models of Balcan-Blum and Wang et al", "text": "In [2] Wang et al. consider a model of learning with distance functions. Their model is similar to our model (except that they limit themselves to the use of a single transfer function, namely the drawing function f = sgn (). Formally, they have the following idea of a good distance function. Definition 8 ([1] Definition 4). A distance function X, d: X \u00b7 X \u00b7 X \u00b7 R is considered to be a (x) good distance for a learning problem where there is one (x), B > 0 if there are two class probability distributions."}], "references": [{"title": "On a Theory of Learning with Similarity Functions", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "On Learning with Dissimilarity Functions", "author": ["Liwei Wang", "Cheng Yang", "Jufu Feng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Fast Image Retrieval via Embeddings", "author": ["Piotr Indyk", "Nitin Thaper"], "venue": "In International Workshop Statistical and Computational Theories of Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "On Combining Dissimilarity Representations", "author": ["El\u017cbieta P\u0229kalska", "Robert P.W. Duin"], "venue": "In Multiple Classifier Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Similarity-based Classification: Concepts and Algorithms", "author": ["Yihua Chen", "Eric K. Garcia", "Maya R. Gupta", "Ali Rahimi", "Luca Cazzanti"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Learning with non-positive Kernels", "author": ["Cheng Soon Ong", "Xavier Mary", "St\u00e9phane Canu", "Alexander J. Smola"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Feature Space Interpretation of SVMs with Indefinite Kernels", "author": ["Bernard Haasdonk"], "venue": "IEEE Transactions on Pattern Analysis and Machince Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Classification on Pairwise Proximity Data", "author": ["Thore Graepel", "Ralf Herbrich", "Peter Bollmann-Sdorra", "Klaus Obermayer"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Improved Guarantees for Learning via Similarity Functions", "author": ["Maria-Florina Balcan", "Avrim Blum", "Nathan Srebro"], "venue": "In 21st Annual Conference on Computational Learning Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "More Generality in Efficient Multiple Kernel Learning", "author": ["Manik Varma", "Bodla Rakesh Babu"], "venue": "In 26th Annual International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Metric and Kernel Learning using a Linear Transformation", "author": ["Prateek Jain", "Brian Kulis", "Jason V. Davis", "Inderjit S. Dhillon"], "venue": "To appear, Journal of Machine Learning (JMLR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Kernels as Features: On Kernels, Margins, and Low-dimensional Mappings", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "How Good Is a Kernel When Used as a Similarity Measure", "author": ["Nathan Srebro"], "venue": "In 20th Annual Conference on Computational Learning Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Computers and Intractability: A Guide to the theory of NP-Completeness", "author": ["M.R. Garey", "D.S. Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1979}, {"title": "The Hardness of Approximate Optima in Lattices, Codes, and Systems of Linear Equations", "author": ["Sanjeev Arora", "L\u00e1szl\u00f3 Babai", "Jacques Stern", "Z. Sweedyk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Designing classifiers for fusion-based biometric verification", "author": ["Krithika Venkataramani", "B.V.K. Vijaya Kumar"], "venue": "In Plataniotis Boulgouris and Micheli-Tzankou, editors, Biometrics: Theory, Methods and Applications. Springer,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Our framework unifies and generalizes the frameworks proposed by [1] and [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Our framework unifies and generalizes the frameworks proposed by [1] and [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "Take for example the case of images on which the most natural notions of distance (Euclidean, Earth-mover) [3] do not form PSD kernels.", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "The first approach tries to coerce a given similarity measure into a PSD one by either clipping or shifting the spectrum of the kernel matrix [4, 5].", "startOffset": 142, "endOffset": 148}, {"referenceID": 4, "context": "The first approach tries to coerce a given similarity measure into a PSD one by either clipping or shifting the spectrum of the kernel matrix [4, 5].", "startOffset": 142, "endOffset": 148}, {"referenceID": 4, "context": "k-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5], or are forced to solve non-convex formulations [6, 7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "k-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5], or are forced to solve non-convex formulations [6, 7].", "startOffset": 144, "endOffset": 150}, {"referenceID": 6, "context": "k-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5], or are forced to solve non-convex formulations [6, 7].", "startOffset": 144, "endOffset": 150}, {"referenceID": 0, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 1, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 7, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 8, "context": "The third approach, which has been investigated recently in a series of papers [1, 2, 8, 9], uses the similarity function to embed the domain into a low dimensional Euclidean space.", "startOffset": 79, "endOffset": 91}, {"referenceID": 9, "context": "Assuming a certain \u201cgoodness\u201d property (that is formally defined) for the similarity function, these models offer both generalization guarantees in terms of how well-suited the similarity function is to the classification task as well as the ability to use fast algorithmic techniques such as linear SVM [10] on the landmarked space.", "startOffset": 304, "endOffset": 308}, {"referenceID": 0, "context": "The model proposed by Balcan-Blum in [1] gives sufficient conditions for a similarity function to be well suited to such a landmarking approach.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "in [2] on the other hand provide goodness conditions for dissimilarity functions that enable landmarking algorithms.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Our framework consequently unifies and generalizes those presented in [1] and [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "Our framework consequently unifies and generalizes those presented in [1] and [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "Now similar to [1, 2], our framework requires random sampling of training points to create the embedding space1.", "startOffset": 15, "endOffset": 21}, {"referenceID": 1, "context": "Now similar to [1, 2], our framework requires random sampling of training points to create the embedding space1.", "startOffset": 15, "endOffset": 21}, {"referenceID": 1, "context": "To address this issue, [2] proposes a heuristic to select the points that are to be used as landmarks.", "startOffset": 23, "endOffset": 26}, {"referenceID": 10, "context": "MKL techniques [11]) or the distance measure (eg.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "[12]) itself.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Akin to [1], our techniques can also be extended to learn a combination of (dis)similarity functions but we do not explore these extensions in this paper.", "startOffset": 8, "endOffset": 11}, {"referenceID": 12, "context": "For PSD kernels, the notion of goodness is defined in terms of the margin offered in the RKHS [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "in the sense that all PSD kernels that offer a good margin in their respective RKHSs satisfy some form of this goodness criterion as well [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Balcan-Blum in [1] present a goodness criteria in which a good similarity function is considered to be one that, for most points, assigns a greater average similarity to similarly labeled points than to dissimilarly labeled points.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Wang et al in [2] define a distance function d to be good if a large fraction of the domain is, on an average, closer to similarly labeled points than to dissimilarly labeled points.", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "However it turns out that their definition is equivalent to one in which one again assigns weights to domain elements, as done by [1], and the following holds E x\u2032,x\u2032\u2032\u223cD\u00d7D [w(x)w(x) sgn (d(x, x)\u2212 d(x, x)) |l(x) = l(x), l(x) 6= l(x)] > \u03b3 (2) Assuming their respective goodness criteria, [1] and [2] provide efficient algorithms to learn classifiers with bounded generalization error.", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "However it turns out that their definition is equivalent to one in which one again assigns weights to domain elements, as done by [1], and the following holds E x\u2032,x\u2032\u2032\u223cD\u00d7D [w(x)w(x) sgn (d(x, x)\u2212 d(x, x)) |l(x) = l(x), l(x) 6= l(x)] > \u03b3 (2) Assuming their respective goodness criteria, [1] and [2] provide efficient algorithms to learn classifiers with bounded generalization error.", "startOffset": 286, "endOffset": 289}, {"referenceID": 1, "context": "However it turns out that their definition is equivalent to one in which one again assigns weights to domain elements, as done by [1], and the following holds E x\u2032,x\u2032\u2032\u223cD\u00d7D [w(x)w(x) sgn (d(x, x)\u2212 d(x, x)) |l(x) = l(x), l(x) 6= l(x)] > \u03b3 (2) Assuming their respective goodness criteria, [1] and [2] provide efficient algorithms to learn classifiers with bounded generalization error.", "startOffset": 294, "endOffset": 297}, {"referenceID": 0, "context": "As in [1, 2], our goodness criterion lends itself to a simple learning algorithm which consists of choosing a set of d random pairs of points from the domain P = {( x+i , x \u2212 i d i=1 (which we refer to We refer the reader to the appendix for a discussion.", "startOffset": 6, "endOffset": 12}, {"referenceID": 1, "context": "As in [1, 2], our goodness criterion lends itself to a simple learning algorithm which consists of choosing a set of d random pairs of points from the domain P = {( x+i , x \u2212 i d i=1 (which we refer to We refer the reader to the appendix for a discussion.", "startOffset": 6, "endOffset": 12}, {"referenceID": 14, "context": "Unfortunately, not only is this problem intractable but also hard to solve approximately [15, 16].", "startOffset": 89, "endOffset": 97}, {"referenceID": 15, "context": "Unfortunately, not only is this problem intractable but also hard to solve approximately [15, 16].", "startOffset": 89, "endOffset": 97}, {"referenceID": 0, "context": "With a similar objective in mind, [1] offers variants of its goodness criterion tailored to the hinge loss function which can be efficiently optimized on large training sets (for example LIBSVM [17]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 16, "context": "With a similar objective in mind, [1] offers variants of its goodness criterion tailored to the hinge loss function which can be efficiently optimized on large training sets (for example LIBSVM [17]).", "startOffset": 194, "endOffset": 198}, {"referenceID": 17, "context": "Similar notions of diversity have been used in the past for ensemble classifiers [18] and k-NN classifiers [5].", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "Similar notions of diversity have been used in the past for ensemble classifiers [18] and k-NN classifiers [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "We refer to the algorithmic formulation presented in [1] as BBS and its augmentation using DSELECT as BBS+D.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "We refer to the formulation presented in [2] as DBOOST.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Note that these functions (approximately) include both the identity function (used by [1]) and the sign function (used by [2]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Note that these functions (approximately) include both the identity function (used by [1]) and the sign function (used by [2]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "To this end, we perform experiments on a few benchmark datasets for learning with similarity (non-PSD) functions [5] as well as on a variety of standard UCI datasets where the similarity function used is the Gaussian kernel function.", "startOffset": 113, "endOffset": 116}, {"referenceID": 9, "context": "For our experiments, we implemented our methods FTUNE and FTUNE+D as well as BBS and BBS+D using MATLAB while using LIBLINEAR [10] for SVM classification.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "For DBOOST, we use the C++ code provided by the authors of [2].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "1 Similarity learning datasets First, we conduct experiments on a few similarity learning datasets [5]; these datasets provide a (non-PSD) similarity matrix along with class labels.", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "Similar to [2], we use the Gaussian kernel function as the similarity function for evaluating our method.", "startOffset": 11, "endOffset": 14}], "year": 2013, "abstractText": "We consider the problem of classification using similarity/distance functions over data. Specifically, we propose a framework for defining the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions. Our framework unifies and generalizes the frameworks proposed by [1] and [2]. An attractive feature of our framework is its adaptability to data we do not promote a fixed notion of goodness but rather let data dictate it. We show, by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems. We propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria. We then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection. We demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark UCI datasets on which our method consistently outperforms existing approaches by a significant margin.", "creator": "LaTeX with hyperref package"}}}