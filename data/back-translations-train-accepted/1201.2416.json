{"id": "1201.2416", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2012", "title": "Stochastic Low-Rank Kernel Learning for Regression", "abstract": "We present a novel approach to learn a kernel-based regression function. It is based on the useof conical combinations of data-based parameterized kernels and on a new stochastic convex optimization procedure of which we establish convergence guarantees. The overall learning procedure has the nice properties that a) the learned conical combination is automatically designed to perform the regression task at hand and b) the updates implicated by the optimization procedure are quite inexpensive. In order to shed light on the appositeness of our learning strategy, we present empirical results from experiments conducted on various benchmark datasets.", "histories": [["v1", "Wed, 11 Jan 2012 21:03:55 GMT  (306kb,D)", "http://arxiv.org/abs/1201.2416v1", "International Conference on Machine Learning (ICML'11), Bellevue (Washington) : United States (2011)"]], "COMMENTS": "International Conference on Machine Learning (ICML'11), Bellevue (Washington) : United States (2011)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pierre machart", "thomas peel", "sandrine anthoine", "liva ralaivola", "herv\u00e9 glotin"], "accepted": true, "id": "1201.2416"}, "pdf": {"name": "1201.2416.pdf", "metadata": {"source": "META", "title": "Stochastic Low-Rank Kernel Learning for Regression", "authors": ["Pierre Machart", "Thomas Peel", "Sandrine Anthoine", "Liva Ralaivola", "Herv\u00e9 Glotin"], "emails": ["PIERRE.MACHART@LIF.UNIV-MRS.FR", "THOMAS.PEEL@LIF.UNIV-MRS.FR", "ANTHOINE@CMI.UNIV-MRS.FR", "LIVA.RALAIVOLA@LIF.UNIV-MRS.FR", "GLOTIN@UNIV-TLN.FR"], "sections": [{"heading": "1. Introduction", "text": "Our goal is to learn a kernel-based regression function and at the same time address two problems commonly encountered with kernel methods: working with a kernel that is tailored to the task at hand, and dealing efficiently with problems whose size prevents the gram matrix from being stored in memory. Although current work focuses on regression, the material presented here might as well be applicable to classifications. Compared to similar methods, we present two innovations. First, we build conical combinations of rank-1 appearing in Proceedings of the 28th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author (s) / owner (s).Nystro-m approximations, whose weights are chosen to fulfill the regression task - this makes our approach different from (Kumar et al., 2009) and (Suykens et al., 2002), which focus on approaching the full matrix."}, {"heading": "2. Proposed Model", "text": "The notation X is the input space, k: X \u00b7 X \u2192 R denotes the (positive) core function that we have available, and \u03c6: X \u2192 H refers to the figure \u03c6 (x): = k (x, \u00b7) of X to the reproducing kernel Hilbert space H. This results in ar Xiv: 120 1.24 16v1 [cs.LG] k (x, x \u2032) = < \u03c6 (x \u2032) >, with < \u00b7, \u00b7 > the internal product of H.The training set is L: = {(xi, yi)} ni = 1 (X \u00d7 R) n, where yi is the target value associated with xi. K = (k (xi, xj) 1 \u2264 i, j \u2264 n \u0445Rn \u00d7 n is the gram matrix of k in relation to L. For m = 1,....."}, {"heading": "2.1. Data-parameterized Kernels", "text": "For m = 1,.., n, p = m: X \u2192 H = > m, the figure is: p = m (x): = < p (x), p (xm) > k (xm, xm) p (xm) p = xm. (1) It immediately follows that k \u00b2 m defined as, x \u00b2 X, k \u00b2 m (x, x \u00b2) is actually a positive nucleus. Therefore, these parameterized nuclei k \u00b2 m lead to a family (K \u00b2 m) p = m \u2264 n of gram matrices of the following form: k \u00b2 m (xm, xm) k \u00b2 m = (k \u00b2 m) p = (x, xj) p = k \u00b2 m of the nuclei."}, {"heading": "2.2. Kernel Ridge Regression", "text": "The kernel ridge regression (KRR) is the kernel version of the popular comb regression (Hoerl & Kennard, 1970). The associated optimization problem is: min w {\u03bb 2 + n \u2211 i = 1 (yi \u2212 < w, \u03c6 (xi) >) 2}, (6) where \u03bb > 0 is a regularization parameter. Using I for the identity matrix, the following dual formulation can be considered: max \u0445 Rn {FKRR (\u03b1): = yT\u03b1 \u2212 1 4\u03bb \u03b1T (\u03bbI + K)}. (7) The solution of the concave problem (7) and the optimal solution of (6) are through the equation of the comb regression (1) \u03b1T (\u03bbI + K). (7) The solution of the concave problem (7) and the optimal solution of (1) are through the equation of the comb regression (1) \u03b1T (\u03bbI + K)."}, {"heading": "2.3. A Convex Optimization Problem", "text": "KRR can be solved by solving the linear system (I + K). In order to solve this possible problem, we work with K (3) instead of the gram matrix K. As we will see, this not only enables the avoidance of memory problems, but also allows us to set up a learning problem in which both \u00b5 and a regression function are searched simultaneously, which is very similar to the Multiple Kernel Learning Paradigm (Rakotomamonjy et al., 2008). To set up the optimization problem in which we are interested, we proceed in a similar way as (Rakotomamonjy et al., 2008). For m = 1,."}, {"heading": "3. Solving the problem", "text": "We are now introducing a new stochastic optimization method to solve (14). It implements a coordinate descent strategy with step variables that use second-order information."}, {"heading": "3.1. A Second-Order Stochastic Coordinate Descent", "text": "Problem (14) is a limited minimization based on the differentiable and convex objective function F. Common methods of convex optimization (such as the projected gradient descent, proximal methods) can be applied to solve this problem, but they can be too computationally expensive when n is very large, largely due to a suboptimal utilization of the parameterization of the problem. Instead, the optimization strategy we propose is specifically tailored to take advantage of the parameterization of K (\u00b5). Algorithm 1 represents our stochastic descent method inspired by (Nesterov, 2010). At each iteration, a randomly chosen coordinate of the \u00b5 is updated via a newton step. This method has two essential features: i) using coordinate-wise updates of the \u00b5 includes only partial derivatives that can be easily calculated, and ii) the stochastic approach ensures a reduced memory coordination."}, {"heading": "3.2. Iterative Updates", "text": "It can be noted that the calculations of the derivatives (20), as well as the calculation of \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = > \"Cold Computation\" = = > Gold \"= =\" Cold Computation \"= =\" Cold Computation \"= > Gold\" = \"Computation of the cij's\" \"s\" \",\" We are the Computation of the Cij's \"s,\" \"\" We are the Size of C and D may. \"(21) and using the Woodbury formula (Theorem 2, Appendix), we have\" Calputation \"K \u2212."}, {"heading": "4. Analysis", "text": "At this point, we discuss the relationship between \u03bb and \u03bd, arguing that there is no need to maintain both hyperparameters. Furthermore, we provide a brief analysis of the runtime complexity of our learning procedure."}, {"heading": "4.1. Pivotal Hyperparameter \u03bb\u03bd", "text": "First of all, let us remember that we are interested in minimizing the problem of limited optimization (14), i.e., for clarity's sake, we purposely show dependence on objective function (14) and on the constant of objective function (14), i.e.: dependence on objective function (14), objective function (14), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (14), objective function (16), objective function (16), objective function (16), objective function (19), objective function (19), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (23), objective function (14), objective function (14), objective function (14), objective function (16), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), 14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), objective function (14), 14), objective function (14), objective function (14), objective function (14), objective function (14),"}, {"heading": "As a direct consequence:", "text": "Suppose we know that the definition (29) of problem (28) is the non-negativity of the components of problem (28), and it immediately follows that \"problem\" (44) is a minimization of \"problem\" (28), to show that \"problem\" (28), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"problem\" (44), \"(44),\" problem \"(44),\" problem \"(44) (44)."}, {"heading": "4.2. Runtime Complexity and Memory Usage", "text": "For the present analysis, we assume that we pre-calculate the M (randomly) selected columns c1,.., cM. If a is the cost of calculating a columncm, the pre-calculation has costs of O (Ma) and has memory usage of O (nM). For each iteration, we must calculate the first and second derivatives of the objective function, as well as their value and weight vector \u03b1. If we use (22), (20), (14) and (15), we can show that these operations have a complexity of O (nm0) when m0 is the zero standard of \u00b5.In addition to C, we must store G for storage costs of O (m20). Overall, if we specify the number of iterations in k, we can predetermine the algorithm with storage costs of O (nM + m20) and a complexity of O (knm0 + Ma).If a memory is a critical case, we can predetermine a column of C (and one of C)."}, {"heading": "5. Numerical Simulations", "text": "We now present the results of various numerical experiments, for which we describe the data sets and the protocol used. We examine the influence of the various parameters of our learning approach on the results and compare the performance of our algorithm with that of related methods."}, {"heading": "5.1. Setup", "text": "First, we use a toy data set (referred to as sinc) to better understand the role and influence of the parameters, which consists in regressing the cardinal sine of the two-standard (i.e. x 7 \u2192 sin (\u0435x) / \u043ex) random two-dimensional points, each uniform between \u2212 5 and + 5. To have a better idea of how the solutions may or may not correspond to the training data, we add a white Gaussian noise to the target variable of the randomly selected 1000 training points (with a signal-to-noise ratio of 10 dB). The test set consists of 1000 non-noise independent instance / target pairs. We then evaluate our method using two UCI data sets: Abalone (Abalone) and Boston Housing (Boston), using the same normalization (ISM data sets) and data partition as in (Smola & Scho Stulkopf, National 2000) and Boston Housing (Boston)."}, {"heading": "5.2. Influence of the parameters", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1. EVOLUTION OF THE OBJECTIVE", "text": "Figure 1 records the evolution of the objective function on the sinc dataset. We observe that the evolution of the objective function between the different gradients is impressively similar, which empirically tends to claim that it is relevant to look for theoretical results on the convergence rate. One question that remains for future work is the effect of the random selection of the column sequence S on the solution achieved. 5.2.2. ZERO-NORM OF. As shown in Section 4.2, both memory usage and the complexity of the algorithm depend on m0. Therefore, it is interesting to take a closer look at how this set develops. Figure 2 and 3 point experimentally to two things: Firstly, the number of active components 0 of the active component M. sm0 = substantially smaller than the active component m0."}, {"heading": "5.3. Comparison to other methods", "text": "This section aims to give an indication of how our method performs on regression tasks by comparing the Mean Square Error (using the test set). In addition to our Stochastic Low-Rank Kernel Learning (SLKL) method, we solve the problem with the Standard Kernel Ridge Regression method, using the n training data (KRRN) and only M training data (KRRM). We also evaluate the performance of the KRR method by using the kernel selected for SLKL (Unif) with uniform weights according to the M Rank-1 approach. The results are presented in Table 2, where the bold font indicates the best low-rank method (KRRM, Unif or SLKL) for each experiment. Table 2 confirms that optimizing the weight vector for SLKL is crucial, as our results are drastically better than those of Unif. As long as M < n exceeds our test set, our method exceeds the RM's explanation of the fact that RM is likely."}, {"heading": "5.4. Large-scale dataset", "text": "To assess the scalability of our method, we conducted experiments with the larger handwritten digits of the MNIST dataset, whose training set consists of 60,000 examples. We used a Gaussian kernel calculated using histograms of oriented gradients such as in (Maji & Malik, 2009) in a \"one-on-all\" setting. ForM = 1000 yielded classification error rates of about 2% over the test set, which, while not competing with current state-of-the-art results, achieve reasonable performance, considering that we use only a small portion of the data (cf. the size of M) and that our method was designed for regression. Although our method overcomes memory usage problems for such large problems, it is still computationally intensive. In fact, a large number of iterations are spent selecting coordinates whose associated weight remains at 0. Although these iterations do not cause an update, they do not require the compression of the column function (e.g., as in 2008, as in the column x) as well as the compression of the corresponding of the matrix."}, {"heading": "6. Conclusion", "text": "The main features of our contribution are the use of a conical combination of data-based cores and the derivation of a stochastic convex optimization method that coordinates and uses second-order information. We offer theoretical convergence guarantees for this optimization method, we present the behavior of our learning method and illustrate its effectiveness through a number of numerical experiments conducted on several benchmark datasets. Of course, the present work raises several questions, among which we can determine the precise convergence rate for the stochastic optimization method and the generalization of our approach to the use of multiple cores. The establishment of data-dependent generalization limits using either the one-standard restriction to \u00b5 or the size M of the kernel combination is of primary importance to us. The link between the one-standard hyperparameter and the ridge-rameter plan mentioned in Section 4 appears, although we may not have any connections between these procedures based on the rocket-1 formalization."}, {"heading": "Acknowledgments", "text": "This work is partly supported by the European Community's IST programme under FP7 Pascal 2 Network of Excellence (ICT-216886-NOE) and the ANR project LAMPADA (ANR-09-EMER-007)."}, {"heading": "A. Matrix Inversion Formulas", "text": "Theorem 2. (Woodbury Matrix Inversion Formula (Woodbury, 1950) Let n and m be positive integers, A-Rn \u00b7 n and C-Rm \u00b7 m be non-singular matrices and let U-Rn \u00b7 m and V-Rm \u00b7 n be two matrices. If C-1 + V A-1U is non-singular, then A + UCV and: (A + UCV) \u2212 1 = A-1 \u2212 A \u2212 1U (C-1 + V A \u2212 1U) \u2212 1V A \u2212 1.Theorem 3. (Matrix inversion with additional column) Given m, integer and M-R (n + 1) \u00b7 (n + 1) divided as: M = (A bb > c), where A-Rn \u00b7 n, b-Rn and c-R."}, {"heading": "If A is non-singular and c\u2212 b>A\u22121b 6= 0, then M is nonsingular and the inverse of M is given by", "text": "M \u2212 1 = (A \u2212 1 + 1kA \u2212 1bb > A \u2212 1 \u2212 1kA \u2212 1b \u2212 1kb > A \u2212 1 1k), (30) where k = c \u2212 b > A \u2212 1b."}], "references": [{"title": "On the nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney", "year": 2005}, {"title": "Pattern Classification and Scene Analysis", "author": ["Duda", "Richard O", "Hart", "Peter E"], "venue": null, "citeRegEx": "Duda et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Duda et al\\.", "year": 1973}, {"title": "A dual coordinate descent method for largescale linear svm", "author": ["Hsieh", "C.-J", "Chang", "K.-W", "Lin", "Keerthi", "S. Sathiya", "S. Sundararajan"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Ensemble nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Fast and accurate digit classification", "author": ["S. Maji", "J. Malik"], "venue": "Technical report, EECS Department, UC Berkeley,", "citeRegEx": "Maji and Malik,? \\Q2009\\E", "shortCiteRegEx": "Maji and Malik", "year": 2009}, {"title": "Efficiency of coordinate descent methods on hugescale optimization problems", "author": ["Y. Nesterov"], "venue": "Core discussion papers,", "citeRegEx": "Nesterov,? \\Q2010\\E", "shortCiteRegEx": "Nesterov", "year": 2010}, {"title": "Input space versus feature space in kernel-based methods", "author": ["B. Sch\u00f6lkopf", "S. Mika", "C.J.C. Burges", "P. Knirsch", "K.R. M\u00fcller", "G. R\u00e4tsch", "A.J. Smola"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Sparse greedy matrix approximation for machine learning", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Smola and Sch\u00f6lkopf,? \\Q2000\\E", "shortCiteRegEx": "Smola and Sch\u00f6lkopf", "year": 2000}, {"title": "Relationship of several variational methods for the approximate solution of ill-posed problems", "author": ["V.V. Vasin"], "venue": "Mathematical Notes,", "citeRegEx": "Vasin,? \\Q1970\\E", "shortCiteRegEx": "Vasin", "year": 1970}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Seeger,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "Inverting modified matrices", "author": ["M.A. Woodbury"], "venue": "Technical report,", "citeRegEx": "Woodbury,? \\Q1950\\E", "shortCiteRegEx": "Woodbury", "year": 1950}], "referenceMentions": [{"referenceID": 3, "context": "Nystr\u00f6m approximations, whose weights are chosen so as to serve the regression task \u2013 this makes our approach different from (Kumar et al., 2009) and (Suykens et al.", "startOffset": 125, "endOffset": 145}, {"referenceID": 5, "context": "Secondly, to solve the convex optimization problem entailed by our modeling choice, we provide an original stochastic optimization procedure based on (Nesterov, 2010).", "startOffset": 150, "endOffset": 166}, {"referenceID": 3, "context": "As studied in (Kumar et al., 2009), it is sensible to consider convex combinations of the K\u0303m if they are of very low rank.", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "all the columns are picked) and we have uniform weights \u03bc, then K\u0303(\u03bc) = KK>, which is a matrix encountered when working with the so-called empirical kernel map (Sch\u00f6lkopf et al., 1999).", "startOffset": 160, "endOffset": 184}, {"referenceID": 8, "context": "Finally, using the equivalence between Tikhonov and Ivanov regularization methods (Vasin, 1970), we obtain the convex and smooth optimization problem we focus on:", "startOffset": 82, "endOffset": 95}, {"referenceID": 5, "context": "Algorithm 1 depicts our stochastic descent method, inspired by (Nesterov, 2010).", "startOffset": 63, "endOffset": 79}, {"referenceID": 5, "context": "Notice that the Stochastic Coordinate Newton Descent (SCND) is similar to the algorithm proposed in (Nesterov, 2010), except that we replace the Lipschitz constants by the second-order partial derivatives \u2202 F (\u03bc) \u2202\u03bc2mk .", "startOffset": 100, "endOffset": 116}, {"referenceID": 2, "context": "techniques such as shrinkage (Hsieh et al., 2008).", "startOffset": 29, "endOffset": 49}, {"referenceID": 10, "context": "(Woodbury matrix inversion formula (Woodbury, 1950)) Let n and m be positive integers, A \u2208 Rn\u00d7n and C \u2208 Rm\u00d7m be non-singular matrices and let U \u2208 Rn\u00d7m and V \u2208 Rm\u00d7n be two matrices.", "startOffset": 35, "endOffset": 51}], "year": 2012, "abstractText": "We present a novel approach to learn a kernelbased regression function. It is based on the use of conical combinations of data-based parameterized kernels and on a new stochastic convex optimization procedure of which we establish convergence guarantees. The overall learning procedure has the nice properties that a) the learned conical combination is automatically designed to perform the regression task at hand and b) the updates implicated by the optimization procedure are quite inexpensive. In order to shed light on the appositeness of our learning strategy, we present empirical results from experiments conducted on various benchmark datasets.", "creator": "LaTeX with hyperref package"}}}