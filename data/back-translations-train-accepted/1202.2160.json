{"id": "1202.2160", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2012", "title": "Scene parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers", "abstract": "Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image.", "histories": [["v1", "Fri, 10 Feb 2012 00:30:48 GMT  (1777kb,D)", "http://arxiv.org/abs/1202.2160v1", "9 pages, 4 figures"], ["v2", "Fri, 13 Jul 2012 21:32:24 GMT  (1777kb,D)", "http://arxiv.org/abs/1202.2160v2", "9 pages, 4 figures - Published in 29th International Conference on Machine Learning (ICML 2012), Jun 2012, Edinburgh, United Kingdom"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["cl\u00e9ment farabet", "camille couprie", "laurent najman", "yann lecun"], "accepted": true, "id": "1202.2160"}, "pdf": {"name": "1202.2160.pdf", "metadata": {"source": "CRF", "title": "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers", "authors": ["Cl\u00e9ment Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "emails": [], "sections": [{"heading": null, "text": "Scene parsing, or semantic segmentation, is to label each pixel in an image with the category of the object to which it belongs. It is a challenging task that involves simultaneously detecting, segmenting, and detecting all objects in the image. Here, the Scene Parsing method begins by calculating a segment tree from a diagram of pixel inequalities, while at the same time computing a series of dense feature vectors that encode regions with multiple sizes centered on each pixel. Feature Extractor is a multi-dimensional Convolutionary Network formed from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier that produces an estimate of the distribution of the object categories contained in the segment. A subset of tree nodes covering the image are then selected to \"average the class.\""}, {"heading": "1. Overview", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2. Related work", "text": "The problem of scenic analysis has been addressed with a variety of methods in recent years."}, {"heading": "3. An end-to-end trainable model for scene", "text": "In the first representation, the image is considered as a point in a high-dimensional space, and we try to find a transformation f: RP \u2192 RQ that maps these images into a space in which a label can be assigned to each pixel using a simple linear classifier. This first representation typically suffers from two main problems: (1) the window under consideration rarely contains an object that is properly centered and scaled, and thus provides a poor basis for observation in order to predict the class of the underlying object; (2) the integration of a large context involves increasing the grid size and thus the dimensionality P of the input; given a limited amount of training data, it is then necessary to enforce a certain inventory in the function f itself. This is usually achieved by using sampling / subsampling layers, which impair the model's ability to accurately locate and delimit objects in the hierarchy."}, {"heading": "3.1. Scale-invariant, scene-level feature extraction", "text": "Our function is based on a revolutionary network, not simply on a patch. In this problem, a typical network of 25,000 people is created, in which weights are replicated across space or, in other words, linear transformation is accomplished using 2D constellations. In fact, each layer of a revolutionary network is explicitly justified by the fact that image statistics are stationary and features and combinations of features relevant in one region of an image are also relevant in other regions. Indeed, each layer of a revolutionary network is explicitly forced to model features that are shiftable. Due to the imposed weight distribution, revolutionary networks are successfully used for a number of image caption problems. More holistic tasks, such as complete understanding of the scene (pixel-wise labeling) require the system to model complex interactions on the scale of complete images, not simply within a patch."}, {"heading": "3.2. Parameter-free hierarchical parsing", "text": "Predicting the class of a given pixel based on its own characteristic vector is difficult and is not sufficient in practice. It is easier when we consider a spatial grouping of characteristic vectors around the pixel, i.e. a neighborhood. Of all possible neighborhoods, one is best suited to predict the class of the pixel. In Section 3.2.1, we propose to formulate the search for the most appropriate neighborhood as an optimization problem, and then describe the construction of the minimized cost function in Section 3.2.2."}, {"heading": "3.2.1 Optimal purity cover", "text": "We define the neighborhood of a pixel as a contiguous component containing that pixel. Let's let Ck, \u0441\u0430k \u0442 {1,.., K} be the set of all possible contiguous components of the grid defined in Figure I. Let's let's let Sk be the cost associated with each of these components. For each pixel i we want to find the index k \u0445 (i) of the component that best explains that pixel, i.e. the component with the minimum cost Sk \u0445 (i): k \u0445 (i) = argmin k | i \u0441Ck Sk (3). Let's note that the components Ck \u0445 (i) are non-disjunctural sentences that form a cover of the grid. Also, note that the total cost S \u0445 = \u0445 i Sk \u0445 (i) are minimal. In practice, the set of components Ck is too large, and only a subset of them can be taken into account. A classical technique for reducing the component set is to represent a simple group of components, by looking through a hierarchy of 18 that leads to a T. [1]"}, {"heading": "3.2.2 Producing the confidence costs", "text": "Given that cost is considered to be the entropy of the distribution of classes in the component, we need to define a function that predicts these costs simply by looking at the component. We will now describe a way to achieve this, as shown in Figure 3.0. We define a compact representation of objects as an elastic spatial arrangement of such properties. In other words, an object or category in general can be described as a spatial arrangement of features. A simple attention function is used to describe the function of objects as an elastic spatial arrangement of such features."}, {"heading": "4. Training procedure", "text": "The training of the model described in Section 3 can be done in two steps. First, we train the low-level feature Extractor fs in complete independence from the rest of the model. The aim of this first step is to generate characteristics (F) F-F that are as discriminatory as possible for pixel-by-pixel classification. Next, we construct the hierarchies (T) T-T over the entire training set, and for all T-T we train the classifier c to predict the distribution of classes into component Ck-T as well as the cost Sk. Once this second part is complete, all functions in Figure 1 are defined and conclusions can be drawn from any images. In the next two sections, we describe these two steps."}, {"heading": "4.1. Learning discriminative scale-invariant features", "text": "As described in Section 3.1, feature vectors in F are obtained by concatenating the outputs of multiple networks f, each taking a different image in a multi-level pyramid as input. Ideally, a linear classifier should generate the correct categorization for all pixel positions i, starting from the feature vectors fi. We train the parameters to achieve this goal. Let's ci be the true target vector for pixels i and c, which should be the normalized prediction from the linear classifier, let's specify: Lcat = \"target\" i \"i\" i \"i\" ixelslcat (c \"i\" ci \"), (7) lcat (c\" i \"i,\" a), (8) c \"i\" i \"i,\" a \"ew\" a \"fi\" b \"classes e wTb\" Fi \"ixelslcat\" (c \"i\"), ci \"(7) cat (c\" i \"i\", \"a\" ci."}, {"heading": "4.2. Teaching a classifier to find its best observation", "text": "LevelConsidering the trained parameters phenomena, we build F and T, i.e. we calculate all vector maps F and hierarchies T on all available training data to generate a new training set of descriptors Ok. This time, the parameters phenc of the classifier c are trained to minimize the KL divergence between the true (known) distribution of labels dk in each component and the prediction from the classifier d-k (Eq 5): ldiv = a-classes dk, aln (dk, a d-k, a). (10) In this setting, the basic truth distributions dk are not hard target vectors, but normalized histograms of the labels contained in component Ck. Once the parameters phenc are trained, d-k predicts exactly the distribution of labels, and Eq 6 can be used to assign a purity to the component."}, {"heading": "5. Experiments", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "6. Discussion", "text": "Our model does not rely on constructed features and uses a multi-scale convolutionary network based on raw pixels to learn how to identify objects at the low and middle level. Convolutionary network is trained in supervised mode to produce captions directly. Unlike many other scene analysis systems, which rely on expensive graphical models to ensure consistent captions, our system relies on a segmentation tree in which the nodes (which correspond to the image segments) are labeled with the entropy of the distribution of the classes contained in the corresponding segment. Instead of diagram cutouts or other inference methods, we use the new concept of optimal coverage to extract the most consistent segmentation from the tree. The complexity of each operation is linear in the number of pixels, with the exception of the creation of the tree, which is quasi linear (which in practice means multiplying cheaply). The system produces highly structured modeling on the background, with high precision."}], "references": [{"title": "Contour Detection and Hierarchical Image Segmentation", "author": ["P. Arbel\u00e1ez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Interactive graph cuts for optimal boundary & region segmentation of objects in n-d images", "author": ["Y. Boykov", "M.P. Jolly"], "venue": "In Proceedings of International Conference of Computer Vision (ICCV),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Power Watersheds: A Unifying Graph Based Optimization Framework", "author": ["C. Couprie", "L. Grady", "L. Najman", "H. Talbot"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Incremental algorithm for hierarchical minimum spanning forests and saliency of watershed cuts", "author": ["J. Cousty", "L. Najman"], "venue": "In 10th International Symposium on Mathematical Morphology (ISMM\u201911),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "A simple algorithm for finding maximal network flows and an application to the hitchcock problem", "author": ["L.R. Ford", "D.R. Fulkerson"], "venue": "Technical report, RAND Corporation, Santa Monica,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1955}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Deep Convolutional Networks for Scene Parsing", "author": ["D. Grangier", "L. Bottou", "R. Collobert"], "venue": "In ICML 2009 Deep Learning Workshop,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Scale-sets image analysis", "author": ["L. Guigues", "J.P. Cocquerez", "H.L. Men"], "venue": "International Journal of Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Learning hybrid models for image annotation with partially labeled data", "author": ["X. He", "R. Zemel"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Supervised learning of image restoration with convolutional networks", "author": ["V. Jain", "J.F. Murray", "F. Roth", "S. Turaga", "V. Zhigulin", "K. Briggman", "M. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "In ICCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "In Proc. International Conference on Computer Vision (ICCV\u201909)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "On the shortest spanning subtree of a graph and the traveling salesman problem", "author": ["J.B. Kruskal"], "venue": "Proceedings of the American Mathematical Society,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1956}, {"title": "Efficiently selecting regions for scene understanding", "author": ["M. Kumar", "D. Koller"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Nonparametric scene parsing: Label transfer via dense scene alignment", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Segmentation, minimum spanning tree and hierarchies", "author": ["F. Meyer", "L. Najman"], "venue": "Mathematical Morphology: from theory to application,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Stacked hierarchical labeling", "author": ["D. Munoz", "J. Bagnell", "M. Hebert"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Geodesic saliency of watershed contours and hierarchical segmentation", "author": ["L. Najman", "M. Schmitt"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "Toward automatic phenotyping of developing embryos from videos", "author": ["F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P. Barbano"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Synergistic face detection and pose estimation with energy-based models", "author": ["M. Osadchy", "Y. LeCun", "M. Miller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Object recognition by scene alignment", "author": ["B. Russell", "A. Torralba", "C. Liu", "R. Fergus", "W. Freeman"], "venue": "In Neural Advances in Neural Information,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Associative hierarchical CRFs for object class image segmentation", "author": ["C. Russell", "P.H.S. Torr", "P. Kohli"], "venue": "In Proc. ICCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "C.C. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "In Proceedings of the 26th International Confer-  ence on Machine Learning (ICML),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Superparsing: scalable nonparametric image parsing with superpixels", "author": ["J. Tighe", "S. Lazebnik"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Maximin affinity learning of image segmentation", "author": ["S. Turaga", "K. Briggman", "M. Helmstaedter", "W. Denk", "H. Seung"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}], "referenceMentions": [{"referenceID": 8, "context": "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].", "startOffset": 137, "endOffset": 159}, {"referenceID": 21, "context": "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].", "startOffset": 137, "endOffset": 159}, {"referenceID": 5, "context": "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].", "startOffset": 137, "endOffset": 159}, {"referenceID": 12, "context": "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].", "startOffset": 137, "endOffset": 159}, {"referenceID": 16, "context": "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].", "startOffset": 137, "endOffset": 159}, {"referenceID": 23, "context": "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].", "startOffset": 137, "endOffset": 159}, {"referenceID": 5, "context": "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.", "startOffset": 86, "endOffset": 101}, {"referenceID": 12, "context": "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.", "startOffset": 86, "endOffset": 101}, {"referenceID": 16, "context": "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.", "startOffset": 86, "endOffset": 101}, {"referenceID": 23, "context": "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.", "startOffset": 86, "endOffset": 101}, {"referenceID": 22, "context": "[23] propose a method to aggregate segments in a greedy fashion using a trained scoring function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] proposed to use the histogram of labels extracted from a coarse scale as input to the labeler that look at finer scales.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Like us, a number of authors have used trees to generate candidate segments by aggregating elementary segments, as in [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "Our system extracts features densely from a multiscale pyramid of images using a convolutional network (ConvNet) [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].", "startOffset": 76, "endOffset": 84}, {"referenceID": 18, "context": "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].", "startOffset": 186, "endOffset": 198}, {"referenceID": 9, "context": "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].", "startOffset": 186, "endOffset": 198}, {"referenceID": 24, "context": "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].", "startOffset": 186, "endOffset": 198}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Unlike [7] however, our system uses a boundary-based over-segmentation to align the labels produced by the ConvNet to the boundaries in the image.", "startOffset": 7, "endOffset": 10}, {"referenceID": 17, "context": "A classical technique to reduce the set of components is to consider a hierarchy of segmentations [18, 1, 8], that can be represented as a tree T .", "startOffset": 98, "endOffset": 108}, {"referenceID": 0, "context": "A classical technique to reduce the set of components is to consider a hierarchy of segmentations [18, 1, 8], that can be represented as a tree T .", "startOffset": 98, "endOffset": 108}, {"referenceID": 7, "context": "A classical technique to reduce the set of components is to consider a hierarchy of segmentations [18, 1, 8], that can be represented as a tree T .", "startOffset": 98, "endOffset": 108}, {"referenceID": 0, "context": "Once we have the set of object descriptors Ok, we define a function c : Ok \u2192 [0, 1]c (where Nc is the number of classes) as predicting the distribution of classes present in component Ck.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "(1) The Stanford Background dataset, introduced in [6] for evaluating methods for semantic scene understanding.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "We use the evaluation procedure introduced in [6], 5-fold cross validation: 572 images used for training, and 142 for testing.", "startOffset": 46, "endOffset": 49}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] have split this dataset into 2, 488 training images and 200 test images and used synonym correction to obtain 33 semantic labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], is derived from the LabelMe subset used in [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[24], is derived from the LabelMe subset used in [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Synonyms were manually consolidated by [24] to produce 170 unique labels.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "In this paper, the hierarchy used to find the optimal cover is a simple hierarchy constructed on the raw image gradient, based on a standard volume criterion [16, 4], completed by a removal of non-informative small components (less than 100 pixels).", "startOffset": 158, "endOffset": 165}, {"referenceID": 3, "context": "In this paper, the hierarchy used to find the optimal cover is a simple hierarchy constructed on the raw image gradient, based on a standard volume criterion [16, 4], completed by a removal of non-informative small components (less than 100 pixels).", "startOffset": 158, "endOffset": 165}, {"referenceID": 4, "context": "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.", "startOffset": 82, "endOffset": 88}, {"referenceID": 1, "context": "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.", "startOffset": 82, "endOffset": 88}, {"referenceID": 11, "context": "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "[6] 76.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] 76.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] 77.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] 78.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] 79.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Performance of our system on the Stanford Background dataset [6]: per-pixel accuracy / average per-class accuracy.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "[15] 74.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] 76.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Performance of our system on the SIFT Flow dataset [15]: per-pixel accuracy / average per-class accuracy.", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "[24] 66.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Performance of our system on the Barcelona dataset [24]: per-pixel accuracy / average per-class accuracy.", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "\u2019s Maximin Learning [25] to learn low-level feature vectors from which better segmentation trees can be produced.", "startOffset": 20, "endOffset": 24}], "year": 2012, "abstractText": "Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image. The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average \u201cpurity\u201d of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 \u00d7 240 image labeling in less than 1 second. 1. Overview Full scene labeling (FSL) is the task of labeling each pixel in a scene with the category of the object to which it belongs. FSL requires to solve the detection, segmentation, recognition and contextual integration problems simultaneously, so as to produce a globally consistent labeling. One of the obstacles to FSL is that the information necessary for the labeling of a given pixel may come from very distant pixels as well as their labels. The category of a pixel may depend on relatively short-range information (e.g. the presence of a human face generally indicates the presence of a human body nearby), as well as on very long-range dependencies (is this grey pixel part of a road, a building, or a cloud?). This paper proposes a new method for FSL, depicted on Figure 1 that relies on five main ingredients: 1) Trainable, dense, multi-scale feature extraction: a multi-scale, dense feature extractor produces a series of feature vectors for regions of multiple sizes centered around every pixel in the image, covering a large context. The feature extractor is a two-stage convolutional network applied to a multi-scale contrast-normalized laplacian pyramid computed from the image. The convolutional network is fed with raw pixels and trained end to end, thereby alleviating the need for hand-engineered features; 2) Segmentation Tree: A graph over pixels is computed in which each pixel is connected to its 4 nearest neighbors through an edge whose weight is a measure of dissimilarity between the colors of the two pixels. A segmentation tree is then constructed using a classical region merging method, based on the minimum spanning tree of the graph. Each node in the tree corresponds to a potential image segment. The final image segmentation will be a judiciously chosen subset of nodes of the tree whose corresponding regions cover the entire image. 3) Region1 ar X iv :1 20 2. 21 60 v1 [ cs .C V ] 1 0 Fe b 20 12 wise feature aggregation: for each node in the tree, the corresponding image segment is encoded by a 5\u00d7 5 spatial grid of aggregated feature vectors. The aggregated feature vector of each grid cell is computed by a component-wise max pooling of the feature vectors centered on all the pixels that fall into the grid cell; This produces a scale-invariant representation of the segment and its surrounding; 4) Class histogram estimation: a classifier is then applied to the aggregated feature grid of each node. The classifier is trained to estimate the histogram of all object categories present in its input segments; 5) Optimal purity cover: a subset of tree nodes is selected whose corresponding segments cover the entire image. The nodes are selected so as to minimize the average \u201cimpurity\u201d of the class distribution. The class \u201cimpurity\u201d is defined as the entropy of the class distribution. The choice of the cover thus attempts to find a consistent overall segmentation in which each segment contains pixels belonging to only one of the learned categories. All the steps in the process have a complexity linear (or almost linear) in the number of pixels. The bulk of the computation resides in the convolutional network feature extractor. The resulting system is very fast, producing a full parse of a 320 \u00d7 240 image in less than 1 second on a conventional CPU. Once trained, the system is parameter free, and requires no adjustment of thresholds or other knobs. There are three key contributions in this paper 1) using a multi-scale convolutional net to learn good features for region classification; 2) using a class purity criterion to decide if a segment contains a single objet, as opposed to several objects, or part of an object; 3) an efficient procedure to obtain a cover that optimizes the overall class purity of a segmentation.", "creator": "LaTeX with hyperref package"}}}