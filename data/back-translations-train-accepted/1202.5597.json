{"id": "1202.5597", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2012", "title": "Hybrid Batch Bayesian Optimization", "abstract": "Bayesian Optimization aims at optimizing an unknown non-convex/concave function that is costly to evaluate. We are interested in application scenarios where concurrent function evaluations are possible. Under such a setting, BO could choose to either sequentially evaluate the function, one input at a time and wait for the output of the function before making the next selection, or evaluate the function at a batch of multiple inputs at once. These two different settings are commonly referred to as the sequential and batch settings of Bayesian Optimization. In general, the sequential setting leads to better optimization performance as each function evaluation is selected with more information, whereas the batch setting has an advantage in terms of the total experimental time (the number of iterations). In this work, our goal is to combine the strength of both settings. Specifically, we systematically analyze Bayesian optimization using Gaussian process as the posterior estimator and provide a hybrid algorithm that, based on the current state, dynamically switches between a sequential policy and a batch policy with variable batch sizes. We provide theoretical justification for our algorithm and present experimental results on eight benchmark BO problems. The results show that our method achieves substantial speedup (up to %78) compared to a pure sequential policy, without suffering any significant performance loss.", "histories": [["v1", "Sat, 25 Feb 2012 02:00:51 GMT  (210kb,D)", "https://arxiv.org/abs/1202.5597v1", null], ["v2", "Wed, 29 Feb 2012 01:55:33 GMT  (211kb,D)", "http://arxiv.org/abs/1202.5597v2", null], ["v3", "Tue, 1 May 2012 03:08:22 GMT  (214kb,D)", "http://arxiv.org/abs/1202.5597v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["javad azimi", "ali jalali", "xiaoli zhang fern"], "accepted": true, "id": "1202.5597"}, "pdf": {"name": "1202.5597.pdf", "metadata": {"source": "CRF", "title": "Hybrid Batch Bayesian Optimization", "authors": ["Javad Azimi", "Ali Jalali", "Xiaoli Fern"], "emails": ["azimi@eecs.oregonstate.edu", "alij@utexas.edu", "xfern@eecs.oregonstate.edu"], "sections": [{"heading": "1 Introduction", "text": "erD rf\u00fc ide eeisrVnlrtee\u00fcgn rf\u00fc ide eeisrteeSn ni rde eeisrrVnlrtee\u00fcgn rf\u00fc ide eeirlrlteee\u00fcgn eaJrha nvo ende nlrrteeaeetnlrVnlrteeu ni rde eeirlrrsdteeVnlrrrrrteeeu in the eeisrdteeeeirsn eaJrha nvo nde nlrlrlrrrrf\u00fc, eerD \"i os os.\" rsE \"D\" iDe eerD nlrrsdddteeVnlrf\u00fc, \"eerD so os os os os,\" nlrf\u00fc os os os os os, rf\u00fc os os os os os os os os os, \"lrf\u00fc os os os os os, ros os, ros os, rf\u00fc sos os, rf\u00fc sos os os os os, ros os os, ros os os, rf\u00fc sf\u00fc sos os os os,\" ros os os os os, ros os os os, rf\u00fc sf\u00fc sos. \"ros os os os,\" ros os os, rf\u00fc sf\u00fc sdsas os os os os os os, \"ros os os os os,\" ros os os, rf\u00fc sf\u00fc sos os os os, \"ros os os os os os,\" ros os os os, \"ros,\" ros os os, \"ros os os,\" ros, \"ros,\" ros, \"eD\" eD."}, {"heading": "2 Gaussian Process", "text": "There are two essential components: a probability model for the unknown function, and a selection criterion for the selection of the next best experiment (s) based on the model. - We select GP as our probability model and EI [9] as our selection criterion. - We examine the properties of the GP in this section and move the analysis of the EI to the next section. - We use GP to form the rear values of our observation group O = (xO, yO), where xO = (x2, xn) is the set of input factors and yO = (y2, y2, y.) is the set of outcomes (of the experiment) so that yj = f (xj) and f () is the underlying unknown function. For a new input point xi, GP models of the unknown output yi = f (xi) as a normal random variable."}, {"heading": "3 Hybrid Batch Bayesian Optimization", "text": "In a sequential approach, we are only consulted for one experiment at a time when we use a selection criterion (max.), mainly because the selection criterion requires the output of the previous query to find the next best result. Suppose we have the ability to perform nb experiments in parallel, and we are limited by the total number of possible experiments nl. At each iteration, the question is whether we can query more than one sample to speed up the experimental prediction without losing performance compared to the sequential approach. We use the expected improvement (EI) as our basic sequential selection criterion. Below, we provide the formal definition for EI definition 2. EI [9] with the associated GP prediction y (\u00b5x | O, \u03c32x | O) is defined to beEI (x | O) = (\u2212 and)."}, {"heading": "5 Conclusion", "text": "In the Bayean optimization framework, we investigated the problem of batch query selection with the aim of maintaining the performance of a sequential policy that uses fewer iterations. Although our results serve general BO problems, for clarity we focused on the task of maximizing an unknown non-convex / concave function. In this paper, there are two main contributions: first, we introduce a systematic method of analyzing the performance and limitations of simulation-based batch BO methods by a) showing universal limits to the prediction errors caused by the simulation error (estimation of the result); and b) analyzing the selection of the second experiment when we have an estimate of the result of the first experiment. In all cases, we offer theoretical limits to the error by relating the simulation error to the prediction error of the next-best experiment. Second, based on the above analysis, we suggest an algorithm that optimally compares the current performance of each algorithm with the one that decides on whether the next algorithm is best."}, {"heading": "A Proof of Theorem 1", "text": "If we recall the notation introduced in the theorem statement, we have \u2206 (\u03c3z) = CA \u2212 1CT \u2212 [C k (z, x)] [A BTB k (x, x)] \u2212 1 [CTk (z, x)] = CA \u2212 1CT \u2212 [C k (z, x)] [A \u2212 1 + A \u2212 1BTDBA \u2212 1 \u2212 A \u2212 1BTD \u2212 DBA \u2212 1 D] [CTk (z, x)] = (CA \u2212 1BT \u2212 k (z, x)))) D (BA \u2212 1CT \u2212 k (z, x)) T. This is the proof of the theorem."}, {"heading": "B Proof of Theorem 2", "text": "By definition and block matrix inversion slemma, we have \u00b5z | O, x \u2212 \u00b5, x = k (z, {xO, x}) k ({xO, x}, {xO, x}) \u2212 1 [0y \u2212 y] = (k (z, x) \u2212 CA \u2212 1BT) D (y \u2212 y).For the second part, we have \u00b5z | O \u2212 \u00b5z | O, x = CA \u2212 1yO \u2212 [C k (z, x)] [A BTB k (x, x)] \u2212 1 [yO y] = CA \u2212 1yO \u2212 [C k (z, x)] [A \u2212 1 + A \u2212 1BTDBA \u2212 1 \u2212 A \u2212 1BTD \u2212 DBA \u2212 1 D] = (CA \u2212 1BT \u2212 k (z, x) D (BA \u2212 1BT \u2212 y) = (CA \u2212 1BT \u2212 k (z, x))) D (This is the proof of the location)."}, {"heading": "C Proof of Lemma 1", "text": "If we use theorem 2, we have the following: = max (ymax, y-1) \u2212 \u00b5 x-1, x-1 = max (ymax, y-1) \u2212 p (z, x-1) \u2212 k (z, x-1) \u2212 k (z, xO) \u2212 k (xO, x-1) \u2212 1k (xO, x-1) \u2212 p (y-1 \u2212 y-1) (y-1) = x-max (z, y-1) \u2212 max (ymax, y-1) \u2212 p (z, x-z, x-z) \u2212 z (z, x-z, z-z, z-z-z, z-z-z-z, z-z-z-z, z-z-z, z-z-z-z, z-z-z-z-z, z-z-z-z-z-z-z, z-z-z-z-z-z-z, z-z-z-z-z-z-z-z, z-z-z-z-z-z-z-z, z-z-z-z-z-z-z-z, z-z-z-z-z-z-z-z, z-z-z-z-z-z-z-z, z-z-z-z-z-z-z, z-z-z-z-z-z, z-z-z-z-z-z-z, z-z-z-z-z-z, z-z-z-z-z-z, z-z-z-z-z-z-z, z-z-z-z-z-z, z-z-z-z-z, z-z-z-z-z, z-z-z-z-z-z, z-z-z-z, z-z-z-z-z, z-z-z-z-z-z, z-z-z-z, z-z-z-z-z, z-z-z-z-z, z-z-z-z-z, z-z-z-z-z-z-z-"}, {"heading": "D Proof of Theorem 3", "text": "After the optimality of x2 and x-2 we have EI (x2) - E-2 (x2) - E-2 (x2) - E-2 (x2) - EI (x-2) - E-2 (x-2) - E-2 (x-2) - E-2 (x2) - E-2 (x2) - E-2 (x2) - E-12 (1 + max (x2) - EI (x-2) - E-2 (x-2) - E-2 (x-2) - E-2 (x-2) - E-2 (x-2) - (x-2) - 2 (x-2) - (x-2) - (x-4) - (x-4) - (x-4) - (x-4) - (x-4) - (x-4) - (x-4) - (x-4) - (x-4-4) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) (x-2) - (x-2) - (x-4) (x-4) (x-4 - (x-4) - (x-4) - (x-4) (x-4) (x-4) - (x-4 - (x-4) - (x-4) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) (x-2) - (x-2) - (x-2) - (x-2) - (x-2) - (x-2) (x-2) - (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2) (x-2"}, {"heading": "E Proof of Corollary 2", "text": "Theorem 1 shows an interesting finding, which shows that the difference of the variance of any point z in the entrance area after addition of the point x \u00b2 to our observation theorem is exactly D (k (z, x \u00b2) \u2212 BA \u2212 1CT) 2, if we consider x \u00b2 1 as a single point. Furthermore, if we look at the point x \u00b2 \u00b2 \u00b2 z > 0, i.e. m \u00b2 0, we can show that m \u2212 1 = \u00b2 \u00b2 \u00b2 k (x \u00b2 1, z) + (D (CA \u2212 1BT) 2) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (CA \u2212 1BT) 2) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "F Proof of Corollary 3", "text": "It is therefore important to note that the \"a \u2212 b \u00b2 \u00b2 \u00b2 b\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (a2 + b2) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (a2 + b2 + b2) \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 (a2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}], "references": [{"title": "A nonparametric approach to noisy and costly optimization", "author": ["B.S. Anderson", "A. Moore", "D. Cohn"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Batch bayesian optimization via simulation matching", "author": ["J. Azimi", "A. Fern", "X. Fern"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Electricity production by geobacter sulfurreducens attached to electrodes", "author": ["D. Bond", "D. Lovley"], "venue": "Applications of Environmental Microbiology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": "Technical Report TR-2009-23,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A memory-based rash optimizer", "author": ["M. Brunato", "R. Battiti", "S. Pasupuleti"], "venue": "Workshop on Heuristic Search, Memory Based Heuristics and Their applications", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "The Global Optimization Problem: An Introduction Toward Global Optimization", "author": ["L. Dixon", "G. Szeg"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1978}, {"title": "Kriging is well-suited to parallelize optimization", "author": ["D. Ginsbourger", "R.L. Riche", "L. Carraro"], "venue": "Computational Intelligence In Expensive Optimization Problems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A taxonomy of global optimization methods based on response surfaces", "author": ["D. Jones"], "venue": "Journal of Global Optimization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Bayesian algorithms for one-dimensional globaloptimization", "author": ["M. Locatelli"], "venue": "J. of Global Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Genetic algorithms + data structures = evolution programs (2nd, extended ed.)", "author": ["Z. Michalewicz"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Memory-based stochastic optimization", "author": ["A. Moore", "J. Schneider"], "venue": "In NIPS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Improved fuel cell and electrode designs for producing electricity from microbial", "author": ["D. Park", "J. Zeikus"], "venue": "degradation. Biotechnol.Bioeng.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}], "referenceMentions": [{"referenceID": 7, "context": "Bayesian optimization tries to optimize an unknown function f(\u00b7) by requesting a set of experiments when f(\u00b7) is costly to evaluate [8, 4].", "startOffset": 132, "endOffset": 138}, {"referenceID": 3, "context": "Bayesian optimization tries to optimize an unknown function f(\u00b7) by requesting a set of experiments when f(\u00b7) is costly to evaluate [8, 4].", "startOffset": 132, "endOffset": 138}, {"referenceID": 2, "context": "MFCs [3] use micro-organisms to generate electricity.", "startOffset": 5, "endOffset": 8}, {"referenceID": 11, "context": "It has been shown that efficiency of generated electricity power significantly depends on the surface properties of the anode [12].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "Most of the proposed selection criteria in BO are sequential, where only one experiment is selected at each iteration [11, 8, 14, 9].", "startOffset": 118, "endOffset": 132}, {"referenceID": 7, "context": "Most of the proposed selection criteria in BO are sequential, where only one experiment is selected at each iteration [11, 8, 14, 9].", "startOffset": 118, "endOffset": 132}, {"referenceID": 8, "context": "Most of the proposed selection criteria in BO are sequential, where only one experiment is selected at each iteration [11, 8, 14, 9].", "startOffset": 118, "endOffset": 132}, {"referenceID": 1, "context": "[2] introduced a batch BO approach that selects a batch of k experiments at each iteration that approximates the behavior of a given sequential heuristic.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] introduced a constant liar heuristic algorithm to select a batch of experiments based on the Expected Improvement (EI) [9] policy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[7] introduced a constant liar heuristic algorithm to select a batch of experiments based on the Expected Improvement (EI) [9] policy.", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "Although these two batch algorithms [2, 7] can speedup the experiment selection by a factor of k, their results show that batch selection in general performs worse than the sequential EI policy, especially when the total number of experiments is small.", "startOffset": 36, "endOffset": 42}, {"referenceID": 6, "context": "Although these two batch algorithms [2, 7] can speedup the experiment selection by a factor of k, their results show that batch selection in general performs worse than the sequential EI policy, especially when the total number of experiments is small.", "startOffset": 36, "endOffset": 42}, {"referenceID": 12, "context": "We select GP [13] as our probabilistic model and EI [9] as our selection criterion.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "We select GP [13] as our probabilistic model and EI [9] as our selection criterion.", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "EI[9] at point x with associated GP prediction y|O \u223c N (\u03bcx|O, \u03c3 x|O) is defined to be", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "This means that the sample estimation is more sensitive to the output estimation error for functions taking value in [0, 1].", "startOffset": 117, "endOffset": 123}, {"referenceID": 0, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 79, "endOffset": 85}, {"referenceID": 4, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 91, "endOffset": 97}, {"referenceID": 5, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 118, "endOffset": 124}, {"referenceID": 5, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 136, "endOffset": 139}, {"referenceID": 0, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 168, "endOffset": 174}, {"referenceID": 5, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 168, "endOffset": 174}, {"referenceID": 9, "context": "We consider 6 well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5] over [0, 1], Hartman(3)[6] over [0, 1], Hartman(6)[6] over [0, 1], Shekel[6] over [3, 6] and Michalewicz [10] over [0, \u03c0].", "startOffset": 191, "endOffset": 195}, {"referenceID": 0, "context": "Both Fuel cell and Hydrogen data are in [0, 1].", "startOffset": 40, "endOffset": 46}, {"referenceID": 1, "context": "01\u03a3i=1li, where, li is the length of the i th dimension [2].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "To demonstrate the effectiveness of our algorithm, we consider two state-of-the-art batch BO algorithms in the literature: 1) simulation matching (Matching) [2] and 2) the constant liar approach in which the output of the selected samples in the batch is set to their mean in order to select the next experiment (CL(\u03bc\u0302)) [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "To demonstrate the effectiveness of our algorithm, we consider two state-of-the-art batch BO algorithms in the literature: 1) simulation matching (Matching) [2] and 2) the constant liar approach in which the output of the selected samples in the batch is set to their mean in order to select the next experiment (CL(\u03bc\u0302)) [7].", "startOffset": 321, "endOffset": 324}, {"referenceID": 6, "context": "[7], which selects a batch of experiments that jointly maximize the EI objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}], "year": 2012, "abstractText": "Bayesian Optimization (BO) aims at optimizing an unknown function that is costly to evaluate. We focus on applications where concurrent function evaluations are possible. In such cases, BO could choose to either sequentially evaluate the function (sequential mode) or evaluate the function at a batch of multiple inputs at once (batch mode). The sequential mode generally leads to better optimization performance as each function evaluation is selected with more information, whereas the batch mode is more time efficient (smaller number of iterations). Our goal is to combine the strength of both settings. We systematically analyze BO using a Gaussian Process as the posterior estimator and provide a hybrid algorithm that dynamically switches between sequential and batch with variable batch sizes. We theoretically justify our algorithm and present experimental results on eight benchmark BO problems. The results show that our method achieves substantial speedup (up to 78%) compared to sequential, without suffering any significant performance loss.", "creator": "LaTeX with hyperref package"}}}