{"id": "1202.5598", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2012", "title": "Clustering using Max-norm Constrained Optimization", "abstract": "We suggest using the max-norm as a convex surrogate constraint for clustering. We show how this yields a better exact cluster recovery guarantee than previously suggested nuclear-norm relaxation, and study the effectiveness of our method, and other related convex relaxations, compared to other clustering approaches.", "histories": [["v1", "Sat, 25 Feb 2012 02:10:20 GMT  (3366kb,D)", "https://arxiv.org/abs/1202.5598v1", null], ["v2", "Sun, 18 Mar 2012 22:09:44 GMT  (3366kb,D)", "http://arxiv.org/abs/1202.5598v2", null], ["v3", "Wed, 11 Apr 2012 20:56:40 GMT  (3371kb,D)", "http://arxiv.org/abs/1202.5598v3", null], ["v4", "Fri, 13 Apr 2012 06:52:44 GMT  (3371kb,D)", "http://arxiv.org/abs/1202.5598v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ali jalali", "nathan srebro"], "accepted": true, "id": "1202.5598"}, "pdf": {"name": "1202.5598.pdf", "metadata": {"source": "CRF", "title": "Clustering using Max-norm Constrained Optimization", "authors": ["Ali Jalali", "Nathan Srebro"], "emails": ["alij@mail.utexas.edu", "nati@uchicago.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is in such a way that it is a matter of a way in which people act in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world"}, {"heading": "1.1 Relationship to the Goemans Willimason SDP Relaxation", "text": "Our convex relaxation approach is related to the classical SDP relaxation factors of Max-Cut [13] and more generally the Cut-Norm [2]. Indeed, if we are interested in splitting up exactly two clusters, the cluster problem is essentially a Max-Cut problem, albeit with positive and negative weights (i.e. a symmetrical Cut-Norm problem), and our relaxation is essentially the classic SDP relaxation problem of these problems. Our approach and our results differ in several respects. First, we deal with problems with multiple clusters, and even if the number of clusters is not predetermined. If the number of clusters k is predetermined, the cluster problem can be written as a holistic square program, with a k variable per node, and can be relaxed to an SDP. But this SDP will be very different from ours and will include a matrix of size nk, as opposed to our relativity."}, {"heading": "1.2 Other Clustering Approaches", "text": "There are several classes of cluster algorithms with different goals. In hierarchical cluster algorithms such as UPGMA [28], SLINK [27], and CLINK [11], the goal is to generate a sequence of clusters by creating a sequence of clusters at each step of the sequence according to a local inconsistency object, as opposed to our global D (C). Due to this locality, these methods are known to be highly sensitive to outsiders. Intersection-based cluster algorithms such as k-mean / median [31, 15], ratio association [26], ratio-average [9], and normalized intersection [34] try to optimize an objective function globally. The main problem with these goals is that they are typically NP-hard and need to know the number of clusters in advance of time, as these targets are monotonous in the number of clusters. In contrast to this, spectral cluster algorithms [32] try to relate to the main components."}, {"heading": "2 Problem Setup", "text": "Our approach is based on the representation of a cluster formation C by its incidence matrix K (C). Our approach is based on the representation of a cluster formation K (K). While Kuv = 1 iff u and v belong to the same cluster in C (i.e. u, v, Ci for some i), and Kuv = 0 otherwise (i.e. if u and v belong to different clusters), the matrix K (C) is therefore a permutated block diagonal matrix and can therefore also be considered a fraction of the objective incidence matrix of a graph with cliques, the clusters in C. We will say that a matrix K is a valid cluster matrix, or sometimes simply valid if it can be written as K = K (C) for some cluster formation C (i.e. if it is a permutated block diagonal matrix, with 1s in the diagonal blocks in C. We will say that a matrix is a valid cluster matrix, if it is sometimes valid or simple."}, {"heading": "3 Max-Norm Relaxation", "text": "As discussed in the previous section, we are interested in optimizing the non-convex set of valid cluster matrices. The approach we are discussing here is to loosen this set to the matrix set with a limited maximum standard [30]. The maximum standard of a matrix K is defined as \"K\" max = min \"K =\" RLT \"or\" R. \"It is not difficult to see that if K is a valid cluster matrix with K = K (C), then\" K \"is max = 1. This is achieved, for example, by factorizing with R = L, and minimizing it goes beyond factorizing any internal dimensionality. It is not difficult to see that if K is a valid cluster matrix with K = K (C), then\" K \"is max = 1. This is achieved, for example, by factorizing with R = L, and where each row Ru of R is a (standard) indicator vector with Rui = 1 (C),\" then K."}, {"heading": "3.1 Theoretical Guarantee", "text": "Assuming that there is an underlying true clustering, we offer a worst-case guarantee for the exact restoration of this clustering in the presence of noise if the affinity matrix A is a binary 0 \u2212 1 matrix with an absolute goal. This corresponds to our intuition that the max standard has a narrower relativization than the trace standard for valid clustering. To present our theoretical result, we will begin with the introduction of an important quantity based on our main result."}, {"heading": "3.2 Comparison to Single-Linkage Algorithm", "text": "Taking into account the single linkage algorithm (SLINK) [27] as the baseline for cluster restoration, we compare the performance of our algorithm in cluster restoration with it. SLINK creates a hierarchy of clusters, starting with each node as a cluster. At each iteration, SLINK measures the similarity of all cluster pairs and combines the most similar cluster pairs into a new cluster. If we look at the coherence of the Ai and Aj columns as a measure of similarity of nodes i and j.Consider the one shown in Fig. 2. Using exhaustive search, we can show that the non-convex problem (3) produces two clusters, as shown. Running SLINK on this graph, the algorithm first finds two clicks of size 17 and the nodes A and B as four separate clusters in the hierarchy. Next, it combines the nodes A and B as separate clusters, as they are more similar to each other than their own means can never be restored."}, {"heading": "3.3 Comparison to Trace-Norm Constrained Clustering", "text": "This year it has come to the point where we will be able to retaliate, \"he said in an interview with\" Welt am Sonntag, \"in which he dealt with\" Welt \"and\" Welt. \""}, {"heading": "4.1 Semi-Definite Programming Method", "text": "Following Srebro et al. [30] we introduce the dummy variables L, R, Rn \u00b7 n and reformulate (4) as follows: SDP problemsK = arg min K, L, R, A \u2212 K, 1s.t. [L K KT R] 0 and Lii, Rii \u2264 1These limitations correspond to the condition that this SDP can be solved with generic SDP solvers, although it is very slow and not applicable to major problems."}, {"heading": "4.2 Factorization Method", "text": "Motivated by Lee et al. [20], we introduce dummy variables L, R-Rn \u00b7 n and leave K = LRT. With this variable change, we can (4) asK = L-R-T = arg min L, R-A-LRT-1s.t. - L-R-1, 2-R-1, 2 \u2264 1. This problem is not convex, but it is guaranteed that there is no local minimum for a sufficiently large size of the problem [7]. Furthermore, if we now have the optimal solution K at most r, we can take L, R as Rn (r + 1). In practice, we cut down to some reasonably high ranks r, even without a known guarantor on the rank of optimal solution. To solve this problem iteratively, Lee et al. [20] suggest the following update [L R] k + 1 = Pmax ([L R] k + ergo-K [Sign."}, {"heading": "4.3 Loss Function Method", "text": "There are gradient methods, such as truncated gradients [18], which produce a sparse solution, but these methods cannot be applied to this problem. We introduce a surrogate optimization problem in (4) by adding a loss function. In some large \u03bb-R, the matrix Z is sparse and contains the discrepancies. In case of sufficiently large values of \u03bb, the loss function ensures that the matrix A-Z is close to the matrix LRT, which is a limited max-standard matrix. To solve this problem iteratively, we use the following updateZk + 1 = P '1 (Zk + 2). The loss function ensures that the matrix A-Z is close to the matrix LRT, which is a limited max-standard matrix. In this case, we use the following updateZk + 1 = P '1 (Zk + 2)."}, {"heading": "4.4 Dual Decomposition Method", "text": "Inspired by Rockafellar [25] We first reformulate (4) by adding a dummy K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "4.5 Numerical Comparison", "text": "We compare the performance of these methods. For three ideal clusters of size 20 with the noise level Dmax, we execute all three algorithms for 2000 iterations. We consider an initial step variable \u03c4 = 1 for all methods, and for the loss function method, we double every 100 iterations. For the dual method, we update for 20 times and perform 100 iterations of the factoring method for the max normative partial problem with each update. We report on the scarcity of the solution A-K and the \"1 standard of error that we update K-K-1 for each algorithm in Figure 4. This result shows that there is a trade-off between scarcity and error - the dual optimization method consistently provides a sparse solution where factoring and loss function methods deliver small errors. The scarcity of the loss function method worsens with increasing noise."}, {"heading": "5 Tighter Relaxations", "text": "In this section, we improve our basic algorithm in two ways: firstly, we use a tighter relaxation than the track standard, and secondly, we want to go further and introduce tighter relaxations. Figure 5 summarizes various possible relaxations based on the Max Norm. The arrows in this figure indicate the stricter sub-relationships between these relaxations. The tightest relaxation we suggest is {K = RRT: area relaxation, 2 \u2264 1, R \u2265 0} based on the intuition that a cluster matrix is symmetrical and has atrivial factorization."}, {"heading": "5.1 Single-linkage Post Processing", "text": "The matrix K extracted from (6) may differ from a valid cluster matrix in two ways: firstly, it may not have the structure of a valid cluster formation, and secondly, even if it has the structure, the values may not be integer. To solve both of the above problems, we use SLINK on K as a \"rounding scheme.\" SLINK returns a sequence of clusters C1,..., Cn. To select the best cluster, we select K = arg min i-K (Ci). (7) Matrix K can be considered a refined version of affinity matrix A, and therefore the second step of the algorithm can be replaced by other hierarchical cluster algorithms. Of course, the criterion for selecting the best cluster formation in the hierarchy results from the formulation of the correlation cluster formation."}, {"heading": "5.2 Comparison with Other Algorithms", "text": "We compare our advanced algorithm with the trace standard algorithm [16] followed by SLINK and SLINK itself. In all cases, we select a cluster from the SLINK hierarchy using (7). The setup is identical to the experiment explained in Section 3.3. Figure 3.1 summarizes the results and shows that our advanced algorithm significantly outperforms all competitive methods. In addition to accurately restoring the underlying cluster, we would like to investigate that with increasing noise level Dmax, the results of our algorithm deteriorate. Using the \"variation of information\" [23] as a distance measure for clustering, we compare our algorithm with linear objects with the trace standard counterpart, SLINK and spectral clusters [32] for both balanced and unbalanced clusters described above. For the spectral cluster method, we first find the largest k = 4 main components of LINA and then show the main component of SexK itself that cluster formation is no result."}, {"heading": "5.3 MNIST Dataset", "text": "To demonstrate our method in a realistic and larger dataset, we perform our advanced algorithm, the trace standard and spectral clustering on the MNIST dataset [19]. For each experiment, we select n data points from 10 different classes (n / 10 from each class) and construct the affinities using the Gaussian kernel as explained in [6]. We report on the temporal complexity and cluster errors as in the previous experiment in Figure 5.2. For spectral clustering, we use SVD with Matlab and select the 10 most important components, followed by k averages."}, {"heading": "A Proof of Lemma 2", "text": "Assuming equivalences (1) and (2), it is clear that {K = LRT: \u0418L-1, 2 \u2264 1, \u0441R-1, 2 \u2264 1} and {K = RRT: \u0432-R-1, 2 \u2264 1} are both convex sets. Since {K = RRT: \u0432 R-3, 2 \u2264 1, R-0} is the intersection of two sets {K = RRT: \u0432-R-3, 2 \u2264 1} and CP {K = RRT: R-0}, it is sufficient to show that CP is a convex set. The set CP is called a set of completely positive matrices and has proven to be a closed convex cone (see Theorem 2.2 in [5] for details). For proof of equivalence (1) see Lemma 15 in [29]. To prove equivalence (2), it is clear that {K = RRT-1, semi-K: This equivalence (K-1) is not mandatory."}, {"heading": "B Proof of Lemma 1", "text": "Let us construct an example with Dmax = 2n2 \u2211 i | Ci | 2 + 5 that cannot be restored. Let us consider the cluster formation shown in Fig. 8 (a). It is clear that for this cluster formation we have Dmax = \u03b3 and B (C1) = \u03b32 k \u2211 i = 1 | Ci | 2 + \u03b32 2 2 2 k \u2211 i = 1 | Ci | (n \u2212 | Ci |). Let us now consider the alternative cluster formation shown in Fig. 8 (b). For this alternative cluster formation we have B (C2) = \u03b3 (1 \u2212 2\u03b3) k \u0445 i = 1 | Ci | 2.It is clear that B (C2) < B (C1) (the alternative is a better cluster formation) for \u03b3 > 2n2 \u0445 i | Ci | 2 + 5."}, {"heading": "C Proof of Theorem 1", "text": "The proof has two main steps; in the first stage we characterize a sufficient optimality condition based on the existence of a dual variable, and in the second stage we construct such a dual variable. For the sake of the factor we consider a reasonable definition of the maximum eigenvalue of the matrix and \"the Hadamard element is produced wisely.\" C.1 NotationIn this section we present our notation and definitions throughout Paper.C.1.1 Residual Matrix NotationsIn a general way, we do not expect the residual matrix B = A \u2212 K \u00b2 to be frugal unless we have a threshold for the affinity matrix (or we have an adjacency matrix)."}], "references": [{"title": "An improved algorithm for bipartite correlation clustering", "author": ["N. Ailon", "N. Avigdor-Elgrabli", "E. Liberty"], "venue": "European Symposium on Algorithms", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Approximating the cut-norm via grothendieck\u2019s inequality", "author": ["N. Alon", "A. Noar"], "venue": "SIAM Journal on Computing ,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Large scale correlation clustering optimization", "author": ["S. Bagon", "M. Galun"], "venue": "Arxiv preprint arXiv:1112.2903", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Correlation clustering", "author": ["N. Bansal", "A. Blum", "S. Chawla"], "venue": "In Proceedings of the 43rd Symposium on Foundations of Computer Science", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Completely Positive Matrices. World Scientific Publication", "author": ["A. Berman", "N. Shaked-Monderer"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Spectral clustering based on the graph p-laplacian", "author": ["T. B\u00fchler", "M. Hein"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Computational enhancements in low-rank semidefinite programming", "author": ["S. Burer", "C. Choi"], "venue": "Optimization Methods and Software,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of ACM ,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Spectral k-way ratio cut partitioning", "author": ["P. Chan", "M. Schlag", "J. Zien"], "venue": "IEEE Trans. CAD- Integrated Circuits and Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Latent variable graphical model selection via convex optimization. arXiv:1008.1290", "author": ["V. Chandrasekaran", "P.A. Parrilo", "A.S. Willsky"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "An efficient algorithm for a complete link method", "author": ["D. Defays"], "venue": "The Computer Journal (British Computer Society),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1977}, {"title": "A unified view of kernel k-means, spectral clustering and graph cuts", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "UTCS Technical Report TR-04-25,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M. Goemans", "D. Williamson"], "venue": "Journal of ACM ,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Nonnegative factorization of positive semidefinite nonnegative matrices", "author": ["L. Gray", "D. Wilson"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1980}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1981}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Convex Analysis and Minimization Algorithms I", "author": ["J.B", "H.-U. C"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE ,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Practical large-scale optimization for max-norm regularization", "author": ["J. Lee", "B. Recht", "R. Salakhutdinov", "N. Srebro", "J. Tropp"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "A direct product theorem for discrepancy", "author": ["T. Lee", "A. Shraibman", "R. Spalek"], "venue": "In Proceedings of the IEEE 23rd Annual Conference on Computational Complexity", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Correlation clustering with noisy input", "author": ["C. Mathieu", "W. Schudy"], "venue": "In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Comparing clusterings\u2014an information based distance", "author": ["M. Meil\u01ce"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Learning segmentation by random walks", "author": ["M. Meil\u01ce", "J. Shi"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Slink: an optimally efficient algorithm for the single-link cluster method", "author": ["R. Sibson"], "venue": "The Computer Journal (British Computer Society),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1973}, {"title": "Learning with Matrix Factorizations", "author": ["N. Srebro"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Robust pca via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Transactions on Information Theory ", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 130, "endOffset": 140}, {"referenceID": 21, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 130, "endOffset": 140}, {"referenceID": 2, "context": "In this paper, we consider the problem of cut-based, or correlation, clustering [4] that has received a lot of attention recently [1, 22, 3]: Given G(V, E) on n nodes with normalized symmetric affinity matrix A (for all u, v \u2208 V: 0 \u2264 Auv \u2264 1 and Auu = 1), we want to partition V into clusters C = {C1, .", "startOffset": 130, "endOffset": 140}, {"referenceID": 3, "context": "Unfortunately, finding a clustering minimizing the disagreement D(C) is NP-Hard [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 15, "context": "[16] suggested a trace-norm (aka nuclear-norm) relaxation, casting the problem as minimizing an `1 loss and a trace-norm penalty, and providing conditions under which the true underlying clustering is recovered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Instead of trace-norm, we propose using the max-norm (aka \u03b32 : `1 \u2192 `\u221e norm) [30], which is a tighter convex relaxation than the trace-norm.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the subject of some interest in the context of \u201crobust PCA\u201d [8, 33] and recovering the structure of graphical models with latent variables [10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 29, "context": "A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the subject of some interest in the context of \u201crobust PCA\u201d [8, 33] and recovering the structure of graphical models with latent variables [10].", "startOffset": 169, "endOffset": 176}, {"referenceID": 9, "context": "A similar optimization problem with a trace-norm constraint (or trace-norm regularization) has recently been the subject of some interest in the context of \u201crobust PCA\u201d [8, 33] and recovering the structure of graphical models with latent variables [10].", "startOffset": 248, "endOffset": 252}, {"referenceID": 12, "context": "1 Relationship to the Goemans Willimason SDP Relaxation Our convex relaxation approach is related to the classic SDP relaxations of max-cut [13] and more generally the cut-norm [2].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "1 Relationship to the Goemans Willimason SDP Relaxation Our convex relaxation approach is related to the classic SDP relaxations of max-cut [13] and more generally the cut-norm [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 15, "context": "This type of guarantee is more in the spirit of compressed sensing, which where exact recovery of a support set is guaranteed subject to conditions on the input [16].", "startOffset": 161, "endOffset": 165}, {"referenceID": 25, "context": "In hierarchical clustering algorithms such as UPGMA [28], SLINK [27] and CLINK [11] the goal is to generate a sequence of clusterings by produce a sequence of clustering by merging/splitting two clusters at each step of the sequence according to a local disagreement objective as opposed to our global D(C).", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "In hierarchical clustering algorithms such as UPGMA [28], SLINK [27] and CLINK [11] the goal is to generate a sequence of clusterings by produce a sequence of clustering by merging/splitting two clusters at each step of the sequence according to a local disagreement objective as opposed to our global D(C).", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Cut-based clustering algorithms such as k-means/medians [31, 15], ratio association [26], ratio cut [9] and normalized cut [34] try to optimize an objective function globally.", "startOffset": 56, "endOffset": 64}, {"referenceID": 24, "context": "Cut-based clustering algorithms such as k-means/medians [31, 15], ratio association [26], ratio cut [9] and normalized cut [34] try to optimize an objective function globally.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "Cut-based clustering algorithms such as k-means/medians [31, 15], ratio association [26], ratio cut [9] and normalized cut [34] try to optimize an objective function globally.", "startOffset": 100, "endOffset": 103}, {"referenceID": 28, "context": "In contrast, spectral clustering algorithms[32] try to find the first k principal component of the affinity matrix or a transformed version of that [24].", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "In contrast, spectral clustering algorithms[32] try to find the first k principal component of the affinity matrix or a transformed version of that [24].", "startOffset": 148, "endOffset": 152}, {"referenceID": 11, "context": "These methods require the number of clusters in advance and has been shown to be tractable (convex) relaxations to NP-Hard cut-based algorithms [12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 27, "context": "The approach we discuss here is to relaxing this set to the set of matrices with bounded max-norm [30].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "The flavor of our result is similar to [16] for trace-norm, except that we show the max-norm constraint problem recovers the underlying clustering with larger noise comparing to tracenorm constraint.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "This definition is inspired by [16] but is slightly different.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "2 Comparison to Single-Linkage Algorithm Considering single-linkage algorithm (SLINK) [27] as a baseline for clustering, we compare the power our algorithm in cluster recovery with that.", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Comparing to the result of [16] on trace-norm (Dmax \u2264 |Cmin| 4n ), the max-norm tolerates more noise.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Running Trace-Norm constrained minimization [16] on the graph shown in Fig.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "Further, we compare our algorithm with trace-norm algorithm [16] and SLINK on a probabilistic setup.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "[30], we introduce dummy variables L,R \u2208 Rn\u00d7n and reformulate (4) as the following SDP problem K\u0302 = arg min K,L,R \u2016A\u2212K\u20161", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], we introduce dummy variables L,R \u2208 Rn\u00d7n and let K = LR .", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "This problem is not convex, but it is guaranteed to have no local minima for large enough size of the problem [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 19, "context": "[20] suggest the following update [ L R ]", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "3 Loss Function Method There are gradient methods such as truncated gradient [18] that produce sparse solution, however, these methods cannot be applied to this problem.", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "This suggests using the tightest convex relaxation, that is constraining to K such that there exists R >= 0, \u2016R\u2016\u221e,2 <= 1 with K = RR (the set of matrices K with a factorization K = RR , R >= 0 is called the set of completely positive matrices and is convex [5]).", "startOffset": 257, "endOffset": 260}, {"referenceID": 15, "context": "2 Comparison with Other Algorithms We compare our enhanced algorithm with the trace-norm algorithm [16] followed by SLINK and SLINK itself.", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Using \u201cvariation of information\u201d [23] as a distance measure for clusterings, we compare our algorithm with linear objective with trace-norm counterpart, SLINK and spectral clustering[32] for both balanced and unbalanced clusterings described before.", "startOffset": 33, "endOffset": 37}, {"referenceID": 28, "context": "Using \u201cvariation of information\u201d [23] as a distance measure for clusterings, we compare our algorithm with linear objective with trace-norm counterpart, SLINK and spectral clustering[32] for both balanced and unbalanced clusterings described before.", "startOffset": 182, "endOffset": 186}, {"referenceID": 18, "context": "3 MNIST Dataset To demonstrate our method in a realistic and larger scale data set, we run our enhanced algorithm, trace-norm and spectral clustering on MNIST Dataset [19].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "For each experiment, we pick a total of n data points from 10 different classes (n/10 from each class) and construct the affinities using Gaussian kernel as explained in [6].", "startOffset": 170, "endOffset": 173}], "year": 2012, "abstractText": "We suggest using the max-norm as a convex surrogate constraint for clustering. We show how this yields a better exact cluster recovery guarantee than previously suggested nuclear-norm relaxation, and study the effectiveness of our method, and other related convex relaxations, compared to other clustering approaches.", "creator": "LaTeX with hyperref package"}}}