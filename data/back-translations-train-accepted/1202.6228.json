{"id": "1202.6228", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2012", "title": "PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification", "abstract": "In this work, we propose a PAC-Bayes bound for the generalization risk of the Gibbs classifier in the multi-class classification framework. The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate. Thanks to very recent and beautiful results on matrix concentration inequalities, we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper-bounded by its empirical risk plus a term depending on the number of training examples in each class. To the best of our knowledge, this is the first PAC-Bayes bounds based on confusion matrices.", "histories": [["v1", "Tue, 28 Feb 2012 14:13:01 GMT  (27kb)", "https://arxiv.org/abs/1202.6228v1", null], ["v2", "Thu, 22 Mar 2012 09:29:28 GMT  (27kb)", "http://arxiv.org/abs/1202.6228v2", null], ["v3", "Thu, 24 May 2012 06:26:12 GMT  (55kb,D)", "http://arxiv.org/abs/1202.6228v3", "Arxiv:this http URL, Accepted at ICML 2012"], ["v4", "Sat, 30 Jun 2012 19:23:14 GMT  (41kb)", "http://arxiv.org/abs/1202.6228v4", "Arxiv:this http URL, Accepted at ICML 2012"], ["v5", "Wed, 4 Jul 2012 14:57:53 GMT  (55kb,D)", "http://arxiv.org/abs/1202.6228v5", "Arxiv:this http URL, Accepted at ICML 2012"], ["v6", "Tue, 22 Oct 2013 08:25:52 GMT  (55kb,D)", "http://arxiv.org/abs/1202.6228v6", "Arxiv:this http URL, Accepted at ICML 2012"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["emilie morvant", "sokol ko\u00e7o", "liva ralaivola"], "accepted": true, "id": "1202.6228"}, "pdf": {"name": "1202.6228.pdf", "metadata": {"source": "CRF", "title": "PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification\u2217", "authors": ["Emilie Morvant", "Sokol Ko\u00e7o", "Liva Ralaivola"], "emails": ["firstname.name@lif.univ-mrs.fr"], "sections": [{"heading": null, "text": "Keywords: machine learning, PAC-Bayes generalization boundaries, confusion matrix, concentration inequality, multi-class classification"}, {"heading": "1 Introduction", "text": "The PAC-Bayesian Framework, first introduced by McAllester (1999b), reflects an important field of research in learning theory. It borrows ideas from the philosophy of Bayesian reasoning and mixes them with techniques used in statistical approaches to learning. Given a family of classifiers F, the ingredients of a PAC-Bayesian bound are a prior distribution P over F, a learning sample S, and a posterior distribution Q over F. Distribution P conveys some prior beliefs about what the best classifiers of F are; the classifiers who are expected to have the most powerful at hand for the classification task therefore have the greatest weights under P. The posterior distribution Q is learned / adapted using the information provided by the training S. The nature of the PAC-Bayesian results is tied to the risk of stochastic Gibbs classifiers associated with Q (Catoni, 2004) - to predict the label of an example."}, {"heading": "2 Setting and Notations", "text": "This section sets out the general framework we are considering and the various tools we will use."}, {"heading": "2.1 General Problem Setting", "text": "We consider classification problems via the input range X Rd of dimension d. The output space is denoted by Y = {1,.., Q}, where Q is the number of classes. The learning sample is denoted by S = {(xi, yi)} mi = 1, with each example usually drawn from a fixed - but unknown - probability distribution D over X \u00b7 Y. Dm denotes the distribution of a m sample. F'RX is a family of classifiers f: X \u2192 Y. P and Q are respectively the previous and the rear distributions over F. Given the previous distribution P and the training set S, the learning process consists of finding the rear distribution Q, which leads to a good generalization. Since we use the previous distribution P to F, a PAC-Bayes generalization depends on the Kullback-Leibler divergence (KL divergence): KL (Q-P) = Ef-Q log (f)."}, {"heading": "2.2 Conventions and Basics on Matrices", "text": "Throughout the thesis, we consider only real square matrices C of order Q (the number of classes). tC is the transposition of matrix C, IdQ denotes the identity matrix of quantity Q and 0 is the null matrix. The results given in this thesis are based on a concentration imbalance of Tropp (2011) for a sum of random, self-adjugated matrices. In the case that a matrix is not self-adjugating and has real value, we use Technical Report V 4.0 2dilation of such a matrix, which is given in Paulsen (2002) and is defined as follows: S (C) def = (0 C tC 0). (2) The symbol corresponds to the operator standard, which is also referred to as the spectral standard: It returns the largest singular value of its reasoning, defined by the note \"C\": max = max {\u03bbmax (C), min \u21d2 (C), mini (C), \u2264 (3) and the two elements are respectively."}, {"heading": "3 The Usual PAC-Bayes Theorem", "text": "In this section, we recall the most important PAC-Bayesian limits in the case of the binary classification Q = Q Q (Q = Q = Q = Q = Q (McAllester, 2003; Seeger, 2002; Langford, 2005).The set of terms we are considering is Y = {\u2212 1; 1} (with Q = 2) and, for each classifier f-F, the predicted output of x-X is given by characters (f (x).The actual risk R (f) and the empirical error RS (f) of f are defined as: R (f) def = E (x, y) - DI (f (x 6 = y)); RS (f) def = 1m m \u00b2 i = 1 (f (xi 6 = yi).The goal of the learner is to select a posterior distribution Q to F so that the risk of the Q-weighted majority (also referred to as Bayes classifier) BQ is as low as possible."}, {"heading": "4 Multiclass PAC-Bayes Bound", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Definitions and Setting", "text": "As already mentioned, we focus on multiclassification. The output space is Y = {1 = q = q = q = q = q = q = 2. We will only consider learning algorithms based on learning sample S = {(xi, yi)} mi = 1, where each example is drawn i.i.d to D, so the confusion matrix and myj \u2265 1 for each class yj, where myj is the number of examples of the real class yj. In the context of multiclassification, an error measurement can be the confusion matrix. In particular, we will consider a confusion matrix based on the classical definition based on conditional probabilities x: It is inherent (and desirable) to hide the effects of the differently represented classes. Specifically, for a given confusion matrix F and a sample S = (xi, yi)} mi = 1 x, the empirical confusion matrix."}, {"heading": "4.2 Main Result: Confusion PAC-Bayes Bound for the Gibbs Classifier", "text": "Our main result is a PAC Bayes-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-"}, {"heading": "4.3 Upper Bound on the Risk of the Majority Vote Classifier", "text": "Our multi-level upper limit on the risk of Gibbs classifiers leads to an upper limit on the risk of Bayes classifiers in the following way by sentence 1. We remember that the Bayes classifier BQ is defined under a given posterior distribution Q. so that for each example it represents the majority class under the scale Q, and we define it as: BQ (x) = argmaxc, q) = Ex-QI (f (x) = c)]. (11) We define the conditional Gibbs risk R (GQ, p, q) and Bayes risk R (GQ, p, q) asR (GQ, p, q) = Ex-QI = pEf-QI (f (x) = q) = q), (12) R (BQ, p, q) and Bayes risk R (BQ, p) = Ex-Profile (argmaxc, q)."}, {"heading": "5 Proof of Theorem 2", "text": "This section provides formal evidence for Theorem 2. We first introduce a concentration inequality for a sum of random square matrices, which allows us to deduce the generalization between PAC and Bayes limited to confusion matrices by following the same \"three-step process\" as given in McAllester (2003); Seeger (2002); Langford (2005) for the classical PAC-Bayes boundary."}, {"heading": "5.1 Concentration Inequality for the Confusion Matrix", "text": "The main results of our work are based on the following corollary sequence of a result on the concentration imbalance (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "5.2 \u201cThree Step Proof\u201d Of Our Bound", "text": "First of all, thanks to the concentration of inequality (19), we prove the following Lemma.Lemma 1. Leave Q the size of CfS and C \u00b2 f i = C \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p p p \u00b2 p p \u00b2 p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p) p p p p p p p p"}, {"heading": "5.3 Simplification", "text": "To this end, we have shown in Section 5.1 that we can choose ai = 1myi for each i-th example, where yi is the class of the i-th example and myi is the number of examples of class yi. Therefore, we have: \u03c32 = m \u00b2 i = 1 m2yi = Q \u00b2 y = 1 my. To simplify the equation (24), and since the term on the right side of this equation is an increasing function in relation to \u03c32, we propose to reach the upper limit 2: \u03c32 = Q \u00b2 y = 1 m2y = Q \u00b2 y = 1 my \u00b2 y = 1 my \u00b2 y = 1 my \u00b2,..., Qmy."}, {"heading": "6 Discussion and Future Work", "text": "This work gives rise to many interesting questions, including the following: Some perspectives focus on the instantiation of our limit given in Theorem 2 for specific multiclass frames, such as SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multiclass reinforcement (AdaBoost.MH / AdaBoost.MR Schapire and Singer (2000); SAMME Zhu et al. (2009); AdaBoost.MM Mukherjee and Schapire (2011). Using our theorem while using the confusion matrices, we can deduce new generalization limits for these methods. Furthermore, we are interested in how effective learning methods differ from the risk-based ones that we propose. For example, in the binary PAC-Bayes setting, the MinCq algorithm of Technical Report V 4.0 Laviolet (2011) could be a problem of inequality."}, {"heading": "7 Conclusion", "text": "The originality of our contribution is that we consider the confusion matrix as an error variable (with the idea that the smaller matrix of the confusion matrix of the learned classifier, the better it is at hand for the classification task).The derivation of our result uses the generalization inequality proposed by Tropp (2011) for the sum of random self-adjoined matrices, which we adapt directly to square matrices that are not adjoint.The main results are presented in Theorem 2 and Corollary 1. The limit in Theorem 2 is the difference between the true risk of the Gibbs classifier and its empirical error."}], "references": [{"title": "A new approach to collaborative filtering", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "Vert", "J.-P"], "venue": null, "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "PAC-bayesian supervised classification: The thermodynamics of statistical learning", "author": ["O. Springer. Catoni"], "venue": null, "citeRegEx": "Catoni,? \\Q2007\\E", "shortCiteRegEx": "Catoni", "year": 2007}, {"title": "On the algorithmic implementation of multiclass kernel-based vector", "author": ["K. ArXiv e-prints. Crammer", "Y. Singer"], "venue": null, "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "PAC-Bayesian Generalization Bounds on the Confusion", "author": ["E. Morvant", "S. Ko\u00e7o", "L. Ralaivola"], "venue": null, "citeRegEx": "Morvant et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Morvant et al\\.", "year": 2007}, {"title": "An improved predictive accuracy bound for averaging", "author": ["Research", "J. 6:273\u2013306. Langford", "M. Seeger", "N. Megiddo"], "venue": null, "citeRegEx": "Research et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Research et al\\.", "year": 2001}, {"title": "PAC-bayes & margins", "author": ["J. Langford", "J. Shawe-Taylor"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Langford and Shawe.Taylor,? \\Q2002\\E", "shortCiteRegEx": "Langford and Shawe.Taylor", "year": 2002}, {"title": "From PAC-Bayes Bounds to Quadratic Programs", "author": ["F. Press. Laviolette", "M. Marchand", "Roy", "J.-F"], "venue": "Processing Systems", "citeRegEx": "Laviolette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Laviolette et al\\.", "year": 2011}, {"title": "Multicategory support vector machines, theory, and application", "author": ["Y. Lin", "G. Wahba"], "venue": "Majority Votes. In Proceedings of the International Conference on Machine", "citeRegEx": "Y. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Y. et al\\.", "year": 2004}, {"title": "A theory of multiclass boosting. CoRR, abs/1108.2989", "author": ["I. Mukherjee", "R.E. Schapire"], "venue": "Computational learning theory (COLT),", "citeRegEx": "Mukherjee and Schapire,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee and Schapire", "year": 2011}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Press. Schapire", "Y. Singer"], "venue": null, "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "BoosTexter: A boosting-based system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q2000\\E", "shortCiteRegEx": "Schapire and Singer", "year": 2000}, {"title": "Multi-class adaboost", "author": ["J. Weston", "C. Watkins"], "venue": "Technical Report V", "citeRegEx": "Weston and Watkins,? \\Q1998\\E", "shortCiteRegEx": "Weston and Watkins", "year": 1998}], "referenceMentions": [{"referenceID": 1, "context": "The essence of PAC-Bayesian results is to bound the risk of the stochastic Gibbs classifier associated with Q (Catoni, 2004) \u2014in order to predict the label of an example x, the Gibbs classifier first draws a classifier f from F according to Q and then returns f(x) as the predicted label. When specialized to appropriate function spaces F and relevant families of prior and posterior distributions, PAC-Bayes bounds can be used to characterize the error of a few existing classification methods. An example deals with the risk of methods based upon the idea of the majority vote in the case of binary classification. We may notice that if Q is the posterior distribution, the error of the Q-weighted majority vote classifier, which makes a prediction for x according to \u2211 f f(x)Q(f), is bounded by twice the error of the Gibbs classifier. If the classifiers from F on which the distribution Q puts a lot of weight are good enough, then the bound on the risk of the Gibbs classifier can be an informative bound for the risk of the Q-weighted majority vote. With a more elaborated argument, Langford and Shawe-Taylor (2002) give a PAC-Bayes bound for Support Vector Machine (SVM) which closely relates the risk of the Gibbs classifier and that of the corresponding majority vote classifier, and where the margin of the examples enter into play.", "startOffset": 111, "endOffset": 1122}, {"referenceID": 6, "context": "This bound is used to derive an algorithm, namely MinCq (Laviolette et al., 2011), which achieves empirical results on par with state-of-the-art methods.", "startOffset": 56, "endOffset": 81}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001).", "startOffset": 42, "endOffset": 112}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al.", "startOffset": 43, "endOffset": 302}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002).", "startOffset": 43, "endOffset": 321}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.", "startOffset": 43, "endOffset": 351}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.", "startOffset": 43, "endOffset": 457}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.", "startOffset": 43, "endOffset": 522}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al.", "startOffset": 43, "endOffset": 588}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al. (2009). The originality of our work is that we consider the confusion matrix of the Gibbs classifier as an error measure.", "startOffset": 43, "endOffset": 629}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al. (2009). The originality of our work is that we consider the confusion matrix of the Gibbs classifier as an error measure. We believe that in the multiclass framework, it is more relevant to consider the confusion matrix as the error measure than the mere misclassification error, which corresponds to the probability for some classifier h to err on x. The information as to what is the probability for an instance of class p to be classified into class q (with p 6= q) by some predictor is indeed crucial in some applications (think of the difference between false-negative and false-positive predictions in a diagnosis automated system). To the best of our knowledge, we are the first to propose a generalization bound on the confusion matrix in the PAC-Bayesian framework. The result that we propose heavily relies on the matrix concentration inequality for sums of random matrices introduced by Tropp (2011). One may anticipate that generalization bounds for the confusion matrix may also be obtained in other framework than the PAC-Bayesian one, such as the uniform stability framework, the online learning framework and so on.", "startOffset": 43, "endOffset": 1533}, {"referenceID": 1, "context": "Some other important results are given in (Catoni, 2007; Seeger, 2002; McAllester, 1999a; Langford et al., 2001). In this paper, we address the multiclass classification problem. Some related works are therefore multiclass formulations for the SVMs, such as the frameworks of Weston and Watkins (1998), Lee et al. (2004) and Crammer and Singer (2002). As majority vote methods, we can also cite multiclass adaptations of AdaBoost Freund and Schapire (1996), such as the framework proposed by Mukherjee and Schapire (2011), AdaBoost.MH/AdaBoost.MR algorithms of Schapire and Singer (1999) and SAMME algorithm by Zhu et al. (2009). The originality of our work is that we consider the confusion matrix of the Gibbs classifier as an error measure. We believe that in the multiclass framework, it is more relevant to consider the confusion matrix as the error measure than the mere misclassification error, which corresponds to the probability for some classifier h to err on x. The information as to what is the probability for an instance of class p to be classified into class q (with p 6= q) by some predictor is indeed crucial in some applications (think of the difference between false-negative and false-positive predictions in a diagnosis automated system). To the best of our knowledge, we are the first to propose a generalization bound on the confusion matrix in the PAC-Bayesian framework. The result that we propose heavily relies on the matrix concentration inequality for sums of random matrices introduced by Tropp (2011). One may anticipate that generalization bounds for the confusion matrix may also be obtained in other framework than the PAC-Bayesian one, such as the uniform stability framework, the online learning framework and so on. The rest of this paper is organized as follows. Sec. 2 introduces the setting of multiclass learning and some of the basic notation used throughout the paper. Sec. 3 briefly recalls the folk PAC-Bayes bound as introduced in McAllester (2003). In Sec.", "startOffset": 43, "endOffset": 1996}, {"referenceID": 7, "context": "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al.", "startOffset": 141, "endOffset": 167}, {"referenceID": 2, "context": "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al.", "startOffset": 168, "endOffset": 194}, {"referenceID": 2, "context": "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.", "startOffset": 168, "endOffset": 213}, {"referenceID": 2, "context": "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.MH/AdaBoost.MR Schapire and Singer (2000), SAMME Zhu et al.", "startOffset": 168, "endOffset": 290}, {"referenceID": 2, "context": "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.MH/AdaBoost.MR Schapire and Singer (2000), SAMME Zhu et al. (2009), AdaBoost.", "startOffset": 168, "endOffset": 315}, {"referenceID": 2, "context": "Some perspectives will be focused on instantiating our bound given in Theorem 2 for specific multi-class frameworks, such as multi-class SVM Weston and Watkins (1998); Crammer and Singer (2002); Lee et al. (2004) and multi-class boosting (AdaBoost.MH/AdaBoost.MR Schapire and Singer (2000), SAMME Zhu et al. (2009), AdaBoost.MM Mukherjee and Schapire (2011)).", "startOffset": 168, "endOffset": 358}, {"referenceID": 0, "context": "Finally, the question of extending the present work to the analysis of algorithms learning (possibly infinite-dimensional) operators as Abernethy et al. (2009) is also very exciting.", "startOffset": 136, "endOffset": 160}], "year": 2013, "abstractText": "In this work, we propose a PAC-Bayes bound for the generalization risk of the Gibbs classifier in the multi-class classification framework. The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate. Thanks to very recent and beautiful results on matrix concentration inequalities, we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper-bounded by its empirical risk plus a term depending on the number of training examples in each class. To the best of our knowledge, this is the first PAC-Bayes bounds based on confusion matrices.", "creator": "LaTeX with hyperref package"}}}