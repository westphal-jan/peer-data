{"id": "1202.6258", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2012", "title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets", "abstract": "We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. Numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms.", "histories": [["v1", "Tue, 28 Feb 2012 15:42:51 GMT  (124kb,D)", "https://arxiv.org/abs/1202.6258v1", null], ["v2", "Thu, 5 Jul 2012 19:41:32 GMT  (216kb,D)", "http://arxiv.org/abs/1202.6258v2", null], ["v3", "Fri, 6 Jul 2012 10:59:04 GMT  (216kb,D)", "http://arxiv.org/abs/1202.6258v3", null], ["v4", "Mon, 11 Mar 2013 19:54:48 GMT  (217kb,D)", "http://arxiv.org/abs/1202.6258v4", "The notable changes over the current version: - worked example of convergence rates showing SAG can be faster than first-order methods - pointing out that the storage cost is O(n) for linear models - the more-stable line-search - comparison to additional optimal SG methods - comparison to rates of coordinate descent methods in quadratic case"]], "reviews": [], "SUBJECTS": "math.OC cs.LG", "authors": ["nicolas le roux", "mark w schmidt", "francis r bach"], "accepted": true, "id": "1202.6258"}, "pdf": {"name": "1202.6258.pdf", "metadata": {"source": "CRF", "title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets", "authors": ["Nicolas Le Roux", "Mark Schmidt", "Francis Bach"], "emails": ["nicolas@le-roux.name", "mark.schmidt@inria.fr", "francis.bach@ens.fr"], "sections": [{"heading": "1 Introduction", "text": "A variety of problems that arise in the field of machine learning involve an approximate minimization of the sum of a loss function over a large number of training examples in which there is a large amount of redundancy between the examples. (1) Although the theory behind the methods of machine learning can be generally applied, the methods of machine learning are typically used to solve the problem of optimizing an example average for an average training. (1) In this paper, we focus on such finite training problems in which each FIS is smooth and the average function ex.As an example, in the case of 2-regulated logistic regression, we have fi. (2)"}, {"heading": "2 Related Work", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are not able to trump themselves. \"(...) Most of them are not able to trump themselves.\" (...) Most of them are not able to trump themselves. \"(...)"}, {"heading": "3 Convergence Analysis", "text": "In our analysis, we assume that each function is distinguishable in (1) and that each gradient f \"i\" is a problem that we all have in relation to constant L, which means that the same method is applied to all x and y in Rp. (1) However, we also assume that the average function g = 1n, which is a relatively weak assumption in FI functions, and in cases where the FI is doubly differentiable, is equivalent to stating that the eigenvalues of the Hessians of each fi are limited beyond L. (In addition, we assume that the average function g = 1n, which is strongly convex in FI functions, is constant \u00b5 > 0, which means that the function x 7 \u2192 g. \"This is a stronger assumption and is not satisfied by all machine learning models. However, it should be noted that in machine learning we are typically free to choose the regulator, and we can always add a regulation 2-Eq."}, {"heading": "4 Implementation Details", "text": "In this section we describe modifications that substantially reduce the storage requirements of SAG iteration, as well as modifications that lead to better practical performance.Structured gradients: For many problems, the storage costs of O (np) are generally prohibitive for the yki vectors, but we can often use the structure in the f \u00b2 i to reduce these costs. For example, many loss functions take the form fi (a), which reduces the storage costs to O (n). Since ai is constant, we only need to save the scalar f \u00b2 ik (u k) for u \u00b2 i = a T ik xk instead of the full gradient aTi f \u00b2 i, reducing the storage costs to O (n). Furthermore, we only need to save the simple form of SAG updates if we can use lazy updates to reduce the iteration costs from O (p) to the economy of the ai.mini batches."}, {"heading": "5 Experimental Results", "text": "Our first experiments focus on the following methods we chose because they do not have dataset-dependent tuning parameters: - Steemost: The full gradient method described by iteration (3), with a line-search that uses cubic hermite polynomial interpolation to find a step size for a step size, and where the parameters of the line-search were tuned for the problems at hand.- AFG: Nesterov's accelerated full gradient method Nesterov [1983], where iterations of (3) with a fixed step size are interleaved with an extrapolation step, and we use an adaptive line-search based on Liu et al. [2009].- L-BFGS: A publicly available limited memory quasi-Newton method that has tuned for log-linear modelar."}, {"heading": "6 Discussion", "text": "In a learning context in which each function fi is the loss associated with a single data point, L is equal to the greatest value of the loss second derivative step. (1 for the square loss, 1 / 4 for the logistical loss) times R2, where R is a uniform limit on the norm of each data point. [2009] where C is a scalar constant, and thus the constraint \u00b5L > 8 n is met when there is an improvement in the regulation parameters of the form C / n Liang et al. [2009] where C is a scalar constant, and thus can violate the constraint. [2009] the improvement in terms of regulation parameters of the form AG is known to be asymptotically foggy, and in any case in such low-dimensional settings, regular chastostic or gradients are efficient enough in practice."}, {"heading": "Acknowledgements", "text": "Nicolas Le Roux, Mark Schmidt and Francis Bach are supported by the European Research Council (SIERRA-ERC-239993) and Mark Schmidt receives a postdoctoral fellowship from the Natural Sciences and Engineering Research Council of Canada (NSERC)."}, {"heading": "Appendix", "text": "In this appendix we first give the evidence for the two theses. Then we compare the convergence rates of primary and dual RG and coordinate methods with the SAG rates for \"2-regulated smallest squares in terms of effective passages through the data."}, {"heading": "A Proofs of the propositions", "text": "Here we present the proofs of sentences 1 and 2."}, {"heading": "A.1 Problem set-up and notations", "text": "We use g = 1n = 1 \u2212 fi to denote a \u00b5 \u2212 strongly convex function where the functions fi, i = 1,.. \u00b7 n are convex functions from Rp to R with continuous gradients of L-Lipschitz. Let us use x \u00b7 the unique minimizer of g.For k > 1 to describe the stochastic average gradient algorithm that selects recursionxk = xk \u2212 1 \u2212 1 \u2212 n n n n n n n \u00b2 n \u00b2 n blocks where an ik in {1,.., n} is uniformly randomly selected and we use setyki = {f \u2032 i (x k \u2212 1) p (x \u2212 p) if i = ik, yk \u2212 m \u2212 ag is a random variable that takes the value 1 \u2212 1n with the probability of 1 n and \u2212 n otherwise (i.e. with zero expectation), this is f \u00b2 p (x \u2212 p \u2212 p) if i is \u2212 k \u2212 n \u2212 n blocks, \u2212 n \u2212 n \u2212 n blocks."}, {"heading": "A.2 Outline of the proofs", "text": "Each proposition will be detectable in several steps.1. We will find a Lyapunov function Q from R (n + 1) p to R (n + 1) so that the sequence EQ (n \u2212 x) decreases at a linear rate.2. We will prove that Q (n \u2212 k) with a constant for all k.3. In the case of Proposition 2 we will show how with a pass of the stochastic gradient as initialization the desired result is obtained. Throughout the course of the proofs Fk will show the field of information up to (and including time k), i.e., Fk is the field generated by z1..., zk.A.3 Convergence results for the stochastic rate We will show the constant in both ranges depending on the selected initialization."}, {"heading": "Proof", "text": "Step 1 - Linear convergence of the Lyapunov function In this case, our Lyapunov function is square, i.e., Q (empirical k) = (empirical k) \u00b7 (empirical k) \u00b7 (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical k) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical) (empirical) (empirical (empirical) (empirical) (empirical) (empirical) (empirical (empi"}, {"heading": "B Comparison of convergence rates", "text": "We consider the \"2-regulated smallest squares\" problematic because they minimize x-constant rpg (x): = 2-constant rpg (x): = 2-constant rpg (x): = 2-constant rpg (x): = 2-constant rpg + 1-constant rpg (aTi x-bi) 2.If we use b to denote a vector containing the values bi and A to denote a matrix without rows ai, we can rewrite this problem so that x-x-x-2 + 1-constant rpp-2 (aTi x-b).The fennel dual of this problem minimizes y-constant rnd (y): = 2-y-y-y-y > AA > y + y > b. We can get the primary variables from the dual variables by the formula x-1 constant d-constant constant constant constant (y-d)."}, {"heading": "B.1 Full Gradient Methods", "text": "If we apply the simple FG method with a step size of 1 / Lg, then (f (xk) \u2212 f (x)) it inevitably converges to zero at the rate (1 \u2212 \u00b5gLg) 2 = (1 \u2212 \u03bb + m\u03c3 / n), while a larger step size of 2 / (Lg + \u00b5g) yields a faster rate of (1 \u2212 \u00b5g + \u00b5gLg) 2 = (1 \u2212 n\u03bb + M\u03c3) 2 \u2264 exp (\u2212 2n\u03bb + m\u0441n) 2, while a larger step size of 2 / (Lg + \u00b5g) yields a faster rate of (1 \u2212 \u00b5g + \u00b5g) 2 = (1 \u2212 n\u03bb + M\u03c3) 2 \u2264 exp (\u2212 2n\u03bb + m\u0441n) 2, while a larger step size of 2 / (Lg + \u00b5g) yields a faster rate of (1 \u2212 \u00b5g + \u00b5g) 2 = (1 \u2212 n\u03bb + M\u03c3) 2 \u2264 exp (\u2212 2n\u03bb + m\u0441n) 2), while a larger step size of 2 / (Lg + \u00b5g) yields a faster rate of (1 \u2212 \u00b5g + \u00b5g) 2 = (1 \u2212 n\u03bb + m\u03bb + m\u03c3) 2 \u2264 exp (1)"}, {"heading": "B.2 Coordinate-Descent Methods", "text": "The cost of applying an iteration of an FG method is O (np). For the same cost, we could apply p-iterations of a coordinate descend method to the primary, assuming that the selection of coordinates to be updated incurs costs equal to O (1). If we randomly select coordinates uniformly, then the convergence rate for p-iterations of the coordinate descend with a step size of 1 / Ljg [Nesterov, 2010, theorem 2] (1 \u2212 \u00b5g pLjg) p = (1 \u2212 \u03bb + m\u03c3 / np (\u03bb + Mj / n)) p = (1 \u2212 n\u03bb + m\u03c3p (n\u03bb + Mj))) p \u2264 exp (\u2212 n\u03bb + mppi + Mj).Here we see that applying a coordinate descend method can be much more efficient than an FG method if it is a variable p."}, {"heading": "B.3 Stochastic Average Gradient", "text": "For the O (np) cost of an iteration of the FG method, we can perform n iterations of the SAG. With a step size of 1 / 2nLg, performing n iterations of the SAG algorithm has a rate of (1 \u2212 \u00b5g 8nLig) n = (1 \u2212 \u03bb + m\u03c3 / n8n (\u03bb + Mi) n = (1 \u2212 n\u03bb + m\u03c38n (n\u03bb + nMi) n \u2264 exp (\u2212 18 n\u03bb + m\u03c3 nMi), which is most similar to the rate achieved with the dual coordinate descend method, but is probably slower due to the n-term scaling Mi. However, the difference in case of overdetermined problems is reduced if m\u03c3 > > m \u2032 \u03c3.Under the condition n > 8Lig / \u00b5g = 8 (\u03bb + Mi) / (\u03bb + m2000), if we have an iteration of the SAG, we cannot achieve the optimal rate (Lig = 8g)."}], "references": [{"title": "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization", "author": ["A. Agarwal", "P.L. Bartlett", "P. Ravikumar", "M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["F. Bach", "E. Moulines"], "venue": null, "citeRegEx": "Bach and Moulines.,? \\Q2011\\E", "shortCiteRegEx": "Bach and Moulines.", "year": 2011}, {"title": "A new class of incremental gradient methods for least squares problems", "author": ["D.P. Bertsekas"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Bertsekas.,? \\Q1997\\E", "shortCiteRegEx": "Bertsekas.", "year": 1997}, {"title": "A convergent incremental gradient method with a constant step size", "author": ["D. Blatt", "A.O. Hero", "H. Gauchman"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Blatt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blatt et al\\.", "year": 2007}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": null, "citeRegEx": "Bottou and Bousquet.,? \\Q2007\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2007}, {"title": "Accelerated stochastic approximation", "author": ["B. Delyon", "A. Juditsky"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Delyon and Juditsky.,? \\Q1993\\E", "shortCiteRegEx": "Delyon and Juditsky.", "year": 1993}, {"title": "Optimal learning rates for least squares SVMs using Gaussian kernels", "author": ["M. Eberts", "I. Steinwart"], "venue": null, "citeRegEx": "Eberts and Steinwart.,? \\Q2011\\E", "shortCiteRegEx": "Eberts and Steinwart.", "year": 2011}, {"title": "Hybrid deterministic-stochastic methods for data fitting", "author": ["M.P. Friedlander", "M. Schmidt"], "venue": "SIAM Journal of Scientific Computing,", "citeRegEx": "Friedlander and Schmidt.,? \\Q2012\\E", "shortCiteRegEx": "Friedlander and Schmidt.", "year": 2012}, {"title": "Optimal stochastic\u2018 approximation algorithms for strongly convex stochastic composite optimization", "author": ["S. Ghadimi", "G. Lan"], "venue": "Optimization Online,", "citeRegEx": "Ghadimi and Lan.,? \\Q2010\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2010}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": null, "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Accelerated stochastic approximation", "author": ["H. Kesten"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Kesten.,? \\Q1958\\E", "shortCiteRegEx": "Kesten.", "year": 1958}, {"title": "Stochastic approximation and recursive algorithms and applications", "author": ["H.J. Kushner", "G. Yin"], "venue": "Springer-Verlag, Second edition,", "citeRegEx": "Kushner and Yin.,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin.", "year": 2003}, {"title": "Asymptotically optimal regularization in smooth parametric models", "author": ["P. Liang", "F. Bach", "M.I. Jordan"], "venue": null, "citeRegEx": "Liang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": null, "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "Convergence rate of incremental subgradient algorithms. In Stochastic Optimization: Algorithms and Applications, pages 263\u2013304", "author": ["A. Nedic", "D. Bertsekas"], "venue": "Kluwer Academic,", "citeRegEx": "Nedic and Bertsekas.,? \\Q2000\\E", "shortCiteRegEx": "Nedic and Bertsekas.", "year": 2000}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A. Nemirovski", "D.B. Yudin"], "venue": null, "citeRegEx": "Nemirovski and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1983}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O(1/k)", "author": ["Y. Nesterov"], "venue": "Doklady AN SSSR,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Mathematical programming,", "citeRegEx": "Nesterov.,? \\Q2009\\E", "shortCiteRegEx": "Nesterov.", "year": 2009}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "CORE Discussion Paper,", "citeRegEx": "Nesterov.,? \\Q2010\\E", "shortCiteRegEx": "Nesterov.", "year": 2010}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Polyak and Juditsky.,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky.", "year": 1992}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Robbins and Monro.,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Local gain adaptation in stochastic gradient descent", "author": ["N. Schraudolph"], "venue": "ICANN,", "citeRegEx": "Schraudolph.,? \\Q1999\\E", "shortCiteRegEx": "Schraudolph.", "year": 1999}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Incremental gradient algorithms with stepsizes bounded away from zero", "author": ["M. Solodov"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Solodov.,? \\Q1998\\E", "shortCiteRegEx": "Solodov.", "year": 1998}, {"title": "Fast rates for regularized objectives", "author": ["K. Sridharan", "S. Shalev-Shwartz", "N. Srebro"], "venue": null, "citeRegEx": "Sridharan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2008}, {"title": "Variable metric stochastic approximation theory", "author": ["P. Sunehag", "J. Trumpf", "S. Vishwanathan", "N. Schraudolph"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Sunehag et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sunehag et al\\.", "year": 2009}, {"title": "A scalable modular convex solver for regularized risk", "author": ["C.H. Teo", "Q. Le", "A.J. Smola", "S.V.N. Vishwanathan"], "venue": null, "citeRegEx": "Teo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teo et al\\.", "year": 2007}, {"title": "An incremental gradient(-projection) method with momentum term and adaptive stepsize rule", "author": ["P. Tseng"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Tseng.,? \\Q1998\\E", "shortCiteRegEx": "Tseng.", "year": 1998}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}], "referenceMentions": [{"referenceID": 23, "context": "The most wildly successful class of algorithms for taking advantage of this type of problem structure are stochastic gradient (SG) methods Robbins and Monro [1951], Bottou and LeCun [2003].", "startOffset": 139, "endOffset": 164}, {"referenceID": 23, "context": "The most wildly successful class of algorithms for taking advantage of this type of problem structure are stochastic gradient (SG) methods Robbins and Monro [1951], Bottou and LeCun [2003]. Although the theory behind SG methods allows them to be applied more generally, in the context of machine learning SG methods are typically used to solve the problem of optimizing a sample average over a finite training set, i.", "startOffset": 139, "endOffset": 189}, {"referenceID": 29, "context": "An extensive list of convex loss functions used in machine learning is given by Teo et al. [2007], and we can even include non-smooth loss functions (or regularizers) by using smooth approximations.", "startOffset": 80, "endOffset": 98}, {"referenceID": 15, "context": "Under certain assumptions this convergence rate is optimal for strongly-convex optimization in a model of computation where the algorithm only accesses the function through unbiased measurements of its objective and gradient (see Nemirovski and Yudin [1983], Nemirovski et al.", "startOffset": 230, "endOffset": 258}, {"referenceID": 15, "context": "Under certain assumptions this convergence rate is optimal for strongly-convex optimization in a model of computation where the algorithm only accesses the function through unbiased measurements of its objective and gradient (see Nemirovski and Yudin [1983], Nemirovski et al. [2009], Agarwal et al.", "startOffset": 230, "endOffset": 284}, {"referenceID": 0, "context": "[2009], Agarwal et al. [2012]).", "startOffset": 8, "endOffset": 30}, {"referenceID": 3, "context": "The primay contribution of this work is the analysis of a new algorithm that we call the stochastic average gradient (SAG) method, a randomized variant of the incremental aggregated gradient (IAG) method Blatt et al. [2007], which combines the low iteration cost of SG methods with a linear convergence rate as in FG methods.", "startOffset": 204, "endOffset": 224}, {"referenceID": 30, "context": "x = x \u2212 \u03b1kf \u2032 ik(x ) + \u03b2k(x k \u2212 xk\u22121), see Tseng [1998]. It is common to set all \u03b2k = \u03b2 for some constant \u03b2, and in this case we can rewrite the SG with momentum method as x = x \u2212 \u2211k j=1 \u03b1j\u03b2 k\u2212jf \u2032 ij (x ).", "startOffset": 43, "endOffset": 56}, {"referenceID": 18, "context": "This approach is used in the dual averaging method Nesterov [2009], and while this averaging procedure leads to convergence for a constant step size and can improve the constants in the convergence rate Xiao [2010], it does not improve on the O(1/k) rate.", "startOffset": 51, "endOffset": 67}, {"referenceID": 18, "context": "This approach is used in the dual averaging method Nesterov [2009], and while this averaging procedure leads to convergence for a constant step size and can improve the constants in the convergence rate Xiao [2010], it does not improve on the O(1/k) rate.", "startOffset": 51, "endOffset": 215}, {"referenceID": 20, "context": "With a suitable choice of step-sizes, this gives the same asymptotic efficiency as Newton-like second-order SG methods and also leads to increased robustness of the convergence rate to the exact sequence of step sizes Polyak and Juditsky [1992]. Baher\u2019s method [Kushner and Yin, 2003, \u00a71.", "startOffset": 218, "endOffset": 245}, {"referenceID": 9, "context": "The epoch SG method uses averaging to obtain the O(1/k) rate even for non-smooth objectives Hazan and Kale [2011]. However, the convergence rates of these averaging methods remain sublinear.", "startOffset": 92, "endOffset": 114}, {"referenceID": 16, "context": "Stochastic versions of FG methods: Various options are available to accelerate the convergence of the FG method for smooth functions, such as the accelerated full gradient (AFG) method Nesterov [1983], as well as classical techniques based on quadratic approximations such as non-linear conjugate gradient, quasi-Newton, and Hessian-free Newton methods.", "startOffset": 185, "endOffset": 201}, {"referenceID": 16, "context": "Stochastic versions of FG methods: Various options are available to accelerate the convergence of the FG method for smooth functions, such as the accelerated full gradient (AFG) method Nesterov [1983], as well as classical techniques based on quadratic approximations such as non-linear conjugate gradient, quasi-Newton, and Hessian-free Newton methods. Several authors have analyzed stochastic variants of these algorithms Schraudolph [1999], Sunehag et al.", "startOffset": 185, "endOffset": 443}, {"referenceID": 16, "context": "Stochastic versions of FG methods: Various options are available to accelerate the convergence of the FG method for smooth functions, such as the accelerated full gradient (AFG) method Nesterov [1983], as well as classical techniques based on quadratic approximations such as non-linear conjugate gradient, quasi-Newton, and Hessian-free Newton methods. Several authors have analyzed stochastic variants of these algorithms Schraudolph [1999], Sunehag et al. [2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010].", "startOffset": 185, "endOffset": 466}, {"referenceID": 8, "context": "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010].", "startOffset": 8, "endOffset": 31}, {"referenceID": 8, "context": "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010].", "startOffset": 8, "endOffset": 47}, {"referenceID": 8, "context": "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al.", "startOffset": 8, "endOffset": 60}, {"referenceID": 8, "context": "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al. [2009]. Alternately, if we split the convergence rate into a deterministic and stochastic part, these methods can improve the dependency on the deterministic part Ghadimi and Lan [2010], Xiao [2010].", "startOffset": 8, "endOffset": 158}, {"referenceID": 8, "context": "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al. [2009]. Alternately, if we split the convergence rate into a deterministic and stochastic part, these methods can improve the dependency on the deterministic part Ghadimi and Lan [2010], Xiao [2010].", "startOffset": 8, "endOffset": 337}, {"referenceID": 8, "context": "[2009], Ghadimi and Lan [2010], Martens [2010], Xiao [2010]. Under certain conditions these variants are convergent with an O(1/k) rate Sunehag et al. [2009]. Alternately, if we split the convergence rate into a deterministic and stochastic part, these methods can improve the dependency on the deterministic part Ghadimi and Lan [2010], Xiao [2010]. However, as with all other methods we have discussed thus far in this section, we are not aware of any existing method of this flavor that improves on the O(1/k) rate.", "startOffset": 8, "endOffset": 350}, {"referenceID": 2, "context": "Constant step size: If the SG iterations are used with a constant step size (rather than a decreasing sequence), then the convergence rate of the method can be split into two parts [Nedic and Bertsekas, 2000, Proposition 2.4], where the first part depends on k and converges linearly to 0 and the second part is independent of k but does not converge to 0. Thus, with a constant step size the SG iterations have a linear convergence rate up to some tolerance, and in general after this point the iterations do not make further progress. Indeed, convergence of the basic SG method with a constant step size has only been shown under extremely strong assumptions about the relationship between the functions fi Solodov [1998]. This contrasts with the method we present in this work which converges to the optimal solution using a constant step size and does so with a linear rate (without additional assumptions).", "startOffset": 192, "endOffset": 724}, {"referenceID": 9, "context": "In particular, accelerated SG methods use a constant step size by default, and only decrease the step size on iterations where the inner-product between successive gradient estimates is negative Kesten [1958], Delyon and Juditsky [1993].", "startOffset": 195, "endOffset": 209}, {"referenceID": 5, "context": "In particular, accelerated SG methods use a constant step size by default, and only decrease the step size on iterations where the inner-product between successive gradient estimates is negative Kesten [1958], Delyon and Juditsky [1993]. This leads to convergence of the method and allows it to potentially achieve periods of linear convergence where the step size stays constant.", "startOffset": 210, "endOffset": 237}, {"referenceID": 2, "context": "functions Bertsekas [1997]. However, the weighting is numerically unstable and the linear convergence rate treats full passes through the data as iterations.", "startOffset": 10, "endOffset": 27}, {"referenceID": 2, "context": "functions Bertsekas [1997]. However, the weighting is numerically unstable and the linear convergence rate treats full passes through the data as iterations. A related strategy is to group the fi functions into \u2018batches\u2019 of increasing size and perform SG iterations on the batches Friedlander and Schmidt [2012]. In both cases, the iterations that achieve the linear rate have a cost that is not independent of n, as opposed to SAG.", "startOffset": 10, "endOffset": 312}, {"referenceID": 3, "context": "Incremental Aggregated Gradient: Finally, Blatt et al. presents the most closely-related algorithm, the IAG method Blatt et al. [2007]. This method is identical to the SAG iteration (5), but uses a cyclic choice of ik rather than sampling the ik values.", "startOffset": 42, "endOffset": 135}, {"referenceID": 17, "context": "\u2013 AFG: Nesterov\u2019s accelerated full gradient method Nesterov [1983], where iterations of (3) with a fixed step size are interleaved with an extrapolation step, and we use an adaptive line-search based on Liu et al.", "startOffset": 7, "endOffset": 67}, {"referenceID": 13, "context": "\u2013 AFG: Nesterov\u2019s accelerated full gradient method Nesterov [1983], where iterations of (3) with a fixed step size are interleaved with an extrapolation step, and we use an adaptive line-search based on Liu et al. [2009].", "startOffset": 203, "endOffset": 221}, {"referenceID": 31, "context": "\u2013 RDA: The regularized dual averaging method Xiao [2010], another recent state-of-the-art SG method.", "startOffset": 45, "endOffset": 57}, {"referenceID": 9, "context": "\u2013 ESG: The epoch SG method Hazan and Kale [2011], which runs SG with a constant step size and averaging in a series of epochs, and is optimal for non-smooth stochastic strongly-convex optimization.", "startOffset": 27, "endOffset": 49}, {"referenceID": 8, "context": "\u2013 NOSG: The nearly-optimal SG method Ghadimi and Lan [2010], which combines ideas from SG and AFG methods to obtain a nearly-optimal dependency on a variety of problem-dependent constants.", "startOffset": 37, "endOffset": 60}, {"referenceID": 18, "context": "AFG: The accelerated full gradient method Nesterov [1983], where iterations of (3) are interleaved with an extrapolation step.", "startOffset": 42, "endOffset": 58}, {"referenceID": 25, "context": "peg: The pegasos algorithm of Shalev-Shwartz et al. [2007], but where we multiply the step size by a constant.", "startOffset": 30, "endOffset": 59}, {"referenceID": 3, "context": "IAG: The incremental aggregated gradient method of Blatt et al. [2007] described by iteration (5) but with a cyclic choice of ik.", "startOffset": 51, "endOffset": 71}, {"referenceID": 12, "context": "In low-dimensional settings, the optimal regularization parameter is of the form C/n Liang et al. [2009] where C is a scalar constant, and may thus violate the constraint.", "startOffset": 85, "endOffset": 105}, {"referenceID": 26, "context": "For example, Sridharan et al. [2008] considers parameters of the form \u03bb = C \u221a n in the parametric setting, while Eberts and Steinwart [2011] considers \u03bb = C n\u03b2 with \u03b2 < 1 in a non-parametric setting.", "startOffset": 13, "endOffset": 37}, {"referenceID": 6, "context": "[2008] considers parameters of the form \u03bb = C \u221a n in the parametric setting, while Eberts and Steinwart [2011] considers \u03bb = C n\u03b2 with \u03b2 < 1 in a non-parametric setting.", "startOffset": 83, "endOffset": 111}, {"referenceID": 4, "context": "However, as shown in our experiments, the testing cost of the SAG iterates often reaches its minimum quicker than existing SG methods, and we could expect to improve the constant in the O(1/k) convergence rate, as is the case with online second-order methods Bottou and Bousquet [2007].", "startOffset": 259, "endOffset": 286}, {"referenceID": 1, "context": "Denoting \u03b4k = E\u2016x\u0303 \u2212 x\u2217\u20162, we have (following Bach and Moulines [2011]) \u03b4k 6 \u03b4k\u22121 \u2212 2\u03b3k(1\u2212 \u03b3kL)E [ g\u2032(x\u0303k\u22121)>(x\u0303k\u22121 \u2212 x\u2217) ] + 2\u03b3 k\u03c3 2 .", "startOffset": 46, "endOffset": 71}], "year": 2013, "abstractText": "We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.", "creator": "LaTeX with hyperref package"}}}