{"id": "1202.6386", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2012", "title": "Relational Reinforcement Learning in Infinite Mario", "abstract": "Relational representations in reinforcement learning allow for the use of structural information like the presence of objects and relationships between them in the description of value functions. Through this paper, we show that such representations allow for the inclusion of background knowledge that qualitatively describes a state and can be used to design agents that demonstrate learning behavior in domains with large state and actions spaces such as computer games.", "histories": [["v1", "Tue, 28 Feb 2012 21:36:22 GMT  (263kb)", "http://arxiv.org/abs/1202.6386v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shiwali mohan", "john e laird"], "accepted": true, "id": "1202.6386"}, "pdf": {"name": "1202.6386.pdf", "metadata": {"source": "CRF", "title": "Relational Reinforcement Learning in Infinite Mario", "authors": ["Shiwali Mohan", "John E. Laird"], "emails": ["laird}@umich.edu"], "sections": [{"heading": null, "text": "IntroductionComputer games have continuous, enormous state spaces, large action spaces, and are characterized by complex relationships between their components. Without the use of abstractions, learning in a computer game area becomes unfeasible. Through this work, we examine some designs that allow tractable learning of reinforcements in symbolic actors operating in complex areas. We show that the imposition of hierarchies for actions and tasks limits the state space, making learning faster. We also show that relational representation allows the use of structural formations, such as the presence of objects with certain characteristics or relationships between objects in the description of derivative politics. Soar (Laird, 2008) is a symbolic architecture that has been used in many computer game domains to design intelligent agents. Reinforcement learning has recently been implemented in architecture, and we investigate agent designs that allow a symbolic actor to learn state action associations by exploring a new and simultaneously anticipated environment."}, {"heading": "State Representation", "text": "As input from the environment, the agent has access to a 16x22 character arrangement, each element of which corresponds to a tile on the visual scene and the value of the elementCopyright \u00a9 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. The reward structure varies according to the type of tile (coin, bricks, etc.). The agent also has access to the locations of different types of monsters (including Mario) on the scene, along with their vertical and horizontal speeds. The reward structure varies in different instances of the game. As a measure of structuring the knowledge that a soar agent has about the environment, we have moved from the low-level tile for tile representation of visual space to a coding composed of objects and their relationships to Mario and to each other. Knowing that \"the tile at the position (x, y) of type (t) is converted from one direction to another, there is a horizontal in one direction.\""}, {"heading": "Hierarchical Reinforcement Learning", "text": "We have a hierarchy of operators based on a GOMS analysis by Mario (John et al., 1990). The authors have shown that the behavior of a Soar agent who used simple, hand-encrypted heuristics was predictable for the behavior of a human expert who learned to play near keyboard instruments (KLO). A KLO is a primitive, atomic action available to a human user; an action that can be performed with a keyboard or joystick. These actions include a movement to the right or to the left shown in the figure."}, {"heading": "Results", "text": "Figure 2 shows the performance of the Soar RL agent as he learns selection knowledge for both the FLO's and the KLO's, compared to a hand-coded sample agent distributed with the Mario environment."}, {"heading": "Conclusions and Future Work", "text": "The current agent design assumes that the agent can interact independently with various objects in his environment. This assumption applies in scenarios where interaction with an object is more important than dealing with other nearby objects, such as when there is a coin and a monster nearby, dealing with the monster takes precedence over grabbing the coin. However, if several objects have a similar meaning, such as when there are two monsters nearby, the adoption collapses and the agent does not learn a viable strategy."}], "references": [{"title": "Extending the Soar Cognitive Architecture", "author": ["J.E. Laird"], "venue": "In", "citeRegEx": "Laird,? 2008", "shortCiteRegEx": "Laird", "year": 2008}, {"title": "Soar-RL, integrating reinforcement", "author": ["S. Nason", "J. Laird"], "venue": null, "citeRegEx": "Nason and Laird,? \\Q2005\\E", "shortCiteRegEx": "Nason and Laird", "year": 2005}, {"title": "Towards Real-time GOMS", "author": ["Machine Learning", "B.E. Finland. John", "A.H. Vera", "A. Newell"], "venue": "Technical Report, CMU-SCS-90-95, School of Computer Science, Carnegie Mellon University.", "citeRegEx": "Learning et al\\.,? 1990", "shortCiteRegEx": "Learning et al\\.", "year": 1990}, {"title": "Online Q-Learning Using", "author": ["R.A. Rummery", "M. Niranjan"], "venue": null, "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "Soar (Laird, 2008) is a symbolic architecture that has been used to design intelligent agents in many computer game domains.", "startOffset": 5, "endOffset": 18}, {"referenceID": 1, "context": "We have been working with Soar-RL (Nason and Laird, 2005) agents operating in Infinite Mario domain from RL Competition 2009.", "startOffset": 34, "endOffset": 57}, {"referenceID": 3, "context": "We have used SARSA (Rummery and Niranjan, 1994) for Soar agents for this domain.", "startOffset": 19, "endOffset": 47}], "year": 2010, "abstractText": "Relational representations in reinforcement learning allow for the use of structural information like the presence of objects and relationships between them in the description of value functions. Through this paper, we show that such representations allow for the inclusion of background knowledge that qualitatively describes a state and can be used to design agents that demonstrate learning behavior in domains with large state and actions spaces such as computer games.", "creator": "PrimoPDF http://www.primopdf.com"}}}