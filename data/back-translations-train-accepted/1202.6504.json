{"id": "1202.6504", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Feb-2012", "title": "Learning from Distributions via Support Measure Machines", "abstract": "This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.", "histories": [["v1", "Wed, 29 Feb 2012 10:09:26 GMT  (101kb)", "https://arxiv.org/abs/1202.6504v1", "Initial submission"], ["v2", "Sat, 12 Jan 2013 12:43:09 GMT  (104kb)", "http://arxiv.org/abs/1202.6504v2", "Advances in Neural Information Processing Systems 25"]], "COMMENTS": "Initial submission", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["krikamol muandet", "kenji fukumizu", "francesco dinuzzo", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1202.6504"}, "pdf": {"name": "1202.6504.pdf", "metadata": {"source": "CRF", "title": "Learning from Distributions via Support Measure Machines", "authors": ["Krikamol Muandet"], "emails": ["krikamol@tuebingen.mpg.de", "fukumizu@ism.ac.jp", "fdinuzzo@tuebingen.mpg.de", "bs@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "ar Xiv: 120 2.65 04v2 [st at.M L] 1"}, {"heading": "1 Introduction", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "2 Regularization on probability distributions", "text": "The aim of this work is to learn a series of examples in which we focus on the binary classification problem, i.e., Y = {+ 1}, in order to learn from the distributions in which we not only preserve necessary information about individual distributions, but also allow efficient compression. That is, we adopt a Hilbert spatial description to represent the distribution as an intermediate function in an RKHS [8, 9]."}, {"heading": "3 Kernels on probability distributions", "text": "Since the board (1) in P is linear, the optimization of functionality (5) boils down to finding a function in Q = > Q that approximates the functions from P to R in function class F < P > X > P (P), where C (P) is a class of limited continuous functions on X. < P (P), where C (P) is a class of limited continuous functions on P, endowed with the topology of weak convergence and the associated Borel algebra, follows that the relationship between the RKHS H caused by the kernel k and the function class F.Lemma 2. Assuming that X is compact, the RKHS H induced by a kernel is in F dense."}, {"heading": "3.1 Support measure machines", "text": "In its general form, an SMM amounts to solving an SVM problem with the expected kernel K (P, Q) = Ex-P, z-Q [k (x, z)], which can be calculated in closed form for certain classes of distributions and cores k. Examples are in Table 1. Alternatively, you can approximate the kernel K (P, Q) by empirical estimation: Kemp (P, n, Q, m) = 1n \u00b7 mn \u2211 i = 1 (1, p) = 1k (xi, zj) (4), where P, n and Q, m are empirical distributions of P and Q, while random samples {xi} ni = 1 and {zj} mj = 1, or a finite sample of size m from a distribution P, are sufficient to calculate an approximation within an error."}, {"heading": "4 Theoretical analyses", "text": "This section presents the most important theoretical aspects of the proposed framework, which show an important link between kernel-based learning algorithms and the space of distributions and the input space on which they are defined."}, {"heading": "4.1 Risk deviation bound", "text": "Considering a training sample (Pi, yi) mi = 1 drawn i.i.d.d. (from some unknown probability distribution P to P \u00b7 Y, a loss function: R \u00b7 R, and a function class, the goal of statistical learning is to find the function f \u00b2 that functionally minimizes the expected risk R (f) = P X (y, f (x))) dP (x) dP (P, y). Since P is unknown, the empirical risk Remp (f) = 1 m \u00b2 m i = 1 X (yi, f (x)) dPi (x) based on the training sample can instead be considered functional. In addition, the risk distribution can be further simplified by adding 1 m \u00b7 n \u00b2 i = 1 xij \u00b2 (yi, f (xij)) based on n samples x x."}, {"heading": "4.2 Flexible support vector machines", "text": "It turns out that for certain distribution options P, the linear SMM formed on the basis of {(Pi, yi)} mi = 1 corresponds to an SVM formed on the basis of some examples {(xi, yi)} mi = 1 with an appropriate choice of core function. Lemma 4. Let us consider k (x, yi) as a limited p.d. kernel on a measurement space designed so that each Pi < and g (x, x) will be a quadratically integrable function, so that g (x, x) dx is designed for all x. If we consider a sample {(Pi, yi)} mi = 1, assuming that each Pi has a bandwidth specified by g (xi, x), the linear SMM corresponds to the SVM on the training sample {(xi, yi) dx)."}, {"heading": "5 Related works", "text": "The kernel K (P, Q) = < \u00b5P, \u00b5Q > H is indeed a special case of Hilbert's metrics [5], with the associated kernel K (P, Q) = EP, Q [k, x]], and a generative mean kernel (GMMK) proposed by [15]. In the GMMK, the kernel is defined between two objects x and y via p-x and p-y, which are estimated probability models of x and y, respectively. That is, a probability model p-x is learned for each example and used as a surrogate to construct the kernel between these examples."}, {"heading": "6 Experimental results", "text": "In the experiments, we primarily look at three different learning algorithms: i) SVM is considered a baseline algorithm; ii) Augmented SVM (ASVM) is an SVM that is trained on augmented samples drawn according to the distribution {Pi} mi = 1. The same number of examples are drawn from each distribution. iii) SMM is a distribution-based method that can be applied directly to distributions."}, {"heading": "6.1 Synthetic data", "text": "First, we conducted a fundamental experiment that showed a fundamental difference between SVM, ASVM, and SMM."}, {"heading": "6.2 Handwritten digit recognition", "text": "In this section, the proposed framework is applied to distributions via equivalence classes of images that are invariant to basic transformations, namely, scaling, translation, and rotation. We look at the handwritten digits obtained from the USPS dataset. For each 16 \u00d7 16 image, the distribution via virtual examples of transformations is determined by a previous parameter associated with such transformations. Scaling and translation are determined by the scale factors (sx, sy) and shifts (tx, ty) along the x- and y-axes. Rotation is parameterized by an angle in which the data is constituted. We assume Gaussian distributions as previous distributions, including digital classes (1, 1 \u00b7 I2), N (0, 0 \u00b7 I2), and N (0; SMIT)."}, {"heading": "6.3 Natural scene categorization", "text": "This section illustrates the advantages of the non-linear kernel between distributions for learning hi hi hi-categories (four categories), in which the Pocket of the Word (BoW) representation is used to represent images in the dataset. Each image is presented as a collection of local patches, each one representing a codeword from a large vocabulary of code words referred to as codewords. Standard BoW representations encode each image as a histogram listing the likelihood of local patches occurring that are recognized in the codebook. On the other hand, our setting presents each image as a distribution across those codewords. Thus, images of different scenes tend to generate different sets of patches. Based on this representation, both the histogram and the local patches can be found in our framework.We use the dataset presented in [19]. Depending on their results, most errors occur among the four interior categories (830 images)."}, {"heading": "7 Conclusions", "text": "The trick is to embed distributions in an RKHS, resulting in a simple and efficient learning algorithm for distributions. A family of linear and nonlinear cores on distributions allows for flexible selection of the kernel function that is suitable for the problems at hand. Our analyses provide insights into the relationships between distribution-based methods and traditional sample-based methods, in particular the flexible SVM, which allows the SVM to place different cores on each training example. Experimental results illustrate the benefits of learning from a pool of distributions compared to a pool of examples, both synthetic and real data."}, {"heading": "Acknowledgments", "text": "KM thanks Zoubin Gharamani, Arthur Gretton, Christian Walder and Philipp Hennig for a fruitful discussion and all three insightful reviewers for their valuable comments."}, {"heading": "A Proof of Theorem 1", "text": "Theorem 1: Given training examples (Pi, yi), P \u00b7 R, i = 1,.., m, a strictly monotonous increasing function (P1, y1, EP1 [f],..., Pm, ym, EPm [f]), (5) allows a representation of the form f = 1 \u03b1iuPi for some \u03b1iuPi, i = 1,..., m.Proof. Due to Proposition 2 in [10], the linear functional EP [\u00b7] is limited for all P-P. Then, given P1, P2,..., Pm, any f-H can be decomposed & & & asf = f\u00b5 + f-H, where the linear functional form EP [\u00b7] is limited for all P-P."}, {"heading": "B Proof of Theorem 3", "text": "Theorem 3: With an arbitrary probability distribution P with variance \u03c32, a Lipschitz permanent function f: R \u00b7 R with constant Cf, an arbitrary loss function: R \u00b7 R \u00b7 R, which is the Lipschitz permanent function in the second argument with constant Cf, it follows that x is distributed according to P [(y, f (x))] \u2212 (y, Ex P [f (x)] \u2212 (y, EP [f (x)]] for each y R.Proof. Let us assume that x is distributed according to P (y, EP [f (x)]."}, {"heading": "C Proof of Lemma 4", "text": "4. Let k (x, z) be a limited p.d. kernel on a measurement range that applies to all x. (Pi, yi) mi = 1, assuming that each pi has a square integrative function, so that the linear SMM corresponds to the SVM in the training sample. (xi, yi) mi = 1 with kernel Kg (x, z) mi = 1 with kernel Kg (x, z) g (x, x) g (z, z) dx, dz).Proof. For a training sample {(xi, yi)} mi = 1 with kernel Kg (x, z) = 1 with kernel Kg (x, z) g (z).Proof."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "This paper presents a kernel-based discriminative learning framework on prob-<lb>ability measures. Rather than relying on large collections of vectorial training<lb>examples, our framework learns using a collection of probability distributions<lb>that have been constructed to meaningfully represent training data. By represent-<lb>ing these probability distributions as mean embeddings in the reproducing kernel<lb>Hilbert space (RKHS), we are able to apply many standard kernel-based learning<lb>techniques in straightforward fashion. To accomplish this, we construct a gener-<lb>alization of the support vector machine (SVM) called a support measure machine<lb>(SMM). Our analyses of SMMs provides several insights into their relationship<lb>to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-<lb>SVM) that places different kernel functions on each training example. Experi-<lb>mental results on both synthetic and real-world data demonstrate the effectiveness<lb>of our proposed framework.", "creator": "gnuplot 4.4 patchlevel 3"}}}