{"id": "1203.1007", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2012", "title": "Agnostic System Identification for Model-Based Reinforcement Learning", "abstract": "A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.", "histories": [["v1", "Mon, 5 Mar 2012 18:58:49 GMT  (149kb,D)", "https://arxiv.org/abs/1203.1007v1", "17 pages"], ["v2", "Tue, 3 Jul 2012 13:48:40 GMT  (127kb,D)", "http://arxiv.org/abs/1203.1007v2", "8 pages, published in ICML 2012"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY stat.ML", "authors": ["st\u00e9phane ross", "drew bagnell"], "accepted": true, "id": "1203.1007"}, "pdf": {"name": "1203.1007.pdf", "metadata": {"source": "META", "title": "Agnostic System Identification for Model-Based Reinforcement Learning", "authors": ["St\u00e9phane Ross", "Andrew Bagnell"], "emails": ["STEPHANEROSS@CMU.EDU", "DBAGNELL@RI.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Background and Notation", "text": "We assume that the real system behaves according to some unknown MDP, represented by a number of states S and measures A (both potentially infinite and continuous), a transitional function T, where Tsa denotes the next state distribution when we take action in states, and the initial state distribution \u00b5 at time 1. We assume that the cost function C: S \u00b7 A \u2192 R is known and seeks to minimize the expected sum of discounted costs over an infinite horizon at a discount."}, {"heading": "3. A Simple Batch Algorithm", "text": "We now describe a simple algorithm called a batch, which can be used to analyze many common approaches from literature, such as learning from a generative model2, stimulation through open loops, or observation by an expert (Ljung, 1999). Let T specify the class of transition models under consideration, and \u03bd an exploration distribution between state and action from which we can scan the system. Batch first executes state action pairs in the real system m, which are i.i.d. sampled to obtain m-sampled transitions, then finds the best model T-T of the observed transitions, and solves (potentially roughly) the optimal control (OC) problem with T and the known cost function C to return a policy for test execution."}, {"heading": "3.1. Analysis", "text": "Our reduction analysis aims to answer the following question: If we define a Model T with small errors in the analysis of terms between terms, we can set the system to any state, perform any action to make a pattern transition. We measure the quality of the OC problem as follows: What guarantees does it provide for the control performance of \u03c0? Our results illustrate the disadvantages of a purely batch method due to the mismatch in the distribution of the tensile test. We measure the quality of the OC problem as follows: What policy examples are there for minimal distribution losses? However, let us assume that the disadvantages of a purely batch method are due to the imbalance in the distribution of the tensile test. We measure the quality of the OC problem as follows: lots for minimal distribution losses."}, {"heading": "4. No-Regret Methods for Agnostic MBRL", "text": "We have the opportunity to solve the problem with the MBRL constellation as follows: \"We could solve the problem with the MBRL constellation as follows:\" We could solve the problem with the MBRL constellation as follows. \"\" We cannot solve it. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" We. \"\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \""}, {"heading": "4.1. Analysis", "text": "Similar to our analysis of batch, we try to answer the following questions: If there is a low error model of the training data, and we solve each OC problem well, what guarantees DAgger for control performance? Our results show that by sampling data from the policies learned, DAgger provides guarantees that do not have a traction test mismatch factor, which leads to improved performance. For each policy performance, we define how well we have solved each OC problem on average via iterations. If we have found an i-optimal policy within any class of policies for each iteration i (s), we are each the value function of the two models T (s). These measures measure how well we iterate each OC problem on average over the iterations i (s)."}, {"heading": "5. Discussion", "text": "We emphasize that we offer reduction-style guarantees. DAgger can sometimes not find good strategies, e.g. when no model in the class achieves small errors in training data. If the latter occurs, we need a better model class. In contrast, batch can find models with low training errors, but still cannot find good strategies with good control performance due to traction / test errors. This happens even in scenarios where DAgger finds good strategies, as shown in the experiments.DAgger has to solve many OC problems, which can be computationally expensive, e.g. with nonlinear or high-dimensional models. Many approximate methods can be used, e.g. political gradients (Williams, 1992), customized value iteration as optimal value iteration."}, {"heading": "6. Experiments on Helicopter Domain", "text": "We demonstrate the effectiveness of the DAgger on a challenging problem: learning to perform flight maneuvers with a simulated helicopter using the simulator of Abbeel & Ng (2005), which has a continuous 21-dimensional state and a 4-dimensional control room. We consider learning to 1) hover and 2) perform a \"nose-in-funnel\" maneuver. We compare DAgger with various methods that add an additional white Gaussian Noise5 to each state and action along the desired trajectory, 2) seek an expert controller and 3) compare the expert controller with additional white Gaussian Noise6 in the expert's controls. The expert controller is achieved by linearising the true model of the desired trajectory and solving the LQR (iLQR for the nose-in funnel). We also compare against Abbeel's algorithm, where the expert will only attempt the first itations."}, {"heading": "7. Conclusion", "text": "It is easy to implement, formalize, and algorithmically do the technical practice of iterating between controller synthesis and system identification, and can be applied to any control problem where an approximate solution to the OC problem is feasible. In addition, sample complexity with the Model9scales the same as CPI, except that the gradient descent is performed directly on the deterministic linear controller. We solve a linear system to estimate the gradient from sample costs with disturbed parameters. 10Zero-mean spherical Gaussian with a standard deviation of 0.1 on the forces and torques applied to the helicopter at each step. 11For each step t we learn offset matrices A \u2032 t, B \u2032 t so that the gradient is estimated from sample costs with disturbed parameters."}, {"heading": "Acknowledgements", "text": "This work is supported by the ONR-MURI grant N0001409-1-1052, Reasoning in Reduced Information Spaces."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Abbeel and Ng,? \\Q2005\\E", "shortCiteRegEx": "Abbeel and Ng", "year": 2005}, {"title": "Robot learning from demonstration", "author": ["C.G. Atkeson", "S. Schaal"], "venue": "In ICML,", "citeRegEx": "Atkeson and Schaal,? \\Q1997\\E", "shortCiteRegEx": "Atkeson and Schaal", "year": 1997}, {"title": "Policy search by dynamic programming", "author": ["J.A. Bagnell", "A.Y. Ng", "S. Kakade", "J. Schneider"], "venue": "In NIPS,", "citeRegEx": "Bagnell et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2003}, {"title": "Error limiting reductions between classification tasks", "author": ["A. Beygelzimer", "V. Dani", "T. Hayes", "J. Langford", "B. Zadrozny"], "venue": "In ICML,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2005}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal"], "venue": "In COLT,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["S. Kakade", "J. Langford"], "venue": "In ICML,", "citeRegEx": "Kakade and Langford,? \\Q2002\\E", "shortCiteRegEx": "Kakade and Langford", "year": 2002}, {"title": "Mind the duality gap: Logarithmic regret algorithms for online optimization", "author": ["S. Kakade", "S. Shalev-Shwartz"], "venue": "In NIPS,", "citeRegEx": "Kakade and Shalev.Shwartz,? \\Q2008\\E", "shortCiteRegEx": "Kakade and Shalev.Shwartz", "year": 2008}, {"title": "Iterative linear quadratic regulator design for nonlinear biological movement systems", "author": ["W. Li", "E. Todorov"], "venue": "In ICINCO,", "citeRegEx": "Li and Todorov,? \\Q2004\\E", "shortCiteRegEx": "Li and Todorov", "year": 2004}, {"title": "System Identification: Theory for the User", "author": ["L. Ljung"], "venue": null, "citeRegEx": "Ljung,? \\Q1999\\E", "shortCiteRegEx": "Ljung", "year": 1999}, {"title": "Helicopter learning nose-in funnel", "author": ["S. Ross"], "venue": "URL http: //www.youtube.com/user/icml12rl", "citeRegEx": "Ross,? \\Q2012\\E", "shortCiteRegEx": "Ross", "year": 2012}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G. Gordon", "J.A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A.L. Strehl", "L. Li", "M.L. Littman"], "venue": null, "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Finite time bounds for sampling based fitted value iteration", "author": ["C. Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "Szepesv\u00e1ri,? \\Q2005\\E", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2005}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "Szita and Szepesv\u00e1ri,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri", "year": 2010}, {"title": "Agnostic kwik learning and efficient approximate reinforcement learning", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In COLT,", "citeRegEx": "Szita and Szepesv\u00e1ri,? \\Q2011\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri", "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}], "referenceMentions": [{"referenceID": 9, "context": "This problem is fully appreciated in the system identification literature and has been attacked by considering \u201copen loop\u201d identification procedures and \u201cpersistent excitation\u201d (Ljung, 1999; Abbeel & Ng, 2005) that attempt to sufficiently \u201ccover\u201d the state-action space.", "startOffset": 177, "endOffset": 209}, {"referenceID": 3, "context": "We adopt a reduction-based analysis (Beygelzimer et al., 2005) that relates the learned policy\u2019s performance to prediction error during training.", "startOffset": 36, "endOffset": 62}, {"referenceID": 11, "context": "This approach is inspired by a recent reduction of imitation learning to no-regret online learning (Ross et al., 2011) that addresses mismatch between train/test distributions.", "startOffset": 99, "endOffset": 118}, {"referenceID": 2, "context": "This enables MBRL methods to match the strongest existing agnostic guarantees of model-free RL methods (Kakade & Langford, 2002; Bagnell et al., 2003).", "startOffset": 103, "endOffset": 150}, {"referenceID": 13, "context": ", using dynamic programming (Puterman, 1994) or approximate methods (Szepesv\u00e1ri, 2005; Williams, 1992).", "startOffset": 68, "endOffset": 102}, {"referenceID": 16, "context": ", using dynamic programming (Puterman, 1994) or approximate methods (Szepesv\u00e1ri, 2005; Williams, 1992).", "startOffset": 68, "endOffset": 102}, {"referenceID": 10, "context": "Similarly, the Dataset Aggregation (DAgger) algorithm of Ross et al. (2011) uses a similar data aggregation procedure over iterations to obtain policies that mimic an expert well in imitation learning.", "startOffset": 57, "endOffset": 76}, {"referenceID": 5, "context": "gorithm (Hazan et al., 2006; Kakade & Shalev-Shwartz, 2008), more specifically, Follow-the-(Regularized)-Leader (Hazan et al.", "startOffset": 8, "endOffset": 59}, {"referenceID": 5, "context": ", 2006; Kakade & Shalev-Shwartz, 2008), more specifically, Follow-the-(Regularized)-Leader (Hazan et al., 2006), and that using any no-regret online algorithm ensures good performance.", "startOffset": 91, "endOffset": 111}, {"referenceID": 2, "context": "Methods such as Conservative Policy Iteration (CPI) (Kakade & Langford, 2002) and Policy-Search by Dynamic Programming (PSDP) (Bagnell et al., 2003) learn a policy directly by updating policy parameters iteratively.", "startOffset": 126, "endOffset": 148}, {"referenceID": 9, "context": "loop excitation or by watching an expert (Ljung, 1999).", "startOffset": 41, "endOffset": 54}, {"referenceID": 4, "context": "loss on m sampled transitions, we can still obtain good guarantees using martingale inequalities as in online-to-batch (Cesa-Bianchi et al., 2004) techniques.", "startOffset": 119, "endOffset": 146}, {"referenceID": 12, "context": "as Rmax, \u00d5( C rng|S| |A| log(1/\u03b4) 3(1\u2212\u03b3)6 ) (Strehl et al., 2009) and a", "startOffset": 44, "endOffset": 65}, {"referenceID": 16, "context": ", policy gradient (Williams, 1992), fitted value iteration (Szepesv\u00e1ri, 2005) or iLQR (Li & Todorov, 2004).", "startOffset": 18, "endOffset": 34}, {"referenceID": 13, "context": ", policy gradient (Williams, 1992), fitted value iteration (Szepesv\u00e1ri, 2005) or iLQR (Li & Todorov, 2004).", "startOffset": 59, "endOffset": 77}, {"referenceID": 10, "context": "A video comparing qualitatively the learned maneuver with DAgger and Batch is available on YouTube (Ross, 2012).", "startOffset": 99, "endOffset": 111}], "year": 2012, "abstractText": "A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a nearoptimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.", "creator": "LaTeX with hyperref package"}}}