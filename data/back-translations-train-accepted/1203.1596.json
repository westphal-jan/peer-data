{"id": "1203.1596", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2012", "title": "Multiple Operator-valued Kernel Learning", "abstract": "This paper addresses the problem of learning a finite linear combination of operator-valued kernels. We study this problem in the case of kernel ridge regression for functional responses with a lr-norm constraint on the combination coefficients. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in Brain-Computer Interface (BCI).", "histories": [["v1", "Wed, 7 Mar 2012 20:31:17 GMT  (408kb)", "https://arxiv.org/abs/1203.1596v1", "No. RR-7900 (2012)"], ["v2", "Thu, 14 Jun 2012 18:44:49 GMT  (411kb)", "http://arxiv.org/abs/1203.1596v2", "No. RR-7900 (2012)"]], "COMMENTS": "No. RR-7900 (2012)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["hachem kadri", "alain rakotomamonjy", "francis r bach", "philippe preux"], "accepted": true, "id": "1203.1596"}, "pdf": {"name": "1203.1596.pdf", "metadata": {"source": "CRF", "title": "Multiple Operator-valued Kernel Learning", "authors": ["Philippe Preux", "Hachem Kadri", "Alain Rakotomamonjy", "Francis Bach"], "emails": ["hachem.kadri@inria.fr", "alain.rakotomamonjy@insa-rouen.fr", "francis.bach@inria.fr", "philippe.preux@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 120 3.15 96v2 [st at.M L] 14 Jun 2012appor t de r r ech er ch eIS SN 0249 -639 9IS RN INR IA / R R-- 7900 --F R + EN GTh\u00e8me COG"}, {"heading": "INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Multiple Operator-valued Kernel Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Hachem Kadri \u2014 Alain Rakotomamonjy \u2014 Francis Bach \u2014 Philippe Preux", "text": "N \u00b0 790 February 2012"}, {"heading": "Centre de recherche INRIA Lille \u2013 Nord Europe Parc Scientifique de la Haute Borne", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "40, avenue Halley, 59650 Villeneuve d\u2019Ascq", "text": "Telephone: + 33 3 59 57 78 00 - Telephone: + 33 3 59 57 78 50"}, {"heading": "Multiple Operator-valued Kernel Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Hachem Kadri\u2217, Alain Rakotomamonjy\u2020, Francis Bach\u2021, Philippe", "text": "Preux \u00a7 The Equipes project SequeLRapport de recherche n \u00b0 7900 - February 2012 - 20 pages Abstract: Positive definitive operator-evaluated nuclei generalize the well-known concept of reproduction of nuclei and are naturally adapted to multilateral learning situations. This paper deals with the problem of learning a finite linear combination of infinitely dimensional operator-evaluated nuclei that are suitable for extending functional data analysis methods to non-linear contexts. We investigate this problem in the case of kernel-firm regression to functional responses with an \"r-norm limitation\" to the combination coefficients (r \u2265 1).The resulting optimization problem is more involved than the methods of multiple operator-evaluated kernel learning processes, since operatively evaluated nuclei pose more technical and theoretical problems."}, {"heading": "Apprentissage de Noyaux a\u0300 Valeurs Ope\u0301rateurs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Multiples", "text": "Re \"sume\": Dans cet article, nous suggests une me \"thode d\" apprentissage de noyaux multiples a \"valeurs ope\" rateurs dans le cas d'une re \"gression ridge a\" ponse fonctionnelle. Notre me \"thode est base\" e sur la re \"solution d'un syste\" me d'e \"quations line\" aires d'ope \"rateurs en utilisant une proce\" dure de type Iterative Coordinate Descent. Nous validons expe \"rimentalement notre approche sur un proble me de pre\" diction de mouvement de doigt dans un contexte d'Interface Cerveau-Machine.Mots-cle \"s: noyaux a\" valeurs ope \"rateurs, apprentissage de noyaux multiples, analysis des donne es fonctionnelles, espace de Hilbert a\" noyau reproductionant. \""}, {"heading": "Multiple Operator-valued Kernel Learning 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "4 Kadri, Rakotomamonjy, Bach & Preux", "text": "Formulated as an optimization problem over the cone of positive semi-defined matrices, we proposed a block descend method to solve this problem, but they did not focus on learning the input core. In contrast, our multi-operator kernel learning formulation can be considered a method for simultaneously learning input and output cores, although we consider a linear combination of cores that are pre-fixed. In this paper, we provide the following contributions: \u2022 We introduce a novel approach to infinitely dimensional kernel learning with multiple operators (MovKL), which is suitable for learning the functional dependencies and interactions between continuous data. \u2022 We extend the original formulation of comb regression in dual variables to the domain of functional data analysis, by showing how to perform non-linear functional regression with functional responses, by including a linear regression operator in an operatively evaluated kernel module feature space containing a functional module sum (section 2)."}, {"heading": "2 Problem Setting", "text": "Before describing the multi-operator kernel learning algorithm that we will explore and experiment with in this paper, we first review concepts and properties of reproducing Hilbert kernel spaces with kernel-weighted cores, show their connection to learning from multiple response data (multiple outputs; see [20] for discrete data and [12] for continuous data), and describe the optimization problem when learning cores with functional response ridge regression."}, {"heading": "2.1 Notations and Preliminaries", "text": "We start with some standardized notations and definitions that are used everywhere on paper. In view of a Hilbert space, H, < \u00b7, \u00b7 > H, and \u0430 \u00b7 H refer to its inner product or norm. We refer to Gx and Gy as the divisible real Hilbert spaces of input or output function data. In function data analysis, continuous data is generally considered to belong to the space of quadratic integral functions L2. In this thesis, we consider Gx and Gy as the Hilbert space L2, which consists of all equivalent classes of quadratic integral functions on a finite set. We consider the problem of estimating a function f (Gx, Gy) as the vector space of functions f: Gx \u2212 \u2192 Gy and with L (Gy) as a set of limited linear operators from Gy to Gy.We consider the problem of estimating a function f (Gx, Gy) = yi when functional data are observed (Gy, INxi = Gy)."}, {"heading": "Multiple Operator-valued Kernel Learning 5", "text": "In this case, we focus mainly on the RKHS F, whose elements are continuous linear operators on Gx with values in Gy. In this thesis, we are looking for a solution to this minimization problem in a function-evaluated reproducing Hilbert space F. Specifically, we focus mainly on the RKHS F, whose elements are continuous linear operators on Gx with values in Gy. The continuity property is achieved by looking at a special class of reproducing kernels called Mercer kernels [7, Proposition 2.2]. Note that F is divisible in this case, since Gx and Gy are separable."}, {"heading": "2.2 Functional Response Ridge Regression in Dual Variables", "text": "We can describe the burr regression with the optimization problem of the functional reactions (1) as follows: min f. F1 2-1-2-2-2-2-2-2-3-3-3-3-3-3-3-3-3-3. (2) RR n \u00b0 7900"}, {"heading": "6 Kadri, Rakotomamonjy, Bach & Preux", "text": "Now we introduce the Lagrange multipliers \u03b1i, i = 1., n, which are functional variables, since the output space is the space of the functions Gy. For the optimization problem (2), the Lagrange multipliers exist, and the Lagrange function is well defined here. For more details, see [15]. Let us leave the method of Lagrange multipliers on Banach spaces, which represents a generalization of the classical (end-dimensional) Lagrange multipliers method, which is suitable for solving certain end-dimensional optimization problems. If we leave the method of Lagrange multipliers on the vector of functions containing the Lagrange multipliers, let us apply the Lagrange function as L (f, \u03b1)."}, {"heading": "2.3 MovKL in Dual Variables", "text": "Let us now consider that the function f (\u00b7) is the sum of M-functions {fk (\u00b7)} M k = 1, where each fk belongs to a Gy-value RKHS with kernel Kk (\u00b7, \u00b7). Similar to the scalar value of multiple kernel learning we accept the convention that x0 = 0, if x = 0 and vice versa, and we solve the problem of learning these functions fk asmin d, D min fk, FkM, K = 1, Fk2dk, 2 Fk2dk + 12\u03bbn = 1, 2 Gym, i = yi \u2212 \u2211 M = 1 fk (xi), (6) where d = [d1, \u00b7 \u00b7 \u00b7, dM], K = 0 and \u04412dk, \u2264 1} and 1 \u0432yi \u2212 fk = 1 fk (xi), where d = [d1, d1 \u00b7 \u00b7 \u00b7, dD = {d: K, dk \u2265 and \u04412dk, \u2264 1} and 1 gm, and 1 gm, i = 1 gm, where d = 1 gk = 1 gk = 1 gk = 1 fk (xi)."}, {"heading": "3 Solving the MovKL Problem", "text": "After we introduced the framework, we are now developing an algorithm to solve this MovKL problem."}, {"heading": "3.1 Block-coordinate descent algorithm", "text": "Since the optimization problem (6) has the same structure as a multiple scalar-weighted kernel learning problem, we can build our MovKL algorithm on the MKL literature. Therefore, we propose to borrow from [14] and consider a block coordinate parentage method. Convergence of a block coordinate parentage algorithm closely related to the Gauss-Seidel method has been studied in the work of [31] and others, the difference being that we have operators and block operator matrices instead of matrices and block matrices, but this does not increase complexity if the inversion of operators is predictable (typically analytical or by spectral decomposition). Our algorithm solves the problem iteratively in relation to d, which is fixed, and then in relation to d, which is fixed (see algorithm 1), fixing algorithm-1 and not the algorithm-1."}, {"heading": "8 Kadri, Rakotomamonjy, Bach & Preux", "text": "Algorithm 1-r-Norm MovKLInput Kk for k = 1,..., M d1k \u2190 -1M for k = 1,..., M\u03b1-Norm MovKLInput Kk for k = 1,... do\u03b1 \". K\". K \". K\". K \". K\" \". K\". K \". K\". K \". K\" \". K\" \". K\" \". K\". K \"\". K \". K\". K \"\". K \".\" K \".\" K \".\" \". K\" \".\" \"K\". \"\" K \".\" \"\" K \".\" \"...\" \"\" K \".\" \"\" K \".\" \"K\". \"\" \".\".. \"\" \"..\" \"\".. \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"..\" \"\" \"..\" \"\".. \"\". \"\" \"..\" \"\".. \"\" \"\" \"..\" \"\" \"\".. \"\" \"\".. \"..\" \"\" \"..\" \"\" \"..\" \"\" \""}, {"heading": "3.2 Solving a linear system with multiple block operatorvalued kernels", "text": "A common method of constructing cores with operator value is to construct cores with operator value, which are transferred to the vector-weighted (or functional) setting by a positive definitive matrix (or operator). In this setting, a kernel with operator value has the following form: K (w, z) = G (w, z) T, where G is a scalar kernel and T is an operator in L (Gy). In multi-task learning, T is a finite dimensional matrix, which is expected to exchange information between tasks [11, 5]. More recently and for monitored functional output learning problems, T is chosen as a multiplication or integral operator [12, 13]. This choice is motivated by the fact that functional linear models for functional reactions [25] are based on these operators, and such cores then represent an interesting alternative to extend these models to non-linear contexts."}, {"heading": "Multiple Operator-valued Kernel Learning 9", "text": "Algorithm 2 Gauss-Seidel Methodchoose an initial vector of functions \u03b1 (0) repeatfor i = multiple operator (1, 2,.., n\u03b1 (t) i \u2190 \u2212 sol. of (13): [K (xi, xi) + \u03bbI] \u03b1 (t) i = siend foruntil convergenceand [4]. In this thesis we take a functional data analysis standpoint and are then interested in a finite combination of operator-weighted cores constructed from identity, multiplication and integral operators. A problem that occurs when working with operator-weighted cores in end-dimensional spaces is the solution of the system of linear operator equations (8). In the following we show how this problem can be solved for two cases of operator-weighted kernel combinations. Case 1: multiple scalar-weighted kernel positions and one operator."}, {"heading": "10 Kadri, Rakotomamonjy, Bach & Preux", "text": "With the help of the nuclei evaluated by the operator [K, xi) + \u03bbI] we can consider the following variation formula of this system: min (t) i1 2 < M + 1 k = 1Kk (t, xi) \u03b1 (t), (t) i > Gy \u2212 < si, (t) i > Gy, wherein si = 2yi \u2212 i \u2212 1 K (xj) \u03b1 (t) j \u2212 n \u2211 + 1 K (xi, xxj) (1) Kk = dkGkTk, pdkGkTk, pdkTk, pdkTk = pdkTk, pdkTk = pdk \u2212 1 K = 1 K (t)"}, {"heading": "4 Experiments", "text": "To highlight the benefits of our multi-operator kernel learning approach, we have considered a series of experiments on a real dataset that involve functional output predictions in a brain-computer interface framework. The problem we have dealt with is a partial problem related to the decoding of finger movements from electrocorticographic (ECoG) signals. We are focusing on the problem of estimating whether a finger moves or not, and also on the direct estimation of finger movement amplitude from the ECoG signals. The development of the full BCI application lies outside the scope of this paper, and our goal here is to prove that this problem of predicting finger movements can benefit from multiple kernel learning processes. To this end, the fourth dataset of the BCI competition IV [21] was used."}, {"heading": "Multiple Operator-valued Kernel Learning 11", "text": "The number of electrodes varies between 48 and 64 depending on the subject, and their position on the cortex was unknown. Fingerflexion of the subject was recorded at 25Hz and at 1KHz using a data exchange that measures finger movements, and the delay appears between finger movements and the measured EcoG signal."}, {"heading": "12 Kadri, Rakotomamonjy, Bach & Preux", "text": "While the inversion of the identity and the multiplication operators can be easily and directly calculated from the analytical expressions of the operators, the inversion of the integral operator is calculated from its spectral decomposition, as in [13]. The number of eigenfunctions and the regularization parameters \u03bb are summarized in Tables 1 and 2. The data set was randomly partitioned into 65% training and 35% test sets, with the aim of minimizing the residual sum of square errors. Empirical results on the BCI data set are summarized in Tables 1 and 2. The data set was randomly partitioned into 65% training and 35% test sets. In the case of \"1\" and \"2 standard restriction to the combination coefficients, we compare our approach with: (1) the basic alarm-weighted kernel regression algorithm by considering each output independently of the others.\""}, {"heading": "5 Conclusion", "text": "In this thesis, we have presented a new method for simultaneously learning an operator and a finite linear combination of cores evaluated by the operator. We have expanded the MKL framework to deal with the regression of functional response kernels, and we have proposed a block coordinate descent algorithm to solve the resulting optimization problem, which is applied to a BCI dataset to predict finger movements in a functional regression environment."}, {"heading": "Multiple Operator-valued Kernel Learning 13", "text": "The results show that our algorithm achieves better results than existing methods. For future work, it would be interesting to thoroughly compare the proposed MKL method for evaluating the operator with previous related methods for multi-class and multi-label MKL in the context of structured learning and collaborative filtering."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Existence of Minimizers", "text": "In this section we will discuss the existence of minimizers of problems (1) and (6). Since we are dealing with infinite dimensional spaces in the optimization problem in both problems, we must consider suitable tools to do so. The presence of f\u03bb in the problem indicated in Equation (1) is guaranteed for \u03bb > 0 by the general Weierstrass theorem and one of its logical consequences, which we both recall below [16]. Theorem 2 Let X be a reflective Banach space and C X a weakly closed and limited quantity. Stock h: C 7 \u2192 R is a proper lower semi-continuous function. Then h is limited from below and has a minimizer on C.Episode 3 Let H have a Hilbert space and h: H 7 \u2192 R is a strongly lower semi-continuous, convex and obsessive space. Then h is limited from below and reaches a minimizer \u2192 Let H have a semi-continuous space and h is strongly subcontinuous H:"}, {"heading": "14 Kadri, Rakotomamonjy, Bach & Preux", "text": "And by paraphrasing problem (6) asmin d-D J (d) with J (d) = min {fk} h1 (f1, \u00b7,, fk; {dk} k), it can be shown with arguments similar to those above that h1 is correct, strictly convex and imperative for fixed, not negative {dk} and \u03bb > 0 (remember the convention that x0 = 0 if x = 0 and vice versa). Therefore, there are minimizers of h1 w.r.t. {f1, \u00b7 \u00b7, fk} and they are unique. Since the function J (d), which is equal to h1 (f-1, \u00b7, f-k; {dk} k), is continuous via the compact subset of RM defined by the constraints on d, it also achieves its minimum. Hence the proof that a solution to the problem (6) exists."}, {"heading": "B Dual Formulation of Functional Ridge Regres-", "text": "The basic arithmetical details regarding the dual formulation of the functional ridge regression presented in Section 2 are discussed here: < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p = p (xi, yi). (16) where (xi, yi) i = 1,..., n \u00b2 (Gx, Gy). Gx and Gy are the Hilbert space L 2, which consists of all equivalence classes of square integrable functions on a finite set, and F \u2212 p is an RKHS whose elements are continuous linear operators on Gx with values in Gy. K is the L (Gy) -weighted reproducible core of F. Since Gx and Gy are functional spaces to derive a \"dual version\" of the problem (16), we use the method of lagrange multipliers on banked spaces that are suitable for solving specific problems."}, {"heading": "Multiple Operator-valued Kernel Learning 15", "text": "Based on the fact that DhL (f) = < L (f), h > F, we get the solution, we get G (f) = 2fii. H (f) = < H (f) = < \u03b1, y \u2212 f (x) > Gnylim. \u2212 \u2192 0 < \u03b1, y \u2212 f (x) > Gnylim. \u2212 < \u03b1, h (x) > Gnylim."}, {"heading": "C Convergence of Algorithm 1", "text": "In this section we present a proof for the convergence of algorithm 1. The proof is an extension of the results obtained by [2] and [24] to infinite dimensional Hilbert spaces with reproducing nuclei evaluated by the operator. Let R (f, d) be the objective function of the MovKL problem, defined by (6): R (f, d) = L + M \u00b2 k = 1, fk \u00b2 2 Fkdkwo L = 1\u03bb \u00b2 i \u2212 x M k = 1 fk (xi) \u00b2 2 Gy. Let's replace the equation (10) in R, we get the objective function: S (f): = R (f, d (f)) = L + (M \u00b2 k = 1, fk \u00b2 2r + 1Fk) r + 1 rThe function S is strictly convex, since L is convex and the function is defined by f 7 \u2212 \u2192."}, {"heading": "16 Kadri, Rakotomamonjy, Bach & Preux", "text": "Thus, the function g (f) = min u {R (u, d)} is indeed G is the minimum of a functional response kernel regression problem in a function-rated RKHS associated with an operator-rated kernel K. So, G (d) = min u {R (u, d)} in fact, G is the minimum of a functional response kernel regression problem in a function-rated RKHS associated with an operator-rated kernel K. So, G (d) = R (d, u) with u (K) -1 y (see Equation (8)). u It is continuous, and hence G (d) is also continuous."}, {"heading": "Multiple Operator-valued Kernel Learning 17", "text": "Da f (nl) < f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n, f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n), f (n, f (n), f (n), f (n, f (n), f (n), f (n (n), f (n), f (n (n), f (n (n), f (n (n), f (n), f (n (n), f (n (n), f (n (n), f (n), f (n (n, f (n), f (n), f (n, f (n (n), f (n (n), f (n), f (n, f (n), f (n (n), f (n (n), f (n (n), f (n (n), f (n (n), f (n), f (n), f (n (n), f (n), f (n (n), f (n (n), f (n), f (n), f"}, {"heading": "18 Kadri, Rakotomamonjy, Bach & Preux", "text": "This implies that f * is a minimizer of R (\u00b7, d (f *), because R (f *, d *) = S (f *). Furthermore, d (f *) is the minimizer of R (\u00b7, f *), which is subject to the constraints of d. Thus, since the objective function R is smooth, the pair (f *, d (f *) is the minimizer of R. At this stage, we have shown that any convergent sub-sequence of {f (n), n * N} converges to the minimizer of R, since the sequence {f (n), n * N} is limited, it follows that the entire sequence converges to the minimizer of R."}, {"heading": "Multiple Operator-valued Kernel Learning 19", "text": "[15] S. Kurcyusz. On the existence and nonexistence of lagrange multipliers in Banach spaces. Journal of Optimization Theory and Applications, 20: 81- 110, 1976. [16] A. Kurdila and M. Zabarankin. Convex Functional Analysis. Birkhauser Verlag, 2005. [17] G. Lanckriet, N. Christianini, L. El Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix with semi-definite programming. JMLR, 5: 27- 72, 2004. [18] H. Lian. Nonlinear functional models for functional reactions in the reproduction of Kernel Hilbert spaces. The Canadian Journal of Statistics, 35: 597-606, 2007. [19] C. Micchelli and M. Pontil. Learning the kernel function via regularization. JMLR, 6: 1099-1125, 2005. [20] C. A. Micchelli and M. Pontil. Learning functions of 20000."}, {"heading": "20 Kadri, Rakotomamonjy, Bach & Preux", "text": "[30] S. Sonnenburg, G. Ra \ufffd tsch, C. Scha \ufffd fer, and B. Scho \ufffd lkopf. Large scale multiple kernel learning. JMLR, 7: 1531-1565, 2006. [31] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. J. Optim. Theory Appl., 109: 475-494, 2001.INRIACentre de recherche INRIA Lille - Nord Europe Parc Scientifique de la Haute Borne - 40, avenue Halley - 59650 Villeneuve d'Ascq (France) Centre de recherche INRIA Bordeaux - Sud Ouest: Domaine Universitaire - 351, cours de la Lib\u00e9ration - 33405 Talence Cedex Centre de recherche INRIA Grenoble - Rh\u00f4ne-Alpes: 655, avenue de l'Europe - 38334 Montbono Saint-Ismitaerche Centre de recherche Centre de la Lib\u00e9ration - 33405 Talence Cedex Centre de recherche INRIA Grenoble - Rh\u00e9blanche Centre de recherche INRIA Grenoble - Rh\u00e955 Grenoble: 655, avenue de l'Europe - 38355."}], "references": [{"title": "Variable sparsity kernel learning", "author": ["J. Aflalo", "A. Ben-Tal", "C. Bhattacharyya", "J. Saketha Nath", "S. Raman"], "venue": "JMLR, 12:565\u2013592", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, 73(3):243\u2013272", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Consistency of the group Lasso and multiple kernel learning", "author": ["F. Bach"], "venue": "JMLR, 9:1179\u20131225", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "F", "author": ["C. Brouard"], "venue": "d\u2019Alch\u00e9-Buc, and M. Szafranski. Semi-supervised penalized output kernel regression for link prediction. In Proc. ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal multitask kernels", "author": ["A. Caponnetto", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "JMLR, 68:1615\u20131646", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Vector valued reproducing kernel Hilbert spaces of integrable functions and mercer theorem", "author": ["C. Carmeli", "E. De Vito", "A. Toigo"], "venue": "Analysis and Applications, 4:377\u2013408", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Vector valued reproducing kernel Hilbert spaces and universality", "author": ["C. Carmeli", "E. De Vito", "A. Toigo"], "venue": "Analysis and Applications, 8:19\u201361", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "L2 regularization for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Proc. UAI", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalization bounds for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Proc. ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C.S. Ong", "P. Gehler", "G. Pillonetto"], "venue": "Proc. ICML", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "JMLR, 6:615\u2013637", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonlinear functional regression: a functional RKHS approach", "author": ["H. Kadri", "E. Duflos", "P. Preux", "S. Canu", "M. Davy"], "venue": "Proc. AISTATS, pages 111\u2013 125", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Functional regularized least squares classification with operator-valued kernels", "author": ["H. Kadri", "A. Rabaoui", "P. Preux", "E. Duflos", "A. Rakotomamonjy"], "venue": "Proc. ICML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "JMLR, 12:953\u2013997", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "On the existence and nonexistence of lagrange multipliers in Banach spaces", "author": ["S. Kurcyusz"], "venue": "Journal of Optimization Theory and Applications, 20:81\u2013 110", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1976}, {"title": "Convex Functional Analysis", "author": ["A. Kurdila", "M. Zabarankin"], "venue": "Birkhauser Verlag", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning the kernel matrix with semi-definite programming", "author": ["G. Lanckriet", "N. Cristianini", "L. El Ghaoui", "P. Bartlett", "M. Jordan"], "venue": "JMLR, 5:27\u2013 72", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonlinear functional models for functional responses in reproducing kernel Hilbert spaces", "author": ["H. Lian"], "venue": "The Canadian Journal of Statistics, 35:597\u2013606", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning the kernel function via regularization", "author": ["C. Micchelli", "M. Pontil"], "venue": "JMLR, 6:1099\u20131125", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation, 17:177\u2013204", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Prediction of finger flexion: 4th brain-computer interface data competition", "author": ["K.J. Miller", "G. Schalk"], "venue": "BCI Competition IV", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Prediction of arm movement trajectories from ecog-recordings in humans", "author": ["T. Pistohl", "T. Ball", "A. Schulze-Bonhage", "A. Aertsen", "C. Mehring"], "venue": "Journal of Neuroscience Methods, 167(1):105\u2013114", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "Y. Grandvalet", "S. Canu"], "venue": "JMLR, 9:2491\u20132521", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "l(p)-l(q) penalty for sparse linear and sparse multiple kernel multitask learning", "author": ["A. Rakotomamonjy", "R. Flamary", "G. Gasso", "S. Canu"], "venue": "IEEE Trans. Neural Netw., 22(8):1307\u201320", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Functional Data Analysis", "author": ["J.O. Ramsay", "B.W. Silverman"], "venue": "2nd ed. Springer Verlag, New York", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimating the mean and covariance structure nonparametrically when the data are curves", "author": ["John A. Rice", "B.W. Silverman"], "venue": "Journal of the Royal Statistical Society. Series B,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "Decoding two-dimensional movement trajectories using electrocorticographic signals in humans", "author": ["G. Schalk", "J. Kubanek", "K.J. Miller", "N.R. Anderson", "E.C. Leuthardt", "J.G. Ojemann", "D. Limbrick", "D. Moran", "L.A. Gerhardt", "J.R. Wolpaw"], "venue": "Journal of Neural Engineering, 4(3):264\u2013275", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "BCI2000: a general-purpose brain-computer interface system", "author": ["G. Schalk", "D.J. McFarland", "T. Hinterberger", "N. Birbaumer", "J.R. Wolpaw"], "venue": "Biomedical Engineering, IEEE Trans. on, 51:1034\u20131043", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with Kernels: Support Vector Machines", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "JMLR, 7:1531\u20131565", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Convergence of block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., 109:475\u2013494, 2001. INRIA  Centre de recherche INRIA Lille \u2013 Nord Europe Parc Scientifique de la Haute Borne - 40, avenue Halley - 59650 Villeneuve d\u2019Ascq ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "Recently, there has been considerable interest in estimating vector-valued functions [20, 5, 7].", "startOffset": 85, "endOffset": 95}, {"referenceID": 4, "context": "Recently, there has been considerable interest in estimating vector-valued functions [20, 5, 7].", "startOffset": 85, "endOffset": 95}, {"referenceID": 6, "context": "Recently, there has been considerable interest in estimating vector-valued functions [20, 5, 7].", "startOffset": 85, "endOffset": 95}, {"referenceID": 10, "context": "Typical learning situations include multi-task learning [11], functional regression [12], and structured output prediction [4].", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "Typical learning situations include multi-task learning [11], functional regression [12], and structured output prediction [4].", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "Typical learning situations include multi-task learning [11], functional regression [12], and structured output prediction [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 26, "context": "More precisely, we are interested in finger movement prediction from electrocorticographic signals [27].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Indeed, for these problems, one of the difficulties arises from the unknown latency between the signal related to the finger movement and the actual movement [22].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "Working in such RKHSs, we are able to draw on the important core of work that has been performed on scalar-valued and vector-valued RKHSs [29, 20].", "startOffset": 138, "endOffset": 146}, {"referenceID": 19, "context": "Working in such RKHSs, we are able to draw on the important core of work that has been performed on scalar-valued and vector-valued RKHSs [29, 20].", "startOffset": 138, "endOffset": 146}, {"referenceID": 11, "context": "Such a functional RKHS framework and associated operator-valued kernels have been introduced recently [12, 13].", "startOffset": 102, "endOffset": 110}, {"referenceID": 12, "context": "Such a functional RKHS framework and associated operator-valued kernels have been introduced recently [12, 13].", "startOffset": 102, "endOffset": 110}, {"referenceID": 16, "context": "In order to overcome the need for choosing a kernel before the learning process, several works have tried to address the problem of learning the scalar-valued kernel jointly with the decision function [17, 30].", "startOffset": 201, "endOffset": 209}, {"referenceID": 29, "context": "In order to overcome the need for choosing a kernel before the learning process, several works have tried to address the problem of learning the scalar-valued kernel jointly with the decision function [17, 30].", "startOffset": 201, "endOffset": 209}, {"referenceID": 8, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 126, "endOffset": 132}, {"referenceID": 2, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 126, "endOffset": 132}, {"referenceID": 22, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 177, "endOffset": 188}, {"referenceID": 0, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 177, "endOffset": 188}, {"referenceID": 13, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 177, "endOffset": 188}, {"referenceID": 9, "context": "It should be pointed out that in a recent work [10], the problem of learning the output kernel was", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "Before describing the multiple operator-valued kernel learning algorithm that we will study and experiment with in this paper, we first review notions and properties of reproducing kernel Hilbert spaces with operator-valued kernels, show their connection to learning from multiple response data (multiple outputs; see [20] for discrete data and [12] for continuous data), and describe the optimization problem for learning kernels with functional response ridge regression.", "startOffset": 318, "endOffset": 322}, {"referenceID": 11, "context": "Before describing the multiple operator-valued kernel learning algorithm that we will study and experiment with in this paper, we first review notions and properties of reproducing kernel Hilbert spaces with operator-valued kernels, show their connection to learning from multiple response data (multiple outputs; see [20] for discrete data and [12] for continuous data), and describe the optimization problem for learning kernels with functional response ridge regression.", "startOffset": 345, "endOffset": 349}, {"referenceID": 19, "context": "The proof of Theorem 1 can be found in [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": ", [5, 6, 7].", "startOffset": 2, "endOffset": 11}, {"referenceID": 5, "context": ", [5, 6, 7].", "startOffset": 2, "endOffset": 11}, {"referenceID": 6, "context": ", [5, 6, 7].", "startOffset": 2, "endOffset": 11}, {"referenceID": 14, "context": "For more details, see [15].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "Before deriving the dual of this problem, it can be shown by means of the generalized Weierstrass theorem [16] that this problem admits a solution (a detailed proof is provided in the supplementary material).", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "Now, following the lines of [23], a dualization of this problem leads to the following equivalent one", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Hence, we propose to borrow from [14], and consider a blockcoordinate descent method.", "startOffset": 33, "endOffset": 37}, {"referenceID": 30, "context": "The convergence of a block coordinate descent algorithm which is related closely to the Gauss-Seidel method was studied in works of [31] and others.", "startOffset": 132, "endOffset": 136}, {"referenceID": 18, "context": "which has a closed-form solution and for which optimality occurs at [19]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "This algorithm is similar to that of [8] and [14] both being based on alternating optimization.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "This algorithm is similar to that of [8] and [14] both being based on alternating optimization.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "The proof is actually an extension of results obtained by [2] and [24] for scalar-valued kernels.", "startOffset": 58, "endOffset": 61}, {"referenceID": 23, "context": "The proof is actually an extension of results obtained by [2] and [24] for scalar-valued kernels.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "In multi-task learning, T is a finite dimensional matrix that is expected to share information between tasks [11, 5].", "startOffset": 109, "endOffset": 116}, {"referenceID": 4, "context": "In multi-task learning, T is a finite dimensional matrix that is expected to share information between tasks [11, 5].", "startOffset": 109, "endOffset": 116}, {"referenceID": 11, "context": "More recently and for supervised functional output learning problems, T is chosen to be a multiplication or an integral operator [12, 13].", "startOffset": 129, "endOffset": 137}, {"referenceID": 12, "context": "More recently and for supervised functional output learning problems, T is chosen to be a multiplication or an integral operator [12, 13].", "startOffset": 129, "endOffset": 137}, {"referenceID": 24, "context": "This choice is motivated by the fact that functional linear models for functional responses [25] are based on these operators and then such kernels provide an interesting alternative to extend these models to nonlinear contexts.", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "In addition, some works on functional regression and structured-output learning consider operator-valued kernels constructed from the identity operator as in [18]", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "and [4].", "startOffset": 4, "endOffset": 7}, {"referenceID": 12, "context": "Thus we can compute an analytic solution of the system of equations (8) by inverting K+ \u03bbI using the eigendecompositions of G and T as in [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "To this aim, the fourth dataset from the BCI Competition IV [21] was used.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "ECoG signals of the subject were recorded at a 1KHz sampling using BCI2000 [28].", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "Due to the acquisition process, a delay appears between the finger movement and the measured ECoG signal [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "For the multiple operator-valued kernels having the form (12), we have used a Gaussian kernel with 5 different bandwidths and a polynomial kernel of degree 1 to 3 combined with three operators T : identity Ty(t) = y(t), multiplication operator associated with the function e 2 defined by Ty(t) = e 2 y(t), and the integral Hilbert-Schmidt operator with the kernel e proposed in [13], Ty(t) = \u222b ey(s)ds.", "startOffset": 378, "endOffset": 382}, {"referenceID": 12, "context": "While the inverses of the identity and the multiplication operators are easily and directly computable from the analytic expressions of the operators, the inverse of the integral operator is computed from its spectral decomposition as in [13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 25, "context": "The number of eigenfunctions as well as the regularization parameter \u03bb are fixed using \u201cone-curve-leave-out cross-validation\u201d [26] with the aim of minimizing the residual sum of squares error.", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "We compare our approach in the case of l1 and l2-norm constraint on the combination coefficients with: (1) the baseline scalar-valued kernel ridge regression algorithm by considering each output independently of the others, (2) functional response ridge regression using an integral operator-valued kernel [13], (3) kernel ridge regression with an evenly-weighted sum of operator-valued kernels, which we denote by l\u221e-norm MovKL.", "startOffset": 306, "endOffset": 310}, {"referenceID": 15, "context": "Existence of f\u03bb in the problem given in Equation (1) is guaranteed, for \u03bb > 0 by the generalized Weierstrass Theorem and one of its corollary that we both remind below [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "1 1 in [15].", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "1 in [15] which deals with more general context.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "The proof is an extension of results obtained by [2] and [24] to infinite dimensional Hilbert spaces with operator-valued reproducing kernels.", "startOffset": 49, "endOffset": 52}, {"referenceID": 23, "context": "The proof is an extension of results obtained by [2] and [24] to infinite dimensional Hilbert spaces with operator-valued reproducing kernels.", "startOffset": 57, "endOffset": 61}], "year": 2012, "abstractText": "Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients (r \u2265 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinatedescent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces. Key-words: Operator-valued kernels, multiple kernel learning, nonparametric functional data analysis, function-valued reproducing kernel Hilbert spaces. \u2217 Sequel Team, INRIA Lille. E-mail: hachem.kadri@inria.fr \u2020 LITIS, Universit\u00e9 de Rouen. E-mail: alain.rakotomamonjy@insa-rouen.fr \u2021 Sierra Team/INRIA, Ecole Normale Sup\u00e9rieure. E-mail: francis.bach@inria.fr \u00a7 Sequel/INRIA-Lille, LIFL/CNRS. E-mail: philippe.preux@inria.fr Apprentissage de Noyaux \u00e0 Valeurs Op\u00e9rateurs Multiples R\u00e9sum\u00e9 : Dans cet article, nous proposons une m\u00e9thode d\u2019apprentissage de noyaux multiples \u00e0 valeurs op\u00e9rateurs dans le cas d\u2019une r\u00e9gression ridge \u00e0 r\u00e9ponse fonctionnelle. Notre m\u00e9thode est bas\u00e9e sur la r\u00e9solution d\u2019un syst\u00e8me d\u2019\u00e9quations lin\u00e9aires d\u2019op\u00e9rateurs en utilisant une proc\u00e9dure de type Iterative Coordinate Descent. Nous validons exp\u00e9rimentalement notre approche sur un probl\u00e8me de pr\u00e9diction de mouvement de doigt dans un contexte d\u2019Interface Cerveau-Machine. Mots-cl\u00e9s : noyaux \u00e0 valeurs op\u00e9rateurs, apprentissage de noyaux multiples, analyse des donn\u00e9es fonctionnelles, espace de Hilbert \u00e0 noyau reproduisant Multiple Operator-valued Kernel Learning 3", "creator": "LaTeX with hyperref package"}}}