{"id": "1203.2557", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2012", "title": "On the Necessity of Irrelevant Variables", "abstract": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.", "histories": [["v1", "Mon, 12 Mar 2012 17:17:34 GMT  (212kb,D)", "https://arxiv.org/abs/1203.2557v1", "A preliminary version of this paper appeared in the proceedings of ICML'11"], ["v2", "Tue, 13 Mar 2012 18:29:37 GMT  (212kb,D)", "http://arxiv.org/abs/1203.2557v2", "A preliminary version of this paper appeared in the proceedings of ICML'11"], ["v3", "Fri, 8 Jun 2012 23:50:17 GMT  (207kb,D)", "http://arxiv.org/abs/1203.2557v3", "A preliminary version of this paper appeared in the proceedings of ICML'11"]], "COMMENTS": "A preliminary version of this paper appeared in the proceedings of ICML'11", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david p helmbold", "philip m long"], "accepted": true, "id": "1203.2557"}, "pdf": {"name": "1203.2557.pdf", "metadata": {"source": "CRF", "title": "On the Necessity of Irrelevant Variables", "authors": ["David P. Helmbold", "Philip M. Long"], "emails": ["dph@soe.ucsc.edu", "plong@sv.nec-labs.com"], "sections": [{"heading": null, "text": "Keywords: Feature Selection, Generalization, Learning Theory"}, {"heading": "1. Introduction", "text": "In this context, it should be noted that this case is an accident."}, {"heading": "2. Tail bounds", "text": "(This section summarizes the various tail boundaries used at various points in the analysis, all of which assume that U1, U2,.., U \"are {0, 1} independently weighted random variables and U = \u2211'i = 1 Ui. We start with some upper tail boundaries. \u2022 The Hoeffding boundary, (see Pollard, 1984): P [1 'U \u2212 E (1' U) \u2264 e \u2212 2\u03b72 '. (4) \u2022 The Chernoff boundary, (Angluin and Valiant, 1979; Motwani and Raghavan, 1995, see) and Appendix A.1. In any other case > P [U > (1 + \u03b7) E (U) < Exp (\u2212 1 + \u03b7) E (U) ln (U) ln (1 + \u03b7e)). (5) \u2022 Any other point that is not possible."}, {"heading": "3. The accuracy of models containing relevant and irrelevant variables", "text": "In this section we will analyze the accuracy of the models (hypotheses) used by the algorithms in Section 4. Each example is represented by a vector of N binary variables and a class name. We will use the following generative model: \u2022 a random class name from {0 > 1} is chosen, both classes being equally probable, then \u2022 each of the K relevant variables is equal to the class name with the probability of 1 / 2 \u2212 ollable (or with the probability of 1 / 2 \u2212 \u03b3), and \u2022 the remaining N \u2212 K irrelevant variables are equal to the class name with the probability of 1 / 2; \u2022 all variables are conditionally independent of the class name. \u2212 Which variables are relevant and whether each is positively or negatively correlated with the class names is chosen arbitrarily before time. A feature is either a variable or its complementarity. The 2 (N \u2212 K) irrelevant attributes are derived from the irrelevant variables that correlate with the probability."}, {"heading": "4. Learning", "text": "We now consider the problem of learning a model M from data. We assume that the algorithm m \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 2 m 3 m 3 m 3 x 4 m 3 x 4 m 3 x 4 m 3 x 4 m 3 m 3 m 3 m 5 of the expected errors of the algorithm: the probability of the training set and the test example that its model will make an erroneous prediction on the test example (the \"prediction model\" by Haussler et al. (1994). We define M\u03b2 as the majority of all the characteristics corresponding to the class designation at least 1 / 2 + \u03b2 2 of the training examples. In order to keep the analysis as clean as possible, our results in this section apply to algorithms that have chosen \u03b2 function of the relevant characteristics, the number of the N, the relevant characteristics, the number of the N, the number of the relevant characteristics, the N, the number of the relevant characteristics."}, {"heading": "5. Lower bound", "text": "The preliminary version of this paper (Helmbold and Long, 2011) includes a related lower limit for algorithms that choose \u03b2 as a function of N, K, m, and \u03b3 and predict with M\u03b2. Here, we present a more general lower limit that applies to algorithms that issue arbitrary hypotheses, including algorithms that use weighted matches (perhaps with L1 regulation). In this section, we include the number of features N, number of relevant features K, and sample size m as a function of \u03b3 in such a way that corollary 10 has a constant lower limit for these combinations of values that apply to \"exclusive\" algorithms (defined below) when the number of relevant features K and the number of relevant features M are considered relevant."}, {"heading": "5.1 Relevant Variables and Good Hypotheses", "text": "This chapter provides some useful facts about relevant variables and good hypotheses."}, {"heading": "If in addition m \u2265 1/\u03b22, then the probability that a given variable has empirical edge at", "text": "The proof: Since relevant variables match the probability 1 / 2 + \u03b3, the probability that a relevant variable has an empirical margin is reduced by the probability that an irrelevant variable has an empirical margin to reduce the probability that an irrelevant variable has an empirical margin of at least \u03b2. An irrelevant variable only has an empirical margin if it matches the class on 1 / 2 + \u03b2 of the sample. By using Bound (9), this is done with a probability of at least 15 Exp (\u2212 16\u03b22). The second part uses Bound (8) instead of (9). The probability that a relevant variable will be included in V\u03b2, again for \u03b2 that does not depend on S. Lemma 19, is the probability that a given relevant variable has an empirical margin of at most \u03b2, by e \u2212 2 (\u03b2 \u2212 3) 2 m.Proof: Use (4) to increase the probability that a feature matches the class more often than expected."}, {"heading": "5.2 Bounding \u03bb-Exclusiveness", "text": "Remember that the number of variables used by A (S) decreases the number of variables, and the number of variables used by A (S) increases. < b (S) is the edge of the variable whose order of precedence is high (S) when the variables are ordered according to their empirical edges. < b (S) is the probability that \u03b2 (S) is at least the critical value \u03b2 (S). < b (S) is the probability that \u03b2 (S) is at least the critical value \u03b2 (S)."}, {"heading": "5.3 Large Error", "text": "Name a variable good if it is relevant and its empirical edge is at least \u03b2 * b * b (25g / 25g / 25g). Let p be the probability that a relevant variable is good. Thus, the number of good variables is binomically distributed with the parameters K and p. So we have that the expected number of good variables is pK and the variance Kp (1 \u2212 p) < Kp. After Chebyshev's inequality, we have P [# good vars \u2265 Kp + a \u221a Kp] \u2264 P [# good vars \u2265 Kp + a \u221a Kp + a \u221a Kp (1 \u2212 p) \u2264 1 a2, (23) and the determination a = \u03b3 Kp, these givesP [# good vars \u2265 2Kp] \u2264 1 Kp. (24) After Lemma 19, Kp \u2264 Ke \u2212 2 (\u03b2 \u0445 Kp \u2212 2), the probability is lower / a \u00b2."}, {"heading": "6. Relaxations of some assumptions", "text": "In order to keep the analysis clean and to facilitate the interpretation of the results, we have analyzed an idealized model. In this section, we briefly consider the consequences of some loosening of our assumptions."}, {"heading": "6.1 Conditionally dependent variables", "text": "Theorem 1 can be generalized to the case where there is a limited dependence between the variables, after conditioning the class name in different ways. For example, suppose that there is a degree r graph G whose nodes are variables, and that, due to the label, each variable is independent of all variables that are not connected to it by an edge in G. Suppose k variables agree with the label with the probability 1 / 2, and the n \u2212 k agree with the label with the probability 1 / 2. Suppose that a source like this has an r-local dependency. Let's then assume a Chernoff-Hoeffding boundary for such sets of random variables based on Pemmaraju (2001), and if r \u2264 n / 2, you get a boundary of c (r + 1) exp (\u2212 2\u03b32k2 n (r + 1)))) of the probability of error."}, {"heading": "6.2 Variables with different strengths", "text": "We have previously assumed that all relevant variables are equally strongly associated with the class label. It is easy to generalize our analysis to the situation when the strengths of the associations fall at an interval [\u03b3min, \u03b3max]. Therefore, relevant variables agree with the class label with a probability of at least 1 / 2 + \u03b3min, and misleading variables agree with a probability of at least 1 / 2 \u2212 \u03b3max. Although a complex analysis would take into account the degree of association of each variable, it is possible to use our previous analysis with a simpler approach. Using the 1 / 2 + \u03b3min and 1 / 2 \u2212 max underestimates the probability that relevant variables and misleading variables agree with the class label, leading to an analogy of Theorem 1. This analogy states that models that choose n variables, of which k are relevant and \"of which are misleading, have probabilities of error limited to probability (2), the probability that the relevant class sign and the relevant variables are limited."}, {"heading": "7. Conclusions", "text": "We analyzed learning when there are few examples, when a small fraction of the variables are relevant, and when the relevant variables are poorly correlated with the class name. In this situation, algorithms that generate hypotheses that are predominantly composed of irrelevant variables can be highly precise (with an error rate of 0), and this inclusion of many irrelevant variables is critical. Any algorithm that limits the expected fraction of irrelevant variables in its hypotheses has an error rate that is below a constant, in stark contrast to many heuristics on feature selection that limit the number of features to a small multiple of the number of examples, or that limit the classifier to the use of variables that pass strict statistical tests for association with the class. These results have two implications for the practice of machine learning. First, they show that the technical practice of producing models that contain an enormous number of intuitive, sometimes intuitive, is justified by the number of variables that they contain."}, {"heading": "Acknowledgements", "text": "We thank Aravind Srinivasan for his help."}, {"heading": "Appendix A. Appendices", "text": "A.1 Proof of (5) and (6) Equation 4.1 of (Motwani and Raghavan, 1995) isP [U > (1 + VI) E (U)] < (e\u03b7 (1 + VI) 1 + VI) E (U) (26) and applies to independently 0-1 rated Ui's each with (possibly different) probabilities pi = P (Ui = 1), being 0 < pi < 1 and 2 + VI > 0. Taking into account the logarithm of RHS, we get (RHS) = E (U) (1 + VI) l (1 + VI) l (1 + VI) l (P) < E (U) + 1 \u2212 V (1 + VI) lV) ln (RHS) = E (U) (U) (1 + V (1 + IV), meaning (5 + VI)."}], "references": [{"title": "Boosting with diverse base classifiers", "author": ["S. Dasgupta", "P.M. Long"], "venue": "COLT,", "citeRegEx": "Dasgupta and Long.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Long.", "year": 2003}, {"title": "On the necessity of irrelevant variables", "author": ["J. Jin"], "venue": "Information and Computation,", "citeRegEx": "Jin.,? \\Q1994\\E", "shortCiteRegEx": "Jin.", "year": 1994}, {"title": "Empirical margin distributions and bounding the gen", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": null, "citeRegEx": "Koltchinskii and Panchenko.,? \\Q2009\\E", "shortCiteRegEx": "Koltchinskii and Panchenko.", "year": 2009}, {"title": "Randomized Algorithms", "author": ["R. Motwani", "P. Raghavan"], "venue": null, "citeRegEx": "2006", "shortCiteRegEx": "2006", "year": 1995}, {"title": "logistic regression and naive bayes", "author": ["S. Pemmaraju"], "venue": "bounds. RANDOM,", "citeRegEx": "Pemmaraju.,? \\Q2001\\E", "shortCiteRegEx": "Pemmaraju.", "year": 2001}, {"title": "Bagging, boosting and C4.5", "author": ["Helmbold", "Long J. Quinlan"], "venue": null, "citeRegEx": "Helmbold and Quinlan.,? \\Q1996\\E", "shortCiteRegEx": "Helmbold and Quinlan.", "year": 1996}], "referenceMentions": [{"referenceID": 3, "context": "Over the past decade or so, a number of empirical and theoretical findings have challenged the traditional rule of thumb described by Bishop (2006) as follows. One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model. The Support Vector Machine literature (see Vapnik, 1998) views algorithms that compute apparently complicated functions of a given set of variables as linear classifiers applied to an expanded, even infinite, set of features. These empirically perform well on test data, and theoretical accounts have been given for this. Boosting and Bagging algorithms also generalize well, despite combining large numbers of simple classifiers \u2013 even if the number of such \u201cbase classifiers\u201d is much more than the number of training examples (Quinlan, 1996; Breiman, 1998; Schapire et al., 1998). This is despite the fact that Friedman et al. (2000) showed the behavior of such classifiers is closely related to performing logistic regression on a potentially vast set of features (one for each possible decision tree, for example).", "startOffset": 142, "endOffset": 980}, {"referenceID": 1, "context": "Relationship to Previous Work Donoho and Jin (see Donoho and Jin, 2008; Jin, 2009) and Fan and Fan (2008), building on a line of research on joint testing of multiple hypotheses (see Abramovich et al.", "startOffset": 41, "endOffset": 106}, {"referenceID": 1, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al.", "startOffset": 8, "endOffset": 42}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case.", "startOffset": 43, "endOffset": 88}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case. (Since these other bounds hold more generally, their overall strength is incomparable to our results.) Ng and Jordan (2001) showed that the Naive Bayes algorithm (which ignores class-conditional dependencies) converges relatively quickly, justifying its use when there are few examples.", "startOffset": 43, "endOffset": 239}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case. (Since these other bounds hold more generally, their overall strength is incomparable to our results.) Ng and Jordan (2001) showed that the Naive Bayes algorithm (which ignores class-conditional dependencies) converges relatively quickly, justifying its use when there are few examples. But their bound for Naive Bayes is also vacuous when m = \u0398(1/\u03b32). Bickel and Levina (2004) studied the case in which the class conditional distributions are Gaussians, and showed how an algorithm which does not model class conditional dependencies can perform nearly optimally in this case, especially when the number of variables is large.", "startOffset": 43, "endOffset": 493}, {"referenceID": 0, "context": "(1998); Koltchinskii and Panchenko (2002); Dasgupta and Long (2003); Wang et al. (2008)) are vacuous in this case. (Since these other bounds hold more generally, their overall strength is incomparable to our results.) Ng and Jordan (2001) showed that the Naive Bayes algorithm (which ignores class-conditional dependencies) converges relatively quickly, justifying its use when there are few examples. But their bound for Naive Bayes is also vacuous when m = \u0398(1/\u03b32). Bickel and Levina (2004) studied the case in which the class conditional distributions are Gaussians, and showed how an algorithm which does not model class conditional dependencies can perform nearly optimally in this case, especially when the number of variables is large. B\u00fchlmann and Yu (2002) analyzed the variance-reduction benefits of Bagging with primary focus on the benefits of the smoother classifier that is obtained when ragged classifiers are averaged.", "startOffset": 43, "endOffset": 766}, {"referenceID": 4, "context": "Then applying a Chernoff-Hoeffding bound for such sets of random variables due to Pemmaraju (2001), if r \u2264 n/2, one gets a bound of c(r + 1) exp ( \u22122\u03b32k2 n(r+1) )", "startOffset": 82, "endOffset": 99}], "year": 2012, "abstractText": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.", "creator": "LaTeX with hyperref package"}}}