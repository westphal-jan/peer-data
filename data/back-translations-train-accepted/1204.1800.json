{"id": "1204.1800", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Apr-2012", "title": "On Power-Law Kernels, Corresponding Reproducing Kernel Hilbert Space and Applications", "abstract": "The role of kernels is central to machine learning. Motivated by the importance of power law distributions in modeling, simulation and learning, in this paper, we propose a power-law generalization of the Gaussian kernel. This generalization is based on q-Gaussian distribution, which is a power-law distribution studied in context of nonextensive statistical mechanics. We prove that the proposed kernel is positive definite, and provide some insights regarding the corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical significance of q-Gaussian kernels in classification, regression and clustering, and present some simulation results.", "histories": [["v1", "Mon, 9 Apr 2012 05:53:27 GMT  (775kb,D)", "http://arxiv.org/abs/1204.1800v1", "7 pages, 3 figures, 4 tables"], ["v2", "Mon, 1 Apr 2013 07:12:43 GMT  (191kb,D)", "http://arxiv.org/abs/1204.1800v2", "7 pages, 3 figures, 3 tables"]], "COMMENTS": "7 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT stat.ML", "authors": ["debarghya ghoshdastidar", "ambedkar dukkipati"], "accepted": true, "id": "1204.1800"}, "pdf": {"name": "1204.1800.pdf", "metadata": {"source": "CRF", "title": "On q-Gaussian kernel and its Reproducing Kernel Hilbert Space", "authors": ["Debarghya Ghoshdastidar", "Ambedkar Dukkipati"], "emails": ["gdebarghya@ee.iisc.ernet.in", "ambedkar@csa.iisc.ernet.in"], "sections": [{"heading": "1. INTRODUCTION", "text": "In recent years, interest in generalized information measures has increased dramatically, one reason being the maximization of Shannon entropy, which leads to exponential distributions of power, a generalization known as non-exclusive entropy, introduced by Havrda & Charva. One reason is the fact that the maximization of Shannon entropy leads to exponential distributions, which are the Shannon-Khinchin axioms of Shannon distributions."}, {"heading": "2. BACKGROUND AND PRELIMINARIES", "text": "A. q-Gau\u00df's distribution Tsallis-Entropie can be generated by generalizing the q q q q q q q q q q q q q q (R, q > 0, q 6 = 1. Tsallis-Entropie is continuously generated as (Dukkipati et al., 2007) Hq (p) = 1 \u2212 X [p (x)] qdxq \u2212 1, q \u00b2 R, q > 0, q 6 \u2212 \u2212 \u2212 1, (1) This function produces the differential Shannon entropy functionally as q \u2192 1. it is called qdxq \u2212 1, x-x, Kullback's minimal discrimination theorem (Kullback, 1959) x-x-x-x-x-x-x-x-x-x-x-x x x x-x x x x x-x x x x-x x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "B. Kernels in Machine Learning", "text": "One of the basic problems with machine learning is to obtain a map between an input space X and an output space Y. The goal varies according to the type of problem. In linear learning methods, the Euclidean distance between the data points is used to distinguish them. In other words, the point product between two vectors is used as a measure of similarity between them. However, this approach does not work well if the data are not linearly separable. In such cases, a better method, known as the kernel-based approach (Scholko \ufffd pf & Smola, 2002), is to transform the data by mapping into a higher-dimensional space H: X 7 \u2192 H, so that the data can be linearly separable into H. The similarity between two points in this transformed space, given by a core function K: X \u00d7 X 7 \u2192 R, is defined as K (x, y) as positive space H (x) xxi (y) x) implicit."}, {"heading": "3. THE PROPOSED KERNEL", "text": "Based on the multidimensional expression of the q-Gaussian core (4) proposed by Vignat & Plastino (2007), we define the q-Gaussian core Kq: X \u00b7 X 7 \u2192 R asKq (x, y) = expq (\u2212 x \u2212 y) 2 (3 \u2212 q) \u03c32) for all x, y X, (8), where X-RN and q-RN and q-Y-R are two parameters that control the behavior of the core and meet conditions q 6 = 1, q 6 = 3 and \u03c3 6 = 0. For 1 < q < 3 the term inside the bracket is not negative and therefore the kernel can be written as Kq (x, y) = (1 + (q \u2212 1) (3 \u2212 q) (3 \u2212 q) (3 \u2212 q) (\u00b2 x \u2212 y)."}, {"heading": "A. Positive Definiteness", "text": "Q Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q"}, {"heading": "B. Relation with common kernels", "text": "Here we show that two popular kernels can be extracted as special cases of the q-Gaussian kernel. (i) Gaussian kernel: The Gaussian kernel is defined as: 1 (x, y) = exp (\u2212 \u0442 x \u2212 y, 22\u03c32), (13), where it is defined as a \"Rational Quadratic Kernel,\" as: 2 (x, y) = (1 \u2212 x \u2212 y, 2 x \u2212 y, 2 + c), (14), where c-R, c > 0. If we put q = 2 in (9), we get (14) with c = \u03c32."}, {"heading": "4. NOTE ON REPRODUCING KERNEL HILBERT SPACE", "text": "Smola et al. (1998) showed that the importance of RKHS for supporting vector nuclei is very widespread with Bochner's theorem (Bochner et al., 1959), which provides an RKHS in the Fourier space for translation invariant nuclei. Other approaches also exist that lead to an explicit description of the Gaussian nucleus (Steinwart et al., 2006), but such an approach does not work for the q-Gaussian case, since the binomic series extension of q-Gaussian does not converge for q > 1. So we follow Bochner's approach."}, {"heading": "A. Realization of RKHS", "text": "We present Bochner's theorem and then use the method presented in (Hofmann et al., 2008) to show how it can be used to construct the RKHS for a p.d. kernel definition 4.1 (Bochner)."}, {"heading": "1.25 1.696 1.329 2.635", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.50 1.428 1.234 2.782", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.75 1.200 1.127 2.947", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.25 0.805 0.826 3.420", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.50 0.613 0.595 3.825", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.75 0.453 0.375 4.031", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.90 0.364 0.293 3.403", "text": "It can be checked whether the values of the constants in Table 1 comply with sentence 4.5. Furthermore, the figure above clearly confirms assumption 4.4."}, {"heading": "5. PERFORMANCE COMPARISON", "text": "In this section, we apply the q-Gauss kernel in various learning algorithms, using illustrative examples to provide insight into q-Gauss behavior, and compare the performance of the kernel for different values of q, as well as with the Gauss kernel, using different data sets from the UCI repository (Frank & Asuncion, 2010)."}, {"heading": "A. Kernel SVM", "text": "Support vector machines (SVMs) are one of the most important classes of kernel machines. While linear q models that use an internal product as a measure of similarity are quite common, in practice other variants with different kernel functions, mostly Gaussian, are also used. The use of cores leads to non-linear q hyperplanes, which sometimes offer a better classification. We formulate an SVM based on the proposed kernel, which would lead to an optimization problem with the following dual form: min \u03b1-Rn-1-2 n-i, j = 1-2-3-jyiyj expq (\u2212 xi \u2212 xj \u00b2 2 (3 \u2212 q) \u03c32) s.t., i > 0, i = 1, 2,.. n, andn = 1-iyi = 0, where, {x1,.., xn} and the ausn classes are very different."}, {"heading": "B. Kernel k-means clustering", "text": "To avoid this problem, the data can be mapped to a higher dimensional attribute space using a non-linear function, and then k-means clustering is applied in this space. We compare the performance of q-Gaussian kernel k averages with Gaussian and ordinary k averages. We use the purity of clusters as a measure of the performance defined as purity (B, C) = 1N m, k = 1 max j averages for sophisticated datasets. We compare the performance of q-Gaussian kernel k averages with Gaussian and ordinary k averages. We use the purity of clusters as a measure of the performance defined as purity (B, C) = 1N m, k = 1 max j averages for ordinary datasets."}, {"heading": "C. Regression", "text": "In basic linear regression models, the output function q = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 0 = 0 + M = 1 = 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 1 = 1 = 2 =.., M = 1, X2,..., XM} are the indicated data points and their corresponding function values are y1, y2,.., yM. The basic functions are usually used for such a model. We use (9) as the q-Gaussian basic functions for observation (X) = expq (\u2212 X \u2212 Xj = 2 = 2 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 0 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 ="}, {"heading": "6. CONCLUSION", "text": "In this paper, we proposed a generalization of the Gaussian nucleus. The main motivation stems from the generalization of the Gaussian distribution to power-legislative Gaussies, which are studied in statistical mechanics. Due to their power-law nature, the tails of the q-Gaussian nucleus fall slower than Gaussian. This fact can be used in learning algorithms, since distant data points are more similar when using q-Gaussian nuclei. We showed that the proposed nucleus is positively defined for all q-nuclei (1, 3). We also showed some results related to the RKHS of the q-Gaussian nucleus using Bochner's theorem and showed that this is a generalization of the RKHS for Gaussian and rational quadratic nuclei. We also demonstrated the use of the proposed nucleus in SVM, regression, and k-mean clustering.The force behavior was recognized in many problems associated with this mathematical analysis a long time ago."}], "references": [{"title": "Itineration of the internet over nonequilibrium stationary states in Tsallis statistics", "author": ["S. Abe", "N. Suzuki"], "venue": "Physical Review E,", "citeRegEx": "Abe and Suzuki,? \\Q2003\\E", "shortCiteRegEx": "Abe and Suzuki", "year": 2003}, {"title": "Scale-free statistics of time interval between successive earthquakes", "author": ["S. Abe", "N. Suzuki"], "venue": "Physica A: Statistical Mechanics and its Applications,", "citeRegEx": "Abe and Suzuki,? \\Q2005\\E", "shortCiteRegEx": "Abe and Suzuki", "year": 2005}, {"title": "Emergence of scaling in random networks", "author": ["A.L. Barab\u00e1si", "R. Albert"], "venue": "Science, 286:509\u2013512,", "citeRegEx": "Barab\u00e1si and Albert,? \\Q1999\\E", "shortCiteRegEx": "Barab\u00e1si and Albert", "year": 1999}, {"title": "Lectures on Fourier Integral", "author": ["S. Bochner"], "venue": null, "citeRegEx": "Bochner,? \\Q1959\\E", "shortCiteRegEx": "Bochner", "year": 1959}, {"title": "Kernel methods for Pattern Analysis", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor,? \\Q2004\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor", "year": 2004}, {"title": "On measuretheoretic aspects of nonextensive entropy functionals and corresponding maximum entropy prescriptions", "author": ["A. Dukkipati", "S. Bhatnagar", "M.N. Murty"], "venue": "Physica A: Statistical Mechanics and its Applications,", "citeRegEx": "Dukkipati et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dukkipati et al\\.", "year": 2007}, {"title": "Producing power-law distributions and damping word frequencies with two-stage language models", "author": ["S. Goldwater", "T.L. Griffiths", "M. Johnson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Goldwater et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2011}, {"title": "Quantification method of classification processes: Concept of structural a-entropy", "author": ["J. Havrda", "F. Charv\u00e1t"], "venue": null, "citeRegEx": "Havrda and Charv\u00e1t,? \\Q1967\\E", "shortCiteRegEx": "Havrda and Charv\u00e1t", "year": 1967}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Annals of Statistics,", "citeRegEx": "Hofmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2008}, {"title": "Information theory and statistical mechanics", "author": ["E.T. Jaynes"], "venue": "The Physical Review,", "citeRegEx": "Jaynes,? \\Q1957\\E", "shortCiteRegEx": "Jaynes", "year": 1957}, {"title": "Information theory and statistics", "author": ["S. Kullback"], "venue": null, "citeRegEx": "Kullback,? \\Q1959\\E", "shortCiteRegEx": "Kullback", "year": 1959}, {"title": "Nonextensive information theoretic kernels on measures", "author": ["A.F.T. Martins", "N.A. Smith", "E.P. Xing", "P.M.Q. Aguiar", "M.A.T. Figueiredo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "On estimating regression", "author": ["E.A. Nadaraya"], "venue": "Theory of Probability and its Applications,", "citeRegEx": "Nadaraya,? \\Q1964\\E", "shortCiteRegEx": "Nadaraya", "year": 1964}, {"title": "Manuale di economica politica", "author": ["V. Pareto"], "venue": "Societa Editrice Libraria,", "citeRegEx": "Pareto,? \\Q1906\\E", "shortCiteRegEx": "Pareto", "year": 1906}, {"title": "Probability density functionals and reproducing kernel Hilbert spaces", "author": ["E. Parzen"], "venue": "In Proceedings of the Symposium on Time Series Analysis,", "citeRegEx": "Parzen,? \\Q1963\\E", "shortCiteRegEx": "Parzen", "year": 1963}, {"title": "Nonextensive foundation of L\u00e9vy distributions", "author": ["D. Prato", "C. Tsallis"], "venue": "Physical Review E.,", "citeRegEx": "Prato and Tsallis,? \\Q1999\\E", "shortCiteRegEx": "Prato and Tsallis", "year": 1999}, {"title": "q-Gaussian distributions and multiplicative stochastic processes for analysis of multiple financial time series", "author": ["A.H. Sato"], "venue": "Journal of Physics: Conference Series,", "citeRegEx": "Sato,? \\Q2010\\E", "shortCiteRegEx": "Sato", "year": 2010}, {"title": "Learning with Kernels", "author": ["B. Scholk\u00f6pf", "A.J. Smola"], "venue": null, "citeRegEx": "Scholk\u00f6pf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Scholk\u00f6pf and Smola", "year": 2002}, {"title": "The connection between regularization operators and support vector kernels", "author": ["A.J. Smola", "B. Sch\u00f6lkopf", "K. M\u00fcller"], "venue": "Neural Networks,", "citeRegEx": "Smola et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Smola et al\\.", "year": 1998}, {"title": "An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels", "author": ["I. Steinwart", "D.R. Hush", "C. Scovel"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2006}, {"title": "Generalization of Shannon-Khinchin axioms to nonextensive systems and the uniqueness theorem for the nonextensive entropy", "author": ["H. Suyari"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Suyari,? \\Q2004\\E", "shortCiteRegEx": "Suyari", "year": 2004}, {"title": "Possible generalization of Boltzmann-Gibbs statistics", "author": ["C. Tsallis"], "venue": "Journal of Statiscal Physics,", "citeRegEx": "Tsallis,? \\Q1988\\E", "shortCiteRegEx": "Tsallis", "year": 1988}, {"title": "Central limit theorem and deformed exponentials", "author": ["C. Vignat", "A. Plastino"], "venue": "Journal of Physics A: Mathematical and Theoretical,", "citeRegEx": "Vignat and Plastino,? \\Q2007\\E", "shortCiteRegEx": "Vignat and Plastino", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "INTRODUCTION Power-law distributions were first studied in economics (Pareto, 1906) in the context of distribution of wealth.", "startOffset": 69, "endOffset": 83}, {"referenceID": 20, "context": "The Shannon-Khinchin axioms of Shannon entropy have been generalized to this case (Suyari, 2004), and this entropy functional has been studied in information theory, statistics and many other fields.", "startOffset": 82, "endOffset": 96}, {"referenceID": 11, "context": "INTRODUCTION Power-law distributions were first studied in economics (Pareto, 1906) in the context of distribution of wealth. Later power-law behavior was observed in various fields such as physics, biology, computer science etc. Barab\u00e1si & Albert (1999) observed this behavior in the World Wide Web.", "startOffset": 70, "endOffset": 255}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics.", "startOffset": 0, "endOffset": 401}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics.", "startOffset": 0, "endOffset": 437}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics. The Shannon-Khinchin axioms of Shannon entropy have been generalized to this case (Suyari, 2004), and this entropy functional has been studied in information theory, statistics and many other fields. Tsallis entropy has been used to study power-law behavior in different cases like earthquakes and network traffic (Abe & Suzuki, 2003, 2005). In kernel based machine learning (Scholk\u00f6pf & Smola, 2002), positive definite kernels are considered as a measure of similarity between points. The choice of kernel is critical to the performance of the learning algorithms, and hence, many kernels have been studied in literature (Cristianini & Shawe-Taylor, 2004). One of the most common kernel used in practical applications is the Gaussian kernel. Nonextensive kernels on probability measures, based on Tsallis divergences, have been proposed by Martins et al. (2009). In this paper, we propose a new kernel based on q-Gaussian, which is a generalization of the Gaussian distribution, obtained by maximizing Tsallis entropy under certain moment constraints.", "startOffset": 0, "endOffset": 1326}, {"referenceID": 6, "context": "Goldwater et al. (2011) used these distributions to study language models. In recent years, interest in generalized information measures has increased dramatically, one reason being while maximization of Shannon entropy gives rise to exponential distributions, these measures give power-law distributions. One such generalization is known as nonextensive entropy introduced by Havrda & Charv\u00e1t (1967), and then studied by Tsallis (1988) in statistical mechanics. The Shannon-Khinchin axioms of Shannon entropy have been generalized to this case (Suyari, 2004), and this entropy functional has been studied in information theory, statistics and many other fields. Tsallis entropy has been used to study power-law behavior in different cases like earthquakes and network traffic (Abe & Suzuki, 2003, 2005). In kernel based machine learning (Scholk\u00f6pf & Smola, 2002), positive definite kernels are considered as a measure of similarity between points. The choice of kernel is critical to the performance of the learning algorithms, and hence, many kernels have been studied in literature (Cristianini & Shawe-Taylor, 2004). One of the most common kernel used in practical applications is the Gaussian kernel. Nonextensive kernels on probability measures, based on Tsallis divergences, have been proposed by Martins et al. (2009). In this paper, we propose a new kernel based on q-Gaussian, which is a generalization of the Gaussian distribution, obtained by maximizing Tsallis entropy under certain moment constraints. The power-law nature of this distribution has been studied by Sato (2010). In fact, the value of q controls the nature of the power-law tails.", "startOffset": 0, "endOffset": 1590}, {"referenceID": 5, "context": "Tsallis entropy in the continuous case is defined as (Dukkipati et al., 2007)", "startOffset": 53, "endOffset": 77}, {"referenceID": 20, "context": "q-Gaussian distribution Tsallis entropy can be obtained by generalizing the information of a single event in the definition of Shannon entropy as shown by Tsallis (1988), where logarithm is replaced with q-logarithm defined as lnq x = x 1\u2212q\u22121 1\u2212q q \u2208 R, q > 0, q 6= 1.", "startOffset": 24, "endOffset": 170}, {"referenceID": 21, "context": "It is called nonextensive because of its pseudo-additive nature (Tsallis, 1988).", "startOffset": 64, "endOffset": 79}, {"referenceID": 10, "context": "Kullback\u2019s minimum discrimination theorem (Kullback, 1959) establishes important connections between statistics and information theory.", "startOffset": 42, "endOffset": 58}, {"referenceID": 9, "context": "A special case is Jaynes\u2019 maximum entropy principle (Jaynes, 1957), by which exponential distributions can be obtained by maximizing Shannon entropy functional, subject to some moment constraints.", "startOffset": 52, "endOffset": 66}, {"referenceID": 14, "context": "Regression using kernel models has been widely used in statistics (Parzen, 1963), where estimating a function is equivalent to a solving a variational problem in the RKHS.", "startOffset": 66, "endOffset": 80}, {"referenceID": 3, "context": "(1998) showed that the significance of RKHS for support vector kernels using Bochner\u2019s theorem (Bochner, 1959), which provides a RKHS in Fourier space for translation invariant kernels.", "startOffset": 95, "endOffset": 110}, {"referenceID": 19, "context": "Other approaches also exist which lead to explicit description of the Gaussian kernel (Steinwart et al., 2006).", "startOffset": 86, "endOffset": 110}, {"referenceID": 8, "context": "Realization of RKHS We state Bochner\u2019s theorem, and then use the method presented in (Hofmann et al., 2008) to show how it can be used to construct the RKHS for a p.", "startOffset": 85, "endOffset": 107}, {"referenceID": 12, "context": "Regression using kernel models has been widely used in statistics (Parzen, 1963), where estimating a function is equivalent to a solving a variational problem in the RKHS. Smola et al. (1998) showed that the significance of RKHS for support vector kernels using Bochner\u2019s theorem (Bochner, 1959), which provides a RKHS in Fourier space for translation invariant kernels.", "startOffset": 67, "endOffset": 192}, {"referenceID": 12, "context": "Another regression model is the Nadaraya-Watson estimator (Nadaraya, 1964), more commonly known as kernel regression, where the estimated function is", "startOffset": 58, "endOffset": 74}], "year": 2017, "abstractText": "The role of kernels is central to machine learning. Motivated by the importance of power law distributions in modeling, simulation and learning, in this paper, we propose a powerlaw generalization of the Gaussian kernel. This generalization is based on q-Gaussian distribution, which is a power-law distribution studied in context of nonextensive statistical mechanics. We prove that the proposed kernel is positive definite, and provide some insights regarding the corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical significance of qGaussian kernels in classification, regression and clustering, and present some simulation results.", "creator": "LaTeX with hyperref package"}}}