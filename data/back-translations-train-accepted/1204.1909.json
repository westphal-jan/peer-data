{"id": "1204.1909", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Apr-2012", "title": "Knapsack Based Optimal Policies for Budget-Limited Multi-Armed Bandits", "abstract": "In budget-limited multi-armed bandit (MAB) problems, the learner's actions are costly and constrained by a fixed budget. Consequently, an optimal exploitation policy may not be to pull the optimal arm repeatedly, as is the case in other variants of MAB, but rather to pull the sequence of different arms that maximises the agent's total reward within the budget. This difference from existing MABs means that new approaches to maximising the total reward are required. Given this, we develop two pulling policies, namely: (i) KUBE; and (ii) fractional KUBE. Whereas the former provides better performance up to 40% in our experimental settings, the latter is computationally less expensive. We also prove logarithmic upper bounds for the regret of both policies, and show that these bounds are asymptotically optimal (i.e. they only differ from the best possible regret by a constant factor).", "histories": [["v1", "Mon, 9 Apr 2012 15:56:56 GMT  (30kb)", "http://arxiv.org/abs/1204.1909v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["long tran-thanh", "archie c chapman", "alex rogers", "nicholas r jennings"], "accepted": true, "id": "1204.1909"}, "pdf": {"name": "1204.1909.pdf", "metadata": {"source": "CRF", "title": "Knapsack based Optimal Policies for Budget\u2013Limited Multi\u2013Armed Bandits", "authors": ["Long Tran\u2013Thanh", "Archie Chapman", "Alex Rogers", "Nicholas R. Jennings"], "emails": ["ltt08r@ecs.soton.ac.uk."], "sections": [{"heading": null, "text": "ar Xiv: 120 4.19 09v1 [cs.AI] 9A pr2 01"}, {"heading": "1 Introduction", "text": "The standard multi-armed bandit (MAB) problem was originally proposed by Robbins (1952) to choose the optimal order (Thantos), and presents one of the clearest examples of the trade-off between exploration and exploitation in amplification learning. In the standard MAB problem, therefore, there are K-arms of a single machine, each of which receives a reward or reward, which are drawn independently of an unknown distribution when an arm of the machine is drawn. Given this fact, an agent must decide which of these weapons he uses to draw. At each step, he pulls one of the weapons of the machine and receives a reward or reward. The objective of the agent is to maximize his return; this is the expected sum of the rewards he receives, via a sequence of pulls. As the reward distribution varies from arm to arm, the goal is to find the arm with the highest expected reward as early as possible, and then continue to play with the best arm."}, {"heading": "2 Model Description", "text": "The budget limited MAB model consists of a machine with K-arms, one of which must be drawn by the agent at each time step = B. By pulling the arm i, the agent must pay a drawing cost called ci, and receive a non-negative reward drawn from a distribution associated with that specific arm. The agent has a cost budget B, which he cannot exceed during his operating time (i.e. the total cost of pulling the arm cannot exceed this budget limit).Now, since reward values are typically limited in the real world, we assume that the reward distribution of each arm has limited supports. Let's name the mean value of the rewards the agent receives by pulling the arm (i.e. the total cost of pulling the arm i).Within our model, the agent's goal is to maximize the sum of the rewards he earns by pulling the arms of the machine, in relation to the sum of his budget B. However, the agent does not have to maximize the sum of any of the rewards he has to receive."}, {"heading": "3 The Algorithms", "text": "Considering the model described in the previous section, we now present two learning methods, KUBE and fractional KUBE, that efficiently meet the challenges discussed in Section 1. Remember that at each step of the algorithms, we determine the optimal set of weapons that provides the best estimated total reward. Due to the similarities of our MAB with unlimited knapsack problems, when the rewards are known, we use techniques from the unlimited knapsack area. Therefore, in this section, we first introduce the unlimited knapsack problem and then show how to use knapsack methods in our algorithms."}, {"heading": "3.1 The Unbounded Knapsack Problem", "text": "The unlimited snapping problem is formulated as follows: A snapping problem of weight capacity C is to be filled with several sets of K of different types of items. Each item type i-K has a corresponding value vi and weight wi, and the problem is to select a set that maximizes the total value of the items in the knapsack so that their total weight does not exceed the knapsack capacity C. The goal is to find the non-negative integers {xi} K = 1 fracture that maximizes the fracture: K \u2211 i = 1xivi, (5) s.t.K \u2211 i = 1xiwi \u2264 C, [1].K}: xi integer.Note: This problem is a generalization of the standard snapping problem in which xi {0, 1}; that is, each item type contains only one item, and we can either select it or not. The unlimited snapping problem is NP-hard. However, approximate methods have been proposed to solve (an approximate method)."}, {"heading": "3.2 KUBE", "text": "The KUBE algorithm is represented in algorithm 1: max.K = 1). Here we leave the time step, and Bt denotes the residual budget in time t \u2265 1, respectively. Note that it is initially only feasible if at least one of the arms can be pulled with the remaining budget. Specifically, if Bt < minj cj (i.e. the residual budget is smaller than the lowest drawing costs), then KUBE (steps 3 \u2212 4) stops. If arm pulling is still feasible, KUBE first pulls each arm once in the initial phase (steps 6 \u2212 7). Afterwards, in each step t > K, it estimates the best set of weapons according to their supreme confidence."}, {"heading": "3.3 Fractional KUBE", "text": "We now turn to the fractional version of KUBE, which follows the underlying concept of KUBE. It also approaches the underlying unlimited knapsack problem at each time step t to determine the frequency of the arms within the estimated best set of arms. However, it differs from KUBE in that it uses the fractionated unlimited knapsack method to approximate the unlimited knapsack problem in step 9 of algorithm 1. Crucially, fractional KUBE uses the fractional stress-based algorithm to solve the following fractionated unlimited knapsack problem at each t: maxK \u2211 i = 1mi, t (\u00b5 \u0441i, ni, t + 2 ln tni, t) s.t.K fraction i = 1mi, tci \u2264 Bt. (9) Considering that within KUBE the frequency of the arms within the approximate solution of the unlimited knapsack of the knapsack forms an optimal one."}, {"heading": "4 Performance Analysis", "text": "We will now focus on the analysis of the expected regret of KUBE and the fraction value of KUBE, defined by Equation 4. To this end, we can make some simplistic assumptions and define some useful terms in this section. Without loss of generality, we assume that the distribution of rewards for each arm has support in [0, 1], and that the draw cost for each i (our result can be scaled as appropriate for different sizes and costs). Let us assume from the Argmaxi \u00b5i / ci the highest true mean, that we are unique (but that our proofs do not exploit this fact at all). Let us leave dmin = minj 6 = I."}, {"heading": "5 Performance Evaluation", "text": "In fact, most of them are able to survive on their own."}, {"heading": "6 Conclusions", "text": "In this thesis, we have introduced two new algorithms, KUBE and fractional KUBE, for the budget-limited MAB problem. These algorithms sampled each arm in an initial phase and then determined an optimal set of weapons at each subsequent time step, according to the agent's current reward estimates plus a confidence interval based on the number of samples taken from each arm. KUBE uses this best set as the probability distribution that randomly selects the next arm to draw. Therefore, both algorithms do not explicitly separate fractional exploration from exploitation. We also have an O ln (B) theoretical upper limit for the performance regret of both algorithms, with KUBE being the budget limit. Furthermore, we have proven that the limits provided are asymptooptimal, but can only be increased by the regret factor."}], "references": [{"title": "Active learning in multi-armed bandits", "author": ["A. Antos", "V. Grover", "Cs. Szepesv\u00e1ri"], "venue": "In Proceedings of the Nineteenth International Conference on Algorithmic Learning Theory,", "citeRegEx": "Antos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2008}, {"title": "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits", "author": ["J-Y. Audibert", "R. Munos", "Cs. Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Finite\u2013time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Pure exploration for multi-armed bandit problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Proceedings of the Twentieth international conference on Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Approximation algorithms for budgeted learning problems", "author": ["S. Guha", "K. Munagala"], "venue": "In Proceedings of the Thirty-Ninth Annual ACM symposium on Theory of Computing,", "citeRegEx": "Guha and Munagala.,? \\Q2007\\E", "shortCiteRegEx": "Guha and Munagala.", "year": 2007}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "ournal of the American Statistical Association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "The Riemann Zeta Function", "author": ["A. Ivic", "editor"], "venue": null, "citeRegEx": "Ivic and editor.,? \\Q1985\\E", "shortCiteRegEx": "Ivic and editor.", "year": 1985}, {"title": "Average performance of greedy heuristics for the integer knapsack problem", "author": ["R. Kohli", "R. Krishnamurti", "P. Mirchandani"], "venue": "European Journal of Operational Research,", "citeRegEx": "Kohli et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kohli et al\\.", "year": 2004}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathemathics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "A utility-based adaptive sensing and multihop communication protocol for wireless sensor networks", "author": ["P. Padhy", "R.K. Dash", "K. Martinez", "N.R. Jennings"], "venue": "ACM Transactions on Sensor Networks,", "citeRegEx": "Padhy et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Padhy et al\\.", "year": 2010}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "Epsilon\u2013first policies for budget\u2013limited multi\u2013armed bandits", "author": ["L. Tran-Thanh", "A. Chapman", "J.E. Munoz de Cote", "A. Rogers", "N.R. Jennings"], "venue": "In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence,", "citeRegEx": "Tran.Thanh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tran.Thanh et al\\.", "year": 2010}, {"title": "Long\u2013term information collection with energy harvesting wireless sensors: A multiarmed bandit based approach", "author": ["L. Tran-Thanh", "A. Rogers", "N.R. Jennings"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Tran.Thanh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tran.Thanh et al\\.", "year": 2011}, {"title": "Multi-armed bandit algorithms and empirical evaluation", "author": ["J. Vermorel", "M. Mohri"], "venue": "European Conference on Machine Learning,", "citeRegEx": "Vermorel and Mohri.,? \\Q2005\\E", "shortCiteRegEx": "Vermorel and Mohri.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "In the standard MAB, this trade\u2013off has been effectively balanced by decision\u2013making policies such as upper confidence bound (UCB) and \u01ebn\u2013greedy (Auer et al., 2002).", "startOffset": 145, "endOffset": 164}, {"referenceID": 9, "context": "1 Introduction The standard multi\u2013armed bandit (MAB) problem was originally proposed by Robbins (1952), and presents one of the clearest examples of the trade\u2013off between exploration and exploitation in reinforcement learning.", "startOffset": 88, "endOffset": 103}, {"referenceID": 0, "context": "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm\u2013pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007).", "startOffset": 226, "endOffset": 292}, {"referenceID": 3, "context": "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm\u2013pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007).", "startOffset": 226, "endOffset": 292}, {"referenceID": 4, "context": "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm\u2013pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007).", "startOffset": 226, "endOffset": 292}, {"referenceID": 11, "context": "To address this limitation, a new bandit model, the budget\u2013limited MAB, was introduced (Tran-Thanh et al. 2010).", "startOffset": 87, "endOffset": 111}, {"referenceID": 9, "context": "For example, in many wireless sensor network applications, a sensor node\u2019s actions, such as sampling or data forwarding, consume energy, and therefore the number of actions is limited by the capacity of the sensor\u2019s batteries (Padhy et al. 2010).", "startOffset": 226, "endOffset": 245}, {"referenceID": 12, "context": "Furthermore, many of these scenarios require that sensors learn the optimal sequence of actions that can be performed, with the goal of maximising the long term value of the actions they take (Tran-Thanh et al., 2011).", "startOffset": 192, "endOffset": 217}, {"referenceID": 0, "context": "To this end, a variety of other related models have been studied recently, and, in particular, a number of researchers have focused on MABs with budget constraints, where arm\u2013pulling is costly and is limited by a fixed budget (Antos et al., 2008; Bubeck et al., 2009; Guha and Munagala, 2007). In these models, the agent\u2019s exploration budget limits the number of times it can sample the arms in order to estimate their rewards, which defines an initial exploration phase. In the subsequent cost\u2013free exploitation phase, an agent\u2019s policy is then simply to pull the arm with the highest expected reward. However, in many settings, it is not only the exploration phase, but the exploitation phase that is also limited by a cost budget. To address this limitation, a new bandit model, the budget\u2013limited MAB, was introduced (Tran-Thanh et al. 2010). In this model, pulling an arm is again costly, but crucially both the exploration and exploitation phases are limited by a single budget. This type of limitation is well motivated by several real\u2013world applications. For example, in many wireless sensor network applications, a sensor node\u2019s actions, such as sampling or data forwarding, consume energy, and therefore the number of actions is limited by the capacity of the sensor\u2019s batteries (Padhy et al. 2010). Furthermore, many of these scenarios require that sensors learn the optimal sequence of actions that can be performed, with the goal of maximising the long term value of the actions they take (Tran-Thanh et al., 2011). In such settings, each action can be considered as an arm, with a cost equal to the amount of energy needed to perform that task. Now, because the battery is limited, both the exploration (i.e. learning the rewards tasks) and exploitation (i.e. taking the optimal actions given reward estimates) phases are budget limited. Against this background, Tran-Thanh et al. (2010) showed that the budget\u2013 limited MAB cannot be derived from any other existing MAB model, and therefore, previous MAB learning methods are not suitable to efficiently deal with this problem.", "startOffset": 227, "endOffset": 1902}, {"referenceID": 8, "context": ", where B is the budget limit, whereas the theoretical best possible regret bound is typically a logarithmic function of the number of pulls (Lai and Robbins, 1985).", "startOffset": 141, "endOffset": 164}, {"referenceID": 7, "context": "However, since unbounded knapsack problems are known to be NP\u2013hard, the algorithm uses an efficient approximation method taken from the knapsack literature, called the density\u2013ordered greedy approach, in order to estimate the best set (Kohli et al., 2004).", "startOffset": 235, "endOffset": 255}, {"referenceID": 7, "context": "In more detail, the density\u2013 ordered greedy algorithm has O (K logK) computational complexity, where K is the number of item types (Kohli et al., 2004).", "startOffset": 131, "endOffset": 151}, {"referenceID": 1, "context": "Thus, we explore and exploit at the same time (for more details, see (Audibert et al., 2009; Auer et al., 2002)).", "startOffset": 69, "endOffset": 111}, {"referenceID": 2, "context": "Thus, we explore and exploit at the same time (for more details, see (Audibert et al., 2009; Auer et al., 2002)).", "startOffset": 69, "endOffset": 111}, {"referenceID": 2, "context": "That is, fractional KUBE can also be seen as the budget\u2013limited version of UCB (see (Auer et al., 2002) for more details of UCB).", "startOffset": 84, "endOffset": 103}, {"referenceID": 5, "context": "To prove this theorem, we will make use of the following version of the Chernoff\u2013 Hoeffding concentration inequality for bounded random variables: Theorem 2 (Chernoff\u2013Hoeffding inequality (Hoeffding, 1963)) Let X1, X2, .", "startOffset": 188, "endOffset": 205}, {"referenceID": 5, "context": "To prove this theorem, we will make use of the following version of the Chernoff\u2013 Hoeffding concentration inequality for bounded random variables: Theorem 2 (Chernoff\u2013Hoeffding inequality (Hoeffding, 1963)) Let X1, X2, . . . , Xn denote the sequence of random variables with common range [0, 1], such that for any 1 \u2264 t \u2264 n, we have E [Xt|X1, . . . , Xt\u22121] = \u03bc. Let Sn = 1 n \u2211n t=1 Xt. Given this, for any \u03b4 \u2265 0, we have: P (Sn \u2265 \u03bc+ \u03b4) \u2264 e \u22122n\u03b4 , P (Sn \u2264 \u03bc\u2212 \u03b4) \u2264 e \u22122n\u03b4 . The proof can be found, for example, in Hoeffding (1963). We now focus on the performance analysis of KUBE.", "startOffset": 82, "endOffset": 529}, {"referenceID": 2, "context": "In this case, the proof of the theorem for that particular value of T is along the same lines as that of Theorem 1 of (Auer et al., 2002).", "startOffset": 118, "endOffset": 137}, {"referenceID": 8, "context": "According to (Lai and Robbins, 1985), the best possible regret that an arm pulling algorithm can achieve within the domain of standard MABs is C ln (T ).", "startOffset": 13, "endOffset": 36}, {"referenceID": 2, "context": "This implies that both KUBE and fractional KUBE achieve O (lnT ) regret within the standard MAB domain, which is optimal (Auer et al., 2002; Lai and Robbins, 1985).", "startOffset": 121, "endOffset": 163}, {"referenceID": 8, "context": "This implies that both KUBE and fractional KUBE achieve O (lnT ) regret within the standard MAB domain, which is optimal (Auer et al., 2002; Lai and Robbins, 1985).", "startOffset": 121, "endOffset": 163}, {"referenceID": 13, "context": "POKER(Vermorel and Mohri, 2005), or UCB).", "startOffset": 5, "endOffset": 31}], "year": 2012, "abstractText": "In budget\u2013limited multi\u2013armed bandit (MAB) problems, the learner\u2019s actions are costly and constrained by a fixed budget. Consequently, an optimal exploitation policy may not be to pull the optimal arm repeatedly, as is the case in other variants of MAB, but rather to pull the sequence of different arms that maximises the agent\u2019s total reward within the budget. This difference from existing MABs means that new approaches to maximising the total reward are required. Given this, we develop two pulling policies, namely: (i) KUBE; and (ii) fractional KUBE. Whereas the former provides better performance up to 40% in our experimental settings, the latter is computationally less expensive. We also prove logarithmic upper bounds for the regret of both policies, and show that these bounds are asymptotically optimal (i.e. they only differ from the best possible regret by a constant factor).", "creator": "LaTeX with hyperref package"}}}