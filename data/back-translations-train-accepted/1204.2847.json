{"id": "1204.2847", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2012", "title": "Segmentation Similarity and Agreement", "abstract": "We propose a new segmentation evaluation metric, called segmentation similarity (S), that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation. We show that S is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance.", "histories": [["v1", "Thu, 12 Apr 2012 22:01:27 GMT  (41kb,D)", "https://arxiv.org/abs/1204.2847v1", "10 pages, Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2012), Montr\\'eal, Canada, June 2012"], ["v2", "Sat, 19 May 2012 19:15:56 GMT  (41kb,D)", "http://arxiv.org/abs/1204.2847v2", "10 pages, LaTeX, corrected a typo in equation 4"]], "COMMENTS": "10 pages, Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2012), Montr\\'eal, Canada, June 2012", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris fournier", "diana inkpen"], "accepted": true, "id": "1204.2847"}, "pdf": {"name": "1204.2847.pdf", "metadata": {"source": "META", "title": "Segmentation Similarity and Agreement", "authors": ["Chris Fournier", "Diana Inkpen"], "emails": ["cfour037@eecs.uottawa.ca", "diana@eecs.uottawa.ca"], "sections": [{"heading": "1 Introduction", "text": "iSe rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Segmentation Evaluation", "text": "Precision is the proportion of selected limits that coincide with a reference segmentation (Pevzner and Hearst, 2002, p. 3). These ratios are inappropriate for segmentation because they punish boundary exceedances as complete wrong decisions, causing them to drastically overestimate the error (Pevzner and Hearst, 2002, p. 3). Near wrong decisions are common in segmentation and can account for a large proportion of errors produced by a coder, and as an inter-annotator agreement they often show that they do not reflect a coder error, but the difficulty of task.Pk (Beeferman and Berger, 1999, pp. 198-200) 2 is a window-based measurement that attempts to resolve the strict near-miss assessment of precision, recall, and F\u03b2 measurement."}, {"heading": "2.2 Inter-Annotator Agreement", "text": "While the need to determine the consistency and reliability between coders for segmentation has been acknowledged, 3Georgescul et al. (2006, p. 48) notes that both FPs and FNs are weighted by 1 / N \u2212 k, and although there are \"equivalent possibilities of having one [FP] in an interval of k units,\" the total number of equivalent possibilities of having one [FN] in an interval of k units is less than (N \u2212 k), which makes the interpretation of a full FN as FN less likely than as an FN by Passonneau and Litman (1993), which adjusted the percentage agreement by Gale et al. (1992, p. 254) for use in segmentation, this percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the overall observed agreement of a possible majority agreement for each boundary."}, {"heading": "3 Segmentation Similarity", "text": "To discuss segmentation, the size (or mass) of a segment is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of notation used by Artstein and Poesio (2008), in which the set of: \u2022 Items is {i | i} with cardinality i; \u2022 Categories is {k | k \u00b2 K} with cardinality k; \u2022 Coders is {c | c \u00b2 C} with cardinality c; \u2022 Segmentations of an item i by a coder c are {s | s \u00b2 S}, where sic is specified with only one subscript, it means sc for all relevant items (i); and \u2022 Types of segmentation boundaries is {t | t \u00b2 with cardinality."}, {"heading": "3.1 Sources of Dissimilarity", "text": "Linear segmentation has three main types of errors: 1. s1 contains a boundary that is disabled in s2 by n PBs; 2. s1 contains a boundary that is not present in s2; or 3. s2 contains a boundary that is not present in s1. This type of error can be seen in Figure 1 and is considered a pair transfer of a boundary for error 1 and the insertion or deletion (depending on the perspective) of a boundary for error 2 and 3. Since we do not refer to segmentation as a reference or hypothesis, we call insertions and deletions both substitutions. It is important not to punish omissions in many segmentation tasks as complete failures, because coders often agree with the existence of a boundary but do not agree with its exact location. In the previous scenario, the assignment of a complete failure would mean that even a loosely agreed boundary, as in Figure 1, would be considered completely incompatible."}, {"heading": "3.2 Edit Distance", "text": "In S, concepts from Damereau-Levenshtein machining distance (Damereau, 1964; Levenshtein, 1966) are applied to the model segmentation machining distance as two operations: substitutions and transpositions.4 These two operations represent complete missteps or near missteps. With these two operations, a new, globally optimal minimum machining distance is applied to a pair of boundary distances to model the sources of dissimilarity identified by transposition. 5Near missteps that are fixed by transposition are punished as b PBs errors (where b is the number of transposed boundaries), as opposed to the 2b PBs errors that would punish them if they were considered two separate substitution operations. Transpositions can also be considered via n > 2 PBs errors (n-wise transpositions)."}, {"heading": "3.3 Method", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to move, to fight, to fight, to fight, to move, to fight, to fight, to move, to fight, to fight, to fight, to move, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3.4 Evaluating Automatic Segmenters", "text": "This improbability is the result of the individual encoders applying slightly different segmentation strategies (i.e. different granularity). Considering this, we propose that the best available assessment strategy for automatic segmentation methods is to directly compare performance with multiple encoders so that performance can be quantified in relation to human reliability and match. To assess whether an automatic segmenter performs as reliably as human performance, the interannotator agreement can be calculated with and without the involvement of an automatic segmentator, where an observed decrease in coefficients would mean that the automatic segmenter does not perform as reliably as the group of human coders. 7 This can be done independently for multiple automatic segmentators to compare them - provided that the coefficient-model random agreement is appropriate because it is calculated and quantified (across all segments)."}, {"heading": "3.5 Inter-Annotator Agreement", "text": "Similarly, human competitiveness is determined by Medelyan et al. (2009, pp. 1324-1325) and Medelyan (2009, pp. 143-145) by comparing drops in the consistency of the interindex. Random agreement occurs in segmentation when coders working with slightly different granularities agree on the basis of their codes, rather than their own innate segmentation. Interannotator agreement coefficients have been developed that require a variety of prior distribution limits to characterize random agreements, and attempt to determine a way to determine whether agreement is primarily based on coincidence or not, and to quantify reliability. Artstein and Poesio (2008) point out that most codiers are not limits."}, {"heading": "3.5.1 Scott\u2019s \u03c0", "text": "Proposed by Scott (1955), \u03c0 assumes that random concordance between programmers can be characterized as the proportion of items assigned by both programmers to category k (Eq.7). We calculate random concordance (A\u03c0a) as a pair mean S (scaled by the size of each item) to allow agreement to quantify near-miss decisions leniently, and random concordance (A\u03c0e) can be calculated as in Artstein and Poesio (2008). A\u03c0a = \u2211 i-I mass (i) \u00b7 S (si1, si2) \u0445 i-mass (i) (6) A\u03c0e = \u0445 k-K (P\u03c0e (k) 2 (7) We calculate random concordance per category as the percentage of boundaries assigned by all programmers (blessed) over the total number of potential boundaries for segments, such as P\u03c0e (k) 2 (7)."}, {"heading": "3.5.2 Cohen\u2019s \u03ba", "text": "The coefficient proposed by Cohen (1960) describes the random coincidence as individual distributions per coder, calculated as in Equations 9-10 using our definition of concordance (A\u03c0a) as already set out in the first row.A\u03baa = A \u03c0 a (9) A\u03bae = \u2211 k-K P\u03bae (k-c1) \u00b7 P\u03bae (k-c2) (10) We calculate the category probabilities as in Scott's \u03c0, but per coder as shown in Equation 11.P\u03bae (segt-c) = \u0445 i-I | boundaries (t, sic) | \u2211 i-I (mass (i) \u2212 1) (11).This adjusted coefficient appropriately estimates the random coincidence for segmentation evaluations where there are coding distortions."}, {"heading": "3.5.3 Fleiss\u2019s Multi-\u03c0", "text": "We use Artstein and Poesios (2008, p. 564) proposal to calculate the actual and expected match, and because all encoders evaluate all items, we express the match as a pair mean S between all encoders, as shown in equations 12-13, using only the equation 12.A\u03c0 * a = 1 (c 2) c \u2212 1 x m = 1 x x x x x n = m + 1 x i * I mass (i) \u00b7 S (sim, sin) \u2022 i * I (mass (i) \u2212 1) (12)"}, {"heading": "3.5.4 Fleiss\u2019s Multi-\u03ba", "text": "We use the suggestion of Artstein and Poesio (2008, extended version) to calculate the correspondence exactly as in \u03c0 *, but with separate distributions per encoder, as in equations 14-15.A\u0445 \u0432 a = A \u03c0 \u0445 a (14) A\u0445 \u0445 \u0445 \u0445 e = \u0445 k-K (1 (c 2) c \u2212 1 \u0445 m = 1 c \u2211 n = m + 1 P\u03bae (k | cm) \u00b7 P\u03bae (k | cn)) (15)"}, {"heading": "3.6 Annotator Bias", "text": "To determine the degree of bias in a group of encoder segmentations, we can use a measure of variance proposed by Artstein and Poesio (2008, p. 572), which is quantified in terms of the difference between expected match, if it is assumed that coincidence varies between encoders, and if it is assumed that this is not the case. B ="}, {"heading": "4 Experiments", "text": "To demonstrate the benefits of using S as opposed to WindowDiff (WD), we compare both metrics using a variety of sophisticated scenarios, and then compare our adjusted agreement coefficients with pairwise meanWD9 for the segmentations collected by Kazantseva and Szpakowicz (2012). Since WD is a penalty-based metric, it is shown as 1 \u2212 WD in this section, so it is easier to compare with S values. If we report this way, 1 \u2212 WD and S are both sufficient from [0, 1] where 1 is no error and 0 is maximum error. 9Permuted and the window size is recalculated for each pair."}, {"heading": "4.1 Segmentation Cases", "text": "In this scenario, both 1 \u2212 WD and S adequately miss that this case represents a maximum error, or 0. Although not shown here, both metrics also report a similarity of 1.0 when comparing identical segmentations. Full Missteps For the most serious error source, misses the full error (i.e. FPs and FNs), both metrics appropriately point to a reduction in the similarity of cases such as Figure 7, which is very similar (1 \u2212 WD = 0.8461). Where the two metrics differ, this type of error is ed.S responds to the increase in full missteps more linear, while WindowDiff can hastily report a maximum number of errors."}, {"heading": "4.2 Segmentation Mass Scale Effects", "text": "An error in a 100-unit segment should be considered less severe than an error in a 2-unit segment, because an additional limit within a 100-unit segment (e.g. Figure 9 with m = 100) would likely have a weak limit, while in a 4-unit segment the probability of an additional limit adjacent to two agreed limits should be small for most tasks, meaning that it is likely that the additional limit is an error, not a weak limit. To show that S is sensitive to the segment size, Figure 5c shows how S and 1-WD react with linearly increasing mass (4 \u2264 m \u2264 100) when comparing segments configured as in Figure 10 (with a match and a full miss)."}, {"heading": "4.3 Variation in Segment Sizes", "text": "When Pevzner and Hearst (2002) proposed WD, they showed that it is not as sensitive as Pk to deviations in the size of segments within a segment. To show this, they simulated how WD performs in a segmentation of 1000 segments with four differently evenly distributed ranges of internal segment sizes (keeping the mean at about 25 units), compared to hypotheses segmentation with errors (false positives, false negatives, and both) evenly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). For each segment size range and probability of error, 10 tests were performed, generating 100 hypotheses per experiment. If we repeat this simulation, we compare the stability of S versus WD, as shown in Table 1."}, {"heading": "4.4 Inter-Annotator Agreement Coefficients", "text": "In this case, it turns out that the adjusted interannotator agreement has coefficients on current sales segmentations of 27 coders of 20 chapters from Wilkie Collins \"novel The Moonstone, collected by Kazantseva and Szpakowicz (2012). Figure 11 shows a heat map of each chapter, representing the percentage of coders that have agreed on each potential limit. Comparing this heat map with the coefficients of the interannotator agreement in Table 2, it is easier to understand why certain chapters have lower reliability. Chapter 1 indicates the lowest percentage of the S-score in the table and also the highest bias (BS). One of the reasons for this low reliability can be attributed to the low reliability of the chapter (m) and to few coders (| c |), which makes it more sensitive to random coincidences (BS)."}, {"heading": "5 Conclusion and Future Work", "text": "We have proposed a segmentation evaluation metric that solves the most important problems faced by segmentation analysis today, including the inability to adequately quantify near-miscalculations in the evaluation of automatic segmentation and human performance; to punish errors equally (or, in configuration, in a way that corresponds to a specific segmentation task); to compare an automatic segmentator directly with human performance; to require a \"true\" reference and deal with multiple boundary types. S can be used to perform task-specific evaluation of automatic and human segmenters using multiple human judgments that are not impeded by the quirks of window-based metrics. In current and future work, we will show how S can be used to analyze hierarchical segmentation, and how S can be applied to linear segmentations with multiple boundary types."}, {"heading": "Acknowledgments", "text": "We thank Anna Kazantseva for her valuable feedback and her corpora, and Stan Szpakowicz, Martin Scaiano and James Cracknell for their feedback."}], "references": [{"title": "Inter-coder agreement for computational linguistics", "author": ["Ron Artstein", "Massimo Poesio."], "venue": "Computational Linguistics, 34(4):555\u2013596. MIT Press, Cambridge, MA, USA.", "citeRegEx": "Artstein and Poesio.,? 2008", "shortCiteRegEx": "Artstein and Poesio.", "year": 2008}, {"title": "Text Segmentation Using Exponential Models", "author": ["Doug Beeferman", "Adam Berger", "John Lafferty."], "venue": "Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 2:35\u201346. Association for Computational Linguistics, Strouds-", "citeRegEx": "Beeferman et al\\.,? 1997", "shortCiteRegEx": "Beeferman et al\\.", "year": 1997}, {"title": "Statistical models for text segmentation", "author": ["Doug Beeferman", "Adam Berger."], "venue": "Machine learning, 34(1\u20133):177\u2013210. Springer Netherlands, NL.", "citeRegEx": "Beeferman and Berger.,? 1999", "shortCiteRegEx": "Beeferman and Berger.", "year": 1999}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["Jacob Cohen."], "venue": "Educational and Psychological Measurement, 20(1):37\u201346. Sage, Beverly Hills, CA, USA.", "citeRegEx": "Cohen.,? 1960", "shortCiteRegEx": "Cohen.", "year": 1960}, {"title": "A technique for computer detection and correction of spelling errors", "author": ["Frederick J. Damerau."], "venue": "Communications of the ACM, 7(3):171\u2013176. Association for Computing Machinery, Stroudsburg, PA, USA.", "citeRegEx": "Damerau.,? 1964", "shortCiteRegEx": "Damerau.", "year": 1964}, {"title": "Measuring agreement for multinomial data", "author": ["Mark Davies", "Joseph L. Fleiss."], "venue": "Biometrics, 38(4):1047\u20131051. Blackwell Publishing Inc, Oxford, UK.", "citeRegEx": "Davies and Fleiss.,? 1982", "shortCiteRegEx": "Davies and Fleiss.", "year": 1982}, {"title": "The topic detection and tracking phase 2 (TDT2) evaluation plan", "author": ["George R. Doddington."], "venue": "DARPA Broadcast News Transcription and Understanding Workshop, pp. 223\u2013229. Morgan Kaufmann, Waltham, MA, USA.", "citeRegEx": "Doddington.,? 1998", "shortCiteRegEx": "Doddington.", "year": 1998}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L. Fleiss."], "venue": "Psychological Bulletin, 76(5):378\u2013382. American Psychological Association, Washington, DC, USA.", "citeRegEx": "Fleiss.,? 1971", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "User-oriented text segmentation evaluation measure", "author": ["Martin", "Franz", "J. Scott McCarley", "Jian-Ming Xu."], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 701\u2013702. Association for", "citeRegEx": "Martin et al\\.,? 2007", "shortCiteRegEx": "Martin et al\\.", "year": 2007}, {"title": "Estimating upper and lower bounds on the performance of word-sense disambiguation programs", "author": ["William Gale", "Kenneth Ward Church", "David Yarowsky."], "venue": "Proceedings of the 30th annual meeting of the Association for Computational Linguistics, pp.", "citeRegEx": "Gale et al\\.,? 1992", "shortCiteRegEx": "Gale et al\\.", "year": 1992}, {"title": "An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms", "author": ["Maria Georgescul", "Alexander Clark", "Susan Armstrong."], "venue": "Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pp. 144\u2013151. Association for", "citeRegEx": "Georgescul et al\\.,? 2006", "shortCiteRegEx": "Georgescul et al\\.", "year": 2006}, {"title": "TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages", "author": ["Marti A. Hearst."], "venue": "Computational Linguistics, 23(1):33\u201364. MIT Press, Cambridge, MA, USA.", "citeRegEx": "Hearst.,? 1997", "shortCiteRegEx": "Hearst.", "year": 1997}, {"title": "Topical Segmentation: a Study of Human Performance", "author": ["Anna Kazantseva", "Stan Szpakowicz."], "venue": "Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. As-", "citeRegEx": "Kazantseva and Szpakowicz.,? 2012", "shortCiteRegEx": "Kazantseva and Szpakowicz.", "year": 2012}, {"title": "Content Analysis: An Introduction to Its Methodology, Chapter 12", "author": ["Klaus Krippendorff."], "venue": "Sage, Beverly Hills, CA, USA.", "citeRegEx": "Krippendorff.,? 1980", "shortCiteRegEx": "Krippendorff.", "year": 1980}, {"title": "Content Analysis: An Introduction to Its Methodology, Chapter 11", "author": ["Klaus Krippendorff."], "venue": "Sage, Beverly Hills, CA, USA.", "citeRegEx": "Krippendorff.,? 2004", "shortCiteRegEx": "Krippendorff.", "year": 2004}, {"title": "On evaluation methodologies for text segmentation algorithms", "author": ["Sylvain Lamprier", "Tassadit Amghar", "Bernard Levrat", "Frederic Saubion"], "venue": "Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence, 2:19\u201326. IEEE Computer Society,", "citeRegEx": "Lamprier et al\\.,? 2007", "shortCiteRegEx": "Lamprier et al\\.", "year": 2007}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["Vladimir I. Levenshtein."], "venue": "Soviet Physics Doklady, 10(8):707\u2013710. American Institute of Physics, College Park, MD, USA.", "citeRegEx": "Levenshtein.,? 1966", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Human-competitive automatic topic indexing", "author": ["Olena Medelyan."], "venue": "PhD Thesis. University of Waikato, Waikato, NZ.", "citeRegEx": "Medelyan.,? 2009", "shortCiteRegEx": "Medelyan.", "year": 2009}, {"title": "Human-competitive tagging using automatic keyphrase extraction", "author": ["Olena Medelyan", "Eibe Frank", "Ian H. Witten."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 1318\u20131327. Association for Compu-", "citeRegEx": "Medelyan et al\\.,? 2009", "shortCiteRegEx": "Medelyan et al\\.", "year": 2009}, {"title": "Intention-based segmentation: human reliability and correlation with linguistic cues", "author": ["Rebecca J. Passonneau", "Diane J. Litman."], "venue": "Proceedings of the 31st annual meeting of the Association for Computational Linguistics, pp. 148\u2013155). Association for Com-", "citeRegEx": "Passonneau and Litman.,? 1993", "shortCiteRegEx": "Passonneau and Litman.", "year": 1993}, {"title": "A critique and improvement of an evaluation metric for text segmentation", "author": ["Lev Pevzner", "Marti A. Hearst."], "venue": "Computational Linguistics, 28(1):19\u201336. MIT Press, Cambridge, MA, USA.", "citeRegEx": "Pevzner and Hearst.,? 2002", "shortCiteRegEx": "Pevzner and Hearst.", "year": 2002}, {"title": "Reliability of content analysis: The case of nominal scale coding", "author": ["William A. Scott."], "venue": "Public Opinion Quarterly, 19(3):321\u2013325. American Association for Public Opinion Research, Deerfield, IL, USA.", "citeRegEx": "Scott.,? 1955", "shortCiteRegEx": "Scott.", "year": 1955}, {"title": "Nonparametric Statistics for the Behavioral Sciences", "author": ["Sidney Siegel", "N. John Castellan", "Jr."], "venue": "2nd Edition, Chapter 9.8. McGraw-Hill, New York, USA.", "citeRegEx": "Siegel et al\\.,? 1988", "shortCiteRegEx": "Siegel et al\\.", "year": 1988}], "referenceMentions": [{"referenceID": 15, "context": "It is not without issues though; Lamprier et al. (2007) demonstrated that WindowDiff penalizes errors less at the beginning and end of a segmentation (this is corrected by padding the segmentation at each end by size k).", "startOffset": 33, "endOffset": 56}, {"referenceID": 15, "context": "It is not without issues though; Lamprier et al. (2007) demonstrated that WindowDiff penalizes errors less at the beginning and end of a segmentation (this is corrected by padding the segmentation at each end by size k). Additionally, variations in the window size k lead to difficulties in interpreting and comparing WindowDiff\u2019s values, and the intuition of the method remains vague. Franz et al. (2007) proposed measuring performance in terms of the number of words that are FNs and FPs, normalized by the number of word positions present (see Equation 2).", "startOffset": 33, "endOffset": 406}, {"referenceID": 13, "context": "Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of \u03b1 (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp.", "startOffset": 213, "endOffset": 253}, {"referenceID": 14, "context": "Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of \u03b1 (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp.", "startOffset": 213, "endOffset": 253}, {"referenceID": 14, "context": "by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al.", "startOffset": 3, "endOffset": 32}, {"referenceID": 8, "context": "by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean \u03ba (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p.", "startOffset": 80, "endOffset": 455}, {"referenceID": 8, "context": "by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean \u03ba (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean \u03ba scores were calculated by comparing a coder\u2019s segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53\u201354). Although mean \u03ba scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan\u2019s (1988) \u03ba has declined in favour of other coefficients (Artstein and Poesio, 2008, pp.", "startOffset": 80, "endOffset": 1035}, {"referenceID": 0, "context": "Although mean \u03ba scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan\u2019s (1988) \u03ba has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555\u2013556). Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of \u03b1 (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp.", "startOffset": 198, "endOffset": 266}, {"referenceID": 0, "context": "For discussing segmentation, a segment\u2019s size (or mass) is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of the notation used by Artstein and Poesio (2008), where the set of:", "startOffset": 193, "endOffset": 220}, {"referenceID": 16, "context": "In S, concepts from Damereau-Levenshtein edit distance (Damereau, 1964; Levenshtein, 1966) are applied to model segmentation edit distance as two operations: substitutions and transpositions.", "startOffset": 55, "endOffset": 90}, {"referenceID": 20, "context": "Proposed by Scott (1955), \u03c0 assumes that chance agreement between coders can be characterized as the proportion of items that have been assigned to category k by both coders (Equation 7).", "startOffset": 12, "endOffset": 25}, {"referenceID": 0, "context": "We calculate agreement (Aa ) as pairwise mean S (scaled by each item\u2019s size) to enable agreement to quantify near misses leniently, and chance agreement (Ae ) can be calculated as in Artstein and Poesio (2008).", "startOffset": 183, "endOffset": 210}, {"referenceID": 3, "context": "Proposed by Cohen (1960), \u03ba characterizes chance agreement as individual distributions per coder, calculated as shown in Equations 9-10 using our definition of agreement (Aa ) as shown earlier.", "startOffset": 12, "endOffset": 25}, {"referenceID": 6, "context": "3 Fleiss\u2019s Multi-\u03c0 Proposed by Fleiss (1971), multi-\u03c0 (\u03c0\u2217) adapts Scott\u2019s \u03c0 for multiple annotators.", "startOffset": 2, "endOffset": 45}, {"referenceID": 3, "context": "Proposed by Davies and Fleiss (1982), multi-\u03ba (\u03ba\u2217) adapts Cohen\u2019s \u03ba for multiple annotators.", "startOffset": 12, "endOffset": 37}, {"referenceID": 12, "context": "To demonstrate the advantages of using S, as opposed to WindowDiff (WD), we compare both metrics using a variety of contrived scenarios, and then compare our adapted agreement coefficients against pairwise meanWD9 for the segmentations collected by Kazantseva and Szpakowicz (2012).", "startOffset": 249, "endOffset": 282}, {"referenceID": 11, "context": "When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation.", "startOffset": 17, "endOffset": 31}, {"referenceID": 12, "context": "Here, we demonstrate the adapted inter-annotator agreement coefficients upon topical paragraph-level segmentations produced by 27 coders of 20 chapters from the novel The Moonstone by Wilkie Collins collected by Kazantseva and Szpakowicz (2012). Figure 11 shows a heat map of each chapter where the percentage of coders who agreed upon each potential boundary is represented.", "startOffset": 212, "endOffset": 245}], "year": 2012, "abstractText": "We propose a new segmentation evaluation metric, called segmentation similarity (S), that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation. We show that S is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance.", "creator": "LaTeX with hyperref package"}}}