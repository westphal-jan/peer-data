{"id": "1204.5043", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2012", "title": "Sparse Prediction with the $k$-Support Norm", "abstract": "We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an L2 penalty and can also be interpreted as a group Lasso norm with overlaps. We show that this new norm provides a tighter relaxation than the elastic net and suggest using it as a replacement for the Lasso or the elastic net in sparse prediction problems.", "histories": [["v1", "Mon, 23 Apr 2012 12:35:56 GMT  (317kb)", "https://arxiv.org/abs/1204.5043v1", null], ["v2", "Tue, 12 Jun 2012 08:59:52 GMT  (304kb)", "http://arxiv.org/abs/1204.5043v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["andreas argyriou", "rina foygel", "nathan srebro"], "accepted": true, "id": "1204.5043"}, "pdf": {"name": "1204.5043.pdf", "metadata": {"source": "CRF", "title": "Sparse Prediction with the k-Support Norm", "authors": ["Andreas Argyriou"], "emails": ["argyriou@ttic.edu", "rina@uchicago.edu", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 4.50 43v2 ["}, {"heading": "1 Introduction", "text": "It is not as if we can engage in such an effort as we expect. (...) It is not as if we can engage in such an effort. (...) It is not as if we can engage in such an effort. (...) It is not as if we have to engage in such an effort. (...) It is as if we are waiting for such an effort. (...) It is as if we can dispense with such an effort. (...) It is as if we have to engage in such an effort. (...) It is as if we expect an effort. (...) It is as if we expect an effort. (...) It is as if we expect an effort. () It is as if we expect an effort. () It is as if we expect an effort. (...) It is as if we expect an effort. (...) It is as if we expect an effort."}, {"heading": "2.1 The Dual Norm", "text": "It is interesting and useful to calculate the duality of the k support norm. We follow the notation of [2] for ordered vectors: for each w support, | w | is the vector of absolute values, and w-i is the i-th largest element of w. We have the notation of [u] sp * k = max {< w, u >: [w] sp k \u2264 1} = max (\u2211 i-I u2i) 1 2: I-Gk = (k-i) 2) 1 2 =: 0 (2) (k).This is the 2-norm of the largest k entries in u and is known as the 2-k symmetric track norm [2]. It is not surprising that this dual norm is interpolated between the 2 norm (when k = d and all entries are taken) and the 2-K norm (when k = 1 and only the largest entry is taken)."}, {"heading": "2.2 Computation of the Norm", "text": "In this section, we derive an alternative formula for the k-support norm starting from which to calculate the value of the norm in O (d log d) steps.Proposition 2.1. For each w-round, b-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "3 Relation to the Elastic Net", "text": "We remember that the elastic mesh with the penalty parameters \"k,\" \"k,\" \"k,\" \"k,\" \"k,\" \"k,\" \"k,\" \"k,\" \"k,\" \"c,\" \"c,\" \"c,\" \"c,\" \"c,\" \"c,\" \"c,\" \"c,\" \"c,\" c, \"c.,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c.\" c. \"c.\" c. \"c.\" c. \"c.\" c. \"c,\" c \"c\" c, \"c\" c \"c,\" c \"c\" c, \"c.\" c c. \"c c c.\" c c c. \"c c.\" c c. \"c c c.\" c c. \"c.\" c. \"c.\" c. \"c.\" c. \"c.\" c. \"c.\" c. \"c.\" c. \"c.\" c \"c\" c \"c\" c \"c.\" c \"c\" c, \"c\" c \"c\" c \"c.\" c \"c\" c \"c,\" c \"c\" c \"c.\" c \"c.\" c, \"c\" c \"c\" c \"c\" c. \"c.\" c. \"c.\" c. \"c\" c. \"c.\" c. \"c.\" c. \"c.\" c. \"c.\" c. \"c.\" c. \"c\" c, c \"c\" c. \"c\" c. \"c, c\" c. \"c.\" c \"c.\" c. \"c.\" c. \"c\" c \"c.\" c \"c\" c. \"c\" c \"c.\" c. \"c.\" c c. \"c.\" c c c c c c. \"c c c c c.\" c c c c c. \"c.\" c c c c c. \"c c c c c c c c.\" c c c c c c c c c c c. \"c c c c c"}, {"heading": "4 Optimization", "text": "The solution of the optimization problem (5) can be efficiently performed with a first-order proximal algorithm = = proximal algorithm. Proximal methods - see [1, 4, 15, 17, 18] and references thereto - are used to solve composite problems of the form min {f (x) + \u03c9 (x): x Rd, with the loss function f (x) and the regulator (x) + convex functions, and f is smooth with an L-Lipschitz gradient. These methods require a quick calculation of the course f and the proximity operator proxxes (x) (x): = argmin {12% u \u2212 x = 2 (u): u-Rd}. In particular, accelerated first-order methods, proposed by Nesterov [14, 15] require two levels of memory at each iteration and exhibit an optimal convergence rate."}, {"heading": "5 Empirical Comparisons", "text": "We have the same data, which we have in recent years in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "6 Summary", "text": "We have introduced the k support standard as the narrowest convex loosening of sparseness plus regulation of 2 and have shown that it is tighter than the elastic network of 5http: / / www.csie.ntu.edu.tw / \u0445 cjlin / libsvmtools / datasets / 6With respect to other sparse prediction methods, we have not been able to compare ourselves to OSCAR due to memory constraints or due to PEN or trace lassos that do not have online code, as demonstrated in our empirical results. We note that the k support standard has better prediction properties but does not necessarily have better normalization-enhancing prediction properties than is demonstrated by narrower relativization."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal of Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Matrix Analysis", "author": ["R. Bhatia"], "venue": "Graduate Texts in Mathematics. Springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR", "author": ["H.D. Bondell", "B.J. Reich"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Signal recovery by proximal forwardbackward splitting", "author": ["P.L. Combettes", "V.R. Wajs"], "venue": "Multiscale Modeling and Simulation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Elastic-net regularization in learning theory", "author": ["C. De Mol", "E. De Vito", "L. Rosasco"], "venue": "Journal of Complexity,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Trace lasso: a trace norm regularization for correlated designs", "author": ["E. Grave", "G.R. Obozinski", "F. Bach"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "P\u00f3lya. Inequalities", "author": ["G.H. Hardy", "J.E. Littlewood"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1934}, {"title": "The Elements of Statistical Learning: Data Mining, Inference and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Kennard. Ridge regression: Biased estimation for nonorthogonal problems", "author": ["R.W.A.E. Hoerl"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1970}, {"title": "Group Lasso with overlap and graph Lasso", "author": ["L. Jacob", "G. Obozinski", "J.P. Vert"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "A modified finite Newton method for fast solution of large scale linear SVMs", "author": ["S.S. Keerthi", "D. DeCoste"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Exploiting covariate similarity in sparse regression via the pairwise elastic net", "author": ["A. Lorbert", "D. Eis", "V. Kostina", "D.M. Blei", "P.J. Ramadge"], "venue": "In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "CORE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Smoothness, low-noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Approximation accuracy, gradient methods, and error bound for structured convex optimization", "author": ["P. Tseng"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Covering number bounds of certain regularized linear function classes", "author": ["T. Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}], "referenceMentions": [{"referenceID": 10, "context": "In particular, the sample complexity 1 of learning a linear predictor with k non-zero entries by empirical risk minimization inside this class (an NP-hard optimization problem) scales as O(k log d), but relaxing to the constraint \u2016w\u20161 \u2264 k yields a sample complexity which scales as O(k log d), because the sample complexity of l1-regularized learning scales quadratically with the l1 norm [11, 19].", "startOffset": 389, "endOffset": 397}, {"referenceID": 18, "context": "In particular, the sample complexity 1 of learning a linear predictor with k non-zero entries by empirical risk minimization inside this class (an NP-hard optimization problem) scales as O(k log d), but relaxing to the constraint \u2016w\u20161 \u2264 k yields a sample complexity which scales as O(k log d), because the sample complexity of l1-regularized learning scales quadratically with the l1 norm [11, 19].", "startOffset": 389, "endOffset": 397}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Constraining (or equivalently, penalizing) both the l1 and l2 norms, as in (1), is known as the \u201celastic net\u201d [5, 20] and has indeed been advocated as a better alternative to the Lasso.", "startOffset": 110, "endOffset": 117}, {"referenceID": 19, "context": "Constraining (or equivalently, penalizing) both the l1 and l2 norms, as in (1), is known as the \u201celastic net\u201d [5, 20] and has indeed been advocated as a better alternative to the Lasso.", "startOffset": 110, "endOffset": 117}, {"referenceID": 9, "context": "To better understand the k-support norm, we show in Section 2 that it can also be described as the group lasso with overlaps norm [10] corresponding to all ( d k ) subsets of k features.", "startOffset": 130, "endOffset": 134}, {"referenceID": 19, "context": "These drawbacks have recently motivated the use of various other regularization methods, such as the elastic net [20], which penalizes the regression coefficients w with a combination of l1 and l2 norms:", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "The elastic net can be viewed as a trade-off between l1 regularization (the Lasso) and l2 regularization (Ridge regression [9]), depending on the relative values of \u03bb1 and \u03bb2.", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": "The pairwise elastic net (PEN), proposed by [13], has a penalty function that accounts for similarity among features: \u2016w\u2016 R = \u2016w\u20162 + \u2016w\u20161 \u2212 |w|\u22a4R|w| , where R \u2208 [0, 1]p\u00d7p is a matrix with Rjk measuring similarity between features Xj and Xk.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "The pairwise elastic net (PEN), proposed by [13], has a penalty function that accounts for similarity among features: \u2016w\u2016 R = \u2016w\u20162 + \u2016w\u20161 \u2212 |w|\u22a4R|w| , where R \u2208 [0, 1]p\u00d7p is a matrix with Rjk measuring similarity between features Xj and Xk.", "startOffset": 161, "endOffset": 167}, {"referenceID": 5, "context": "The trace Lasso [6] is a second method proposed to handle correlations within X , defined by \u2016w\u2016 X = \u2016Xdiag(w)\u2016\u2217 , where \u2016 \u00b7 \u2016\u2217 denotes the matrix trace-norm (the sum of the singular values) and promotes a low-rank solution.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Another existing penalty is OSCAR [3], given by \u2016w\u2016 c = \u2016w\u20161 + c \u2211", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "In fact, the k-support norm is equivalent to the norm used by the group lasso with overlaps [10], when the set of overlapping groups is chosen to be Gk (however, the group lasso has traditionally been used for applications with some specific known group structure, unlike the case considered here).", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "We follow the notation of [2] for ordered vectors: for any w \u2208 R, |w| is the vector", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "This is the l2-norm of the largest k entries in u, and is known as the 2-k symmetric gauge norm [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "For properties of such norms, see [2].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "We will use the inequality \u3008w, u\u3009 \u2264 \u3008w\u2193, u\u2193\u3009 [7].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "As typical in regularization-based methods, both \u03bb and k can be selected by cross validation [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 3, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 14, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 16, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 17, "context": "Proximal methods \u2013 see [1, 4, 15, 17, 18] and references therein \u2013 are used to solve composite problems of the form min{f(x) + \u03c9(x) : x \u2208 Rd}, where the loss function f(x) and the regularizer \u03c9(x) are convex functions, and f is smooth with an L-Lipschitz gradient.", "startOffset": 23, "endOffset": 41}, {"referenceID": 13, "context": "In particular, accelerated first-order methods, proposed by Nesterov [14, 15] require two levels of memory at each iteration and exhibit an optimal O ( 1 T 2 )", "startOffset": 69, "endOffset": 77}, {"referenceID": 14, "context": "In particular, accelerated first-order methods, proposed by Nesterov [14, 15] require two levels of memory at each iteration and exhibit an optimal O ( 1 T 2 )", "startOffset": 69, "endOffset": 77}, {"referenceID": 0, "context": "We can now apply a standard accelerated proximal method, such as FISTA [1], to (5), at each iteration using the gradient of the loss and performing a prox step using Algorithm 1.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "South African Heart Data This is a classification task which has been used in [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 11, "context": "20 Newsgroups This is a binary classification version of 20 newsgroups created in [12] which can be found in the LIBSVM data repository.", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "It is well understood that there is often a tradeoff between sparsity and good prediction, and that even if the population optimal predictor is sparse, a denser predictor often yields better predictive performance [20, 3, 10].", "startOffset": 214, "endOffset": 225}, {"referenceID": 2, "context": "It is well understood that there is often a tradeoff between sparsity and good prediction, and that even if the population optimal predictor is sparse, a denser predictor often yields better predictive performance [20, 3, 10].", "startOffset": 214, "endOffset": 225}, {"referenceID": 9, "context": "It is well understood that there is often a tradeoff between sparsity and good prediction, and that even if the population optimal predictor is sparse, a denser predictor often yields better predictive performance [20, 3, 10].", "startOffset": 214, "endOffset": 225}], "year": 2012, "abstractText": "We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an l2 penalty. We show that this new k-support norm provides a tighter relaxation than the elastic net and is thus a good replacement for the Lasso or the elastic net in sparse prediction problems. Through the study of the k-support norm, we also bound the looseness of the elastic net, thus shedding new light on it and providing justification for its use.", "creator": "LaTeX with hyperref package"}}}