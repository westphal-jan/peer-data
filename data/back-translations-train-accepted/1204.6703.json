{"id": "1204.6703", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2012", "title": "A Spectral Algorithm for Latent Dirichlet Allocation", "abstract": "Topic models can be seen as a generalization of the clustering problem, in that they posit that observations are generated due to multiple latent factors (e.g. the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic probability vectors (the distributions over words for each topic), when only the words are observed and the corresponding topics are hidden.", "histories": [["v1", "Mon, 30 Apr 2012 17:06:06 GMT  (23kb)", "http://arxiv.org/abs/1204.6703v1", null], ["v2", "Tue, 22 May 2012 02:08:38 GMT  (24kb)", "http://arxiv.org/abs/1204.6703v2", null], ["v3", "Tue, 3 Jul 2012 20:01:11 GMT  (34kb)", "http://arxiv.org/abs/1204.6703v3", null], ["v4", "Thu, 17 Jan 2013 21:01:29 GMT  (34kb)", "http://arxiv.org/abs/1204.6703v4", "Changed title to match conference version, which appears in Advances in Neural Information Processing Systems 25, 2012"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["anima anandkumar", "dean p foster", "daniel j hsu", "sham kakade", "yi-kai liu"], "accepted": true, "id": "1204.6703"}, "pdf": {"name": "1204.6703.pdf", "metadata": {"source": "CRF", "title": "Two SVDs Suffice: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation", "authors": ["Animashree Anandkumar", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Yi-Kai Liu"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 120 4.67 03v1 [cs.LWe provide a simple and efficient learning method that guarantees that the parameters for a wide class of mixing models are restored, including the popular latent dirichlet model (LDA).For LDA, the method correctly restores both the subject probability vectors and the previous ones across the topics, using only trigram statistics (i.e., third-order moments that can be estimated with documents containing only three words).The method, called Excess Correlation Analysis (ECA), is based on a spectral decomposition of low-order moments (third and fourth order) over two singular value decompositions (SVD).In addition, the algorithm is scalable since SVD operations are performed on k matrices, where k is the number of latent factors (e.g. the number of topics) and not in the d-dimensional observation space (typically ik ')."}, {"heading": "1 Introduction", "text": "It is generally agreed that there are several unobserved or latent factors influencing observed data. Mixture models provide a powerful framework for integrating the effects of these latent variables. A family of Mixture models, popularly known as theme models, has generated a broad interest on both theoretical and practical fronts.Topic models contain latent variables that explain the observed simultaneous occurrences of words in documents.They claim that each document has a mixture of active topics (possibly contributions to this work by NIST, a US government agency, are not subject to copyrights.sparse) and that each active topic determines the occurrence of words in the document. Normally, a didactic is assigned before the distribution of topics in documents, which leads to the so-called latent dirichlet assignments (LDA et al., Mlei et al., 2003). These models have a rich representational power, as they allow more words to be generated from each document than the others."}, {"heading": "1.1 Summary of Contributions", "text": "We present an approach known as Excess Correlation Analysis (ECA), based on knowledge of low-order moments between the observed variables, assumed to be interchangeable (or, more generally, from a multi-view mixing model). ECA differs from Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) in that it is based on two singular value decompositions: the first SVD knows the data (based on the correlation between two variables) and the second SVD uses higher-order moments (based on third or fourth order) to find directions that have moments suggested by a Gaussian distribution. Both SVDs are performed on matrices of magnitude k, where k is the number of latent factors, making the algorithm scalable (typically the dimension of observed space d, k).The method is applicable to a multimodal class of interchangeable models, including interchangeable ones."}, {"heading": "1.2 Related Work", "text": "There are few demonstrably correct methods for learning topics that encompass more than one topic: the current work by Arora et al. (2012) There are only a limited number of approaches that deal with the question of whether and how such a development can occur. (2012) There are few approaches that deal with the question of how this development can occur. (2012) There are few examples that deal with the question of how such a development can occur. (2012) There are only two different approaches that deal with the question of how such a development can occur."}, {"heading": "2 The Exchangeable and Multi-view Models", "text": "This vector specifies the latent factors (i.e. the hidden state), where hi indicates the value of the i-th factor. Name the variance of hi as\u03c32i = E [(hi \u2212 E [hi \u2212 E] 2]], which we assume is strictly positive for each i, and designate the higher l-th central moments of hi as: \u00b5i, l: = E [(hi \u2212 E [hi] l]. At most, we will only use the first four moments in our analysis. Suppose we also have a sequence of interchangeable random vectors {x1, x2, x3, x4,...}."}, {"heading": "2.1 Independent Latent Factors", "text": "Suppose h had a product distribution, i.e. each component of hi is independent of the rest. Two important examples of this setting are: (Multiple) mixtures of Gaussians: Suppose xv = Oh + \u03b7, where \u03b7 is Gaussian noise and h is a binary vector (below a product distribution). In this case, the i-th column Oi can be considered the mean of the i-th Gaussian component. This generalizes the classic mixture of k-Gaussian noise, since the model now allows any number of Gaussians to be responsible for generating the hidden state (i.e. h may be any of the 2k vectors on the hypercube, whereas in the classical mixing problem only one component is responsible). We can also allow \u03b7 to be heteroscedastic (i.e. the noise can depend on h, provided the linearity assumption E [xv | h] Oh holds."}, {"heading": "2.2 The Dirichlet Model", "text": "Suppose the hidden state h is a distribution itself, with a density specified by the Dirichlet distribution with the parameter \u03b1-Rk + (\u03b1 is a strictly positive real vector). We often consider h as a distribution over topics. Strictly speaking, the density of h-k-1 (where the probability Simplex-k-1 denotes the set of possible distributions over k results) is by: p\u03b1 (h): = 1Z (\u03b1) k-i = 1h\u03b1i \u2212 1iwhereZ (\u03b1): = \u0441k i = 1-x (\u03b10) and \u03b10: = 1 + \u03b12 + \u00b7 \u00b7 \u00b7 + \u03b1k. Intuitively, \u03b10 (the sum of the \"pseudo-counts\") is a rough measure of the uniformity of the distribution."}, {"heading": "2.3 The Multi-View Model", "text": "The Multiview setting can be considered an extension of the interchangeable model. Here, the random vectors {x1, x2, x3,..} of the dimensions d1, d2, d3,.. Instead of a single O-matrix, we assume that Ov is fully ranked for each v. Although the variables are no longer interchangeable, the setting shares much of the statistical structure as interchangeable. Moreover, it allows for significantly richer models. Anandkumar et al. (2012) consider a specific case of this Multiview model (where there is only one theme present in h)."}, {"heading": "3 Identifiability", "text": "The underlying question is: What can we hope to restore about O only with knowledge of the distribution on x1, x2, x3,...? At the other extreme, suppose that no a priori knowledge of the distribution of h is assumed (e.g., it cannot even be a product distribution), where we can at best restore the range of O. Suppose h is distributed according to a multivariate Gauss, then the columns of O are clearly not identifiable. To see this, we transform O into OM (where M is a k \u00d7 k invertible matrix) and transform the distribution to h (after M \u2212 1); after this transformation, the distribution on xv remains unchanged and the distribution on h is still a multivariate Gaussian. Therefore, O and OM are indistinguishable from any observable statistic. (These problems are well understood in the placement of an independent separation."}, {"heading": "4 Excess Correlation Analysis (ECA)", "text": "We now present precise and efficient algorithms for recovering O. The algorithm is based on two singular value decompositions: the first SVD presents the data (based on the correlation between algorithm 1 ECA, with distorted factor input: vector. (See Remark 1 for a quick procedure.) 2. Whiten: Find V'Rk'k so V '(U'PairsU) V is the k'k identity matrix. Set: W = UV3. SVD: Let the set of (right) singular vectors, with unique singular values, ofW'PairsU so V'. (U'PairsU) V is the k identity matrix. Set: W = UV3. SVD: Let's the set of (right) singular vectors, with unique singular values, ofW'PairsU's so V '."}, {"heading": "4.1 Independent and Skewed Latent Factors", "text": "The question that arises is whether this is some kind of distributional inequality or some kind of distributional inequality; the question of whether the distributional inequalities are actually as large as the distributional effects; the question of whether the distributional effects are actually as large as they are; the question of whether the distributional effects are actually as large as they are; the question of whether the distributional effects are as large as they are; the question of whether the distributional effects are as large as they are; the question of whether the distributional effects of the distributional effects are as large as they are; the question of whether the distributional effects of the distributional effects are as large as they are."}, {"heading": "4.2 Independent and Kurtotic Latent Factors", "text": "We define the following matrix: quadruples (\u03b7, \u03b7) singular vectors, with singular values. (x1 \u2212 \u00b5) (x2 \u2212 \u00b5) (x2 \u2212 \u00b5) (x3 \u2212 \u00b5, x4 \u2212 \u00b5 >] (\u03b7 Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (Pairs) (PairsU) (PairsU)))."}, {"heading": "4.3 Latent Dirichlet Allocation", "text": "Now let us turn to the case where h has a dirichlet-density, in which each hi is not sampled = 1x sampled. Even if the distribution to h is the product of h\u03b11 \u2212 1i,. h \u00b2 \u2212 1 i, the hi's are not independent due to the simplex. These dependencies point to a change for the moments to be used in ECA that we now provide. The common practice is, the whole parameter-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "4.4 The Multi-View Extension", "text": "It is not as if it is a. \"S\" S \"S, but a.\" S. \"(. S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S \"S.\" S. \"S\" S. \"S.\" S \"S.\" S. \"S.\" S. \"S.\" S \"S.\" S. \"S.\" S. \"S\" S. \"S\" S. \"S.\" S. \"S.\" S \"S\" S. \"S\" S. \"S.\" S \"S.\" S. \"S\" S. \"S\" S. \"S\" S \"S.\" S \"S.\" S \"S\" S. \"S\" S \"S.\" S \"S\" S. \"S\" S \"S.\" S \"S.\" S \"S\" S \"S.\" S \"S\" S \"S.\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S. \"S\" S \"S\" S \"S\" S. \"S\" S. \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\""}, {"heading": "5 Discussion: Sample Complexity and Sparsity", "text": "Sample Complexity and Matrix Perturbation: A detail sample complexity analysis will include the details of the noise model on xv. Furthermore, the sample complexity of obtaining an accurate whitening matrix W is well understood (e.g. see Anandkumar et al. (2012) for example). Generally, this sample complexity does not explicitly depend on dimension d; rather, it is dependent on the distribution properties of xv and the minimum singular value of pairs. Instead, we focus on the accuracy required in step 3 of the ECA, as this is the only novel aspect of the algorithm (say, compared to analyses of PCA and CCA). To focus on this problem, we assume that we have an exact whitening matrix W. Define the whitened random vector x = W (xv \u2212 E), and denote the empirical average by: E."}, {"heading": "Acknowledgements", "text": "We thank Kamalika Chaudhuri, Adam Kalai, Percy Liang, Chris Meek, David Sontag and Tong Zhang for many valuable insights."}, {"heading": "A Analysis with Independent Factors", "text": "Lemma A.1. (Hidden state moments) Let z = h \u2212 E [h] [h]. For all vectors u, v = > Rk, E [zz's] = diag (i, 3, 2,3,., E [zz's < u, z >] = diag (u) diag (u) diag (u) diag (i, 3, 2,3,.,.. \u2212 k, 3) and E [zz's < v, z >] = diag (v) diag (u) diag (u) diag (u) s 41, ul 2,4 \u2212 3\u0441\u04424 \u2212 4k) + (u E [zz's] v [zz's] + (E [zz's]) diag (v) diag (u's) diag (u's) diag (u's) diag (u's) diag (u's)."}, {"heading": "B Analysis with Dirichlet Factors", "text": "We first supply the functions of the first, second and third moment, thus proving Lemma 4.3.B.1 Dirichlet MomentsLemma B.1. (Dirichlet moments) We have: E [h 'h] = 1 (h' h) = 1 (h 'h) + 1 (h' h) + 1 (h 'h) + 1 (h' h) + 1 (h 'h) + 1 (h' h) + 2 (h 'h' h) + 2 (h 'h' h) + 2 (h 'h' h) + 2 (h 'h' h + 2 (h 'h) + 2 (h' h) + 2 (h 'h' h) + 2 (h 'h' h 'h) (h' h) + 1 h (h 'h) + 1 h (h' h) + 1 h (h 'h) (h' h) (h 'h) + 1 h (h)"}, {"heading": "C Sample Complexity", "text": "Let us examine the complexity relevant to step 3 of the ECA. To focus on this issue, let us assume that we have an exact W brightening matrix. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "D Concentration and Matrix Perturbation Lemmas", "text": "Lemma D.1. (by Dasgupta and Gupta (2003) Let us assume a random vector evenly distributed over Sn \u2212 1, and a vector against Rn.1. If \u03b2 (0, 1), thenPr [< \u03b8, v > | \u2264 \u0432 v 2 \u00b7 1 \u221a n \u00b7 \u03b2] \u2264 exp (12 (1 \u2212 \u03b22 + ln \u03b22) This is a special case of Lemma 2.2 by Dasgupta and Gupta (2003).Lemma D.2 (Wedins Theorem 2 \u00b7 1 \u221a n \u00b7 \u03b2] \u2264 exp (12 (1 \u2212 \u03b22 + ln \u03b22).Proof. This is a special case of Lemma 2.2 by Dasgupta and Gupta (2003).Lemma D.2 (Wedins Theorem; Theorem 4.1, p. 260 in Stewart and Sun (1990)."}], "references": [{"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": null, "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Two-view feature generation model for semi-supervised learning", "author": ["R. Ando", "T. Zhang"], "venue": "In ICML,", "citeRegEx": "Ando and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Ando and Zhang.", "year": 2007}, {"title": "Learning topic models \u2014 going beyond svd", "author": ["Saneev Arora", "Rong Ge", "Ankur Moitra"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Distributed gibbs sampling for latent variable models", "author": ["A. Asuncion", "P. Smyth", "M. Welling", "D. Newman", "I. Porteous", "S. Triglia"], "venue": "In Scaling Up Machine Learning: Parallel and Distributed Approaches. Cambridge Univ Pr,", "citeRegEx": "Asuncion et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2011}, {"title": "Independent component analysis, a survey of some algebraic methods", "author": ["Jean-Franois Cardoso", "Pierre Comon"], "venue": "In IEEE International Symposium on Circuits and Systems,", "citeRegEx": "Cardoso and Comon.,? \\Q1996\\E", "shortCiteRegEx": "Cardoso and Comon.", "year": 1996}, {"title": "Full reconstruction of Markov models on evolutionary trees: Identifiability and consistency", "author": ["J.T. Chang"], "venue": "Mathematical Biosciences,", "citeRegEx": "Chang.,? \\Q1996\\E", "shortCiteRegEx": "Chang.", "year": 1996}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["K. Chaudhuri", "S. Rao"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "In ICML,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Handbook of Blind Source Separation: Independent Component Analysis and Applications", "author": ["P. Comon", "C. Jutten"], "venue": "Academic Press. Elsevier,", "citeRegEx": "Comon and Jutten.,? \\Q2010\\E", "shortCiteRegEx": "Comon and Jutten.", "year": 2010}, {"title": "Learning the parts of objects by nonnegative matrix", "author": ["Daniel D. Lee", "H. Sebastian Seung"], "venue": "Computer Science,", "citeRegEx": "Lee and Seung.,? \\Q2007\\E", "shortCiteRegEx": "Lee and Seung.", "year": 2007}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "Ji-Guang Sun"], "venue": "SIAM Review,", "citeRegEx": "Stewart and Sun.,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun.", "year": 1990}, {"title": "Complexity Let us examine on the sample complexity relevant for Step 3 of ECA. To focus on this issue, suppose that we have an exact whitening matrix W", "author": ["C Sample"], "venue": null, "citeRegEx": "Sample,? \\Q2012\\E", "shortCiteRegEx": "Sample", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "ExpectationMaximization (EM) (Redner and Walker, 1984), Gibbs sampling (Asuncion et al., 2011), and variational approaches (Hoffman et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 0, "context": "For this setting, we establish that ECA correctly recovers the parameters and is simpler than the eigenvector decomposition methods of Anandkumar et al. (2012). Finally, while our presentation focuses on utilizing known statistics of the observed moments to emphasize the correctness of the methodology, \u201cplug-in\u201d moment estimates can be used with sampled data.", "startOffset": 135, "endOffset": 160}, {"referenceID": 4, "context": "The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996).", "startOffset": 100, "endOffset": 125}, {"referenceID": 1, "context": "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception.", "startOffset": 154, "endOffset": 174}, {"referenceID": 1, "context": "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions.", "startOffset": 154, "endOffset": 224}, {"referenceID": 1, "context": "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions. These results also apply to certain settings with correlated topics. Our approach does not require this assumption 2, and our algorithm is affine invariant. The underlying approach taken is a certain diagonalization technique of the observed moments. We know of at least two different settings which utilize this idea for parameter estimation. Chang (1996) utilizes eigenvector methods for discrete Markov models of evolution, where the models involve multinomial distributions.", "startOffset": 154, "endOffset": 676}, {"referenceID": 1, "context": "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions. These results also apply to certain settings with correlated topics. Our approach does not require this assumption 2, and our algorithm is affine invariant. The underlying approach taken is a certain diagonalization technique of the observed moments. We know of at least two different settings which utilize this idea for parameter estimation. Chang (1996) utilizes eigenvector methods for discrete Markov models of evolution, where the models involve multinomial distributions. The idea has been extended to other discrete mixture models such as discrete hidden Markov models (HMMs) and mixture models with single active topics (see Mossel and Roch (2006); Hsu et al.", "startOffset": 154, "endOffset": 976}, {"referenceID": 1, "context": "2 Related Work There are relatively few provably correct methods for learning topic models when documents contain more than one topic; the recent work of Arora et al. (2012) provides a notable exception. Arora et al. (2012) provides a non-negative matrix factorization approach, under natural separability assumptions. These results also apply to certain settings with correlated topics. Our approach does not require this assumption 2, and our algorithm is affine invariant. The underlying approach taken is a certain diagonalization technique of the observed moments. We know of at least two different settings which utilize this idea for parameter estimation. Chang (1996) utilizes eigenvector methods for discrete Markov models of evolution, where the models involve multinomial distributions. The idea has been extended to other discrete mixture models such as discrete hidden Markov models (HMMs) and mixture models with single active topics (see Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al.", "startOffset": 154, "endOffset": 995}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)).", "startOffset": 8, "endOffset": 33}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs).", "startOffset": 8, "endOffset": 62}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models.", "startOffset": 8, "endOffset": 348}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian).", "startOffset": 8, "endOffset": 743}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996).", "startOffset": 8, "endOffset": 1319}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the \u201ctopic\u201d modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation.", "startOffset": 8, "endOffset": 1351}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the \u201ctopic\u201d modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al.", "startOffset": 8, "endOffset": 1589}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the \u201ctopic\u201d modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al.", "startOffset": 8, "endOffset": 1608}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the \u201ctopic\u201d modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012), in that the eigenvector methods are no longer necessary (e.", "startOffset": 8, "endOffset": 1634}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the \u201ctopic\u201d modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012), in that the eigenvector methods are no longer necessary (e.g. the approach leads to methods for parameter estimation in HMMs with only two SVDs rather than using eigenvector approaches, as in previous work). Showing that estimating the third order moments is not as difficult as it might naively seem since we only need a k \u00d7 k matrix to be accurate. For a detailed discussion of these techniques, refer to Anandkumar et al. (2012). The underlying intuition and idea of separability in Arora et al.", "startOffset": 8, "endOffset": 2067}, {"referenceID": 0, "context": "(2009); Anandkumar et al. (2012)). A key idea in Chang (1996) is the ability to handle multinomial distributions, which comes at the cost of being able to handle only certain single latent factor/topic models (where the latent factor is in only one of k states, such as in HMMs). For these single topic models, the work in Anandkumar et al. (2012) shows how this method is quite general in that the noise model is essentially irrelevant, making it applicable to both discrete models like HMMs and certain Gaussian mixture models. The second setting is the body of algebraic methods used for the problem of blind source separation (Cardoso and Comon, 1996). These approaches rely on tensor decomposition approaches (see Comon and Jutten (2010)) tailored to independent source separation with additive noise (usually Gaussian). Much of literature focuses on understanding the effects of measurement noise (without assuming knowledge of their statistics) on the tensor decomposition, which often requires more sophisticated algebraic tools. The underlying insight that our method exploits is that we have exchangeable (or multi-view) variables, e.g. we have multiple words (or sentences) in a document, which are drawn independently from the same hidden state. This allows us to borrow from both the ideas in Chang (1996) and in Cardoso and Comon (1996). In particular, we show that the \u201ctopic\u201d modeling problem exhibits a rather simple algebraic solution, where only two SVDs suffice for parameter estimation. Moreover, this approach also simplifies the algorithms in Mossel and Roch (2006); Hsu et al. (2009); Anandkumar et al. (2012), in that the eigenvector methods are no longer necessary (e.g. the approach leads to methods for parameter estimation in HMMs with only two SVDs rather than using eigenvector approaches, as in previous work). Showing that estimating the third order moments is not as difficult as it might naively seem since we only need a k \u00d7 k matrix to be accurate. For a detailed discussion of these techniques, refer to Anandkumar et al. (2012). The underlying intuition and idea of separability in Arora et al. (2012) may help for practical guidance in the choice of \u03b8 to help reduce the sample size, in terms of the k dependence.", "startOffset": 8, "endOffset": 2141}, {"referenceID": 3, "context": "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al.", "startOffset": 73, "endOffset": 86}, {"referenceID": 0, "context": "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model.", "startOffset": 87, "endOffset": 112}, {"referenceID": 0, "context": "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al.", "startOffset": 87, "endOffset": 344}, {"referenceID": 0, "context": "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al.", "startOffset": 87, "endOffset": 370}, {"referenceID": 0, "context": "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al.", "startOffset": 87, "endOffset": 396}, {"referenceID": 0, "context": "This construction bridges the gap between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model. More generally, the multi-view approach has been exploited in previous works for semi-supervised learning and for learning mixtures of well-separated distributions (e.g as in Ando and Zhang (2007); Kakade and Foster (2007); Chaudhuri and Rao (2008); Chaudhuri et al. (2009)).", "startOffset": 87, "endOffset": 421}, {"referenceID": 0, "context": "For example, Anandkumar et al. (2012) consider a special case of this multi-view model (where there is only one topic present in h) for the purposes of learning hidden Markov models.", "startOffset": 13, "endOffset": 38}, {"referenceID": 0, "context": "For example, Anandkumar et al. (2012) consider a special case of this multi-view model (where there is only one topic present in h) for the purposes of learning hidden Markov models. A simple factorial HMM: Here, suppose we have a time series of random hidden vectors h1, h2, h3, . . . and observations x1, x2, x3, . . . (we slightly abuse notation as h1 is a vector). Assume that each factor [ht]i \u2208 {\u22121, 1}. The model parameters and evolution are specified as follows: We have an initial (product) distribution over the first h1. The \u201cfactorial\u201d assumption we make is that each factor [ht]i evolves independently; in particular, for each component i, there are (time independent) transition probabilities pi,1\u2192\u22121 and pi,1\u2192\u22121. Also suppose that E[xt|ht] = Oht (where, again, O does not depend on the time). To learn this model, consider the first three observations x1, x2, x3. We can embed this three timestep model into the multiview model using a single hidden state, namely h2, and, with an appropriate construction (of O1, O2, O3 and means shifts of xv to make the linearity assumption hold). Furthermore, if we recover O1, O2, O3 we can recover O and the transition model. See Anandkumar et al. (2012) for further discussion of this idea (for the single topic case).", "startOffset": 13, "endOffset": 1209}, {"referenceID": 8, "context": "See Comon and Jutten (2010)).", "startOffset": 4, "endOffset": 28}, {"referenceID": 3, "context": "As discussed in the Introduction, these approaches can been seen as extensions of the methodologies in Chang (1996); Cardoso and Comon (1996).", "startOffset": 103, "endOffset": 116}, {"referenceID": 3, "context": "As discussed in the Introduction, these approaches can been seen as extensions of the methodologies in Chang (1996); Cardoso and Comon (1996). Furthermore, as we shall see, the Dirichlet distribution bridges between the single topic models (as in Chang (1996); Anandkumar et al.", "startOffset": 117, "endOffset": 142}, {"referenceID": 3, "context": "As discussed in the Introduction, these approaches can been seen as extensions of the methodologies in Chang (1996); Cardoso and Comon (1996). Furthermore, as we shall see, the Dirichlet distribution bridges between the single topic models (as in Chang (1996); Anandkumar et al.", "startOffset": 117, "endOffset": 260}, {"referenceID": 0, "context": "Furthermore, as we shall see, the Dirichlet distribution bridges between the single topic models (as in Chang (1996); Anandkumar et al. (2012)) and the independent factor model.", "startOffset": 118, "endOffset": 143}, {"referenceID": 0, "context": "(Limiting behaviors) ECA seamlessly blends between the single topic model (\u03b10 \u2192 0) of Anandkumar et al. (2012) and the skewness based ECA, Algorithm 1 (\u03b10 \u2192 \u221e).", "startOffset": 86, "endOffset": 111}, {"referenceID": 0, "context": "(Limiting behaviors) ECA seamlessly blends between the single topic model (\u03b10 \u2192 0) of Anandkumar et al. (2012) and the skewness based ECA, Algorithm 1 (\u03b10 \u2192 \u221e). In the single topic case, Anandkumar et al. (2012) provide eigenvector based algorithms.", "startOffset": 86, "endOffset": 212}, {"referenceID": 0, "context": "Using the methods in Anandkumar et al. (2012), eigenvector based method are straightforward to derive.", "startOffset": 21, "endOffset": 46}, {"referenceID": 0, "context": "(Simpler algorithms for HMMs) Mossel and Roch (2006); Anandkumar et al. (2012) provide eigenvector based algorithms for HMM parameter estimation.", "startOffset": 54, "endOffset": 79}, {"referenceID": 0, "context": "(Simpler algorithms for HMMs) Mossel and Roch (2006); Anandkumar et al. (2012) provide eigenvector based algorithms for HMM parameter estimation. These results show that we can achieve parameter estimation with only two SVDs (See Anandkumar et al. (2012) for the reduction of an HMM to the multi-view setting).", "startOffset": 54, "endOffset": 255}, {"referenceID": 0, "context": "see Anandkumar et al. (2012) for example).", "startOffset": 4, "endOffset": 29}], "year": 2012, "abstractText": "Topic models can be seen as a generalization of the clustering problem, in that they posit that observations are generated due to multiple latent factors (e.g. the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic probability vectors (the distributions over words for each topic), when only the words are observed and the corresponding topics are hidden. We provide a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e. third order moments, which may be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on k \u00d7 k matrices, where k is the number of latent factors (e.g. the number of topics), rather than in the d-dimensional observed space (typically d \u226b k).", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}