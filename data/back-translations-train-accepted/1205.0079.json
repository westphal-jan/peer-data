{"id": "1205.0079", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2012", "title": "Complexity Analysis of the Lasso Regularization Path", "abstract": "The regularization path of the Lasso can be shown to be piecewise linear, making it possible to \"follow\" and explicitly compute the entire path. We analyze in this paper this popular strategy, and prove that its worst case complexity is exponential in the number of variables. We then oppose this pessimistic result to an (optimistic) approximate analysis: We show that an approximate path with at most O(1/sqrt(epsilon)) linear segments can always be obtained, where every point on the path is guaranteed to be optimal up to a relative epsilon-duality gap. We complete our theoretical analysis with a practical algorithm to compute these approximate paths.", "histories": [["v1", "Tue, 1 May 2012 03:37:13 GMT  (56kb)", "https://arxiv.org/abs/1205.0079v1", "Accepted to the 29th International Conference on Machine Learning (ICML 2012). To appear in the proceedings"], ["v2", "Sat, 19 May 2012 21:06:21 GMT  (56kb)", "http://arxiv.org/abs/1205.0079v2", "To appear in the proceedings of 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Accepted to the 29th International Conference on Machine Learning (ICML 2012). To appear in the proceedings", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["julien mairal", "bin yu 0001"], "accepted": true, "id": "1205.0079"}, "pdf": {"name": "1205.0079.pdf", "metadata": {"source": "META", "title": "Complexity Analysis of the Lasso Regularization Path", "authors": [], "emails": ["julien@stat.berkeley.edu", "binyu@stat.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.00 79v2 [st at.M L] 19 May 201 2 \u221a \u03b5) it is always possible to obtain linear segments at which every point on the path is guaranteed to be optimal up to a relative \u03b5-duality gap. We complete our theoretical analysis with a practical algorithm to calculate these approximate paths."}, {"heading": "1. Introduction", "text": "It is often difficult to estimate a model or make predictions, either because the number of observations is too small or the problem dimension is too high. If a problem solution is known to be sparse, sparse-inducing penalties have proven useful in improving both the quality of the prediction and its integrity, especially the number of approaches to solving all the values of the regulation parameter is used in the lasso formulation (Tibshirani, 1996).Controlling the regulatory gap often requires fine-tuning of a parameter. In a few cases, the regulation path - that is, the set of solutions for all values of the regulation parameter can be shown to be piecewise linear (Rosset & Zhu, 2007).This property is exploited in homotopy methods consisting of the consequence of the cepiewise linear path, by calculating the direction of the current linear segment and changing the points at which are known as the kinks."}, {"heading": "2. Background on the Lasso", "text": "In this section, we present the Lasso formulation of Tibshirani (1996) and the known facts, which we evaluate later in our analysis. (J = J = J = J = J = J = J = J = J = J J = J = J J = J = J J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J = J J = J = J = J = J = J = J = J = J"}, {"heading": "3. Worst-Case Complexity", "text": "We note that the number of linear segments in the lasso regulation path is smaller than the number of linear segments in the lasso regulation path. We have already noticed that the number of linear segments in the lasso regulation path is smaller than the number of linear segments in the lasso regulation path. We have already noticed that the number of linear segments in the lasso regulation path is smaller than the number of linear segments in the lasso regulation path."}, {"heading": "3.1. Numerical Simulations", "text": "We have implemented algorithm 1 in Matlab that optimizes numerical precision independently of computing power, which has allowed us to test our theoretical results for small values of p. For example, we get a path with (3p + 1) / 2 = 88 574 linear segments for p = 11 and present such a pathological path in Figure 1. Note that when p becomes larger, these examples quickly lead to precision problems where some kinks are very close to each other. Our implementation and pathological examples are made publicly available. In the next section, we present more optimistic results on approximate regulatory paths."}, {"heading": "4. Approximate Homotopy", "text": "5 - 5 - 5 - 5 5 - 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 5 - 5 5 5 5 - 5 5 5 - 5 5 5 5 - 5 5 5 5 - 5 5 5 5 - 5 5 5 - 5 5 5 5 - 5 5 5 5 5 - 5 5 5 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 - 5 5 5 5 - 5 5 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 - 5 -"}, {"heading": "4.1. Numerical Simulations", "text": "We have implemented algorithm 2 with a few modifications of the code used in section 3.1. The inner solver is a coordinate descend algorithm (see Fu, 1998), with a stop criterion based on definition 2.We are looking at 4 datasets. The first is called SYNTH and consists of a pure noise fit scenario without statistical significance. The entries of the corresponding vector y and matrix X are i.i.d. Draws from a standard normal distribution. The next dataset is called PATHOL and is a pathological example derived from the analysis of section 3. Finally, we are looking at two datasets based on real data, each called MADELON5 and PCMAC6. For each dataset we center and normalize the columns of X and the vector y and select the parameter \u03bb1 according to the last kink of the true path."}, {"heading": "5. Conclusion", "text": "We have presented new results on the regularization path and thus on homotopy methods for the lasso. Firstly, we have shown that the path has an exponential worst-case complexity that, as far as we know, has never been formally proven. Secondly, our result is more optimistic and shows that when no exact path is required, only a relatively small number of points on the path need to be calculated. Finally, we propose a practical homotopy algorithm that can provide such approximate paths with the desired precision."}, {"heading": "Acknowledgments", "text": "This article was partially supported by NSF grants SES0835531, CCF-0939370, DMS-1107000, DMS-0907632 and ARO-W911NF-11-1-0114."}], "references": [{"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Convex analysis and nonlinear optimization: theory and examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": null, "citeRegEx": "Borwein and Lewis,? \\Q2006\\E", "shortCiteRegEx": "Borwein and Lewis", "year": 2006}, {"title": "Maximization of a linear function of variables subject to linear inequalities", "author": ["G.B. Dantzig"], "venue": "In Koopmans, T .C. (ed.), Activity Analysis of Production and Allocation, pp. 339\u2013347.Wiley,", "citeRegEx": "Dantzig,? \\Q1951\\E", "shortCiteRegEx": "Dantzig", "year": 1951}, {"title": "Penalized regressions: The bridge versus the Lasso", "author": ["W.J. Fu"], "venue": "J. Comput. Graph. Stat.,", "citeRegEx": "Fu,? \\Q1998\\E", "shortCiteRegEx": "Fu", "year": 1998}, {"title": "Recovery of exact sparse representations in the presence of bounded noise", "author": ["J.J. Fuchs"], "venue": "IEEE T. Inform. Theory.,", "citeRegEx": "Fuchs,? \\Q2005\\E", "shortCiteRegEx": "Fuchs", "year": 2005}, {"title": "An exponential lower bound on the complexity of regularization", "author": ["B. G\u00e4rtner", "M. Jaggi", "C. Maria"], "venue": "paths. preprint arXiv:0903.4817v2,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2010\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2010}, {"title": "Approximating parameterized convex optimization problems", "author": ["J. Giesen", "M. Jaggi", "S. Laue"], "venue": "In Algorithms - ESA, Lectures Notes Comp. Sci", "citeRegEx": "Giesen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Giesen et al\\.", "year": 2010}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "Sparse Convex Optimization Methods for Machine Learning", "author": ["M. Jaggi"], "venue": "PhD thesis, ETH Zu\u0308rich,", "citeRegEx": "Jaggi,? \\Q2011\\E", "shortCiteRegEx": "Jaggi", "year": 2011}, {"title": "How good is the simplex algorithm", "author": ["V. Klee", "G.J. Minty"], "venue": "In Shisha, O. (ed.), Inequalities,", "citeRegEx": "Klee and Minty,? \\Q1972\\E", "shortCiteRegEx": "Klee and Minty", "year": 1972}, {"title": "A new approach to variable selection in least squares problems", "author": ["M. Osborne", "B. Presnell", "B. Turlach"], "venue": "IMA J. Numer. Anal.,", "citeRegEx": "Osborne et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2000}, {"title": "Ein verfahren zur l\u00f6sung parameterabh\u00e4ngiger, nichtlinearer maximum-probleme", "author": ["K. Ritter"], "venue": "Math. Method Oper. Res.,", "citeRegEx": "Ritter,? \\Q1962\\E", "shortCiteRegEx": "Ritter", "year": 1962}, {"title": "Piecewise linear regularized solution paths", "author": ["S. Rosset", "J. Zhu"], "venue": "Ann. Stat.,", "citeRegEx": "Rosset and Zhu,? \\Q2007\\E", "shortCiteRegEx": "Rosset and Zhu", "year": 2007}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Stat. Soc. B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}], "referenceMentions": [{"referenceID": 13, "context": "In particular, the l1-norm has been used for that purpose in the Lasso formulation (Tibshirani, 1996).", "startOffset": 83, "endOffset": 101}, {"referenceID": 11, "context": "(2010), all of these examples are in fact particular instances of parametric quadratic programming formulations, for which path-following algorithms appear early in the optimization literature (Ritter, 1962).", "startOffset": 193, "endOffset": 207}, {"referenceID": 8, "context": "by Osborne et al. (2000) and Efron et al.", "startOffset": 3, "endOffset": 25}, {"referenceID": 8, "context": "by Osborne et al. (2000) and Efron et al. (2004) for the Lasso, and by Hastie et al.", "startOffset": 3, "endOffset": 49}, {"referenceID": 6, "context": "(2004) for the Lasso, and by Hastie et al. (2004) for the support vector machine (SVM).", "startOffset": 29, "endOffset": 50}, {"referenceID": 5, "context": "As observed by G\u00e4rtner et al. (2010), all of these examples are in fact particular instances of parametric quadratic programming formulations, for which path-following algorithms appear early in the optimization literature (Ritter, 1962).", "startOffset": 15, "endOffset": 37}, {"referenceID": 2, "context": "This is notably the case for the simplex algorithm (Dantzig, 1951), which performs empirically well for solving linear programs even though it suffers from exponential worst-case complexity (Klee & Minty, 1972).", "startOffset": 51, "endOffset": 66}, {"referenceID": 2, "context": "This is notably the case for the simplex algorithm (Dantzig, 1951), which performs empirically well for solving linear programs even though it suffers from exponential worst-case complexity (Klee & Minty, 1972). Similarly, by using geometrical tools originally developed to analyze the simplex algorithm, G\u00e4rtner et al. (2010) have shown that the complexity of the SVM regularization path can be exponential.", "startOffset": 52, "endOffset": 327}, {"referenceID": 5, "context": "We remark that our proof is constructive and significantly different than the ones proposed by Klee & Minty (1972) for the simplex algorithm and by G\u00e4rtner et al. (2010) for SVMs.", "startOffset": 148, "endOffset": 170}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010) and Jaggi (2011), who have presented weaker results but in a more general setting for parameterized convex optimization problems.", "startOffset": 37, "endOffset": 58}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010) and Jaggi (2011), who have presented weaker results but in a more general setting for parameterized convex optimization problems.", "startOffset": 37, "endOffset": 75}, {"referenceID": 13, "context": "In this section, we present the Lasso formulation of Tibshirani (1996) and well known facts, which we exploit later in our analysis.", "startOffset": 53, "endOffset": 71}, {"referenceID": 10, "context": "With the assumption that the matrixXJ is always fullrank, we can formally recall a well-known property of the Lasso (see Markowitz, 1952; Osborne et al., 2000; Efron et al., 2004) in the following lemma:", "startOffset": 116, "endOffset": 179}, {"referenceID": 10, "context": "Assuming again that XJ is always full rank, we can now present in Algorithm 1 the homotopy method (Osborne et al., 2000; Efron et al., 2004).", "startOffset": 98, "endOffset": 140}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010), later refined by Jaggi (2011), on approximate regularization paths of parameterized convex functions.", "startOffset": 37, "endOffset": 58}, {"referenceID": 6, "context": "We follow in part the methodology of Giesen et al. (2010), later refined by Jaggi (2011), on approximate regularization paths of parameterized convex functions.", "startOffset": 37, "endOffset": 89}, {"referenceID": 8, "context": "Note that our criterion is not exactly the same as in Jaggi (2011). Whereas Jaggi (2011) consider a formulation where the l1-norm appears in a constraint, Eq.", "startOffset": 54, "endOffset": 67}, {"referenceID": 8, "context": "Note that our criterion is not exactly the same as in Jaggi (2011). Whereas Jaggi (2011) consider a formulation where the l1-norm appears in a constraint, Eq.", "startOffset": 54, "endOffset": 89}, {"referenceID": 6, "context": "In contrast, the analysis of Giesen et al. (2010) and Jaggi (2011) give us: \u2022 an approximate path with O(1/\u03b5) linear segments can be obtained with a weaker approximation guarantee than ours.", "startOffset": 29, "endOffset": 50}, {"referenceID": 6, "context": "In contrast, the analysis of Giesen et al. (2010) and Jaggi (2011) give us: \u2022 an approximate path with O(1/\u03b5) linear segments can be obtained with a weaker approximation guarantee than ours.", "startOffset": 29, "endOffset": 67}, {"referenceID": 3, "context": "It exploits the piecewise linearity of the path, but uses a firstorder method (Beck & Teboulle, 2009; Fu, 1998) when the linear segments of the path are too short.", "startOffset": 78, "endOffset": 111}, {"referenceID": 8, "context": "Note that as noticed in footnote 3, Jaggi (2011) uses a slightly different duality gap than ours.", "startOffset": 36, "endOffset": 49}], "year": 2012, "abstractText": "The regularization path of the Lasso can be shown to be piecewise linear, making it possible to \u201cfollow\u201d and explicitly compute the entire path. We analyze in this paper this popular strategy, and prove that its worst case complexity is exponential in the number of variables. We then oppose this pessimistic result to an (optimistic) approximate analysis: We show that an approximate path with at most O(1/ \u221a \u03b5) linear segments can always be obtained, where every point on the path is guaranteed to be optimal up to a relative \u03b5-duality gap. We complete our theoretical analysis with a practical algorithm to compute these approximate paths.", "creator": "LaTeX with hyperref package"}}}