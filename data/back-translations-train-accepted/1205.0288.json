{"id": "1205.0288", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2012", "title": "A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning", "abstract": "We consider the problem of learning a predictor by combining possibly infinitely many linear predictors whose weights are to be learned, too, an instance of multiple kernel learning. To control overfitting a group p-norm penalty is used to penalize the empirical loss. We consider a reformulation of the problem that lets us implement a randomized version of the proximal point algorithm. The key idea of the new algorithm is to use randomized computation to alleviate the problem of dealing with possibly uncountably many predictors. Finite-time performance bounds are derived that show that under mild conditions the method finds the optimum of the penalized criterion in an efficient manner. Experimental results confirm the effectiveness of the new algorithm.", "histories": [["v1", "Tue, 1 May 2012 23:42:57 GMT  (197kb,S)", "https://arxiv.org/abs/1205.0288v1", null], ["v2", "Mon, 7 Jan 2013 17:42:46 GMT  (120kb,D)", "http://arxiv.org/abs/1205.0288v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["arash afkanpour", "andr\u00e1s gy\u00f6rgy", "csaba szepesv\u00e1ri", "michael bowling"], "accepted": true, "id": "1205.0288"}, "pdf": {"name": "1205.0288.pdf", "metadata": {"source": "CRF", "title": "A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning", "authors": ["Arash Afkanpour", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri", "Michael Bowling"], "emails": ["afkanpou@ualberta.ca", "gyorgy@ualberta.ca", "szepesva@ualberta.ca", "mbowling@ualberta.ca"], "sections": [{"heading": "1 Introduction", "text": "It is a question of whether and in what form people will be able to play by the rules. (...) It is a question of whether and in what form they play by the rules. (...) It is a question of the way in which they play by the rules. (...) It is a question of the way in which they play by the rules. (...) It is a question of the way in which they play by the rules. (...) It is a question of the way in which they play by the rules. (...) It is a question of the way in which they play by the rules. (...) It is a question of the way in which they play by the rules. \"(...) It is a question of the way in which they play by the rules.\" (...) It is a question of the way in which they play by the rules. \""}, {"heading": "2 Preliminaries", "text": "In this section we give the formal definition of our problem. Let me indicate a finite problem that we (fw) = 1 = 1 risk (1) must be combined, and define the number of predictors that we are able to grasp in the real world (1). (1) We have the question of whether we should grasp in the real world (1) the real world (1). (1) We have the question of whether we can grasp in the real world (1). (1) We have the problem that we are subject to optimization of the world (fw) + Pen (fw). (1) Where we grasp the real world (1) is the real world (1). (1) We have discussed the real world (1). (1) We have discussed the real world (1) the real world (1). (1) We have discussed the real world (1). (1) We have discussed the real world (1) the real world (1)."}, {"heading": "3 The new approach", "text": "If I am small or moderately large, the convexity of J can cause one to use standard solvers to find the minimum of J. However, if I am large, standard solvers can be slow or run out of memory. To get a handle on this situation, we suggest the following approach: By re-exploiting that J (w, \u03b8) is jointly convex, we find the optimal weights by finding the minimizer of J (\u03b8). = Influenza J (w, \u03b8), or alternatively J (\u03b8) = J (w, \u03b8), \u03b8), where w (\u03b8) is proposed. = arg minw J (w, \u03b8) (here we have slightly abusive notation by reusing the symbol J). Note that J (\u03b8) is convex by the common convexity of J (w, \u03b8)."}, {"heading": "3.1 A randomized mirror descent algorithm", "text": "Before we give the algorithm, we need a few definitions."}, {"heading": "3.2 Application to multiple kernel learning", "text": "Along the way, we will also derive some explicit expressions that will help us understand the procedure (to construct the predictor fw), but also during iterations, when we need to calculate the derivatives of J with respect to the derivatives. The following approach adds up to how this kind of result can be achieved (see, for example, Shawe-Taylor and Cristini, 2004; Scho-Lkopf and Smola, 2002), so we include them only for the purpose of completeness."}, {"heading": "4 Example: Learning polynomial kernels", "text": "In this section, we will show how our method can be applied in the context of multiple kernel learning. We will give an example if the kernels in me are tensor products of a number of basic kernels (this is what we will call polynomial kernels).The significance of this example arises from the observation by Go-nen and Alpayd\u0131n (2011) that the non-linear kernel learning methods of Cortes et al. (2009), which can be considered a restricted form of learning polynomial kernels, are by far the best MKL methods in practice and can significantly exceed the state of the art SVM with a single kernel or with the uniform combination of kernels."}, {"heading": "4.1 Correctness of the sampling procedure", "text": "In this section, we prove the correctness of the algorithm 3. As already said, we assume that the results can be taken into account as long as the range of weights (0: 1: d) is kept under control and that, by using the calculation, we will point this out. Now, let us look at how to use qk \u2212 1, \u00b7 the implementation is based on the fact that (1: 1: d) d = 1: d: d: d: d, d: d: d, d: d: d: d: d, d: d: d: d, d: d: d: d, d: d: d: d: d, d: d: d: d: d: d: d: d, d: d: d: d: d, d: d: d: d: d: c: c c: d: d: d: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: k: c: c = 1: k: c, k: c: c: c: c: c: c: d: d: d: d: d: d: d: d: d: d: d: d: c: c: c c: c: c: c: k: k: k: k: k: k: k: k: k: d: d: d: k: d: d: d: d: d: d: d: d: d: d: d: d: k: k: k: k: k: k: k: k: d: k: d: d: d: d: k: k: k: k: k: d: k: k: k: d: k: d: d: k: d: k: d: k: k: d: k: d: k: d: d: k: d: k: d: d: k: d:: k: k: k: d: k: k: k: d: k: k: d: d: k: k: d:: k: k: d:: k: k: d: d: k: k: k: d: k: d: d::"}, {"heading": "5 Experiments", "text": "In this section, we apply our method to the problem of multiple kernel learning in regression with quadratic loss: L (w) = 12 \u2211 n t = 1 (fw (xt) \u2212 yt) 2, where (xt, yt) and (Rr \u00b7 R) are the input output pairs in the data. In these experiments, our goal is to learn polynomial cores (cf. Section 4). We compare our method with several kernel learning algorithms from literature on synthetic and real data. In all experiments, we report angular errors compared to test sets. A constant property is added to act as an offset, and the inputs and outputs are normalized so that they have no mean and unit variance. Each experiment is conducted with 10 runs in which we randomly select training, validation and test sets. The results are averaged via these runs."}, {"heading": "5.1 Convergence speed", "text": "In this experiment, we examine the speed of convergence of our method and compare it with one of the fastest standard multiple kernel learning algorithms, that is, the p-standard multiple kernel learning algorithm by Kloft et al. (2011) with p = 2,4 and the uniform coordinate descent algorithm, which randomly updates one coordinate per iteration (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richta \u0301 rik and Taka \"c,\" 2011). We aim to learn polynomial cores from up to degree 3 with all algorithms. Our method uses algorithms 3 for scanning with D = 3. The set of provided base cores is the linear cores, which are composed of input variables, that is, that we (i) x algorithms (i) x algorithms (i) x algorithms (algorithm) where x-ith is (i)."}, {"heading": "5.2 Synthetic data", "text": "In this experiment we investigate the effect of the non-linear learning method of Cortes et al. (2009) (2009) However, the results of this method are randomly selected from [\u2212 1, 1] r. The results of each instance are the uniform combination of 10 monomial terms of degree 3 or less. These terms are uniformly randomly selected from all possible terms. However, the results are free. (5, 10, 20,., 100), with 500 training points and 1000 test points. The regulation parameters of the first regression algorithms were uniformly chosen at random. (102) Using a separate validation set with 1000 data points, we compare our method (poke) against the algorithms of Kloft et al. (2011) (Kloft et al), the non-linear learning method of Cortes et al. (2009) (Cortes)."}, {"heading": "5.3 Real data", "text": "In this experiment we try to compare several MKL methods in real data sets. We compare our new algorithm (Stoch), the algorithm of Bach (2008) (Bach) and the algorithm of Cortes et al. (2009) (Cortes). However, for each algorithm we consider polynomial cores of grade 2 and 3. We also take into account uniform combinations of product cores of grade D, i.e. we use an MKL algorithm to which we only feed linear cores (D = 1) D, for D {1, 2, 3} (uniform). To find out whether the interaction of input variables leads to improved performance, we use an MKL algorithm to which we only feed linear cores (D = 1). We use the MKL algorithm of Kloft et al. (2011) with p {1, 2} (Kloft).We compare these methods on six UCL data sets of the I data set, the Deltoryve machine and Deltoryve data sets."}, {"heading": "6 Conclusion", "text": "We have introduced a new method of learning a predictor by combining exponentially many linear predictors using a randomized mirror descent algorithm. We have derived apocalyptic performance limits that show that the method efficiently optimizes our proposed criterion. Our proposed method is a variant of a randomized stochastic descent algorithm where the main trick is the careful construction of an unbiased randomized estimate of the gradient vector that controls the variance of the method and can be efficiently calculated when the base nuclei have a specific combinatorial structure. In this case, our method's efficiency has been demonstrated for the practically important problem of learning polynomial nuclei on a variety of synthetic and real datasets compared to a representative set of algorithms from literature."}, {"heading": "Acknowledgements", "text": "This work was supported by Alberta Innovates Technology Futures and NSERC."}, {"heading": "A Proofs", "text": "In this section, we present the evidence of the theory. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "B Calculating the derivative of J(\u03b8)", "text": "In this section we show that under mild conditions the derivative of J exists and we also give explicit forms. These derivatives are quite standard and a similar argument can be found in the work of (e.g.) Rakotomamonjy et al. (2008), provided J = J (w, \u03b8) is such that \"t is the hinge.\" As it is ongoing thanks to the implicit function theory (e.g. Brown and Page, 1970, theorem 7.5.6), the gradient of J (2001) can be calculated provided that J = J (w, \u03b8) is continuous in relation to (w, \u03b8) and wJ (w, \u03b8)."}], "references": [{"title": "Learning convex combinations", "author": ["A. Argyriou", "C. Micchelli", "M. Pontil"], "venue": null, "citeRegEx": "Argyriou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2005}, {"title": "Mirror descent and nonlinear projected subgradient meth", "author": ["A. Beck", "M. Teboulle"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Beck and Teboulle,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2003}, {"title": "Learning non-linear combinations", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": null, "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Infinite kernel learning", "author": ["A. Asuncion"], "venue": "Neural Information Processing Systems,", "citeRegEx": "A. and Asuncion,? \\Q2010\\E", "shortCiteRegEx": "A. and Asuncion", "year": 2010}, {"title": "Multiple kernel learning algorithms", "author": ["M. Planck Institute For Biological Cybernetics. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "Journal of Machine", "citeRegEx": "G\u00f6nen and Alpayd\u0131n,? 2011", "shortCiteRegEx": "G\u00f6nen and Alpayd\u0131n", "year": 2011}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Learning Research,", "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning Journal, 69(2-3):169\u2013192.", "citeRegEx": "Hazan et al\\.,? 2007", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "Proceedings of the 24th Annual Conference on Learning Theory, volume 19 of JMLR Workshop and Conference Proceedings, pages 421\u2013436.", "citeRegEx": "Hazan and Kale,? 2011", "shortCiteRegEx": "Hazan and Kale", "year": 2011}, {"title": "lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "Journal of Machine Learning Research, 12:953\u2013997.", "citeRegEx": "Kloft et al\\.,? 2011", "shortCiteRegEx": "Kloft et al\\.", "year": 2011}, {"title": "Perturbation des m\u00e9thodes d\u2019optimisation", "author": ["B. Martinet"], "venue": "Applications. RAIRO Analyse Num\u00e9rique, 12:153\u2013171.", "citeRegEx": "Martinet,? 1978", "shortCiteRegEx": "Martinet", "year": 1978}, {"title": "Learning the kernel function via regularization", "author": ["C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 6:1099\u20131125.", "citeRegEx": "Micchelli and Pontil,? 2005", "shortCiteRegEx": "Micchelli and Pontil", "year": 2005}, {"title": "On the algorithmics and applications of a mixed-norm based kernel learning formulation", "author": ["J. Nath", "G. Dinesh", "S. Raman", "C. Bhattacharyya", "A. Ben-Tal", "K. Ramakrishnan"], "venue": "Advances in Neural Information Processing Systems, volume 22, pages 844\u2013852.", "citeRegEx": "Nath et al\\.,? 2009", "shortCiteRegEx": "Nath et al\\.", "year": 2009}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optimization, 4:1574\u20131609.", "citeRegEx": "Nemirovski et al\\.,? 2009a", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization, 19(4):1574\u2013 1609.", "citeRegEx": "Nemirovski et al\\.,? 2009b", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovski", "D. Yudin"], "venue": "Wiley.", "citeRegEx": "Nemirovski and Yudin,? 1998", "shortCiteRegEx": "Nemirovski and Yudin", "year": 1998}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "CORE Discussion paper, (2010/2).", "citeRegEx": "Nesterov,? 2010", "shortCiteRegEx": "Nesterov", "year": 2010}, {"title": "Subgradient methods for huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "CORE Discussion paper, (2012/2).", "citeRegEx": "Nesterov,? 2012", "shortCiteRegEx": "Nesterov", "year": 2012}, {"title": "Ultra-fast optimization algorithm for sparse multi kernel learning", "author": ["F. Orabona", "J. Luo"], "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 249\u2013256.", "citeRegEx": "Orabona and Luo,? 2011", "shortCiteRegEx": "Orabona and Luo", "year": 2011}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "Journal of Machine Learning Research, 9:2491\u20132521.", "citeRegEx": "Rakotomamonjy et al\\.,? 2008", "shortCiteRegEx": "Rakotomamonjy et al\\.", "year": 2008}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u0109"], "venue": "(revised July 4, 2011) submitted to Mathematical Programming.", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u0109,? 2011", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u0109", "year": 2011}, {"title": "Monotone operators and the proximal point algorithm", "author": ["R. Rockafellar"], "venue": "SIAM Journal on Control and Optimization, 14(1):877\u2013898. 21", "citeRegEx": "Rockafellar,? 1976", "shortCiteRegEx": "Rockafellar", "year": 1976}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press, Cambridge, MA, USA.", "citeRegEx": "Sch\u00f6lkopf and Smola,? 2002", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Stochastic methods for l1-regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research, 12:1865\u20131892.", "citeRegEx": "Shalev.Shwartz and Tewari,? 2011", "shortCiteRegEx": "Shalev.Shwartz and Tewari", "year": 2011}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge Univ Press.", "citeRegEx": "Shawe.Taylor and Cristianini,? 2004", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "The Journal of Machine Learning Research, 7:1531\u20131565.", "citeRegEx": "Sonnenburg et al\\.,? 2006", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "An extended level method for efficient multiple kernel learning", "author": ["Z. Xu", "R. Jin", "I. King", "M. Lyu"], "venue": "Advances in Neural Information Processing Systems, volume 21, pages 1825\u20131832.", "citeRegEx": "Xu et al\\.,? 2008", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Z. Xu", "R. Jin", "H. Yang", "I. King", "M.R. Lyu"], "venue": "Proceedings of the 27th International Conference on Machine Learning, pages 1175\u20131182. 22", "citeRegEx": "Xu et al\\.,? 2010", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Just like some previous works (e.g. Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach that views the MKL problem as a nested, large scale convex optimization problem, where the first layer optimizes the weights of the kernels to be combined.", "startOffset": 30, "endOffset": 99}, {"referenceID": 11, "context": "Just like some previous works (e.g. Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach that views the MKL problem as a nested, large scale convex optimization problem, where the first layer optimizes the weights of the kernels to be combined.", "startOffset": 30, "endOffset": 99}, {"referenceID": 22, "context": "However, as opposed to these works whose underlying iterative methods have a complexity of \u03a9(d) for just any one iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iteration complexity to O(1).", "startOffset": 134, "endOffset": 216}, {"referenceID": 19, "context": "However, as opposed to these works whose underlying iterative methods have a complexity of \u03a9(d) for just any one iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iteration complexity to O(1).", "startOffset": 134, "endOffset": 216}, {"referenceID": 13, "context": "However, as opposed to these works whose underlying iterative methods have a complexity of \u03a9(d) for just any one iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iteration complexity to O(1). The role of randomization in our method is to use it to build an unbiased estimate of the gradient at the most recent iteration. The issue then is how the variance (and so the number of iterations required) scales with d. As opposed to the above mentioned works, in this paper we propose to make the distribution over the updated coordinate dependent on the history. We will argue that sampling from a distribution that is proportional to the magnitude of the gradient vector is desirable to keep the variance (actually, second moment) low and in fact we will show that there are interesting cases of MKL (in particular, the case of combining kernels coming from a polynomial family of kernels) when efficient sampling (i.e., sampling at a cost of O(log d)) is feasible from this distribution. Then, the variance is controlled by the a priori weights put on the kernels, making it potentially independent of d. Under these favorable conditions (and in particular, for the polynomial kernel set with some specific prior weights), the complexity of the method as a function of d becomes logarithmic, which makes our MKL algorithm feasible even for large scale problems. This is to be contrasted to the approach of Nesterov (2010, 2012) where a fixed distribution is used and where the a priori bounds on the method\u2019s convergence rate, and, hence, its computational cost to achieve a prescribed precision, will depend linearly on d (note that we are comparing upper bounds here, so the actual complexity could be smaller). Our algorithm is based on the mirror descent (or mirror descent) algorithm (similar to the work of Richt\u00e1rik and Tak\u00e1\u0109 (2011) who uses uniform distributions).", "startOffset": 135, "endOffset": 2002}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d.", "startOffset": 140, "endOffset": 196}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d.", "startOffset": 140, "endOffset": 332}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d. The algorithm of Bach (2008), though practically very efficient, suffers from the same deficiency.", "startOffset": 140, "endOffset": 387}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d. The algorithm of Bach (2008), though practically very efficient, suffers from the same deficiency. A very interesting proposal by Cortes et al. (2009) considers learning to combine a large number of kernels and comes with guarantees, though their algorithm restricts the family of kernels in a specific way.", "startOffset": 140, "endOffset": 509}, {"referenceID": 5, "context": ", Hastie et al. (2009). In supervised learning problems `t(y) = `(yt, y) for some loss function ` : R \u00d7 R \u2192 R, such as the squared-loss, `(yt, y) = 1 2(y \u2212 yt) 2, or the hinge-loss, `t(yt, y) = max(1 \u2212 yyt, 0), where in the former case yt \u2208 R, while in the latter case yt \u2208 {\u22121,+1}.", "startOffset": 2, "endOffset": 23}, {"referenceID": 5, "context": ", Hastie et al. (2009). In supervised learning problems `t(y) = `(yt, y) for some loss function ` : R \u00d7 R \u2192 R, such as the squared-loss, `(yt, y) = 1 2(y \u2212 yt) 2, or the hinge-loss, `t(yt, y) = max(1 \u2212 yyt, 0), where in the former case yt \u2208 R, while in the latter case yt \u2208 {\u22121,+1}. We note in passing that for the sake of simplicity, we shall sometimes abuse notation and write Ln(w) for Ln(fw) and even drop the index n when the sample-size is unimportant. As mentioned above, in this paper we consider the special case in (1) when the penalty is a so-called group p-norm penalty with 1 \u2264 p \u2264 2, a case considered earlier, e.g., by Kloft et al. (2011). Thus our goal is to solve", "startOffset": 2, "endOffset": 654}, {"referenceID": 20, "context": "More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled.", "startOffset": 110, "endOffset": 173}, {"referenceID": 9, "context": "More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled.", "startOffset": 110, "endOffset": 173}, {"referenceID": 14, "context": "More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled.", "startOffset": 110, "endOffset": 173}, {"referenceID": 12, "context": "In particular, compared to (Nemirovski et al., 2009a; Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent.", "startOffset": 27, "endOffset": 135}, {"referenceID": 22, "context": "In particular, compared to (Nemirovski et al., 2009a; Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent.", "startOffset": 27, "endOffset": 135}, {"referenceID": 19, "context": "In particular, compared to (Nemirovski et al., 2009a; Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent.", "startOffset": 27, "endOffset": 135}, {"referenceID": 6, "context": "The convergence rate in the above theorem can be improved if stronger assumptions are made on J , for example if J is assumed to be strongly convex, see, for example, (Hazan et al., 2007; Hazan and Kale, 2011).", "startOffset": 167, "endOffset": 209}, {"referenceID": 7, "context": "The convergence rate in the above theorem can be improved if stronger assumptions are made on J , for example if J is assumed to be strongly convex, see, for example, (Hazan et al., 2007; Hazan and Kale, 2011).", "startOffset": 167, "endOffset": 209}, {"referenceID": 21, "context": "Note that this type of result is standard (see, e.g., Shawe-Taylor and Cristianini, 2004; Sch\u00f6lkopf and Smola, 2002), thus we include it only for the sake of completeness (the proof is included in Section A in the appendix).", "startOffset": 42, "endOffset": 116}, {"referenceID": 3, "context": "The importance of this example follows from the observation of G\u00f6nen and Alpayd\u0131n (2011) that the non-linear kernel learning methods of Cortes et al.", "startOffset": 63, "endOffset": 89}, {"referenceID": 2, "context": "The importance of this example follows from the observation of G\u00f6nen and Alpayd\u0131n (2011) that the non-linear kernel learning methods of Cortes et al. (2009), which can be viewed as a restricted form of learning polynomial kernels, are far the best MKL methods in practice and can significantly outperform state-of-the-art SVM with a single kernel or with the uniform combination of kernels.", "startOffset": 136, "endOffset": 157}, {"referenceID": 22, "context": "(2011) with p = 2,4 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011).", "startOffset": 127, "endOffset": 209}, {"referenceID": 19, "context": "(2011) with p = 2,4 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011).", "startOffset": 127, "endOffset": 209}, {"referenceID": 8, "context": "1 Convergence speed In this experiment we examine the speed of convergence of our method and compare it against one of the fastest standard multiple kernel learning algorithms, that is, the p-norm multiple kernel learning algorithm of Kloft et al. (2011) with p = 2,4 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011).", "startOffset": 235, "endOffset": 255}, {"referenceID": 8, "context": "In this figure Stoch represents our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents the uniform coordinate descent algorithm.", "startOffset": 82, "endOffset": 102}, {"referenceID": 8, "context": "In this figure Stoch represents our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents the uniform coordinate descent algorithm. The results show that our method consistently outperforms the other algorithms in convergence speed. Note that our stochastic method updates one kernel coefficient per iteration, while Kloft updates ( r+D D ) kernel coefficients per iteration. The difference between the two methods is analogous to the difference between stochastic gradient vs. full gradient algorithms. While UCD also updates one kernel Note that p = 2 in Kloft et al. (2011) notation corresponds to p = 4/3 or \u03bd = 2 in our notation, which gives the same objective function that we minimize with Algorithm 2.", "startOffset": 82, "endOffset": 609}, {"referenceID": 7, "context": "We compare our method (Stoch) against the algorithm of Kloft et al. (2011) (Kloft), the nonlinear kernel learning method of Cortes et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).", "startOffset": 56, "endOffset": 77}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).", "startOffset": 56, "endOffset": 149}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel.", "startOffset": 56, "endOffset": 290}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.", "startOffset": 56, "endOffset": 665}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.", "startOffset": 56, "endOffset": 711}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al.", "startOffset": 56, "endOffset": 1046}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al.", "startOffset": 56, "endOffset": 1075}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al.", "startOffset": 56, "endOffset": 1093}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al.", "startOffset": 56, "endOffset": 1117}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al.", "startOffset": 56, "endOffset": 1138}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al. (2011) has the best performance overall.", "startOffset": 56, "endOffset": 1262}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al. (2011) has the best performance overall. Hence, we decided to compare against only this algorithm. Also note that the memory and computational cost of all these methods still scale linearly with the number of kernels, making them unsuitable for the case we are most interested in. Furthermore, to keep the focus of the paper we compare our algorithm to methods with sound theoretical guarantees. As such, it remains for future work to compare with other methods, such as the infinite kernel learning of Gehler and Nowozin (2008), which lack such guarantees but exhibit promising performance in practice.", "startOffset": 56, "endOffset": 1784}, {"referenceID": 2, "context": "Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases.", "startOffset": 43, "endOffset": 64}, {"referenceID": 2, "context": "Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases. This is due to the restricted family of kernels that this method considers. The method of Bach (2008), which is well-suited to learn sparse combination of product kernels, performs better than Cortes et al.", "startOffset": 43, "endOffset": 200}, {"referenceID": 2, "context": "Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases. This is due to the restricted family of kernels that this method considers. The method of Bach (2008), which is well-suited to learn sparse combination of product kernels, performs better than Cortes et al. (2009) for higher input dimensions.", "startOffset": 43, "endOffset": 312}, {"referenceID": 2, "context": "We compare our new algorithm (Stoch), the algorithm of Bach (2008) (Bach), and the algorithm of Cortes et al. (2009) (Cortes).", "startOffset": 96, "endOffset": 117}, {"referenceID": 2, "context": "We compare our new algorithm (Stoch), the algorithm of Bach (2008) (Bach), and the algorithm of Cortes et al. (2009) (Cortes). For each algorithm we consider learning polynomial kernels of degree 2 and 3. We also include uniform combination of product kernels of degree D, i.e. \u03baD = ( \u2211r i=1 \u03bai) , for D \u2208 {1, 2, 3} (Uniform). To find out if considering higherorder interaction of input variables results in improved performance we also included a MKL algorithm to which we only feed linear kernels (D = 1). We use the MKL algorithm of Kloft et al. (2011) with p \u2208 {1, 2} (Kloft).", "startOffset": 96, "endOffset": 556}, {"referenceID": 1, "context": "1 is based on the standard proof of the convergence rate of the proximal point algorithm, see, for example, (Beck and Teboulle, 2003), or the proof of Proposition 2.", "startOffset": 108, "endOffset": 133}, {"referenceID": 1, "context": "1 is based on the standard proof of the convergence rate of the proximal point algorithm, see, for example, (Beck and Teboulle, 2003), or the proof of Proposition 2.2 of Nemirovski et al. (2009b), which carry over the same argument to solve very similar but less general problems.", "startOffset": 109, "endOffset": 196}, {"referenceID": 18, "context": ") Rakotomamonjy et al. (2008) specialized to the case when `t is the hinge loss.", "startOffset": 2, "endOffset": 30}], "year": 2013, "abstractText": "We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels d to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in d are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group p-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show the surprising result that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows the implementation of sampling from this distribution to run in O(log(d)) time, making the total computational cost of the method to achieve an optimal solution to be O(log(d)/ ), thereby allowing our method to operate for very large values of d. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives.", "creator": "LaTeX with hyperref package"}}}