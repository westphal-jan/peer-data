{"id": "1205.1782", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2012", "title": "Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds", "abstract": "Approximate dynamic programming is a popular method for solving large Markov decision processes. This paper describes a new class of approximate dynamic programming (ADP) methods- distributionally robust ADP-that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss. This approach turns ADP into an optimization problem, for which we derive new mathematical program formulations and analyze its properties. DRADP improves on the theoretical guarantees of existing ADP methods-it guarantees convergence and L1 norm based error bounds. The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems.", "histories": [["v1", "Tue, 8 May 2012 19:22:43 GMT  (64kb,D)", "https://arxiv.org/abs/1205.1782v1", null], ["v2", "Mon, 21 May 2012 16:30:22 GMT  (62kb,D)", "http://arxiv.org/abs/1205.1782v2", "In Proceedings of International Conference on Machine Learning, 2012"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["marek petrik"], "accepted": true, "id": "1205.1782"}, "pdf": {"name": "1205.1782.pdf", "metadata": {"source": "CRF", "title": "Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds", "authors": ["Marek Petrik"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Many ADP algorithms have been developed and studied, often with impressive empirical performance, but since many ADP methods need to be carefully calibrated to work well and offer insufficient theoretical guarantees, it is important to develop new methods that have both good theoretical guarantees and empirical performance. (ALP) - an ADP method was developed with the aim of achieving convergence and good theoretical guarantees (de Farias & van Roy, 2003). (D) Bilinear programming (ABP) improves the theoretical properties of ALP at the cost of additional computational complexity (Petrik & Zilberstein, 2009, 2011). Both ALP and ABP provide guarantees based on conservative errors regarding the L standard and often under-conditions in practice (Petrik & Zilberstein, 2011)."}, {"heading": "2 Framework and Notation", "text": "In this section we define the basic concepts needed to solve the Markov decision processes: value functions, and allocation frequencies. We use the following general notation throughout the work. The symbols 0 and 1 denote vectors of all zeros and ones of appropriate dimensions respectively; the symbol I denotes an identity matrix of an appropriate dimension. The operator [\u00b7] + denotes an elementary non-negative part of a vector. We will often use linear algebra and expectation notations interchangeably; for example: Eu [X] = uTx, where x is a vector of the values of the random variable X. We alsouse RX to denote the set of all functions of a finite set of R. Note that RX is trivially a vector space.A Markov Decision Process is a vector (S, A, P, r, \u03b1)."}, {"heading": "3 Distributionally Robust Approximate Dynamic Programming", "text": "In this section, we formalize DRADP and describe it in terms of generic optimization problems. Practical DRADP implementations are scanned based on the optimization problems described in this section. However, as is common in the ADP literature, we do not explicitly analyze the sampling method used with DRADP because the sampling error can easily be added to the error limits we deduce. Sampling method is performed and errors are used identically to approximate linear programming and approximate bilinear programming to select a subset of limitations and variables (de Farias & van Roy, 2003; Petrik et al., 2010; Petrik & Zilberstein, 2011).The main goal of ADP is to compile a policy that maximizes return."}, {"heading": "4 Approximation Error Bounds", "text": "This section describes the a priori approximation properties of DRADP solutions; these limits can be evaluated before a solution is calculated. We focus on several types of limits that not only show the performance of the method, but also make it easier to theoretically compare DRADP with existing ADP methods. These limits show that DRADP has stronger theoretical guarantees than most other ADP methods. The first limit reflects some simple limits for approximate political iteration (API) relative to the L standard (Munos, 2007): lim sup \u2192 v? \u2212 vp \u00b2 v \u00b2 v \u00b2 v \u00b2 n (1 \u2212) 2 limsup \u00b2 v \u00b2 n \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (4.1), where the policy and L \u00b2 s \u00b2 s \u00b2 s errors in iteration k.Theorem 4.1."}, {"heading": "5 Computational Models", "text": "In this section we describe how to solve the DRADP optimization problem. As DRADP generalizes the problem (Petrik & Zilberstein, 2009), it is necessarily complete in theory, but relatively easy to solve in practice. Note: NP completeness is based on the number of samples and characteristics, not the number of states or actions of the MDP. To solve DRADPs in practice, we derive bilinear and mixed linear program formulations, for which many powerful solvers have been developed. These formulations lead to solutions that can be solved at any time - even approximate solutions in valid policies - and thus can easily deal with time-complex formulations of the DRADP."}, {"heading": "6 Experimental Results", "text": "This year it has come to the point where it will be able to mention the aforementioned rf\u00fc the aforementioned lcihsrc\u00fceSe."}, {"heading": "7 Conclusion", "text": "In this paper, DRADP - a new ADP method - is proposed and analyzed. DRADP is based on a mathematical optimization formulation - like ALP - but offers significantly stronger theoretical guarantees and better empirical performance. The DRADP framework also makes it easy to improve the solution quality by incorporating additional assumptions about the frequency of government occupation, such as the low coefficient of concentration. Given the encouraging theoretical and empirical properties of DRADP, we hope it will lead to better methods for solving large MDPs and help deepen the understanding of ADPs. Thanks to Dan Iancu and Dharmashankar Subramanian for the discussions that inspired this paper. I also thank the anonymous critics of ICML 2012 and EWRL 2012 for their detailed comments."}, {"heading": "A Basic Properties of Value Functions", "text": "Lemma A.1. For each v-V applies: L (v + k1) = Lv + \u03b3k1. Moreover, the sets of greedy strategies are identical with respect to v and v + k1. Lemma A.2. The operators P and (I \u2212 \u03b3P) \u2212 1 are monotonous for each stochastic matrix P: x \u2265 y \u21d2 P x \u2265 P yx \u2265 y (I \u2212 \u03b3P) \u2212 1 x \u2265 (I \u2212 \u03b3P) \u2212 1 year for all x and y. Lemma A.3. Let us suppose that v-V v \u2265 satisfies Lv. Then v \u2265 v?.Lemma A.4. Each v-V satisfies: v \u2212 Lv \u2264 v \u2212 L\u03c0v."}], "references": [{"title": "Robust Optimization", "author": ["Ben-Tal", "Aharon", "Ghaoui", "Laurent El", "Nemirovski", "Arkadi"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["de Farias", "Daniela P", "van Roy", "Ben"], "venue": "Operations Research,", "citeRegEx": "Farias et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Farias et al\\.", "year": 2003}, {"title": "Distributionally robust optimization under moment uncertainty with application to data driven problems", "author": ["Delage", "Eric", "Ye", "Yinyu"], "venue": "Operations Research,", "citeRegEx": "Delage et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Delage et al\\.", "year": 2010}, {"title": "Symmetric approximate linear programming for factored MDPs with application to constrained problems", "author": ["Dolgov", "Dmitri", "Durfee", "Edmund"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Dolgov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dolgov et al\\.", "year": 2006}, {"title": "Global optimization: Deterministic approaches", "author": ["Horst", "Reiner", "Tuy", "Hoang"], "venue": null, "citeRegEx": "Horst et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Horst et al\\.", "year": 1996}, {"title": "Least-squares policy iteration", "author": ["Lagoudakis", "Michail G", "Parr", "Ronald"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis et al\\.", "year": 2003}, {"title": "A simplicial branch-and-bound algorithm for solving quadratically constrained quadratic programs", "author": ["Linderoth", "Jeff"], "venue": "Mathematical Programming, Series B,", "citeRegEx": "Linderoth and Jeff.,? \\Q2005\\E", "shortCiteRegEx": "Linderoth and Jeff.", "year": 2005}, {"title": "Error bounds for approximate policy iteration", "author": ["Munos", "Remi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Munos and Remi.,? \\Q2003\\E", "shortCiteRegEx": "Munos and Remi.", "year": 2003}, {"title": "Performance bounds in Lp norm for approximate value iteration", "author": ["Munos", "Remi"], "venue": "SIAM Journal of Control and Optimization,", "citeRegEx": "Munos and Remi.,? \\Q2007\\E", "shortCiteRegEx": "Munos and Remi.", "year": 2007}, {"title": "Approximate dynamic programming by minimizing distributionally robust bounds", "author": ["Petrik", "Marek"], "venue": null, "citeRegEx": "Petrik and Marek.,? \\Q2012\\E", "shortCiteRegEx": "Petrik and Marek.", "year": 2012}, {"title": "Robust value function approximation using bilinear programming", "author": ["Petrik", "Marek", "Zilberstein", "Shlomo"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Petrik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2009}, {"title": "Robust approximate bilinear programming for value function approximation", "author": ["Petrik", "Marek", "Zilberstein", "Shlomo"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Petrik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2011}, {"title": "Feature selection using regularization in approximate linear programs for Markov decision processes", "author": ["Petrik", "Marek", "Taylor", "Gavin", "Parr", "Ron", "Zilberstein", "Shlomo"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Petrik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2010}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q2005\\E", "shortCiteRegEx": "Puterman and L.", "year": 2005}, {"title": "Generalized polynomial approximations in Markovian decision processes", "author": ["Schweitzer", "Paul J", "Seidmann", "Abraham"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Schweitzer et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer et al\\.", "year": 1985}, {"title": "New Representations and Approximations for Sequential Decision Making", "author": ["Wang", "Tao"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "Wang and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Wang and Tao.", "year": 2007}, {"title": "Stable dynamic programming", "author": ["Wang", "Tao", "Lizotte", "Daniel", "Bowling", "Michael", "Schuurmans", "Dale"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 16, "context": "Occupancy frequencies have been used, for example, to solve factored MDPs (Dolgov & Durfee, 2006) and in dual dynamic programming (Wang, 2007; Wang et al., 2008) (The term \u201cdual dynamic programming\u201d also refers to unrelated linear stochastic programming methods).", "startOffset": 130, "endOffset": 161}, {"referenceID": 0, "context": "Robust optimization is a recently revived approach for modeling uncertainty in optimization problems (Ben-Tal et al., 2009).", "startOffset": 101, "endOffset": 123}, {"referenceID": 12, "context": "The sampling is performed and errors bounded identically to approximate linear programming and approximate bilinear programming\u2014state and action samples are used to select a subset of constraints and variables (de Farias & van Roy, 2003; Petrik et al., 2010; Petrik & Zilberstein, 2011).", "startOffset": 210, "endOffset": 286}, {"referenceID": 12, "context": "Note the poor performance of ABP and ALP with the 10 standard features; better results have been obtained with large and different feature spaces (Petrik et al., 2010) but even these do not match DRADP.", "startOffset": 146, "endOffset": 167}], "year": 2012, "abstractText": "Approximate dynamic programming is a popular method for solving large Markov decision processes. This paper describes a new class of approximate dynamic programming (ADP) methods\u2014 distributionally robust ADP\u2014that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss. This approach turns ADP into an optimization problem, for which we derive new mathematical program formulations and analyze its properties. DRADP improves on the theoretical guarantees of existing ADP methods\u2014it guarantees convergence and L1 norm-based error bounds. The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems.", "creator": "LaTeX with hyperref package"}}}