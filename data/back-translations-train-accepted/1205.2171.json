{"id": "1205.2171", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2012", "title": "A Generalized Kernel Approach to Structured Output Learning", "abstract": "We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) problem using operator-valued kernels. We show that some of the existing formulations of this problem are special cases of our framework. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on two structured output problems, and compare it to the state-of-the-art kernel-based structured output regression methods.", "histories": [["v1", "Thu, 10 May 2012 07:01:00 GMT  (398kb)", "https://arxiv.org/abs/1205.2171v1", null], ["v2", "Wed, 15 Jul 2015 18:42:11 GMT  (167kb,D)", "http://arxiv.org/abs/1205.2171v2", "in International Conference on Machine Learning (ICML), Jun 2013, Atlanta, United States. 2013"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["hachem kadri", "mohammad ghavamzadeh", "philippe preux"], "accepted": true, "id": "1205.2171"}, "pdf": {"name": "1205.2171.pdf", "metadata": {"source": "META", "title": "A Generalized Kernel Approach to Structured Output Learning", "authors": ["Hachem Kadri", "Mohammad Ghavamzadeh", "Philippe Preux"], "emails": ["hachem.kadri@lif.univ-mrs.fr", "mohammad.ghavamzadeh@inria.fr", "philippe.preux@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2. Preliminaries", "text": "In this section, we will first present the notations used throughout the essay, describe the settings of the problem examined in the paper, and provide a comprehensive description of our approach to this problem. Before we report on our KDE formulation, we will give a brief overview of the cores with operator values and the associated RKHSs. To make reading easier, we will list the notations used in the work in Table 1."}, {"heading": "2.1. Problem Setting and Notations", "text": "Given (xi, yi) n i = 1 \u0445 X \u00b7 Y, where X and Y are the input and structured output ranges, we consider the problem of learning a mapping f from X to Y. The idea of KDE is to embed the output data by mapping \u03a6l between the structured output space Y and a Euclidean attribute space FY, which is defined by a scaled mapping of the kernel l. Instead of learning f to predict output y for input x, the KDE methods first learn the mapping g from X to FY and then compute the mapping of g (x) by mapping the kernel l in reverse, i.e. y = f (x) = p \u2212 1l (g (x)). (see Figure 1). All existing KDE methods use a comb regression with a scaled kernel k (x) to learn the mapping g. This approach has the disadvantage of overcoming the dependencies between the Y in the characteristics space."}, {"heading": "2.2. Operator-valued Kernels and Associated RKHSs", "text": "We now offer some definitions of operator-rated kernels and the related RKHSs used in the paper (see Micchelli & Pontil, 2005; Caponnetto et al., 2008; A \u0301 lvarez et al., 2012). These kernel spaces have recently received more attention because they are suitable for problems where the results are vectors (as in multitask learning (Evgeniou et al., 2012)) or functions (as in function regression (Kadri et al., 2010)) rather than scalars. Furthermore, it has recently been shown that these spaces are suitable for learning-related intermediate embedding (Grunewalder et al., 2012). Leave L (FY) the group of restricted operators from FY to FY. Definition 1 (FY), FY. Definition 1 (FY), negative FY properties (FY), FY-j (FY) kernel-rated."}, {"heading": "3. Operator-valued Kernel Formulation of Kernel Dependency Estimation", "text": "In this section, we describe our operator-weighted KDE formulation, in which the feature spaces are associated, in two steps: Step 1 (Kernel ridge) Regression: We use operator-weighted kernel-based regression and learn the function g in the FY -weighted RKHS-FXY from the training data (xi, \u03a6l (yi)) n i = 1 (Kernel ridge) Regression: We use operator-based regression and learn the function g in the FY -weighted RKHS-FY formulations. Similar to other KDE formulations, we consider the following regression problem: arg min g FXY n = 1 (xi) -2 (yi) -3 (yi)."}, {"heading": "4. Covariance-based Operator-valued Kernels", "text": "In this section, we will examine the problem of designing operationally evaluated kernels for structured outputs in the KDE formulation. < l > This is very important to take full advantage of the operationally evaluated KDE formulation. < l (1) The main purpose of using the operationally evaluated Y formulation is to take into account the dependencies between the Y (yi) variables, i.e., the projection of yi in the attribute operator FY, with the aim of capturing the structure of the output data encapsulated in Y (yi). Operator-evaluated kernels have been studied more in the context of multi-task learning, where the output is assumed as in Rd the number of tasks (Evgeniou et al, 2005). Some work also focuses on extending this kernel to the domain of functional data analysis to address the problem of regression with functional responses, where the outputs are considered in L2 space."}, {"heading": "5. Related Work", "text": "In this section, we discuss the related work on kernel-based structured output learning and compare it to our proposed operator-evaluated kernel formulation."}, {"heading": "5.1. KDE", "text": "Existing KDE formulations attempt to solve this problem either by performing a kernel PCA to correlate the results (Weston et al., 2003), or by incorporating some form of prior knowledge into the regression step, using some specific limitations of the regression matrix that performs the mapping between input and output attribute spaces (Cortes et al., 2007). Compared to kernel PCA 1), our KDE formulation does not require a dimension reduction step that can lead to information loss if the range of the output kernel matrix does not decrease rapidly, 2) it does not need to assume that the dimensionality of the low-dimensional subspace (the number of major components) is known and fixed in advance."}, {"heading": "5.2. Joint Kernels Meet Operator-valued Kernels", "text": "Another approach to consider input-output correlations is the use of common cores, which are scaled functions (measure of similarity) of input-output pairs (Weston et al., 2007). In this context, the problem of learning the mapping f from X to Y is reformulated so that a function f from X \u00b7 Y to R is learned using a common kernel (JK) (Tsochantaridis et al., 2005). Our kernel formulation by operator value includes the JK approach. Similar to common cores, operatively evaluated cores (implicitly) induce a measurement of similarity between operator-evaluated cores. This can be seen in the characteristic space formulation of operator-evaluated cores (Caponnetto et al., 2008; Kadri et al al al al al al al al al al., 2011) an operator-evaluated kernel (implicitly): a characteristic map by operator-evaluated kernel with an operator-evaluated kernel value is a function evaluated by operator (Caponnetto et al al, 2008; Kadri et al al al al al al al al al al.): a product by operator-evaluated by an operator-evaluated kernel (&xri)."}, {"heading": "6. Experimental Results", "text": "We evaluate our operator-rated KDE formulation based on three structured prediction problems, namely image reconstruction, optical character recognition, and face-to-face mapping. In the first problem, we compare our method with both the covariance and conditional covariance of cores evaluated by the operator with the KDE algorithms of Weston et al. (2003) and Cortes et al. (2005); in the second problem, we evaluate the two implementations of our KDE method with a limited regression version of KDE (Cortes et al., 2007) and Max-Margin Markov Networks (M3Ns) (Taskar et al., 2004); and in the third problem, we compare them with the approach of the Joint Kernel Map (JKM) by Weston et al. (2007) in addition to the scalar-rated KDE."}, {"heading": "6.1. Image Reconstruction", "text": "Here we look at the image reconstruction problem used in Weston et al. (2003). This problem takes the upper half (the first 8 pixel lines) of a USPS zip code as input and estimates its lower half. We use exactly the same data set and settings as in the experiments of (Weston et al., 2003). We apply our KDE method using both covariance and conditional covariance and compare it with the KDE algorithms of Weston et al. (2003) and Cortes et al. (2005). In all of these methods we use RBF cores for both input and output with the parameters shown in Table 2 (left). This table also contains the ridge parameters used by these algorithms. We have tried to determine a number of values for these parameters, and the results obtained in the table are the best."}, {"heading": "6.2. Optical Character Recognition", "text": "To evaluate the effectiveness of our proposed method for problems with non-numerical results, we use an optical character recognition (OCR) problem, which is used in Taskar et al. (2004) and Cortes et al. (2005). The data set is a subset of handwritten words collected by Rob Kassel at the MIT Spoken Language Systems Group. It contains 6,877 word instances with a total of 52, 152 characters. The image of each character has been normalized into a 16 to 8 binary pixel representation. The OCR task is to predict a word from the sequence of pixel-based images of its handwritten representations. Table 2 (right) reports on the results of our experiments. Performance is correctly recognized as a percentage number of word characters (WRC). We compare our approach with a limited regression version of the Cortes formulation."}, {"heading": "6.3. Face-to-Face Mapping", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "7. Conclusions and Future Work", "text": "In this paper, we presented a general formulation of the Kernel Dependency Assessment (KDE) for structured output learning using cores with operator value and illustrated its use in several experiments. We also proposed a new covariance-based kernel with operator value that takes into account the structure of the output kernel feature space, which encodes the interactions between the outputs but does not refer to the input space. We addressed this problem by introducing a variant of our KDE method based on the conditional covariance operator, which in addition to the correlation between the outputs takes into account the effects of the input variables.In our work, we focused on regression-based structured output predictions. An interesting direction for future research is the exploration of cores with operator value in connection with classification-based structured output learning. Common cores and cores with operator value have strong connections, but further studies are needed to show how to deal with this problem with operator value can be used."}, {"heading": "Acknowledgments", "text": "We would like to thank Arthur Gretton and Alain Rakotomamonjy for their help and discussions, which were financed by the Ministry of Higher Education and Research and the ANR project LAMPADA (ANR-09-EMER-007)."}, {"heading": "8. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1. (Generalized) Kernel Trick", "text": "In this section we will demonstrate the (generalized) kernel trick used in section 3 = > functions, i.e., < T \u03a6l (y1), < l (y2) > FY = [T l (y1, \u00b7) (y2), where T-L (FY) and FY are an RKHS (y1) with kernel l.Case 1: < l is the function card associated with the reproducing kernel l, i.e., l (y) = l (\u00b7, y).Here the proof is simple, we can < T \u03a6lt; l (y1) < T (y1) > FY = < T l (y1), l (\u00b7, y2) > FY = [T l (y1, \u00b7)]] (y2).The second equality follows the reproducing kernel l.Case 2: The second equality is an implicit function of a kernel, T (j) and T (y2)."}, {"heading": "8.2. Proof of Proposition 1", "text": "In this section we provide the proof for Proposition 1. We only show the proof for the covariance-based operators, since the proof for the other case (conditional covariance-based operator-weighted nuclei) is quite similar. Note that the pre-image problem of the form (x) = arg min y (y, y) \u2212 2 [Kx (K + M) \u2212 1L \u00b7] (y), and our goal is to make its gram matrix expression4 in case K (xi, xj) = k (xi, xj)."}, {"heading": "8.3. Computational Complexity of Solving the Pre-image Problem", "text": "As discussed in Section 4, solving the preimage problem of Eq. 8 requires the calculation of the following expression: C (x, y) = l (x, y) \u2212 l (k \u2212 x \u2212 l) (k \u2212 l) \u2212 l (in). (14) Simple calculation of C (x, y) requires storage and inversion of the matrix (k \u2212 l \u2212 l) (k \u2212 l). (14) Simple calculation of C (x, y) and O (n6). (s). (In this section, we offer an efficient method for this calculation that reduces the space and computational complexity to O (nm1m2, m \u00b2 2) and O (m31m 3)."}], "references": [{"title": "Kernels for vector-valued functions: a review", "author": ["M. \u00c1lvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "\u00c1lvarez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "\u00c1lvarez et al\\.", "year": 2012}, {"title": "Kernel independent component analysis", "author": ["F. Bach", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2002}, {"title": "Predicting Structured Data", "author": ["G. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A. Smola", "B. Taskar", "Vishwanathan", "S. (eds"], "venue": null, "citeRegEx": "Bakir et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bakir et al\\.", "year": 2007}, {"title": "Semi-supervised penalized output kernel regression for link prediction", "author": ["C. Brouard", "F. d\u2019Alch\u00e9 Buc", "M. Szafranski"], "venue": "In Proc. ICML, pp", "citeRegEx": "Brouard et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brouard et al\\.", "year": 2011}, {"title": "Optimal rates for the regularized least-squares algorithm", "author": ["A. Caponnetto", "E. De Vito"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Caponnetto and Vito,? \\Q2007\\E", "shortCiteRegEx": "Caponnetto and Vito", "year": 2007}, {"title": "Universal multi-task kernels", "author": ["A. Caponnetto", "C. Micchelli", "M. Pontil", "Y. Ying"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Caponnetto et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Caponnetto et al\\.", "year": 2008}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana,? \\Q1997\\E", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "A general regression technique for learning transductions", "author": ["C. Cortes", "M. Mohri", "J. Weston"], "venue": "In Proc. ICML, pp", "citeRegEx": "Cortes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2005}, {"title": "A General Regression Framework for Learning String-to-String Mappings", "author": ["C. Cortes", "M. Mohri", "J. Weston"], "venue": null, "citeRegEx": "Cortes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2007}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Evgeniou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2005}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["K. Fukumizu", "F. Bach", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Kernel methods for measuring independence", "author": ["A. Gretton", "R. Herbrich", "A. Smola", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "Conditional mean embeddings as regressors", "author": ["S. Grunewalder", "G. Lever", "A. Gretton", "L. Baldassarre", "S. Patterson", "M. Pontil"], "venue": "In Proc. ICML,", "citeRegEx": "Grunewalder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grunewalder et al\\.", "year": 2012}, {"title": "Nonlinear functional regression: a functional RKHS approach", "author": ["H. Kadri", "E. Duflos", "P. Preux", "S. Canu", "M. Davy"], "venue": "In Proc. AISTATS,", "citeRegEx": "Kadri et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kadri et al\\.", "year": 2010}, {"title": "Functional regularized least squares classification with operator-valued kernels", "author": ["H. Kadri", "A. Rabaoui", "P. Preux", "E. Duflos", "A. Rakotomamonjy"], "venue": "In Proc. ICML,", "citeRegEx": "Kadri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kadri et al\\.", "year": 2011}, {"title": "A generalized kernel approach to structured output learning", "author": ["H. Kadri", "M. Ghavamzadeh", "P. Preux"], "venue": "Technical Report 00695631,", "citeRegEx": "Kadri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kadri et al\\.", "year": 2012}, {"title": "On learning vector-valued functions", "author": ["C. Micchelli", "M. Pontil"], "venue": "Neural Computation,", "citeRegEx": "Micchelli and Pontil,? \\Q2005\\E", "shortCiteRegEx": "Micchelli and Pontil", "year": 2005}, {"title": "Functional Data Analysis, 2nd edition", "author": ["J. Ramsay", "B. Silverman"], "venue": null, "citeRegEx": "Ramsay and Silverman,? \\Q2005\\E", "shortCiteRegEx": "Ramsay and Silverman", "year": 2005}, {"title": "Input space vs. feature space in kernel-based methods", "author": ["B. Sch\u00f6lkopf", "S. Mika", "C.J.C. Burges", "P. Knirsch", "M\u00fcller", "K.-R", "G. R\u00e4tsch", "A.J. Smola"], "venue": "IEEE Trans. on Neural Networks,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1999}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Face recognition under varying poses: The role of texture and shape", "author": ["N. Troje", "H. Bulthoff"], "venue": "Vision Research,", "citeRegEx": "Troje and Bulthoff,? \\Q1996\\E", "shortCiteRegEx": "Troje and Bulthoff", "year": 1996}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "A kernel regression framework for SMT", "author": ["Z. Wang", "J. Shawe-Taylor"], "venue": "Machine Translation,", "citeRegEx": "Wang and Shawe.Taylor,? \\Q2010\\E", "shortCiteRegEx": "Wang and Shawe.Taylor", "year": 2010}, {"title": "Kernel dependency estimation", "author": ["J. Weston", "O. Chapelle", "A. Elisseeff", "B. Sch\u00f6lkopf", "V. Vapnik"], "venue": "In Proceedings of the Advances in Neural Information Processing Systems", "citeRegEx": "Weston et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2003}, {"title": "Joint Kernel Maps", "author": ["J. Weston", "G. BakIr", "O. Bousquet", "B. Sch\u00f6lkopf", "T. Mann", "W. Noble"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "In many practical problems such as statistical machine translation (Wang & Shawe-Taylor, 2010) and speech recognition or synthesis (Cortes et al., 2005), we are faced with the task of learning a mapping between", "startOffset": 131, "endOffset": 152}, {"referenceID": 6, "context": "The focus in the machine learning and statistics communities has been mainly on multi-task learning (vector outputs) and functional data analysis (functional outputs) (Caruana, 1997; Ramsay & Silverman, 2005), where in both cases output data reside in a Euclidean space, but there has also been considerable interest in expanding general learning algorithms to structured outputs.", "startOffset": 167, "endOffset": 208}, {"referenceID": 2, "context": "Two different, but closely related, kernel-based approaches for structured output learning can be found in the literature (Bakir et al., 2007): kernel dependency estimation (KDE) and joint kernel maps (JKM).", "startOffset": 122, "endOffset": 142}, {"referenceID": 2, "context": "Two different, but closely related, kernel-based approaches for structured output learning can be found in the literature (Bakir et al., 2007): kernel dependency estimation (KDE) and joint kernel maps (JKM). KDE is a regression-based approach that was first proposed by Weston et al. (2003) and then reformulated by Cortes et al.", "startOffset": 123, "endOffset": 291}, {"referenceID": 2, "context": "Two different, but closely related, kernel-based approaches for structured output learning can be found in the literature (Bakir et al., 2007): kernel dependency estimation (KDE) and joint kernel maps (JKM). KDE is a regression-based approach that was first proposed by Weston et al. (2003) and then reformulated by Cortes et al. (2005). The idea is to define a kernel on the output space Y to project the structured output to ar X iv :1 20 5.", "startOffset": 123, "endOffset": 337}, {"referenceID": 21, "context": "On the other hand, the JKM approach is based on joint kernels, which are nonlinear similarity measures between input-output pairs (Tsochantaridis et al., 2005; Weston et al., 2007).", "startOffset": 130, "endOffset": 180}, {"referenceID": 24, "context": "On the other hand, the JKM approach is based on joint kernels, which are nonlinear similarity measures between input-output pairs (Tsochantaridis et al., 2005; Weston et al., 2007).", "startOffset": 130, "endOffset": 180}, {"referenceID": 20, "context": "On the other hand, the JKM approach is based on joint kernels, which are nonlinear similarity measures between input-output pairs (Tsochantaridis et al., 2005; Weston et al., 2007). While in KDE separate kernels are used to project input and output data to two (possibly different) feature spaces, the joint kernel in JKM maps them into a single feature space, which then allows us to take advantage of our prior knowledge on both input-output and output correlations. However, this improvement requires an exhaustive pre-image computation during training, a problem that is encountered by KDE only in the test phase. Avoiding this computation during training is an important advantage of KDE over JKM methods. In this paper, we focus on the KDE approach to structured output learning. The main contributions of this paper can be summarized as follows: 1) Building on the works of Caponnetto & De Vito (2007) and Brouard et al.", "startOffset": 131, "endOffset": 909}, {"referenceID": 3, "context": "The main contributions of this paper can be summarized as follows: 1) Building on the works of Caponnetto & De Vito (2007) and Brouard et al. (2011), we propose a more general KDE formulation (prediction and pre-image steps) based on operator-valued (multi-task) kernels instead of scalar-valued ones used by the existing methods (Sec.", "startOffset": 127, "endOffset": 149}, {"referenceID": 3, "context": "The main contributions of this paper can be summarized as follows: 1) Building on the works of Caponnetto & De Vito (2007) and Brouard et al. (2011), we propose a more general KDE formulation (prediction and pre-image steps) based on operator-valued (multi-task) kernels instead of scalar-valued ones used by the existing methods (Sec. 3). This extension allows KDE to capture the dependencies between the outputs as well as between the input and output variables, which is an improvement over the existing KDE methods that fail to take into account these dependencies. 2) We also propose a variant (generalization) of the kernel trick to cope with the technical difficulties encountered when working with operator-valued kernels (Sec. 3). This allows us to (i) formulate the pre-image problem using only kernel functions (not feature maps that cannot be computed explicitly), and (ii) avoid the computation of the inner product between feature maps after being modified with an operator whose role is to capture the structure of complex objects. 3) We then introduce a novel family of operator-valued kernels, based on covariance operators on RKHSs, that allows us to take full advantage of our KDE formulation. These kernels offer a simple and powerful way to address the main limitations of the original KDE formulation, namely the decoupling between outputs in the image space and the inability to use a joint feature space (Sec. 4). 4) We show how the pre-image problem, in the case of covariance and conditional covariance operator-valued kernels, can be expressed only in terms of input and output Gram matrices, and provide a low rank approximation to efficiently compute it (Sec. 4). 5) Finally, we empirically evaluate the performance of our proposed KDE approach and show its effectiveness on three structured output prediction problems involving numeric and non-numerical outputs (Sec. 6). It should be noted that generalizing KDE using operator-valued kernels was first proposed in Brouard et al. (2011). The authors have applied this generalization to the problem of link prediction which did not require a preimage step.", "startOffset": 127, "endOffset": 2017}, {"referenceID": 5, "context": "We now provide a few definitions related to operatorvalued kernels and their associated RKHSs that are used in the paper (see (Micchelli & Pontil, 2005; Caponnetto et al., 2008; \u00c1lvarez et al., 2012) for more details).", "startOffset": 126, "endOffset": 199}, {"referenceID": 0, "context": "We now provide a few definitions related to operatorvalued kernels and their associated RKHSs that are used in the paper (see (Micchelli & Pontil, 2005; Caponnetto et al., 2008; \u00c1lvarez et al., 2012) for more details).", "startOffset": 126, "endOffset": 199}, {"referenceID": 9, "context": "These kernel spaces have recently received more attention, since they are suitable for leaning in problems where the outputs are vectors (as in multitask learning (Evgeniou et al., 2005)) or functions (as in functional regression (Kadri et al.", "startOffset": 163, "endOffset": 186}, {"referenceID": 13, "context": ", 2005)) or functions (as in functional regression (Kadri et al., 2010)) instead of scalars.", "startOffset": 51, "endOffset": 71}, {"referenceID": 12, "context": "Also, it has been shown recently that these spaces are appropriate for learning conditional mean embeddings (Grunewalder et al., 2012).", "startOffset": 108, "endOffset": 134}, {"referenceID": 21, "context": "Our generalized formulation consists of learning the mapping g using an operator-valued kernel ridge regression rather than a scalar-valued one as in the formulations of Weston et al. (2003) and Cortes et al.", "startOffset": 170, "endOffset": 191}, {"referenceID": 7, "context": "(2003) and Cortes et al. (2005). Using an operator-valued kernel mapping, we construct a joint feature space from information of input and output spaces in which inputoutput and output correlations can be taken into account.", "startOffset": 11, "endOffset": 32}, {"referenceID": 7, "context": "input and output kernels can be infinite dimensional, contrary to Cortes et al. (2005) that only considers finite feature spaces.", "startOffset": 66, "endOffset": 87}, {"referenceID": 7, "context": "Note that the KDE regression and prediction steps of Cortes et al. (2005) can be recovered from Eqs.", "startOffset": 53, "endOffset": 74}, {"referenceID": 9, "context": "Operator-valued kernels have been studied more in the context of multi-task learning, where the output is assumed to be in R with d the number of tasks (Evgeniou et al., 2005).", "startOffset": 152, "endOffset": 175}, {"referenceID": 10, "context": "These operators that provide the simplest measure of dependency have been successfully applied to the problem of dimensionality reduction (Fukumizu et al., 2004), and played an important role in dealing with a number of statistical test problems (Gretton et al.", "startOffset": 138, "endOffset": 161}, {"referenceID": 11, "context": ", 2004), and played an important role in dealing with a number of statistical test problems (Gretton et al., 2005).", "startOffset": 92, "endOffset": 114}, {"referenceID": 0, "context": "Although this property can be restrictive in specifying input-output correlations, because of its simplicity, most of the operator-valued kernels proposed in the literature belong to this category (see (\u00c1lvarez et al., 2012) for a review of separable and beyond separable operator-valued kernels).", "startOffset": 202, "endOffset": 224}, {"referenceID": 23, "context": "Existing KDE formulations try to address this issue either by performing a kernel PCA to decorrelate the outputs (Weston et al., 2003), or by incorporating some form of prior knowledge in the regression step using some specific constraints on the regression matrix which performs the mapping between input and output feature spaces (Cortes et al.", "startOffset": 113, "endOffset": 134}, {"referenceID": 8, "context": ", 2003), or by incorporating some form of prior knowledge in the regression step using some specific constraints on the regression matrix which performs the mapping between input and output feature spaces (Cortes et al., 2007).", "startOffset": 205, "endOffset": 226}, {"referenceID": 8, "context": "Moreover, in contrast to (Cortes et al., 2007), our approach allows us to deal with infinitedimensional feature spaces, and encodes prior knowledge on input-output dependencies without requiring any particular form of constraints between input and output mappings.", "startOffset": 25, "endOffset": 46}, {"referenceID": 24, "context": "Another approach to take into account input-output correlations is to use joint kernels, that are scalarvalued functions (similarity measure) of input-output pairs (Weston et al., 2007).", "startOffset": 164, "endOffset": 185}, {"referenceID": 21, "context": "In this context, the problem of learning the mapping f from X to Y is reformulated as learning a function f\u0302 from X \u00d7 Y to R using a joint kernel (JK) (Tsochantaridis et al., 2005).", "startOffset": 151, "endOffset": 180}, {"referenceID": 5, "context": "This can be seen from the feature space formulation of operator-valued kernels (Caponnetto et al., 2008; Kadri et al., 2011).", "startOffset": 79, "endOffset": 124}, {"referenceID": 14, "context": "This can be seen from the feature space formulation of operator-valued kernels (Caponnetto et al., 2008; Kadri et al., 2011).", "startOffset": 79, "endOffset": 124}, {"referenceID": 24, "context": "We now show how two joint kernels in the literature (Weston et al., 2007) can be recovered by a suitable choice of operator-valued kernel.", "startOffset": 52, "endOffset": 73}, {"referenceID": 8, "context": "In the second problem, we evaluate the two implementations of our KDE method with a constrained regression version of KDE (Cortes et al., 2007) and Max-Margin Markov Networks (MNs) (Taskar et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 19, "context": ", 2007) and Max-Margin Markov Networks (MNs) (Taskar et al., 2004).", "startOffset": 45, "endOffset": 66}, {"referenceID": 20, "context": "In the first problem, we compare our method using both covariance and conditional covariance operator-valued kernels with the KDE algorithms of Weston et al. (2003) and Cortes et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 7, "context": "(2003) and Cortes et al. (2005). In the second problem, we evaluate the two implementations of our KDE method with a constrained regression version of KDE (Cortes et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 7, "context": "(2003) and Cortes et al. (2005). In the second problem, we evaluate the two implementations of our KDE method with a constrained regression version of KDE (Cortes et al., 2007) and Max-Margin Markov Networks (MNs) (Taskar et al., 2004). In the third problem, in addition to scalar-valued KDE, we compare them with the joint kernel map (JKM) approach of Weston et al. (2007).", "startOffset": 11, "endOffset": 374}, {"referenceID": 23, "context": "We use exactly the same dataset and setting as in the experiments of (Weston et al., 2003).", "startOffset": 69, "endOffset": 90}, {"referenceID": 21, "context": "Here we consider the image reconstruction problem used in Weston et al. (2003). This problem takes the top half (the first 8 pixel lines) of a USPS postal digit as input and estimates its bottom half.", "startOffset": 58, "endOffset": 79}, {"referenceID": 21, "context": "Here we consider the image reconstruction problem used in Weston et al. (2003). This problem takes the top half (the first 8 pixel lines) of a USPS postal digit as input and estimates its bottom half. We use exactly the same dataset and setting as in the experiments of (Weston et al., 2003). We apply our KDE method using both covariance and conditional covariance operator-valued kernels and compare it with the KDE algorithms of Weston et al. (2003) and Cortes et al.", "startOffset": 58, "endOffset": 453}, {"referenceID": 7, "context": "(2003) and Cortes et al. (2005). In all these methods, we use RBF kernels for both input and output with the parameters shown in Table 2 (left).", "startOffset": 11, "endOffset": 32}, {"referenceID": 19, "context": "(Right) Performance (mean and standard deviation of Well Recognized word Characters (WRC)) of Max-Margin Markov Networks (MNs) algorithm (Taskar et al., 2004), constrained regression version of KDE (Cortes et al.", "startOffset": 137, "endOffset": 158}, {"referenceID": 8, "context": ", 2004), constrained regression version of KDE (Cortes et al., 2007), and our KDE method on an optical character recognition (OCR) task.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "(Left) Performance (mean and standard deviation of RBF loss) of the KDE algorithms of Weston et al. (2003) and Cortes et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 7, "context": "(2003) and Cortes et al. (2005), and our KDE method with covariance and conditional covariance operator-valued kernels on an image reconstruction problem of handwritten digits.", "startOffset": 11, "endOffset": 32}, {"referenceID": 7, "context": "In this problem, kPCA-based KDE performed better than the KDE formulation of Cortes et al. (2005). In fact, the latter is equivalent to using an identity-based operator-valued kernel in our formulation, and thus, it is incapable of capturing the dependencies in the output feature space (contrary to the other methods considered here).", "startOffset": 77, "endOffset": 98}, {"referenceID": 8, "context": "We compare our approach with a constrained regression version of Cortes\u2019s KDE formulation (Cortes et al., 2007) and Max-Margin Markov Networks (MNs) (Taskar et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 19, "context": ", 2007) and Max-Margin Markov Networks (MNs) (Taskar et al., 2004).", "startOffset": 45, "endOffset": 66}, {"referenceID": 8, "context": "Results for these two methods are reported from (Cortes et al., 2007).", "startOffset": 48, "endOffset": 69}, {"referenceID": 8, "context": "We use exactly the same experimental setup described in (Cortes et al., 2007) to evaluate our operator-valued KDE approach.", "startOffset": 56, "endOffset": 77}, {"referenceID": 17, "context": "This problem is the one used in Taskar et al. (2004) and Cortes et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 7, "context": "(2004) and Cortes et al. (2005). The dataset is a subset of the handwritten words collected by Rob Kassel at the MIT Spoken Language Systems Group.", "startOffset": 11, "endOffset": 32}, {"referenceID": 7, "context": "In this experiment, we first compare the covariancebased operator-valued KDE with the KDE algorithm of Cortes et al. (2005) and the JKM approach of Weston et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 7, "context": "In this experiment, we first compare the covariancebased operator-valued KDE with the KDE algorithm of Cortes et al. (2005) and the JKM approach of Weston et al. (2007), and then show how we can speed up the training of our proposed KDE method using incomplete Cholesky decomposition; see (Kadri et al.", "startOffset": 103, "endOffset": 169}, {"referenceID": 7, "context": "In this experiment, we first compare the covariancebased operator-valued KDE with the KDE algorithm of Cortes et al. (2005) and the JKM approach of Weston et al. (2007), and then show how we can speed up the training of our proposed KDE method using incomplete Cholesky decomposition; see (Kadri et al., 2012, Appendix C) for more details on applying incomplete Cholesky decomposition to block kernel matrices associated to separable operator-valued kernels. Similar to the \u201clearning to smile\u201d experiment in Weston et al. (2007), we consider the problem of mapping the rotated view of a face to the plain expression (frontal view) of the same face.", "startOffset": 103, "endOffset": 529}, {"referenceID": 21, "context": "Mean-squared errors (MSE) of the JKM algorithm of Weston et al. (2007), KDE algorithm of Cortes et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 7, "context": "(2007), KDE algorithm of Cortes et al. (2005), and our KDE method with covariance operatorvalued kernels on the face-to-face mapping problem.", "startOffset": 25, "endOffset": 46}, {"referenceID": 24, "context": "We apply a JKM using the patch-wise joint kernel defined in (Weston et al., 2007), with patches of size 10\u00d710 that overlap by 5 pixels.", "startOffset": 60, "endOffset": 81}, {"referenceID": 7, "context": "2 compares the performance of the efficient implementation (using incomplete Cholesky decomposition) of our conditional covariance operator-valued KDE method with the original KDE algorithm (Cortes et al., 2005).", "startOffset": 190, "endOffset": 211}, {"referenceID": 7, "context": "We compare the efficient implementation (using incomplete Cholesky decomposition) of our conditional covariance operator-valued KDE method with the KDE algorithm of Cortes et al. (2005). While the parameter n is m1 = m2 = n in the incomplete Cholesky decomposition, it is the number of faces randomly selected from 1,200 training faces in the KDE algorithm of Cortes et al.", "startOffset": 165, "endOffset": 186}, {"referenceID": 7, "context": "We compare the efficient implementation (using incomplete Cholesky decomposition) of our conditional covariance operator-valued KDE method with the KDE algorithm of Cortes et al. (2005). While the parameter n is m1 = m2 = n in the incomplete Cholesky decomposition, it is the number of faces randomly selected from 1,200 training faces in the KDE algorithm of Cortes et al. (2005). The right-most point is the MSE of training on the full training set of n = 1, 200 examples.", "startOffset": 165, "endOffset": 381}], "year": 2015, "abstractText": "We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach on three structured output problems, and compare it to the state-of-the-art kernelbased structured output regression methods.", "creator": "LaTeX with hyperref package"}}}