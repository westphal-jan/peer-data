{"id": "1205.2874", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2012", "title": "Decoupling Exploration and Exploitation in Multi-Armed Bandits", "abstract": "We consider a multi-armed bandit problem where the decision maker can explore and exploit different arms at every round. The exploited arm adds to the decision maker's cumulative reward (without necessarily observing the reward) while the explored arm reveals its value. We devise algorithms for this setup and show that the dependence on the number of arms, k, can be much better than the standard square root of k dependence, depending on the behavior of the arms' reward sequences. For the important case of piecewise stationary stochastic bandits, we show a significant improvement over existing algorithms. Our algorithms are based on a non-uniform sampling policy, which we show is essential to the success of any algorithm in the adversarial setup. Finally, we show some simulation results on an ultra-wide band channel selection inspired setting indicating the applicability of our algorithms.", "histories": [["v1", "Sun, 13 May 2012 15:11:00 GMT  (113kb,D)", "https://arxiv.org/abs/1205.2874v1", null], ["v2", "Wed, 27 Jun 2012 21:56:07 GMT  (114kb,D)", "http://arxiv.org/abs/1205.2874v2", "Paper as published in the proceedings of ICML 2012"], ["v3", "Sat, 30 Jun 2012 08:17:09 GMT  (114kb,D)", "http://arxiv.org/abs/1205.2874v3", "Full version of the paper presented at ICML 2012"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["orly avner", "shie mannor", "ohad shamir"], "accepted": true, "id": "1205.2874"}, "pdf": {"name": "1205.2874.pdf", "metadata": {"source": "META", "title": "Decoupling Exploration and Exploitation in Multi-Armed Bandits", "authors": ["Orly Avner", "Shie Mannor", "Ohad Shamir"], "emails": ["orlyka@tx.technion.ac.il", "shie@ee.technion.ac.il", "ohadsh@microsoft.com"], "sections": [{"heading": null, "text": "\u221a k dependence, depending on the behavior of the weapon reward sequences. In the important case of piecemeal stochastic bandits, we show a significant improvement over existing algorithms. Our algorithms are based on a non-uniform sampling rate, which is indispensable for the success of any algorithm in the opposing setup. Finally, we show some simulation results based on a setting inspired by the selection of an ultra-wide channel, indicating the applicability of our algorithms."}, {"heading": "1. Introduction", "text": "In fact, we are able to set out in search of new paths that will lead us into the future."}, {"heading": "2. Problem Setting", "text": "We use [k] as shorthand for {1,., k}. Bold-faced letters represent vectors, and 1A represents the indicator function for an event A. We use the standard bigOh notation O (\u00b7) to hide constants, and O (\u00b7) to hide constants and logarithmic factors. For a distribution vector p on the k-simplex, we use the notation-p-1 / 2 = k-2 to describe the \"1 / 2\" standard of distribution. It is straightforward to show that for a distribution vector this quantity is always used in [1, k]. Specifically, it is for uniform distribution and becomes smaller than the uneven distribution that reaches the value of 1 / 2 \"standard of distribution when p is a unit."}, {"heading": "3. Basic Algorithm and Results", "text": "In the analysis of our \"decoupled\" algorithms, the first question you could ask yourself is whether you can always get an improved regret performance compared to the standard bandit setting; the second question is whether there can be an improved regret performance (compared to the standard bandit setting); the third question is whether there can be a better regret performance (compared to the standard bandit algorithm setting) while the regret over any \"decoupled\" algorithm setting will be smaller and smaller; therefore, one cannot hope that there simply needs to be a strategy applied to maintain the standard bandit algorithms (compared to the standard bandit algorithm setting); however, how we will soon be able to achieve the actions rewards under certain realistic conditions; we now turn to present our first algorithm setting (algorithm 1 below) and the related regret analysis."}, {"heading": "4. Decoupling Provably Helps in some Adversarial Settings", "text": "However, this does not mean that our approach will actually perform better in practice: for example, it may be possible that our approach is demonstrably better than any standard bandit algorithm, for information-theoretical reasons you can provide a closer analysis of the standard bandit algorithms and achieve a similar result. In this section, we show that the idea of decoupling is helpful in cases where we will remember the situation (Yu & Mannor, 2009), but here we can provide performance demonstrably better than any standard bandit algorithm. We point out that the idea of decoupling is helpful in cases where we will be discussing (Yu & Mannor, 2009), but in the general and challenging Adversarial Setting.Instead, here we will discuss a slightly more general attitude where our goal is not achieved, in terms of the best individual action, but rather in terms of the best sequence of the specific actions."}, {"heading": "5. The Necessity of a Non-Uniform Querying Distribution", "text": "The theoretical results described above have shown the effectiveness of our approach in comparison to conventional bandit algorithms. However, the exact form of our query distribution (querying action i with a probability proportional to \u221a pj (t)) may still seem a little puzzling, but perhaps similar results can be obtained by simply querying actions randomly? In fact, this has happened in some other online learning scenarios where queries were allowed (e.g. (Yu & Manor, 2009; Agarwal et al., 2010). However, below, we show that in the adverse environment, an adaptive and non-uniform query distribution is actually necessary to achieve regret better than \u221a kT. For simplicity, we return to our default setting, where our goal is to compete with only the best single fixed action retrospectively. Theorem 6. Consider any online algorithm via k > 2 actions and horizon T, in which actions based on a fixed distribution are queried at the least there is a fixed strategy for the strategy."}, {"heading": "6. Experiments", "text": "We compare the decoupled approach to the common multi-armed bandit algorithms in a simulated adversary setting. Our user chooses between the communication channels where sampling and transmission can be decoupled. In other words, he can select a specific channel for transmission while choosing a different, seemingly less attractive distribution form. We simulate a heavily loaded UWB environment with a single alternating channel suitable for transmission. The reward for the k \u2212 1 channels are drawn from alternating even and truncated outdoors distributions with random parameters, resulting in the missed rewards in the area."}, {"heading": "7. Discussion", "text": "In this paper, we analyzed whether and how one can benefit in situations where exploration and exploitation can be \"decoupled\" from each other: namely, that one can ask for rewards, regardless of the action actually chosen. We developed some algorithms for this setting and showed that these can indeed lead to improved results, compared to the standard bandit environment, under certain conditions. We also conducted some experiments that replaced our theoretical finds.For simplicity, we focused on the case where only a single reward can be queried. If c > 1 queries are allowed, it is not difficult to show parallel guarantees for those in this essay in which dependence on k is replaced by dependence on k / c. Algorithmically, one simply has to query repeatedly from the query distribution c times instead of a single time. We suspect that similar lower limits can also be reached by actually selecting the reward and reward from the reward."}, {"heading": "Acknowledgements.", "text": "This research was partially supported by the CORNET consortium (http: / / www.cornet.org.il /)."}, {"heading": "A. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1. Proof of Thm. 1", "text": "We start by noting that for any possible distribution p1 (t),., pk (\u03b2) (\u03b2) (\u03b2), it must apply that we (t), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (p), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (p), p), p (1), p), p (1), p (p), p), p (1), p), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p, p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p, p (1), p (1), p, p, p (1), p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p (1, p, p, p, p, p (1, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "A.2. Proof of Thm. 2", "text": "For notational simplicity, we will use the O notation to hide both constants and second order factors (such as T / k) (such as T / k). Taking into account the evidence of Thm. 1, it is easy to note that it implies that with a probability of at least 1 \u2212 \u03b4 \u2212 P (v, \u03b4, \u00b5), max i (k), max i (k), max i (k) T + 1 gi (t) \u2212 T = 1 k = 1 pj (t) gj (t) \u2264 O (v2\u00b5) T + k2\u00b5 + k2T 3 / 2 T (k).o.g. Action 1 is in G. Then follows the thatT ability t = 1 k \u2264 j = 1 pj (t) (g1 (t) \u2212 gj (t) \u2212 gj (t) (t) (t)))) that we (v2\u00b5) that we (v2\u00b5 + v) have the difference (v2\u00b5 + v)."}, {"heading": "A.3. Proof of Thm. 3", "text": "The detection is very similar to that of Thm. 1, and we will therefore skip the derivation of some steps that are identical. We define the potential function Wt = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "A.4. Proof of Thm. 4", "text": "The proof is almost identical to that of Thm. 2, and we will only point to the difference (v = j = j = j = j = j = j = j = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p (t) (gis (t) \u2212 gj (t) \u2212 gj (t) \u2264 O (t))) (n = S (v2\u00b5 + v) T + k2T + k2T 3 / 2) This limit applies to any kind of rewards. Since each gj (t) i.i.d. and independent of pj (t) is chosen, we get this k = p = 1 pj (t) (t) \u2212 kis (t) \u2212 gj (t) \u2212 gj (t) \u2212 s (t) \u2212 gis (t) \u2212 gj (t) \u2212 gt (t) \u2212 g (t) (t) (t) (t) is a fairy-like (t) (t)."}, {"heading": "A.5. Proof of Thm. 5", "text": "After the standard proofs for multi-armed bandits (T \u2212 1), we will focus on deterministic algorithms (T = 6); we will show that there is a randomized adversarial strategy, so that for each deterministic algorithm a random algorithm is chosen that chooses the probability of action (this is because such an algorithm can be considered randomization via deterministic algorithms); the proof is inspired by the lower boundary result3 of (Garivier & Moulines, 2011); we will consider the following random adverse strategy as the first to be fixed; it then chooses an action a {2,., k} uniformly randomly, and an action t0. [t] with probability Pr (t0 = 12 and Pr."}, {"heading": "A.6. Proof Sketch of Thm. 6", "text": "The idea of proof is a reduction to the problem of distinguishing distorted coins. Suppose we have two Bernoulli random variables X, Y, one of which has only one parameter 12 and one parameter 1 2 +. It is generally known that for a universal constant c it is not possible to distinguish the two, with a fixed probability, using at most c / 2 samples from each other. We begin by stating that for the fixed distribution (p1,.., pk) there must be two actions whose probabilities are at most 1 / (k \u2212 1) each (otherwise there will be at least k \u2212 1 actions whose probabilities are greater than 1 / (k \u2212 1), which is impossible). Suppose that these are actions 1, 2. We construct a bandit problem in which the reward of the act 1 / (k \u2212 1) is the reward of the act c, whereby the reward of the act, c."}], "references": [{"title": "Optimal algorithms for online convex optimization with multipoint bandit feedback", "author": ["A. Agarwal", "O. Dekel", "L. Xiao"], "venue": "In COLT,", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Audibert", "J.-Y", "S. Bubeck"], "venue": "In COLT,", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Best arm identification in multi-armed bandits", "author": ["Audibert", "J.-Y", "S. Bubeck", "R. Munos"], "venue": "In COLT,", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Stochastic bandits with pathwise constraints", "author": ["O. Avner", "S. Mannor"], "venue": "In 50th IEEE Conference on Decision and Control,", "citeRegEx": "Avner and Mannor,? \\Q2011\\E", "shortCiteRegEx": "Avner and Mannor", "year": 2011}, {"title": "Decoupling exploration and exploitation in multi-armed bandits", "author": ["O. Avner", "S. Mannor", "O. Shamir"], "venue": "[cs.LG],", "citeRegEx": "Avner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Avner et al\\.", "year": 2012}, {"title": "Pure exploration in finitely-armed and continuous-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Bubeck et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2011}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Action elimination and stopping conditions for the multiarmed bandit and reinforcement learning problems", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "On tail probabilities for martingales", "author": ["D.A. Freedman"], "venue": "Annals of Probability,", "citeRegEx": "Freedman,? \\Q1975\\E", "shortCiteRegEx": "Freedman", "year": 1975}, {"title": "On upper-confidence bound policies for switching bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "In ALT,", "citeRegEx": "Garivier and Moulines,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Moulines", "year": 2011}, {"title": "Tracking the best expert", "author": ["M. Herbster", "M.K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Herbster and Warmuth,? \\Q1998\\E", "shortCiteRegEx": "Herbster and Warmuth", "year": 1998}, {"title": "Medium access in cognitive radio networks: A competitive multiarmed bandit framework", "author": ["L. Lai", "H. Jiang", "H.V. Poor"], "venue": "In Proc. Asilomar Conference on Signals, Systems, and Computers,", "citeRegEx": "Lai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2008}, {"title": "Distributed learning in multiarmed bandit with multiple players", "author": ["K. Liu", "Q. Zhao"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Liu and Zhao,? \\Q2010\\E", "shortCiteRegEx": "Liu and Zhao", "year": 2010}, {"title": "Piecewise-stationary bandit problems with side observations", "author": ["J.Y. Yu", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Yu and Mannor,? \\Q2009\\E", "shortCiteRegEx": "Yu and Mannor", "year": 2009}, {"title": "Substituting \u03b4 = exp(\u2212\u03b2 ), solving for , and using a union bound to make the result hold simultaneously for all i, the result follows. We will also need the following straightforward corollary of Freedman\u2019s inequality (Freedman", "author": ["\u2264 exp(\u2212\u03b2"], "venue": null, "citeRegEx": "..,? \\Q2006\\E", "shortCiteRegEx": "..", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "We should mention that UWB networks are highly complex, with many issues such as power constraints and multi-agency that have been considered in the multiarmed bandit framework (Liu & Zhao, 2010; Avner & Mannor, 2011; Lai et al., 2008), but the decoupling of ar X iv :1 20 5.", "startOffset": 177, "endOffset": 235}, {"referenceID": 5, "context": "The proofs of our theorems are provided in the appendix of the full version (Avner et al., 2012).", "startOffset": 76, "endOffset": 96}, {"referenceID": 0, "context": "(Agarwal et al., 2010) study a bandit setting with (one or more) queries per round.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "A different line of work ((Even-Dar et al., 2006; Audibert et al., 2010; Bubeck et al., 2011)) considers multi-armed bandits in a stochastic setting, where the goal is to identify the best action by performing pure exploration.", "startOffset": 26, "endOffset": 93}, {"referenceID": 2, "context": "A different line of work ((Even-Dar et al., 2006; Audibert et al., 2010; Bubeck et al., 2011)) considers multi-armed bandits in a stochastic setting, where the goal is to identify the best action by performing pure exploration.", "startOffset": 26, "endOffset": 93}, {"referenceID": 6, "context": "A different line of work ((Even-Dar et al., 2006; Audibert et al., 2010; Bubeck et al., 2011)) considers multi-armed bandits in a stochastic setting, where the goal is to identify the best action by performing pure exploration.", "startOffset": 26, "endOffset": 93}, {"referenceID": 3, "context": "P (Auer et al., 2002), with a \u00d5( \u221a kT ) regret upper bound, holding with high probability, or the Implicitly Normalized Forecaster of (Audibert & Bubeck, 2009) with O( \u221a kT ) regret.", "startOffset": 2, "endOffset": 21}, {"referenceID": 3, "context": "setting (Auer et al., 2002).", "startOffset": 8, "endOffset": 27}, {"referenceID": 3, "context": "This setting is well-known in the online learning literature, and has been considered for instance in (Herbster & Warmuth, 1998) for full-information online learning (under the name of \u201ctracking the best expert\u201d) and in (Auer et al., 2002) for the bandit setting (under the name of \u201cregret against arbitrary strategies\u201d).", "startOffset": 220, "endOffset": 239}, {"referenceID": 3, "context": "The algorithm we use follows the lead of (Auer et al., 2002) and is presented as Algorithm 2.", "startOffset": 41, "endOffset": 60}, {"referenceID": 3, "context": "It is interesting to note that unlike the standard lower bound proof for standard bandits (Auer et al., 2002), we obtain here an \u03a9( \u221a kT ) regret even when \u2206 > 0 is fixed and doesn\u2019t decay with T .", "startOffset": 90, "endOffset": 109}, {"referenceID": 0, "context": ", (Yu & Mannor, 2009; Agarwal et al., 2010)).", "startOffset": 2, "endOffset": 43}, {"referenceID": 3, "context": "We implemented Algorithm 1, Exp3 (Auer et al., 2002), Exp3.", "startOffset": 33, "endOffset": 52}, {"referenceID": 3, "context": "P (Auer et al., 2002), a simple round robin policy (which just cycles through the arms in a fixed order) and a \u201cgreedy\u201d decoupled form of round robin, which performs uniform queries and picks actions greedily based on the highest empirical average reward.", "startOffset": 2, "endOffset": 21}, {"referenceID": 3, "context": "Finally, it remains to extend other bandit-related algorithms, such as EXP4 (Auer et al., 2002), to our setting, and study the advantage of decoupling in other adversarial online learning problems.", "startOffset": 76, "endOffset": 95}, {"referenceID": 9, "context": "We will also need the following straightforward corollary of Freedman\u2019s inequality (Freedman, 1975) (see also Lemma A.", "startOffset": 83, "endOffset": 99}, {"referenceID": 3, "context": "1 in (Auer et al., 2002)), we have that for any function f(r) of the reward sequence r, whose range is at most [0, b], it holds that", "startOffset": 5, "endOffset": 24}], "year": 2012, "abstractText": "We consider a multi-armed bandit problem where the decision maker can explore and exploit different arms at every round. The exploited arm adds to the decision maker\u2019s cumulative reward (without necessarily observing the reward) while the explored arm reveals its value. We devise algorithms for this setup and show that the dependence on the number of arms, k, can be much better than the standard \u221a k dependence, depending on the behavior of the arms\u2019 reward sequences. For the important case of piecewise stationary stochastic bandits, we show a significant improvement over existing algorithms. Our algorithms are based on a non-uniform sampling policy, which we show is essential to the success of any algorithm in the adversarial setup. Finally, we show some simulation results on an ultra-wide band channel selection inspired setting indicating the applicability of our algorithms.", "creator": "LaTeX with hyperref package"}}}