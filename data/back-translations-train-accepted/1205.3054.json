{"id": "1205.3054", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "Approximate Modified Policy Iteration", "abstract": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three approximate MPI (AMPI) algorithms that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide an error propagation analysis for AMPI that unifies those for approximate policy and value iteration. We also provide a finite-sample analysis for the classification-based implementation of AMPI (CBMPI), which is more general (and somehow contains) than the analysis of the other presented AMPI algorithms. An interesting observation is that the MPI's parameter allows us to control the balance of errors (in value function approximation and in estimating the greedy policy) in the final performance of the CBMPI algorithm.", "histories": [["v1", "Mon, 14 May 2012 15:01:31 GMT  (66kb,D)", "https://arxiv.org/abs/1205.3054v1", null], ["v2", "Fri, 18 May 2012 06:56:47 GMT  (68kb,D)", "http://arxiv.org/abs/1205.3054v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bruno scherrer", "victor gabillon", "mohammad ghavamzadeh", "matthieu geist"], "accepted": true, "id": "1205.3054"}, "pdf": {"name": "1205.3054.pdf", "metadata": {"source": "META", "title": "Approximate Modified Policy Iteration", "authors": ["Bruno Scherrer", "Victor Gabillon", "Mohammad Ghavamzadeh"], "emails": ["Bruno.Scherrer@inria.fr", "Victor.Gabillon@inria.fr", "Mohammad.Ghavamzadeh@inria.fr", "Matthieu.Geist@supelec.fr"], "sections": [{"heading": "1. Introduction", "text": "It is an iterative algorithm for calculating the optimal policy and value function of a Markov decision-making process (MDP). Starting from an arbitrary value function v0, it generates a sequence of value policy analysis pairs\u03c0k + 1 = G vk (greedy step) (1) vk + 1 = (T\u03c0k + 1) mvk (evaluation step) (2) appearing in the proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner (s).where G vk is a greedy policy, the vk, T\u03c0k is the Bellman operator associated with politics, and m \u2265 1 is a parameter. MPI generalizes the well-known dynamic programming algorithms Value Iteration (VI) and Policy Iteration () for Large Spaces (MPI)."}, {"heading": "2. Background", "text": "We consider a discounted MDP < S, A, P, r, \u03b3 >, where S is a state space, A is a finite space of action, P (ds) | s, a), for all (s, a), is a probability core on S, the reward function r: S \u00b7 A \u2192 R is limited by Rmax, and \u03b3 (0, 1) is a discount factor. A deterministic policy is defined as a mapping: S \u2192 A. For a policy action, we can write r\u03c0 (s) = r (s), \u03c0 (s), and P\u03c0 (ds) = P (ds) | s, \u03c0 (s). The value of a policy action in a state s is defined as the expected discounted sum of the rewards received from the states and follow the policy, i.e., v\u03c0 (s) is a policy that v\u03c0 (s) = an optimal measure, v [s] is an optimal policy, v [s]."}, {"heading": "3. Approximate MPI Algorithms", "text": "In this section we describe three approximate MPI (AMPI) algorithms. These algorithms are based on a function space F to approximate value functions, and in the third algorithm also on a policy space to represent greedy strategies. In the following we describe the iteration k of these iterative algorithms."}, {"heading": "3.1. AMPI-V", "text": "For the first and simplest AMPI algorithm presented in the work, we assume that the values vk (i) are represented in a function space F'R | S |. In each state, the action \u03c0k + 1 (s), which is greedy, can be estimated as follows: \u03c0k + 1 (s) = arg max a'A1M (M'J = 1 r (j'J) a + \u03b3vk (s (s) a)))), (3), where the actions a and 1 \u2264 j \u2264 M, r (j) a and s (j) a are samples of rewards and next states when action a is taken in state s. Thus, in order to approximate the greedy action in a state s, M \u2212 A | requires samples. The algorithm works as follows: It first stings N states from a distribution series, i.e., {s (i), i (i) r (i), i (i) (i) (i) (i)."}, {"heading": "3.2. AMPI-Q", "text": "In AMPI-Q, we replace the value function v: S \u2192 R with an action-value function Q: S \u00b7 A \u2192 R. The Bellman operator for a policy \u03c0 on a state-action pair (s, a) can then be written as [T\u03c0Q] (s, a) = reward q (s, a) + \u03b3Q (s \u2032, \u03c0 (s \u2032) | s \u00b2 P (\u00b7 s, a), and the greedy operator is defined as \"s \u00b2 s \u00b2 s\" (s) = arg max a \u00b2 A Q (s, a).In AMPI-Q, the action functions Qk are displayed in a function space F R | S \u00b7 A |, and the greedy action w.r.t. Qk on a state s, i.e., the computing power + 1 (s) is calculated as \"s\" (s)."}, {"heading": "3.3. Classification-Based MPI", "text": "The third AMPI algorithm presented in this paper, referred to as the classification-based MPI (CBMPI), uses an ex-plicit representation for politics in addition to the functions used for value functions, and the idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011), in which we seek politics in a political space (defined by a classifier) rather than calculating it from the estimated value or action value (as in AMPI-V and AMPI-Q). To describe the CBMPI formulation, we must first rewrite the MPI formulation (Eqs 1 and 2) asvk = (Tbk) asvk \u2212 1 (evaluation step) (5)."}, {"heading": "4. Error propagation", "text": "In this section, we derive a general formulation for the propagation of the error by the iterations of an AMPI algorithm = = two functions. The error propagation analysis line is different in VI and PI algorithms. VI analysis is based on the fact that this algorithm calculates the fixed point of the Bellman optimality operator, and this operator is a game contraction in max-norm (Bertsekas & Tsitsiklis, 1996). On the other hand, we can implement CBMPI more efficiently by reusing the rollouts generated for the greedy step in the evaluation. It can be shown that the operator by which the PI updates the value from one iteration to the next is not a contraction in max-norm in general. Unfortunately, we can show that the same property applies to MPI if it is not reduced to VI (i.e., m > 1) Proposition, there is no standard for the MPI."}, {"heading": "1) for any set of n policies {\u03c01, . . . , \u03c0n},", "text": "(\u03b3P\u03c01) (\u03b3P\u03c02 et al., 2012, Appendix B). According to the definition I, we now derive a punctual limit on the loss.Lemma 2. (\u03b3P\u03c02 et al.) After the definition I that we have introduced, we will use the terms \"Pi,\" \"P2\" and \"P3,\" \"P3,\" \"\" P3, \"\" \"P3,\" \"\" P3, \"\" \"\" \"P3,\" \"\" \"\" and \"P4,\" \"\" \"P3,\" \"\" \"\" P4, \"\", \"\" \",\" \",\" \",\" \",\" \",\", \"\", \"\", \",\" \",\", \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \"\", \",\", \",\", \"\", \",\", \",\" \",\", \"\", \",\" \"\", \",\" \",\", \",\" \",\" \"\", \"\", \",\", \"\", \"\", \",\", \"\", \",\", \"\", \",\" \",\", \",\", \",\", \"\" \",\", \",\", \",\", \",\", \",\" \",\", \",\" \",\", \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\""}, {"heading": "5. Finite-Sample Analysis of CBMPI", "text": "In this section, we will focus on CBMPI and detail the possible form of the error concepts present in the boundary of Thm. 1. We select CBMPI from among the proposed algorithms because its analysis is more general than the others, since we have both errors and assessment steps (in some norms), and also because it has an interesting influence on the parameter m (see Remark 5). We first provide a boundary for the greedy step error. From the definition of \"k\" for CBMPI (Eq. 12) and the description of the greedy step in CBMPI, we can easily observe that we have the parameter \"k\" 1, \"s = L.K = L.K \u2212 1 (number).Lemma 4 (Proof in (Scherrer et al., 2012, Appendix E). Let us leave a political space with a finite VCdimension h = V C (p.) and a distribution over the state space S. Let us read the number of states in D."}, {"heading": "6. Summary and Extensions", "text": "In this paper, we examined a DP algorithm called Modified Policy Iteration (MPI), which despite its generality, which includes the famous methods of policy and value titeration, has not been thoroughly examined in the literature. We proposed three approximate MPI (AMPI) algorithms, which are extensions of the known ADP algorithms: Customized iteration, fittedQ iteration, and classification-based policy iteration. We reported on an error propagation analysis for the AMPI, which combines those for approximate policy and value teration. We also provided a finite sample analysis for the classification-based implementation of the AMPI (CBMPI), the analysis of which is more general than the other AMPI methods presented. Our results suggest that the parameter of the MPI allows us to examine the balance of errors (in terms of value function approximation and estimation of greedy policies) in the AMPI's final performance of the existing BMPI algorithm, although the AMPI requires additional control of the AMPI's problems."}, {"heading": "Ernst, D., Geurts, P., and Wehenkel, L. Tree-based batch", "text": "Journal of Machine Learning Research, 6: 503-556, 2005."}, {"heading": "Farahmand, A., Munos, R., and Szepesva\u0301ri, Cs. Error", "text": "In Proceedings of NIPS, pp. 568-576, 2010. Fern, A., Yoon, S., and Givan, R. Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes. Journal of Artificial Intelligence Research, 25: 75-118, 2006.Gabillon, V., Lazaric, A., Ghavamzadeh, M., and Scherrer, B. Classification-based policy iteration with a Kritik. In Proceedings of ICML, pp. 1049-1056, 2011."}, {"heading": "Lagoudakis, M. and Parr, R. Reinforcement Learning as", "text": "Classification: Leveraging Modern Classifiers. In Proceedings of ICML, pp. 424-431, 2003."}, {"heading": "Lazaric, A., Ghavamzadeh, M., and Munos, R. Analysis", "text": "of a classification-based policy iteration algorithm. In Proceedings of ICML, pp. 607-614, 2010."}, {"heading": "Munos, R. Error Bounds for Approximate Policy Iteration.", "text": "In Proceedings of ICML, pp. 560-567, 2003.Munos, R. Performance Bounds in Lp-norm for Approximate Value Iteration. SIAM J. Control and Optimization, 46 (2): 541-561, 2007."}, {"heading": "Munos, R. and Szepesva\u0301ri, Cs. Finite-Time Bounds for", "text": "Fitted Value Iteration. Journal of Machine Learning Research, 9: 815-857, 2008.Puterman, M. and Shin, M. Modified Political Iteration Algorithms for Discounted Markov Decision Problems. Management Science, 24 (11), 1978."}, {"heading": "Scherrer, Bruno, Gabillon, Victor, Ghavamzadeh, Mohammad, and Geist, Matthieu. Approximate Modified Policy", "text": "Iteration. Technical Report, INRIA, May 2012."}, {"heading": "Szepesva\u0301ri, Cs. Reinforcement Learning Algorithms for", "text": "MDPs. In Wiley Encyclopedia of Operations Research. Wiley, 2010."}, {"heading": "Thiery, Christophe and Scherrer, Bruno. Performance", "text": "Technical Report, INRIA, 2010. Supplementary material for an approximate modified policy iteration"}, {"heading": "A. Proof of Lemma 1", "text": "Before we begin, let us remember the following definitions: bk = vk \u2212 T\u03c0k + 1vk, dk = v \u0445 \u2212 (T\u03c0k) mvk \u2212 1 = v \u0445 \u2212 (vk \u2212 k), sk = (T\u03c0k) mvk \u2212 1 \u2212 v\u03c0k = (vk \u2212 k) \u2212 v\u03c0k."}, {"heading": "Bounding bk", "text": "bk = vk \u2212 T\u03c0kvk + 1vk = vk \u2212 T\u03c0kvk + T\u03c0kvk \u2212 T\u03c0k + 1vk (a) \u2264 vk \u2212 T\u03c0kvk + \u2032 k + 1 = vk \u2212 k \u2212 T\u03c0kvk + \u03b3P\u03c0k k k k + k \u2212 \u03b3P\u03c0k + \u2032 k + 1 (b) = vk \u2212 k \u2212 T\u03c0k (vk \u2212 k) + (I \u2212 \u03b3P\u03c0k) k + \u2032 k + 1. (18) Using the definition of xk, i.e., xk \u0445 = (I \u2212 \u03b3P\u03c0k) k + \u2032 k + 1, (19) we can equate (18) asbk \u2212 k \u2264 vk \u2212 k (vk \u2212 k) + xk (c) mvk \u2212 k (c) \u2212 k \u2212 k (vk \u2212 k) + xk (c) = (T\u0432k \u2212 k \u2212 k) + xk (c)."}, {"heading": "Bounding dk", "text": "dk 1 = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p p = p = p = p = p p = p = p = p = p"}, {"heading": "B. Proof of Lemma 2", "text": "We start by focusing our analysis on AMPI. Here, we are interested in limiting the loss lk = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v v = v = v = v v = v = v = v v = v v v = v = v v v = v = v v v = v v = v v v = v v v = v v v = v = v v = v v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v = v v = v = v = v = v v = v = v = v = v = v = v v = v v = v = v = v v = v = v = v = v = v = v = v v v = v = v = v v = v = v = v v v v = v = v = v v v v v v = v v v v = v = v v v v v = v v v = v v v v = v v v v v = v v v v = v"}, {"heading": "C. Proof of Lemma 3", "text": "For each integer t and each vector z implies the definition of \"t\" and the inequality of \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l, \"l,\" l \"l,\" l, \"l,\" l \"l\" l, \"l,\" l \"l,\" l, \"l,\" l \"l,\" l \"l,\" l, \"l\" l, \"l,\" l \"l\" l, \"l\" l, \"l,\" l \"l\" l, \"l,\" l \"l,\" l, \"l\" l \"l\" l, \"l,\" l, \"l\" l \"l,\" l \"l\" l, \"l\" l, \"l,\" l, \"l,\" l \"l\" l, \"l,\" l \"l,\" l, \"l\" l \"l\" l, \"l\" l \"l,\" l, \"l,\" l \"l,\" l \"l,\" l, \"l\" l, \"l\" l \"l,\" l"}, {"heading": "D. Proof of Theorem 1 & other Bounds on the Loss", "text": "We define I = {1, 2, \u00b7 \u00b7, 2k}, the partition I = {I1, I2, I3} as I1 = {1,.., k \u2212 1}, I2 = {k \u2212 1,..), if k \u2264 i \u2212 1,. (i \u2212 k), if k \u2264 i \u2264 2k \u2212 1, 2d0 (or 2b0), if i = 2k, and Ji = {i \u2212 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7} if 1 \u2264 i \u2264 k \u2212 1, \"k \u2212 (i \u2212 k), if k \u2264 i \u2264 2k \u2212 1, 2d0 (or 2b0), if i \u2212 p,\" and Ji \u2212. \""}, {"heading": "E. Proof of Lemma 4", "text": "The proof of this problem is similar to the proof of theory 1 in Lazaric et al. (2010). Before we provide the proof, we report on the following two lemmas, which are shown in the Proof. Lemma 6. Then let us present a political space with a finite VC dimension h = V C (2009) < p and N the number of states in the rollout group Dk \u2212 p. Then we have a political space with a finite VC dimension h = V C (2009) < p and N the number of states in the rollout group Dk \u2212 p (2009). Then we have a protocol eNh \u2212 p \u2212 p \u2212 p Proof. This is a restitution of Lemma 1 in Lazaric et al. (2010).Lemma 7. Let us leave a political space with a finite VC dimension h (2011)."}, {"heading": "F. Proof of Lemma 5", "text": "Let us define two n-dimensional vectors: z = ((T\u03c0k) mvk \u2212 1) (s (1)),.., [T\u03c0k) mvk \u2212 1) (s (n)), and y = (v (v) k (s (1),., v (s) k (n)), and their orthogonal projections on the vector space Fn as z (from Vmax), and y = (v) k (s (1),., v) k (s (n)), and v) k (n), and their orthogonal projections on the vector space Fn as z (from Vmax), is vk = (v), k (v) k (see Figure 2)."}, {"heading": "G. Experimental Results", "text": "In this section we report on the empirical evaluation of the CBMPI and compare it with DPI and LSPI. In the experiments we show that the CBMPI can be improved by combining policy and value alignment with DPI and LSPI. In these experiments we use the same setting as in Gabillon et al. (2011) to facilitate the comparison."}, {"heading": "G.1. Setting", "text": "We look at the Mountaincar problem (MC) with its standard formulation, in which the action noise is limited to [\u2212 1, 1] and \u03b3 = 0.99. The value function is roughly calculated based on a linear space encompassed by a series of radial base functions (RBFs) that are evenly distributed across the state space. Each CBMPI-based algorithm is executed with the same fixed budget B per iteration. CBMPI splits the budget into a rollout budget BR = B (1 \u2212 p), which is used to create the training set of the greedy step, and a critical budget BC = Bp, which is used to build the training set of the evaluation step, where p (0, 1) splits the rollout budget into M rollouts of the length m for each action into A and each state in the rollout set D \u2032, i.e. BR = mMN | A. The critic budget is divided into one rollout for each action of the length."}, {"heading": "G.2. Experiments", "text": "As discussed in Remark 5, the parameter m balances between the error in evaluating the value function and the error in evaluating the policy. The value function approximation error tends to zero for large values of m. Although this would indicate that large values for m, the size of rollout sets would decrease accordingly, such as N = O (B / m) and n = O (B / m), thus increasing the accuracy of both regression and classification problems. This leads to a trade-off between long rollouts and the number of states in the rollout sets. The solution for this trade-off depends exclusively on the capacity of the value function space F. A rich value space would lead to resolve the trade-off for small values of m. On the other hand, if the value functional space is poor, or as in the DPI case, m should be selected in a way to guarantee a sufficient number of informative rollouts."}], "references": [{"title": "Fitted Qiteration in continuous action-space MDPs", "author": ["A. Antos", "R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Antos et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2007}, {"title": "approximate) iterated successive approximations algorithm for sequential decision processes", "author": ["Canbolat", "Pelin", "Rothblum", "Uriel"], "venue": "Annals of Operations Research, pp", "citeRegEx": "Canbolat et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Canbolat et al\\.", "year": 2012}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Error propagation for approximate policy and value iteration", "author": ["A. Farahmand", "R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Farahmand et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2010}, {"title": "Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes", "author": ["A. Fern", "S. Yoon", "R. Givan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fern et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fern et al\\.", "year": 2006}, {"title": "Classification-based policy iteration with a critic", "author": ["V. Gabillon", "A. Lazaric", "M. Ghavamzadeh", "B. Scherrer"], "venue": "In Proceedings of ICML,", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Reinforcement Learning as Classification: Leveraging Modern Classifiers", "author": ["M. Lagoudakis", "R. Parr"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Analysis of a Classification-based Policy Iteration Algorithm", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Lazaric et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lazaric et al\\.", "year": 2010}, {"title": "Error Bounds for Approximate Policy Iteration", "author": ["R. Munos"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Munos,? \\Q2003\\E", "shortCiteRegEx": "Munos", "year": 2003}, {"title": "Performance Bounds in Lp-norm for Approximate Value Iteration", "author": ["R. Munos"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Munos,? \\Q2007\\E", "shortCiteRegEx": "Munos", "year": 2007}, {"title": "Finite-Time Bounds for Fitted Value Iteration", "author": ["R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Munos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2008}, {"title": "Modified policy iteration algorithms for discounted Markov decision problems", "author": ["M. Puterman", "M. Shin"], "venue": "Management Science,", "citeRegEx": "Puterman and Shin,? \\Q1978\\E", "shortCiteRegEx": "Puterman and Shin", "year": 1978}, {"title": "Approximate Modified Policy Iteration", "author": ["Scherrer", "Bruno", "Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Geist", "Matthieu"], "venue": "Technical report,", "citeRegEx": "Scherrer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2012}, {"title": "Reinforcement Learning Algorithms for MDPs", "author": ["Szepesv\u00e1ri", "Cs"], "venue": "In Wiley Encyclopedia of Operations Research. Wiley,", "citeRegEx": "Szepesv\u00e1ri and Cs.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Cs.", "year": 2010}, {"title": "Performance bound for Approximate Optimistic Policy Iteration", "author": ["Thiery", "Christophe", "Scherrer", "Bruno"], "venue": "Technical report,", "citeRegEx": "Thiery et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thiery et al\\.", "year": 2010}, {"title": "Let \u03a0 be a policy space with finite VC-dimension h = V C(\u03a0) <\u221e and s", "author": ["Lazaric"], "venue": null, "citeRegEx": "Lazaric,? \\Q2010\\E", "shortCiteRegEx": "Lazaric", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "3) that generalize the AVI implementations of Ernst et al. (2005); Antos et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.", "startOffset": 8, "endOffset": 126}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al.", "startOffset": 8, "endOffset": 146}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al.", "startOffset": 8, "endOffset": 169}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al. (2011). We then provide an error propagation analysis of AMPI (Sec.", "startOffset": 8, "endOffset": 193}, {"referenceID": 2, "context": "Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1.", "startOffset": 43, "endOffset": 83}, {"referenceID": 0, "context": "Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1.", "startOffset": 43, "endOffset": 83}, {"referenceID": 4, "context": "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space \u03a0 (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).", "startOffset": 62, "endOffset": 151}, {"referenceID": 7, "context": "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space \u03a0 (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).", "startOffset": 62, "endOffset": 151}, {"referenceID": 5, "context": "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space \u03a0 (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).", "startOffset": 62, "endOffset": 151}, {"referenceID": 7, "context": "Note that when m tends to \u221e, we recover the DPI algorithm proposed and analyzed by Lazaric et al. (2010).", "startOffset": 83, "endOffset": 105}, {"referenceID": 9, "context": "VI analysis is based on the fact that this algorithm computes the fixed point of the Bellman optimality operator, and this operator is a \u03b3-contraction in max-norm (Bertsekas & Tsitsiklis, 1996; Munos, 2007).", "startOffset": 163, "endOffset": 206}, {"referenceID": 8, "context": "We also know that the analysis of PI usually relies on the fact that the sequence of the generated values is non-decreasing (Bertsekas & Tsitsiklis, 1996; Munos, 2003).", "startOffset": 124, "endOffset": 167}, {"referenceID": 12, "context": "1 and report most proofs in (Scherrer et al., 2012).", "startOffset": 28, "endOffset": 51}, {"referenceID": 12, "context": "1 and report most proofs in (Scherrer et al., 2012). We follow the line of analysis developped by Thiery & Scherrer (2010). The results are obtained using the following three quantities: 1) The distance between the optimal value function and the value before approximation at the k iteration: dk \u2206 = v\u2217 \u2212 (T\u03c0k)vk\u22121 = v\u2217 \u2212 (vk \u2212 k).", "startOffset": 29, "endOffset": 123}, {"referenceID": 7, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al.", "startOffset": 0, "endOffset": 46}, {"referenceID": 3, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP.", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients.", "startOffset": 70, "endOffset": 407}, {"referenceID": 3, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients. We will discuss the potential advantage of this new class in Remark 4. We will also show through the proofs of Thms. 1 and 3, how the result of Lemma 3 provides us with a flexible tool for turning point-wise bounds into Lp-norm bounds. Thm. 3 in (Scherrer et al., 2012, Appendix D) provides an alternative bound for the loss of AMPI, which in analogy with the results of Farahmand et al. (2010) shows that the last iterations have the highest impact on the loss (the influence exponentially decreases towards the initial iterations).", "startOffset": 70, "endOffset": 853}, {"referenceID": 8, "context": "15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).", "startOffset": 41, "endOffset": 54}, {"referenceID": 9, "context": "15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).", "startOffset": 63, "endOffset": 76}, {"referenceID": 7, "context": "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = \u221e and q\u2032 = 1 in Munos (2007); Munos & Szepesv\u00e1ri (2008), and q = q\u2032 = 2 in Farahmand et al.", "startOffset": 175, "endOffset": 188}, {"referenceID": 7, "context": "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = \u221e and q\u2032 = 1 in Munos (2007); Munos & Szepesv\u00e1ri (2008), and q = q\u2032 = 2 in Farahmand et al.", "startOffset": 175, "endOffset": 215}, {"referenceID": 3, "context": "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = \u221e and q\u2032 = 1 in Munos (2007); Munos & Szepesv\u00e1ri (2008), and q = q\u2032 = 2 in Farahmand et al. (2010). Remark 5.", "startOffset": 234, "endOffset": 258}, {"referenceID": 3, "context": "In analogy with the results of Farahmand et al. (2010), this new bound shows that the last iterations have the highest influence on the loss (the influence exponentially decreases towards the initial iterations).", "startOffset": 31, "endOffset": 55}, {"referenceID": 7, "context": "Proof of Lemma 4 The proof of this lemma is similar to the proof of Theorem 1 in Lazaric et al. (2010). Before stating the proof, we report the following two lemmas that are used in the proof.", "startOffset": 81, "endOffset": 103}, {"referenceID": 7, "context": "This is a restatement of Lemma 1 in Lazaric et al. (2010). Lemma 7.", "startOffset": 36, "endOffset": 58}, {"referenceID": 5, "context": "In these experiments, we are using the same setting as in Gabillon et al. (2011) to facilitate the comparison.", "startOffset": 58, "endOffset": 81}], "year": 2012, "abstractText": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last classification-based implementation, we develop a finite-sample analysis that shows that MPI\u2019s main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.", "creator": "LaTeX with hyperref package"}}}