{"id": "1205.3109", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search", "abstract": "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty. In this setting, a Bayes-optimal policy captures the ideal trade-off between exploration and exploitation. Unfortunately, finding Bayes-optimal policies is notoriously taxing due to the enormous search space in the augmented belief-state MDP. In this paper we exploit recent advances in sample-based planning, based on Monte-Carlo tree search, to introduce a tractable method for approximate Bayes-optimal planning. Unlike prior work in this area, we avoid expensive applications of Bayes rule within the search tree, by lazily sampling models from the current beliefs. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems.", "histories": [["v1", "Mon, 14 May 2012 17:20:29 GMT  (965kb,D)", "http://arxiv.org/abs/1205.3109v1", null], ["v2", "Sat, 13 Oct 2012 15:19:09 GMT  (846kb,D)", "http://arxiv.org/abs/1205.3109v2", "14 pages, 7 figures, includes supplementary material. Advances in Neural Information Processing Systems (NIPS) 2012"], ["v3", "Thu, 3 Jan 2013 14:44:59 GMT  (846kb,D)", "http://arxiv.org/abs/1205.3109v3", "14 pages, 7 figures, includes supplementary material. Advances in Neural Information Processing Systems (NIPS) 2012"], ["v4", "Wed, 18 Dec 2013 11:45:49 GMT  (846kb,D)", "http://arxiv.org/abs/1205.3109v4", "14 pages, 7 figures, includes supplementary material. Advances in Neural Information Processing Systems (NIPS) 2012"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["arthur guez", "david silver", "peter dayan"], "accepted": true, "id": "1205.3109"}, "pdf": {"name": "1205.3109.pdf", "metadata": {"source": "CRF", "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search", "authors": ["Arthur Guez"], "emails": ["aguez@gatsby.ucl.ac.uk", "d.silver@cs.ucl.ac.uk", "dayan@gatsby.ucl.ac.uk"], "sections": [{"heading": null, "text": "In this context, a Bayes-optimized policy captures the ideal trade-off between exploration and exploitation. Unfortunately, finding Bayes-optimized strategies is notorious due to the vast search space in the expanded faith state of MDP. In this paper, we use recent advances in sample-based planning based on the Monte Carlo tree search to introduce a comprehensible method for approximate Bayes-optimized planning. Unlike previous work in this area, we avoid costly applications of the Bayes rule within the search tree by needlessly sampling models from current beliefs. Our approach has clearly outperformed previous Bayes-based RL algorithms on several known benchmark problems."}, {"heading": "1 Introduction", "text": "An important goal in the theory of Markov decision-making processes (MDPs) is to maximize the expected sum of discounted rewards when the dynamics of the MDP are (perhaps partially) unknown. A discount factor pushes the agent to favor short-term rewards, but potentially costly explorations are necessary to figure out how to get such rewards. This conflict leads to the known exploration processes for what he knows about the dynamics. It was early recognized that a solution to the resulting partially observable MDP is able to extend the regular state of the agent in the world."}, {"heading": "2 Bayesian RL", "text": "We describe the generic Bayesian formulation of optimal decision-making in an unknown MDP = > DP according to [Martin, 1967] and [Duff, 2002]. An MDP is described as 5x M = < S, A, P, R, \u03b3 >, where S is the set of states, A is the set of actions, P: S \u00b7 A \u00b7 S \u2192 R is the state transition predictor, R: S \u00b7 A \u00b7 R is a limited reward function, and \u03b3 is the discount factor [Szepesva \u0301 ri, 2010]. If all components of the MDP tupel are known, standard MDP planning algorithms can be used to calculate the optimal value function and political action outside the line. Generally, the dynamics are unknown, and we assume that P is a latent variable distributed according to a distribution P (P). After observing a history of actions and transitions, s1a1s2a2 a2 a2."}, {"heading": "3 The BMCP algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Algorithm Description", "text": "The goal of a BAMDP planning algorithm is to meet for each decision point < < s, h > Q > action a, which maximizes Equation 2. < s, Bayes-adaptive Monte-Carlo Planning (BMCP), does this by conducting a forward search in the space of possible future developments of the BAMDP using Monte Carlo Tree Search. We use the UCT algorithm [Kocsis and Szepesva \"ri,\" 2006] to distribute search efforts among promising branches of the state action tree, and use sample-based rollouts to provide estimates at each node. Sample-based search in the BAMDP would normally require the generation of samples from P + at each node. This operation requires integration across all possible transition models or at least a sample of a transition model P - an expensive one for all but the simplest generative models (P)."}, {"heading": "3.2 Rollout Policy", "text": "Choosing the rollout policy \u03c0ro is important when there are few simulations, especially when the domain does not have a significant location or when rewards require a carefully selected sequence of actions. & < < < < < < < Q < < Q < Q < Q < Q < Q (B) requires a carefully selected sequence of actions to be taken. < M (B) we learn Qro, the optimal Q value in the real MDP, in a model-free way (e.g. through Q-Learning) from samples (st, at, rt, st + 1) obtained as a result of the Bayesian agent's interaction with the non-policy environment."}, {"heading": "3.3 Lazy Sampling", "text": "In previous work on sample-based tree search, including POMCP [Silver and Veness, 2010], a complete sampling state is taken from the back of the tree, but this can be very costly in arithmetical terms. Instead, we try P inertly, creating only the specific transition probabilities required as the simulation traverses the tree, and also during rollout.If P (s, a, \u00b7) is parameterized by a latent variable size, one for each state and action pair that potentially depends on each other, and on an additional set of latent variables \u03c6, then the back part can be written as P."}, {"heading": "3.4 Theoretical properties", "text": "Define V (< s, h >) = max a \u0394A Q (< s, h >, a). < s, h >, h > H.Theorem 1. BMCP constructs for all > 0 and a suitably selected c (e.g. c > Rmax1 \u2212 \u03b3), from state < s, ht >, a value function likely to converge to an -optimal value function in the finite search horizon of BAMDP. Furthermore, for sufficiently large N (< s, h >) the inclination of V (< s, h >) decreases for all states < s, h >, as O (log (N (< s, h >) / N (< s, h >)."}, {"heading": "4 Related Work", "text": "In fact, it is not that we provide a comprehensive list of planning algorithms for MDP exploration; rather, we focus on related random algorithms for BDP exploration. BDP algorithms generalize this idea by applying it to the transition phase."}, {"heading": "5 Experiments", "text": "We present empirical results of BMCP on a number of standard problems and compare the results with other popular algorithms."}, {"heading": "5.1 Algorithms", "text": "The following algorithms were executed: \u2022 BMCP - The algorithm presented in section 3, implemented with lazy sampling. The algorithm was executed for different number of simulations (10 to 10000) to cover different scheduling times. In all experiments we determined that it should be a -greedy policy with = 0.5. The UCT exploration constant was left unchanged for all experiments (c = 3), we experimented with other values of c = 0.5, 1, 5} with similar results. \u2022 SBOSS - The BOSS variant [Asmuth et al., 2009] by [Castro and Precup, 2010] with an adaptive resampling criterion. For each domain we varied the number of samples K = 2, 4, 8, 16, 32} and the resampling threshold of ha {3, 5, 7}. \u2022 BEB - The internal reward algorithm in [Kolter and Ng, 2009], 15, 15, 5, 5, 5, domain, 5, we varied, 5, 1, 1, 1, 1, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15"}, {"heading": "5.2 Domains", "text": "In fact, most of us are able to play by the rules that they have imposed on themselves and are able to play by the rules that they have imposed on themselves."}, {"heading": "6 Future Work", "text": "The UCT algorithm is known to have several drawbacks. Firstly, there are no end-time limits to regret. It is possible to construct malicious environments, for example, where the optimal policy is hidden in a generally low reward region of the tree where UCT can be misled over long periods of time [Coquelin and Munos, 2007]. Secondly, the UCT algorithm treats each action node as a multi-armed bandit problem. However, there is no real benefit if rewards accrue during planning, and therefore it is theoretically more appropriate to use pure exploration bandits [Bubeck et al., 2009]. Nevertheless, the UCT algorithm has produced excellent empirical performance in many areas. We have focused on researching the dynamics of a fully observable MDP. If the reward function is also unknown, then BMCP could be expanded to maintain beliefs about both transition dynamics and then maintain both dynamics."}, {"heading": "7 Conclusion", "text": "We have proposed a random sample-based algorithm for Bayes RL called BMCP that significantly exceeds the performance of existing algorithms for several standard tasks, the main idea being to use Monte Carlo tree search to efficiently explore the extended Bayes adaptive search space. Furthermore, BMCP expands previous work on Monte Carlo tree search (e.g. [Silver and Veness, 2010]): To use a model-free learning algorithm for amplification, to learn an adaptive rollout policy, and to test subsequent beliefs cost-effectively using a rotten sampling scheme. Unlike other random sample-based algorithms for Bayes RL [Wang et al., 2005, Asmuth and Littman, 2011] BMCP is compatible and rather traceable: it only requires belief sets to be sampled at the beginning of each simulation, making rotten sampling considerably more economical."}], "references": [{"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["Asmuth et al", "J. 2009] Asmuth", "L. Li", "M. Littman", "A. Nouri", "D. Wingate"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Approaching Bayes-optimality using MonteCarlo tree search", "author": ["Asmuth", "Littman", "J. 2011] Asmuth", "M. Littman"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Asmuth et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2011}, {"title": "On adaptive control processes", "author": ["Bellman", "Kalaba", "R. 1959] Bellman", "R. Kalaba"], "venue": "Automatic Control, IRE Transactions", "citeRegEx": "Bellman et al\\.,? \\Q1959\\E", "shortCiteRegEx": "Bellman et al\\.", "year": 1959}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["Bubeck et al", "S. 2009] Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Proceedings of the 20th international conference on Algorithmic learning theory,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Smarter sampling in model-based Bayesian reinforcement learning", "author": ["Castro", "Precup", "P. 2010] Castro", "D. Precup"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Castro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2010}, {"title": "Bandit algorithms for tree search", "author": ["Coquelin", "Munos", "P. 2007] Coquelin", "R. Munos"], "venue": "Arxiv preprint cs/0703062", "citeRegEx": "Coquelin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Coquelin et al\\.", "year": 2007}, {"title": "Bayesian Q-learning", "author": ["Dearden et al", "R. 1998] Dearden", "N. Friedman", "S. Russell"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "A", "author": ["Feldbaum"], "venue": "(1960). Dual control theory. Automation and Remote Control, 21(9):874\u2013", "citeRegEx": "Feldbaum. 1960", "shortCiteRegEx": null, "year": 1039}, {"title": "Efficient Bayesian parameter estimation in large discrete domains. Advances in neural information processing systems, pages 417\u2013423", "author": ["Friedman", "Singer", "N. 1999] Friedman", "Y. Singer"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1999}, {"title": "Combining online and offline knowledge in UCT", "author": ["Gelly", "Silver", "S. 2007] Gelly", "D. Silver"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Gelly et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gelly et al\\.", "year": 2007}, {"title": "Multi-armed bandit allocation indices. Wiley Online Library", "author": ["Gittins et al", "J. 1989] Gittins", "R. Weber", "K. Glazebrook"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al. et al\\.", "year": 1989}, {"title": "A sparse sampling algorithm for nearoptimal planning in large Markov decision processes", "author": ["Kearns et al", "M. 1999] Kearns", "Y. Mansour", "A. Ng"], "venue": "In Proceedings of the 16th international joint conference on Artificial intelligence-Volume", "citeRegEx": "al. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "al. et al\\.", "year": 1999}, {"title": "Bandit based Monte-Carlo planning", "author": ["Kocsis", "Szepesv\u00e1ri", "L. 2006] Kocsis", "C. Szepesv\u00e1ri"], "venue": "Machine Learning: ECML", "citeRegEx": "Kocsis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis et al\\.", "year": 2006}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["Kolter", "Ng", "J. 2009] Kolter", "A. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Exploration of multi-state environments: Local measures and back-propagation of uncertainty", "author": ["Meuleau", "Bourgine", "N. 1999] Meuleau", "P. Bourgine"], "venue": "Machine Learning,", "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "A Bayesian approach for learning and planning in Partially Observable Markov Decision Processes", "author": ["Ross et al", "S. 2011] Ross", "J. Pineau", "B. Chaib-draa", "P. Kreitmann"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Monte-Carlo planning in large POMDPs", "author": ["Silver", "Veness", "D. 2010] Silver", "J. Veness"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Silver et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2010}, {"title": "Variance-based rewards for approximate Bayesian reinforcement learning", "author": ["Sorg et al", "J. 2010] Sorg", "S. Singh", "R. Lewis"], "venue": "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Integrating sample-based planning and model-based reinforcement learning", "author": ["Walsh et al", "T. 2010] Walsh", "S. Goschin", "M. Littman"], "venue": "In Proceedings of the Twenty-Fourth Conference on Artificial Intelligence (AAAI)", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["Wang et al", "T. 2005] Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}], "referenceMentions": [], "year": 2017, "abstractText": "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty. In this setting, a Bayesoptimal policy captures the ideal trade-off between exploration and exploitation. Unfortunately, finding Bayes-optimal policies is notoriously taxing due to the enormous search space in the augmented belief-state MDP. In this paper we exploit recent advances in sample-based planning, based on Monte-Carlo tree search, to introduce a tractable method for approximate Bayes-optimal planning. Unlike prior work in this area, we avoid expensive applications of Bayes rule within the search tree, by lazily sampling models from the current beliefs. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems.", "creator": "LaTeX with hyperref package"}}}