{"id": "1205.3181", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "Multiple Identifications in Multi-Armed Bandits", "abstract": "We study the problem of identifying the top $m$ arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem.", "histories": [["v1", "Mon, 14 May 2012 20:10:04 GMT  (173kb,D)", "http://arxiv.org/abs/1205.3181v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["s\u00e9bastien bubeck", "tengyao wang", "nitin viswanathan"], "accepted": true, "id": "1205.3181"}, "pdf": {"name": "1205.3181.pdf", "metadata": {"source": "CRF", "title": "Multiple Identifications in Multi-Armed Bandits", "authors": ["S\u00e9bastien Bubeck"], "emails": ["sbubeck@princeton.edu", "tengyaow@princeton.edu", "nviswana@princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "rE \"s tis rf\u00fc ide rf\u00fc nlrf\u00fc,\" eeitS os, rf\u00fc ide nlrf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "2 Problem setup", "text": "We adopt the terminology of the multi-armed bandits of the past. The agent faces K-weapons and he has a budget of n-evaluations (or draws). For each arm i (1,.) there is an associated probability distribution problem. < < m) However, these distributions are unknown to the agent. < m) The sequential evaluation protocol goes like this: at each round t = 1,. \u2212 m, the agent selects an arm and observes 1In the m-best arms identification problem we write un = O (vn), if un = O (vn), if un = O (vn), up to logarithmic factor in K 2In the multi-armed bandit identification problem we write un = O (vn) up to logarithmic factors in MK 3One we can generalize the discussion directly."}, {"heading": "4 Multi-bandit best arm identification", "text": "In this section, we use the idea of SAR to identify the best arm in the multibandit. In addition, at the end of each phase, we estimate the gaps i (m) within each problem and reject the arm with the largest estimated gap. If a problem has only one active arm, then that arm is accepted and the problem is deactivated. The corresponding strategy is precisely described in Figure 2Theorem 2. The probability of an error of the SAR in the multibandit identification problem is satisfactory. 2M2K2 Exp (\u2212 n \u2212 MK8log (MK) H [M] 2).Proof Consider the event defined by defining the individual arms (1 \u2264 i \u2264 K, 1 \u2264 m \u2264 M, 1 \u2264 MK \u2212 1 \u2264 m \u2264 M2K2 Exp (\u2212 n \u2212 K2 Exp (MK8log (MK) H [M] 2)."}, {"heading": "5 Experiments", "text": "In this section, we look at the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our goal is simply to illustrate our theoretical analysis, we focus on the m-best arms identification problem, but similar numerical simulations could be performed in the multi-bandit environment and with the results of Gabillon et al. [2011]. We compare our proposed strategy SAR with three competitors: the uniform sampling strategy, which equally splits the allocation budget n between the K arms, and then return the m arms with the highest empirical means (see Bubeck et al. [2011] for a discussion of this strategy in the Single Best Arm Identification). The SR strategy is the simple rejection strategy of Audibert et al. [2010], designed to find the (single) best arm."}], "references": [{"title": "Best arm identification in multi-armed bandits", "author": ["J.-Y. Audibert", "S. Bubeck", "R. Munos"], "venue": "In Proceedings of the 23rd Annual Conference on Learning Theory (COLT),", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "A single-sample multiple decision procedure for ranking means of normal populations with known variances", "author": ["R.E. Bechhofer"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Bechhofer.,? \\Q1954\\E", "shortCiteRegEx": "Bechhofer.", "year": 1954}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Pure exploration in finitely-armed and continuously-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Theoretical Computer Science,", "citeRegEx": "Bubeck et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2011}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the Fifteenth Annual Conference on Computational Learning Theory (COLT),", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "This setting was introduced in Bubeck et al. [2009], where the goal was to identify the distribution with maximal mean.", "startOffset": 31, "endOffset": 52}, {"referenceID": 0, "context": "possible formulation is the one of the PAC model studied in Even-Dar et al. [2002], Mannor and Tsitsiklis [2004] where there is an accuracy of \u03b5 and a probability of correctness \u03b4 that are prespecified, and one wants to minimize the number of evaluations to attain this prespecified accuracy and probability of correctness.", "startOffset": 60, "endOffset": 83}, {"referenceID": 0, "context": "possible formulation is the one of the PAC model studied in Even-Dar et al. [2002], Mannor and Tsitsiklis [2004] where there is an accuracy of \u03b5 and a probability of correctness \u03b4 that are prespecified, and one wants to minimize the number of evaluations to attain this prespecified accuracy and probability of correctness.", "startOffset": 60, "endOffset": 113}, {"referenceID": 0, "context": "This latter formulation has a long history which goes back to the seminal work Bechhofer [1954]. In this paper we focus on the fixed budget setting of Bubeck et al.", "startOffset": 79, "endOffset": 96}, {"referenceID": 0, "context": "This latter formulation has a long history which goes back to the seminal work Bechhofer [1954]. In this paper we focus on the fixed budget setting of Bubeck et al. [2009]. For this fixed budget problem, Audibert et al.", "startOffset": 79, "endOffset": 172}, {"referenceID": 0, "context": "For this fixed budget problem, Audibert et al. [2010] proposed a new analysis and an optimal algorithm (up to a logarithmic factor).", "startOffset": 31, "endOffset": 54}, {"referenceID": 0, "context": "However, as it was argued in Audibert et al. [2010], for a first order analysis it is enough to focus on the quantity en.", "startOffset": 29, "endOffset": 52}, {"referenceID": 0, "context": "In the (single) best arm identification, Audibert et al. [2010] introduced the following complexity measures.", "startOffset": 41, "endOffset": 64}, {"referenceID": 0, "context": "It is easy to see that these two complexity measures are equivalent up to a logarithmic factor since we have (see Audibert et al. [2010]) H2 \u2264 H1 \u2264 log(2K)H2.", "startOffset": 114, "endOffset": 137}, {"referenceID": 0, "context": "[Theorem 4, Audibert et al. [2010]] shows that the complexity H1 represents the hardness of the best arm identification problem.", "startOffset": 12, "endOffset": 35}, {"referenceID": 0, "context": "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H \u3008m\u3009 1 holds true for the m-best arms identification problem.", "startOffset": 56, "endOffset": 79}, {"referenceID": 0, "context": "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H [M ] 1 holds true for the multi-bandit best arm identification problem.", "startOffset": 56, "endOffset": 79}, {"referenceID": 0, "context": "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H [M ] 1 holds true for the multi-bandit best arm identification problem. In this paper we shall prove an upper bound on en that gets small when n = \u00d5 ( H [M ] 2 ) (recall that by (1), \u00d5 ( H [M ] 2 ) = \u00d5 ( H [M ] 1 ) ). This result, derived in Section 4, builds upon the SAR strategy introduced in Section 3. The improvement with respect to Gabillon et al. [2011] is that our strategy is parameter-free, while the theoretical Gap-E introduced in Gabillon et al.", "startOffset": 56, "endOffset": 464}, {"referenceID": 0, "context": "We conjecture that a similar lower bound to [Theorem 4, Audibert et al. [2010]] with H1 replaced by H [M ] 1 holds true for the multi-bandit best arm identification problem. In this paper we shall prove an upper bound on en that gets small when n = \u00d5 ( H [M ] 2 ) (recall that by (1), \u00d5 ( H [M ] 2 ) = \u00d5 ( H [M ] 1 ) ). This result, derived in Section 4, builds upon the SAR strategy introduced in Section 3. The improvement with respect to Gabillon et al. [2011] is that our strategy is parameter-free, while the theoretical Gap-E introduced in Gabillon et al. [2011] requires the knowledge of H [M ] 1 to tune its parameter.", "startOffset": 56, "endOffset": 569}, {"referenceID": 0, "context": "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications.", "startOffset": 53, "endOffset": 76}, {"referenceID": 0, "context": "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al.", "startOffset": 53, "endOffset": 376}, {"referenceID": 0, "context": "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification).", "startOffset": 53, "endOffset": 615}, {"referenceID": 0, "context": "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strategy of Audibert et al. [2010] which was designed to find the (single) best arm.", "startOffset": 53, "endOffset": 772}, {"referenceID": 0, "context": "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strategy of Audibert et al. [2010] which was designed to find the (single) best arm. We slightly improve it for m-best identification by running only K \u2212 m \u2212 1 phases (while still using the full budget n) and then returning the last m surviving arms. Finally we consider the extension of UCB-E to the m-best arms identification problem, which is based on a similar idea than the extension Gap-E of Gabillon et al. [2011] for the multi-bandit best arm identification, see Figure 3 for the details.", "startOffset": 53, "endOffset": 1158}, {"referenceID": 0, "context": "In this section we revisit the simple experiments of Audibert et al. [2010] in the setting of multiple identifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m-best arms identification problem, but similar numerical simulations could be conducted in the multi-bandit setting and compared to the results of Gabillon et al. [2011]. We compare our proposed strategy SAR to three competitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empirical mean (see Bubeck et al. [2011] for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strategy of Audibert et al. [2010] which was designed to find the (single) best arm. We slightly improve it for m-best identification by running only K \u2212 m \u2212 1 phases (while still using the full budget n) and then returning the last m surviving arms. Finally we consider the extension of UCB-E to the m-best arms identification problem, which is based on a similar idea than the extension Gap-E of Gabillon et al. [2011] for the multi-bandit best arm identification, see Figure 3 for the details. Note that this last algorithm requires to know the complexity H 1 . One could propose an adaptive version, using ideas described in Audibert et al. [2010], but for sake of simplicity we restrict our attention to the non-adaptive algorithm.", "startOffset": 53, "endOffset": 1389}], "year": 2012, "abstractText": "We study the problem of identifying the top m arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem.", "creator": "LaTeX with hyperref package"}}}