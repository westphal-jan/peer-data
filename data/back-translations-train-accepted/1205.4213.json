{"id": "1205.4213", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2012", "title": "Online Structured Prediction via Coactive Learning", "abstract": "We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. At each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking). The user responds by correcting the system if necessary, providing a slightly improved -- but not necessarily optimal -- object as feedback. We argue that such feedback can often be inferred from observable user behavior, for example, from clicks in web-search. Evaluating predictions by their cardinal utility to the user, we propose efficient learning algorithms that have ${\\cal O}(\\frac{1}{\\sqrt{T}})$ average regret, even though the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search.", "histories": [["v1", "Fri, 18 May 2012 18:19:13 GMT  (93kb,S)", "http://arxiv.org/abs/1205.4213v1", null], ["v2", "Wed, 27 Jun 2012 16:25:02 GMT  (57kb)", "http://arxiv.org/abs/1205.4213v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IR", "authors": ["pannaga shivaswamy", "thorsten joachims"], "accepted": true, "id": "1205.4213"}, "pdf": {"name": "1205.4213.pdf", "metadata": {"source": "CRF", "title": "Online Structured Prediction via Coactive Learning", "authors": ["Pannaga Shivaswamy"], "emails": ["pannaga@cs.cornell.edu", "tj@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.42 13v1 [cs.LG] 1 8M ay2 01T) average regret, even if the learning algorithm never complies with cardinal benefit values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms to a film recommendation task as well as the ranking for web search."}, {"heading": "1. Introduction", "text": "In a wide range of systems used today, human-system interaction takes the following form of feedback, but the user gives a command (e.g. query) and receives a - possibly structured - result in response (e.g. ranking), and the user then interacts with the results (e.g. clicks), giving implicit feedback on the user's usability. Here are three examples of such systems and their typical patterns of interaction: Web Search: In response to a query, a search engine presents the ranking [A, B, C, D,...] and notes that the user clicks on documents B and D. Appears in proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner (s).Movie Recommendation: An online service recommends movie A for a user."}, {"heading": "2. Related Work", "text": "The model of coactive learning bridges the gap between two forms of feedback that have been well studied in online learning: on the one hand, the multi-armed bandit model (Auer et al., 2002b; a), in which an algorithm selects an action and observes the usefulness (only) of that action; on the other, the usefulness of all possible actions is revealed when learning with expert advice (Cesa-Bianchi & Lugosi, 2006); on-line convex optimization (Zinkevich, 2003) and on-line convex optimization in the bandit environment (Flaxman et al., 2005) are continuous relaxations of the expert or bandit problems; our model, in which information about two arms is revealed at each iteration, lies between the expert and the bandit environment; the dueling bandit environment is most closely related to coactive learning (Yue et al., 2009; Ys & imue, 2009)."}, {"heading": "3. Coactive Learning Model", "text": "We now introduce coactive learning as a model of interaction (in rounds) between a learning system (e.g. search engine) and a human (e.g. search user), in which both the human and the learning algorithm pursue the same goal (to achieve good results).In each round, the learning algorithm will observe a context in which the user's behavior (e.g. a search query) is unknown and represents a structured object (e.g. a ranking of URLs).The use of yt-Y for the user for the context is described by a utility function (xt, yt), which is unknown to the learning algorithm. As feedback, the user returns an improved object (e.g. a reordered list of URLs), i.e. U (xt, y) > U (xt, yt), if such an object exists."}, {"heading": "3.1. Quantifying Preference Feedback Quality", "text": "In order to give any theoretical guarantees about the regret of a learning algorithm in the coactive environment, we must quantify the quality of user feedback. Note that this quantification is a tool for theoretical analysis, not a prerequisite or parameter for the algorithm. We quantify the feedback quality by how much improvement y offers in the usable space. In the simplest case, we say that user feedback is strictly \u03b1-informative if the following inequality is met: U (xt, y, t) \u2212 U (xt, yt) \u2265 \u03b1 (U (xt, y) \u2212 U (xt, yt)). (4) In the above inequality \u03b1 (0, 1) is an unknown parameter. Feedback is such that the usable value y-t is higher than the usable value yt by a fraction \u03b1 of the maximum usable area U (xt, y, y, t)."}, {"heading": "3.2. User Study: Preferences from Clicks", "text": "We now confirm that reliable preferences, as indicated in Equation (1), can indeed be derived from the overall behavior of users. (1) In particular, we focus on preference feedback from clicks in web search and draw on data from a user study. (1) In their study, we do not explicitly show this dependence on whether the subjects (undergraduate students, n = 16) are actually dependent on the results of the Google search engine. (1) All queries, result lists and clicks were recorded. (2) On average, each query chain contained 2.2 queries and 1.8 clicks in the result lists. We use the following strategy to derive a ranking from the clicks of the user."}, {"heading": "4. Coactive Learning Algorithms", "text": "In this section, we present algorithms for minimizing regret in the coactive learning model. In the rest of this work, we will use a linear model for the utility function, U (x, y) = w (x, y) = w (x, y), (7) where both x and y are an unknown parameter vector and why we present and analyze most basic algorithms for the coactive learning model, which we call the preference perceptron (algorithm 1). The preference perceptron maintains a weight vector wt, which is initialized to 0. At all times, the algorithm observes the context and presents an object, the w (xt, y)."}, {"heading": "4.1. Lower Bound", "text": "We now show that the upper limit in Theorem 1 generally cannot be improved. Lemma 2 For each coactive learning algorithm A with linear utility there are xt, objects Y and w *, so that REGT from A is equivalent to T-steps. Proof Let us consider a problem where Y = {\u2212 1, + 1}, X = {x-RT: preservation x = 1}. Let us define the common characteristic map as \u03c6 (x, y) = yx. Let us consider T-contexts e1,..., eT such that only the jth component is equal and all others equal zero. Let us leave y1,... yT the sequence of outputs from A to contexts e1,... eT. Let us construct w = [\u2212 y1 / \u221a T \u2212 y2 / \u2012 T \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 yT], for this construction we have the sequence from A to contexts e1,."}, {"heading": "4.2. Batch Update", "text": "In some applications, due to the high feedback volume, it may not be possible to perform an update after each round. In such scenarios, it is natural to consider a variant of algorithm 1 that updates each k iteration; the algorithm simply uses wt gained from the previous update until the next update. It is easy to show the following regret that is required for batch updates: REGT \u2264 1\u03b1TT \u0445t = 1\u0442t + 2R \u0445 \u0445 \u0445\u043d\u043d\u043d\u0438\u043d\u0438\u043d\u0438\u043c\u043e\u0441\u0442\u043e k\u0430 \u0432 T."}, {"heading": "4.3. Expected \u03b1-Informative Feedback", "text": "So far we have characterized the user behaviour with regard to deterministic feedback actions. However, if a restriction on the expected regrets is sufficient, the weaker model of the expected \u03b1-informative feedbacks from Equation (6) can be applied as follows to the upper limit: E [REGT] \u2264 1\u03b1TT t = 1\u0445 T + 2R VESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTESTEST"}, {"heading": "4.4. Convex Loss Minimization", "text": "We assume that at any point in time there is an (unknown) convexe loss function which causes the loss (U (xt, yt) \u2212 u (xt, yt) \u2212 u (xt, yt) \u2212 u (xt, y) \u2212 u (n) (xt, y) \u2212 u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) u (n) n (n) u (n) u (n) u (n) u (n) u (n) u (n) n (n) n n n (n) n n n n (n) n n (n) n n n (n) n n (n) n n (n) n n n n (n) n n n n n n (n) n n n n n n n (n) n n n n n n (n) n n n n (n) n n n n n n n n (n) n n n n n (n) n (n) n n n n n n n (n) n n n (n) n (n) n (n) n n n n) n (n n (n) n (n) n n) n n n (n) n n n (n) n n (n) n) n (n n) n n (n n n) n n (n) n (n) n (n) n) n n (n) n n (n) n (n) n (n) n n (n) n) n n n (n) n n (n) n) n n (n (n) n n n (n) n n (n) n) n n (n) n) n (n (n) n (n) n) n (n) n (n) n n) n (n n) n n (n) n (n) n) n n (n) n (n (n) n) n (n (n) n (n) n (n) n (n) n) n n (n) n (n) n n (n) n) n (n"}, {"heading": "5. Experiments", "text": "In this section, we evaluate the Preference Perceptron algorithm empirically on two real datasets, the two experiments differ in terms of prediction and feedback. In the first experiment, the algorithm works with structured objects (rankings), in the second experiment, atomic items (films) are presented and received as feedback."}, {"heading": "5.1. Structured Feedback: Learning to Rank", "text": "We have evaluated our Preference Perceptron algorithm based on the Yahoo! Learning to Rank Dataset (Chapelle & Chang, 2011), which consists of query-url feature vectors (referred to as xqi for query q and URL i), each of which has a relevance rating rqi ranging from zero (irrelevant) to four (completely relevant). In order to present the ranking as a structured prediction problem, we have defined our common feature map as: w'p (q, y) = 5 x i = 1w xq yilog (i + 1). (14) In the above equation, y refers to a ranking in such a way that yi is the index of the URL placed at position i in the ranking. Therefore, the above measure considers the five best URLs for a query q and compiles a score based on a graduated relevance. (Note that the above-mentioned utility is not evaluated by Manning, for example, see the very efficient Dqp feature @)."}, {"heading": "5.1.1. Strong Vs Weak Feedback", "text": "The aim of the first experiment was to see how the regret of the algorithm changes with the feedback quality. In order to get feedback at different quality levels \u03b1, we used the following mechanism. Given the predicted ranking yt, the user would move the list down until he found five URLs, so that the resulting y did not meet the strictly \u03b1-informative feedback condition w.r.t. Figure 2 shows the results of this experiment for two different \u03b1 values. As expected, the regret with \u03b1 = 1.0 is lower than the regret with respect to \u03b1 = 0.1. Note, however, that the difference between the two curves is much smaller than a factor of 10. This is because strictly \u03b1-informative feedback is also strictly \u03b2-informative feedback for each \u03b2 \u2264 \u03b1. Thus, there could be several cases where the user feedback was much stronger than required."}, {"heading": "5.1.2. Noisy Feedback", "text": "In the previous experiment, user feedback was based on actual usage values calculated from the optimal w \u043a. Next, we use the actual relevance tags provided in the data set for user feedback. Now, given a ranking for a query, the user would go down the list and inspect the top 10 URLs (or all URLs if the list is shorter) as before. Five URLs with the highest relevance tags (rqi) are placed at the top five locations in user feedback. Note that this produces noisy feedback, as no linear can perfectly match the relevance tags on this data set. As a baseline, we repeatedly trained a conventional SVM4 ranking. In each iteration, the previous SVM model was used to provide the user with a ranking."}, {"heading": "5.2. Item Feedback: Movie Recommendation", "text": "In contrast to the structured prediction problem in the previous section, we now rate the Preference Perceptron based on a task with atomic predictions, namely recommending movies. In each iteration, a movie is presented to the user, and the feedback also consists of a movie. We use the MovieLens dataset, which consists of one million reviews of 3,090 movies rated by 6,040 users. Film ratings ranged from one to five.We randomly divided users into two sets of equal size. The first set was used to create a feature vector mj for5The error bars are extremely tiny at higher iterations. Each movie j uses the \"SVD embedding method\" for collaborative filtering (see Bell & Koren, 2007), Eqn. (15). The dimensions of the spring vectors and the regulation parameters were selected to optimize the cross-validation accuracy of the first dataset relative to an error."}, {"heading": "5.2.1. Strong Vs Weak Feedback", "text": "Analogous to the web search experiments, we first examine how the performance of the Preference Perceptron changes with the feedback quality \u03b1. In particular, we recommended a film with maximum usefulness according to the current weight value of the algorithm, and the user returns as feedback a film with the lowest usefulness, which still strictly satisfies \u03b1-informative feedback according to wi \u043a. For each user in the second sentence, the algorithm iteratively recommended 1500 films in this way. Repentance was calculated after each iteration and separately for each user, and all remorse was averaged over all users in the second sentence. Figure 4 shows the results for this experiment. Since the feedback in this case is strictly \u03b1-informative, the average regret in all cases is expected to fall towards zero. Note that even with a moderate value of \u03b1, regret is significantly reduced after just 10 equipment."}, {"heading": "5.2.2. Noisy Feedback", "text": "s feedback does not necessarily match the linear usage pattern used by the algorithm. Specifically, feedback is now given on the basis of actual ratings, if available, or the score u i \u0445 mj rounded to the closest possible rating value. In each iteration, the user returned a film whose rating was one rating higher than the one presented to him. If the algorithm had already presented a film with the highest rating, it was assumed that the user would use the same film as feedback. As a starting point, we re-ranked SVM. As in the web search experiment, it was retrained every time 10% more training data was added. Figure 5 shows the results of this experiment. Again, the regret of the Preference Perceptron is significantly lower than that of the SVM and at a small fraction of the computing cost."}, {"heading": "6. Conclusions", "text": "We proposed a new model of online learning in which preference feedback is observed but cardinal feedback is never observed. We proposed a suitable term of regret and showed that it can be minimized within our feedback model. We also provided several enhancements to the model and algorithms. Furthermore, experiments demonstrated its effectiveness for ranking web search and a film recommendation task. A future direction is the consideration of \u03bb-strongly convex functions, and we suspect that in this case it is possible to derive algorithms with O (log (T) / T) repentance. Thanks to Peter Frazier, Bobby Kleinberg, Karthik Raman and Yisong Yue for helpful discussions. This work was partly funded within the framework of the NSF awards IIS-0905467 and IIS-1142251."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The non-stochastic multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Predicting Structured Data", "author": ["G.H. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar", "Vishwanathan", "S.V.N. (eds"], "venue": null, "citeRegEx": "Bakir et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bakir et al\\.", "year": 2007}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["R.M. Bell", "Y. Koren"], "venue": "In ICDM,", "citeRegEx": "Bell and Koren,? \\Q2007\\E", "shortCiteRegEx": "Bell and Koren", "year": 2007}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Yahoo! learning to rank challenge overview", "author": ["O. Chapelle", "Y. Chang"], "venue": "JMLR - Proceedings Track,", "citeRegEx": "Chapelle and Chang,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Chang", "year": 2011}, {"title": "Preference learning with gaussian processes", "author": ["W. Chu", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "Chu and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Chu and Ghahramani", "year": 2005}, {"title": "Pranking with ranking", "author": ["K. Crammer", "Y. Singer"], "venue": "In NIPS,", "citeRegEx": "Crammer and Singer,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2001}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A. Flaxman", "A.T. Kalai", "H.B. McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R.D. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classifiers", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": null, "citeRegEx": "Herbrich et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2000}, {"title": "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search", "author": ["T. Joachims", "L. Granka", "Pan", "Bing", "H. Hembrooke", "F. Radlinski", "G. Gay"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Joachims et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2007}, {"title": "Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs", "author": ["R. Jones", "K. Klinkner"], "venue": "In CIKM,", "citeRegEx": "Jones and Klinkner,? \\Q2008\\E", "shortCiteRegEx": "Jones and Klinkner", "year": 2008}, {"title": "Learning to rank for information retrieval", "author": ["Liu", "T-Y"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "Liu and T.Y.,? \\Q2009\\E", "shortCiteRegEx": "Liu and T.Y.", "year": 2009}, {"title": "Introduction to Information Retrieval", "author": ["C. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Yue and Joachims,? \\Q2009\\E", "shortCiteRegEx": "Yue and Joachims", "year": 2009}, {"title": "The k-armed dueling bandits problem", "author": ["Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims"], "venue": "In COLT,", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 2, "context": "The learning algorithms perform structured output prediction (see (Bakir et al., 2007)) and thus can be applied in a wide variety of problems.", "startOffset": 66, "endOffset": 86}, {"referenceID": 17, "context": "Online convex optimization (Zinkevich, 2003) and online convex optimization in the bandit setting (Flaxman et al.", "startOffset": 27, "endOffset": 44}, {"referenceID": 8, "context": "Online convex optimization (Zinkevich, 2003) and online convex optimization in the bandit setting (Flaxman et al., 2005) are continuous relaxations of the expert and the bandit problems respectively.", "startOffset": 98, "endOffset": 120}, {"referenceID": 16, "context": "Most closely related to Coactive Learning is the dueling bandits setting (Yue et al., 2009; Yue & Joachims, 2009).", "startOffset": 73, "endOffset": 113}, {"referenceID": 10, "context": "More closely related is learning with pairs of examples (Herbrich et al., 2000; Freund et al., 2003; Chu & Ghahramani, 2005), since it circumvents the need for ranks; however, existing approaches require an iid assumption and typically perform batch learning.", "startOffset": 56, "endOffset": 124}, {"referenceID": 9, "context": "More closely related is learning with pairs of examples (Herbrich et al., 2000; Freund et al., 2003; Chu & Ghahramani, 2005), since it circumvents the need for ranks; however, existing approaches require an iid assumption and typically perform batch learning.", "startOffset": 56, "endOffset": 124}, {"referenceID": 11, "context": "In particular, we focus on preference feedback from clicks in web-search and draw upon data from a user study (Joachims et al., 2007).", "startOffset": 110, "endOffset": 133}, {"referenceID": 14, "context": "(Manning et al., 2008)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "We can now bound the first term in (12) using a standard telescoping argument from (Zinkevich, 2003) as follows:", "startOffset": 83, "endOffset": 100}, {"referenceID": 14, "context": "(Manning et al., 2008)) after replacing the relevance label with a linear prediction based on the features.", "startOffset": 0, "endOffset": 22}], "year": 2012, "abstractText": "We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. At each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking). The user responds by correcting the system if necessary, providing a slightly improved \u2013 but not necessarily optimal \u2013 object as feedback. We argue that such feedback can often be inferred from observable user behavior, for example, from clicks in web-search. Evaluating predictions by their cardinal utility to the user, we propose efficient learning algorithms that have O( 1 \u221a T ) average regret, even though the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search.", "creator": "gnuplot 4.4 patchlevel 3"}}}