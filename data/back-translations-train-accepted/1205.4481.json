{"id": "1205.4481", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure", "abstract": "In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.", "histories": [["v1", "Mon, 21 May 2012 03:29:17 GMT  (1334kb)", "https://arxiv.org/abs/1205.4481v1", "A short version of this paper appears in ICML 2012"], ["v2", "Mon, 2 Jul 2012 14:53:38 GMT  (1199kb)", "http://arxiv.org/abs/1205.4481v2", "This camera-ready version is uploaded for ICML 2012 proceedings"], ["v3", "Wed, 25 Jul 2012 15:15:42 GMT  (1334kb)", "http://arxiv.org/abs/1205.4481v3", "Full length version of ICML'12 with all proofs"], ["v4", "Mon, 1 Oct 2012 16:55:06 GMT  (1335kb)", "http://arxiv.org/abs/1205.4481v4", "Full length version of ICML'12 with all proofs. In this version, a bug in proving Theorem 6 is fixed. We'd like to thank Dr. Francesco Orabona for pointing it out"]], "COMMENTS": "A short version of this paper appears in ICML 2012", "reviews": [], "SUBJECTS": "cs.LG stat.CO stat.ML", "authors": ["hua ouyang", "alexander g gray"], "accepted": true, "id": "1205.4481"}, "pdf": {"name": "1205.4481.pdf", "metadata": {"source": "CRF", "title": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure", "authors": ["Hua Ouyang", "Alexander Gray"], "emails": ["agray}@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.44 81v4 [cs.LG] 1 Oct 201 2"}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "1.1 A Different \u201cComposite Setting\u201d", "text": "In the classic Black Box Setting of First-Order stochastic algorithms Nemirovski et al. (2009), the structure of objective functioning minx (x) = Eaccuration (x): [2], [3], [4], [5], [5], [6], [6], [6], [7], [7], [7], [7], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [9], [9, [9, 9, 9, 9, 9, [9], [9, 9, 9, 9, [9], [9], [9], [9, 9, 9, 9, 9, 9, [9], 9, [9], [9, 9, 9,], [9, [9], [9, 9,], [9, [9], [9, 9, 9,], [9, 9, [9], [9], [9, 9, 9, 9,], [9, 9], [9, 9, 9, 9, [, 9], [9], [9, 9], [9, 9, 9], [9], 9, [9], 9, [9, 9], [9, 9, 9, 9, 9], 9, [9], [9, 9], [9, 9, 9, 9], [9], 9, 9], [9, 9, [9], 9], [9, [9], [9], 9], [9], 9, [9], [9], 9, [9], [9, [9], [9], [9], 9], [9], 9], [9, [, [[[[[[[[[[[[[[[[[[[[[[[[[6], 6],],],]],],],"}, {"heading": "2. Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Stochastic Smoothing Method", "text": "An important breakthrough in nonsmooth minimization was made by Nesterov (5), where a series of Nesterov works (2005b, a, 2007b) was completed. (1) We (2) have shown that in many application areas a well-structured nonsmooth function f (x) as equivalent saddle point formmin x (x) = min x x x max. (4), where u-Rm is a convex set, a linear operator mapping RD \u2192 Rm and Q (u) is a continuous convex function. (4) one observes a smooth approximation of the original nonsmooth function ov (x): max u > U (u > Q) \u2212 Q (u)."}, {"heading": "3. Convergence Analysis", "text": "To illustrate our representation, we use Table 1 to list some notations used throughout the paper. Algorithm 1 Accelerated Nonsmooth Stochastic Gradient Departure (ANSGD) INPUT: Series Smooth, Smooth, Smooth, Smooth \u2264 1; OUTPUT: xt + 1; [0.] Initialize x0 and v0; for t = 0, 1, 2,. Do [1.] yt (1 \u2212 \u03b1t) (p) (p) (p) + 1 (x) maxu-U [< A\u0445t + 1x, u > Q (u) \u2212 \u043et (u) (p) [3.] xt + 1 \u00b2 yt deviation (n) (p) + 1 \u00b2 deviation from the smooth + 1 (n) (n)."}, {"heading": "3.1 How to Choose Stepsizes \u03b7t", "text": "In the RHS of (10), non-negative scalars p, q \u2265 0 are data dependent and could be arbitrarily large. Therefore, we need to set correct step variables \u03b7t so that the last two terms in (10) Arena positive. It might be assumed that: there is a series of ct \u2265 0 that makes these two terms dependent on each other. (11) It is easy to verify that if we \u03b7t = \u03b1t \u00b5 + successt and take each row ct \u03b1t2 (\u00b5 + \u03b8t) + Lt + 1) \u2265 0, then (11) we are satisfied. To maintain a narrow limit, we take = \u03b1t2 (\u00b5 + successt \u00b5 + successt and each row ct \u03b1t2 (\u00b5 + \u03b8t \u2212 \u03b1tLt + 1) \u2265 0, then (11). (12) Let's take the expectation on both sides of (10) and notice that the ectut + 1 \u2212 \u03b1t + 1 = 0, ectut + Lt + 1 on both sides are different from each other and the two sides are unequal."}, {"heading": "3.2 Optimal Rates for Composite Minimizations when \u00b5 = 0", "text": "Theorem 6 Let us take: \u03b1t = 2 t + 2, \u03b3t + 1 = \u03b1t, \u03b8t = Lg\u03b1t + 0, \u0445 t = E + E + 2 and \u03b7t = \u03b1t in Alg.1, where there is a constant. We have: \u0432 x and \u0432 t + 0, E [\u03a6 (xt + 1) \u2212 \u03a6 (x)] \u2264 4LgD2 (t + 2) 2 + 2E-A-D + 2 + 4 DU t + 2 + 2 (\u0420D2 + \u03c32 / 2), (14) where D2: = maxiD 2 i. In this result, the limit of deviation is optimal up to a constant factor Agarwal et al al al al al al. (2012) The dominating factor: the deviation from the deviation is still due to the deviation, but not affected by the deviation from the deviation from the deviation from the deviation from the deviation. (This deviation is optimal up to a constant factor Agaral al al al al al al al al the deviation from the deviation from the variation from the variation from the variation from the variation from the variation from the variation from the variation from the variation."}, {"heading": "3.3 Nearly Optimal Rates for Strongly Convex Minimizations", "text": "If \u00b5 > 0, g () is strongly convex, and the convergence rate from ANSGD to O (1 / t) can be improved. Theorem 7 Take \u03b1t = 2 t + 1, \u03b3t + 1 = \u03b1t, \u03b8t = Lg\u03b1t + \u00b5 2\u03b1t + E-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N-N"}, {"heading": "3.4 Batch-to-Online Conversion", "text": "The performance of online learning (online minimization) is typically measured in terms of remorse, which can be expressed as R (t): = t \u2212 1 \u2211 i = 0 [\u0438 (xi, amouni + 1) \u2212 \u03a6 (x \u0445 t, amouni + 1)], (18) where x \u0445 t: = argminx \u2211 t \u2212 1 (i = 0 [\u03a6 (i + 1))). In learning theory literature, many approaches are proposed that use online learning algorithms for batch learning (stochastic optimization), called \"online-to-batch\" (O-to-B) conversions. For convex functions, many of these approaches use a \"averaged\" solution as a definitive solution.On the contrary, we show that stochastic optimization algorithms can also be used directly for online learning. This \"batch-to-online conversion\" (B-to-O) is virtually free of additional efforts: i.i."}, {"heading": "4. Examples", "text": "In this section, two non-smooth functions are given as examples: We will show how to approach these functions stochastically and how to calculate the parameters used in our algorithm."}, {"heading": "4.1 Hinge Loss SVM Classification", "text": "Hinge loss is a convex substitute for the 0 \u2212 1 loss. Name a pattern designation pair with the premise: = {s, l} \u0445 P, where s-RD and l-R. Hinge loss can be expressed as fhinge (x): = max {0, 1 \u2212 lsTx}. It has been widely used for SVM classifiers where the target is min\u0445 (x) = minE\u0109fhinge (x) + \u03bb 2 \u00b2 x \u00b2 2. Note that the regulation term g (x) = 2 \u00b2 x \u00b2 \u00b2 \u00b2 is convex, i.e., according to Thm.7, ANSGD O (1 / (\u03bbt)) rates. Note that the regulation terms g (x) = 2 in (8) are easy to verify whether the smooth stostic approximation of Hinge loss isf \u00b2, i.e. according to Thm.7, ANSGD, is (1 / Rate)."}, {"heading": "4.2 Absolute Loss Robust Regression", "text": "Absolute loss is an alternative to the popular square loss for robust regressions Hastie et al. (2009). Using the same notation as Sec.4.1, it can be expressed as fabs (x): = | l \u2212 sTx |. Taking \u03c9 (u) = 12 \u20ac2 in (8), its smooth stochastic approximation can be expressed as ASF-abs (x, \u0442t, \u03b3t) = max \u2212 1 \u2264 u \u2264 1 {u (lt \u2212 sTt x) \u2212 \u03b3t u22}. (22) If we solve this maximization, we obtain an equivalent form: f \u2212 abs (x, \u0442t) = lt \u2212 sTt x \u2212 \u03b3t2, if lt \u2212 sTt x, (lt \u2212 sTt x) 22\u0445t, if \u2212 \u03b3t2, (lt \u2212 sTt x)."}, {"heading": "5. Experimental Results", "text": "In this section, five publicly available data sets from different fields of application are used to evaluate the efficiency of ANSGD. The data sets \"svmguide1,\" \"real-sim,\" \"rcv1\" and \"alpha\" represent binary classifications and \"abalone\" represent robust regressions.1 According to our examples in paragraph 4, we will evaluate our algorithm based on approximate hinge losses for classifications and approximate absolute losses for regression. Exact hinges and absolute losses are used for subgrade descendant algorithms with which we will compare as described in the following paragraph. All losses are quadratic l2 norm regulated. The regulation parameter \u03bb is shown on each figure."}, {"heading": "5.1 Algorithms for Comparison and Parameters", "text": "We compare ANSGD with three state-of-the-art algorithms. Each algorithm has a data-pendent tuning parameter, referred to by. (Although they have different physical meanings) The best values of the results are based on a tuning subset of samples. Note that our ANSGD is almost parameter-free. (As discussed in Thm.7, our experiments suggest that the optimal number of samples is taken in such a way that the number of samples can be easily taken. (This means that one can simply take the number of samples.) + \u00b5 + \u00b5 2\u03b1t + 1 \u2212 \u00b5. \"The classic stochastic approximation of Robbin and Monro (1951) is assumed to be: xt + 1\" xt \u2212 xt tf. \"(xt) is the subgradient. (xt)\" If we only assume the convexity (\u00b5 = 0), we use the stepsize."}, {"heading": "5.2 Results", "text": "Due to the stochasticity of all the algorithms, the program is executed ten times for each adjustment of the experiments and the mean and standard deviation of the results is plotted using error bars. In the first set of experiments, we compare ANSGD with two subgradient-based algorithms SGD and Averaged SGD. Classification results are shown in Fig.2, 3, 4 and 5, and regression results are shown in Fig.6. In each figure, the left column is used for algorithms without strongly convex assumptions, while the algorithms in the right column assume strong convexity and take for the classification results \u00b5 = \u03bb. Function values are plotted via the test set in the first line and plot accuracy is shown in the second line. It is clear that in all these experiments the ANSGD function values are convergentially faster than the other SGD algorithms."}, {"heading": "6. Conclusions and Future Work", "text": "In this setting, we propose a stochastic smoothing method and a novel stochastic algorithm ANSGD. Convergence analysis shows that it achieves (almost) optimal rates under both convex and strongly convex assumptions. We also propose a \"batch-to-online\" conversion for online learning and show that optimal remorse can be achieved. We will expand our method to limited minimizations, as well as cases where approximate function f () cannot be achieved simply by maximizing and Nesterov's excessive gap technique, with the \"true\" optimal 1 / t2 limit, and we will explore the possibility of integrating it into our algorithm. Use of linkages with statistical learning theories may also be promising."}, {"heading": "Appendix A. Proof of Lemma 3", "text": "The detection (xt) \u2212 figure (x) = [f (xt) \u2212 f (x)] + [g (xt) \u2212 g (x)] = E (f (xt,)] + E (f (x,) + g (xt,) \u2212 g (x,)] = E (max,) \u2212 g (x,) \u2212 g (x,)] \u2264 E (max, u > \u2212 Q (u) \u2212 g (u) + g (u) + g (u)] + E (f (x,) \u2212 g (x,) \u2212 g (x,)] \u2264 E (max u (< ax, u > \u2212 Q (u) \u2212 g (u) + max u (u)] + E (f (x,) + g (g,) + g (xt,) \u2212 g (x,) \u2212 g (v) = E (f (xt, g) \u2212 t) + g (f (f, f, f, f) g), g (t)."}, {"heading": "Appendix B. Proof of Lemma 4", "text": "Before we get to the proof of this problem, we present two further results. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "Appendix C. Proof of Theorem 6", "text": "Proving that it is easy to verify this by \u03b1t = 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t + 2 t (+ 1 \u2212 2) (+ 1 \u2212 1 \u2212 1 \u2212 \u2212 1 \u2212 t). (1 \u2212 1) D2t + 1 t (1 \u2212 1). (41) Next, we define the weighted sums of D2 t + 2 t (+ 1 \u2212 1 \u2212 \u2212 \u2212 t). (1 \u2212 1 \u2212 1 \u2212 1 \u2212 t). (1) (1) (1) (1) (1) (1) (1) \u2212 1 (1). (1) (1) (1) (1) (1)."}, {"heading": "Appendix D. Proof of Theorem 7", "text": "Evidence It is easy to prove that we are not really satisfied if we comply with the provisions. (1 \u2212 \u03b1t \u2212 1 \u2212 \u03b1t) (1 \u2212 \u03b1t) (1 \u2212 \u03b1t) (1 \u2212 \u03b1t) (1 \u2212 \u2212 \u03b1t) (47) and (1 \u2212 \u03b1t) (1 \u2212 \u03b12t (48) DenoteSt: = \u03b1t\u03b8t \u2212 (1 \u2212 \u03b1t) (1 \u2212 \u03b1t) (1 \u2212 \u03b1t \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 1). (49) The determination of the provisions is not (1 + E) (2). (1 \u2212 \u00b5) DenoteSt: (1 \u2212 \u03b1t) (1 \u2212 \u03b1t) (1 \u2212 2t2 + \u00b5t). (49) The determination of the provision is not fulfilled (50). (50) We want to find the smallest iteration index C so that we do not know the provisions. (1)."}, {"heading": "Appendix E. Proof of Proposition 8", "text": "The proof was that [t] R (t) = E [t] t \u2212 1 \u00b2 i = 0 [t] t (xi, i + 1) \u2212 1 \u00b2 (x, i + 1) = E [t] t \u2212 1 \u00b2 i = 0 [xi, i + 1) \u2212 1 \u00b2 (x, i + 1) + E [t] t (x, i + 1)] = t \u2212 1 \u00b2 i = 0 (x, i + 1) [t (xi, i + 1) \u2212 1 \u00b2 (x, i + 1) + E [t] t \u2212 1 (x, i + 1) \u2212 0 (x, i + 1) + E [xi, i + 1) \u2212 1 (x, i + 1) \u2212 1 (x, i + 1)."}], "references": [{"title": "Theory of classification: A survey of some recent advances", "author": ["St\u00e9phane Boucheron", "Olivier Bousquet", "G\u00e1bor Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Boucheron et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2005}, {"title": "Convergence analysis of a proximal-like minimization algorithm using bregman functions", "author": ["Gong Chen", "Marc Teboulle"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Chen and Teboulle.,? \\Q1993\\E", "shortCiteRegEx": "Chen and Teboulle.", "year": 1993}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": null, "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer"], "venue": "JMLR, (10):2899\u20132934,", "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Randomized smoothing for stochastic optimization", "author": ["John Duchi", "Peter L. Bartlett", "Martin J. Wainwright"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In COLT,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Accelerated gradient methods for stochastic optimization and online learning", "author": ["Chonghai Hu", "James T. Kwok", "Weike Pan"], "venue": "In NIPS 22,", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Robust estimation of a location parameter", "author": ["Peter J. Huber"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Huber.,? \\Q1964\\E", "shortCiteRegEx": "Huber.", "year": 1964}, {"title": "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, i: a generic algorithmic framework", "author": ["G. Lan", "S. Ghadimi"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Lan and Ghadimi.,? \\Q2011\\E", "shortCiteRegEx": "Lan and Ghadimi.", "year": 2011}, {"title": "An optimal method for stochastic composite optimization", "author": ["Guanghui Lan"], "venue": "Mathematical Programming,", "citeRegEx": "Lan.,? \\Q2010\\E", "shortCiteRegEx": "Lan.", "year": 2010}, {"title": "Splitting algorithms for the sum of two nonlinear operators", "author": ["P.L. Lions", "B. Mercier"], "venue": "SIAM J. on Numerical Analysis,", "citeRegEx": "Lions and Mercier.,? \\Q1979\\E", "shortCiteRegEx": "Lions and Mercier.", "year": 1979}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovski", "D. Yudin"], "venue": null, "citeRegEx": "Nemirovski and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1983}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Introductory Lectures on Convex Optimization, A Basic Course", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}], "referenceMentions": [{"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning).", "startOffset": 126, "endOffset": 154}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied.", "startOffset": 126, "endOffset": 506}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied.", "startOffset": 126, "endOffset": 642}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)\u2212minx f(x) \u2264 O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a). In this paper, we extend Nesterov\u2019s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions.", "startOffset": 126, "endOffset": 851}, {"referenceID": 13, "context": "In spite of the attractive properties, nonsmooth functions are theoretically more difficult to optimize than smooth functions Nemirovski and Yudin (1983). In this paper we focus on minimizing nonsmooth functions where the functions are either stochastic (stochastic optimization), or learning samples are provided incrementally (online learning). Smoothness and strong-convexity are typically certificates of the existence of fast global solvers. Nesterov\u2019s deterministic smoothing method Nesterov (2005b) deals with the difficulty of nonsmooth functions by approximating them with smooth functions, for which optimal methods Nesterov (2004) can be applied. It converges as f(xt)\u2212minx f(x) \u2264 O(1/t) after t iterations. If a nonsmooth function is strongly convex, this rate can be improved to O(1/t2) using the excessive gap technique Nesterov (2005a). In this paper, we extend Nesterov\u2019s smoothing method to the stochastic setting by proposing a stochastic smoothing method for nonsmooth functions. Combining this with a stochastic version of the optimal gradient descent method, we introduce and analyze a new algorithm named Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), for a class of functions that include the popular ML methods of interest. To our knowledge ANSGD is the first stochastic first-order algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions without Polyak\u2019s averaging Polyak and Juditsky (1992). In comparison, the classic SGD converges in O(ln t/t) for 1.", "startOffset": 126, "endOffset": 1464}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011).", "startOffset": 34, "endOffset": 59}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011).", "startOffset": 34, "endOffset": 114}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011).", "startOffset": 34, "endOffset": 125}, {"referenceID": 7, "context": "(2007), and is usually not robust Nemirovski et al. (2009). Even with Polyak\u2019s averaging Bach and Moulines (2011); Xu (2011), there are cases where SGD\u2019s convergence rate still can not be faster than O(ln t/t) Shamir (2011). Numerical experiments on real-world datasets also indicate that ANSGD converges much faster in comparing with these state-of-the-art algorithms.", "startOffset": 34, "endOffset": 224}, {"referenceID": 3, "context": "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario.", "startOffset": 97, "endOffset": 117}, {"referenceID": 3, "context": "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds. In machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of \u201ccomposite minimizations\u201d that we will pursue in this paper, along with our notations and assumptions. 1.1 A Different \u201cComposite Setting\u201d In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = E\u03bef(x, \u03be) : \u03be \u223c P} is unknown.", "startOffset": 97, "endOffset": 705}, {"referenceID": 3, "context": "A perturbation-based smoothing method is recently proposed for stochastic nonsmooth minimization Duchi et al. (2011). This work achieves similar iteration complexities as ours, in a parallel computation scenario. In serial settings, ANSGD enjoys better and optimal bounds. In machine learning, many problems can be cast as minimizing a composition of a loss function and a regularization term. Before proceeding to the algorithm, we first describe a different setting of \u201ccomposite minimizations\u201d that we will pursue in this paper, along with our notations and assumptions. 1.1 A Different \u201cComposite Setting\u201d In the classic black-box setting of first-order stochastic algorithms Nemirovski et al. (2009), the structure of the objective function minx{f(x) = E\u03bef(x, \u03be) : \u03be \u223c P} is unknown. In each iteration t, an algorithm can only access the first-order stochastic oracle and obtain a subgradient f \u2032(x, \u03bet). The basic assumption is that f \u2032(x) = E\u03bef \u2032(x, \u03be) for any x, where the random vector \u03be is from a fixed distribution P . The composite setting (also known as splitting Lions and Mercier (1979)) is an extension of the black-box model.", "startOffset": 97, "endOffset": 1102}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a).", "startOffset": 118, "endOffset": 143}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a).", "startOffset": 118, "endOffset": 169}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 187}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 252}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 276}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al.", "startOffset": 118, "endOffset": 301}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010).", "startOffset": 118, "endOffset": 319}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function \u03a6(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = E\u03bef(x, \u03be) and a nonsmooth (but simple and deterministic) function g().", "startOffset": 118, "endOffset": 332}, {"referenceID": 2, "context": "Driven by applications of sparse signal reconstruction, it has gained significant interest from different communities Daubechies et al. (2004); Beck and Teboulle (2009); Nesterov (2007a). Stochastic variants have also been proposed recently Lan (2010); Lan and Ghadimi (2011); Duchi and Singer (2009); Hu et al. (2009); Xiao (2010). A stochastic composite function \u03a6(x) := f(x) + g(x) is the sum of a smooth stochastic convex function f(x) = E\u03bef(x, \u03be) and a nonsmooth (but simple and deterministic) function g(). To minimize \u03a6, previous work construct the following model iteratively: \u3008\u2207f(xt, \u03bet),x \u2212 xt\u3009+ 1 \u03b7t D(x,xt) + g(x), (1) where \u2207f(xt, \u03bet) is a gradient, D(\u00b7, \u00b7) is a proximal function (typically a Bregman divergence) and \u03b7t is a stepsize. A successful application of the composite idea typically relies on the assumption that model (1) is easy to minimize. If g() is very simple, e.g. \u2016x\u20161 or the nuclear norm, it is straightforward to obtain the minimum in analytic forms. However, this assumption does not hold for many other applications in machine learning, where many loss functions (not the regularization term, here the nonsmooth g() becomes the nonsmooth loss function) are nonsmooth, and do not enjoy separability properties Wright et al. (2009). This includes important examples such as hinge loss, absolute loss, and \u01eb-insensitive loss.", "startOffset": 118, "endOffset": 1265}, {"referenceID": 15, "context": "In Section 2 we propose to form a smooth stochastic approximation of f(), such that the optimal methods Nesterov (2004) can be applied to attain optimal convergence rates.", "startOffset": 104, "endOffset": 120}, {"referenceID": 15, "context": "The key property of this approximation is: Lemma 1 Nesterov (2005b)(Theorem 1) Function f\u0302(x, \u03b3) is convex and continuously differentiable, and its gradient is Lipschitz continuous with constant Lf\u0302 := \u2016A\u20162 \u03b3\u03b6 , where \u2016A\u2016 := max x,u {\u3008Ax,u\u3009 : \u2016x\u2016 = 1, \u2016u\u2016 = 1}.", "startOffset": 51, "endOffset": 68}, {"referenceID": 15, "context": "This stochastic algorithm is obtained by applying Nesterov\u2019s optimal method to our smooth surrogate function, and thus has a similar form to that of his original deterministic method Nesterov (2004)(p.", "startOffset": 50, "endOffset": 199}, {"referenceID": 3, "context": "This bound is better than that of stochastic gradient descent or stochastic dual averaging Dekel et al. (2010) for minimizing L-Lipschitz smooth functions, whose rate is O ( LD2 0 t + D2 0+\u03c3 2 \u221a t )", "startOffset": 91, "endOffset": 111}, {"referenceID": 0, "context": "Boucheron et al. (2005)): t\u22121 i=1 \u03a6(x \u2217 t )\u2212\u03a6(xt , \u03bei+1) = O( \u221a t).", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Boucheron et al. (2005)): t\u22121 i=1 \u03a6(x \u2217 t )\u2212\u03a6(xt , \u03bei+1) = O( \u221a t). Together with summing up the RHS of (14), we can obtain an O( \u221a t) regret bound. When \u03a6() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009): t\u22121 i=1 \u03a6(x \u2217 t ) \u2212 \u03a6(xt , \u03bei+1) = O(ln t).", "startOffset": 0, "endOffset": 252}, {"referenceID": 0, "context": "Boucheron et al. (2005)): t\u22121 i=1 \u03a6(x \u2217 t )\u2212\u03a6(xt , \u03bei+1) = O( \u221a t). Together with summing up the RHS of (14), we can obtain an O( \u221a t) regret bound. When \u03a6() is strongly convex, the second term in (19) can be bounded using Shalev-Shwartz et al. (2009): t\u22121 i=1 \u03a6(x \u2217 t ) \u2212 \u03a6(xt , \u03bei+1) = O(ln t). Together with summing up the RHS of (16), an O(ln t) regret bound is achieved. The O( \u221a t) and O(ln t) regret bounds are known Using our proposed ANSGD for online learning by B-to-O achieves the same (optimal) regret bounds as state-of-the-art algorithms designated for online learning. However, using O-to-B, one can only retain an O(ln t/t) rate of convergence for stochastic strongly convex optimization. From this perspective, O-to-B is inferior to B-to-O. The sub-optimality of O-to-B is also discussed in Hazan and Kale (2011). 4.", "startOffset": 0, "endOffset": 830}, {"referenceID": 6, "context": "2 Absolute Loss Robust Regression Absolute loss is an alternative to the popular squared loss for robust regressions Hastie et al. (2009). Using same notations as Sec.", "startOffset": 117, "endOffset": 138}, {"referenceID": 9, "context": "This approximation looks similar to the well-studied Huber loss Huber (1964), though they are different.", "startOffset": 53, "endOffset": 77}, {"referenceID": 10, "context": "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov\u2019s optimal method, begging the question of whether it has similar behavior.", "startOffset": 14, "endOffset": 25}, {"referenceID": 10, "context": "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov\u2019s optimal method, begging the question of whether it has similar behavior.", "startOffset": 26, "endOffset": 49}, {"referenceID": 10, "context": "This approach Lan (2010); Lan and Ghadimi (2011) is interesting to compare because like ANSGD, it is another way of obtaining a stochastic algorithm based on Nesterov\u2019s optimal method, begging the question of whether it has similar behavior. Theoretically, according to Prop.8 and 9 in Lan and Ghadimi (2011), the bound for the nonsmooth part is of O(1/ \u221a t) for \u03bc = 0 and O(1/t) for \u03bc > 0.", "startOffset": 26, "endOffset": 309}, {"referenceID": 9, "context": "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the \u201c3-point identity\u201d Chen and Teboulle (1993)(Lemma 3.", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the \u201c3-point identity\u201d Chen and Teboulle (1993)(Lemma 3.", "startOffset": 94, "endOffset": 119}, {"referenceID": 1, "context": "It appeared in Lan and Ghadimi (2011)(Lemma 2), and is an extension of the \u201c3-point identity\u201d Chen and Teboulle (1993)(Lemma 3.1). Lemma 10 Lan and Ghadimi (2011) Let l(x) be a convex function.", "startOffset": 94, "endOffset": 163}], "year": 2012, "abstractText": "In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.", "creator": "LaTeX with hyperref package"}}}