{"id": "1205.4810", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2012", "title": "Safe Exploration in Markov Decision Processes", "abstract": "In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.", "histories": [["v1", "Tue, 22 May 2012 06:02:09 GMT  (724kb,D)", "https://arxiv.org/abs/1205.4810v1", null], ["v2", "Sat, 30 Jun 2012 09:17:38 GMT  (722kb,D)", "http://arxiv.org/abs/1205.4810v2", null], ["v3", "Fri, 6 Jul 2012 20:56:23 GMT  (725kb,D)", "http://arxiv.org/abs/1205.4810v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["teodor mihai moldovan", "pieter abbeel"], "accepted": true, "id": "1205.4810"}, "pdf": {"name": "1205.4810.pdf", "metadata": {"source": "META", "title": "Safe Exploration in Markov Decision Processes", "authors": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "emails": ["moldovan@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"], "sections": [{"heading": null, "text": "In environments with uncertain dynamics, exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms offer strong exploration guarantees, but they tend to rely on an occupational assumption. The essence of occupational learning is that each state is ultimately reachable from any other state by pursuing an appropriate policy. This assumption enables exploration algorithms that simply favor states that have not been visited before. For most physical systems, this assumption is impractical, since the systems would break down before reasonable exploration has taken place, i.e. most physical systems do not satisfy the occupational assumption. In this paper, we address the need for safe exploration methods in Markov decision-making processes. We first propose a general formulation of occupational safety. We show that imposing safety by restricting attention to subjective exploration is a hard-to-present the resulting safe set of guaranteed algorithms."}, {"heading": "1. Introduction", "text": "It is indeed the case that we are able to manoeuvre ourselves into a situation where we have to put ourselves at the centre."}, {"heading": "2. Notation and Assumptions", "text": "For an introduction to the MDPs, we refer readers to (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996).We use capital letters to denote random variables; for example, the total reward is: V: = \u00b2 t = 0RSt, At. We represent the guidelines and initial state distributions by probability measures. Usually, the measurement \u03c0 corresponds to a policy, and the measurement s: = \u03b4 (s), which puts the measurement only in state s, will correspond to the starting point in state s. With this notation, the usual value recursion, assuming a known transitional measure, p, is: Eps [V] = a, s \u00b2, s \u00b2 s \u00b2 s \u00b2 s, a \u00b2 s \u00b2 s, s \u00b2 s \u00b2, p. \"We specify the transitional measure as a superscript of the expectation operator rather than as a dynamic, which in this case is sometimes subordinated."}, {"heading": "3. Problem formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Exploration Objective", "text": "Exploration methods as proposed in (Brafman & Tennenholtz, 2001; Kolter & Ng, 2009) operate by finding optimal strategies in constructed MDPs with exploration bonuses. For example, the R-Max algorithm constructs an MDP based on the discounted expected transition measure and the assumption of rewards, and adds a deterministic exploration bonus corresponding to the highest possible reward in the MDP, a = rmax, for transitions that are not sufficiently known. Our method allows safety restrictions to be added to such exploration methods. Henceforth, we will limit attention to such exploration methods that can be formulated as optimization problems of form: Maximize E-p s0, E-p = 0 (rSt, At + \u03b2 St., At). (1)"}, {"heading": "3.2. Safety Constraint", "text": "The question of safety is closely related to occupational safety. Almost all proposed exploration techniques presuppose occupational safety; the authors present them as a harmless technical assumption, but it rarely holds on to interesting practical problems. Whenever this happens, their efficient exploration techniques are no longer applicable, often leading to very inefficient strategies. Informally, an environment is ergodic when every mistake can ultimately be forgiven. Specifically, a belief in MDP's is ergodic when and only when any state is reachable via any policy or, equally, if and only when: \"s,\" r, such a mistake can ultimately be forgiven. [Bs \"] = 1, (2) where Bs\" is a random indicator that is variable for the event that the system reaches the state at least once: Bs \"= 1 {t < s\" that we do something that s = min} (unfortunately, many of us are not leaving the city)."}, {"heading": "3.3. Safety Counter-Examples", "text": "We close this section with counter-examples to three other, perhaps at first glance more intuitive, definitions of security. First, we could have tried to define security in terms of safe states or state measures; that is, we might think that it is enough not to make the unsafe states and measures available to the planner (or simply to make them inaccessible) to guarantee security. Figure 1 shows that security should be defined in terms of a safe policy, not in terms of safe states or state measures. Second, we might think that it may be enough to ensure that there is a return policy for each potential sample, but does not impose that it should be the same for all samples. That is, we might think that Condition 3 is too strong and would be sufficient instead."}, {"heading": "4. Guaranteed Safe, Potentially Sub-optimal Exploration", "text": "Although the introduction of the safety restriction in Equation (3) is difficult, as shown in Theorem 1, we can efficiently restrict a lower limit of the safety target so that the safety condition is still demonstrably met, which could lead to sub-optimal exploration approaches, as the number of strategies we optimize over time has dwindled. However, we should not forget that the exploration targets are approximate solutions to other NP-hard problems, so that the optimism is already present in existing (non-secure) approaches to starting outAlgorithm 1 Safe exploration algorithmRequire: Previous belief, discounting, safety level. Prerequisite: Mode of operation: Faith \u2192 Exploration Bonus M, N \u00b2 new MDP objects repeat the current state and observations update belief with information."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Grid World", "text": "Our first experiment models a terrain exploration problem in which the agent has limited sensor capacity. We consider a simple rectangular P \u03b2 \u03b2 = P 2001 as an uncertain world in which each state has a height Hs. From our Bayesian point of view, these heights are independent, evenly distributed categorical random variables on the set {1, 2, 3, 4, 5}. The agent can always try to move to any immediately adjacent state. Such a step is likely to succeed if the height of the target state is no more than one level above the current state; otherwise, the agent remains probable One. In other words, the agent can always descend cliffs, but cannot climb up if they are too steep. Whenever the agent enters a new state, he can see the exact heights of all surrounding states. We present this grid world experiment to build intuition and to play an easy reproducible way to provide a clear result of exploration while P clearly demonstrates the safe result of a 2001 number of exploration."}, {"heading": "5.2. Martian Terrain", "text": "For our second experiment, we are modelling the problem of autonomous exploration of the surface of Mars by a rover such as the Mars Science Laboratory (MSL) (Lockwood, 2006). The MSL is designed so that it can be remotely controlled from Earth, but communication suffers from a latency of 16.6 minutes. At top speed, it could cross about 20 meters before receiving new instructions, so it must be able to navigate autonomously. In the future, when such rovers are faster and cheaper to deploy, the ability to plan their paths autonomously will become even more important. The MSL is designed for a static stability of 45 degrees, but it would only be able to explore inclines up to 5 degrees without slipping (MSL, 2007). Digital terrain models for parts of the surface of Mars are available (HiRISE) on a scale of 1.00 meters / pixel and accurately up to a quarter of a meter."}, {"heading": "5.3. Computation Time", "text": "We implemented our algorithm in Python 2.7.2.7 and used Numpy 1.5.1 for manipulating dense arrays, SciPy 0.9.0 for manipulating sparse matrices, and Mosek 6.0.0.119 for linear programming. The discount factor was set at.99 for the grid world experiment and at.999 for Mars exploration. In the latter experiment, we also limited the accuracy to 10 \u2212 6 to avoid numerical instabilities in the LP solver. Table 1 summarizes the scheduling times for our Mars exploration experiments."}, {"heading": "6. Discussion", "text": "In addition to the safety formulation we discussed in Section 3.2, our framework also supports a number of other safety criteria that we have not discussed due to space constraints: \u2022 Stricter ergonomics that ensures that a return within a horizon is possible, H, not just at some point, with the probability \u03b4. \u2022 Ensuring that the probability of leaving a predefined set of safe government measures is less than 1 \u2212 \u043c. \u2022 Ensuring that the expected total reward under faith is higher than \u03b4.In addition, any number and combination of these restrictions can be imposed at different \u03b4 levels simultaneously."}, {"heading": "Acknowledgements", "text": "This material is based on work supported in part by the NSF under the award IIS-0931463, by ARO under the MAST program, by a Sloan Fellowship, a gift from Intel, by the US Army Research Laboratory and the US Army Research Office under contract / grant number W911NF-11-1-0391."}], "references": [{"title": "Constrained Markov Decision Processes", "author": ["Altman", "Eitan"], "venue": null, "citeRegEx": "Altman and Eitan.,? \\Q1999\\E", "shortCiteRegEx": "Altman and Eitan.", "year": 1999}, {"title": "Extensions of Learning-Based Model Predictive Control for Real-Time Application to a Quadrotor Helicopter", "author": ["Aswani", "Anil", "Bouffard", "Patrick"], "venue": "In Proc. American Control Conference (ACC) (to appear),", "citeRegEx": "Aswani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Aswani et al\\.", "year": 2012}, {"title": "A survey of computational complexity results in systems and control", "author": ["Blondel", "Vincent D", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Blondel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2000}, {"title": "R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning", "author": ["Brafman", "Ronen I", "Tennenholtz", "Moshe"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Brafman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Brafman et al\\.", "year": 2001}, {"title": "Percentile optimization in uncertain Markov decision processes with application to efficient exploration", "author": ["Delage", "Erick", "Mannor", "Shie"], "venue": "ICML; Vol. 227,", "citeRegEx": "Delage et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Delage et al\\.", "year": 2007}, {"title": "UAV Cooperative Control with Stochastic Risk Models", "author": ["A Geramifard", "J Redding", "N Roy", "How", "J P"], "venue": "In Proceedings of the American Control Conference (ACC),", "citeRegEx": "Geramifard et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Geramifard et al\\.", "year": 2011}, {"title": "Guaranteed safe online learning of a bounded system", "author": ["Gillula", "Jeremy H", "Tomlin", "Claire J"], "venue": "In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Gillula et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gillula et al\\.", "year": 2011}, {"title": "Safe exploration for reinforcement learning", "author": ["A Hans", "D Schneega\u00df", "AM Sch\u00e4fer", "S. Udluft"], "venue": "ESANN", "citeRegEx": "Hans et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hans et al\\.", "year": 2008}, {"title": "Near-Optimal Reinforcement Learning in Polynomial Time", "author": ["Kearns", "Michael", "Singh", "Satinder"], "venue": "Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2002}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning - ICML", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Knows what it knows: a framework for self-aware learning", "author": ["Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Li et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Li et al\\.", "year": 2008}, {"title": "Introduction: Mars Science Laboratory: The Next Generation of Mars Landers", "author": ["Lockwood", "Mary Kae"], "venue": "Journal of Spacecraft and Rockets,", "citeRegEx": "Lockwood and Kae.,? \\Q2006\\E", "shortCiteRegEx": "Lockwood and Kae.", "year": 2006}, {"title": "Robust Control of Markov Decision Processes with Uncertain Transition Matrices", "author": ["Nilim", "Arnab", "El Ghaoui", "Laurent"], "venue": "Operations Research,", "citeRegEx": "Nilim et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nilim et al\\.", "year": 2005}, {"title": "A theoretical analysis of Model-Based Interval Estimation", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "In Proceedings of the 22nd international conference on Machine learning - ICML", "citeRegEx": "Strehl et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2005}, {"title": "Reinforcement learning: an introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 5, "context": "The safe exploration for MDP methods proposed by (Geramifard et al., 2011; Hans et al., 2008) gauge safety based on the best best estimate of the transition measure but they ignore the level of uncertainty in this estimate.", "startOffset": 49, "endOffset": 93}, {"referenceID": 7, "context": "The safe exploration for MDP methods proposed by (Geramifard et al., 2011; Hans et al., 2008) gauge safety based on the best best estimate of the transition measure but they ignore the level of uncertainty in this estimate.", "startOffset": 49, "endOffset": 93}, {"referenceID": 10, "context": "Provably efficient exploration is a recurring theme in reinforcement learning (Strehl & Littman, 2005; Li et al., 2008; Brafman & Tennenholtz, 2001; Kearns & Singh, 2002; Kolter & Ng, 2009).", "startOffset": 78, "endOffset": 189}], "year": 2012, "abstractText": "In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don\u2019t satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "LaTeX with hyperref package"}}}