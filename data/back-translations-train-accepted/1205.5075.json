{"id": "1205.5075", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2012", "title": "Efficient Sparse Group Feature Selection via Nonconvex Optimization", "abstract": "Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) statistically, we introduce a nonconvex sparse group feature selection model which can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved; (2) computationally, we propose an efficient algorithm that is applicable to large-scale problems. Numerical results suggest that the proposed nonconvex method compares favorably against its competitors on synthetic data and real-world applications, thus achieving desired goal of delivering high performance.", "histories": [["v1", "Wed, 23 May 2012 00:02:01 GMT  (22kb)", "http://arxiv.org/abs/1205.5075v1", null], ["v2", "Fri, 18 Jan 2013 21:06:49 GMT  (22kb)", "http://arxiv.org/abs/1205.5075v2", "Accepted by the 30th International Conference on Machine Learning (ICML 2013)"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shuo xiang", "xiaoshen tong", "jieping ye"], "accepted": true, "id": "1205.5075"}, "pdf": {"name": "1205.5075.pdf", "metadata": {"source": "CRF", "title": "Efficient Sparse Group Feature Selection via Nonconvex Optimization", "authors": ["Shuo Xiang", "Xiaotong Shen", "Jieping Ye"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 120 5.50 75v1 [cs.LG]"}, {"heading": "1 Introduction", "text": "During the last decade, the selection of sparse characteristics has been extensively studied, both on optimization algorithms [1] and on statistical properties [28, 20, 3]. However, if the data have a specific group structure, the sparse modeling in [24, 16, 13] has been explored for the selection of group characteristics. However, the group lasso [24] proposes an L2 regulation method for each group, which ultimately produces a group-by-group sparse model. The benefit of such a method has been demonstrated in the detection of splice sites [23] - an important step in gene discovery and theoretically justified in [13]. The sparse group lasso [11] makes it possible to promote thrift at the level of both characteristics and groups at the same time."}, {"heading": "2 Nonconvex Formulation and Computation", "text": "A major difficulty of the solution (1) stems from non-convex and discrete constraints, which require a detailed enumeration of all possible combinations of features and groups in order to achieve the optimal solution. \u2212 Therefore, we approach these constraints through their continuous computer-aided surrogates: minimize x1 (2): minimize x2 (2): minimize x2 (2): minimize x2 (2): minimize x2 (2): minimize x2 (2): minimize x2 (2), which is a truncated L1 function approaching the L0 function [19, 26], and are quantifying parameters approaching the indicator function (z). \u2212 z | z | 6 = 0) as approaches zero.In order to solve the non-convex problem (2), we develop a difference in the convex (DC) based on any non-convex function."}, {"heading": "3 Theoretical Results", "text": "In this section, theoretical aspects of the proposed method are examined. Specifically, we show that the Orakel-Sch\u00e4tzer x-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-"}, {"heading": "4 Optimization Procedures", "text": "As mentioned in Section 2, the efficient calculation of the convex partial problem (6) is crucial for the proposed DC algorithm. Note that (6) has an identical form of the confined group lasso problem: Minimize x1, 2, Ax \u2212 y, 22 bearing in mind that (7) x is limited to the two support sets. As shown in Section 4.3, a solution algorithm (6) can only be achieved by making a few changes to (7). Therefore, we will first focus on the solution (7)."}, {"heading": "4.1 Accelerated Gradient Method", "text": "For large-scale problems, the dimensionality of the data can be very high, which is why first-order optimization is often preferred. We adapt the well-known method of accelerated gradient (AGM) [18, 2], which is commonly applied due to its rapid convergence rate. To apply AGM to our formulation (7), the crucial step is to solve the following Sparse Group Lasso Projection (SGLP): Minimize x1, 2, x \u2212 v if A is the identity. For simplicity's sake, leave C1 and C2 with the two above limitations in what follows. As the AGM is a standard framework whose efficiency mainly depends on the projection stage, we leave the detailed description of the AGM in the supplement and introduce the efficient algorithm for this projection (8)."}, {"heading": "4.2 Efficient Projection", "text": "We start with some specific cases of (8). If only C1 exists, (8) becomes the known L1 ball projection [9], whose optimal solution is called Ps11 (v), which stands for the projection of v onto the L1 ball with a radius of s1. However, if only C2 is involved, it becomes the group lasso projection, which is called Ps2G. Furthermore, we say that a restriction is active if and only if an equality holds x at the optimal solution; otherwise it is inactive. Preliminary results are summarized in Lemma 1: Lemma 1. Identify a global minimizer of (8) as x. Then the following results apply: 1. If both C1 and C2 are inactive, then x."}, {"heading": "4.2.1 Computing x\u2217 from the optimal dual variables", "text": "Next we will consider the case in which both C1 and C2 are active. According to the konvexen duality theory [6] there are unique, non-negative dual variables, in such a way that x * is also the global minimizer of the following regularized problem: minimize x1 2 x \u2212 v \u00b2 22 + x \u00b2 x \u00b2 1 + x \u00b2 G, (9), whose solution is given by the following theory. The optimal solution x * of (9) results from the calculation of (9) Gi = max \u00b2 2 \u2212 x \u00b2 Gi \u00b2, 0} v2 \u00b2 Gi \u00b2 2 i = 1, 2 \u00b7 \u00b7 I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I."}, {"heading": "4.2.2 Computing \u03bb\u2217: bisection", "text": "Considering an initial assumption (estimator) of the GLP, states that we can easily deduce the optimal course of action (provided there is an oracle method that indicates whether the optimal value is greater than the optimal value) from the assumption that both C1 and C2 are objective. Let us design the triples (x) that the upper limit and a lower limit of \u03bb (v, s1, s2) should be taken into account in order to be active in the optimal solution of (8) with both limitations. Let us consider the triples (x, \u03bb, \u043c). (SGLP, s2, s2) consider the optimal double course of action. (x, s2) Let us consider the optimal double course of action."}, {"heading": "5 Significance", "text": "This section is dedicated to a brief discussion of the advantages of our work statistically and computationally. Furthermore, it explains why the proposed method is useful for performing an efficient and interpretable selection of characteristics with a given natural group structure.Interpretation.The parameters in (2) are highly interpretable insofar as s1 and s2 represent both upper limits of the number of elements not equal to zero and those of groups. This is advantageous, especially in the presence of certain prior knowledge regarding the number of characteristics and / or those of groups. However, such an interpretation disappears with convex methods such as lasso or sparse group lasso, with the inclusion of such prior knowledge often repeated attempts of different parameters.Parametering. Typically, the adjustment of parameters for good generalization requires considerable work due to a large number of parameter selections. However, coordination in (1) can and can be further simplified by means of integral values within a limited range."}, {"heading": "6 Empirical Evaluation", "text": "In this section, numerical experiments are performed to evaluate the proposed methods in terms of efficiency and accuracy of selecting sparse group attributes. Evaluations are performed on a PC with i7-2600 CPU, 8.0 GB of memory and 64-bit Windows operating system."}, {"heading": "6.1 Evaluation of Projection Algorithms", "text": "Since DC programming and accelerated gradient methods are both standards, the efficiency of the proposed non-convex formula (2) depends on the projection step in (8). Therefore, we focus on evaluating the projection algorithms and comparing them with two popular projection algorithms: Alternating Direction Multiplier Method (ADMM) [5] and Dykstra's projection algorithm [7]. We provide a detailed derivation of the adaptation of these two algorithms to our formulation in the supplement. To evaluate the efficiency, we first generate the vector v, whose entries are chosen in [\u2212 50, 50] and the dimensions of v, which are designated as p. Next, we partition the vector into 10 groups of the same size. Finally, s2 is logged to 5 (p) and s1, the radius of the L1 ball is projected."}, {"heading": "6.2 Performance on Synthetic Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Experimental Setup", "text": "The 100 characteristics (columns) are divided into 10 groups of the same size. The basic truth vector x0 has unequal elements in only 4 of the 10 groups. To further increase the sparseness, in each group unequal zero only t (t \u2264 10) elements are unequal zero, where t from [1,5] is evenly distributed. Finally, according to Ax0 + z, y is generated with the following z-distribution N (0, 0.52), where A and y are divided into training and test sets of the same size. We adapt our method to the training group and compare it with lasso, group lasso and sparse group lasso. The tuning parameters of the convex methods are selected from {0.01, 0.1, 1, 10}, whereas our method selects the number of unequal groups from the group {2, 4, 6, 8} and selects the number of characteristics from 2 {s2, 0.1, 1, 10}."}, {"heading": "6.2.2 Results and Discussions", "text": "We use the following metrics to evaluate: \u2022 Estimation error: \u2022 x-x-x0-22 \u2022 Prediction error: x-x-y-22 \u2022 Group accuracy: | T2 (x-x) and T2 (x0) | / | T2 (x-x) | \u2022 Group recall: | T2 (x-x) and T2 (x0) | / | T2 (x0) | where x-x is the estimator from (2) and y is an independent vector following the same distribution as y. Group accuracy and callback show the ability to recover the group structure from the data. We report the results in Table 3 and note that our model generally performs better. Note that although our model does not provide the best result on the group recall metric, the group accuracy of our model is significantly better than that of the others, illustrating the fact that the three convex methods restore more redundant groups."}, {"heading": "6.3 Performance on Real-world Application", "text": "Our method will be further evaluated for the application of the study of electroencephalography (EEG), although it relates genetic predisposition to alcoholism [10]. The EEG records the spontaneous electrical activity of the brain by measuring voltage fluctuations across multiple electrodes on the scalp. However, this technology has been widely used in clinical diagnosis, such as coma, brain death and genetic predisposition to alcoholism. In fact, a specific group structure is encoded in the EEG data, as each electrode records the electrical activity of a specific region of the scalp. Identification and use of such spatial information has the potential to increase the stability of a prediction. The training kit contains 200 samples of 16384 dimensions, sampled from 64 electrodes placed on the scalp of the subject at 256 Hz (3.9-msec period)."}, {"heading": "7 Conclusion and Future Work", "text": "This paper expands a non-convex paradigm towards the selection of sparse group characteristics, in particular by analysing theoretical properties regarding the accuracy of selection and parameter estimation, and developing an efficient optimization scheme based on DC programming, the accelerated gradient method and efficiency3\u03bblasso = Logspace (10 \u2212 3, 1), \u03bbglasso = Logspace (10 \u2212 2, 1) 4The product space of \u03bbglassoprojection. Efficiency and effectiveness of the proposed method are validated both on synthetic data and on real-world applications, and the proposed method will be further investigated in real-world applications that incorporate the group structure. Furthermore, the extension of the proposed model to multi-task multimodal learning is another promising direction [25]."}, {"heading": "1 Proof of Theorem 1", "text": "The evidence uses a high probability of discrepancy from [22] to treat one-sided Log probability ratios with limitations (= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "2 Proof of Theorem 3", "text": "Let us use an intermediate goal of [4]: Lemma 2. Let us use X as metric space and consider U as standardized space. Let us assume that for all x x x the function x (x, \u00b7) is differentiable and that we (x, Y) and DY (x, Y) (the partial derivative of x, Y) are continuous in relation to X \u00b7 U. Let us also define a compact subset of X. Define the optimal value function as result (Y) = infx (x, Y). The optimal function of x (Y) is directionally differentiable. Furthermore, if for any Y solution U (\u00b7, Y) a unique minimizer x (Y) in relation to the situation exists, then the solution in Y is differentiable and the gradient in relation to Y (Y)."}, {"heading": "4 Accelerated Gradient Method", "text": "The HV procedure is listed in algorithms 4, in which f (x) is the objective function, where f (x) is the objective function 12 (x) Ax (y) = f (x) + f (u) denotes its gradient at x. Furthermore, fL, u (x) defines the linearization of f (x) at u as follows: fL, u (x) = f (u) + f (u) T (x \u2212 u) + L2 (x \u2212 u) 22.Algorithm 4 Accelerated gradient method [18, 2] for (7) Input: A, y, s1, s2, L0, x0, Output: Solution x to (7) 1: Initialize: L0, x1 = x0, \u03b1 \u2212 1 = 0, \u03b10 = 1, t = 0. 2: Repeat 3: t = t + 1, \u03b2t = t \u2212 2 \u2212 1, ut = xt + \u03b2t (xt) (xt 1) 4: Text line search as follows: + 1, jl + 1"}, {"heading": "5 ADMM Projection algorithm", "text": "Before applying ADMM, we (8) transform (8) into an equivalent form as follows: minimize x1 2 x \u2212 v \u00b2 22 + x \u00b2 T (u \u2212 x) whenever we (w \u2212 x) + 2 x \u00b2 G \u2264 s2 u = x, w = x.The extended Lagrange form is: L (x \u2212 p \u00b2 22 + x \u00b2 T (u \u2212 p \u00b2) + x \u00b2 T (w \u2212 p \u00b2) + x \u00b2 T (w \u2212 p \u00b2) 2 x \u00b2 (w \u2212 p \u00b2).Using the scaled form [5], i.e., let it happen = p \u2212 p \u00b2 22 + x \u00b2 T (u \u2212 p \u00b2) + x \u00b2 T (u \u2212 p \u00b2 p \u2212 p \u00b2) + x \u00b2 T (u \u2212 p \u2212 p \u00b2 p \u2212 p \u00b2): L (x \u2212 p \u00b2 p \u00b2 p p \u00b2 p p p p \u00b2 p p p p \u00b2 t: 1 x \u00b2 p \u00b2 p \u00b2 p \u00b2 t: 2 x \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p = p = p p = p p = p = p p = p p = p p = p p = p p = p p = p p = p p p = p = p p p p = p p p = p p = p p p p = p p p p = p = p p p p = p p p p = p p p p - p = p = p p p - p = p = p p p p - p = p \u00b2 t: 0, p - p = p = p - p = p - p = p - p = p - p - p - p - p = p - p - p - p - p = p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p - p -"}, {"heading": "6 Dykstra\u2019s Algorithm", "text": "Dykstra's algorithm is a general scheme for calculating the projection on intersections of convex sets. It is executed by intelligently applying Euclidean projections to each convex set alternately, guaranteeing the convergence of the least square objective function [7]. The details of applying the Dykstra algorithm to our projection problem are set out in Algorithm 6.Algorithm 6 Dykstra's algorithm [7] for (8) Input: v, s1, s2 Output: an optimal solution x to (8) 1: Initialize: x0 = v, p0 = 0, q0 = 0, t = 0 2: repeat3: t = t + 1 4: yt \u2212 1 = Ps2G (xt \u2212 1 + pt \u2212 1) 5: pt = xt \u2212 1 \u2212 yt \u2212 1 6: xt = Ps11 (yt \u2212 1 + qt) \u2212 verge: 7 \u2212 qt: 1 \u2212 qt \u2212 1 \u2212 qt \u2212 converge: \u2212 8 \u2212 qt \u2212 qt"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Sparse feature selection has been demonstrated to be effective in handling high-dimensional data.<lb>While promising, most of the existing works use convex methods, which may be suboptimal in terms<lb>of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex<lb>paradigm to sparse group feature selection, which is motivated by applications that require identifying<lb>the underlying group structure and performing feature selection simultaneously. The main contributions<lb>of this article are twofold: (1) statistically, we introduce a nonconvex sparse group feature selection<lb>model which can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter<lb>estimation can be achieved; (2) computationally, we propose an efficient algorithm that is applicable to<lb>large-scale problems. Numerical results suggest that the proposed nonconvex method compares favorably<lb>against its competitors on synthetic data and real-world applications, thus achieving desired goal of<lb>delivering high performance.", "creator": "LaTeX with hyperref package"}}}