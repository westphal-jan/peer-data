{"id": "1205.6432", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2012", "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications", "abstract": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension $d$, and in particular from the class of halfspaces over $\\reals^d$. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the \\emph{approximation error} of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.", "histories": [["v1", "Tue, 29 May 2012 17:40:04 GMT  (29kb,D)", "https://arxiv.org/abs/1205.6432v1", null], ["v2", "Fri, 1 Jun 2012 14:12:58 GMT  (29kb,D)", "http://arxiv.org/abs/1205.6432v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "sivan sabato", "shai shalev-shwartz"], "accepted": true, "id": "1205.6432"}, "pdf": {"name": "1205.6432.pdf", "metadata": {"source": "CRF", "title": "Multiclass Learning Approaches: A Theoretical Comparison with Implications", "authors": ["Amit Daniely", "Sivan Sabato", "Shai Shalev-Shwartz"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "1.1 Related work", "text": "The problem with such a \"reduction\" analysis is that such an analysis becomes problematic when the underlying binary problems are very hard. In fact, our analysis shows that the underlying binary problems are too small to be left to chance, but in their experiments the number of classes is too small."}, {"heading": "2 Definitions and Preliminaries", "text": "We start with the binary classification that we associate with the child."}, {"heading": "3 Main Results", "text": "In Section 3.1 we analyze the sample complexity of the different hypotheses categories. We set lower limits for the natarajan dimensions of the different hypotheses categories, thus concluding in the light of theorem 2.1 a lower limit for the sample complexity of any algorithm. We also set upper limits for the graph dimensions of these hypotheses categories, which results in an upper limit for the estimation error of the ERM according to the same theorem. In Section 3.2 we analyze the approximation error of the different hypotheses categories."}, {"heading": "3.1 Sample Complexity", "text": "Together with Theorem 2.1, the following theorems estimate the sample complexity of the studied classes up to logarithmic factors. We note that these theorems support the rule of thumb that the half-value and graph dimensions are in the same order of magnitude of the number of parameters. The first theorem shows that the sample complexity of the MSVM depends on the sample complexity of the TC and ECOC. These methods are based on an underlying hypothesis of the class of binary classifiers. While our main focus is the case where the binary hypotheses category is half-value spaces above Rd, the upper limits are based on the sample complexity of TK and ECOC. These methods are based on an underlying hypotheses category of binary classifiers."}, {"heading": "3.2 Approximation error", "text": "First of all, we show that class L is essentially better than OvA and TC. This is expected as the complexity of sample T, provided, of course, that H is the class of half spaces in Rd. We find this result quite surprising, since the sample complexity of all of these classes is of the same order. The next result shows that this is not the case. Theorem 3.6 Any embedding in a higher dimension that allows HOvA or HT (for some tree T for k classes) would essentially involve L inHT or for OvA. The next result shows that this is not the case. Theorem 3.6 Any embedding in a higher dimension that allows HOvA or HT is necessarily embedded in a dimension of at least two dimensions (dk).The next theorem shows that the approximation error of AP is better than that of OvA and TC)."}, {"heading": "4 Proof Techniques", "text": "In this section we give a brief description of our most important evidentiary techniques. Most of our evidences for the estimation error results mentioned in Section 3.1 are based on a similar method that we describe now. Let L: {\u00b1 1} l \u2192 [k] be a multicasper reduction (e.g. a tree), and for H {\u00b1 1} X, designate L (H) = {x 7 \u2192 L (h1 (x),.. hl (x) l, | h1,. hl \u00b2 H}. Our upper limits for dG (L (H)) are largely based on the following simple Lemma.Lemma 4.1. If VC (H) = d then dG (L (H)) = O (ld ln (ld))).The technique for the lower limit to dN (L (W)), if W is the class of half spaces, we are very far from it."}, {"heading": "5 Implications", "text": "The first immediate implication of our results is that if the number of examples is set in training, this detailed analysis will be taken into account > We recommend preferring MSVM over OvA and TC. This certainly applies if the hypothesis class of MSVM, L, has a zero approximation error (the realizable case), since the ERM is then solvable in relation to L. Note that since the inclusions given in Theorem 3.5 are strict, there are cases where the data are realizable with MSVM, but not withHOvA or in relation to a tree. In the unrealizable case, the implementation of the ERM is untractable for all of these methods. Nevertheless, for each method there is reasonable heuristics to approach the ERM, which should work well if the approximation error is small. Therefore, we believe that MSVM is the method of choice in this case as well as Oftable for the lower approximation MCOM error."}, {"heading": "A Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Notation and Definitions", "text": "& & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W & # 246; W # 246; W & # 246; W # 246; W & # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W & # 246; W # 246; W # 246; W # 246; W # 246; W & # 246; W # 246; W & # 246; W # 246; W # 246; W # 246; W # 246; W & # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W & # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W & # 246; W # 246; W # 246; W # 246; W & # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246; W # 246"}, {"heading": "A.2 Multiclass SVM", "text": "The proof for theorem 3.1. the lower limit is derived from theorems 3.5 and 3.2. for the upper limit dG: (= dG (L), let S = {x1,. \u2212 \u2212 f) which is disrupted by L, and let f: S \u2192 [k] be a function which testifies to the fragmentation, for the x-x-x-x-x-x-x-x and the j-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x [k],. For each (i, j), [dG], [k], we define zi-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x."}, {"heading": "A.4 Trees", "text": "Let A X be a G-splintered theorem with | A | = dG (Htrees). By Sauer's lemmas, and since the number of trees is limited by kk, we have 2 | A | \u2264 kk \u00b7 | H | k \u2264 kk \u2264 kk \u00b7 | dk, i.e. dG (Htrees) = | A \u2212 O (dk)). To prove the lower limit, it is sufficient to show that dN (GlT) \u2265 d \u00b7 (k \u2212 1) for some l. We will take l = | N (T) | = k \u2212 1. Line sequence N (T) so that for each node v the nodes in the left subtree are smaller than the nodes in the corresponding right subtree. We will identify [l] with N (T)."}, {"heading": "A.5 ECOC, One vs. All and All Pairs", "text": "In order to prove the results for ECOC and its special cases, we must first prove a more general theory based on the idea of a sensitive vector for a certain code. [...] We say that a binary vector u = j = q = q = q = q (...) is closest to where q indices j (...) exist. [...] If there is a q-sensitive vector for a code M (...), then dN (...) is closest. [...], \u2212 u [...]. [...].Theorem A.4. If there is a q-sensitive vector for a code M (...), then dN (...) is.q.Proof. From Lemma A.1 it is sufficient to show that dN (...)."}, {"heading": "A.6 Approximation", "text": "The proof for Theorem 3.5. First, we show that for each tree of k classes T < p = > p = > p = < p = < p = < p = < p (p) p = < p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p (p) p) p (p) p (p) p (p) p) p (p) p) p (p) p) p (p) p (p) p (p) p) p (p) p (p) p (p) p) p (p) p (p) p) p (p) p (p) p (p) p) p (p) p (p) p) p (p) p (p) p) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p\" p. \"p\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p.\" p. \"p\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p\" p. \"p\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p"}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E.L. Allwein", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2000}, {"title": "Characterizations of learnability for classes", "author": ["S. Ben-David", "N. Cesa-Bianchi", "D. Haussler", "P. Long"], "venue": "n}-valued functions. Journal of Computer and System Sciences,", "citeRegEx": "Ben.David et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1995}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Multiclass classification with filter trees", "author": ["A. Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": null, "citeRegEx": "Beygelzimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2007}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer.,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer.", "year": 2001}, {"title": "Multiclass learnability and the erm principle", "author": ["A. Daniely", "S. Sabato", "S. Ben-David", "S. Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Daniely et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2011}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Classification by pairwise coupling", "author": ["Trevor Hastie", "Robert Tibshirani"], "venue": "The Annals of Statistics,", "citeRegEx": "Hastie and Tibshirani.,? \\Q1998\\E", "shortCiteRegEx": "Hastie and Tibshirani.", "year": 1998}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rifkin and Klautau.,? \\Q2004\\E", "shortCiteRegEx": "Rifkin and Klautau.", "year": 2004}, {"title": "Learning internal representations by error propagation", "author": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams"], "venue": "Parallel Distributed Processing \u2013 Explorations in the Microstructure of Cognition,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Convex polyhedron learning and its applications", "author": ["G. Takacs"], "venue": "PhD thesis, Budapest University of Technology and Economics,", "citeRegEx": "Takacs.,? \\Q2009\\E", "shortCiteRegEx": "Takacs.", "year": 2009}, {"title": "Support vector machines for multi-class pattern recognition", "author": ["J. Weston", "C. Watkins"], "venue": "In Proceedings of the Seventh European Symposium on Artificial Neural Networks,", "citeRegEx": "Weston and Watkins.,? \\Q1999\\E", "shortCiteRegEx": "Weston and Watkins.", "year": 1999}, {"title": "\u03b9). Combining with equations", "author": [], "venue": null, "citeRegEx": "\u25e6,? \\Q1998\\E", "shortCiteRegEx": "\u25e6", "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "A different reduction is the All-Pairs (AP) approach in which all pairs of classes are compared to each other [Hastie and Tibshirani, 1998].", "startOffset": 110, "endOffset": 139}, {"referenceID": 5, "context": "Rumelhart et al. [1986]).", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "These two approaches have been unified under the framework of Error Correction Output Codes (ECOC) [Dietterich and Bakiri, 1995, Allwein et al., 2000]. A tree-based classifier (TC) is another reduction in which the prediction is obtained by traversing a binary tree, where at each node of the tree a binary classifier is used to decide on the rest of the path (see for example Beygelzimer et al. [2007]).", "startOffset": 129, "endOffset": 403}, {"referenceID": 10, "context": "Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]).", "startOffset": 15, "endOffset": 41}, {"referenceID": 4, "context": "Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]).", "startOffset": 42, "endOffset": 68}, {"referenceID": 4, "context": "Vapnik [1998], Weston and Watkins [1999], Crammer and Singer [2001]). In this paper we analyze the Multiclass SVM (MSVM) formulation of Crammer and Singer [2001], in which each hypothesis is of the form hW (x) = argmaxi\u2208[k](Wx\u0304)i, where W is a k\u00d7 (d+ 1) matrix and (Wx\u0304)i is the i\u2019th element of the vector Wx\u0304 \u2208 R.", "startOffset": 42, "endOffset": 162}, {"referenceID": 3, "context": "If no prior knowledge on how to break the symmetry is known, it is suggested in Beygelzimer et al. [2007] to break symmetry by choosing a random permutation of the labels.", "startOffset": 80, "endOffset": 106}, {"referenceID": 6, "context": "The ECOC paradigm described in [Dietterich and Bakiri, 1995] proposes to choose a code with a large distance.", "startOffset": 31, "endOffset": 60}, {"referenceID": 5, "context": "Daniely et al. [2011] For every hypothesis classH, and for every ERM rule,", "startOffset": 0, "endOffset": 22}, {"referenceID": 5, "context": "2 improve results from Daniely et al. [2011] where it was shown that bd2cb k 2 c \u2264 dN (L) \u2264 O(dk log(dk)), and for every tree dG(HT ) \u2264 O(dk log(dk)).", "startOffset": 23, "endOffset": 45}, {"referenceID": 0, "context": "In particular, if the length of the code is O(log(k)), as suggested in Allwein et al. [2000], and the number of classes is \u03a9\u0303(d), then the code is expected to perform poorly.", "startOffset": 71, "endOffset": 93}], "year": 2012, "abstractText": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension d, and in particular from the class of halfspaces over R. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the approximation error of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.", "creator": "LaTeX with hyperref package"}}}