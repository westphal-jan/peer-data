{"id": "1206.1106", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "No more pesky learning rates", "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.", "histories": [["v1", "Wed, 6 Jun 2012 02:06:57 GMT  (629kb,D)", "http://arxiv.org/abs/1206.1106v1", "Submitted to NIPS 2012"], ["v2", "Mon, 18 Feb 2013 16:09:50 GMT  (1097kb,D)", "http://arxiv.org/abs/1206.1106v2", null]], "COMMENTS": "Submitted to NIPS 2012", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tom schaul", "sixin zhang", "yann lecun"], "accepted": true, "id": "1206.1106"}, "pdf": {"name": "1206.1106.pdf", "metadata": {"source": "CRF", "title": "No More Pesky Learning Rates", "authors": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "emails": ["schaul@cims.nyu.edu", "zsx@cims.nyu.edu", "yann@cims.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Major learning problems require algorithms that scale benevolently (e.g. sublinear), with the size of the dataset and the number of traceable parameters, and this has recently led to a resurgence of interest in stochastic gradient descent methods (SGD). In addition to rapid convergence, SGD has sometimes been observed to provide significantly better generalization errors than batch methods [1]. In practice, good performance with SGD requires some manual adjustments of the initial value of the learning rate (or step size) for each model and problem, as well as the design of an annealing plan for stationary data. The problem is particularly acute for non-stationary data. The contribution of this paper is a novel method of automatic adjustment of learning rates (possibly different learning rates for different parameters) to minimize some estimates of the expectation of loss at any point in time."}, {"heading": "2 Background", "text": "SGD methods have a long history in adaptive signal processing, neural networks and machine learning and have an extensive literature (see [2, 1] for current reviews).While the practical benefits of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever increasing amount of streaming data, to theoretical optimization results for generalization errors [4] and to competitions won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], in which quasi-Newton approximation ofar Xiv: 120 6.11 06v1 [st at.M L] 6J un2 01the Hessian was used within the SGD. Nevertheless, practitioners must deal with a sensitive hyperparameter tuning phase in order to achieve peak performance: each of the PASCAL tasks uses very different parameter settings, each of which is typically very easy to adjust."}, {"heading": "3 Optimal Adaptive Learning Rates", "text": "In this section, we briefly review the SGD and derive an optimal learning rate plan using an idealized quadratic and dividable loss function. We show that the use of this learning rate plan preserves convergence guarantees of the SGD. Subsequently, we find out how the optimal learning rate values can be estimated on the basis of available information and describe a few possible approximations. Samples, indexed by j, are generally taken from a data distribution P. Each sample contributes one loss per sample L (j) (\u03b8) to the expected loss: J (3) = Ej \u00b2 P [L (j) (\u03b8)] (1), where the detectable parameter vector is."}, {"heading": "3.1 Noisy Quadratic Loss", "text": "We assume that the pro-sample loss functions are smooth around minimums and can be approximated locally by a square function. We also assume that the minimum value of the pro-sample loss functions is zero: L (j) (\u03b8) = 1 2 (\u03b8 \u2212 c (j) > H (j) (2001) + Q (j) + H (2001) \u2212 c (j)) (2), where Hi is the (positive semi-definitive) Hessian matrix of the pro-sample loss of the sample j, and c (j) is the optimum for this sample. The distribution of the pro-sample optima c (j) has the mean and the deviation. Figure 1 illustrates the scenario in one dimension. To simplify the analysis, we assume that the remainders of this section are identical to the Hessians of the permitted losses for all samples, and that the problem is separable, i.e., we are ignoring the diagrams."}, {"heading": "3.2 Convergence", "text": "To prove this, we use classical techniques based on the Lyapunov stability theory [9]. Note that the expected loss follows E [J (\u03b8 (t + 1))) | \u03b8 (t)] = 1 2 h \u00b7 [\u03c32 (\u03b8 (t) \u2212 \u03b8) 2 + 2 (\u03b8 (t) \u2212 \u03b8) 2 + 2 (\u03b8) 2 + \u03c32] \u2264 J (\u03b8 (t)). Note therefore that J (\u03b8 (t)) is a positive supermartingale, which indicates that almost certainly J (\u03b8 (t)) \u2192 J. We have to prove that almost certainly J \u221e = J (\u03b8) = 12h\u043c 2. Note that J (\u03b8 (t) \u2212 E [J (TB) \u2212 J (T) \u2212 T (T) \u2212 T (T) is the limit of probability, and E [J] J (JT) \u2212 T (J) below the limit (T) \u2212 T (T) that the probability of the limit is almost zero."}, {"heading": "3.3 Global vs. Parameter-specific Learning Rates", "text": "The previous subsections considered the optimal learning rate in a one-dimensional case, which can be trivially generalized to d-dimensions if we assume that all parameters are separable by using an individual learning rate \u03b7-i for each dimension i. This is especially useful if the problem is poorly conditioned. Alternatively, we can derive an optimal global learning rate \u03b7-g as a consequence."}, {"heading": "4 Approximations", "text": "In practice, we are not confronted with the quantities \u03c3i, hi and (\u03b8 (t) i \u2212 certaini), but with the following quantities. However, on the basis of Equation 4, we can estimate them on the basis of the observed samples of the gradient: \u03b7 \u0432 i = 1 hi \u00b7 (E [\u0432 \u03b8i]) 2 (E [\u0432 \u03b8i]) 2 + V ar [\u043d\u0438\u0441\u0442i] = 1 hi \u00b7 (E [\u043d\u043d\u0438\u0442i] (9) The situation differs slightly from the global learning rate \u043e g. Here, we assume that it is possible to estimate the maximum curvature h + = maxi (hi) (which can be done efficiently, e.g. using the diagonal Hessian calculation method described in [3]). Then we have the limits g (t)."}, {"heading": "4.1 Approximate Variability", "text": "We use an exponential moving average with a time constant process constant (the approximate number of samples to be taken from the current memory) for online estimates: gi (t + 1) = (1 \u2212 2 \u2212 4) \u00b7 gi (t) \u2212 4 \u2212 4 (t + 1) = (1 \u2212 4) \u00b7 vi (t) + 4 (t) + 5 (2 \u2212 4) \u00b7 2 (t) \u2212 4 (t) \u00b7 1) \u00b7 l (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 2 (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 2 (t) \u00b7 2 (t) \u00b7 1) \u00b7 2 (t) \u00b7 2 (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 1 (t) \u00b7 2 (t) \u00b7 2 (t) \u00b7 1 (t) \u00b7 2 (t), (t) \u00b7 2 (t) \u00b7 2 (t), (t), (t), (t, (t), (), (t, (), (t, (t), (,), (t, (t,), (), (t, (t,), (t, (,), (t,), (, (t,), (, (t,), (t,), (, (t,), (, (t,), (,), (, (t,), (, (t), (,), (,), (,),."}, {"heading": "4.2 Approximate Curvature", "text": "There are a number of methods to obtain an online estimate of the diagonal Hessen. We apply the method \"bbprop,\" which calculates positive estimates of the diagonal Hessen terms for a single sample h (j) i, using a formula for backpropagation [3]. The diagonal estimates are proceeded at an exponentially moving average procedurehi (t + 1) = (1 \u2212 \u03c4 \u2212 1i) \u00b7 hi (t) + \u03c4 \u2212 1 i \u00b7 h (t) i. If the curvature for a component is close to zero, this can drive \u03b7 into infinity. Thus, to avoid numerical instability (to bind the conditional number of the approximate Hessen), we force a lower hi \u2265. This trick is not necessary in the global case, since h + = maxi (hi) is rarely zero."}, {"heading": "5 Adaptive SGD", "text": "The simplest version of the method considers each component in isolation.This form of algorithm is called \"vSGD\" (for \"variance-based SGD\").In realistic environments with high-dimensional parameter vectors, it is not clear from the outset whether it is best to have a single, global learning rate (which can be robustly estimated), or a set of local, dimensional, or block-specific learning rates (whose estimation will be less robust.The local vs. global choice can be made independently to estimate the Hessian diagonal terms and the correction term based on the variance of the gradient.The various combinations (which all have linear complexity in time and space.g.) are as follows: vSGD-ld uses local gradient terms (\"l\" for local) and local Hessian diagonal terms (\"d\" for diagonal Hessian estimates), with vessian terms vi = (\"gi\" 2) and vessional terms being global (\") terms."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Noisy Quadratic", "text": "To gain an intuitive understanding of the effects of the optimal adaptive learning rate method and the effects of approximation, we illustrate the oscillatory behavior of the SGD and compare the decrease in loss function and the resulting change in learning rates in the toy model of Section 3.1 (see Figure 2)."}, {"heading": "6.2 Non-stationary Quadratic", "text": "In realistic online learning scenarios, the curvature or noise level in a given dimension changes over time (for example, due to the effects of updating other parameters), and therefore learning rates must be reduced and increased. Of course, this cannot be achieved by a fixed learning rate or a fixed cooling plan. Figure 3 illustrates how vSGD, with its adaptive memory size, handles such cases appropriately; its initially high learning rate allows it to quickly approach the optimum, then it gradually decreases the learning rate as the deviation of the gradient increases relative to the square norm of the average gradient, bringing the parameter continuously closer to the optimum. If the data distribution changes abruptly, the algorithm automatically detects that the norm of average gradient increases in relation to variance, and the learning rate bounces back and adapts to the new circumstances."}, {"heading": "6.3 Experiments with Standard Benchmarks and Architectures", "text": "Experiments used a variety of architectures trained on the widely tested MNIST dataset [11] (with 60,000 training samples and 10,000 test samples), and the subset \"batch1\" of the CIFAR-10 dataset [12], which contains 20% of the complete training set (10,000 training samples, 10,000 test samples).The architectures tested included logistical regression (MNIST-1C, CIFAR-1C), which is convex in parameters, and fully connected 2 and 3-layer multilayer neural networks (-2C and -3C).These architectures used tanh nonlinearity for hidden units and were trained with cross-entropy loss. Loss is non-convex in the parameters for the multilayer architectures."}, {"heading": "7 Conclusion and Future Work", "text": "Based on the idealized case of square loss contributions from each sample, we have developed a method for calculating an optimal learning rate with each update and (possibly) for each parameter that optimizes the expected loss after the next update, based on the square norm of the expectation of the gradient and the expectation of the square norm of the gradient. Proof of the conversion of probability has been provided. We have shown various ways to approximate these learning rates in linear time and space in practice. Experimental results confirm the theoretical prediction: the adaptive learning rate method eliminates the need for manual adjustment of the learning rate or systematic search for its best value. In a number of experiments (not included in this paper due to lack of space), the method has proven to be very effective when used in online training scenarios with non-stationary signals."}, {"heading": "Acknowledgments", "text": "The authors thank Camille Couprie, Cle'ment Farabet and Arthur Szlam for the helpful discussions and Shane Legg for the title of the paper. This work was partly funded by the AFR Postdoctoral Scholarship No. 2915104 of the National Research Fund Luxembourg and the ONR Scholarship 5-74100-F6510."}], "references": [{"title": "The tradeoffs of large scale learning", "author": ["L Bottou", "O. Bousquet"], "venue": "Optimization for Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou"], "venue": "Online Learning and Neural Networks", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Efficient backprop", "author": ["Y LeCun", "L Bottou", "G Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Large scale online learning", "author": ["L Bottou", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A Bordes", "L Bottou", "P. Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A stochastic approximation method", "author": ["H Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1951}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["W. Xu"], "venue": "ArXiv-CoRR, abs/1107.2490,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["F Bach", "E. Moulines"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Stability and positive supermartingales", "author": ["R.S. Bucy"], "venue": "Journal of Differential Equations,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1965}, {"title": "The mnist dataset of handwritten digits", "author": ["Y LeCun", "C. Cortes"], "venue": "http://yann.lecun.com/exdb/mnist/", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, Department of Computer Science, University of Toronto,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Besides fast convergence, SGD has sometimes been observed to yield significantly better generalization errors than batch methods [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "SGD methods have a long history in adaptive signal processing, neural networks, and machine learning, with an extensive literature (see [2, 1] for recent reviews).", "startOffset": 136, "endOffset": 142}, {"referenceID": 0, "context": "SGD methods have a long history in adaptive signal processing, neural networks, and machine learning, with an extensive literature (see [2, 1] for recent reviews).", "startOffset": 136, "endOffset": 142}, {"referenceID": 2, "context": "While the practical advantages of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever-increasing amounts of streaming data, to theoretical optimality results for generalization error [4], and to competitions being won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], where Quasi-Newton approximation of", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "While the practical advantages of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever-increasing amounts of streaming data, to theoretical optimality results for generalization error [4], and to competitions being won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], where Quasi-Newton approximation of", "startOffset": 268, "endOffset": 271}, {"referenceID": 4, "context": "While the practical advantages of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever-increasing amounts of streaming data, to theoretical optimality results for generalization error [4], and to competitions being won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], where Quasi-Newton approximation of", "startOffset": 369, "endOffset": 372}, {"referenceID": 5, "context": "Originally proposed as \u03b7(t) = O(t\u22121) in [6], this form was recently analysed in [7, 8] from a non-asymptotic perspective to understand how hyper-parameters like \u03b70 and \u03b3 affect the convergence speed.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "Originally proposed as \u03b7(t) = O(t\u22121) in [6], this form was recently analysed in [7, 8] from a non-asymptotic perspective to understand how hyper-parameters like \u03b70 and \u03b3 affect the convergence speed.", "startOffset": 80, "endOffset": 86}, {"referenceID": 7, "context": "Originally proposed as \u03b7(t) = O(t\u22121) in [6], this form was recently analysed in [7, 8] from a non-asymptotic perspective to understand how hyper-parameters like \u03b70 and \u03b3 affect the convergence speed.", "startOffset": 80, "endOffset": 86}, {"referenceID": 8, "context": "To prove that, we follow classical techniques based on Lyapunov stability theory [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "Here we assume that it is feasible to estimate the maximal curvature h = maxi(hi) (which can be done efficiently, for example using the diagonal Hessian computation method described in [3]).", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "We adopt the \u201cbbprop\u201d method, which computes positive estimates of the diagonal Hessian terms for a single sample h i , using a back-propagation formula [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 9, "context": "Experiments used a variety of architectures trained on the widely tested MNIST dataset [11] (with 60,000 training samples, and 10,000 test samples), and the \u201cbatch1\u201d subset of the CIFAR-10 dataset [12], which contains 20% of the full training set (10,000 training samples, 10,000 test samples).", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Experiments used a variety of architectures trained on the widely tested MNIST dataset [11] (with 60,000 training samples, and 10,000 test samples), and the \u201cbatch1\u201d subset of the CIFAR-10 dataset [12], which contains 20% of the full training set (10,000 training samples, 10,000 test samples).", "startOffset": 197, "endOffset": 201}], "year": 2012, "abstractText": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.", "creator": "LaTeX with hyperref package"}}}