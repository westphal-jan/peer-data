{"id": "1206.1270", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "Factoring nonnegative matrices with linear programs", "abstract": "This paper describes a new approach for computing nonnegative matrix factorizations (NMFs) with linear programming. The key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features. More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X is approximately equal to CX and some linear constraints. The matrix C selects features, which are then used to compute a low-rank NMF of X. A theoretical analysis demonstrates that this approach has the same type of guarantees as the recent NMF algorithm of Arora et al. (2012). In contrast with this earlier work, the proposed method (1) has better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms. Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice. An optimized C++ implementation of the new algorithm can factor a multi-Gigabyte matrix in a matter of minutes.", "histories": [["v1", "Wed, 6 Jun 2012 16:42:27 GMT  (66kb,D)", "https://arxiv.org/abs/1206.1270v1", "16 pages, 10 figures"], ["v2", "Sat, 2 Feb 2013 23:40:56 GMT  (67kb,D)", "http://arxiv.org/abs/1206.1270v2", "17 pages, 10 figures. Modified theorem statement for robust recovery conditions. Revised proof techniques to make arguments more elementary. Results on robustness when rows are duplicated have been superseded by arxiv.org/1211.6687"]], "COMMENTS": "16 pages, 10 figures", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["ben recht", "christopher r\u00e9", "joel a tropp", "victor bittorf"], "accepted": true, "id": "1206.1270"}, "pdf": {"name": "1206.1270.pdf", "metadata": {"source": "CRF", "title": "Factoring nonnegative matrices with linear programs", "authors": ["Victor Bittorf", "Benjamin Recht", "Christopher R\u00e9", "Joel A. Tropp"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Nonnegative Matrix Factorization, Linear Programming, Stochastic Gradient Descent, Machine Learning, Parallel Computing, Multicore."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "2 Separable Nonnegative Matrix Factorizations and Hott Topics", "text": "It is not easy to solve the NMF problem. Vavasis showed that it is NP-complete to decide whether a matrix leads to negative factorization. [4] It is notoriously difficult to solve the NMF problem. [5] Vavasis showed that it is NP-complete to decide whether a matrix can lead to negative factorization. [6] Vavasis showed whether a matrix leads to negative factorization."}, {"heading": "3 Main Theoretical Results: NMF by Linear Programming", "text": "A major advantage of this formulation is that it scales to very large sets of data. Here's the key observation: Suppose Y is an arbitrary f \u00b7 n non-negative matrix that allows a rank-r separable factorization Y = FW. If we fill F with zeros to form a f \u00b7 f matrix, we have Y = ig [Ir 0 M 0], Y =: CYWe call the matrix factorization localization. Note that any factorization that locates matrix C is an element of the polyhedron set (Y): = {C \u2265 0: CY, Tr (C) = r, Cjj \u2264 1: j, Cij \u2264 Cjj \u0432i, j}. Thus, to find an exact NMF of Y, it is sufficient to find a feasible element of C (Y)."}, {"heading": "3.1 Robustness to Noise", "text": "Eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven, eleven,"}, {"heading": "3.2 Related Work", "text": "Interpolative decomposition such as rank-revealing QR [14] and CUR [19] have favorable efficiency characteristics compared to factorizations (such as SVD) that are not based on examples. Factorization localization has been used in subspace clusters and has proven robust against outliers [9, 23]. In recent work on dictionary learning, Esser et al. and Elhamifar et al. have proposed a factorization solution for nonnegative matrix factorization using group sparseness techniques [8,10]. Esser et al. demonstrate asymptotic exact restoration in a restricted sound model, but this result requires pre-processing to remove duplicated or nearly duplicated rows. Elhamifar shows an exact representative restoration in the noiseless setting assuming that no hott topics are duplicated."}, {"heading": "4 Incremental Gradient Algorithms for NMF", "text": "The rudiments of our rapid implementation are based on two standardized optimization techniques: dual decomposition and incremental gradients. Both techniques are described in depth in chapters 3.4 and 7.8 of Bertsekas and Tstisklis (5)."}, {"heading": "4.1 Sparsity and Computational Enhancements for Large Scale.", "text": "For small problems, Hottopixx can be implemented in a few lines of Matlab code, but for the very large data sets examined in Section 5, we use natural parallelism and a variety of low-level optimizations that are also enabled by our formulation. As in any numerical program, memory layout and cache behavior can be critical factors for performance. We use standard techniques: in-memory clustering to increase prefetching capabilities, advanced data structures for better cache alignment, and compiler policies that allow the Intel compiler to apply vectorization. Note that the incremental gradient step (Step 6 in Algorithm 4) only changes C entries where X \u00b7 k is non-zero, so we can parallelize the algorithm to either store the rows or columns of C. We store X in large contiguous blocks of memory to promote hardware prefetching."}, {"heading": "5 Experiments", "text": "Apart from the acceleration curves, all experiments were performed on an identical configuration: a two-stage Xeon X650 (6 cores) with 128GB RAM each. The kernel is Linux 2.6.32-131.In small-scale synthetic experiments, we compared Hottopixx with the AGKM algorithm and the linear programming formulation of algorithms 3 implemented in Matlab. Both AGKM and algorithm 3 were coupled with CVX [13] to use the SDPT3 solution [25]. We ran Hottopixx for 50 epochs with primary step size 1e-2 and double step size. We match F with two cleaning stages of incremental gradients for all three algorithms. To generate our instances, we sampled hott curves in the Simplex unit in Rn."}, {"heading": "6 Discussion", "text": "This paper provides an algorithmic and theoretical framework for the analysis and use of any factorization problem that can be posed as a linear (or convex) factorization program. Future work should examine the applicability of Hottopixx to other factorization localization algorithms such as subspace clusters and reconsider earlier theoretical limitations of this state of the art."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the authors for behaving in the way they have done in the past years."}, {"heading": "A Proofs", "text": "Let us be a non-negative matrix whose lines lead to one. Let us assume that Y assumes an exact, separable factorization of the order."}, {"heading": "B Projection onto \u03a60", "text": "To calculate the number of \u03a60 projections, please note that we can calculate the projection with one column each. Also, the projection for each individual column (after permutation of column entries) is {x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>This paper describes a new approach, based on linear programming, for computing nonneg-<lb>ative matrix factorizations (NMFs). The key idea is a data-driven model for the factorization<lb>where the most salient features in the data are used to express the remaining features. More<lb>precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X \u2248 CX<lb>and some linear constraints. The constraints are chosen to ensure that the matrix C selects<lb>features; these features can then be used to find a low-rank NMF of X. A theoretical analysis<lb>demonstrates that this approach has guarantees similar to those of the recent NMF algorithm<lb>of Arora et al. (2012). In contrast with this earlier work, the proposed method extends to more<lb>general noise models and leads to efficient, scalable algorithms. Experiments with synthetic and<lb>real datasets provide evidence that the new approach is also superior in practice. An optimized<lb>C++ implementation can factor a multigigabyte matrix in a matter of minutes.", "creator": "TeX"}}}