{"id": "1206.1529", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2012", "title": "Sparse projections onto the simplex", "abstract": "The past decade has seen the rise of $\\ell_1$-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard $\\ell_1$-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation.", "histories": [["v1", "Thu, 7 Jun 2012 15:33:12 GMT  (399kb,D)", "http://arxiv.org/abs/1206.1529v1", "13 Pages"], ["v2", "Thu, 14 Jun 2012 07:21:01 GMT  (399kb,D)", "http://arxiv.org/abs/1206.1529v2", "13 Pages"], ["v3", "Thu, 10 Jan 2013 16:33:23 GMT  (361kb,D)", "http://arxiv.org/abs/1206.1529v3", "10 Pages"], ["v4", "Thu, 28 Mar 2013 15:01:33 GMT  (1182kb,D)", "http://arxiv.org/abs/1206.1529v4", "9 Pages"], ["v5", "Wed, 10 Apr 2013 08:39:10 GMT  (1350kb,D)", "http://arxiv.org/abs/1206.1529v5", "9 Pages"]], "COMMENTS": "13 Pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["anastasios kyrillidis", "stephen becker", "volkan cevher", "christoph koch 0001"], "accepted": true, "id": "1206.1529"}, "pdf": {"name": "1206.1529.pdf", "metadata": {"source": "CRF", "title": "Sparse projections onto the simplex", "authors": ["Anastasios Kyrillidis", "Stephen Becker", "Volkan Cevher"], "emails": ["volkan.cevher}@epfl.ch", "stephen.becker@upmc.fr"], "sections": [{"heading": "1 Introduction", "text": "In this context, we prefer frugal solutions, even if there are other solutions that could lead to better loss values, but which are also backed up by rigorous theoretical analysis. For example, frugality has been shown to avoid a better generalization of learning algorithms, and it has been shown to circumvent the ill-defined nature of regression not only with improved interpretation possibilities, but also with rigorous theoretical analysis. Frugality inherently leads to learning problems that are not desirable according to conventional wisdom. For example, in order to obtain a f (\u03b2) minimizer with specific frugality, we have to deal with a non-convective constraint."}, {"heading": "2 Preliminaries", "text": "Problem wording: In this paper we mainly deal with the following problem and its variant: Problem 1. In view of a vector w-Rp, we find a Euclidean projection of w on the intersection of s-sparse signals and the standard simplex implementation. Problems 1-2 are special cases of mixed integer programming and are generally difficult to combine. However, there are well-built solvers based on good industry and bound heuristics, such as CPLEX [14]. Subsequently, we prove that problem 1 can be optimally solved. Therefore, we use CPLEX as a benchmark for our relaxed solution to the problem 2. Projections: We write PLAN for the projector in either the general or the general context, which means x."}, {"heading": "3 Efficient sparse projections onto the simplex", "text": "Imagine that C (\u03b2) is a figure that encodes the constraint K\u03bb. For example, we can evaluate C (\u03b2) = \u2211 i \u03b2i so C (\u03b2) = \u03bb the simplex constraint. We first make an elementary observation based on Remark 2. Given S? = supp (\u03b2?), we can find \u03b2? by projecting wS? to K\u03bb within the s-dimensional space. So, the difficulty is S?. Algorithm 1 suggests an obvious greedy approach. We choose the sentence S? by naive projection w on \u0432 s s s s s s s s s s s s s s s s s s s s. Remarkably, this results in the correct support for problem 1 as we prove in \u00a7 4.1. After we have found the support, we project (limited to the support) back on KIS. We call this algorithm the greedy selector and simplex projector."}, {"heading": "4 Main results", "text": "Note 3. If the symbol S is used as S = supp (\u03b2), where \u03b2 = P\u0121s (w) stands for any w elements, then we magnify if | S | < s, S until it has s elements, taking the first s \u2212 | S | elements that are not already in S and defining \u03b2 = 0 on these elements. The lexicographic approach is used to break connections when there is more than one solution to problem 1."}, {"heading": "4.1 Projection onto the standard simplex with cardinality constraints", "text": "Theorem 1. Algorithm 1 solves exactly the problem 1. Proof. Through Remark 2. (Remark 2.) we divide the problem into the task of finding the support and then finding the values on the support. \u2212 Given any support S \u2212 \u2212 \u2212 p = 0, whereS? arg min S: S-p (w)) S \u2212 w = arg max S: S-p = arg max S: S-p (w) S \u00b2 22 \u2212 p = 0, whereS \u2212 arg min S: S \u00b2 s (w) S \u2212 p (w) S \u00b2 2 = arg max S: S \u00b2 s (w) S \u00b2 22 \u2212 p (w) \u2212 p \u00b2 s max S: S \u00b2 s (w2i \u2212 p)."}, {"heading": "4.2 Projection onto the general simplex with cardinality constraints", "text": "We are in a position in which we have to specialize on this problem. (Theorem 2: \"For all \u03bb except for a series of finite scales, Algorithm 2 calculates the projection from p to p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p p = p p = p = p = p = p = p p = p = p = p = p = p = p = p = p p = p = p = p = p p = p = p = p p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Portfolio optimization", "text": "In view of a covariance matrix, we naturally lead to limitations of cardinalism in the optimisation process. This additional development leads to mixed approaches with regard to optimisation techniques."}, {"heading": "5.2 Sparse kernel density estimation", "text": "We leave x (1), x (2), x (n), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (2), x (2), x (2), x (2), x (2), x (2), x (1), x (1), x (1), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (2), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1, x (1), x (1), x (1), x (1), x (1, x (1), x (1), x (1), x (1), x (1), x (1, x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1), x (1, x (1), x (1), x (1), x (1), x (1), x (1), x (1), x ("}, {"heading": "5.3 Sparse projections onto the general simplex", "text": "While the general Simplex constraints also have applications with Markowitz portfolio selection, it is better to use synthetic data to evaluate the quality and efficiency of our projection on the general Simplex. Here, we generate random vector realizations w-Rp with p = {102, 104} and compare the projection performance of the following approaches across various s and \u03bb: (i) the GSSP based on what we do, (ii) the proximal alternating minimizing projection (PAMP) [26], (iii) the commercially mixed integer-square programming toolbox CPLEX Studio V12.3 [14], and (iv) the RSSP based on the proximal minimizing projection (PAMP approach) [26], (iii) the commercial, square programming combination CPLEX Studio V12.3 [14], and general (ixRP) based on an attempt to solve the problem."}, {"heading": "6 Conclusions", "text": "While the non-convexity of learning algorithms is traditionally undesirable, many problems may be difficult to avoid. In particular, sparse learning problems with simplex constraints prevent the application of \"1 relaxation to economize on the solution. In this context, we show how to efficiently obtain sparse projections on positive and general simplex and sparsity constraints. To this end, we provide an accurate sparse projector for the positive simplex constraints and a relaxation-based approach for approximate sparse projections on the general simplex and sparsity constraints. Interestingly, the latter approach can provide online optimization guarantees. We also empirically demonstrate that our projectors offer significant benefits in generalization and interpretability for sparse portfolio selection and density estimation applications."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the European Commission within the framework of the grant MIRG-268398, the ERC Future Proof, the SNSF 200021 132548 and the DARPA KeCoM Program # 11-DARPA-1055. VC would also like to recognize Rice University for its faculty fellowship, and SB would like to recognize the Fondation Sciences Maths \u0301 matiques de Paris for its fellowship."}], "references": [{"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Computational learning theory, pages 23\u201337. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Statistics for High-Dimensional Data: Methods, Theory and Applications", "author": ["P. B\u00fchlmann", "S. Van De Geer"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput., 24(2):227\u2013234", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms", "author": ["H. Attouch", "J. Bolte", "B.F. Svaiter"], "venue": "forward\u2013backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, pages 1\u201339", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "An O(n) algorithm for quadratic knapsack problems", "author": ["P. Brucker"], "venue": "Operations Research Letters, 3(3):163\u2013166", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1984}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "A generalized approach to portfolio optimization: Improving performance by constraining portfolio norms", "author": ["V. DeMiguel", "L. Garlappi", "F.J. Nogales", "R. Uppal"], "venue": "Management Science, 55(5):798\u2013812", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse and stable Markowitz portfolios", "author": ["J. Brodie", "I. Daubechies", "C. De Mol", "D. Giannone", "I. Loris"], "venue": "Proceedings of the National Academy of Sciences, 106(30):12267\u201312272", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiscale poisson intensity and density estimation", "author": ["R.M. Willett", "R.D. Nowak"], "venue": "Information Theory, IEEE Transactions on, 53(9):3171\u20133187", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "SPADES and mixture models", "author": ["F. Bunea", "A.B. Tsybakov", "M.H. Wegkamp", "A. Barbu"], "venue": "The Annals of Statistics, 38(4):2525\u20132558", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust ensemble learning", "author": ["G. Ratsch", "B. Scholkopf", "A.J. Smola", "S. Mika", "T. Onoda", "K.R. Muller"], "venue": "Advances in Neural Information Processing Systems, pages 207\u2013220", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Combinatorial selection and least absolute shrinkage via the CLASH algorithm", "author": ["A. Kyrillidis", "V. Cevher"], "venue": "EPFL Technical Report", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Hard thresholding with norm constraints", "author": ["A. Kyrillidis G. Puy", "V. Cevher"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Integer and combinatorial optimization", "author": ["G.L. Nemhauser", "L.A. Wolsey"], "venue": "volume 18. Wiley New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Risk reduction in large portfolios: Why imposing the wrong constraints helps", "author": ["R. Jagannathan", "T. Ma"], "venue": "The Journal of Finance, 58(4):1651\u20131684", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Algorithm for cardinality-constrained quadratic optimization", "author": ["D. Bertsimas", "R. Shioda"], "venue": "Computational Optimization and Applications, 43(1):1\u201322", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Portfolio selection problems in practice: a comparison between linear and quadratic optimization models", "author": ["F. Cesarone", "A. Scozzari", "F. Tardella"], "venue": "Arxiv preprint arXiv:1105.3594", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Heuristics for cardinality constrained portfolio optimisation", "author": ["T.J. Chang", "N. Meade", "J.E. Beasley", "Y.M. Sharaiha"], "venue": "Computers and Operations Research, 27(13):1271\u20131302", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Hybrid search for cardinality constrained portfolio optimization", "author": ["M.A. Gomez", "C.X. Flores", "M.A. Osorio"], "venue": "Proceedings of the 8th annual conference on Genetic and evolutionary computation, pages 1865\u20131866. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "YALMIP: A toolbox for modeling and optimization in MATLAB", "author": ["J. Lofberg"], "venue": "Computer Aided Control Systems Design, 2004 IEEE International Symposium on, pages 284\u2013289. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Least squares mixture decomposition estimation", "author": ["D. Kim"], "venue": "PhD thesis", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The annals of mathematical statistics, 33(3):1065\u20131076", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1962}, {"title": "Alternating minimization and projection methods for nonconvex problems", "author": ["H. Attouch", "J. Bolte", "P. Redont", "A. Soubeyran"], "venue": "Arxiv preprint arXiv:0801.1780", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Benchmarking optimization software with performance profiles", "author": ["E.D. Dolan", "J.J. Mor\u00e9"], "venue": "Mathematical Programming, 91(2):201\u2013213", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 1, "context": "For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "Surprisingly, it is possible to obtain sparse critical points of general loss functions with the projected gradient algorithm [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "The Euclidean projection onto the convex constraint \u2016\u03b2\u20161 \u2264 \u03bb can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions.", "startOffset": 111, "endOffset": 117}, {"referenceID": 5, "context": "The Euclidean projection onto the convex constraint \u2016\u03b2\u20161 \u2264 \u03bb can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions.", "startOffset": 111, "endOffset": 117}, {"referenceID": 1, "context": "Moreover, if f(\u03b2) is convex, then we can leverage decades of research in convex optimization, which not only provide computationally efficient algorithms to obtain accurate solutions, but also strong analytical tools to establish consistency and prediction efficiency of the solutions as \u03bb varies [2].", "startOffset": 297, "endOffset": 300}, {"referenceID": 6, "context": "As a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector \u03b2 that minimizes a return-adjusted empirical risk objective [7, 8].", "startOffset": 165, "endOffset": 171}, {"referenceID": 7, "context": "As a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector \u03b2 that minimizes a return-adjusted empirical risk objective [7, 8].", "startOffset": 165, "endOffset": 171}, {"referenceID": 6, "context": "The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8].", "startOffset": 135, "endOffset": 140}, {"referenceID": 7, "context": "The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8].", "startOffset": 135, "endOffset": 140}, {"referenceID": 8, "context": "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].", "startOffset": 81, "endOffset": 88}, {"referenceID": 9, "context": "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].", "startOffset": 81, "endOffset": 88}, {"referenceID": 10, "context": "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "The closest works to ours are the papers [12, 13], where the authors recover a vector \u03b2 in regression setting, where \u03b2 is already sparse and within a convex norm-ball constraint.", "startOffset": 41, "endOffset": 49}, {"referenceID": 12, "context": "The closest works to ours are the papers [12, 13], where the authors recover a vector \u03b2 in regression setting, where \u03b2 is already sparse and within a convex norm-ball constraint.", "startOffset": 41, "endOffset": 49}, {"referenceID": 11, "context": "While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.", "startOffset": 18, "endOffset": 25}, {"referenceID": 12, "context": "While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.", "startOffset": 18, "endOffset": 25}, {"referenceID": 11, "context": "The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15].", "startOffset": 95, "endOffset": 102}, {"referenceID": 13, "context": "The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15].", "startOffset": 95, "endOffset": 102}, {"referenceID": 14, "context": "The projector onto the general simplex is [16]", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "1 Portfolio optimization Given a sample covariance matrix \u03a3 and mean \u03bc, the Markowitz mean-variance (MV) framework selects a portfolio \u03b2 = R (without short positions) via \u03b2 \u2208 arg \u03b2\u2208\u22061 [ \u03b2\u03a3\u03b2 \u2212 \u03b1\u03bc\u03b2 ] , where \u22061 encodes the normalized capital constraint, and \u03b1 trades off risk and return [7, 8].", "startOffset": 285, "endOffset": 291}, {"referenceID": 7, "context": "1 Portfolio optimization Given a sample covariance matrix \u03a3 and mean \u03bc, the Markowitz mean-variance (MV) framework selects a portfolio \u03b2 = R (without short positions) via \u03b2 \u2208 arg \u03b2\u2208\u22061 [ \u03b2\u03a3\u03b2 \u2212 \u03b1\u03bc\u03b2 ] , where \u22061 encodes the normalized capital constraint, and \u03b1 trades off risk and return [7, 8].", "startOffset": 285, "endOffset": 291}, {"referenceID": 16, "context": "Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein.", "startOffset": 79, "endOffset": 87}, {"referenceID": 17, "context": "Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein.", "startOffset": 79, "endOffset": 87}, {"referenceID": 6, "context": "We refer the reader to [7, 17] for an excellent discussion.", "startOffset": 23, "endOffset": 30}, {"referenceID": 15, "context": "We refer the reader to [7, 17] for an excellent discussion.", "startOffset": 23, "endOffset": 30}, {"referenceID": 16, "context": "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).", "startOffset": 184, "endOffset": 196}, {"referenceID": 18, "context": "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).", "startOffset": 184, "endOffset": 196}, {"referenceID": 19, "context": "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).", "startOffset": 184, "endOffset": 196}, {"referenceID": 7, "context": "Red solid curve denotes the quadratic programming solution as obtained by (9) and blue squares represent a variation of `1-norm regularized solver in [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 20, "context": "3 [14] using YALMIP [23] and our positive simplex, cardinality constrained, projected gradient method.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "written as follows [10, 24]", "startOffset": 19, "endOffset": 27}, {"referenceID": 21, "context": "written as follows [10, 24]", "startOffset": 19, "endOffset": 27}, {"referenceID": 22, "context": "We compare the density estimation performance of: (i) the Parzen window method [25], (ii) the quadratic programming formulation in (11), and (iii) our cardinality-constrained version of (11) using GSSP.", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "Here, we generate random vector realizations w \u2208 R with p = {10, 10} and compare the projection performance of the following approaches over various s and \u03bb: (i) the GSSP on \u03a3s\u2229\u2206\u03bb, (ii) the proximal alternating minimization projection (PAMP) [26], (iii) the commercial mixed integer quadratic programming toolbox CPLEX Studio V12.", "startOffset": 242, "endOffset": 246}, {"referenceID": 24, "context": "Then, we use the performance profile notion [27] where \u03c1(\u03b1) is the probability for a solver that the performance ratio ei,j min{ei,j :i\u2208I} is within a factor \u03b1 \u2208 R of the best possible ratio.", "startOffset": 44, "endOffset": 48}], "year": 2017, "abstractText": "The past decade has seen the rise of `1-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard `1-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation.", "creator": "LaTeX with hyperref package"}}}