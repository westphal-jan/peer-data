{"id": "1206.3137", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2012", "title": "Identifiability and Unmixing of Latent Parse Trees", "abstract": "This paper explores unsupervised learning of parsing models along two directions. First, which models are identifiable from infinite data? We use a general technique for numerically checking identifiability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models. Second, for identifiable models, how do we estimate the parameters efficiently? EM suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences. We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models.", "histories": [["v1", "Thu, 14 Jun 2012 15:21:24 GMT  (95kb,D)", "http://arxiv.org/abs/1206.3137v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["daniel j hsu", "sham m kakade", "percy liang"], "accepted": true, "id": "1206.3137"}, "pdf": {"name": "1206.3137.pdf", "metadata": {"source": "CRF", "title": "Identifiability and Unmixing of Latent Parse Trees", "authors": ["Daniel Hsu", "Sham M. Kakade", "Percy Liang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Notation", "text": "For a positive integer n, define [n] def = {1,.., n} and < n > = {e1,.., en}, where ei is the vector that is 1 in component i and 0 elsewhere. For integers a, b [n], a n = (a \u2212 1) n + b [n2] should be the integer encoding of the pair (a, b). For a matrix pair A, B Rm \u00b7 n, define the column-by-column tensor product A c B Rm2 \u00b7 n such that (A c B) (i1 mi2) j = Ai1jBi2j. For a matrix A Rm \u00b7 n, A \u2020 should denote the Moore-Penrose pseudoinverse."}, {"heading": "3 Parsing models", "text": "A sentence is a sequence of L-words, x = (x1,.., xL), where each word is one of d possible word types. A (generative) parsing model defines a common distribution P\u03b8 (x, z) over a sentence x and its parse tree z (will be specified later), where \u03b8 are the model parameters (a collection of multinomials). Each parse tree z has a topology topology (z) that varies both unnoticed and across sentences. The learning problem is that only examples from x.Two important classes of models of natural language syntax are restored: constituency models that represent a hierarchical grouping and labeling of phrases of a sentence (e.g. Figure 1 (a)), and dependency models that represent pairs of relationships between the words of a sentence (e.g. Figure 1 (b))."}, {"heading": "3.1 Constituency models", "text": "A constituency tree z = (V, s) consists of a series of nodes V > and a collection of hidden states s = {sv} v \u00b2 V. Each state sv \u00b2 < k > represents one of k's possible syntactical categories. Each node v has the form [i: j] for 0 \u2264 i < j \u2264 L according to the phrase between positions i and j of the sentence. These nodes form a binary tree as follows: the root node is [0: L]. Leave topology (z) an integer coding of V with j \u2212 i > 1, there exists a unique m with i < m < j definition of the two child nodes [i: m]."}, {"heading": "3.2 Dependency tree models", "text": "Unlike constituency trees, which postulate internal nodes with latent states, dependency trees connect the words directly to each other. A dependency tree z is a series of directed edges (i, j), where i, j, [L] 1 Normally, a PCFG induces a topology about a state-dependent probability to choose a binary production against an emission. Our model is a constraint corresponding to a state-independent probability. Are different positions in the sentence. Let root (z) have the position of the root node of z. Consider only projective dependence trees [27]: z is projective if for each path from i to j to k in z we have j and k on the same side of i (i.e. j \u2212 i and k \u2212 i have the same sign). Let topology (z) have an integer encoding of z-DEP-I. Let's consider the simple dependency model of [4]."}, {"heading": "4 Identifiability", "text": "Our goal is to estimate model parameters that are too strong for two reasons. First, models often have natural symmetries, for example, phenomena of phenomena of phenomena of phenomena of phenomena of phenomena. Specifically, we ask ourselves a fundamental question: Is it possible from information theory to identify phenomena (x) of observed moments of phenomena? (x) Def = E\u03b80 [x)? To be more precise, we define the equivalence class of phenomena 0 in order to determine the parameters of phenomena that produce the same observed moments: Sample (x) = {\u03b8 of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena of phenomena 0)."}, {"heading": "4.1 Observation functions", "text": "An observation function \u03c6 (x) and the associated observation moments \u00b5 (\u03b80) = E\u03b80 [\u03c6 (x)] reveals aspects of the distribution P\u03b80 (x). For example, \u03c6 (x) = x1 would only reveal the boundary distribution of the first word, whereas \u03c6 (x) = x1 \u00b7 \u00b7 xL shows the total distribution of x. There is a compromise: higher moments provide more information, but are more difficult to estimate reliably since finite data is available, and are also more expensive from a mathematical point of view. In this paper we consider the following intermediate moments: \u03c612 (x) def = x1 x2 \u03c6 (x) def = (x) def = (xi xj) def = (xi xj): i, j def = (x) def = (x) def = (x1 x2 x3), def = (xi x3), def = (xi x2)."}, {"heading": "4.2 Automatically checking identifiability", "text": "s theorem [10,11], but (i) it is not directly applicable to models with random topologies, and (ii) there are only sufficient conditions for identification, and it cannot be used to determine non-identifiability, since it is common practice to explore many different models for a given problem in quick succession, we would quickly and reliably verify identifiability. In this section, we are developing an automatic procedure to do so. To establish identifiability, let us examine the algebraic structure of targets 0, where we assume that the parameter space is defined as an open subset of [0, 1] n.2 Recall that Solls (0)."}, {"heading": "4.4 Identifiability of constituency and dependency tree models", "text": "We have verified the identification status of various constituency and dependency tree models using our implementation of CheckIdentifiability. We focus on the regime in which d \u2265 k is used for PCFGs; additional results for d < k are included in Appendix B. The results are presented in Figure 2. Firstly, we have determined that the PCFG is unidentifiable by all (and therefore not unidentifiable by all results); we believe that the same is true for all L. This negative result motivates research into limited subclasses of PCFGs, such as PCFGI and PCFG-IE, which factorise binary productions. 3 For these classes, we have found that the sentence length L and the choice of the observation function can influence identifiability: both models are sufficiently identifiable for L (e.g. L \u2265 3) and with a sufficiently rich observation function (e.g. E 123\u03b7)."}, {"heading": "5 Unmixing algorithms", "text": "Once we have determined which parameter models are identifiable, we will now turn to the parameter estimation for these models. We will look at algorithms based on moment matching - those that try to find a \u03b8 satisfactory \u00b5 (\u03b8) = u for some u. Typically, u is an empirical estimate of \u00b5 (\u03b80) = E\u03b80 [\u03c6 (x)] based on samples x \u0445 P\u03b80.4In general, the solution of \u00b5 (\u03b8) = u corresponds to the search for solutions for systems of multivariate polynomials, which is NP-hard [28]. However, \u00b5 (\u03c6) often has an additional structure that we can exploit. For example, for a HMM, the sliced moments of third order \u00b5123\u03b7 (\u03b8) can be written in \u03b8 as a product of parameter matrices, and each matrix can be restored by decomposition of the product [1]. For parsing models, the challenge is that the topology is random, but the product is not a mixture of individual moments."}, {"heading": "5.1 General case", "text": "We assume that the observation function \u03c6 (x) consists of a collection of observation matrices that can be composed. (X) o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o-o (e.g. for o = (i, j), \u03c6o (x) = xi xj). In view of an observation matrix-o (x) and a topology t over topologies, we will consider a finite number of such mappings based on this topology. We refer to these mappings as composite parameters that are composed."}, {"heading": "5.2 Application to the PCFG-IE model", "text": "As a concrete example, we consider the PCFG IE model over L = 3 words. Write A = > Setpoint = > Setpoint (Setpoint) = > Setpoint (Setpoint) = > Setpoint (Setpoint) = > Setpoint (Setpoint) = > Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint: Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint): Setpoint (Setpoint):::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::"}, {"heading": "5.3 Application to the DEP-IES model", "text": "s leave D = diag (\u03c0) = diag (A\u03c0), whereby the second equality is due to the stationarity of \u03c0.\u00b51 def = E [x1] = \u03c0, \u00b512 def = E [x1 x2] = 7 \u2212 1 (DA > + DA > + DA > + AD), \u00b513 def = E [x1 x3] = 7 \u2212 1 (DA > A > + DA > + AD + AD), \u00b5 \u0445 12 def = E [x1 x2] = 2 \u2212 1 (DA > + AD), where E [\u00b7] is taken with regard to the length of 2 sentences."}, {"heading": "6 Discussion", "text": "In this paper, we have shed some light on the identifiability of standard generative parsing models using our numerical identifiability check. Given the ease with which this checker can be applied, we believe that it should be a useful tool for analyzing more complex models [6], as well as for developing new models that are meaningful yet identifiable. There is still a large gap between the representation of identifiability and the development of explicit algorithms. We have made some progress in closing this model with our separation technique, which can handle models where tree topology does not vary trivially."}, {"heading": "A Proof of Theorem 1", "text": "Theorem 1 (restored).Evidence of Theorem 1 is crucially reliant on the following problem: \"We have only one case that is not empty.\" [0, 1] n and \u00b5: Rn \u2192 Rm is a polynomial card. [0, 2, 3, 4, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "B Additional results from the identifiability checker", "text": "PCFG models with d < k. The PCFG models we have looked at so far assume that the number of words d is at least the number of hidden states k, which is a realistic assumption for natural language. However, there are applications, such as computer biology, where the word size d is relatively small. In this regime, identifiability becomes more difficult because the data does not reveal so much about the hidden states and brings us closer to the boundary between identifiability and non-identifiability. In this section, we look at the d < k regime. The following table returns additional identifiability of the results from CheckIdentifiability for the values d, k, and L, where d < k (remember that the results in Section 4.4 are considered only as values where d) k). In each cell, we show the (k, d, L) values for CheckIdentifiability back \"yes\"; the values tested were < k < {2, 8}, {, 8, d, k"}, {"heading": "C Dynamic programs", "text": "For a set of length L, the number of parsection trees is exponential in L. Therefore, dynamic programming is often used as an efficient calculation of expectations about the parsection trees, the core calculation in the E level of the EM algorithm. In the case of PCFG, this dynamic program is called the CKY algorithm, which in both cases expires in O (L3k3) time, in which k includes the number of hidden states. In this paper, we must calculate the states of the dynamic program in both cases as the spans [i: j] of the set (and for the PCFG) the set (and for the PCFG) of hidden states z (i: j) of the nodes (i) of the Jacobian matrix for testing the ability (Section 4.2) and (ii) of the mixing matrix for gathering the relationships (Section 5.1)."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "<lb>This paper explores unsupervised learning of parsing models along two directions. First,<lb>which models are identifiable from infinite data? We use a general technique for numerically<lb>checking identifiability based on the rank of a Jacobian matrix, and apply it to several stan-<lb>dard constituency and dependency parsing models. Second, for identifiable models, how do<lb>we estimate the parameters efficiently? EM suffers from local optima, while recent work using<lb>spectral methods [1] cannot be directly applied since the topology of the parse tree varies across<lb>sentences. We develop a strategy, unmixing, which deals with this additional complexity for<lb>restricted classes of parsing models.", "creator": "LaTeX with hyperref package"}}}