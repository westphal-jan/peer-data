{"id": "1206.3318", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2012", "title": "On Local Regret", "abstract": "Online learning aims to perform nearly as well as the best hypothesis in hindsight. For some hypothesis classes, though, even finding the best hypothesis offline is challenging. In such offline cases, local search techniques are often employed and only local optimality guaranteed. For online decision-making with such hypothesis classes, we introduce local regret, a generalization of regret that aims to perform nearly as well as only nearby hypotheses. We then present a general algorithm to minimize local regret with arbitrary locality graphs. We also show how the graph structure can be exploited to drastically speed learning. These algorithms are then demonstrated on a diverse set of online problems: online disjunct learning, online Max-SAT, and online decision tree learning.", "histories": [["v1", "Thu, 14 Jun 2012 20:07:30 GMT  (185kb,D)", "http://arxiv.org/abs/1206.3318v1", "This is the longer version of the same-titled paper appearing in the Proceedings of the Twenty-Ninth International Conference on Machine Learning (ICML), 2012"]], "COMMENTS": "This is the longer version of the same-titled paper appearing in the Proceedings of the Twenty-Ninth International Conference on Machine Learning (ICML), 2012", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael bowling", "martin zinkevich"], "accepted": true, "id": "1206.3318"}, "pdf": {"name": "1206.3318.pdf", "metadata": {"source": "CRF", "title": "On Local Regret", "authors": ["Michael Bowling", "Martin Zinkevich"], "emails": ["bowling@cs.ualberta.ca", "maz@yahoo-inc.com"], "sections": [{"heading": "1 Introduction", "text": "An online learning task involves the repeated taking of action and, after an action has been selected, the observation of the outcome of that action. This is in contrast to offline learning, where decisions are made based on a fixed amount of training data. Consequently, offline learning typically requires assumptions about how the results of actions are generated (based on training data and all future data). No such assumptions are required in online learning. Instead, the metric of performance used is regret: the added benefit that could be achieved if an alternative sequence of action had been chosen. The set of alternative sequences that are taken into account defines the term regret. Regret is more than just a measure of performance, although it also conducts algorithms. For specific notions of regret, there are no-regret algorithms for which total regret increases in the worst case sublinear time, so their average regret goes back to zero."}, {"heading": "2 Background", "text": "For all t, maxa, b, b, ut (a) \u2212 ut (b) \u2212 ut (a)), where 1 (condition) equals 1 if the condition is true: 120 6.33 18v1 [cs. AI] 14 Jun 20 and 0 otherwise. We can use this block to define the traditional terms of regret. RTintern = max a, b) \u2212 ut (a), where 1 (condition) equals 1 if the condition is true: 120 6.33 18v1 [cs. AI] 14 Jun 20 and 0 otherwise. RTintern = max a, b) \u2212 ut (a), b) RT, + a, b R T swap = 2, the condition is a condition."}, {"heading": "2.1 Infinite Action Spaces", "text": "This paper looks at situations where A is infinite. To keep the notation simple, we will use maximum operations over actions to mean suprema operations and sums over actions to mean the suprema of sum over all finite subsets of actions. Since we will focus on regret over a finite period of time, there will always be only a finite set of actually selected actions, and therefore only a finite number of non-zero regrets, RTa, b. The sums over actions will always be considered limited to that finite period of time. None of the three traditional regret concepts lends itself well to making A infinite. Not only does | A | appear within the limits of regret, but one can show that it is impossible to have no regrets over actions in some finite cases. Consider A = N and let it be a step function, so ut (a) = 1 if a > yt appears for some yt and 0 otherwise."}, {"heading": "3 Local Regret Concepts", "text": "Leave G = (V, E) E RT, + a, b (4) G = a, b) G = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, b) D = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = a, c = b = a, c = b = a, c = a, c = b = a, c = a, c = b = b =."}, {"heading": "4 An Algorithm for Local Swap Regret", "text": "The algorithm essentially selects actions according to the stationary distribution of a Markov process on the graph plane, with transition probabilities at the edges being proportional to the accumulated repentance. However, there are two cavities needed to handle infinite graphics: It prevents playing beyond a certain distance from a certain root vertex, and there is an internal bias regarding the actual actions chosen.Formally, let the root of any particular vertex be. Define d1 to be the unweighted shortest path between two vertex. Define the level of a vertex as its distance from root: L (v) = d1 (root, v) Note that, L (root), and L (j)."}, {"heading": "5 Exploiting Locality Structure", "text": "The local swap repentance algorithm in the previous section successfully drops any dependence on the size of the action set and can therefore even be applied to infinite action sets. However, the occurrence of | EL | in the limit in theorem 3 as | EL | HO (DL) is undesirable, and L is rather 100 than 2 to keep the first term of the limit low. Thus, the limit offers practically little more than an asymptotic guarantee for even the simplest setting of example 1. In this section, we will appeal to (i) the structure in the local chart and (ii) local external regret to achieve a more practical repentance limit and an algorithm."}, {"heading": "5.1 Cartesian Product Graphs", "text": "We begin by considering the case of G, which has a very strong structure, where it can be completely dissected into a series of product diagrams. In this case, we can show that by independently minimizing the local remorse in the product diagrams, we can minimize the local remorse in the full graphic.Theorem 4. LetG is a Cartesian product of graphs, G = G1. Gk whereGl = (Vl, El). For all l 1,.., k}, define utl: Vl \u2192 R, so that utl (al) = ut (at1,.., a t l l \u2212 1, al, a t, a k), so utl is a supply function on the lth component of the action at the time t assuming the other components.Leave E [l], which change only on the lth component, so {E] l = 1,..."}, {"heading": "5.2 Color Regret", "text": "It is not as if it is a product that does not fit a product structure, but only a product that does not fit a product structure. (...) We can summarize these edges and only worry about the regret of the group. (...) We generalize this fact to graphs that do not fit a product structure. (...) An edge coloring C = {Ci} i = 1,2, an arbitrary chart G = 1,2, an arbitrary chart G = 1,2, an arbitrary chart G = 1,2, an arbitrary chart G = 1,2, an arbitrary chart G with an edge length of E: Ci E, and Ci Cj = 3. (...) We are in favor of an arbitrary chart G with an edge length of E: Ci E, and Ci Cj = 5. (...)"}, {"heading": "6 Experimental Results", "text": "In the previous section, algorithms were introduced that minimize the local swap and the local external regret (by minimizing the locally colored regret).The limits of regret are not dependent on the size of the graph beyond the degree of the graph and therefore provide a guarantee for infinite graphs.We are now investigating the practicality of these algorithms and illustrate the universality of the concepts by applying them to a variety of online problems.The first two tasks we are examining, Online Max 3SAT and Online Decision Tree Learning, have not yet been investigated in the online environment.The last task, Online Disjunct Learning, has already been researched and will help illustrate some drawbacks of local regret.In all three areas, we are examining two algorithms. The first minimizes the local swap regret by applying (DL / (L + 1), L) Reue Matching with L, which is specifically selected for the problem, called \"Local Swap.\""}, {"heading": "6.1 Online Max-3SAT", "text": "First, we look at Example 1. We randomly constructed problem cases with n = 20 Boolean variables and 201 clauses with 3 literals each that regret. On each timeframe, the algorithms selected a mapping of the variables, a clause was randomly selected from the sentence, and the algorithm was given a tool of 1 if the mapping met the clause, 0 otherwise. This was repeated for 1000 timeframes. The local diagram used was the n-dimensional hypercube from Example 1. In both cases, we set L = and b = 0, because the limits do not depend on L once it exceeds 20. This has also achieved the best performance for both algorithms. The average results over 200 randomly constructed sets of clauses are shown in 2% clauses."}, {"heading": "6.2 Online Decision Tree Learning", "text": "Second, we look at three sets of data from the UCI Machine Learning Repository (each with categorical inputs and a large number of instances): Nursery, Mushroom and King-Rocker versus King-Pawn [Frank and Asuncion, 2010]. Categorical attributes were converted into Boolean attributes (which simplified the implementation of localization diagrams) by having a separate Boolean function for each attribute."}, {"heading": "6.3 Online Disjunct Learning", "text": "In fact, the number of errors that have to be made in the instance is very high, since it is limited by the number of attribute errors. (That is, the number of attribute-attribute-attributes that have to be satisfied in the instance.) In these experiments, we compare our algorithms with the number of attribute-attribute-errors. (That is, the number of attribute-attribute-attribute-attribute-attribute-attribute-attribute-attribute-attribute-attribute-attribute-attribute-attribute-fulfillment-instance-processing.) In these experiments, we will compare our algorithm performance with that of Winnow2.000. (This means that we have two learning processes-tasks-processing-processing-editing-processing-fulfillment-instance-fulfillment-processing-processing-processing-processing-processing-processing-processing-processing-processing-processing-processing-processing-processing-processing-on Winnow2.000.)"}, {"heading": "7 Conclusion", "text": "We then presented algorithms to minimize these concepts, even if the number of hypotheses is infinite; we also demonstrated that we can exploit the structure in the diagram to achieve narrower boundaries and better performance; these new regret concepts mimic local search methods that are common approaches to offline optimization with insolubly hard hypotheses spaces; as such, our concepts and algorithms allow us to provide online guarantees with similar flavor to their offline counterparts with these hypotheses spacings; there are a number of interesting approaches for future work as well as open hypotheses; permissible colorations can lead to radically improved boundaries and empirical performance; how can such permissible colorations be constructed for general diagrams? What graph structures lead to exponentially small permissible colorations compared to the size of the diagram? We can easily constructively reduce the result to a minimum of color."}, {"heading": "Acknowledgements", "text": "This work was supported by NSERC and Yahoo! Research, where the first author was a visiting scientist at the time of the research."}, {"heading": "A Proof for Local Swap Regret", "text": "The Blackwell condition is (roughly speaking) that the probability flows into an action, equal to the probability flow from an action. In the variant here, there are two ways to see this flow. Definition f is such that for all (i, j) -E, fi, j (i) -E, fi (i, j) -E, fi (i, j) -E, fi (i) -E, fi (i) -E, fi (i) -E, fi (i) -E, + i, j. \"Implicit depends on the time t, but we point out how we always refer to a time t. This flow f is similar to the flows in Hart and Mas-Colell, as they apply to the Blackwell condition. However, we lack the conservation of the flow property. So we consider a second flow f\" that satisfies the conservation of flow. To do this, we look at the planes of the graph."}, {"heading": "B Proof for Color Regret", "text": "The edge colouring is such that c (i, j) = c (i) + c (i) + c (i) + c (i, j) + c (i) = c (i) = c (i) = c (i) = c (i) = c (i) = c (i) = c (i) = c (i), c) + c (i) = c (i), c) = c (i) = c (i) = c (i), c (c) + c (i), c (c) = c (c), c), c), c (c), c (c), c), c), c), c), c (c), c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c (c), c, c (c), c (c), c (c), c, c (c), c (c, c (c), c, c (c), c (c (c), c, c (c), c, c (c (c), c, c (c), c (c, c, c (c), c (c, c), c (c (c), c, c (c), c (c (c), c (c), c, c (c), c, c (c (c), c (c), c, c (c, c (c), c (c), c (c, c, c (c), c (c), c (c), c (c, c (c), c (c), c (c), c, c (c (c), c (c), c (c), c, c (c), c, c (c), c (c), c (c"}, {"heading": "C Decision Tree Graphs", "text": "A decision tree is a representation of a hypothesis (S) = false."}, {"heading": "If these rules do not apply, it is not on the shortest path.", "text": "Suppose that A is our current tree. Suppose that C = Rp, (v, l1, l2) (A). First, we determine that if the conditions are met, the edge is on the shortest path. Note that if v is on the path p in B, and there is a leaf or other decision node on path p in A, then v is in structural disagreement. So, if we replace this node with v, we reduce the structural disagreement. However, we must be careful not to increase any disagreement with the leaves. If for any nodes from v in B, they are corrected in A, then the disagreement with the leaves will not increase. Therefore, by reducing the structural disagreement by 1, we decrease the distance by 1.1, which means that the edge is on the shortest path. Second, we cannot traverse the conditions one by one to realize any violated condition."}], "references": [{"title": "An analog of the minimax theorem for vector payoffs", "author": ["D. Blackwell"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "Blackwell.,? \\Q1956\\E", "shortCiteRegEx": "Blackwell.", "year": 1956}, {"title": "From external to internal regret", "author": ["A. Blum", "Y. Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blum and Mansour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Mansour.", "year": 2007}, {"title": "A general class of no regret learning algorithms and game-theoretic equilibria", "author": ["A. Greenwald", "A. Jafari"], "venue": "In Proceedings of the Sixteenth Annual Conference on Learning Theory,", "citeRegEx": "Greenwald and Jafari.,? \\Q2003\\E", "shortCiteRegEx": "Greenwald and Jafari.", "year": 2003}, {"title": "Approximation to bayes risk in repeated plays", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "A simple adaptive procedure leading to correlated equilibrium", "author": ["S. Hart", "A. Mas-Colell"], "venue": null, "citeRegEx": "Hart and Mas.Colell.,? \\Q2002\\E", "shortCiteRegEx": "Hart and Mas.Colell.", "year": 2002}, {"title": "A wide range no-regret theorem", "author": ["E. Lehrer"], "venue": "Games and Economic Behavior,", "citeRegEx": "Lehrer.,? \\Q2003\\E", "shortCiteRegEx": "Lehrer.", "year": 2003}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["N. Littlestone"], "venue": "Machine Learning,", "citeRegEx": "Littlestone.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone.", "year": 1988}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan.,? \\Q1993\\E", "shortCiteRegEx": "Quinlan.", "year": 1993}, {"title": "Local search strategies for satisfiability testing", "author": ["B. Selman", "H. Kautz", "B. Cohen"], "venue": null, "citeRegEx": "Selman et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Selman et al\\.", "year": 1993}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In Twentieth International Conference on Machine Learning,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 4, "context": "Internal regret [Hart and Mas-Colell, 2002] is the maximum utility that could be gained if one action had been chosen in place of some other action.", "startOffset": 16, "endOffset": 43}, {"referenceID": 2, "context": "Swap regret [Greenwald and Jafari, 2003] is the maximum utility gained if each action could be replaced by another.", "startOffset": 12, "endOffset": 40}, {"referenceID": 3, "context": "External regret [Hannan, 1957], which is the original pioneering concept of regret, is the maximum utility gained by replacing all actions with one particular action.", "startOffset": 16, "endOffset": 30}, {"referenceID": 9, "context": "For example, if A is a compact, convex subset of R and the utilities are convex with bounded gradient on A, then you can minimize regret even though A is infinite [Zinkevich, 2003].", "startOffset": 163, "endOffset": 180}, {"referenceID": 1, "context": "Swap regret [Greenwald and Jafari, 2003] is the maximum utility gained if each action could be replaced by another. External regret [Hannan, 1957], which is the original pioneering concept of regret, is the maximum utility gained by replacing all actions with one particular action. This is the most relaxed of the three concepts, and while the others must concern themselves with |A|2 possible regret values (for all pairs of actions) external regret only need worry about |A| regret values. So although the guarantee is weaker, it is a simpler concept to learn which can make it considerably more attractive. These three regret notions have the following relationships. R internal \u2264 R swap \u2264 |A|R internal R external \u2264 R swap (3) 2.1 Infinite Action Spaces This paper considers situations where A is infinite. To keep the notation simple, we will use max operations over actions to mean suprema operations and summations over actions to mean the suprema of the sum over all finite subsets of actions. Since we will be focused on regret over a finite time period, there will only ever be a finite set of actually selected actions and, hence only a finite number of non-zero regrets, R a,b. The summations over actions will always be thought to be restricted to this finite set. None of the three traditional regret concepts are well-suited to A being infinite. Not only does |A| appear in the regret bounds, but one can demonstrate that it is impossible to have no regret in some infinite cases. Consider A = N and let u be a step function, so u(a) = 1 if a > y for some y and 0 otherwise. Imagine y is selected so that Pr[a > yt|u1,...,T\u22121, a1,...,T\u22121] \u2264 0.001, which is always possible. Essentially, high utility is always just beyond the largest action selected. Now, consider y\u2217 = 1 + maxt\u2264T y. In expectation 1 T \u2211T t=1 u (at) \u2264 0.001 while 1 T \u2211T t=1 u t(y\u2217) = 1 (i.e., there is large internal and external regret for not having played y\u2217,) so the average regret cannot approach zero. Most attempts to handle infinite action spaces have proceeded by making assumptions on both A and u. For example, if A is a compact, convex subset of R and the utilities are convex with bounded gradient on A, then you can minimize regret even though A is infinite [Zinkevich, 2003]. We take an alternative approach where we make use of a notion of locality on the set A, and modify regret concepts to respect this locality. Different notions of locality then result in different notions of regret. Although this typically results in a weaker form of regret for finite sets, it breaks all dependence of regret on the size ofA and allows it to even be applied whenA is infinite and u is an arbitrary (although still bounded) function. Wide range regret methods Lehrer [2003] can also bound regret with respect to a set of (countably) infinite \u201calternatives\u201d, but unlike our results, their asymptotic bound does not apply uniformly across the set, and uniform finite-time bounds depend upon a finite action space Blum and Mansour [2007].", "startOffset": 13, "endOffset": 2765}, {"referenceID": 1, "context": "Wide range regret methods Lehrer [2003] can also bound regret with respect to a set of (countably) infinite \u201calternatives\u201d, but unlike our results, their asymptotic bound does not apply uniformly across the set, and uniform finite-time bounds depend upon a finite action space Blum and Mansour [2007].", "startOffset": 277, "endOffset": 301}, {"referenceID": 8, "context": ", WalkSAT [Selman et al., 1993]) on the maximum satisfiability problem, an offline task where all of the clauses are known up front.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": "5 [Quinlan, 1993].", "startOffset": 2, "endOffset": 17}, {"referenceID": 4, "context": "4 An Algorithm for Local Swap Regret We now present an algorithm for minimizing local swap regret, similar to global swap regret algorithms [Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003], but with substantial differences.", "startOffset": 140, "endOffset": 195}, {"referenceID": 2, "context": "4 An Algorithm for Local Swap Regret We now present an algorithm for minimizing local swap regret, similar to global swap regret algorithms [Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003], but with substantial differences.", "startOffset": 140, "endOffset": 195}, {"referenceID": 4, "context": "These probabilities are always computed according to the following requirement, which is a generalization of [Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003].", "startOffset": 109, "endOffset": 164}, {"referenceID": 2, "context": "These probabilities are always computed according to the following requirement, which is a generalization of [Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003].", "startOffset": 109, "endOffset": 164}, {"referenceID": 4, "context": "There are two distinguishing factors of our algorithm from [Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003]: R\u0303 6= R, and past a certain distance from the root, we loop back.", "startOffset": 59, "endOffset": 114}, {"referenceID": 2, "context": "There are two distinguishing factors of our algorithm from [Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003]: R\u0303 6= R, and past a certain distance from the root, we loop back.", "startOffset": 59, "endOffset": 114}, {"referenceID": 0, "context": "The overall structure of the proof is similar to [Blackwell, 1956; Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003] with a few significant changes.", "startOffset": 49, "endOffset": 121}, {"referenceID": 4, "context": "The overall structure of the proof is similar to [Blackwell, 1956; Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003] with a few significant changes.", "startOffset": 49, "endOffset": 121}, {"referenceID": 2, "context": "The overall structure of the proof is similar to [Blackwell, 1956; Hart and Mas-Colell, 2002; Greenwald and Jafari, 2003] with a few significant changes.", "startOffset": 49, "endOffset": 121}, {"referenceID": 8, "context": "We also ran WalkSAT [Selman et al., 1993] offline on the set of 201 clauses, and on average it was able to satisfy all but 4% of the clauses, which gives an offline lower bound for what is possible.", "startOffset": 20, "endOffset": 41}, {"referenceID": 6, "context": "This task has received considerable attention, notably the celebrated Winnow algorithm [Littlestone, 1988], which is guaranteed to make a finite number of mistakes if the instances can be perfectly classified by some disjunction.", "startOffset": 87, "endOffset": 106}], "year": 2012, "abstractText": "Online learning aims to perform nearly as well as the best hypothesis in hindsight. For some hypothesis classes, though, even finding the best hypothesis offline is challenging. In such offline cases, local search techniques are often employed and only local optimality guaranteed. For online decision-making with such hypothesis classes, we introduce local regret, a generalization of regret that aims to perform nearly as well as only nearby hypotheses. We then present a general algorithm to minimize local regret with arbitrary locality graphs. We also show how the graph structure can be exploited to drastically speed learning. These algorithms are then demonstrated on a diverse set of online problems: online disjunct learning, online Max-SAT, and online decision tree learning.", "creator": "TeX"}}}