{"id": "1206.4599", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Unified Robust Classification Model", "abstract": "A wide variety of machine learning algorithms such as support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA), exist for binary classification. The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVM become applicable to MPM and FDA, and vice versa. Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and propose a non-convex optimization algorithm that can be applied to non-convex variants of existing learning methods.", "histories": [["v1", "Mon, 18 Jun 2012 14:39:39 GMT  (752kb)", "http://arxiv.org/abs/1206.4599v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akiko takeda", "hiroyuki mitsugi", "takafumi kanamori"], "accepted": true, "id": "1206.4599"}, "pdf": {"name": "1206.4599.pdf", "metadata": {"source": "META", "title": "A Unified Robust Classification Model", "authors": ["Akiko Takeda", "Hiroyuki Mitsugi", "Takafumi Kanamori"], "emails": ["takeda@ae.keio.ac.jp,", "kiyurohi7@z2.keio.jp", "kanamori@is.nagoya-u.ac.jp"], "sections": [{"heading": "1. Introduction", "text": "Support Vector Machine (SVM) is one of the most successful classification algorithms in modern machine learning (Scholkopf & Smola, 2002).The Minimax Probability Machine (MPM) (FDA et al., 2002) and Fisher Discriminant Analysis (FDA) (FDA) (Fukunaga, 1990) also address the binary classification of invisible methods.Their problem assumes that only the mean and covariance matrix of each class is known. MPM's optimal hyperlevel is determined by minimizing the worst-case probability (maximum) of misclassification of invisible methods.The FDA expects to find a direction that maximizes sesesAppearing at the 29th International Conference on Machine Learning, Edinburgh, UK, 2012."}, {"heading": "2. Unified Robust Classification Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Problem Settings", "text": "We begin with the introduction of problem definition and notation; the observed training samples are referred to as (xi, yi) \"Rd \u00b7 {+ 1, \u2212 1}\"; let M + be referred to as indexes for training samples labeled + 1; likewise for M \u2212; let M + | = m + and | M \u2212 | = m \u2212, where | \u00b7 | shows the size of the sentence; the aim of the classification task is to obtain a classifier that minimizes the prediction error rate for invisible test samples; for simplicity, we will focus on linear classifiers, i.e. x > w + b, where w (most likely Rd) is a vector and b (most likely R) is a prediction error parameter; most of the discussions in this work can be applied directly to training points (Scholkopf & Smola, 2002)."}, {"heading": "2.2. Properties of RCM", "text": "In the geometric meaning of U + and U \u2212 are disjoint. (0) The optimal meaning of U + and U \u2212 are. (0) The optimal meaning of U + and U \u2212 is only given if there is an optimal difference between the two cases. (0) The geometric meaning of U + and U \u2212 is equivalent to the question of whether U + or not. (0) As shown in Theorem 2.2, there is a large difference in the calculation between the two cases. (0) Before introducing an intuitive geometric interpretation of RCM in Theorem 2.2, we introduce Lemma 2.1, which further separates the case of U in two cases: U includes 0 inside, int (U) int (U), at its boundary, bd (U). (0 6) stops when U + and U \u2212 are disjoint. (0) implies that U + and U \u2212 are disjoint."}, {"heading": "3. Equivalence to Existing Classifiers", "text": "We will show that RCM can be reduced to support vector machine (SVM), minimax probability machine (MPM) or Fisher discriminant analysis (FDA) depending on the prescribed uncertainty group U. In Table 1, \"\u00b7\" means that the corresponding cases never occur. \"\u221a\" means that, as far as we know, there are no corresponding existing models. The models indicated by \u221a are the objective of this paper. We designate an optimal solution of (2) as w * and define the concept of bias b such that the decision limit exceeds the center of x * + and x * \u2212, i.e. b = \u2212 (x * + x *) > w * / 2. Here, x * +, U + and x * * \u2212 U \u2212 represent the optimal solutions of inner minimization in (2) for w = w *."}, {"heading": "3.1. Hard-Margin SVM, \u03bd-SVM and E\u03bd-SVM", "text": "When a dataset is linearly separable, there are many hyperplanes that correctly classify all training samples. Vapnik-Chervonenki's theory shows that a large margin classifier has a small generalization error, which can be turned into a quadratic programming problem, and the classification method is called the Hard Margin Support Vector Classification Machine (HM-SVM), where we define the uncertainty (convex fuselage, CH) as U \u00b1 = conv {xi), (6) where conv means a convex vector is hull hull. By using wolf duality, the equivalence of HM-SVM and RCM (4) is evident for U \u2212 =. HM-SVM has been expanded to deal with non-separable data. C-SVM (Cortes & Vapnik, 1995) and Vapnik-SVM (SVM-Marche, 2000) are examples of \"SVM\" soft."}, {"heading": "3.2. Minimax Probability Machine and Its Extension", "text": "The Minimax probability machine (MPM) uses only the mean and covariance matrix of each class for classification tasks (Lanckriet et al., 2002). Suppose that x + (or x \u2212) is a d-dimensional random vector with mean x + (or x +) and covariance \u03a3 + (or \u03a3 \u2212). We assume that x + 6 = x \u00b2 and \u03a3 \u2212 are positively defined. MPM minimizes the misclassification probabili-ties under the worst-case setting as follows: max \u03b1, w, b \u03b1 s.t. inf x \u00b1 \u0445 (x \u00b1, \u03a3 \u00b1) Pr {x > \u00b1 w + b 0}. \u2212 D \u2212 D (9), where x + (x +, \u03a3 +) refers to the class of distributions that have an average of x + and a covariance of x but are otherwise arbitral."}, {"heading": "3.3. Fisher Discriminant Analysis and Its Extension", "text": "In the Fisher Discriminant Analysis (FDA) as in MPM (9), a discriminatory hyperplane is calculated from the means and covariances of the random vectors x + and x \u2212 and the hyperplane is determined from the optimal solution w * to the following problem (Fukunaga, 1990): max w (x \u00b2 + \u2212 x \u00b2) > w \u00b2 (\u03a3 + \u03a3 \u2212) 1 / 2w \u00b2. (14) The problem finds a direction that maximizes the projected class variance while minimizing class variance in this direction. Likewise, the FDA has a probable interpretation for MPM under the worst-case scenario. Using the ellipsoidal uncertainty defined by ULA = {x = (x \u00b2 + (x + \u2212 x \u2212 x \u2212), + (TA + + +) 1 / 2U \u00b2 (15) FDA (14), the ellipsoidal uncertainty can be replaced by UBA = (16)."}, {"heading": "4. Statistical Interpretation for RCM", "text": "We can give a statistical interpretation for RCM based on statistical learning theory."}, {"heading": "5. Solution Method for RCM", "text": "The RCM has a much wider range of parameters than an existing convex model such as MPM, MM-MPM, FDA or FS-FD (see Table 1).Therefore, the RCM improves the ability to improve these existing classification models.Perez-Cruz et al. (2003) have experimentally demonstrated that the generalization performance of the E\u03bd-SVM is often better than that of the original \u03bd-SVM. In this section, we propose a solution method generalized by the local algorithms of the (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008)."}, {"heading": "5.1. Two-stage Optimization Strategy", "text": "Suppose we solve RCM (2) with the set uncertainty U\u03b7 with a parameter \u03b7 and that U\u03b71-int (U\u03b72) applies to \u03b71 < \u03b72. Let us define \u03b7max in such a way that the optimal value of (2) with U = U\u03b7max is zero. First, we must calculate \u03b7max to confirm that the given problem (2) is essentially convex or not. Algorithm 5.1. The parameter \u03b7max is obtained as the optimal solution to the convex problem: min \u03b7\u03b7 0. (20) If U\u03b7 \u00b1 ellipsoid sets are of (11) (or (15), the problem is reduced to MPM (10) (or FDA (16). If U\u03b7 \u00b1 RCHs are, the problem is reduced to a linear programming problem and gives us a minute. If the input parameter \u03b7 is equal, we have already obtained an optimal solution (20)."}, {"heading": "5.2. Local Optimization Algorithm for Non-convex RCM", "text": "For us it is a problem that differs only in the presence of a reversible convex-convex-convex-convex-convex-convex-convex-convex-convex-convex-convex-convex-convex-program. If all functions are linear, the RCP problem is called in particular a linear reverse-convex-convex-program (LRCP)."}, {"heading": "6. Conclusions", "text": "We developed the Rugged Classification Model (RCM), a model that includes SVM, MPM, and FDA for specific uncertainty sets. Choosing the uncertainty set is important in this model. This model enables the application of extensions and improvements to SVM to MPM and FDA and vice versa. The unified model will be helpful in clarifying the relationships between existing models and in finding new classifiers and new algorithms. That is, we could develop a new classifier by finding a reasonable uncertainty set for RCM. It will be important to see how the learning algorithm, uncertainty set, and predictive accuracy relate to each other."}], "references": [{"title": "Geometric intuition and algorithms for E\u03bd-svm", "author": ["\u00c1. Barbero", "A. Takeda", "J. L\u00f3pez"], "venue": null, "citeRegEx": "Barbero et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Barbero et al\\.", "year": 2012}, {"title": "Robust Optimization", "author": ["A. Ben-Tal", "L. El-Ghaoui", "A. Nemirovski"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "Duality and geometry in SVM classifiers", "author": ["K.P. Bennett", "E.J. Bredensteiner"], "venue": "In ICML, pp", "citeRegEx": "Bennett and Bredensteiner,? \\Q2000\\E", "shortCiteRegEx": "Bennett and Bredensteiner", "year": 2000}, {"title": "Second order cone programming formulations for feature selection", "author": ["C. Bhattacharyya"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bhattacharyya,? \\Q2004\\E", "shortCiteRegEx": "Bhattacharyya", "year": 2004}, {"title": "Minimum distance to the complement of a convex set: Duality result", "author": ["W. Briec"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Briec,? \\Q1997\\E", "shortCiteRegEx": "Briec", "year": 1997}, {"title": "A geometric interpretation of \u03bd-SVM classifiers", "author": ["D.J. Crisp", "C.J.C. Burges"], "venue": "In NIPS, pp", "citeRegEx": "Crisp and Burges,? \\Q2000\\E", "shortCiteRegEx": "Crisp and Burges", "year": 2000}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": null, "citeRegEx": "Fukunaga,? \\Q1990\\E", "shortCiteRegEx": "Fukunaga", "year": 1990}, {"title": "A robust minimax approach to classification", "author": ["G.R.G. Lanckriet", "Ghaoui", "L. El", "C. Bhattacharyya", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2002}, {"title": "Optimization by Vector Space Methods", "author": ["D.G. Luenberger"], "venue": null, "citeRegEx": "Luenberger,? \\Q1969\\E", "shortCiteRegEx": "Luenberger", "year": 1969}, {"title": "Maximum margin classifiers with specified false positive and false negative error rates", "author": ["J.S. Nath", "C. Bhattacharyya"], "venue": "In Proceedings of the seventh SIAM International Conference on Data mining,", "citeRegEx": "Nath and Bhattacharyya,? \\Q2007\\E", "shortCiteRegEx": "Nath and Bhattacharyya", "year": 2007}, {"title": "Extension of the \u03bd-SVM range for classification", "author": ["F. Perez-Cruz", "J. Weston", "D.J.L. Hermann", "B. Sch\u00f6lkopf"], "venue": "In Advances in Learning Theory: Methods, Models and Applications", "citeRegEx": "Perez.Cruz et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Perez.Cruz et al\\.", "year": 2003}, {"title": "Optimization: Algorithms and Consistent Approximations", "author": ["E. Polak"], "venue": null, "citeRegEx": "Polak,? \\Q1997\\E", "shortCiteRegEx": "Polak", "year": 1997}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A. Smola", "R. Williamson", "P. Bartlett"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "\u03bd-support vector machine as conditional value-at-risk minimization", "author": ["A. Takeda", "M. Sugiyama"], "venue": "In ICML, pp", "citeRegEx": "Takeda and Sugiyama,? \\Q2008\\E", "shortCiteRegEx": "Takeda and Sugiyama", "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "The minimax probability machine (MPM) (Lanckriet et al., 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem.", "startOffset": 38, "endOffset": 62}, {"referenceID": 6, "context": ", 2002) and Fisher discriminant analysis (FDA) (Fukunaga, 1990) also address the binary classification problem.", "startOffset": 47, "endOffset": 63}, {"referenceID": 1, "context": "The purpose of this paper is to provide a unified framework for learning algorithms, including SVM, MPM, and FDA, from the viewpoint of robust optimization (Ben-Tal et al., 2009).", "startOffset": 156, "endOffset": 178}, {"referenceID": 13, "context": "When U+ and U\u2212 are defined as reduced convex hulls (Bennett & Bredensteiner, 2000), (2) reduces to \u03bd-SVM (Sch\u00f6lkopf et al., 2000) if U+ \u2229 U\u2212 = \u2205 and reduces to E\u03bd-SVM (Perez-Cruz et al.", "startOffset": 105, "endOffset": 129}, {"referenceID": 10, "context": ", 2000) if U+ \u2229 U\u2212 = \u2205 and reduces to E\u03bd-SVM (Perez-Cruz et al., 2003), otherwise.", "startOffset": 45, "endOffset": 70}, {"referenceID": 10, "context": "\u2019s extension (Perez-Cruz et al., 2003) from convex \u03bd-SVM to nonconvex E\u03bd-SVM.", "startOffset": 13, "endOffset": 38}, {"referenceID": 8, "context": "By applying the discussion on the minimum norm duality (Luenberger, 1969) to (3), we can confirm the equivalence of (3) and minx\u2208U \u2016x\u2016, and the optimal solution w = x\u2217/\u2016x\u2217\u2016.", "startOffset": 55, "endOffset": 73}, {"referenceID": 4, "context": "1 of (Briec, 1997) under the assumption that a convex U has a nonempty interior.", "startOffset": 5, "endOffset": 18}, {"referenceID": 13, "context": "C-SVM (Cortes & Vapnik, 1995) and \u03bd-SVM (Sch\u00f6lkopf et al., 2000) are typical examples of \u201csoft-margin\u201d SVMs.", "startOffset": 40, "endOffset": 64}, {"referenceID": 5, "context": "Crisp and Burges (2000) showed \u03bdmax = 2min{m+,m\u2212}/m and gave a geometric interpretation for \u03bdmin.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "Crisp and Burges (2000) showed \u03bdmax = 2min{m+,m\u2212}/m and gave a geometric interpretation for \u03bdmin. For \u03bd \u2208 (\u03bdmax, 1], the optimization problem of \u03bd-SVM is unbounded, and for \u03bd \u2208 [0, \u03bdmin), \u03bd-SVM provides a trivial solution (w = 0 and b = 0). Perez-Cruz et al. (2003) devised extended \u03bd-SVM (E\u03bdSVM) as a way of avoiding such a trivial solution:", "startOffset": 0, "endOffset": 266}, {"referenceID": 10, "context": "It was experimentally found in (Perez-Cruz et al., 2003) that E\u03bd-SVM often has better generalization performance than \u03bd-SVM.", "startOffset": 31, "endOffset": 56}, {"referenceID": 4, "context": "Crisp and Burges (2000) showed that \u03bdmin is the largest \u03bd such that two RCHs, U\u03bd + and U\u03bd \u2212, intersect.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Barbero et al. (2012) transformed \u03bd-SVM and E\u03bd-SVM (7) into RCM (2) with U\u03bd \u00b1 in order to give them a geometric interpretation.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "The minimax probability machine (MPM) only uses the mean and covariance matrix of each class for classification tasks (Lanckriet et al., 2002).", "startOffset": 118, "endOffset": 142}, {"referenceID": 1, "context": "Robust optimization techniques for ellipsoidal uncertainty (Ben-Tal et al., 2009) transform RCM (2) with U\u00b1 = U\u00b1 \u00b1 into", "startOffset": 59, "endOffset": 81}, {"referenceID": 6, "context": "w \u2217 to the following problem (Fukunaga, 1990): max w (x\u0304+ \u2212 x\u0304\u2212)w \u2016(\u03a3+ + \u03a3\u2212)1/2w\u2016 .", "startOffset": 29, "endOffset": 45}, {"referenceID": 3, "context": "In replacing the Euclidean norm \u2016w\u2016 with the L1-norm \u2016w\u20161, MM-FDA is equivalent to a sparse feature selection model based on FDA (FSFD) (Bhattacharyya, 2004).", "startOffset": 136, "endOffset": 157}, {"referenceID": 10, "context": "In this section, we propose a solution method that is generalized from the local algorithms of (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008).", "startOffset": 95, "endOffset": 145}, {"referenceID": 10, "context": "Indeed, Perez-Cruz et al. (2003) experimentally showed that the generalization performance of E\u03bd-SVM is often better than that of original \u03bd-SVM.", "startOffset": 8, "endOffset": 33}, {"referenceID": 10, "context": "E\u03bd-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.", "startOffset": 29, "endOffset": 54}, {"referenceID": 10, "context": "E\u03bd-SVM is an LRCP, for which Perez-Cruz et al. (2003) proposed a local optimum search algorithm and Takeda and Sugiyama (2008) proposed a global optimum search algorithm.", "startOffset": 29, "endOffset": 127}, {"referenceID": 10, "context": "1) that is generalized from the local algorithms (Perez-Cruz et al., 2003; Takeda & Sugiyama, 2008) of E\u03bd-SVM for non-convex RCM.", "startOffset": 49, "endOffset": 99}, {"referenceID": 11, "context": "21 in (Polak, 1997)).", "startOffset": 6, "endOffset": 19}], "year": 0, "abstractText": "A wide variety of machine learning algorithms such as support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA), exist for binary classification. The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVM become applicable to MPM and FDA, and vice versa. Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and propose a non-convex optimization algorithm that can be applied to nonconvex variants of existing learning methods.", "creator": "LaTeX with hyperref package"}}}