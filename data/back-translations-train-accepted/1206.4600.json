{"id": "1206.4600", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes", "abstract": "We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.", "histories": [["v1", "Mon, 18 Jun 2012 14:40:38 GMT  (405kb)", "http://arxiv.org/abs/1206.4600v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["murat dundar", "ferit akova", "alan qi", "bartek rajwa"], "accepted": true, "id": "1206.4600"}, "pdf": {"name": "1206.4600.pdf", "metadata": {"source": "META", "title": "Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes", "authors": ["Murat Dundar", "Ferit Akova", "Bartek Rajwa"], "emails": ["dundar@cs.iupui.edu", "ferakova@cs.iupui.edu", "alanqi@cs.purdue.edu", "brajwa@purdue.edu"], "sections": [{"heading": "1. Introduction", "text": "A training set is considered exhaustive if it contains samples from all classes of information value. If some classes are missing and therefore not represented, the resulting training set is considered not exhaustive. It is impractical, often impossible, to define a training set with a complete set of classes and then collect samples for each class, mainly because some of the classes appear in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author / owner may not exist at the time of training, but they do not exist or their existence is known, but samples are simply not available. A traditional supervised classifier trained on a non-exhaustive training set incorrectly classifies a sample of a missing class with a probability of one, thereby poorly defining the associated learning problem."}, {"heading": "1.1. Motivation", "text": "Current research is mainly driven by a biosensory problem involving predicting the presence of specific and unmatched / emerging pathogenic microorganisms in different biological samples. A global increase in the number of outbreaks, coupled with heightened biosecurity concerns, has led to enormous interest among scientific communities and government agencies in the development of reagent-free techniques for rapid pathogen identification. Traditional detection methods based on antibodies or genetic matching remain labor-intensive and time-consuming and require multiple steps. Recent studies based on quantitative phenotypic evaluation have shown that promising methods for distinguishing bacterial cultures of the genus, species and strain level hold promise. The main advantage of label-free methods is their ability to quantify phenotypes for which there are no available antibodies or genetic markers. This information can be used within a traditional visualized framework of supertagged learning and used independently in the training of labeled samples."}, {"heading": "1.2. Proposed approach in a nutshell", "text": "The parameters of the basic distribution, which is selected as the bivariate Normal \u00d7 Inverted Wishart distribution, are estimated from samples originally available for known classes. It is proposed to perform a sequential resampling technique (SIR) to efficiently evaluate online the probability of a new sample belonging to an emerging or existing class, without requiring explicit knowledge of the class names of the previously observed samples. In this context, new classes characterized by a rapid increase in sample size are important for early identification of potentially interesting class formations."}, {"heading": "1.3. Related Work", "text": "In fact, it is the case that most of them are able to move without being able to see themselves moving. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move. They are able to move."}, {"heading": "2. Problem formulation", "text": "In this section, we present a general framework for learning based on a non-exhaustively defined training dataset that allows new classes to be discovered and modeled online. To distinguish classes discovered online from those originally available in the training library, we introduce the term \"labeled vs. unlabeled classes,\" with the terms \"labeled\" and \"unlabeled\" referring to verified or unverified classes. When referring to classes discovered online, the terms \"unlabeled class\" and \"cluster\" are used interchangeably in this text. In the proposed framework, the modeling of online classes by a DPP model is addressed (Ferguson, 1973)."}, {"heading": "2.1. Dirichlet process prior", "text": "Let xi, i = {1,.., n} be the characteristic vector that characterizes a sample in d-dimensional vector space <, and yi be its corresponding class indicator variable. If xi is distributed according to an unknown distribution p (. | \u03b8i), then the definition of a DPP via class distributions is equivalent to modelling the previous distribution of \u03b8 by means of a Dirichlet process. More formally speaking: xi | \u03b8i \u0445 p (\u00b7 \u03b8i) \u03b8i \u0445 G (\u00b7) G \u0445 DP (\u00b7 G0, \u03b1) (1), where G is a random probability measure distributed according to a Dirichlet process (DP), defined by a base distribution, G0, and the precision parameter, \u03b1. Since G is distributed according to a DP, the pitch-breaking construction based on (Ishwaran & James, 2001) indicates that G = 1 \u03b2i\u0445i \u2012 l = 1 (\u03b2 \u00b2) is equal to double probability."}, {"heading": "2.2. DPP in a nonexhaustive framework", "text": "The suitability of the DPP model for incomplete learning processes can be better determined by using the pre-introduction condition V =. Let us assume that at a given time the learning set contains a sequence of n samples. V + 1 is conditioned on all past samples, i = {1,., n} can be obtained by integrating G in (1), which becomes a mixture of two distributions. Any sample resulting from this prior distribution comes from the base class G0 (\u00b7) with a probability of V + n G0 (\u00b7) + n n n n n samples i = 1 (2) This condition condition can be interpreted as a mixture of two distributions. Each sample resulting from this prior distribution is generated from a probability of V + n or uniformly generated from {1,."}, {"heading": "3. Inference with a nonexhaustive set of classes", "text": "Before moving on to the discussion of how conclusions can be drawn in this context, we introduce a new notation to distinguish between the two types of samples that are available during online conduct: samples originally available in the training dataset with known class membership information, and samples that are observed online without verified class membership information. Let X = {x1,..., x '} be the set of all originally available training samples, Y = {y1,..., y' the corresponding set of known class indicators variables with yi = {1,...,..., k}, k = the number of known class indicator variables, X-n = {x-1,......,..., x-n} the set of n samples that are observed sequentially online, and Y-n = {y-1,..., y-n} the corresponding set of unknown class indicators associated with these n samples."}, {"heading": "3.1. Inference by Gibbs Sampling", "text": "We are interested in predicting Y-n + 1, i.e. the class name for all X-n + 1 at the time of observation of x-n + 1, which can be achieved by determining the mean of the latent distribution p (Y-n + 1 | X-n + 1, X-n + 1, Y). Although this integral cannot be easily evaluated, the closed form solution for the conditional distributions of the latent variables y-i can easily be obtained. Thus, Gibbs sample with the sample state consisting of variables y-i, i = {1,..., n + 1} can be used to determine approximate values of p (Y-n + 1 | X-n + 1, X-Y)."}, {"heading": "3.2. Inference by Sequential Importance Resampling (SIR)", "text": "With the Gibbs sampler approach, a new sample is observed each time in order to predict whether the current sample belongs to one of the existing classes (+ 1) or to a new class. This sampler scheme ultimately becomes unfeasible as the number of undescribed samples increases. (In this approach, at any given time, the sampler only depends on a set of particles and their corresponding weights, which are efficiently updated in a sequential manner without observing a new sample for the past samples. (Doucet et et al., 2000) In this approach, Y is at any given time dependent only on a series of particles and their corresponding weights, each of which is observed in a sequential manner, with no need for the past samples. Specifically, we are interested in evaluating the expectation Y. (Y)"}, {"heading": "3.3. Estimating the precision parameter \u03b1", "text": "In the proposed framework, \u03b1 is the parameter that controls the previous probability of mapping a new sample to a new cluster and therefore plays a decisive role in the number of clusters generated. If the training samples are collected to reflect the true proportion of each class and the actual number of classes as in a training set with the same number of samples in real time, the boundary distribution of the number of clusters p (k) can be maximized to obtain the maximum probability estimate of \u03b1. However, in many machine learning applications, only the most widespread classes are available in the training set and the training samples are almost never collected in real time. Therefore, p (k) cannot model a training set that is collected offline very well.A workable approach for predicting \u03b1 when training samples are not collected in real time is to derive it from the distribution p (\u03b1 | k, n) of training samples (EscoWest & Batulak, 1994) which is mainly used as a mixed sensitivity class, although this approach is mainly in the sensitivity class."}, {"heading": "4. A Normally distributed data model", "text": "Both the Gibbs sampler and the SIR require the evaluation of the predictive p (x-i | Dj) and the marginal p (x-i) distributions. The predictive distribution for both the designated and the blank classes can be obtained by integrating computations. The marginal distribution can be obtained from p (x-i | Dj) by specifying an empty set. In general, the exact solution for the predictive and blank distributions does not exist and approximate values are required. However, a closed solution exists for a normally distributed data model and a properly chosen base distribution as presented next. We give them a Gaussian distribution with mean \u00b5j and covariance."}, {"heading": "4.1. Estimating the parameters of the prior model", "text": "The maximum probability estimates for \"0\" and \"m\" do not exist, and the study in (Greene & Rayens, 1989) proposes to numerically estimate \"0\" by the unbiased and consistent estimate of \"Sp,\" i.e. the pooled covariance, and to maximize the marginal probability of \"(nj \u2212 1)\" Sj \"for\" m > d + 1. \"Sp is the pooled covariance matrix defined by\" Sp \"= (m \u2212 d \u2212 1)\" k = 1 (nj \u2212 1) \"Sjn \u2212 k\" (11), where \"n\" is the total number of samples in the training set, i.e. \"n = k = 1\" j = 1. \"The marginal distribution of\" Sj \u2212 1 \"Sj\" can be achieved by integrating the community distribution \"(nj \u2212 1)\" Smal \"and\" Sj \"(Sj = 1) Sj\" (Sp = 1) and \"Sj = 1.\""}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. An illustrative example", "text": "We present an illustrative example of the proposed algorithm for discovering and modelling classes with a simulated 2-D dataset. We create twenty-three classes in which the class covariance matrix of each class is obtained from an inverted wishart distribution with the parameters \u0430 = 10I and m = 20, and middle vectors are placed evenly next to the peripheries of two circles with radius 4 and 8, creating a flower-shaped dataset. At this point, I refer to the 2-D identity matrix. Three of the twenty-three classes are randomly selected as not being represented; the non-exhausting training data contains twenty classes, each class being represented by 100 samples (a total of 2000 samples), while the exhaustive test data contains twenty-three classes of 100 samples each (a total of 2,300 samples). The aim here is to discover and model the three unrepresented classes while ensuring that samples of the represented classes are classified as accurately as possible."}, {"heading": "5.2. Bacteria detection", "text": "A total of 2,054 samples from 28 classes, each representing different bacteria serovar, were considered in this study, while 28 samples are initially unavailable. These are the types of serovars most commonly found in food samples. Each serovar is represented by 40 to 100 samples, in which the samples represent the advance scattering patterns that characterize the phenotype of a bacterial colony, which is achieved by illuminating the colonial surface with a laser light. Each of these patterns is a grayscale pattern characterized by a series of 50 characteristics. More information on this data is available in (Akova et al., 2010). Samples are randomly divided into two classes, with 70% of the samples going into training and the remaining classes included in the test. Stratified sampling is used to ensure that each class is represented proportionally both in training and in test sets. Four of the classes are considered unknown and all of their samples are moved from the training set."}, {"heading": "6. Conclusions", "text": "In this approach, the rear distribution of the class indicator variables is approximated by a discrete distribution expressed by a series of particles and the corresponding weights; the particles and their weights are efficiently updated each time a new sample is observed; in this way, the rear distribution is updated sequentially without having access to past samples, allowing efficient online inference in a non-exhaustive environment; our algorithm is validated with a 28-class bacterium, achieving four of the previously unknown classes and promising results in terms of classification accuracy and class detection; the data model used in this study has been limited to the Normal Model; the proposed approach can be extended to problems involving more flexible class distributions by selecting a mixture model for each class, and we believe that the ytail function can be kept across the distribution class as a function of the distributionability."}, {"heading": "Acknowledgments", "text": "The project was supported by grant number 5R21AI085531-02 of the National Institute of Allergy and Infectious Diseases (NIAID). Content is the sole responsibility of the authors and does not necessarily reflect the official views of NIAID or the National Institutes of Health."}], "references": [{"title": "A machine-learning approach to detecting unknown bacterial serovars", "author": ["Akova", "Ferit", "Dundar", "Murat", "Davisson", "V. Jo", "Hirleman", "E. Daniel", "Bhunia", "Arun K", "Robinson", "J. Paul", "Rajwa", "Bartek"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "Akova et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Akova et al\\.", "year": 2010}, {"title": "Exchangeability and related topics", "author": ["Aldous", "David J"], "venue": "In E\u0301cole d\u2019E\u0301te\u0301 St Flour", "citeRegEx": "Aldous and J.,? \\Q1983\\E", "shortCiteRegEx": "Aldous and J.", "year": 1983}, {"title": "An Introduction to Multivariate Statistical Analysis, 3rd Edition", "author": ["Anderson", "Theodore W"], "venue": "WileyInterscience, 3rd edition,", "citeRegEx": "Anderson and W.,? \\Q2003\\E", "shortCiteRegEx": "Anderson and W.", "year": 2003}, {"title": "On sequential Monte Carlo sampling methods for Bayesian filtering", "author": ["Doucet", "Arnaud", "Godsill", "Simon", "Andrieu", "Christophe"], "venue": "Statistics and Computing,", "citeRegEx": "Doucet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2000}, {"title": "Bayesian density estimation and inference using mixtures", "author": ["Escobar", "Michael D", "West", "Mike"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Escobar et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Escobar et al\\.", "year": 1994}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["Ferguson", "Thomas S"], "venue": "Annals of Statistics,", "citeRegEx": "Ferguson and S.,? \\Q1973\\E", "shortCiteRegEx": "Ferguson and S.", "year": 1973}, {"title": "Partially pooled covariance matrix estimation in discriminant analysis", "author": ["Greene", "Tom", "Rayens", "William"], "venue": "Commun. Statist. Theory Meth.,", "citeRegEx": "Greene et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Greene et al\\.", "year": 1989}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["Ishwaran", "Hemant", "James", "Lancelot F"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ishwaran et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ishwaran et al\\.", "year": 2001}, {"title": "A mixture model and embased algorithm for class discovery, robust classification, and outlier rejection in mixed labeled/unlabeled data sets", "author": ["D.J. Miller", "J. Browning"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Miller and Browning,? \\Q2003\\E", "shortCiteRegEx": "Miller and Browning", "year": 2003}, {"title": "A non-parametric Bayesian alternative to spike sorting", "author": ["F. Wood", "M.J. Black"], "venue": "Journal of Neuroscience Methods,", "citeRegEx": "Wood and Black,? \\Q2008\\E", "shortCiteRegEx": "Wood and Black", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Early work most similar to nonexhaustive learning includes the two studies reported in (Akova et al., 2010; Miller & Browning, 2003).", "startOffset": 87, "endOffset": 132}, {"referenceID": 0, "context": "In (Akova et al., 2010) a Bayesian approach based on the maximum likelihood detection of novelties using a dynamically updated class set was proposed.", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "We believe that this problem can be addressed to a greater extent by developing a sequential sampling approach based on Sequential Importance Resampling (SIR) (Doucet et al., 2000).", "startOffset": 159, "endOffset": 180}, {"referenceID": 0, "context": "More information about this dataset is available in (Akova et al., 2010).", "startOffset": 52, "endOffset": 72}, {"referenceID": 0, "context": "The proposed NEL-SIR is compared against the BayesNoDe algorithm proposed in (Akova et al., 2010).", "startOffset": 77, "endOffset": 97}], "year": 2012, "abstractText": "We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.", "creator": "LaTeX with hyperref package"}}}