{"id": "1206.4601", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Convex Multitask Learning with Flexible Task Clusters", "abstract": "Traditionally, multitask learning (MTL) assumes that all the tasks are related. This can lead to negative transfer when tasks are indeed incoherent. Recently, a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships. However, they are limited to modeling these relationships at the task level, which may be restrictive in some applications. In this paper, we propose a novel MTL formulation that captures task relationships at the feature-level. Depending on the interactions among tasks and features, the proposed method construct different task clusters for different features, without even the need of pre-specifying the number of clusters. Computationally, the proposed formulation is strongly convex, and can be efficiently solved by accelerated proximal methods. Experiments are performed on a number of synthetic and real-world data sets. Under various degrees of task relationships, the accuracy of the proposed method is consistently among the best. Moreover, the feature-specific task clusters obtained agree with the known/plausible task structures of the data.", "histories": [["v1", "Mon, 18 Jun 2012 14:40:55 GMT  (3237kb)", "http://arxiv.org/abs/1206.4601v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["wenliang zhong", "james tin-yau kwok"], "accepted": true, "id": "1206.4601"}, "pdf": {"name": "1206.4601.pdf", "metadata": {"source": "CRF", "title": "Convex Multitask Learning with Flexible Task Clusters", "authors": ["Leon Wenliang Zhong", "James T. Kwok"], "emails": ["WZHONG@CSE.UST.HK", "JAMESK@CSE.UST.HK"], "sections": [{"heading": "1. Introduction", "text": "Many problems involve learning a set of tasks. Instead of learning them individually, it is now known that better generalization performance can be achieved by using intrinsic task relationships and allowing tasks to borrow strength from each other. However, in recent years, a number of techniques have become available under this multi-task learning (MTL) framework. Traditional MTL methods assume that all tasks are interrelated (Evgeniou & Pontil, 2004; Evgeniou et al., 2005).Appeared in presentations at the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by author (s) / owner (s). However, if this assumption does not hold, performance can be even worse than single task learning. If the tasks are known to be clustered, a simple task division should be limited."}, {"heading": "2. The Model", "text": "Suppose there are T tasks. The tth task has nt training samples {(x (t) 1, y (t) 1),..., (x (t) nt, y (t) nt), with input x (t) i-RD and output y (t) i-R. We stack the inputs and outputs to matrices X (t) = [x (t) 1,..., x (t) nt] \"and y (t) = [y (t) 1,..., y (t) nt].\" To learn each task, a linear model is used."}, {"heading": "2.1. Simultaneous Clustering of Task Parameters", "text": "It is not the first time that the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the EU), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the EU (the), the EU (the), the EU (the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the), the EU (the EU (the), the EU (the EU (the), the EU (the), the EU (the), the EU (the EU (the), the (the), the EU (the), the EU (the EU (the EU (the), the EU (the), the EU (the), the EU (the EU (the), the EU (the EU (the), the EU (the), the EU (the EU (the), the EU (the EU (the), the EU (the"}, {"heading": "2.2. Properties", "text": "The following proposal shows that if the tasks i and j have similar weights to feature d, the corresponding U-1 entries are combined, the corresponding U-2 entries are listed together. On the other hand, if for an outlier task t its ut component is separated from the main group, then the proposal 2: W-2 is separated from the main group. For simplicity, all T tasks are assumed to be identical to the number of training instances n. Suppose that the data for task t are generated as y (t) = X (t) w-1) w-2, with the probability N (0, p 2I) being identical."}, {"heading": "2.3. Optimization via Accelerated Proximal Method", "text": "In recent years, accelerated proximal methods (Nesterov, 2007) have been used by the machine learning community (Bach et al., 2011) for convex problems of the form f (\u03b8) + r (\u03b8), where f (\u03b8) is convex and smooth, and r (\u03b8) is convex but not smooth. Convergence rate is optimal for the class of primary methods. Together with their algorithmic and implementing simplifications, they can be applied to large, smooth convex problems. In this work, we use the well-known method of FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) (Beck & Teboulle, 2009). Extending to other accelerated proximal methods is simple. Each FISTA iteration performs the following proximal method."}, {"heading": "2.3.1. COMPUTING V", "text": "If U is fixed, the sub-problem in (5) is V minV, V \u2212 V, 2F + \u03bb, 3, V, 2F. If you set the gradient of the lens w.r.t. V to zero, you get V = [v, ij1 + \u03bb, 3]."}, {"heading": "2.3.2. COMPUTING U", "text": "If V is fixed, the partial problem in (5) is in connection with U minU, U, U, 2F, 1, U, clus, 2, U, 2F. Due to the number of terms in (U, 2) in (U, 2F, this is more difficult than the calculation of V in Section 2.3.1. However, since the U rows are independent of each other, U can be optimized line by line. In the dth line we have u, u, u, u, u, 2, 1, i < j, ui, ui, uj, 2, (7) where u, i, u, d,..., u, T, etc. (7) can be rewritten as an optimization problem that is considered in (Zhong & Kwok, 2011) and can therefore be efficiently solved with the algorithm proposed there."}, {"heading": "2.3.3. TIME COMPLEXITY", "text": "Calculation of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Gradients of Grades of Gradients of Gradients of Gradients of Gradients of Grades of Grades of Grades of Grades of Grades of Grades of Gra"}, {"heading": "2.4. Adaptive Clustering", "text": "As in the adaptive lasso (Zou, 2006), weights can be added to the individual terms of \"U Clus\": \"D\" = 1 \"i\" < j, \"\" Udi \"and\" Udj, \"where\" d, \"\" ij \"is the weight associated with the ith\" and \"jst\" largest entries (Udi \"and\" Udj \"respectively) in the dth\" row of \"U.\" To determine the weights \"d,\" \"ij\" and \"i,\" we first run model (1) with the unweighted \"U\" clusters to obtain W, and then set \"d,\" ij \"= 1\" Wdi \"- Wdj.\" So if Wdi \",\" Wdj \"are similar, Udi,\" Udj \"are strongly encouraged to be combined, and vice versa. Furthermore, the optimization method can be used in\" Algorithm 1. \""}, {"heading": "3. Experiments", "text": "In this section, we conduct experiments with a number of synthetic and real data sets. All data sets are standardized so that the characteristics for each task have an average value and a variance of 0 units."}, {"heading": "3.1. Synthetic Data Sets", "text": "In this experiment we have the dimensionality D = 30 and is from the multivariate i.S (0, I).D's \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. D's \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "3.2. Examination Score Prediction", "text": "As in this section (Chen et al., 2011a), we use 10%, 20% and 30% of the data for training, another 45% for testing, and the remaining 45% for validation. To reduce statistical variability, the results are calculated on average over 5 repetitions.The results are presented in Table 2. Note that while the school data is generally used as an MTL benchmark, it has previously been pointed out that all tasks are actually the same (Bakker & Heskes, 2003; Evgeniou et al., 2005).Therefore, the trend in Table 2 is similar to that of C2 in Table 1. As can be seen, both versions of FlexTClus are very competitive and better than the other MTL methods in this cluster case. Figure 3 shows the cluster structure obtained through adaptive FlexTClus."}, {"heading": "3.3. Handwritten Digit Recognition", "text": "In this section, we conduct experiments with two popular handwritten numerical datasets, USPS and MNIST. As in (Kang et al., 2011), PCA is used to reduce the feature dimension to 64 for USPS and 87 for MNIST. For each digit, we randomly select 10, 30, 50 samples for training, 500 samples for validation, and a further 500 samples for the test. The 10-class classification problem breaks down into 10 one-on-one-rest binary problems, each of which is treated as a task. Results, on average over 5 repetitions, are presented in Table 3. We do not compare ourselves to pooling, which assumes that all tasks are identical and is clearly invalid in this one-against-rest setting. As can be seen, FlexTClus and its adaptive version are consistently among the best, while many other MTL methods suffer from clustered transfer and are comparable or worse than the grades in many of the 4."}, {"heading": "3.4. Rating of Products", "text": "In this section, we use data from the computer survey in (Argyriou et al., 2008), which includes the ratings of 201 students on 20 different PCs, each described by 13 attributes. After removing the invalid ratings and students with more than 8 zeros, there are 172 students (tasks) left. For each task, we randomly divide the 20 instances into training, validation, and test sets of 8.8 and 4, respectively. Table 4 shows the average square error (RMSE) of an average of more than 10 random splits. Here, too, FlexTClus and its adaptive variant outperform the other models. Figure 5 shows the cluster structure of tasks obtained in a typical run. Note that the first 12 characteristics relate to PC performance (such as memory and CPU speed). As can be seen, there is a main cluster indicating that most students in this survey have similar preferences in terms of these attributes."}, {"heading": "4. Conclusion and Future Work", "text": "While existing MTL methods can only model task relationships at task level, we introduced this paper-based novel MTL formulation that captures task relationships at feature level. Depending on the myriad relationships between tasks and features, the proposed method can flexibly group tasks feature by feature without having to specify the number of clusters in advance. In addition, the proposed formulation is (strongly) convex and can be solved by accelerated proximal methods with an efficient and scalable proximal step. Experiments with a number of synthetic and real data sets show that the proposed method is accurate. The function-specific cluster structure obtained also matches the known / plausible cluster structure of the tasks."}, {"heading": "Acknowledgments", "text": "This research was partially supported by the Hong Kong Special Administrative Region Research Grants Council (Grant 614311)."}], "references": [{"title": "Learning multiple tasks using manifold regularization", "author": ["A. Agarwal", "H. Daum\u00e9 III", "S. Gerber"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Convex multitask feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Convex optimization with sparsity-inducing norms", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "In Optimization for Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "Task clustering and gating for Bayesian multitask learning", "author": ["B. Bakker", "T. Heskes"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bakker and Heskes,? \\Q2003\\E", "shortCiteRegEx": "Bakker and Heskes", "year": 2003}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Learning incoherent sparse and low-rank patterns from multiple tasks", "author": ["J. Chen", "J. Liu", "J. Ye"], "venue": "In Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Integrating low-rank and group-sparse structures for robust multi-task tearning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "In Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Smoothing proximal gradient method for general structured sparse learning", "author": ["X. Chen", "Q. Lin", "S. Kim", "J.G. Carbonell", "E.P. Xing"], "venue": "In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In Proceedings of the 10th International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Evgeniou and Pontil,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou and Pontil", "year": 2004}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Evgeniou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2005}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["L. Jacob", "F. Bach", "J. Vert"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jacob et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2008}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jalali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2010}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "K. Sha"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Multitask learning via conic programming", "author": ["T. Kato", "H. Kashima", "M. Sugiyama", "K. Asai"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kato et al\\.", "year": 2007}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "Technical Report 76, Catholic University of Louvain,", "citeRegEx": "Nesterov,? \\Q2007\\E", "shortCiteRegEx": "Nesterov", "year": 2007}, {"title": "Pairwise fused lasso", "author": ["S. Petry", "C. Flexeder", "G. Tutz"], "venue": "Technical Report 102,", "citeRegEx": "Petry et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petry et al\\.", "year": 2011}, {"title": "Sparse regression with exact clustering", "author": ["Y. She"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "She,? \\Q2010\\E", "shortCiteRegEx": "She", "year": 2010}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Tibshirani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2005}, {"title": "Learning Gaussian processes from multiple tasks", "author": ["K. Yu", "V. Tresp", "A. Schwaighofer"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2005}, {"title": "Learning multiple tasks with a sparse matrix-normal penalty", "author": ["Y. Zhang", "J. Schneider"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhang and Schneider,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Schneider", "year": 2010}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "Yeung", "D.-Y"], "venue": "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Efficient sparse modeling with automatic feature grouping", "author": ["L.W. Zhong", "J.T. Kwok"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Zhong and Kwok,? \\Q2011\\E", "shortCiteRegEx": "Zhong and Kwok", "year": 2011}, {"title": "Clustered multi-task learning via alternating structure optimization", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "Traditional MTL methods assume that all the tasks are related (Evgeniou & Pontil, 2004; Evgeniou et al., 2005).", "startOffset": 62, "endOffset": 110}, {"referenceID": 2, "context": "If it is known that the tasks are clustered, a simple remedy is to constrain task sharing to be just within the same cluster (Argyriou et al., 2008; Evgeniou et al., 2005).", "startOffset": 125, "endOffset": 171}, {"referenceID": 10, "context": "If it is known that the tasks are clustered, a simple remedy is to constrain task sharing to be just within the same cluster (Argyriou et al., 2008; Evgeniou et al., 2005).", "startOffset": 125, "endOffset": 171}, {"referenceID": 14, "context": "This can be further extended to the case where task relationships are represented in the form of a network (Kato et al., 2007).", "startOffset": 107, "endOffset": 126}, {"referenceID": 19, "context": "For example, some assume that the task parameters share a common prior in a Bayesian model (Yu et al., 2005; Zhang & Schneider, 2010; Zhang & Yeung, 2010); that the data follows a dirty model (Jalali et al.", "startOffset": 91, "endOffset": 154}, {"referenceID": 12, "context": ", 2005; Zhang & Schneider, 2010; Zhang & Yeung, 2010); that the data follows a dirty model (Jalali et al., 2010); that most of the tasks lie in a low-dimensional subspace (Ando & Zhang, 2005; Chen et al.", "startOffset": 91, "endOffset": 112}, {"referenceID": 6, "context": ", 2010); that most of the tasks lie in a low-dimensional subspace (Ando & Zhang, 2005; Chen et al., 2010), or that outlier tasks are present (Chen et al.", "startOffset": 66, "endOffset": 105}, {"referenceID": 2, "context": "In this paper, we will mainly be interested in techniques that assume the tasks are clustered (Argyriou et al., 2008; Evgeniou et al., 2005), and then infer the clustering structure automatically during learning (Jacob et al.", "startOffset": 94, "endOffset": 140}, {"referenceID": 10, "context": "In this paper, we will mainly be interested in techniques that assume the tasks are clustered (Argyriou et al., 2008; Evgeniou et al., 2005), and then infer the clustering structure automatically during learning (Jacob et al.", "startOffset": 94, "endOffset": 140}, {"referenceID": 11, "context": ", 2005), and then infer the clustering structure automatically during learning (Jacob et al., 2008; Kang et al., 2011).", "startOffset": 79, "endOffset": 118}, {"referenceID": 13, "context": ", 2005), and then infer the clustering structure automatically during learning (Jacob et al., 2008; Kang et al., 2011).", "startOffset": 79, "endOffset": 118}, {"referenceID": 23, "context": "Interestingly, it is recently shown that this clustered MTL approach is equivalent to alternating structure optimization (Ando & Zhang, 2005) that assumes the tasks share a low-dimensional structure (Zhou et al., 2011).", "startOffset": 199, "endOffset": 218}, {"referenceID": 11, "context": "Moreover, a key difference with (Jacob et al., 2008) is that we do not require the number of clusters to be pre-specified.", "startOffset": 32, "endOffset": 52}, {"referenceID": 13, "context": "For example, in (Kang et al., 2011), it leads to a mixed integer program, which has to be relaxed as a nonlinear optimization problem and then solved by gradient descent.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "On the other hand, the proposed approach directly leads to a (strongly) convex optimization problem, which can then be efficiently solved by accelerated proximal methods (Nesterov, 2007) after some transformations.", "startOffset": 170, "endOffset": 186}, {"referenceID": 18, "context": "Note that this is different from the fused lasso regularizer (Tibshirani et al., 2005), which is used for clustering features in single-task learning while \u2016U\u2016clus is for clustering tasks in MTL.", "startOffset": 61, "endOffset": 86}, {"referenceID": 10, "context": "In (Evgeniou et al., 2005), ut is the (single) cluster center of all the tasks; in (Ando & Zhang, 2005; Chen et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": ", 2005), ut is the (single) cluster center of all the tasks; in (Ando & Zhang, 2005; Chen et al., 2010; 2011a),", "startOffset": 64, "endOffset": 110}, {"referenceID": 0, "context": "ut comes from a low-dimensional linear subspace, which is extended to a nonlinear manifold in (Agarwal et al., 2010); in (Jalali et al.", "startOffset": 94, "endOffset": 116}, {"referenceID": 12, "context": ", 2010); in (Jalali et al., 2010), ut is the component that uses features shared by other tasks.", "startOffset": 12, "endOffset": 33}, {"referenceID": 10, "context": "Thus, wt reduces to \u016b + vt for some \u201cmean weight\u201d \u016b, and (1) reduces to the model in (Evgeniou et al., 2005).", "startOffset": 85, "endOffset": 108}, {"referenceID": 15, "context": "In recent years, accelerated proximal methods (Nesterov, 2007) have been popularly used by the machine learning community (Bach et al.", "startOffset": 46, "endOffset": 62}, {"referenceID": 3, "context": "In recent years, accelerated proximal methods (Nesterov, 2007) have been popularly used by the machine learning community (Bach et al., 2011) for convex problems of the form min\u03b8 f(\u03b8)+r(\u03b8), where f(\u03b8) is convex and smooth, and r(\u03b8) is convex but nonsmooth.", "startOffset": 122, "endOffset": 141}, {"referenceID": 16, "context": "Though (7) is similar to the optimization problems of the pairwise fused lasso in (Petry et al., 2011; She, 2010), using the optimization procedures there are much more expensive.", "startOffset": 82, "endOffset": 113}, {"referenceID": 17, "context": "Though (7) is similar to the optimization problems of the pairwise fused lasso in (Petry et al., 2011; She, 2010), using the optimization procedures there are much more expensive.", "startOffset": 82, "endOffset": 113}, {"referenceID": 16, "context": "Specifically, the procedure in (Petry et al., 2011) takes O(T ) time, as it involves a QP with ( T", "startOffset": 31, "endOffset": 51}, {"referenceID": 17, "context": "2 ) additional optimization variables; while (She, 2010) relies on annealing, which is even more complicated and expensive.", "startOffset": 45, "endOffset": 56}, {"referenceID": 24, "context": "As in the adaptive lasso (Zou, 2006), weights can be added to each term of \u2016U\u2016clus as \u2211D d=1 \u2211 \u0129<j\u0303 \u03b1d,\u0303ij\u0303 |Ud\u0129 \u2212 Udj\u0303 |, where \u03b1d,\u0303ij\u0303 is the weight associated with the ith and jth largest entries (Ud\u0129 and Udj\u0303 , respectively) on the dth row of U.", "startOffset": 25, "endOffset": 36}, {"referenceID": 12, "context": "It is compared with a variety of single-task and state-of-the-art MTL algorithms, including: 1) Independent ridge regression on each task; 2) Pooling all the training data together to learn a single model: This assumes that all the tasks are identical; 3) Regularized MTL: This assumes that all the tasks come from a single cluster (Evgeniou & Pontil, 2004); 4) The dirty model in (Jalali et al., 2010); 5) Low-rank-based robust MTL (Chen et al.", "startOffset": 381, "endOffset": 402}, {"referenceID": 6, "context": ", 2011a); 6) Sparse-LowRank MTL (Chen et al., 2010), which learns sparse and low-rank patterns from the tasks; 7) Clustered MTL (Jacob et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 11, "context": ", 2010), which learns sparse and low-rank patterns from the tasks; 7) Clustered MTL (Jacob et al., 2008)2 and 8) Multitask relationship learning (MTRL) (Zhang & Yeung, 2010).", "startOffset": 84, "endOffset": 104}, {"referenceID": 11, "context": "The clustered MTL algorithm of (Jacob et al., 2008) requires the number of task clusters as input.", "startOffset": 31, "endOffset": 51}, {"referenceID": 10, "context": "Note that though the school data has been popularly used as a MTL benchmark, it has been pointed out previously that all the tasks are indeed the same (Bakker & Heskes, 2003; Evgeniou et al., 2005).", "startOffset": 151, "endOffset": 197}, {"referenceID": 13, "context": "As in (Kang et al., 2011), PCA is used to reduce the feature dimensionality to 64 for USPS and 87 for MNIST.", "startOffset": 6, "endOffset": 25}, {"referenceID": 2, "context": "In this section, we use the computer survey data in (Argyriou et al., 2008).", "startOffset": 52, "endOffset": 75}], "year": 2012, "abstractText": "Traditionally, multitask learning (MTL) assumes that all the tasks are related. This can lead to negative transfer when tasks are indeed incoherent. Recently, a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships. However, they are limited to modeling these relationships at the task level, which may be restrictive in some applications. In this paper, we propose a novel MTL formulation that captures task relationships at the feature-level. Depending on the interactions among tasks and features, the proposed method construct different task clusters for different features, without even the need of pre-specifying the number of clusters. Computationally, the proposed formulation is strongly convex, and can be efficiently solved by accelerated proximal methods. Experiments are performed on a number of synthetic and real-world data sets. Under various degrees of task relationships, the accuracy of the proposed method is consistently among the best. Moreover, the feature-specific task clusters obtained agree with the known/plausible task structures of the data.", "creator": "TeX"}}}