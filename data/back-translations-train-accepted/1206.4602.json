{"id": "1206.4602", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Quasi-Newton Methods: A New Direction", "abstract": "Four decades after their invention, quasi-Newton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.", "histories": [["v1", "Mon, 18 Jun 2012 14:41:11 GMT  (475kb)", "http://arxiv.org/abs/1206.4602v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.NA cs.LG stat.ML", "authors": ["philipp hennig", "martin kiefel"], "accepted": true, "id": "1206.4602"}, "pdf": {"name": "1206.4602.pdf", "metadata": {"source": "META", "title": "Quasi-Newton Methods: A New Direction", "authors": ["Philipp Hennig", "Martin Kiefel"], "emails": ["philipp.hennig@tuebingen.mpg.de", "martin.kiefel@tuebingen.mpg.de"], "sections": [{"heading": "1. Introduction", "text": "It is not as if it is a local minimum along a line search direction \u2212 B \u2212 1i (x), an estimate of the eponymous Newton-Raphson search direction f (x), some of the best known members of this family include Broyden's (1965) method, the SR1 formula (Davidon, 1959), the DFP method (Davidon, 1959; Fletcher & Powell, 1963), and the BFGS method (Broyden, 1969; Goldfarb, 1970; Shanno, 1970)."}, {"heading": "2. Quasi-Newton Methods as approximate Bayesian Regressors", "text": "From a probable perspective, equation (1) is a probability for B. With si = xi \u2212 xi \u2212 1, we can do it with Dirac's distributionp (yi-B, si) = \u2212 \u2212 B \u2212 2, \u2212 B \u2212 2, and the linear operator Si = (I si), the real numbers in yi are of course not sufficient to identify the N2 numbers in B. Classical derivatives (Dennis & More), 1977; Nocedal & Wright, 1999 provides a regulator based on the weighted Frobenius norm around the current best estimate Bi \u2212 1. The weight in the Frobenius norm is encoded with a positive definitive matrix, which we will suggestively call V \u2212 i."}, {"heading": "2.1. Symmetric Estimates, but no Symmetric Beliefs", "text": "The correct probabilistic method for encoding the Hessian symmetry is to include an additional probability sterm\u03b4 = \"double i\" variation. (3) The correct probabilistic method for encoding the Hessian symmetry is to include an additional probability sterm\u03b4 = \"double i-variation\" (4). (4) The resulting postsymmetry is analytical and Gaussian's operator - the linear map defined by \"double X\" (4). (5) The order of the \"symmetric\" is, however, 1 / 2 \"N\" (N \u2212 1) (e.g. Lu \u00bc tkepohl, 1996, \u00a7 4.3.1, Eqeris. 12 & 20), so that the corresponding updating rule does not comply with the first requirement of Section 1. However, the structure of \"i\" points to another idea, which in fact leads to the most popular quasiton methods."}, {"heading": "2.2. Positive Definiteness: Meaning or Decoration?", "text": "Consider the choice of Vi \u2212 1 = B. The previous decision is then p (B), B \u2212 N2 / 2 \u2012 SI (N \u2212 2 tr (Bi \u2212 1B \u2212 1), Bi \u2212 1B \u2212 1B \u2212 1B \u2212 1B \u2212 1)), (16), This is a fascinating Previous (N \u2212 2 tr (Bi \u2212 1B \u2212 1), (Bi \u2212 1B \u2212 1), Bi \u2212 1B \u2212 1), (Bi \u2212 1), (Bi \u2212 1), (Bi \u2212 1), (Bi \u2212 1), (Bi \u2212 1), (Bi \u2212 1), (Bi), (Bi \u2212 \u2212 1), (Bi), (Bi), (Bi), (Bi \u2212 1), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi, (Bi), (Bi), (Bi), (Bi), (Bi, (Bi), (Bi), (Bi), (Bi), (Bi), (Bi), (Bi, (Bi), (Bi), (Bi), (Bi), (Bi), (Bi, (Bi), (Bi), (Bi), (Bi), (Bi, (Bi), (Bi), (Bi), (Bi), (Bi, (Bi), (Bi), (Bi), (Bi, (Bi), (Bi), (Bi), (Bi), (Bi), (Bi,"}, {"heading": "2.3. Rank M Updates", "text": "The classical quasi-Newton algorithms update the mean of faith at each step of an operation from rank 2, then implicitly reset its uncertainty in the next step, discarding previously acquired information. Although from Bayesian point of view inelegant, this scheme is still a good idea given other aspects of the frame: Since the quasi-Newton probability models the objective function as a square, model-like incongruence, it would result in a strong overmatch under accurate Bayean inference. However, it is instructive to look at the effect of the coding more than just the most recent observation. It is easy to extend equivalent (2) to observations (Y, S) from multiple line searches: Ynm = nf (xim \u2212 1) Snm = xim, n \u2212 xim \u2212 \u2212 1 \u2212 S \u2212 1, n (19) of symmetry (B0, V0, V0, V0) \u2212 posterior then has an average and variance of VY (BS) V1S (VS) V1S \u2212 S (VS)."}, {"heading": "2.4. Summary", "text": "The previous section showed that quasi-Newton algorithms, including the state-of-the-art BFSS and DFP algorithms, can be interpreted as approximate Bayesian regression from the original and double probability of Equs. (2) and (12), with different priorities, in the following sense: At each quasi-Newton step, fix a Gaussian previous ad hoc, update the mean, and then \"forget\" the covariance update. Two particularly interesting observations concern the way in which the symmetry and positive definition of the MAP estimator are achieved in these algorithms. Symmetry is encoded by dual observations, which is a useful but imperfect abbreviation. Positive definitivity is achieved not by encoding relevant information, but by shifting the previous post hoc. It is therefore doubtful whether the demonstrably good performance of BGS and DFP is actually due to a clear limitation of the pathology, rather than a simple BGS observation."}, {"heading": "3. A Nonparametric Bayesian Quasi-Newton Method", "text": "In this second part of the paper, we deviate from the traditional framework to construct a non-parametric Bayesian quasi-Newton method de novo. To motivate this effort, we note some other shortcomings of DFP / BFGS in terms of the use of available information: Equation (2) assumes that the function is (local) square. Thus, old observations collected \"far\" from the current location (in the sense that second-order expansion is a poor approximation) can be useless or even harmful. Therefore, the fact that the function is not square should be part of the model. At a marginally related point, individual line searches typically involve multiple assessments of the objective function f and its progression; but the algorithms use only one of these aspects (the latter). This is clearly wasteful, but even the exact Bayesian parametric algorithm of Section 2.3 has this problem: The Vrix-S observations can be found along all of the matrix-S series."}, {"heading": "3.1. A Nonparametric Prior", "text": "The mean function is assumed to be any integrable function B0 (x) (in our implementation we use a constant function, but the analytical derivatives need not be so restrictive).The basic idea is to assume that the covariance between the element Bij at position x \u00be and the entry Bk'an position x \u00bc iscov (Bij (x \u00be), Bk '(x \u00bc) = kik (x \u00be, x \u00bc) kj' (x \u00be, x \u00bc) (ij) (x \u00be, x \u00bc) (22) with a N \u00d7 N matrix of cores, k. To give a more specific requirement: In our implementation we use a common square exponential kernel for all elements. I.kij (x \u00bc) with an N matrix of cores, nucleus, nucleus, nucleus, nucleus, nucleus, nucleus, nucleus, nucleus, nucleus, nucleus, nucleus, nucleus, can not yet be defined."}, {"heading": "3.2. Line Integral Observations", "text": "For the Hessian B (x) of a general function f, the quasi-Newton equation (2) is only an approximation of the second order (an approximation of the second order to f itself), which assumes a Hessian that is constant everywhere. In our treatment, we will replace it with the exact statement: We note the value of the line integral along the path ri: [0,1] _ RN, ri (t) = xi \u2212 1 + t (xi \u2212 xi \u2212 1). Yni = certain m fields Bnm (x) dxm = \u2211 m Smi-1 0 Bnm (ri (t) dt. (24) Here, the classical result is used that line integrals are completely defined by the start and end point of the path, independent of the path itself. Therefore, the non-parametric version of the QuasiNewton equation is the calculus of probability (Y-B (x), S) = li\u00df \u00b2 n \u00b2 S \u00b2 K-K (n) K-k \u00b2 S-K (n)."}, {"heading": "3.3. Gaussian Process Inference from Integral Observations", "text": "Since the Gaussian exponential family is closed under linear transformations, the Gaussian process conclusion is analytical under any linear operator. As integration is a linear operation, Gaussian process conclusion is possible in closed form from holistic observations. Nevertheless, this idea has rarely been used in literature (e.g. by Minka, 2000). Figure 1 provides a 1D toy example of intuition.The posterior distribution under our nonparametric previous aspects includes the probability of equivalents (25) and its dual equivalent is a Gaussian process with mean and covariance functions B (x \u00b2) = B0 (x \u00b2) + (Y \u2212 B0) K \u2212 1k k (x \u00b2) K \u2212 1 (Y \u2212 B0) -1 (Y \u2212 B0) -B0 (Y \u2212 B0) -B0."}, {"heading": "3.4. Numerical Implementation", "text": "Another advantage of the Bayean formulation is that previous assumptions are easy to analyze and understand: The square exponential before boils down to the assumption that the elements of Hessian vary independently across the parameter space, on a single set of length scales. Multiple length scales could be modeled using core sums, but our implementation does not currently offer this option. Changing the length scales amounts to automatic pre-conditioning, another advantage of a Bayean formulation that we cannot go into for space reasons. Hyper parameters could be modeled by sums of cores, but our implementation currently does not offer this option. Changing the length scales amounts to automatic pre-conditioning, another advantage of a Bayean formulation that we cannot go into in more detail for space reasons. Hyper parameters can be adjusted by type II maximum probability, as in canonical Gaussian process regressions. Unfortunately, this is itself an optimization problem."}, {"heading": "4. Experiments", "text": "The objective functions were the logarithms of the products of the gamma distributions with different parameters for each dimension (a simplified version of hyperparameter learning for Gaussian process regression).In this experiment, the non-parametric algorithm clearly outperforms its predecessors; the performance advantage is not always as drastic (the journal version contains additional empirical results, including less pronounced cases).Despite the relatively precise numerical treatment of the integrals involved, the non-parametric Bayesian quasi-Newton algorithm presents more numerical challenges than its predecessors, and this problem becomes apparent when the quadratic functions whose constant Hessional gaps underpin the modeling advantage of the non-parametric method are minimized."}, {"heading": "5. Outlook", "text": "Due to the limitations of a conference publication, we have outlined only some of our core findings. To give an intuition of the potential of probabilistic formulations of numerical optimization, we consider some of the most immediate future work: Perhaps the most obvious insight is that Gaussian process integration is trivial to extend to noisy evaluations. Combined with a robust replacement for traditional character search, our work can therefore lead to robust numerical optimizers. Repeated integration, rather than Gaussian probabilities combined with approximate inference, can allow optimization without gradients or only from observations of gradient marks. Structured and hierarchical priors are a third direction that offers new possibilities for optimizing very high-dimensional functions."}, {"heading": "6. Conclusion", "text": "We have shown that the most popular Quasi-Newton algorithms can be interpreted as approximations of the Bayesian regression under Gauss and other predecessors, which deepens our understanding of these algorithms. In particular, it was found that the symmetry in the estimators of SR1, PSB, DFP and BFGS and the positive determination in the estimators of DFP and BFGS are only approximate and incomplete coded. As a parallel result, our analysis also leads to a new class of Bayesian Quasi-Newton algorithms, which use a core model to use all observations in each line search, explicitly follow the uncertainty and thus achieve a faster convergence towards the true Hessian. While the new methods are not trivial to understand and implement, their computational costs are within a constant of those of their predecessors. A demonstrative implementation can be found at www.probabilistic-optimization.org."}, {"heading": "Acknowledgments", "text": "The authors thank Christian Schuler, Tom Minka and Carl Rasmussen for helpful discussions and Carl Rasmussen for his publication of minimize.m, which simplified development. MK is supported by a grant from Microsoft Research Ltd."}], "references": [{"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "A class of methods for solving nonlinear simultaneous equations", "author": ["C.G. Broyden"], "venue": "Math. Comp.,", "citeRegEx": "Broyden,? \\Q1965\\E", "shortCiteRegEx": "Broyden", "year": 1965}, {"title": "Quasi-Newton methods and their application to function minimization", "author": ["C.G. Broyden"], "venue": "Math. Comp.,", "citeRegEx": "Broyden,? \\Q1967\\E", "shortCiteRegEx": "Broyden", "year": 1967}, {"title": "A new double-rank minimization algorithm", "author": ["C.G. Broyden"], "venue": "Notices American Math. Soc,", "citeRegEx": "Broyden,? \\Q1969\\E", "shortCiteRegEx": "Broyden", "year": 1969}, {"title": "Variable metric method for minimization", "author": ["W.C. Davidon"], "venue": "Technical report, Argonne National Laboratories, Ill.,", "citeRegEx": "Davidon,? \\Q1959\\E", "shortCiteRegEx": "Davidon", "year": 1959}, {"title": "Quasi-Newton methods, motivation and theory", "author": ["Dennis", "J.E. Jr.", "J.J. Mor\u00e9e"], "venue": "SIAM Review, pp", "citeRegEx": "Dennis et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dennis et al\\.", "year": 1977}, {"title": "A new approach to variable metric algorithms", "author": ["R. Fletcher"], "venue": "The Computer Journal,", "citeRegEx": "Fletcher,? \\Q1970\\E", "shortCiteRegEx": "Fletcher", "year": 1970}, {"title": "A rapidly convergent descent method for minimization", "author": ["R. Fletcher", "M.J.D. Powell"], "venue": "The Computer Journal,", "citeRegEx": "Fletcher and Powell,? \\Q1963\\E", "shortCiteRegEx": "Fletcher and Powell", "year": 1963}, {"title": "Numerical computation of rectangular bivariate and trivariate normal and t probabilities", "author": ["A. Genz"], "venue": "Statistics and Computing,", "citeRegEx": "Genz,? \\Q2004\\E", "shortCiteRegEx": "Genz", "year": 2004}, {"title": "A family of variable metric updates derived by variational means", "author": ["D. Goldfarb"], "venue": "Math. Comp.,", "citeRegEx": "Goldfarb,? \\Q1970\\E", "shortCiteRegEx": "Goldfarb", "year": 1970}, {"title": "Deriving quadrature rules from Gaussian processes", "author": ["T.P. Minka"], "venue": "Technical report,", "citeRegEx": "Minka,? \\Q2000\\E", "shortCiteRegEx": "Minka", "year": 2000}, {"title": "Updating quasi-Newton matrices with limited", "author": ["J. Nocedal"], "venue": "storage. Math. Comp.,", "citeRegEx": "Nocedal,? \\Q1980\\E", "shortCiteRegEx": "Nocedal", "year": 1980}, {"title": "A new algorithm for unconstrained optimization", "author": ["M.J.D. Powell"], "venue": "Nonlinear Programming. AP,", "citeRegEx": "Powell,? \\Q1970\\E", "shortCiteRegEx": "Powell", "year": 1970}, {"title": "Conditioning of quasi-Newton methods for function minimization", "author": ["D.F. Shanno"], "venue": "Math. Comp.,", "citeRegEx": "Shanno,? \\Q1970\\E", "shortCiteRegEx": "Shanno", "year": 1970}], "referenceMentions": [{"referenceID": 4, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 102, "endOffset": 132}, {"referenceID": 2, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 102, "endOffset": 132}, {"referenceID": 4, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 149, "endOffset": 189}, {"referenceID": 3, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 210, "endOffset": 271}, {"referenceID": 6, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 210, "endOffset": 271}, {"referenceID": 9, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 210, "endOffset": 271}, {"referenceID": 13, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 210, "endOffset": 271}, {"referenceID": 1, "context": "Some of the most widely known members of this family include Broyden\u2019s (1965) method, the SR1 formula (Davidon, 1959; Broyden, 1967), the DFP method (Davidon, 1959; Fletcher & Powell, 1963) and the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970).", "startOffset": 61, "endOffset": 78}, {"referenceID": 1, "context": "Choosing a unit variance prior \u03a3i\u22121 = I \u2297 I recovers one of the oldest quasi-Newton algorithms: Broyden\u2019s method (1965): Bi = Bi\u22121 + (yi \u2212Bi\u22121si)s\u22bai s\u22bai si (7)", "startOffset": 96, "endOffset": 120}, {"referenceID": 4, "context": "(2) (Dennis & Mor\u00e9e, 1977) is the symmetric rank 1 (SR1) method (Davidon, 1959; Broyden, 1967): Bi = Bi\u22121 + (yi \u2212Bi\u22121si)(yi \u2212Bi\u22121si)\u22ba s\u22bai (yi \u2212Bi\u22121si) .", "startOffset": 64, "endOffset": 94}, {"referenceID": 2, "context": "(2) (Dennis & Mor\u00e9e, 1977) is the symmetric rank 1 (SR1) method (Davidon, 1959; Broyden, 1967): Bi = Bi\u22121 + (yi \u2212Bi\u22121si)(yi \u2212Bi\u22121si)\u22ba s\u22bai (yi \u2212Bi\u22121si) .", "startOffset": 64, "endOffset": 94}, {"referenceID": 9, "context": "(13) gives what is known as Powell\u2019s (1970) symmetric Broyden (PSB) update.", "startOffset": 28, "endOffset": 44}, {"referenceID": 4, "context": "(16) gives the DFP method (Davidon, 1959; Fletcher & Powell, 1963)", "startOffset": 26, "endOffset": 66}, {"referenceID": 3, "context": "And, if we exchange in the entire preceding derivation s ] y, B ] B\u22121, Bi\u22121 ] B\u22121 i\u22121, then we arrive at the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970), which ranks among the most widely used algorithms in machine learning overall.", "startOffset": 121, "endOffset": 182}, {"referenceID": 6, "context": "And, if we exchange in the entire preceding derivation s ] y, B ] B\u22121, Bi\u22121 ] B\u22121 i\u22121, then we arrive at the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970), which ranks among the most widely used algorithms in machine learning overall.", "startOffset": 121, "endOffset": 182}, {"referenceID": 9, "context": "And, if we exchange in the entire preceding derivation s ] y, B ] B\u22121, Bi\u22121 ] B\u22121 i\u22121, then we arrive at the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970), which ranks among the most widely used algorithms in machine learning overall.", "startOffset": 121, "endOffset": 182}, {"referenceID": 13, "context": "And, if we exchange in the entire preceding derivation s ] y, B ] B\u22121, Bi\u22121 ] B\u22121 i\u22121, then we arrive at the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970), which ranks among the most widely used algorithms in machine learning overall.", "startOffset": 121, "endOffset": 182}, {"referenceID": 1, "context": "And, if we exchange in the entire preceding derivation s ] y, B ] B\u22121, Bi\u22121 ] B\u22121 i\u22121, then we arrive at the BFGS method (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970), which ranks among the most widely used algorithms in machine learning overall. DFP and BFGS owe much of their popularity to the fact that the updated Bi,DFP and B \u22121 i,BFGS are guaranteed to be positive definite whenever Bi\u22121,DFP and B\u22121 i\u22121,BFGS are positive definite, respectively, and additionally y\u22bai si > 0. How helpful is this property? It is relatively straightforward to extend a theorem by Dennis & Mor\u00e9e (1977) to find that, assuming Bi\u22121 is positive definite, the posterior mean of Eq.", "startOffset": 122, "endOffset": 605}, {"referenceID": 10, "context": "Another option, not yet explored by us, may be offered by spline kernels (Minka, 2000).", "startOffset": 73, "endOffset": 86}, {"referenceID": 8, "context": "Fortunately, good, lightweight numerical approximations are available for this problem (Genz, 2004).", "startOffset": 87, "endOffset": 99}, {"referenceID": 11, "context": "Using a diagonal prior mean B0 and an argument largely analogous to the derivation of the L-BFGS algorithm (Nocedal, 1980) lowers cost to O(NM +M3), linear in N .", "startOffset": 107, "endOffset": 122}], "year": 2012, "abstractText": "Four decades after their invention, quasiNewton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.", "creator": "LaTeX with hyperref package"}}}