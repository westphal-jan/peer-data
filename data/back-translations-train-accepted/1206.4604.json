{"id": "1206.4604", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Learning the Experts for Online Sequence Prediction", "abstract": "Online sequence prediction is the problem of predicting the next element of a sequence given previous elements. This problem has been extensively studied in the context of individual sequence prediction, where no prior assumptions are made on the origin of the sequence. Individual sequence prediction algorithms work quite well for long sequences, where the algorithm has enough time to learn the temporal structure of the sequence. However, they might give poor predictions for short sequences. A possible remedy is to rely on the general model of prediction with expert advice, where the learner has access to a set of $r$ experts, each of which makes its own predictions on the sequence. It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of $\\log(r)$. But, without firm prior knowledge on the problem, it is not clear how to choose a small set of {\\em good} experts. In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences. We demonstrate the merits of our approach by applying it on the task of click prediction on the web.", "histories": [["v1", "Mon, 18 Jun 2012 14:42:16 GMT  (174kb)", "http://arxiv.org/abs/1206.4604v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["elad eban", "aharon birnbaum", "shai shalev-shwartz", "amir globerson"], "accepted": true, "id": "1206.4604"}, "pdf": {"name": "1206.4604.pdf", "metadata": {"source": "META", "title": "Learning the Experts for Online Sequence Prediction", "authors": ["Elad Eban", "Aharon Birnbaum", "Shai Shalev-Shwartz", "Amir Globerson"], "emails": ["elade@cs.huji.ac.il", "aharob01@cs.huji.ac.il", "shais@cs.huji.ac.il", "gamir@cs.huji.ac.il"], "sections": [{"heading": "1. Introduction", "text": "It is about predicting the next element in a sequence that takes into account the previous elements. Typical applications include stock market forecasts, clicks on predictions in web browsing and consumption, prediction appearance in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner (s). While the sequence prediction problem has been well studied, current solutions either require long sequences or require strong foreknowledge. In this work, we provide a method that uses training data to learn how to predict a novel sequence. As we will show, we use the training sequences to obtain the prior knowledge to predict novel sequences, it is most natural as an online prediction problem (Cesa-Bianchi and Lugosi, 2006) where we predict the next element, and then get the true value of the element when we suffer a loss."}, {"heading": "2. Problem Formulation", "text": "Let's use a finite alphabet (see pseudocode below) to use the prediction almost as a guarantee of the best performance. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) We have a number of such experts. (we) Algorithms. (we) Algorithms. (we) Algorithms. (we)"}, {"heading": "3. The Learning Algorithm", "text": "A simple approach to learning F if r > 1 follows the empirical risk mitigation problem (ERM). (ERM) 1As measured, for example, by its Littlestone dimension (Ben-David et al., 2009).Principle, namely, solving the optimization problem in F-H: | F | = r 1 m \u00b2 m \u00b2 i = 1 WM (F, x (i)).This problem could be difficult to optimize, since the objective function involves activating an algorithm and does not have a simple mathematical formulation. To overcome this difficulty, we show how an easier target can be used. In the light of Theorem 1 (generalized to convex replacement losses) we know that for each sequence x, WM (F, x) \u2264 min f \u00b2 F (f, x) + 4 log (F)."}, {"heading": "3.1. The class of bounded norm context trees", "text": "So far we have given a general scheme and have not described the specific amount of experts we are going to use. In the one that follows, we are going to give these two terms. Each function f: V * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "3.2. The LEX algorithm", "text": "We are now ready to describe our algorithm, which we call LEX (for learning experts). Our goal is to minimize the loss in Equation 4 with respect to the vectors wi and the parameters of the experts. As described in Section 3.1, we parameterize each expert using a context tree matrix U-Rk-Rk-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-d. As already mentioned, we can minimize Equation 4 by alternating optimization, minimizing via w in a closed form."}, {"heading": "4. Analysis", "text": "If we define the complexity of H (H), if we define a class of functions from Z to Q, we let Y set a target, and let us allow the complexity of H (H) to be a loss class. (H) We say that the complexity of H (H) is a loss class. (H) We say that the complexity of H (H) is a loss class. (H) We say that the complexity of H (H) is a loss class from Z to Q, let Y be a target group. (H) We say that the complexity of H (H) is a loss class. (H)"}, {"heading": "5. Related Work", "text": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al., 1992; Willems et al., 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al., 2010). One of the most useful tools is context trees that store informative stories and the likelihood of the next symbol in the face of this. However, all of this work considers predicting a sequence from a single source to be probable. In fact, our work extends to these single sequence predictions in the collaborative environment in which we model different sequences but limit the predictors to share a common structure (i.e. the experts used in the prediction)."}, {"heading": "6. Experiments", "text": "In the following, we evaluate the performance of the LEX algorithm (see Section 3.2) on two sets of data: the synthetic world and the real world. We compare it with the baselines described below."}, {"heading": "6.1. Baselines Models", "text": "We are looking at three different baseline models. The first is our LEX algorithm with r = 1 (we call this baseline 1-LEX), which is actually a batch trained PST (where the training uses the log loss).In this approach, all training sequences are modeled using a single PST that corresponds to an expert, so it does not directly model multiple time behaviors of the sequences in the data. Our second baseline is an online PST model that is evaluated individually on each test sequence.The training is done using the algorithm in (Dekel et al., 2010).As it is an online algorithm, it does not use the training data. However, with sufficiently long sequences, it will be able to optimally model each deterministic temporal behavior. In other words, this algorithm has the advantage of customization, but its performance depends crucially on the length of the sequence."}, {"heading": "6.2. Synthetic Data", "text": "We start by looking at sequences that follow one of two temporal patterns, and the sequences are generated as follows: First, randomly j, 1, 2} and then T samples are drawn according to the (independent) distribution: Pr (xt = x) = {2 \u2212 1 if x = j (2 (| \u03a3 | \u2212 1) \u2212 1 otherwise). We used the | \u03a3 | = 200 and generated a set of m = 1000 sequences, each with the length T = 250 (these parameters were selected to resemble the surfing data). We find that the maximum possible generalization accuracy of this data by construction is 0.5. We evaluate the accuracy of the online prediction using 400 test sequences. In Fig. 1 we show the accuracy (on test data) of LEX and the three baselines. We find that LEX approaches 0.5 accuracy by using about 50 sequences, 1-LEX and LMM to get closer to this accuracy (on test data) than LEX expresses for this accuracy (LEX is much less than LEX for 45 sequences)."}, {"heading": "6.3. Click Prediction Data", "text": "Here we consider a challenging task of predicting the surfing patterns of web users. Specifically, we use browser logs for users of an intra-net page. For each session, the sequence of URLs visited by each user was recorded by the web server. The data set contains 2000 such sequences with a length of 70-150. The area of the prediction problem consists of different URLs and its order of magnitude is | \u03a3 | = 189. The data was divided into train, validation and test sets, the sizes of the training sets vary, while the sizes of the validation and test sets were fixed at 200 and 800 sequences respectively. This addition smooths out the performance of the three base models and compares their performance with LEX. In this experiment, the R experts trained by LEX were combined with additional experts gained from the training of a 1-LEX algorithm, resulting in a pool of r + 1 learned experts who may not have enough time to follow the World Cup complement, which is the algorithm of short duration."}, {"heading": "7. Discussion", "text": "We have described and analyzed a method to learn the experts for online sequence prediction. Specifically, we have assigned them to the class of predictive sufficiency trees. Thus, our experts can capture dependencies on stories of any length. This is achieved by mapping context trees into a vector space and designing a norm for that space that takes into account long stories. As our generalization results show, the complexity of the model is not punished by the maximum possible length of the story (dimensionality of the matrix U), but by the effectively needed context-based story (captured by the norm of U). Our empirical results show that the temporal user-specific structure can actually be used to improve predictive accuracy. The proposed approach can be extended in several ways. First, we can consider different prediction goals: Instead of predicting the next symbol in the sequence that corresponds to the next URL, we can retract a ficator if there is an alternate problem, and we can retract a classifier if there is a probable one."}, {"heading": "R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data.", "text": "JMLR, 6: 1817-1853, 2005."}, {"heading": "S. Ben-David, D. Pa\u0301l, and S. Shalev-Shwartz. Agnostic", "text": "Online learning. In COLT, 2009."}, {"heading": "D. Blackwell. An analog of the minimax theorem for vector", "text": "Payouts. Pacific Journal of Math., 6 (1): 1-8, 1956.N. Cesa-Bianchi and G. Lugosi. Prediction, Learning and Games. Cambridge University Press, 2006.T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Trans. on Information Theory, IT-13 (1): 21-27, Jan. 1967."}, {"heading": "T. Cover and A. Shenhar. Compound Bayes predictors", "text": "for sequences with an obvious Markov structure. IEEE Transactions on Systems, Man, and Cybernetics, SMC7 (6): 421-424, June 1977."}, {"heading": "O. Dekel, S. Shalev-Shwartz, and Y. Singer. Individual sequence prediction using memory-efficient context trees.", "text": "IEEE Trans. on Information Theory, 2010.M. Feder, N. Merhav, and M. Gutman. Universal prediction of individual sequences. IEEE Trans. on Information Theory, 38: 1258-1270, 1992."}, {"heading": "P. F. Felzenszwalb, D. A. McAllester, and D. Ramanan.", "text": "A discriminatory, multi-scale, deformable sub-model. In CVPR, 2008."}, {"heading": "J. Hannan. Approximation to Bayes risk in repeated play.", "text": "In: M. Dresher, A. W. Tucker, and P. Wolfe, editors, Contributions to the Theory of Games, Volume III, pp. 97-139. 1957."}, {"heading": "D. P. Helmbold and R. E. Schapire. Predicting nearly as", "text": "and the best cut of a decision tree. Machine learning, 27 (1): 51-68, Apr. 1997."}, {"heading": "T. Hofmann. Probabilistic latent semantic analysis. In", "text": "Proc. of Uncertainty in Artificial Intelligence, 1999.M. Hutter. On the Foundations of Universal Sequence Prediction. Theory and Application of Calculation Models, pp. 408-420, 2006."}, {"heading": "N. Littlestone and M. K. Warmuth. The weighted majority", "text": "Algorithm. Information and Computation, 108: 212-261, 1994.A. Nemirovski and D. Yudin. Problem Complexity and Methodological Efficiency in Optimization. Nauka Publishing House, Moscow, 1978."}, {"heading": "F. Pereira and Y. Singer. An efficient extension to mixture", "text": "Techniques for Prediction and Decision Trees. Machine Learning, 36 (3): 183-199, 1999.S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation. In Inter. Conf. on WWW, pp. 811-820, 2010.H. Robbins. Asymptotically subminimax solutions of compound statistical decision problems. In Proceedings of the 2nd Berkeley symposium on mathematical statistics and probability, pp. 131-148, 1951.N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and rapid rates. NIPS, 2010.V. G. Vovk. Aggregating strategies. In COLT, 1990.F. Willems, Y. Shtarkov, and T. Tjalkens. The contexttree woghting method: basic properties. IEEE Trans. on Information Theory, 41 (3) -653."}, {"heading": "F. Wood, C. Archambeau, J. Gasthaus, L. James, and", "text": "Y. W. Teh. A stochastic memoizer for sequence data. In ICML, pp. 1129-1136, 2009.C.-N. J. Yu and T. Joachims. Learning structure svms with latent variables. In ICML, 2009."}, {"heading": "T. Zhang. Covering number bounds of certain regularized", "text": "Linear Function Classes. Journal of Machine Learning Research, 2: 527-550, 2002."}], "references": [{"title": "Multitask learning with expert advice", "author": ["J. Abernethy", "P. Bartlett", "A. Rakhlin"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2007}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "JMLR, 6:1817\u20131853,", "citeRegEx": "Ando and Zhang.,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "Agnostic online learning", "author": ["S. Ben-David", "D. P\u00e1l", "S. Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Ben.David et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2009}, {"title": "An analog of the minimax theorem for vector payoffs", "author": ["D. Blackwell"], "venue": "Pacific Journal of Math.,", "citeRegEx": "Blackwell.,? \\Q1956\\E", "shortCiteRegEx": "Blackwell.", "year": 1956}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "Cover and Hart.,? \\Q1967\\E", "shortCiteRegEx": "Cover and Hart.", "year": 1967}, {"title": "Compound Bayes predictors for sequences with apparent Markov structure", "author": ["T. Cover", "A. Shenhar"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Cover and Shenhar.,? \\Q1977\\E", "shortCiteRegEx": "Cover and Shenhar.", "year": 1977}, {"title": "Individual sequence prediction using memory-efficient context trees", "author": ["O. Dekel", "S. Shalev-Shwartz", "Y. Singer"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Universal prediction of individual sequences", "author": ["M. Feder", "N. Merhav", "M. Gutman"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "Feder et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Feder et al\\.", "year": 1992}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["P.F. Felzenszwalb", "D.A. McAllester", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2008}, {"title": "Approximation to Bayes risk in repeated play", "author": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "Predicting nearly as well as the best pruning of a decision tree", "author": ["D.P. Helmbold", "R.E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Helmbold and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Helmbold and Schapire.", "year": 1997}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "In Proc. of Uncertainty in Artificial Intelligence,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "On the foundations of universal sequence prediction", "author": ["M. Hutter"], "venue": "Theory and Applications of Models of Computation,", "citeRegEx": "Hutter.,? \\Q2006\\E", "shortCiteRegEx": "Hutter.", "year": 2006}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A. Nemirovski", "D. Yudin"], "venue": "Nauka Publishers,", "citeRegEx": "Nemirovski and Yudin.,? \\Q1978\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1978}, {"title": "An efficient extension to mixture techniques for prediction and decision trees", "author": ["F. Pereira", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Pereira and Singer.,? \\Q1999\\E", "shortCiteRegEx": "Pereira and Singer.", "year": 1999}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "In Inter. Conf. on WWW,", "citeRegEx": "Rendle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2010}, {"title": "Asymptotically subminimax solutions of compound statistical decision problems", "author": ["H. Robbins"], "venue": "In Proceedings of the 2nd Berkeley symposium on mathematical statistics and probability,", "citeRegEx": "Robbins.,? \\Q1951\\E", "shortCiteRegEx": "Robbins.", "year": 1951}, {"title": "Smoothness, lownoise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": null, "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Aggregating strategies", "author": ["V.G. Vovk"], "venue": "In COLT,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "The contexttree weighting method: basic properties", "author": ["F. Willems", "Y. Shtarkov", "T. Tjalkens"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "Willems et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Willems et al\\.", "year": 1995}, {"title": "A stochastic memoizer for sequence data", "author": ["F. Wood", "C. Archambeau", "J. Gasthaus", "L. James", "Y.W. Teh"], "venue": "In ICML,", "citeRegEx": "Wood et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2009}, {"title": "Learning structural svms with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Yu and Joachims.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Joachims.", "year": 2009}, {"title": "Covering number bounds of certain regularized linear function classes", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2002\\E", "shortCiteRegEx": "Zhang.", "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "Sequence prediction is most naturally cast as an online prediction problem (Cesa-Bianchi and Lugosi, 2006), where at every step we predict the next element, and then receive the true value of the element while suffering a loss if we made a prediction error.", "startOffset": 75, "endOffset": 106}, {"referenceID": 8, "context": "One classical approach to the problem is the so called universal sequence prediction class of methods (Feder et al., 1992; Hutter, 2006).", "startOffset": 102, "endOffset": 136}, {"referenceID": 13, "context": "One classical approach to the problem is the so called universal sequence prediction class of methods (Feder et al., 1992; Hutter, 2006).", "startOffset": 102, "endOffset": 136}, {"referenceID": 14, "context": "An alternative approach which does introduce prior knowledge is predicting with expert advice (Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 94, "endOffset": 137}, {"referenceID": 20, "context": "An alternative approach which does introduce prior knowledge is predicting with expert advice (Littlestone and Warmuth, 1994; Vovk, 1990).", "startOffset": 94, "endOffset": 137}, {"referenceID": 14, "context": "The Weighted-Majority algorithm (Littlestone and Warmuth, 1994) uses such experts to do online prediction, and is guaranteed to perform almost as well as the best expert.", "startOffset": 32, "endOffset": 63}, {"referenceID": 14, "context": "A performance guarantee for WM is provided in the following theorem (Littlestone and Warmuth, 1994).", "startOffset": 68, "endOffset": 99}, {"referenceID": 2, "context": "As measured, for example, by its Littlestone dimension (Ben-David et al., 2009).", "startOffset": 55, "endOffset": 79}, {"referenceID": 7, "context": "For our experts, we will be using a generalization of multiclass context trees following Dekel et al. (2010), described below.", "startOffset": 89, "endOffset": 109}, {"referenceID": 7, "context": "Following (Dekel et al., 2010), we aim to balance between long histories (can be very informative but are rare in the data hence are hard to learn) and short histories (less informative but easier to learn).", "startOffset": 10, "endOffset": 30}, {"referenceID": 19, "context": "Then, we bound the Rademacher complexity of this class using a generalization of Dudley\u2019s chaining technique, which is similar to a technique recently proposed in Srebro et al. (2010).", "startOffset": 163, "endOffset": 184}, {"referenceID": 15, "context": ", see Nemirovski and Yudin, 1978). This is similar to a method due to Zhang (2002), although our bound is slightly better.", "startOffset": 6, "endOffset": 83}, {"referenceID": 18, "context": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al.", "startOffset": 113, "endOffset": 159}, {"referenceID": 3, "context": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al.", "startOffset": 113, "endOffset": 159}, {"referenceID": 10, "context": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al.", "startOffset": 113, "endOffset": 159}, {"referenceID": 5, "context": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al., 1992; Willems et al., 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al.", "startOffset": 183, "endOffset": 272}, {"referenceID": 6, "context": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al., 1992; Willems et al., 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al.", "startOffset": 183, "endOffset": 272}, {"referenceID": 8, "context": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al., 1992; Willems et al., 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al.", "startOffset": 183, "endOffset": 272}, {"referenceID": 21, "context": "The problem of sequence prediction has a fairly long history and has received much attention from game theorists (Robbins, 1951; Blackwell, 1956; Hannan, 1957), information theorists (Cover and Hart, 1967; Cover and Shenhar, 1977; Feder et al., 1992; Willems et al., 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al.", "startOffset": 183, "endOffset": 272}, {"referenceID": 11, "context": ", 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al., 2010).", "startOffset": 42, "endOffset": 148}, {"referenceID": 16, "context": ", 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al., 2010).", "startOffset": 42, "endOffset": 148}, {"referenceID": 4, "context": ", 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al., 2010).", "startOffset": 42, "endOffset": 148}, {"referenceID": 7, "context": ", 1995), and machine learning researchers (Helmbold and Schapire, 1997; Pereira and Singer, 1999; Cesa-Bianchi and Lugosi, 2006; Dekel et al., 2010).", "startOffset": 42, "endOffset": 148}, {"referenceID": 0, "context": "Another related line of work is multitask prediction (e.g., see Ando and Zhang, 2005; Abernethy et al., 2007), in which one considers several different multiclass prediction problems and seeks a common feature space for those.", "startOffset": 53, "endOffset": 109}, {"referenceID": 22, "context": "A more recent approach to sequence modeling is the \u201csequence memoizer\u201d, which is based on nonparametric Bayesian models (Wood et al., 2009).", "startOffset": 120, "endOffset": 139}, {"referenceID": 12, "context": "Another possible approach to the problem is to use probabilistic latent variable models (Hofmann, 1999) or their discriminative counterparts (Felzenszwalb et al.", "startOffset": 88, "endOffset": 103}, {"referenceID": 9, "context": "Another possible approach to the problem is to use probabilistic latent variable models (Hofmann, 1999) or their discriminative counterparts (Felzenszwalb et al., 2008; Yu and Joachims, 2009).", "startOffset": 141, "endOffset": 191}, {"referenceID": 23, "context": "Another possible approach to the problem is to use probabilistic latent variable models (Hofmann, 1999) or their discriminative counterparts (Felzenszwalb et al., 2008; Yu and Joachims, 2009).", "startOffset": 141, "endOffset": 191}, {"referenceID": 17, "context": "Prediction in such a setting was recently addressed in Rendle et al. (2010). Unlike in our case, they have access to multiple training sequences from particular users, and prediction is done on these users.", "startOffset": 55, "endOffset": 76}, {"referenceID": 7, "context": "Training is done using the algorithm in (Dekel et al., 2010).", "startOffset": 40, "endOffset": 60}], "year": 2012, "abstractText": "Online sequence prediction is the problem of predicting the next element of a sequence given previous elements. This problem has been extensively studied in the context of individual sequence prediction, where no prior assumptions are made on the origin of the sequence. Individual sequence prediction algorithms work quite well for long sequences, where the algorithm has enough time to learn the temporal structure of the sequence. However, they might give poor predictions for short sequences. A possible remedy is to rely on the general model of prediction with expert advice, where the learner has access to a set of r experts, each of which makes its own predictions on the sequence. It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of log(r). But, without firm prior knowledge on the problem, it is not clear how to choose a small set of good experts. In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences. We demonstrate the merits of our approach by applying it on the task of click prediction on the web.", "creator": "LaTeX with hyperref package"}}}