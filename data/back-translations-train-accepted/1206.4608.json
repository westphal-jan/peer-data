{"id": "1206.4608", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Hybrid Algorithm for Convex Semidefinite Optimization", "abstract": "We present a hybrid algorithm for optimizing a convex, smooth function over the cone of positive semidefinite matrices. Our algorithm converges to the global optimal solution and can be used to solve general large-scale semidefinite programs and hence can be readily applied to a variety of machine learning problems. We show experimental results on three machine learning problems (matrix completion, metric learning, and sparse PCA) . Our approach outperforms state-of-the-art algorithms.", "histories": [["v1", "Mon, 18 Jun 2012 14:44:28 GMT  (247kb)", "http://arxiv.org/abs/1206.4608v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.NA stat.ML", "authors": ["s\u00f6ren laue"], "accepted": true, "id": "1206.4608"}, "pdf": {"name": "1206.4608.pdf", "metadata": {"source": "META", "title": "A Hybrid Algorithm for Convex Semidefinite Optimization", "authors": ["S\u00f6ren Laue"], "emails": ["soeren.laue@uni-jena.de"], "sections": [{"heading": "1. Introduction", "text": "We consider the following unrestricted semidefinitive optimization problem: min f (X) s.t. X 0, (1) where f (X): Rn \u00d7 n \u2192 R is a convex and differentiable function over the cone of positive semidefined matrices. Many machine learning problems can be considered semidefinitive optimization problems. Prominent examples are sparse PCA (d'Aspremont et al., 2007), remote metric learning (Xing et al., 2002), nonlinear dimension reduction (Weinberger et al., 2006), multiple kernel learning (Lanckriet et al., 2004), multitasking learning (Obozinski et al., 2010) and matrix completion (Srebro et al., 2004). We provide an algorithm that efficiently solves general large-scale, unrestricted semidefinitive optimization problems (Lanckriet et al, 2004), multibounded optimization algorithm (Learning)."}, {"heading": "1.1. Related Work", "text": "The fact is that we are able to assert ourselves, that we are able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able, that we are going to be able."}, {"heading": "2. The Hybrid Algorithm", "text": "Our algorithm is in pseudo-code in algorithm 1.Algorithm 1 Hybrid algorithm Input: Smooth, convex function f: Rn \u00b7 n \u2192 R Output: Approximate solution of problem 1 Initialize X0 = 0, V0 = 0 repeatIncrease rank by 1 using Hazan update: Compute vi = ApproxEV (\u2212 approximate f (ViV Ti), \u03b5 Xi + 1 by finding a local minimum of f (V V V V T) s. t. \u03b1, \u03b2 0.Set Vi + 1 = [\u221a \u03b1 \u00b7 Vi \u00b7 vi].Run nonlinear update: Improve Vi + 1 by finding a local minimum of f (V V T) would be V starting with Vi + 1.until approximation guarantee has been reached edThe notation [Vi, vi]."}, {"heading": "3. Applications and Experiments", "text": "We implemented our hybrid algorithm in Matlab exactly as described in Algorithm 1. For the non-linear update, we use minFunc (Schmidt), which implements the BFGS algorithm with limited memory. The two-variable optimization problem in the Ranking 1 update is also solved with minFunc. We use the default settings of minFunc. Approximate eigenvector calculation is done with the Matlab function pages. We conducted all experiments in single-thread mode on a 2.50 GHz CPU."}, {"heading": "3.1. Matrix Completion", "text": "In this section we look at the matrix completion problem, which is used for the collaborative filtering of all problems. (D) W's \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s. (D) W's\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "3.2. Metric Learning", "text": "The second problem we are addressing is the metric learning problem (i, j, j, j). We are a marked problem X = (xi, yi) i. Let's be a set that contains all the pairs of indices (i, j) whose data points xi and xj are similar to each other, i.e. its labels yi and yj are the same; let's include all indices of data points that are dissimilar to each other. The metric learning problem is that we can find a positive semidefinitive matrix A, so that the Mahalanobis distance between xi and xj is defined as dA (i, j, j) TA (xi \u2212 xj) TA (xi \u2212 xj) The metric learning problem is that the search for a positive semidefinitive matrix A, which is located among the induced Mahalanobis distance, points that are similar to each other and points that are dissimilar, can be considered a semi-definitive optimization problem (Xing, 2002)."}, {"heading": "3.3. Sparse PCA", "text": "As a third problem, we consider the sparse principal component analysis problem (sparse PCA). For a given covariance matrix A-Rn \u00b7 n, the sparse PCA tries to find a sparse vector X that maximizes xTAx, i.e. a sparse major component of A. However, this problem can be eased into the following SDP (d'Aspremont et al., 2007): a minimal eigenvector of the solution of the problem (i, j) | Xij | \u2212 A-X s. Tr (X) = 1X 0, (6) where A \u2022 X stands for Tr (ATX). In a subsequent rounding step, the largest eigenvector of the solution of the problem (6) is returned as solution vector x. Parameter 2 controls the compromise between the frugality of x and the declared variance xTAx. Problem (6) is not in the form (1)."}, {"heading": "4. Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. The Duality Gap", "text": "In this section we have a duality gap for problem (1) and analyze the runtime of algorithm (1). (1) Let us (1) Let us (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) Let (1) (1) Let (1) (1)"}, {"heading": "4.2. Runtime and Convergence Analysis", "text": "In this section we will show that after most O (1\u03b5) iterations, algorithm 1 (\u03b2) returns a solution that represents an approximation to the global optimal solution. Proof follows the lines of (Clarkson, 2008) and (Xi Xi Xi Xi, 2008). However, we improve by lowering the accuracy required for eigenvector calculation from O (\u03b52) to O (\u03b5). Let us define the curvature constant Cf as follows: Cf: = sup X, Z (Xi), Xi (Xi), Y = X (1) to O (1)."}, {"heading": "5. Discussion", "text": "We have provided an algorithm that optimizes convex, smooth functions over the cone of positive semi-defined matrices. It can easily be used for a variety of machine learning problems, as many of them fall within this framework or can be transferred into such a problem. In this paper, we have conducted experiments on three of these problems and demonstrated that our algorithm significantly outperforms modern solvers without having to match it to any of the specific tasks. In addition, the proposed algorithm has the advantage of being very simple and has the guarantee of always adapting to the global optimal solution. In the future, we plan to implement our algorithm for further acceleration in C + +."}, {"heading": "Acknowledgements", "text": "The author would like to thank Georgiana Dinu and Joachim Giesen for the stimulating discussions on the topic, which were supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) with the GI-711 / 3-2 grant."}], "references": [{"title": "Fast algorithms for approximate semidefinite programming using the multiplicative weights update method", "author": ["Arora", "Sanjeev", "Hazan", "Elad", "Kale", "Satyen"], "venue": "In FOCS,", "citeRegEx": "Arora et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2005}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["Burer", "Samuel", "Monteiro", "Renato D.C"], "venue": "Math. Program.,", "citeRegEx": "Burer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Burer et al\\.", "year": 2003}, {"title": "Robust principal component analysis", "author": ["Cand\u00e8s", "Emmanuel J", "Li", "Xiaodong", "Ma", "Yi", "Wright", "John"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm", "author": ["Clarkson", "Kenneth L"], "venue": "In SODA,", "citeRegEx": "Clarkson and L.,? \\Q2008\\E", "shortCiteRegEx": "Clarkson and L.", "year": 2008}, {"title": "A direct formulation of sparse PCA using semidefinite programming", "author": ["d\u2019Aspremont", "Alexandre", "El Ghaoui", "Laurent", "Jordan", "Michael I", "Lanckriet", "Gert R. G"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Approximating semidefinite programs in sublinear time", "author": ["Garber", "Dan", "Hazan", "Elad"], "venue": "In NIPS,", "citeRegEx": "Garber et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garber et al\\.", "year": 2011}, {"title": "Low-rank matrix approximation with weights or missing data is NP-hard", "author": ["Gillis", "Nicolas", "Glineur", "Fran\u00e7ois"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Gillis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gillis et al\\.", "year": 2011}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Goemans", "Michel X", "Williamson", "David P"], "venue": "J. ACM,", "citeRegEx": "Goemans et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Goemans et al\\.", "year": 1995}, {"title": "CVX: Matlab software for disciplined convex programming, version 1.21", "author": ["Grant", "Michael", "Boyd", "Stephen"], "venue": "http: //cvxr.com/cvx,", "citeRegEx": "Grant et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grant et al\\.", "year": 2011}, {"title": "A sublinear-time randomized approximation algorithm for matrix games", "author": ["Grigoriadis", "Michael D", "Khachiyan", "Leonid G"], "venue": "Operations Research Letters,", "citeRegEx": "Grigoriadis et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Grigoriadis et al\\.", "year": 1995}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["Hazan", "Elad"], "venue": "In LATIN,", "citeRegEx": "Hazan and Elad.,? \\Q2008\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2008}, {"title": "Random conic pursuit for semidefinite programming", "author": ["Kleiner", "Ariel", "Rahimi", "Ali", "Jordan", "Michael I"], "venue": "In NIPS,", "citeRegEx": "Kleiner et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kleiner et al\\.", "year": 2010}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Lanckriet", "Gert R. G", "Cristianini", "Nello", "Bartlett", "Peter", "Ghaoui", "Laurent El", "Jordan", "Michael I"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski and Arkadi.,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski and Arkadi.", "year": 2004}, {"title": "Smooth minimization of non-smooth functions", "author": ["Nesterov", "Yurii"], "venue": "Math. Program.,", "citeRegEx": "Nesterov and Yurii.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 2005}, {"title": "Smoothing technique and its applications in semidefinite optimization", "author": ["Nesterov", "Yurii"], "venue": "Math. Program.,", "citeRegEx": "Nesterov and Yurii.,? \\Q2007\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 2007}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["Obozinski", "Guillaume", "Taskar", "Ben", "Jordan", "Michael I"], "venue": "Statistics and Computing,", "citeRegEx": "Obozinski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2010}, {"title": "Large-scale convex minimization with a low-rank constraint", "author": ["Schmidt", "Shai", "Gonen", "Alon", "Shamir", "Ohad"], "venue": "In ICML,", "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Maximum-margin matrix factorization", "author": ["Srebro", "Nathan", "Rennie", "Jason D. M", "Jaakkola", "Tommi"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Using sedumi 1.02, a MATLAB toolbox for optimization over symmetric cones", "author": ["Sturm", "Jos F"], "venue": "Optimization Methods and Software,", "citeRegEx": "Sturm and F.,? \\Q1999\\E", "shortCiteRegEx": "Sturm and F.", "year": 1999}, {"title": "Graph laplacian regularization for largescale semidefinite programming", "author": ["Weinberger", "Kilian Q", "Sha", "Fei", "Zhu", "Qihui", "Saul", "Lawrence K"], "venue": "In NIPS,", "citeRegEx": "Weinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2006}, {"title": "Alternating direction augmented lagrangian methods for semidefinite programming", "author": ["Wen", "Zaiwen", "Goldfarb", "Donald", "Yin", "Wotao"], "venue": "Math. Prog. Comp.,", "citeRegEx": "Wen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2010}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Xing", "Eric P", "Ng", "Andrew Y", "Jordan", "Michael I", "Russell", "Stuart"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "Prominent examples include sparse PCA (d\u2019Aspremont et al., 2007), distance metric learning (Xing et al.", "startOffset": 38, "endOffset": 64}, {"referenceID": 22, "context": ", 2007), distance metric learning (Xing et al., 2002), nonlinear dimensionality reduction (Weinberger et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 20, "context": ", 2002), nonlinear dimensionality reduction (Weinberger et al., 2006), multiple kernel learning (Lanckriet et al.", "startOffset": 44, "endOffset": 69}, {"referenceID": 12, "context": ", 2006), multiple kernel learning (Lanckriet et al., 2004), multitask learning (Obozinski et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 16, "context": ", 2004), multitask learning (Obozinski et al., 2010), and matrix completion (Srebro et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 18, "context": ", 2010), and matrix completion (Srebro et al., 2004).", "startOffset": 31, "endOffset": 52}, {"referenceID": 4, "context": "Prominent examples include sparse PCA (d\u2019Aspremont et al., 2007), distance metric learning (Xing et al., 2002), nonlinear dimensionality reduction (Weinberger et al., 2006), multiple kernel learning (Lanckriet et al., 2004), multitask learning (Obozinski et al., 2010), and matrix completion (Srebro et al., 2004). We provide an algorithm that solves general large-scale unconstrained semidefinite optimization problems efficiently. The idea to our algorithm is a hybrid approach: we combine the algorithm of Hazan (2008) with a standard quasi-Newton algorithm.", "startOffset": 39, "endOffset": 522}, {"referenceID": 0, "context": "Examples include (Nesterov, 2007; Nemirovski, 2004) and (Arora et al., 2005) where the multiplicative weights update rule is employed.", "startOffset": 56, "endOffset": 76}, {"referenceID": 0, "context": "The algorithm of (Arora et al., 2005) has been randomized by (Garber & Hazan, 2011) based on the same idea as in (Grigoriadis & Khachiyan, 1995) to achieve sublinear running time.", "startOffset": 17, "endOffset": 37}, {"referenceID": 11, "context": "Another randomized algorithm has appeared in (Kleiner et al., 2010).", "startOffset": 45, "endOffset": 67}, {"referenceID": 21, "context": "Furthermore, alternating direction methods have been proposed to solve SDPs (Wen et al., 2010).", "startOffset": 76, "endOffset": 94}, {"referenceID": 2, "context": "to the robust PCA approach for matrix completion with low rank (Cand\u00e8s et al., 2011).", "startOffset": 63, "endOffset": 84}, {"referenceID": 22, "context": "This problem can be cast as a semidefinite optimization problem(Xing et al., 2002):", "startOffset": 63, "endOffset": 82}, {"referenceID": 11, "context": "We follow the experimental setting of (Kleiner et al., 2010).", "startOffset": 38, "endOffset": 60}, {"referenceID": 22, "context": "We compared our approach against an interior point method implemented in SeDuMi (Sturm, 1999) (via CVX (Grant & Boyd, 2011)) and the algorithm of (Xing et al., 2002) which is a projected gradient approach and was specifically designed to solve the above SDP.", "startOffset": 146, "endOffset": 165}, {"referenceID": 11, "context": "We could not directly compare our algorithm to that of (Kleiner et al., 2010) as the code was not available.", "startOffset": 55, "endOffset": 77}, {"referenceID": 22, "context": "However, the authors show that it performs similarly to (Xing et al., 2002).", "startOffset": 56, "endOffset": 75}, {"referenceID": 11, "context": "Following (Kleiner et al., 2010), we ran a second set of experiments on synthetic data in order to measure the dependence on the dimension d.", "startOffset": 10, "endOffset": 32}, {"referenceID": 4, "context": "This problem can be relaxed into the following SDP (d\u2019Aspremont et al., 2007): min \u03c1 \u2211 (i,j) |Xij | \u2212A \u2022X s.", "startOffset": 51, "endOffset": 77}, {"referenceID": 11, "context": "We again follow the experimental setting of (Kleiner et al., 2010) and we compare to an interior point method and to a state-of-the-art algorithm, the DSPCA algorithm (d\u2019Aspremont et al.", "startOffset": 44, "endOffset": 66}, {"referenceID": 4, "context": ", 2010) and we compare to an interior point method and to a state-of-the-art algorithm, the DSPCA algorithm (d\u2019Aspremont et al., 2007) which is specifically designed to solve Problem (6).", "startOffset": 108, "endOffset": 134}], "year": 2012, "abstractText": "We present a hybrid algorithm for optimizing a convex, smooth function over the cone of positive semidefinite matrices. Our algorithm converges to the global optimal solution and can be used to solve general largescale semidefinite programs and hence can be readily applied to a variety of machine learning problems. We show experimental results on three machine learning problems. Our approach outperforms state-of-the-art algorithms.", "creator": "LaTeX with hyperref package"}}}