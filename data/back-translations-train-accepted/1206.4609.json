{"id": "1206.4609", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "On multi-view feature learning", "abstract": "Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations.", "histories": [["v1", "Mon, 18 Jun 2012 14:45:17 GMT  (402kb)", "http://arxiv.org/abs/1206.4609v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["roland memisevic"], "accepted": true, "id": "1206.4609"}, "pdf": {"name": "1206.4609.pdf", "metadata": {"source": "META", "title": "On multi-view feature learning", "authors": ["Roland Memisevic"], "emails": ["ro@cs.uni-frankfurt.de"], "sections": [{"heading": "1. Introduction", "text": "In recent years, we have focused a lot of attention on image pairs because they produce images that are useful for recognition. However, while recognition is important in a variety of tasks, many problems in vision are associated with coding the relationship between observations and individual observations. The basic idea behind these models is that hidden variables go beyond filter reactions applied to two observations and thus correlate the responses. In the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, the relationships between these models are encrypted."}, {"heading": "2. Background on multi-view sparse coding", "text": "In this context, it should be noted that this is an attempt to refute the presumption that the presumption that it is a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presumption, a presumption is a presupposition, a presupposition is a presupposition, a presupposition is a presupposition is a presupposition, a presupposition is a presupposition is a presupposition is a presupposition, a presupposition is a presupposition is a presupposition, a presupposition is a presupposition is a presupposition, a presupposition is a presupposition is a presupposition, a presupposition is a presupposition is a presupposition, a presupposition is a presupposition, a presupposition is a presupposition, a presupposition is a presupposition, a presupposition is a presupposition, a presupposition is a presupposition, a presupposition is a presupposition, a presupposition, a presupposition is a presupposition, a presupposition, a presupposition is a presupposition, a presupposition, a presupposition is a presupposition, a presupposition, a presupposition is, a presupposition, a presupposition, a presupposition is, a presupposition, a presupposition is, a presupposition, a presupposition, a supposition, a presupposition, a presupposition is, a presupposition, a presupposition is, a presupposition, a supposition,"}, {"heading": "2.1. Energy models", "text": "The activity of a hidden unit in an energy model is typically defined as the sum of quadratic filter responses, which can be regarded as writtenz = WT (BTx) * (BTx), with B containing image filters in its columns. W is usually limited so that each hidden variable, zk, calculates the sum of only a subset of all products. In this way, hidden variables can be regarded as encoding the norm of a projection of x onto a subspace3. Energy models are also referred to as \"subspace\" or \"square compaction.\" For our analysis, it is important to note that when we apply an energy model to the concatenation of two images, x and y, we get an answer closely related to the reaction of a multipart model."}, {"heading": "3. Eigenspace analysis", "text": "In fact, we need to focus our attention on the transformations we are undertaking in \"pixel-space.\" It is also the case that we need to apply non-linearity in order to achieve the activity. It is the case that all the relevant transformations, such as the transformation, we are undergoing."}, {"heading": "3.1. Commuting warps share eigenspaces", "text": "If eigenspaces are divided, then two transformations differ only in the rotation angles within eigenspaces. In this case, we can represent multiple transformations with a single set of characteristics, as we will show. An example of a common eigenspace is the Fourier base, which is shared among translations (Gray, 2005). Less obvious examples are gabor characteristics, which can be imagined as the eigenbases of local translations, or characteristics that represent spatial rotations. Formally, a group of matrices shares eigenvectors when they oscillate (Horn & Johnson, 1990). This can be seen by considering two arbitrary matrices A and B with AB = BA and \u03bb, v as eigenvalue / eigenvector pair of B with multiplicity Eins. It is true that BAv = ABv = AvAvAvAvAv.Therefore, Av is also an eigenvector of B with the same eigenvalue."}, {"heading": "3.2. Extracting transformations", "text": "Consider the following task: For two images x and y, we determine the transformation L that relates it to each other, assuming L belongs to a certain class of transformations.The significance of pendulum transformations for our analysis is that, since they have an inherent basis, two arbitrary transformations differ only in the rotation angles in the common inherent spaces. Consequently, the transformation can be extracted from the given image pair (x, y) by simply restoring the rotation angles between the projections of x and y on the inherent spaces. To this end, we consider the real and complex parts vR and vI of a characteristic v = vR + ivI, where i = \u221a \u2212 1. The real and imaginary coordinates of the projection pvx on the invariant subspace associated with v are given by vTRx and v T I x, respectively."}, {"heading": "3.3. The subspace aperture problem", "text": "Note, however, that the normalization of each projection to 1 of a division by the sum of the quadratic filter responses is equivalent to an operation that is highly unstable when a projection is close to zero. This is the case when one of the images is almost orthogonal to the invariant subspace. This, in turn, means that the rotation angle from the given image cannot be restored because the image is too close to the axis of rotation. It can be seen as a subspace generalization of the well-known aperture problem beyond the translation to the set of orthogonal transformations. Normalization would ignore this problem and provide the illusion of a recovered angle, even if the aperture problem makes detection of the transformation component impossible. In the next section, we will discuss how to achieve this problem by reframing the problem as a detection task."}, {"heading": "3.4. Mapping units as rotation detectors", "text": "For each eigenvector, v, and rotation angle, \u03b8, define the complex output image filterv\u03b8 = exp (i\u03b8) vwhich represent a projection and simultaneous rotation by \u03b8. This allows us to define a subspace rotation detector with preferred angle \u03b8 as follows: r\u03b8 = (vTRy) (v \u03b8 R x) + (vTI y) (v \u03b8 x) (4), where drawings R and I define the real and imaginary part of the filters as before. If projections are normalized to length 1, we have haverordable = cosprehy cos (\u03c6x x +) + sin\u03c6y sin (sp x +) = cos (sp \u2212 sp), the maximum shape of the filters as before. If projections are standardized to length 1, we are haverzzling = cosprehi cos cos cos (sp) + sin\u03c6y sin (sp)."}, {"heading": "3.5. Relation to energy models", "text": "By concatenating images x and y and filters v and v\u03b8, we can approach the reaction of the partial space rotation detector (equation 4) with the response of an energy detector: r\u03b8 = (((vR Ty) + (vR T x))) 2 + (vI Ty) + (vI Ty) 2 = 2 (vR Ty) (v\u03b8I T x) 2 (7) Equation 7 corresponds to Equation 4 up to the four square terms. The four square terms correspond to the sum of the square norms of the square norms of the projections of x and y on the immutable partial space. Thus, they provide information on the unfoldability of transformations. These energy formations make the response of the energy domain to the two energy squares of the detectors more interesting (equation of x and y on the immutable partial space)."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Learning quadrature pairs", "text": "Figure 2 shows random subsets of input-output filter pairs obtained from rotations of random dot maps (upper plot) and from a mixed dataset consisting of random rotations and random translations (lower plot). We separate the two levels of pooling, W and P, and we limit P to a band diagonal with the entries Pi, i = Pi, i + 1 = 1 and 0 elsewhere. Therefore, filters must come in pairs, which we roughly square (each pair spans the subspace connected to a rotation detector). Figure 2 shows that this is indeed the case after training, using a modification of a higher-order autoencoder (Memisevic, 2011) for training, but we expect similarities to apply to other multi-view models5."}, {"heading": "4.2. Learning \u201ceigenmovies\u201d", "text": "Both energy models and cross-correlation models can be applied to more than two images: G4 can be modified to include all cross-references, or any that are considered relevant (for example, adjacent frames that would amount to a \"Markov\" type gating model of a video); alternatively, we can calculate for the energy mechanism the square of concatenating more than two images instead of G7, in which case we get the detector response = (vtI T xs) 2 + (vsI T xs) 2 = (vsI T xs) 2 = vsR T xs) (vtR T xs) (vtR T xt) + vtI T xs) (8) 5Code and datasets are available at http: / www.cs.toronto.edu / ~ rfm / multiview / index.htmlwhere contains the quadratic terms of the energy model for satisfaction."}, {"heading": "5. Learning invariances by learning transformations", "text": "Our analysis suggests that the reactions of the detector to this pair are not affected by transformations to which they are attuned, as these cause only a rotation within the subspace of the detector, leaving the norm of projections unchanged. However, any other transformation (such as showing a completely different pair of images) can change representation: projections can become larger or smaller as the transformation changes the degree of alignment of the images with the invariant subspaces. This suggests that we can learn features that are invariant in terms of one type of transformation and at the same time selective in terms of any other type of transformation as follows: we separate the two pooling levels into a band matrix P and a full matrix W (cf)."}, {"heading": "6. Conclusions", "text": "Our analysis helps us understand why Fourier characteristics and circular Fourier characteristics appear when we train transformation models for shifts and rotations, and why square pooling models work well in action and motion detection tasks. Our analysis also shows how the aperture problem implies that we can learn invariant characteristics as a by-product of learning about transformations. The fact that squaring nonlinearity and multiplicative interactions can support relationship learning suggests that these can help increase the role of statistical learning in vision in general. By learning about relationships, we can extend the applicability of sparse coding models beyond recognizing objects in static, single images to tasks that involve the merging of multiple views, including geometric inferences."}, {"heading": "Acknowledgments", "text": "This work was supported by the Federal Ministry of Education and Research (BMBF) in the project 01GQ0841 (BFNT Frankfurt)."}], "references": [{"title": "Spatiotemporal energy models for the perception of motion", "author": ["E.H. Adelson", "J.R. Bergen"], "venue": "J. Opt. Soc. Am. A,", "citeRegEx": "Adelson and Bergen,? \\Q1985\\E", "shortCiteRegEx": "Adelson and Bergen", "year": 1985}, {"title": "Suitability of V1 energy models for object classification", "author": ["Bergstra", "James", "Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me"], "venue": "Neural Computation, pp", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Unsupervised learning of a steerable basis for invariant image representations", "author": ["M Bethge", "S Gerwinn", "Macke", "JH"], "venue": "In Human Vision and Electronic Imaging XII. SPIE,", "citeRegEx": "Bethge et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bethge et al\\.", "year": 2007}, {"title": "Toeplitz and circulant matrices: a review", "author": ["Gray", "Robert M"], "venue": "Commun. Inf. Theory,", "citeRegEx": "Gray and M.,? \\Q2005\\E", "shortCiteRegEx": "Gray and M.", "year": 2005}, {"title": "Matrix Analysis", "author": ["Horn", "Roger A", "Johnson", "Charles R"], "venue": null, "citeRegEx": "Horn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Horn et al\\.", "year": 1990}, {"title": "Emergence of phaseand shift-invariant features by decomposition of natural images into independent feature subspaces", "author": ["Hyv\u00e4rinen", "Aapo", "Hoyer", "Patrik"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2000}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision", "author": ["Hyv\u00e4rinen", "Aapo", "J. Hurri", "Hoyer", "Patrik O"], "venue": null, "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Larochelle", "Hugo", "Erhan", "Dumitru", "Courville", "Aaron", "Bergstra", "James", "Bengio", "Yoshua"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "In Proc. CVPR,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Gradient-based learning of higherorder image features", "author": ["Memisevic", "Roland"], "venue": "In Proceedings of the International Conference on Computer Vision,", "citeRegEx": "Memisevic and Roland.,? \\Q2011\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2011}, {"title": "Learning to represent spatial transformations with factored higherorder Boltzmann machines", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Computing stereo disparity and motion with known binocular cell properties", "author": ["Qian", "Ning"], "venue": "Neural Computation,", "citeRegEx": "Qian and Ning.,? \\Q1994\\E", "shortCiteRegEx": "Qian and Ning.", "year": 1994}, {"title": "Modeling Pixel Means and Covariances Using Factorized ThirdOrder Boltzmann Machines", "author": ["Ranzato", "Marc\u2019Aurelio", "Hinton", "Geoffrey E"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Modeling the joint density of two images under a variety of transformations", "author": ["J. Susskind", "R. Memisevic", "G. Hinton", "M. Pollefeys"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Susskind et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2011}, {"title": "Convolutional learning of spatiotemporal features", "author": ["Taylor", "W. Graham", "Fergus", "Rob", "LeCun", "Yann", "Bregler", "Christoph"], "venue": "In Proc. European Conference on Computer Vision,", "citeRegEx": "Taylor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Independent component analysis of natural image sequences yields spatiotemporal filters similar to simple cells in primary visual cortex", "author": ["L. van Hateren", "J. Ruderman"], "venue": "Proc. Biological Sciences,", "citeRegEx": "Hateren and Ruderman,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Ruderman", "year": 1998}], "referenceMentions": [{"referenceID": 13, "context": "Task-specific filterpairs emerge when training on natural transformations, like facial expression changes (Susskind et al., 2011) or natural video (Taylor et al.", "startOffset": 106, "endOffset": 129}, {"referenceID": 14, "context": ", 2011) or natural video (Taylor et al., 2010), and they were shown to yield state-of-the-art recognition performance in these domains.", "startOffset": 25, "endOffset": 46}, {"referenceID": 8, "context": "Multi-view feature learning models are also closely related to energy models of complex cells (Adelson & Bergen, 1985), which, in turn, have been successfully applied to video understanding, too (Le et al., 2011).", "startOffset": 195, "endOffset": 212}, {"referenceID": 1, "context": "They have also been used to learn within-image correlations by letting input and output images be the same (Ranzato & Hinton, 2010; Bergstra et al., 2010).", "startOffset": 107, "endOffset": 154}, {"referenceID": 6, "context": "To adapt the parameters, W , based on a set of example patches {x\u03b1} one can use a variety of methods, including maximizing the average sparsity of z, minimizing a form of reconstruction error, maximizing the likelihood of the observations via Gibbs sampling, and others (see, for example, (Hyv\u00e4rinen et al., 2009) and references therein).", "startOffset": 289, "endOffset": 313}, {"referenceID": 2, "context": "(Bethge et al., 2007) use the term generalized quadrature pair to refer to the eigen-features of these transformations.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Imposing a norm constraint has been a common approach to stabilizing learning (Ranzato & Hinton, 2010; Memisevic, 2011; Susskind et al., 2011), but it has not been clear why imposing norm constraints help.", "startOffset": 78, "endOffset": 142}, {"referenceID": 14, "context": "It is interesting to note that both, multiview sparse coding models (Taylor et al., 2010) and energy models (Le et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 8, "context": ", 2010) and energy models (Le et al., 2011) were recently shown to yield highly competitive performance in action recognition tasks, which require the encoding of motion in videos.", "startOffset": 26, "endOffset": 43}, {"referenceID": 7, "context": "We tested this approach on the \u201crotated MNIST\u201ddataset from (Larochelle et al., 2007), which consists of 72000 MNIST digit images of size 28 \u00d7 28 pixels that are rotated by arbitrary random angles (\u2212180 to 180 degrees; 12000 train-, 60000 test-cases, classes range from 0 \u2212 9).", "startOffset": 59, "endOffset": 84}, {"referenceID": 7, "context": "Since the number of training cases is fairly large, most exemplars are represented at most angles, so even linear classifiers perform well (Larochelle et al., 2007).", "startOffset": 139, "endOffset": 164}], "year": 2012, "abstractText": "Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations.", "creator": "LaTeX with hyperref package"}}}