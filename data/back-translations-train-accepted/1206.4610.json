{"id": "1206.4610", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Manifold Relevance Determination", "abstract": "In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a \"softly\" shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.", "histories": [["v1", "Mon, 18 Jun 2012 14:45:37 GMT  (2585kb)", "http://arxiv.org/abs/1206.4610v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["andreas c damianou", "carl henrik ek", "michalis k titsias", "neil d lawrence"], "accepted": true, "id": "1206.4610"}, "pdf": {"name": "1206.4610.pdf", "metadata": {"source": "META", "title": "Manifold Relevance Determination", "authors": ["Andreas C. Damianou", "Carl Henrik Ek", "Michalis K. Titsias", "Neil D. Lawrence"], "emails": ["ANDREAS.DAMIANOU@SHEFFIELD.AC.UK", "CHEK@CSC.KTH.SE", "MTITSIAS@WELL.OX.AC.UK", "N.LAWRENCE@SHEFFIELD.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "Multiview Learning is characterized by data that include observations from different modalities: for example, depth cameras provide color and depth images from the same scene, or a meeting could be represented by both. This motivates latently variable models that align the different views by assuming that part of the data variance is shared between the modalities, while the remaining variance is explained by latent spaces associated with each modality. This model structure allows conclusions when only a subset of modalities is available and the observation spaces are aligned, it is possible to transfer information between modalities, while the remaining variations are explained by latent spaces that belong privately to each modality. A working line aims to find a low-dimensional representation of the observation spaces by transforming between modalities by conditioning the model by conditioning the underlying concept. Multiple approaches that combine multiple views."}, {"heading": "2. The Model", "text": "(D) We assume that the existence of a single latent variable X (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F) F (R) F (R) F) F (R) F (R) F (R) F (R) R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F) F (R) F (R) F (R) F (R) F (R) F) F (R) F (R) F (R) F (R) F (R) F (R) F (R) F) F (R) F (R) F (R) F (R) F (R) F (R) (R) F (R) (R) F (R) F (R) (R) F (R) (R) F (R) (R) (R) F (R) (R) (R) (R) F (R) (R) (R)) (R) (R) (R) (R) (R) (R) (R) (R) (R) (R) () (R) (R) () () () (R)"}, {"heading": "2.1. Manifold Relevance Determination", "text": "We want to restore a factored latent representation, so that the variance that is divided between different observation spaces, q = q q q = q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z Z = Z Z = Z Z Z = Z Z = Z Z = Z Z = Z Z = Z Z = Z Z = Z = Z Z = Z Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z = Z"}, {"heading": "2.2. Bayesian training", "text": "The complete medical history method, which leads to an analytical approach, we regard as a middle ground, which we regard as a method that has enabled us to learn a way in which we are able to learn a way in which X (X), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), X (Z), Y (Z), Y (Y), Y (Y), Y (Y), Y (Y), Y (Y)."}, {"heading": "3. Experiments", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "4. Conclusions", "text": "We have introduced a new factored latent variable model for multi-view data: the model automatically factorises the data based on variables that represent the variance in each view regardless of the variance specific to a particular view; the model learns a variable distribution across the latent points; this allows us to automatically find the dimensionality of latent space and incorporate previous knowledge of its structure; as an example, we demonstrated how dynamic priors can be incorporated into latent space; this allowed us to use the time continuity to disprove the model's predictions in an ambiguous human estimation problem; the model is able to learn from extremely high-dimensional data; we illustrated this by learning a model directly on the pixel representation of an image; our model is able to learn a compact intuitive representation of such data that we can exemplify by generating new images by sampling from structured representation."}, {"heading": "Acknowledgments", "text": "The research was supported in part by the University of Sheffield Moody Foundation and the Greek State Scholarships Foundation (IKY) and we thank the reviewers for their useful feedback."}], "references": [{"title": "Recovering 3D human pose from monocular images", "author": ["Agarwal", "Ankur", "Triggs", "Bill"], "venue": "doi: 10.1109/TPAMI", "citeRegEx": "Agarwal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2006}, {"title": "Analysis of multiphase flows using dual-energy gamma densitometry and neural networks", "author": ["Bishop", "Christopher M", "James", "Gwilym D"], "venue": "Nuclear Instruments and Methods in Physics Research,", "citeRegEx": "Bishop et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1993}, {"title": "Variational gaussian process dynamical systems", "author": ["Damianou", "Andreas C", "Titsias", "Michalis", "Lawrence", "Neil D"], "venue": "In NIPS, pp", "citeRegEx": "Damianou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Damianou et al\\.", "year": 2011}, {"title": "Shared Gaussian Process Latent Variable Models", "author": ["Ek", "Carl Henrik"], "venue": "PhD Thesis,", "citeRegEx": "Ek and Henrik.,? \\Q2009\\E", "shortCiteRegEx": "Ek and Henrik.", "year": 2009}, {"title": "Gaussian process latent variable models for human pose estimation", "author": ["Ek", "Carl Henrik", "Torr", "Phil", "Lawrence", "Neil"], "venue": "Proceedings of the 4th international conference on Machine learning for multimodal interaction,", "citeRegEx": "Ek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ek et al\\.", "year": 2007}, {"title": "Ambiguity modeling in latent spaces", "author": ["Ek", "Carl Henrik", "J Rihan", "Torr", "Phil", "G Rogez", "Lawrence", "Neil"], "venue": "Machine Learning and Multimodal Interaction,", "citeRegEx": "Ek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ek et al\\.", "year": 2008}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "Semisupervised alignment of manifolds", "author": ["J Ham", "D Lee", "Saul", "Lawrence K"], "venue": "In Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Ham et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2005}, {"title": "Generative models that discover dependencies between data sets", "author": ["Klami", "Arto", "Kaski", "Samuel"], "venue": "In Proceedings of MLSP\u201906, IEEE International Workshop on Machine Learning for Signal Processing,", "citeRegEx": "Klami et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Klami et al\\.", "year": 2006}, {"title": "The Geometry Of Kernel Canonical Correlation Analysis", "author": ["Kuss", "Malte", "Graepel", "Thore"], "venue": null, "citeRegEx": "Kuss et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kuss et al\\.", "year": 2003}, {"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["Lawrence", "Neil"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lawrence and Neil.,? \\Q2005\\E", "shortCiteRegEx": "Lawrence and Neil.", "year": 2005}, {"title": "Hierarchical Gaussian process latent variable models", "author": ["Lawrence", "Neil D", "Moore", "Andrew J"], "venue": "In ICML,", "citeRegEx": "Lawrence et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2007}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.C. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "Lee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2005}, {"title": "Shared Kernel Information Embedding for Discriminative Inference", "author": ["Memisevic", "Roland", "Sigal", "Leonid", "Fleet", "David J"], "venue": "Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Memisevic et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["Rasmussen", "Carl Edward", "Williams", "Christopher K. I"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Factorized Orthogonal Latent Spaces", "author": ["Salzmann", "Mathieu", "Ek", "Carl Henrik", "Urtasun", "Raquel", "Darrell", "Trevor"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salzmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Salzmann et al\\.", "year": 2010}, {"title": "Learning shared latent structure for image synthesis and robotic imitation", "author": ["A Shon", "K Grochow", "A. Hertzmann"], "venue": "In Neural Information Processing,", "citeRegEx": "Shon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shon et al\\.", "year": 2006}, {"title": "Bayesian Gaussian Process Latent Variable Model", "author": ["Titsias", "Michalis", "Lawrence", "Neil"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2010}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["Titsias", "Michalis K"], "venue": "In Proceedings of the Twelfth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias and K.,? \\Q2009\\E", "shortCiteRegEx": "Titsias and K.", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": "Different approaches exploit different characteristics of the data such as, correlation (Kuss & Graepel, 2003; Ham et al., 2005), or mutual information (Memisevic et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 13, "context": ", 2005), or mutual information (Memisevic et al., 2011).", "startOffset": 31, "endOffset": 55}, {"referenceID": 16, "context": "In particular, approaches formulated as Gaussian Processes Latent Variable Models (GP-LVMs) (Lawrence, 2005) have been especially successful (Shon et al., 2006; Ek et al., 2007).", "startOffset": 141, "endOffset": 177}, {"referenceID": 4, "context": "In particular, approaches formulated as Gaussian Processes Latent Variable Models (GP-LVMs) (Lawrence, 2005) have been especially successful (Shon et al., 2006; Ek et al., 2007).", "startOffset": 141, "endOffset": 177}, {"referenceID": 5, "context": "To overcome this, the idea of a factorized latent space was presented in (Ek et al., 2008) where each view is associated with an additional private space, representing the variance which cannot be aligned, in addition to the shared space (Ek, 2009), an idea independently suggested by Klami & Kaski (2006).", "startOffset": 73, "endOffset": 90}, {"referenceID": 4, "context": ", 2006; Ek et al., 2007). However, these models assume that a single latent variable is capable of representing each modality, implying that the modalities can be fully aligned. To overcome this, the idea of a factorized latent space was presented in (Ek et al., 2008) where each view is associated with an additional private space, representing the variance which cannot be aligned, in addition to the shared space (Ek, 2009), an idea independently suggested by Klami & Kaski (2006). The main challenge for the applicability of the proposed models is that the factorization of the latent variable is a structural and essentially discrete property of the model, making it very challenging to learn.", "startOffset": 8, "endOffset": 484}, {"referenceID": 4, "context": ", 2006; Ek et al., 2007). However, these models assume that a single latent variable is capable of representing each modality, implying that the modalities can be fully aligned. To overcome this, the idea of a factorized latent space was presented in (Ek et al., 2008) where each view is associated with an additional private space, representing the variance which cannot be aligned, in addition to the shared space (Ek, 2009), an idea independently suggested by Klami & Kaski (2006). The main challenge for the applicability of the proposed models is that the factorization of the latent variable is a structural and essentially discrete property of the model, making it very challenging to learn. Salzmann et al. (2010) intro-", "startOffset": 8, "endOffset": 722}, {"referenceID": 16, "context": "In practice, a maximum a posteriori solution (Shon et al., 2006; Ek et al., 2007; Salzmann et al., 2010) was often used.", "startOffset": 45, "endOffset": 104}, {"referenceID": 4, "context": "In practice, a maximum a posteriori solution (Shon et al., 2006; Ek et al., 2007; Salzmann et al., 2010) was often used.", "startOffset": 45, "endOffset": 104}, {"referenceID": 15, "context": "In practice, a maximum a posteriori solution (Shon et al., 2006; Ek et al., 2007; Salzmann et al., 2010) was often used.", "startOffset": 45, "endOffset": 104}, {"referenceID": 2, "context": "We achieve this by building on recent variational approximations for standard GP-LVMs (Titsias & Lawrence, 2010; Damianou et al., 2011).", "startOffset": 86, "endOffset": 135}, {"referenceID": 16, "context": "the observations was shared (Shon et al., 2006).", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "Secondly, Ek et al. (2008) introduced private latent spaces to explain variance specific", "startOffset": 10, "endOffset": 27}, {"referenceID": 15, "context": "full manifold, as traditionally assumed, nor by a subspace geometrically orthogonal to that, as assumed in Salzmann et al. (2010).", "startOffset": 107, "endOffset": 130}, {"referenceID": 2, "context": "For the dynamical scenario we follow Damianou et al. (2011); Lawrence & Moore (2007) and choose the prior on the latent space to depend on the observation times t \u2208 R , e.", "startOffset": 37, "endOffset": 60}, {"referenceID": 2, "context": "For the dynamical scenario we follow Damianou et al. (2011); Lawrence & Moore (2007) and choose the prior on the latent space to depend on the observation times t \u2208 R , e.", "startOffset": 37, "endOffset": 85}, {"referenceID": 6, "context": "Yale faces: To show the ability of our method to model very high-dimensional spaces our first experiment is applied to the Yale dataset (Georghiades et al., 2001; Lee et al., 2005) which contains images of several human faces under different poses and 64 illumination conditions.", "startOffset": 136, "endOffset": 180}, {"referenceID": 12, "context": "Yale faces: To show the ability of our method to model very high-dimensional spaces our first experiment is applied to the Yale dataset (Georghiades et al., 2001; Lee et al., 2005) which contains images of several human faces under different poses and 64 illumination conditions.", "startOffset": 136, "endOffset": 180}, {"referenceID": 5, "context": "We also compared against the shared GP-LVM (Ek et al., 2008; Ek, 2009) which optimises the latent points using MAP and, therefore, requires an initial factorisation of the inputs to be given a priori.", "startOffset": 43, "endOffset": 70}], "year": 2012, "abstractText": "In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear (in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a \u201csoftly\u201d shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.", "creator": "LaTeX with hyperref package"}}}