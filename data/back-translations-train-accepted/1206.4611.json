{"id": "1206.4611", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Convex Feature Learning Formulation for Latent Task Structure Discovery", "abstract": "This paper considers the multi-task learning problem and in the setting where some relevant features could be shared across few related tasks. Most of the existing methods assume the extent to which the given tasks are related or share a common feature space to be known apriori. In real-world applications however, it is desirable to automatically discover the groups of related tasks that share a feature space. In this paper we aim at searching the exponentially large space of all possible groups of tasks that may share a feature space. The main contribution is a convex formulation that employs a graph-based regularizer and simultaneously discovers few groups of related tasks, having close-by task parameters, as well as the feature space shared within each group. The regularizer encodes an important structure among the groups of tasks leading to an efficient algorithm for solving it: if there is no feature space under which a group of tasks has close-by task parameters, then there does not exist such a feature space for any of its supersets. An efficient active set algorithm that exploits this simplification and performs a clever search in the exponentially large space is presented. The algorithm is guaranteed to solve the proposed formulation (within some precision) in a time polynomial in the number of groups of related tasks discovered. Empirical results on benchmark datasets show that the proposed formulation achieves good generalization and outperforms state-of-the-art multi-task learning algorithms in some cases.", "histories": [["v1", "Mon, 18 Jun 2012 15:00:07 GMT  (442kb)", "http://arxiv.org/abs/1206.4611v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["pratik jawanpuria", "j saketha nath"], "accepted": true, "id": "1206.4611"}, "pdf": {"name": "1206.4611.pdf", "metadata": {"source": "META", "title": "A Convex Feature Learning Formulationfor Latent Task Structure Discovery", "authors": ["Pratik Jawanpuria", "J. Saketha Nath"], "emails": ["pratik.j@cse.iitb.ac.in", "saketh@cse.iitb.ac.in"], "sections": [{"heading": null, "text": "Published in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by author (s) / owner (s)."}, {"heading": "1. Introduction", "text": "The idea behind this is that the individual groups are groups in which the respective tasks are closely linked to each other. (...) The focus of this paper is on the multilateral learning process, in which some relevant features can be shared. (...) Such situations are taken up by others in the real world. (...) There are a number of situations in which the respective tasks are closely linked to each other. (...) There are a number of situations in which the individual tasks differ. (...) There are a number of situations in which the individual tasks are linked to each other. (...) There are a number of situations in which the individual tasks are linked to each other. (...) There are a number of tasks in which the individual tasks are linked to each other. (...) There are a number of tasks in which the individual tasks are linked to each other. (...) There are a number of tasks in which the individual tasks are linked to each other."}, {"heading": "2. Notations and set-up", "text": "Consider a set of learning tasks, T in number. < The training data for the tth task = divided tasks are represented by: Dt = {(xti, yti), i = 1,..., m}, t = 1,.., T, where (xti, yti) is the ith input output pair of the tth task. To simplify the notation, we assume that the number of training examples is the same for all tasks. < The task predictors are assumed to be affin: Ft (x) = < ft, \u03c6 (x) > \u2212 bt, t = 1,..., T, where ft is the weight vector of the tth task."}, {"heading": "3. A Novel Convex Formulation", "text": "That is, it is not so that we can get involved in the task. \"(fT) It is a suitable convex loss function (like the hinge) and C is the regulation parameter. (fT) It is common to choose a regulator who knows the relationship between the given tasks. (f1) It is, for example, when all tasks are independent (f1). (fT) 2. It can be taken as the solution to the task. (fT) 2. It is possible that all tasks are independent (f1). (fT) 2. It is possible that all tasks are independent (f1). (fT) 2. It can be taken as the solution that leads to a factoring of the problem. (fT) 2. It is possible that all tasks are independent (f1). (fT) 2. It is possible that all tasks are independent of each other (f1)."}, {"heading": "4. Active-set Algorithm", "text": "In this case, it is not the size of the initial active group that is usually taken as a minimum. After solving the problem with the current active group, a sufficiency condition for optimizing the solution is verified. In the case of the solution, the solution is optimal, the algorithms are not updated, and the active group is updated with the new active group."}, {"heading": "5. Experimental Results", "text": "This year it has come to the point that it will be able to erenie.n the aforementioned lrVo rf\u00fc eid nlrVo"}, {"heading": "6. Conclusions", "text": "The main contribution of the thesis is a convex formulation to solve this problem. By means of a novel graph-based regularizer, the search becomes feasible in the exponentially large space of the task groups. Experimental results illustrate the effectiveness of the proposed approach."}, {"heading": "Acknowledgments", "text": "We thank Ganesh Ramakrishnan for the insightful discussions on this paper."}], "references": [{"title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning", "author": ["F. Bach"], "venue": "In NIPS,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Multiple Kernel Learning, Conic Duality, and the SMO Algorithm", "author": ["F. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "In ICML,", "citeRegEx": "Bach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2004}, {"title": "Lectures on Modern Convex Optimization: Analysis, Algorithms and Engineering Applications", "author": ["A. Ben-Tal", "A. Nemirovski"], "venue": "MPS/ SIAM Series on Optimization,", "citeRegEx": "Ben.Tal and Nemirovski,? \\Q2001\\E", "shortCiteRegEx": "Ben.Tal and Nemirovski", "year": 2001}, {"title": "Non-linear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas", "year": 1999}, {"title": "Graph-structured multi-task regression and an efficient optimization method for general fused lasso", "author": ["X. Chen", "S. Kim", "Q. Lin", "J.G. Carbonell", "E.P. Xing"], "venue": "CoRR, abs/1005.3579,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In ACM SIGKDD,", "citeRegEx": "Evgeniou and Pontil,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou and Pontil", "year": 2004}, {"title": "Clustered Multi-Task Learning: A Convex Formulation", "author": ["L. Jacob", "F. Bach", "J.P. Vert"], "venue": "In NIPS,", "citeRegEx": "Jacob et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2008}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan"], "venue": "In NIPS,", "citeRegEx": "Jalali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2010}, {"title": "An Accelerated Gradient Method for Trace Norm Minimization", "author": ["Ji", "Shuiwang", "Ye", "Jieping"], "venue": "In ICML,", "citeRegEx": "Ji et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2009}, {"title": "Efficient and accurate lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "P. Laskov", "K.R. M\u2019\u0301uller", "A. Zien"], "venue": "In NIPS,", "citeRegEx": "Kloft et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2009}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Phase transitions for high-dimensional joint support recovery", "author": ["S. Negahban", "M. Wainwright"], "venue": "In NIPS,", "citeRegEx": "Negahban and Wainwright,? \\Q2009\\E", "shortCiteRegEx": "Negahban and Wainwright", "year": 2009}, {"title": "Fast training of support vector machines using sequential minimal optimization, pp. 185\u2013208", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Algorithms for simultaneous sparse approximation: part ii: Convex relaxation", "author": ["J.A. Tropp"], "venue": "Signal Process.,", "citeRegEx": "Tropp,? \\Q2006\\E", "shortCiteRegEx": "Tropp", "year": 2006}, {"title": "Inferring latent task structure for multi-task learning by multiple kernel learning", "author": ["C. Widmer", "N. Toussaint", "Y. Altun", "G. Ratsch"], "venue": "BMC Bioinformatics,", "citeRegEx": "Widmer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Widmer et al\\.", "year": 2010}, {"title": "Multitask learning for classification with dirichlet process priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}, {"title": "The sparsity and bias of the lasso selection in high-dimensional linear regression", "author": ["C. Zhang", "J. Huang"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang and Huang,? \\Q2008\\E", "shortCiteRegEx": "Zhang and Huang", "year": 2008}, {"title": "Semi-Supervised Multi-Task Regression", "author": ["Y. Zhang", "D.Y. Yeung"], "venue": "In ECML/PKDD,", "citeRegEx": "Zhang and Yeung,? \\Q2009\\E", "shortCiteRegEx": "Zhang and Yeung", "year": 2009}, {"title": "Learning multiple tasks with a sparse matrix-normal penalty", "author": ["Zhang", "Yi", "Schneider", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Grouped and Hierarchical Model Selection through Composite Absolute Penalties", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Following Evgeniou & Pontil (2004); Jacob et al. (2008); Jalali et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 6, "context": "Following Evgeniou & Pontil (2004); Jacob et al. (2008); Jalali et al. (2010), tasks are said to be related if the corresponding task parameters are close to each other.", "startOffset": 36, "endOffset": 78}, {"referenceID": 13, "context": "Such situations arise in several real world applications (Tropp, 2006; Jalali et al., 2010).", "startOffset": 57, "endOffset": 91}, {"referenceID": 7, "context": "Such situations arise in several real world applications (Tropp, 2006; Jalali et al., 2010).", "startOffset": 57, "endOffset": 91}, {"referenceID": 7, "context": "Existing works in this setting (Turlach et al., 2005; Zhang & Huang, 2008; Negahban & Wainwright, 2009; Jalali et al., 2010) employ a `1/`\u221e-norm based regularizer that promotes sparsity among features and low variance among the parameters of all the given tasks in the shared feature space.", "startOffset": 31, "endOffset": 124}, {"referenceID": 6, "context": "Such situations arise in several real world applications (Tropp, 2006; Jalali et al., 2010). Existing works in this setting (Turlach et al., 2005; Zhang & Huang, 2008; Negahban & Wainwright, 2009; Jalali et al., 2010) employ a `1/`\u221e-norm based regularizer that promotes sparsity among features and low variance among the parameters of all the given tasks in the shared feature space. Success of such methods depends on the extent to which the given tasks are related and the extent to which the features are shared among the tasks. In fact, Negahban & Wainwright (2009) show that `1/`\u221e regularization could actually perform worse than simple element-wise `1 regularization when the extent to which the features are shared is less than a threshold or when the task parameters are not all close-by.", "startOffset": 71, "endOffset": 570}, {"referenceID": 4, "context": "Alternatively, Chen et al. (2010) assume that the relations between the tasks are known and propose employing a regularizer that penalizes deviations in weight vectors for highly correlated tasks.", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "up of Multiple Kernel Learning (MKL) (Bach et al., 2004), the feature space in each group is taken to be that induced by a conic combination of a given set of base kernels.", "startOffset": 37, "endOffset": 56}, {"referenceID": 14, "context": "Note that Widmer et al. (2010) also attempt a search among all possible groups of tasks; however the runtime of their algorithm is exponential in the number of tasks.", "startOffset": 10, "endOffset": 31}, {"referenceID": 18, "context": "Motivated by the graph-based regularizers employed in Zhao et al. (2009); Bach (2008), we propose the following novel regularizer for the problem at hand:", "startOffset": 54, "endOffset": 73}, {"referenceID": 0, "context": "(2009); Bach (2008), we propose the following novel regularizer for the problem at hand:", "startOffset": 8, "endOffset": 20}, {"referenceID": 9, "context": "In the special case p\u0304 = q\u0304, this problem is same as the `q\u0302-MKL formulation (Kloft et al., 2009) with q\u0302 = q\u0304 q\u0304\u22121 and with base kernels as k\u0302 w = (\u03bbw(\u03b3)) 1 q\u0304 k w \u2200w \u2208 V and \u2200j = 1, .", "startOffset": 77, "endOffset": 97}, {"referenceID": 10, "context": "Following a common practice for solving large-scale convex sparsity problems (Lee et al., 2007; Bach, 2008), we propose solving the dual (4) using an active set algorithm.", "startOffset": 77, "endOffset": 107}, {"referenceID": 0, "context": "Following a common practice for solving large-scale convex sparsity problems (Lee et al., 2007; Bach, 2008), we propose solving the dual (4) using an active set algorithm.", "startOffset": 77, "endOffset": 107}, {"referenceID": 3, "context": "Note that (4) has a simple constraint set, which is a simplex and the gradient \u2207H(\u03b3) can be computed using the Danskin\u2019s theorem (Bertsekas, 1999): the i component of this sub-gradient is given by (\u2207H(\u03b3))i = \u2212 q i \u03b3 \u2212q i 2q\u0304 \u00d7 (\u2211 w\u2208V \u03bbw(\u03b3) (\u2211k j=1 ( \u03b2\u0304Kw\u03b2\u0304 )p\u0304) q\u0304 p\u0304) 1 \u22121\u00d7 (\u2211 w\u2208D(i) \u03bbw(\u03b3) q (\u2211k j=1 ( \u03b2\u0304Kw\u03b2\u0304 )p\u0304) q\u0304 p\u0304) , where \u03b2\u0304 is", "startOffset": 129, "endOffset": 146}, {"referenceID": 12, "context": "Hence we use a sequential minimal optimization (SMO) algorithm (Platt, 1999) for solving (5).", "startOffset": 63, "endOffset": 76}, {"referenceID": 14, "context": "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010).", "startOffset": 63, "endOffset": 81}, {"referenceID": 14, "context": "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010). It contains examples collected from various landmine fields.", "startOffset": 63, "endOffset": 107}, {"referenceID": 14, "context": "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010). It contains examples collected from various landmine fields. Each example is represented as a 9dimensional real valued feature vector. Each task is a binary classification problem with the goal being to predict landmines (positive class) or clutter (negative class). Following Xue et al. (2007); Zhang & Schneider (2010), we jointly learn 19 tasks from the landmine fields numbered 1 \u2212 10 and 16 \u2212 24 in the data set.", "startOffset": 63, "endOffset": 403}, {"referenceID": 14, "context": "Landmine A benchmark multi-task classification dataset used in Xue et al. (2007); Zhang & Schneider (2010). It contains examples collected from various landmine fields. Each example is represented as a 9dimensional real valued feature vector. Each task is a binary classification problem with the goal being to predict landmines (positive class) or clutter (negative class). Following Xue et al. (2007); Zhang & Schneider (2010), we jointly learn 19 tasks from the landmine fields numbered 1 \u2212 10 and 16 \u2212 24 in the data set.", "startOffset": 63, "endOffset": 429}, {"referenceID": 6, "context": "MHC-I A multi-task classification dataset used in Jacob et al. (2008). It contains binding affinities of various peptides with different MHC-I molecules.", "startOffset": 50, "endOffset": 70}, {"referenceID": 6, "context": "MHC-I A multi-task classification dataset used in Jacob et al. (2008). It contains binding affinities of various peptides with different MHC-I molecules. Each task here is a binary classification problem. We perform experiments on the same 10 tasks reported in Jacob et al. (2008). Total number of instances in the 10 tasks is 1200 and the input space consists of 180 binary features.", "startOffset": 50, "endOffset": 281}, {"referenceID": 6, "context": "MHC-I A multi-task classification dataset used in Jacob et al. (2008). It contains binding affinities of various peptides with different MHC-I molecules. Each task here is a binary classification problem. We perform experiments on the same 10 tasks reported in Jacob et al. (2008). Total number of instances in the 10 tasks is 1200 and the input space consists of 180 binary features. The number of instances per task varies from 59 to 197 and the the dataset is biased against the positive class. Letter A multi-task classification dataset used in Ji & Ye (2009). It consists of handwritten letters from different writers.", "startOffset": 50, "endOffset": 564}, {"referenceID": 6, "context": "CMTL The clustered multi-task learning formulation proposed in Jacob et al. (2008). Finds clusters of tasks having similar weight vectors.", "startOffset": 63, "endOffset": 83}, {"referenceID": 6, "context": "CMTL The clustered multi-task learning formulation proposed in Jacob et al. (2008). Finds clusters of tasks having similar weight vectors. No feature learning is performed. DMTL The multi-task feature learning formulation in Jalali et al. (2010). Performs feature selection to discover features shared across all the tasks as well as task-specific features.", "startOffset": 63, "endOffset": 246}], "year": 2012, "abstractText": "This paper considers the multi-task learning problem and in the setting where some relevant features could be shared across few related tasks. Most of the existing methods assume the extent to which the given tasks are related or share a common feature space to be known apriori. In real-world applications however, it is desirable to automatically discover the groups of related tasks that share a feature space. In this paper we aim at searching the exponentially large space of all possible groups of tasks that may share a feature space. The main contribution is a convex formulation that employs a graphbased regularizer and simultaneously discovers few groups of related tasks, having closeby task parameters, as well as the feature space shared within each group. The regularizer encodes an important structure among the groups of tasks leading to an efficient algorithm for solving it: if there is no feature space under which a group of tasks has closeby task parameters, then there does not exist such a feature space for any of its supersets. An efficient active set algorithm that exploits this simplification and performs a clever search in the exponentially large space is presented. The algorithm is guaranteed to solve the proposed formulation (within some precision) in a time polynomial in the number of groups of related tasks discovered. Empirical results on benchmark datasets show that the proposed formulation achieves good generalization and outperforms state-of-the-art multi-task learning algorithms in some cases. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "LaTeX with hyperref package"}}}