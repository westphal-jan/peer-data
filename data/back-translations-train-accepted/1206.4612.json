{"id": "1206.4612", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Exact Soft Confidence-Weighted Learning", "abstract": "In this paper, we propose a new Soft Confidence-Weighted (SCW) online learning scheme, which enables the conventional confidence-weighted learning method to handle non-separable cases. Unlike the previous confidence-weighted learning algorithms, the proposed soft confidence-weighted learning method enjoys all the four salient properties: (i) large margin training, (ii) confidence weighting, (iii) capability to handle non-separable data, and (iv) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-the-art algorithms (including AROW, NAROW and NHERD), we found that SCW generally achieves better or at least comparable predictive accuracy, but enjoys significant advantage of computational efficiency (i.e., smaller number of updates and lower time cost).", "histories": [["v1", "Mon, 18 Jun 2012 15:00:20 GMT  (255kb)", "http://arxiv.org/abs/1206.4612v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["steven c h hoi", "jialei wang", "peilin zhao"], "accepted": true, "id": "1206.4612"}, "pdf": {"name": "1206.4612.pdf", "metadata": {"source": "META", "title": "Exact Soft Confidence-Weighted Learning", "authors": ["Jialei Wang", "Peilin Zhao", "Steven C.H. Hoi"], "emails": ["jl.wang@ntu.edu.sg", "zhao0106@ntu.edu.sg", "chhoi@ntu.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "In fact, we will be able to solve the problems of the past by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them."}, {"heading": "2. Related Work and Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Overview of Online Learning", "text": "Online learning works on the basis of a sequence of data samples with timestamps. In step t, the algorithm processes an in-depth example of xt-Rd by first predicting its labeling y-t-1 + 1. After the prediction, the true labeling yt-1 + 1 is revealed and then the loss (yt, y-t), which is the difference between its prediction and the revealed true labeling yt, is suffered. Finally, the loss is used to update the weights of the model based on some criteria. Overall, the goal of online learning is to minimize the cumulative error across the entire sequence of data examples. Our work is closely related to several first and second order online learning algorithms, including passive-aggressive learning (Crammer et al., 2006), confidence-weighted learning (Dredze et al., 2008) and adaptive regulation of weights (Crammer et al., 2009b)."}, {"heading": "2.2. Passive-Aggressive Learning", "text": "As a state-of-the-art online learning algorithm of the first order, the optimization of passive-aggressive (PA) learning is formulated as follows: wt + 1 = arg min w-w- wt-wt-wt-2s.t. (w; (xt, yt)) = 0 (1), whereby the loss function is based on the hinge loss: (w; (xt, yt) = {0 if yt (w \u00b7 xt) \u2265 1 \u2212 yt (w \u00b7 xt) otherThe above optimization has the closed solution: wt + 1 = wt + \u03b7 PA t-ytxt (2), whereby \u03b7PAt = (wt; (xt, yt) \u0441xt 2. Furthermore, in order for PA to be able to handle non-separable instances and more robust instances, one of the two types of penalties has been introduced into the optimization (1): linear and square, which leads to the following two formulations of passive-w- w- (w-), w-PA-white."}, {"heading": "2.3. Confidence-Weighted Learning", "text": "To better explore the underlying structure between the features, the confidence-weighted (CW) learning algorithm assumes a Gaussian distribution of weights with mean vector (p) and covariance matrix (p) and covariance matrix (p). The weight distribution is updated by minimizing the divergence between the new and the old weight distribution, ensuring that the probability of a correct classification is higher than a threshold as follows: (p) + 1, p + 1, p + 1) = argmin \u00b5, p (n (p), p (p), p (p), p (p), p (p), p (p), p (p), p (p), p, p, p, p, p, p, p, p, p, p, p (p), p (p, p, p, p, p, p, p, p, p, p, p, p (p), p (p), p (p, p, p, p, p, p, p, p (p), p (p, p, p, p (p), p (p, p, p, p, p, p (p), p (p, p, p, p, p, p, p, p, p (p), p, p (p, p, p, p, p, p, p, p, p, p (p), p (p, p, p, p, p, p, p, p, p, p, p, p (p), p (p, p, p, p, p, p, p, p, p, p, p, p, p (p, p), p (p, p, p), p (p (p, p, p, p, p, p, p, p, p, p, p, p, p (p, p, p, p, p, p, p, p, p, p), p (p, p (p, p, p, p, p, p, p, p, p, p (p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p), p (p"}, {"heading": "2.4. Adaptive Regularization of Weights", "text": "In contrast to the original CW learning algorithm, Adaptive Regularization Of Weights (AROW) Learning introduces an adaptive regularization of the prediction function when processing a new instance in each learning step, making it more resistant to sudden changes in label noise in the learning tasks. In particular, the optimization of AROW is formulated as follows: (\u00b5t + 1, \u0442t + 1) = argmin \u00b5, \u0394DKL (N (\u00b5, \u03a3), N (\u00b5t, \u0442t))) + 12\u03b3 2 (\u00b5; (xt, yt)) + 12\u03b3 xtT\u0445txtwhere \u0445 2 (\u00b5; ((xt, yt)) = (max {0, 1 \u2212 yt (\u00b5 \u00b7 xt)} 2 and \u03b3 is a regularization parameter."}, {"heading": "3. Soft Confidence-Weighted Learning", "text": "In this section, we present a new online learning method that aims to address the constraints of CW and AROW learning processes. (Following the same problem settings of confidence-weighted learning, we assume that the weight vector w follows the Gaussian distribution with the mean vector number and covariance matrix \u03a3. Note that the probability constraint in CW learning, i.e., the Pr w formulas N (.4) [yt (w \u00b7 xt) 0] can be rewritten with more systemic margin (.xt).It is easy to verify that the satisfaction of the probability constraint \u2212 1 (.4) is the loss function as follows: The loss function Kamp # 160; (N,.8)."}, {"heading": "4. Analysis and Discussions", "text": "We will first give an overview of the comparison of the proposed SCW methods with respect to several existing online learning algorithms of first and second order, followed by discussions on nonlinear extension and bound analyses.Algorithm 1 SCW learning algorithms (SCW) INPUT: Parameters C > 0, \u03b7 > 0. INITIALisation: \u00b50 = (0,..., 0), 0 = I. for t = 1,..., T get an example xt-Rd; make prediction: y-t = sgn (\u00b5t \u2212 1 \u00b7 xt); get real labelling; suffer losses (N (\u00b5t \u2212 1, \u0441t \u2212 1); (xt, yt \u2212 1)); if the results (N (\u00b5t \u2212 1, \u0445t \u2212 1); (xt, yt \u2212 1) > 0 then \u00b5t + 1 = \u00b5t-Prot-Prot (SCW-Prot) or (SCT-Prot-Prot-Prot) Prot at the end and end of each (SCW-2)."}, {"heading": "4.1. Comparison with the existing methods", "text": "Unlike the previous second-order algorithms, the proposed SCW algorithm has all four outstanding properties. In particular, SCW improves over the original CW algorithm by adding the ability to handle the inseparable cases, and improves over AROW by adding the adaptive margin property. To our knowledge, SCW is the first second-order online learning that has all four properties."}, {"heading": "4.2. Extension to Nonlinear Cases", "text": "Similar to other linear online learning methods, the proposed SCW learning can be extended to non-linear cases. Lemma 1. (Represent Theorem) The mean value of \u00b5i and the covariance of the parameters calculated by the soft confidence weight algorithm can be written as linear combinations of the input vectors with coefficients that depend only on the internal products of the input vectors, i.e., \"i = i \u2212 1\" p, \"q = 1\u03c0 (i) p,\" \"qxpxq\" T + aI, \"\" i \u2212 1 \u2211 pempp (i) xpwhere \"i (i) = 1 and\" p \"(i + 1) =\" p (i + 1), \"\" p \"(i),\" c \"p (i),\" p (i), \"p,\" p, \"p,\" \"(i),\" (p, \"(p),\" p, \"(i) p,\" p, \"p,\" (i), \"p,\" p, \"p (i)."}, {"heading": "4.3. Analysis of the Loss Bound", "text": "Our analysis begins with the definition of the loss of confidence used in (Crammer et al., 2008). Loss is a function of the margin normalized by (m) v, i.e., m) i = mi) vi. We modified the loss of confidence (Crammer et al., 2008) as a loss with an upper limit: (m) i (m) i (m) = (0 m) i (m) i (m) i (m) i (m) i (m) i (m) i (m) i (m) i < (where (m) i (m) = (m)."}, {"heading": "5. Empirical Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets and compared algorithms", "text": "We accept a variety of data sets from different fields: \u2022 Synthetic data: we generated this data using the method described in (Crammer et al., 2008), which is used to examine the effectiveness of second-order algorithms. \u2022 Digital recognition: we use two benchmarks: \"USPS\" 1 and \"MNIST\" 2. For binary classification, we choose \"1\" vs \"all\" for \"USPS,\" and \"1\" vs \"2\" for \"MNIST.\" \u2022 Face data: we use the MIT-CBCL face imags 3. \u2022 Machine Learning datasets: we randomly choose several public machine learning datasets from 4. Table 2 shows the statistics of the list of data used. We compare our methods with various online learning algorithms, including Perceptron (Li Rosenblatt et), Long 2006 (Crammer et)."}, {"heading": "5.2. Experimental Results", "text": "Table 3 summarizes the results of our empirical evaluation, in which we have only margin-based second-order learning algorithms due to space constraints. For a more complete comparison, refer to our supplementary material. The bold elements show the best performance when paired with t-test at significance level of 95%. We can make several observations as follows. First, by examining the overall errors, we found that second-order algorithms usually outperform first-order algorithms, and margin-based algorithms usually outperform margin-based methods. This shows the effectiveness of \"large margins\" and \"trust properties\" for learning better classifiers. Second, by examining the original CW algorithm, we found that it significantly outperforms first-order algorithms (e.g. Perceptron, ROMMA, and PA algorithms) when the synthetic data is executed without noise."}, {"heading": "6. Conclusion", "text": "This paper proposed Soft Confidence-Weighted (SCW) Learning, a new second-order online learning method with state-of-the-art empirical performance. Unlike existing second-order algorithms, SCW has all four characteristics: (i) large margin training, (ii) confidence weighting, (iii) adaptive margin, and (iv) ability to handle non-separable data. Empirically, we found that the proposed SCW algorithms work significantly better than the original CW algorithm and exceed the state-of-the-art AROW algorithm in most cases in terms of both accuracy and efficiency."}, {"heading": "Appendix: Proof of Proposition 1 and 2", "text": "Proving that the solution is valid is easy if the solution is not valid. (xt, yt) > 0 (xt, yt) > 0 (xt, yt) > 0 (gt, yt) > 0 (gt, yt) > 0 (gt, yt) > 0 (gt, yt) = 0 (t) Since the solution is semi-unique (PSD), it can be written as 2 to do the optimization with a convex constraint in both digit and digit at the same time. (xt, yt) But for convenience, we will still use digit instead of digit 2 in the following analysis."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Singapore MOE Tier 1 Project (RG33 / 11) and the Microsoft Research Project (M4060936)."}], "references": [{"title": "A second-order perceptron algorithm", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Conconi", "Alex", "Gentile", "Claudio"], "venue": "SIAM J. Comput.,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2005}, {"title": "Learning via gaussian herding", "author": ["Crammer", "Koby", "D.Lee", "Daniel"], "venue": "In NIPS, pp", "citeRegEx": "Crammer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2010}, {"title": "Online passiveaggressive algorithms", "author": ["Crammer", "Koby", "Dekel", "Ofer", "Keshet", "Joseph", "ShalevShwartz", "Shai", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "Exact convex confidence-weighted learning", "author": ["Crammer", "Koby", "Dredze", "Mark", "Pereira", "Fernando"], "venue": "In NIPS, pp", "citeRegEx": "Crammer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2008}, {"title": "Multiclass confidence weighted algorithms", "author": ["Crammer", "Koby", "Dredze", "Mark", "Kulesza", "Alex"], "venue": "In EMNLP, pp", "citeRegEx": "Crammer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2009}, {"title": "Adaptive regularization of weight vectors", "author": ["Crammer", "Koby", "Kulesza", "Alex", "Dredze", "Mark"], "venue": "In NIPS,", "citeRegEx": "Crammer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2009}, {"title": "Confidence-weighted linear classification", "author": ["Dredze", "Mark", "Crammer", "Koby", "Pereira", "Fernando"], "venue": "In ICML, pp", "citeRegEx": "Dredze et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John C", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Online multiple kernel learning: Algorithms and mistake bounds", "author": ["Jin", "Rong", "Hoi", "Steven C. H", "Yang", "Tianbao"], "venue": "In ALT,", "citeRegEx": "Jin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2010}, {"title": "Pamr: Passive aggressive mean reversion strategy for portfolio selection", "author": ["Li", "Bin", "Zhao", "Peilin", "Hoi", "Steven C. H", "Gopalkrishnan", "Vivekanand"], "venue": "Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "The relaxed online maximum margin algorithm", "author": ["Li", "Yi", "Long", "Philip M"], "venue": "In NIPS, pp", "citeRegEx": "Li et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Li et al\\.", "year": 1999}, {"title": "New adaptive algorithms for online classification", "author": ["Orabona", "Francesco", "Crammer", "Koby"], "venue": "In NIPS,", "citeRegEx": "Orabona et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2010}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psych. Rev.,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1958}, {"title": "Online learning by ellipsoid method", "author": ["Yang", "Liu", "Jin", "Rong", "Ye", "Jieping"], "venue": "In ICML, pp", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Double updating online learning", "author": ["Zhao", "Peilin", "Hoi", "Steven C. H", "Jin", "Rong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Online auc maximization", "author": ["Zhao", "Peilin", "Hoi", "Steven C. H", "Jin", "Rong", "Yang", "Tianbao"], "venue": "In ICML, pp", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": ", 2011a;b) represent a family of fast and simple machine learning techniques, which usually make few statistical assumptions and can be applied to a wide range of applications (Li et al., 2012).", "startOffset": 176, "endOffset": 193}, {"referenceID": 2, "context": "Online learning has been actively studied in machine learning community, in which a variety of online learning algorithms have been proposed, including a number of first-order algorithms such as the well-known Perceptron algorithm (Rosenblatt, 1958) and the Passive-Aggressive (PA) algorithms (Crammer et al., 2006).", "startOffset": 293, "endOffset": 315}, {"referenceID": 0, "context": "Recent years have seen a surge of studies on the second-order online learning algorithms (Cesa-Bianchi et al., 2005; Dredze et al., 2008; Crammer et al., 2009b; Orabona & Crammer, 2010; Duchi et al., 2011), which have shown that parameter confidence information can be explored to guide and improve online learning performance (Cesa-Bianchi et al.", "startOffset": 89, "endOffset": 205}, {"referenceID": 6, "context": "Recent years have seen a surge of studies on the second-order online learning algorithms (Cesa-Bianchi et al., 2005; Dredze et al., 2008; Crammer et al., 2009b; Orabona & Crammer, 2010; Duchi et al., 2011), which have shown that parameter confidence information can be explored to guide and improve online learning performance (Cesa-Bianchi et al.", "startOffset": 89, "endOffset": 205}, {"referenceID": 7, "context": "Recent years have seen a surge of studies on the second-order online learning algorithms (Cesa-Bianchi et al., 2005; Dredze et al., 2008; Crammer et al., 2009b; Orabona & Crammer, 2010; Duchi et al., 2011), which have shown that parameter confidence information can be explored to guide and improve online learning performance (Cesa-Bianchi et al.", "startOffset": 89, "endOffset": 205}, {"referenceID": 0, "context": ", 2011), which have shown that parameter confidence information can be explored to guide and improve online learning performance (Cesa-Bianchi et al., 2005).", "startOffset": 129, "endOffset": 156}, {"referenceID": 6, "context": "For example, Confidence-weighted (CW) learning (Dredze et al., 2008; Crammer et al., 2009a) maintains a Gaussian distribution over some linear classifier hypotheses and applies it to control the direction and scale of parameter updates (Dredze et al.", "startOffset": 47, "endOffset": 91}, {"referenceID": 6, "context": ", 2009a) maintains a Gaussian distribution over some linear classifier hypotheses and applies it to control the direction and scale of parameter updates (Dredze et al., 2008).", "startOffset": 153, "endOffset": 174}, {"referenceID": 3, "context": "Although CW learning has formal guarantees in the mistakebound model (Crammer et al., 2008), it can overfit in certain situations due to its aggressive update rules based upon a separable data assumption.", "startOffset": 69, "endOffset": 91}, {"referenceID": 2, "context": "Our work is closely related to several first and second order online learning algorithms, including Passive-Aggressive (PA) learning (Crammer et al., 2006), Confidence-Weighted learning (Dredze et al.", "startOffset": 133, "endOffset": 155}, {"referenceID": 6, "context": ", 2006), Confidence-Weighted learning (Dredze et al., 2008), and Adaptive Regularization of Weights learning (Crammer et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 3, "context": "The above lemma can be proved by induction similar to the proof in (Crammer et al., 2008).", "startOffset": 67, "endOffset": 89}, {"referenceID": 3, "context": "Our analysis begins with the definition of confidence loss, which is used in (Crammer et al., 2008).", "startOffset": 77, "endOffset": 99}, {"referenceID": 3, "context": "We modified the confidence loss in (Crammer et al., 2008) as an upper-bounded loss by:", "startOffset": 35, "endOffset": 57}, {"referenceID": 3, "context": "It is easy to see that the loss l\u03c6(m\u0303) holds the properties of Lemma 5 in (Crammer et al., 2008) for SCW-I.", "startOffset": 74, "endOffset": 96}, {"referenceID": 3, "context": "The above theorem can be proved by applying Lemma 7 and property 6 in Lemma 5 in (Crammer et al., 2008).", "startOffset": 81, "endOffset": 103}, {"referenceID": 3, "context": "We adopt a variety of datasets from different domains: \u2022 synthetic data: we generated this data set by the method described in (Crammer et al., 2008), which is used to examine the effectiveness of second-order algorithms.", "startOffset": 127, "endOffset": 149}, {"referenceID": 2, "context": "We compare our methods with various online learning algorithms, including Perceptron (Rosenblatt, 1958), PA (Crammer et al., 2006), ROMMA (Li & Long, 1999) and its aggressive version agg-ROMMA, SecondOrder Perceptron (Cesa-Bianchi et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 0, "context": ", 2006), ROMMA (Li & Long, 1999) and its aggressive version agg-ROMMA, SecondOrder Perceptron (Cesa-Bianchi et al., 2005), Confidence Weighted Learning (Crammer et al.", "startOffset": 94, "endOffset": 121}, {"referenceID": 3, "context": ", 2005), Confidence Weighted Learning (Crammer et al., 2008), Improved Ellipsoid Method for Online Learning(IELLIP) (Yang et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 13, "context": ", 2008), Improved Ellipsoid Method for Online Learning(IELLIP) (Yang et al., 2009), AROW (Crammer et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 6, "context": "Following the similar parameter setting methods in (Dredze et al., 2008) and (Crammer et al.", "startOffset": 51, "endOffset": 72}], "year": 2012, "abstractText": "In this paper, we propose a new Soft Confidence-Weighted (SCW) online learning scheme, which enables the conventional confidence-weighted learning method to handle non-separable cases. Unlike the previous confidence-weighted learning algorithms, the proposed soft confidence-weighted learning method enjoys all the four salient properties: (i) large margin training, (ii) confidence weighting, (iii) capability to handle non-separable data, and (iv) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-theart algorithms (including AROW, NAROW and NHERD), we found that SCW generally achieves better or at least comparable predictive accuracy, but enjoys significant advantage of computational efficiency (i.e., smaller number of updates and lower time cost).", "creator": "LaTeX with hyperref package"}}}