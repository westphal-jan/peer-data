{"id": "1206.4613", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Near-Optimal BRL using Optimistic Local Transitions", "abstract": "Model-based Bayesian Reinforcement Learning (BRL) allows a found formalization of the problem of acting optimally while facing an unknown environment, i.e., avoiding the exploration-exploitation dilemma. However, algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms. This paper introduces BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is optimistic about the transition function. We analyze BOLT's sample complexity, and show that under certain parameters, the algorithm is near-optimal in the Bayesian sense with high probability. Then, experimental results highlight the key differences of this method compared to previous work.", "histories": [["v1", "Mon, 18 Jun 2012 15:00:40 GMT  (502kb)", "http://arxiv.org/abs/1206.4613v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["mauricio araya-l\u00f3pez", "olivier buffet", "vincent thomas"], "accepted": true, "id": "1206.4613"}, "pdf": {"name": "1206.4613.pdf", "metadata": {"source": "META", "title": "Near-Optimal BRL using Optimistic Local Transitions", "authors": ["Mauricio Araya-L\u00f3pez", "Vincent Thomas", "Olivier Buffet"], "emails": ["maraya@loria.fr", "vthomas@loria.fr", "buffet@loria.fr"], "sections": [{"heading": "1. Introduction", "text": "Acting in an unknown environment requires trading in exploration (to acquire knowledge) and exploitation (to maximize the expected return), and Bayesian Reinforcement Learning (BRL) model-based algorithms achieve this while maintaining and using a probability distribution over possible models (which requires expertise in the form of a previous one), which usually fall into one of the three following classes (Asmuth et al., 2009). Belief in predictive approaches attempts to address exploration and exploitation by reformulating RL as the problem of solving a POMDP where the state is a pair in a state of observation and distribution over possible models; nevertheless, this problem is irrevocable and allows only computorically expensive, uncompromised solutions (Poupart et al., 2006).Published in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Reinforcement Learning", "text": "A Markov Decision Process (MDP) (Puterman, 1994) is defined by a tuple < S, A, T, R >, where S is a finite set of states, A is a finite set of actions, the transitional function T is the likelihood of transition from state s to state s \"if an act a is performed: T (s, a, s\") = Pr (s), and R (s, a, s \") is the immediate scalar reward that is obtained during this transition. Reinforcement Learning (RL) (Sutton & Barto, 1998) is the problem of finding an optimal decision policy - a mapping policy: S 7 \u2192 A - if the model (T without R in our case) is unknown, but while it interacts with the system. A typical performance criterion is the expected discounting of the returnees (s), i.e. the way of exploiting optimal decision-making (A without R - the model - S-7 is unknown in our case)."}, {"heading": "2.2. Model-based Bayesian RL", "text": "We are looking at model-based Bayesian Reinforcement Learning (Strens, 2000), i.e., model-based DP system, in which knowledge of the model is represented by a probability distribution b across all possible transition models. An initial prior distribution b0 = Pr (\u00b5) must be specified, which is then updated using the Bayes rule. However, this update can be applied sequentially based on the Markov assumption, i.e., the rear bt model depends on the initial distribution b0 and the state story so far ht = s0, a0, \u00b7, st \u2212 1, st. We only need bt and the triplets (st + 1) to calculate the new distribution: bt + 1 = Pr (\u00b5 | ht + 1, b0) = Pr (\u00b5 | st, at, at, at, st + 1, bt).The bt distribution is known as the belief about the model, and summarizes the information we have gathered about the model."}, {"heading": "2.3. Flat and Structured Priors", "text": "A naive approach is to consider an independent Dirichlet distribution for each transition between states, known as the Flat Dirichlet Multinomial Prior (FDM), the pdf of which is asb = f (\u00b5; \u03b8) = Ps, a D (\u00b5s, a; \u03b8s, a), where D (\u00b7; \u00b7) are independent Dirichlet distributions. FDMs can be applied to all discrete state MDP, but are appropriate only on the strong assumption of the independence of the state pair of shareholders in the transition function. However, due to their simplicity, this prediction has been generally used to calculate Bayesian updates and the expected value. Considering that the vector of parameters are the counterparts of the observed transitions, then the expected value of a transition probability E [Pr (s) | s, a problem of Indian actualization and the expected value is."}, {"heading": "3. PAC Algorithms", "text": "Probably Approximately Correct Learning (PAC) provides a way to analyze the quality of learning algorithms (Valiant, 1984).The general idea is that a machine with a low training error is highly likely to generate 1 \u2212 \u03b4 (probably) a low generalization error limited by (roughly correct).If the number of steps required to arrive at this condition is limited by a polynomic function, then the algorithm is PACefficient."}, {"heading": "3.1. PAC-MDP Analysis", "text": "In RL, the PAC-MDP property (Strehl et al., 2009) guarantees that an algorithm generates a -close policy with the probability of 1 \u2212 \u03b4 in all but a polynomic number of steps. An important result is the general PACMDP theorem 10 in Strehl et al. (2009), in which three sufficient conditions are submitted to match the PAC-MDP property. First, the algorithm must use at least approximately optimistic values with a high probability. Furthermore, the algorithm must guarantee with a high probability that it is accurate, meaning that its actual valuation for the known parts of the model will be -close to the optimal value function. Finally, the number of non-close steps (also referred to as sample complexity) must be limited by a polynomic function. In mathematical terms, PAC-MDP algorithms are those for which the probability of 1 \u2212 3 is the RDP policy."}, {"heading": "3.2. PAC-BAMDP Analysis", "text": "An alternative to the PAC-MDP approach is the PAC in terms of optimal Bayesian policy, rather than optimal utopian policy. We will call this PAC-BAMDP analysis because it aims to ensure proximity to the optimal solution of the Bayes Adaptive MDP. This type of analysis was first introduced in Kolter & Ng (2009) under the name of near-Bayesian property, which shows that a modified version of beb PAC-BAMDP is for the non-discounted finite horizon. 1. Let us define how to evaluate policy in the Bayesian sense: Definition 3.1. Bayesian rating V of a policy is the expected value that has been detected in a distribution over 1However, some - reciprocable - errors have been detected in the evidence of near-Bayesian policy."}, {"heading": "4. Optimistic BRL Algorithms", "text": "Sec. 2.2 has shown how to theoretically calculate the optimal Bayesian value function. This computation2 We use a different notation for the Bayesian rating V to distinguish it from a normal MDP rating V. Since it is insoluble, it is common to use suboptimal - but efficient - algorithms. A popular technique is to maintain a back scale above belief, select a representative MDP based on the back scale and act according to its value function. The basic algorithm in this family is called exploit (Poupart et al., 2006), in which the expected model of b is selected at each time step. Therefore, the algorithm must resolve another MDP of the horizon H - an algorithm parameter, not the problem horizon - at each time step t as shown in Figure 1. We will consider for analysis that H is the number of iterations i that a value titeration can be achieved in each time step, but can be performed in practice."}, {"heading": "4.1. Bayesian Optimistic Local Transitions", "text": "In this section, we present a novel algorithm called Bayesian Optimistic Local Transitions, which relies on being optimistic in each step of the time, following the optimal policy for an optimistic variant of the current expected model. This variant is obtained by optimistically increasing the Bayean updates for each state-action pair before calculating the local expected transition model. This is achieved by using a new MDP with an extended action space A = A \u00b7 S, in which the transition model for action problems \u03b1 = (a) in the state s derives the local expected model, which is updated with an artificial proof for transitions."}, {"heading": "5. Analysis of BOLT", "text": "The other algorithm turned out to be PACBAMDP is beb, but the analysis presented in Kolter & Ng (2009) refers only to finite horizon ranges with an imposed stop condition for the Bayes update. Therefore, we need to include an analysis of beb in (Araya-Lo'pez et al., 2012) in order to be able to compare these algorithms theoretically retrospectively. According to Definition 3.2, we need to analyze the policy generated by bolts at a time t, i.e. At = argmax\u03c0 V-bolt, \u03c0 H (st), and show that this policy is highly probable and close to the optimal Bayesian policy for all but a polynomic number of steps. Theorem 5.1 (Bolt is PAC-BAMDP)."}, {"heading": "5.1. Mixed Value Function", "text": "To prove that Stud is PAC-BAMDP, we present some preliminary concepts and results. First, for the analysis, we assume that we maintain a vector of transition counters, even though the priorities may be different for the specific problem presented in this section. Since faith is monitored, we can define at each step a set of K = (s, a) | x, x (m) known pairs of state shareholders (Kearns & Singh, 1998), i.e., pairs of state shareholders with \"sufficient\" evidence. Also, to analyze an exploitative algorithm A in general (such as exploitation, stud, or beb), we introduce a mixed value function, which we obtain by performing an exact Bayesian update when a pair of state shareholders is in K, and an update of A when these concepts are not in K. We can use Lemma 5 of Kolter & Ng (2009) for the Discussed Lem.2 Reduced Lemma (Ib) (Ib)."}, {"heading": "5.2. BOLT is PAC-BAMDP", "text": "The difference between the optimistic value achieved by the mixing function and the value generated by the mixing function is generated by the mixing function; the difference between the optimistic value achieved by the mixing function and the value of the mixing function is generated by the mixing function; the difference between the optimistic value achieved by the mixing function and the value of the mixing function is generated by the mixing function; the difference between the mixing function and the value of the mixing function is generated by the mixing function; and the difference between the mixing function and the value of the mixing function is generated by the mixing function."}, {"heading": "6. Experiments", "text": "To illustrate the characteristics of the bolt, we present here experimental results over a number of areas. For all areas, we have tried different parameters for bolts and bolts, but also used an \u03b5-greedy variant of exploitation. However, for all the problems shown, plain exploit (\u03b5 = 0,0) exceeds the \u03b5greedy variant. Please remember that the theoretical values for the parameters \u03b2 and \u03b7 - which guarantee optimism - depend on the horizon H of the MDPs solved at each time step. Instead of using this horizon, we relied in these experiments on asynchronous value classification, which stopped when \"Vi + 1 \u2212 Vi \u0432\u0438\u043c <. To solve these infinite MDPs, we used \u03b3 = 0.95 and = 0.01, but note that the performance criterion used here is on average non-discounted total rewards."}, {"heading": "6.1. The Chain Problem", "text": "In the 5-state chain problem (Strens, 2000; Poupart et al., 2006), each state is associated with state s1 by taking measures b and each state si is associated with the next state si + 1, with the action a, except s5, which is associated with itself. At each step, the agent can \"slip\" with the probability p by performing the opposite action as intended. Remaining in s5 had a reward of 1.0, while returning to s1 had a reward of 0.2. All other rewards are 0. The priors used for these problems were Full (FDM), exhausted, where the probability p is taken into account for all transitions, and Semi, where each action has an independent factor probability. Table 1 shows that beb used other algorithms with a matched \u03b2 value for the FDM values before, as already shown by Kolter & Ng (2009), is very different."}, {"heading": "6.2. Other Structured Problems", "text": "Another illustrative example is the Paint / Polish problem, where the goal is to deliver multiple polished and painted objects without a scratch, using several stochastic actions with unknown probabilities. A full description of the problem can be found in Walshet al. (2009). Here, the possible results of each action are given to the agent, but the probabilities of each result are not. We have used a structured prediction that encodes this information and the results are summarized in Fig. 3, using both high and low resolution analyses. We have also performed this experiment with an FDM, obtaining results similar to those for the chain problem. Unsurprisingly, the use of a structured prediction yields better results than the use of FDMs. However, the high impact of over-optimization shown in Fig. 3 does not apply to FDMs, mainly because the learning phase using a structured advance is much shorter than before."}, {"heading": "7. Conclusion", "text": "We have presented Bolts, a novel and simple algorithm that gives the Bayes update an optimistic thrust, which is therefore more optimistic about uncertainty than just uncertainty. We have shown that bolts are strictly optimistic for certain \u03b7 parameters, and have used this result to prove that they are also PACBAMDP. The boundaries of sample complexity for bolts are tighter than for beb. Experiments show that bolts are more efficient than beb when using the theoretically derived parameters in the chain problem, and generally that bolts appear more robust for parameter adjustment. Future work includes the use of a dynamic \u03b7 bonus for bolts, which should be particularly suitable for finite horizons, and the exploration of general evidence to guarantee PAC-BAMDP ownership for a broader family of priors than FDMs.5On average over 100 trials with H = 100."}], "references": [{"title": "Nearoptimal BRL using optimistic local transitions (extended version)", "author": ["M. Araya-L\u00f3pez", "V. Thomas", "O. Buffet"], "venue": "Technical Report 7965,", "citeRegEx": "Araya.L\u00f3pez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Araya.L\u00f3pez et al\\.", "year": 2012}, {"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "In Proc. of UAI,", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "JMLR, 3:213\u2013231,", "citeRegEx": "Brafman and Tennenholtz,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 2003}, {"title": "Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["M. Duff"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Duff,? \\Q2002\\E", "shortCiteRegEx": "Duff", "year": 2002}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "In Machine Learning,", "citeRegEx": "Kearns and Singh,? \\Q1998\\E", "shortCiteRegEx": "Kearns and Singh", "year": 1998}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J. Kolter", "A. Ng"], "venue": "In Proc. of ICML,", "citeRegEx": "Kolter and Ng,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "In Proc. of ICML,", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": "WileyInterscience,", "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Variance-based rewards for approximate Bayesian reinforcement learning", "author": ["J. Sorg", "S. Singh", "R. Lewis"], "venue": "In Proc. of UAI,", "citeRegEx": "Sorg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Reinforcement learning in finite MDPs", "author": ["A.L. Strehl", "L. Li", "M.L. Littman"], "venue": "PAC analysis. JMLR,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Strens", "Malcolm J. A"], "venue": "In Proc. of ICML,", "citeRegEx": "Strens and A.,? \\Q2000\\E", "shortCiteRegEx": "Strens and A.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Szita", "Istvn", "Szepesvri", "Csaba"], "venue": "In Proc. of ICML,", "citeRegEx": "Szita et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Szita et al\\.", "year": 2010}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "In Proc. of STOC. ACM,", "citeRegEx": "Valiant,? \\Q1984\\E", "shortCiteRegEx": "Valiant", "year": 1984}, {"title": "Exploring compact reinforcement-learning representations with linear regression", "author": ["T.J. Walsh", "I. Szita", "C. Diuk", "M.L. Littman"], "venue": "In Proc. of UAI,", "citeRegEx": "Walsh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Walsh et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "These algorithms typically fall within one of the three following classes (Asmuth et al., 2009).", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "Belief-lookahead approaches try to optimally trade off exploration and exploitation by reformulating RL as the problem of solving a POMDP where the state is a pair \u03c9 = (s, b), s being the observed state and b the distribution over the possible models; yet, this problem is intractable, allowing only computationally expensive approximate solutions (Poupart et al., 2006).", "startOffset": 348, "endOffset": 370}, {"referenceID": 8, "context": "Optimistic approaches propose exploration mechanisms that explicitly attempt to reduce the model uncertainty (Brafman & Tennenholtz, 2003; Kolter & Ng, 2009; Sorg et al., 2010; Asmuth et al., 2009) by relying on the principle of \u201coptimism in the face of uncertainty\u201d.", "startOffset": 109, "endOffset": 197}, {"referenceID": 1, "context": "Optimistic approaches propose exploration mechanisms that explicitly attempt to reduce the model uncertainty (Brafman & Tennenholtz, 2003; Kolter & Ng, 2009; Sorg et al., 2010; Asmuth et al., 2009) by relying on the principle of \u201coptimism in the face of uncertainty\u201d.", "startOffset": 109, "endOffset": 197}, {"referenceID": 9, "context": "For some algorithms, recent work proves that they are either PAC-MDP (Strehl et al., 2009)\u2014with high probability they often act as an optimal policy would do (if the MDP model were known)\u2014or PAC-BAMDP (Kolter & Ng, 2009)\u2014with high probability they often act as an ideal belief-lookahead algorithm would do.", "startOffset": 69, "endOffset": 90}, {"referenceID": 1, "context": "Then, Section 4 introduces a novel algorithm, bolt, which, (1) as boss (Asmuth et al., 2009), is optimistic about the transition model\u2014which is intuitively appealing since the uncertainty is about the model\u2014and, (2) as beb (Kolter & Ng, 2009), is (almost) deterministic\u2014which leads to a better control over this approach.", "startOffset": 71, "endOffset": 92}, {"referenceID": 7, "context": "A Markov Decision Process (MDP) (Puterman, 1994) is defined by a tuple \u3008S,A, T,R\u3009 where S is a finite", "startOffset": 32, "endOffset": 48}, {"referenceID": 3, "context": "The belief-state can thus be written as \u03c9 = (s, b), which defines a Bayes-Adaptive MDP (BAMDP) (Duff, 2002), a special kind of belief-MDP where the belief-state is factored into the (visible) system state and the belief over the (hidden) model.", "startOffset": 95, "endOffset": 107}, {"referenceID": 3, "context": "The optimal Bayesian policy can then be obtained by computing the optimal Bayesian value function (Duff, 2002; Poupart et al., 2006): V\u2217(s, b)", "startOffset": 98, "endOffset": 132}, {"referenceID": 6, "context": "The optimal Bayesian policy can then be obtained by computing the optimal Bayesian value function (Duff, 2002; Poupart et al., 2006): V\u2217(s, b)", "startOffset": 98, "endOffset": 132}, {"referenceID": 8, "context": "Some of them solve the MDP generated by the expected model (at some stage) with an added exploration reward which favors transitions with lesser known models, as in r-max (Brafman & Tennenholtz, 2003), beb (Kolter & Ng, 2009), or with variance based rewards (Sorg et al., 2010).", "startOffset": 258, "endOffset": 277}, {"referenceID": 1, "context": "Another approach, used in boss (Asmuth et al., 2009), is to solve, when the model has changed sufficiently, an optimistic estimate of the true MDP (obtained by merging multiple sampled models).", "startOffset": 31, "endOffset": 52}, {"referenceID": 1, "context": "One can for example encode the fact that multiple actions share the same model by factoring multiple Dirichlet distributions, or allow the algorithm to identify such structures using Dirichlet distributions combined using Chinese Restaurant Processes or Indian Buffet Processes (Asmuth et al., 2009).", "startOffset": 278, "endOffset": 299}, {"referenceID": 13, "context": "Probably Approximately Correct Learning (PAC) provides a way of analyzing the quality of learning algorithms (Valiant, 1984).", "startOffset": 109, "endOffset": 124}, {"referenceID": 9, "context": "In RL, the PAC-MDP property (Strehl et al., 2009) guarantees that an algorithm generates an -close policy with probability 1\u2212\u03b4 in all but a polynomial number of steps.", "startOffset": 28, "endOffset": 49}, {"referenceID": 9, "context": "In RL, the PAC-MDP property (Strehl et al., 2009) guarantees that an algorithm generates an -close policy with probability 1\u2212\u03b4 in all but a polynomial number of steps. An important result is the general PACMDP Theorem 10 in Strehl et al. (2009), where three sufficient conditions are presented to comply with the PAC-MDP property.", "startOffset": 29, "endOffset": 245}, {"referenceID": 9, "context": "For example, r-max and Delayed Q-Learning (Strehl et al., 2009) are some classic RL algorithms for which this property has been proved, whereas BOSS (Asmuth et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 1, "context": ", 2009) are some classic RL algorithms for which this property has been proved, whereas BOSS (Asmuth et al., 2009) is a Bayesian RL algorithm which is also PAC-MDP.", "startOffset": 93, "endOffset": 114}, {"referenceID": 6, "context": "This utopic policy (Poupart et al., 2006) cannot be computed, because it is impossible to learn exactly the model with a finite number of samples, but it is possible to reason on the probabilistic error bounds of an approximation to this policy.", "startOffset": 19, "endOffset": 41}, {"referenceID": 3, "context": "This definition has already been presented implicitly by Duff (2002) , but it is very important to point out the difference between a normal MDP evaluation over some known MDP, and the Bayesian evaluation.", "startOffset": 57, "endOffset": 69}, {"referenceID": 6, "context": "The baseline algorithm in this family is called exploit (Poupart et al., 2006), where the expected model of b is selected at each time step.", "startOffset": 56, "endOffset": 78}, {"referenceID": 1, "context": "In contrast, boss (Asmuth et al., 2009) does not use the exploit approach, but samples different models from the prior and uses them to construct an optimistic MDP.", "startOffset": 18, "endOffset": 39}, {"referenceID": 0, "context": "[Proof in (Araya-L\u00f3pez et al., 2012)]", "startOffset": 10, "endOffset": 36}, {"referenceID": 0, "context": "Therefore, we include in (Araya-L\u00f3pez et al., 2012) an analysis of beb using the results of this section in order to be able to compare these algorithms theoretically afterwards.", "startOffset": 25, "endOffset": 51}, {"referenceID": 0, "context": "[Proof in (Araya-L\u00f3pez et al., 2012)]", "startOffset": 10, "endOffset": 36}, {"referenceID": 0, "context": "[Proof in (Araya-L\u00f3pez et al., 2012)]", "startOffset": 10, "endOffset": 36}, {"referenceID": 0, "context": "-close steps (see (Araya-L\u00f3pez et al., 2012)), which is a worse result than bolt.", "startOffset": 18, "endOffset": 44}, {"referenceID": 6, "context": "In the 5-state chain problem (Strens, 2000; Poupart et al., 2006), every state is connected to state s1 by taking action b and every state si is connected to the next state si+1 with action a, except s5 that is connected to itself.", "startOffset": 29, "endOffset": 65}, {"referenceID": 1, "context": "The last example is the Marble Maze problem (Asmuth et al., 2009) where we have explicitly encoded the 16 possible clusters in the prior, leading to little exploration requirements.", "startOffset": 44, "endOffset": 65}], "year": 2012, "abstractText": "Model-based Bayesian Reinforcement Learning (BRL) allows a sound formalization of the problem of acting optimally while facing an unknown environment, i.e., avoiding the exploration-exploitation dilemma. However, algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms. This paper introduces bolt, a simple and (almost) deterministic heuristic algorithm for BRL which is optimistic about the transition function. We analyze bolt\u2019s sample complexity, and show that under certain parameters, the algorithm is nearoptimal in the Bayesian sense with high probability. Then, experimental results highlight the key differences of this method compared to previous work.", "creator": "LaTeX with hyperref package"}}}