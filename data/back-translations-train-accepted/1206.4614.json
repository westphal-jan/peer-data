{"id": "1206.4614", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Information-theoretic Semi-supervised Metric Learning via Entropy Regularization", "abstract": "We propose a general information-theoretic approach called Seraph (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, Seraph is regularized by encouraging a low-rank projection induced from the metric. The optimization of Seraph is solved efficiently and stably by an EM-like scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that Seraph compares favorably with many well-known global and local metric learning methods.", "histories": [["v1", "Mon, 18 Jun 2012 15:01:43 GMT  (492kb)", "http://arxiv.org/abs/1206.4614v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["gang niu", "bo dai", "makoto yamada", "masashi sugiyama"], "accepted": true, "id": "1206.4614"}, "pdf": {"name": "1206.4614.pdf", "metadata": {"source": "META", "title": "Information-theoretic Semi-supervised Metric Learningvia Entropy Regularization", "authors": ["Gang Niu", "Bo Dai", "Makoto Yamada", "Masashi Sugiyama"], "emails": ["GANG@SG.CS.TITECH.AC.JP", "BOHR.DAI@GMAIL.COM", "YAMADA@SG.CS.TITECH.AC.JP", "SUGI@CS.TITECH.AC.JP"], "sections": [{"heading": "1. Introduction", "text": "A good measure of input data is a key factor for many machine learning algorithms. Classical metric learning methods fall into three types: (a) Supervised type require class labels (e.g., Sugiyama, 2007); (b) Supervised type require weak labels, i.e., {\u00b1 1} -rated labels that indicate the similarity / dissimilarity of data pairs (e.g., Weinberger et al., 2005; Davis et al., 2007); (c) Unsupervised type that does not require label information (e.g., Belkin & Niyogi, 2001). Types (a) and (b) have a strict limitation on the application of the real world in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Copyright 2012 by the author (s) / owner (s).tions, as they require many labels."}, {"heading": "2. Proposed Approach", "text": "In this section we first formulate the model of SERAPH and then develop the EM-like algorithm to solve the model."}, {"heading": "2.1. Notations", "text": "Let's assume we have a training set X = {xi | xi, xj} ni = 1, which contains n dots with m attributes each. Let's leave the sentences more similar and unequal data pairs beS = {(xi, xj) | xi and xj are similar, D = {(xi, xj) | xi and xj are unequal. With some misuse of terminology, let's refer to S = D as labeled data andU = {(xi, xj) | i = j = j, (xi, xj) 6. We abbreviate the words \"S,\" \"\" D, \"\" \"xi,\" x, \"\" x, \"\" and \"y,\" \"\" (xi, xj), \"\" S, \"or\" yi, \"j = \u2212 1 to (xi, xj). We abbreviate the words\" S, \"\" \"\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"and\" x, \"where the labels\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \"x,\", \"x,\" x, \"x,\" x, \"x,\" x, \"x,\" x, \",\" x, \"x,\" x, \"x,\", \",\" x, \"x,\" x, \",\" x, \",\", \",\" x, \"x,\", \",\" x, \"x,\" x, \",\", \"x,\", \"x,\", \"x,\" x, \"x,\" x, \"x,\", \",\" x, \",\", \"x,\", \",\", \",\", \""}, {"heading": "2.2. Basic model", "text": "First, we derive a probability model by which the conditional probability of y = > 1 given (x, x) given (x, x) given (x, x) given (x, x) given (x, x) given (x) given (x) given (x, x). We use a parametric form of pA (y, x, x) given (x) given (x, x) given (x) given (x) given (x) given (Jaynes, 1957) and focus on it for the capacities lying outside the sample. The maximum entropy principle (j) ln p (j) p i, j (y), j) x, the entropy probabilities corresponding to the data moments. Letter 3H (pAi, j) = \u2212 y pAi, j (y), j (y) ln p (j) i, j (y), andf (x), the entropy Ai, j (y), andf (x), andf (x): Rm \u00b7 function, j (y) ln p (i), j (j) ln p (j), j (y), j) x, the entropy probabilities corresponding to the data moments."}, {"heading": "2.3. Regularization", "text": "In this subsection, we extend L1 (A) through entropy regularization to semi-supervised learning. In addition, we regulate our goal through trace norm regularization. Our unsupervised part does not rely on multiple assumptions and does not correspond to the paradigm of smoothing the projected training data. To be more naturally integrated into the supervised part of the philosophy, we follow the minimum entropy principle (Grandvalet & Bengio, 2004), and therefore pAi, j should be a low entropy or uncertainty for (xi, xj). Roughly speaking, the resulting discriminatory models prefer peak distributions on unlabeled data that perform a probable low-density separation."}, {"heading": "2.4. Algorithm", "text": "From now on, we will simplify the model (6) and derive a practical algorithm from it. (1) First, we will use a simple function (4) in (1).Theorem 2. Define the simplified optimization problem as5max A = q = q = q = q = q = q = q = x x x x x x x x = x x x x x = 1 + x x x x x x x x x x x x x x x x (1). (8) Let us leave A = q = q (A), (7) where the simplified probability model isp = 1 x x x x x x x x x = 1 x x x x x (2). (8) Let A and (A) be the optimal solutions to (7) and (6). (7) Then there are well-defined hyperparameters."}, {"heading": "3. Discussions", "text": "In this section we discuss the thriftiness problems, namely the risk of a significant overvaluation of data. (Grac-a-et-al, 2009) We discuss the thriftiness problems, namely that we can maintain the rear thriftiness. (Grac-a-et-al, 2009) We think that the uncertainty (i.e., entropy or variance) is low. (Figure 1 as an example) We point out that the overarching metric learning goals in the same class are close and the data from different classes are far apart. (These results in the metric, which ignores the horizontal characteristics and focuses on the vertical characteristics, are important, and maintaining the rear thriftiness would lead to a better metric. (e) and (f) We prefer to consider the rear thriftiness problems, and then consider the risk of a significant overvaluation."}, {"heading": "4. Related Works", "text": "Xing et al. (2002) initiated metric learning based on pairwise similarity / dissimilarity Limitations of Worldwide Metric Learning (GDM) Several excellent metric learning methods have been developed over the past decade, including Neighborhood Component Analysis (NCA; Goldberger et al., 2004), Large Margin Close Neighborhood Classification (LMNN; Weinberger et al., 2005), and Information Theory Metric Learning (ITML; Davis et al., 2007). Both ITML and SERAPH are information theory, but the ideas and models differ significantly. ITML defines a generative model pA (x) = exp (\u2212 x \u2212 \u00b5 2 A) / Z, in which an unknown mean and Z is a normalizing constant. Compared with GDM, ITML regulates the KL divergence between pA0 (x) and pA (x), transforming this term into a log definition."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Setup", "text": "We compared SERAPH with Euclidean distance, four famous monitored and two representative semi-monitored metric learning methods6: global remote metric learning (GDM; Xing et al., 2002), neighbourhood component analysis (NCA; Goldberger et al., 2004), large margin in the next neighbourhood classification (LMNN; Weinberger et al., 2005), information theory metric learning (ITML; Davis et al., 2007), local metric learning (LDM; Yang et al., 2006) and diverse Fisher discriminant analysis (MFDA; Baghshah & Shouraki, 2009).Table 1 describes the specification of the datasets used in our experiments. The six best datasets (i.e. iris, wine, ionosphere, breast cancer and diabetes) are from the UCI machine learning class repository7, while the USPS and MNIST are downloaded from the late Roweiss homepage."}, {"heading": "5.2. Results", "text": "Numbers 1 and 2 had previously brought the visually comprehensive results of the Regularization division to two artificial data sets. (c) and (d) in both numbers were obtained from GDM, while (e) and (f) were marked by SERAPH. (D) The experimental results of the one-sided classification are discussed in Table 2. (D) Figures 1 and 2 dramatically increase global metric learning. (D) The experimental results of the one-sided classification are discussed in Table 2. (D)"}, {"heading": "6. Conclusions", "text": "In this paper, we proposed an information theory semi-supervised metric learning approach SERAPH as an alternative to the multiple methods, based on the generalized maximum entropy estimation for supervised metric learning. Then, a semi-supervised extension was achieved, which can achieve posterior sparseness through entropy regularization, and we implemented a trace norm regularization, which can achieve the sparseness of projection. The resulting optimization was solved by an EM-like scheme with several nice algorithmic properties, and the learned metrics proved highly distinguishable even under a noisy environment. Experiments with benchmark datasets showed that SERAPH often exceeded the state of the art in fully / semi-supervised metric learning methods, which were supervised only to a limited extent. A final remark is that in our experiments the rear and projection sparsenses proved very helpful for high-dimensional data when combined with future work, i.e. when they were integrated into our hyper-perimeter."}, {"heading": "Acknowledgments", "text": "The authors would like to thank anonymous reviewers for their helpful comments. GN is supported by the MEXT scholarship No. 103250, MY by the JST PRESTO program and MS by the FIRST program."}], "references": [{"title": "Semi-supervised metric learning", "author": ["M. Baghshah", "S. Shouraki"], "venue": null, "citeRegEx": "Baghshah and Shouraki,? \\Q2006\\E", "shortCiteRegEx": "Baghshah and Shouraki", "year": 2006}, {"title": "Laplacian eigenmaps and spectral tech", "author": ["M. Belkin", "P. Niyogi"], "venue": null, "citeRegEx": "Belkin and Niyogi,? \\Q2009\\E", "shortCiteRegEx": "Belkin and Niyogi", "year": 2009}, {"title": "Informationtheoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Maximum entropy distribution estimation with generalized regularization", "author": ["M. Dud\u0131\u0301k", "R.E. Schapire"], "venue": "In COLT,", "citeRegEx": "Dud\u0131\u0301k and Schapire,? \\Q2006\\E", "shortCiteRegEx": "Dud\u0131\u0301k and Schapire", "year": 2006}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of Eugenics,", "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Discriminative clustering by regularized information maximization", "author": ["R. Gomes", "A. Krause", "P. Perona"], "venue": "In NIPS,", "citeRegEx": "Gomes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2010}, {"title": "Posterior vs. parameter sparsity in latent variable models", "author": ["J. Gra\u00e7a", "K. Ganchev", "B. Taskar", "F. Pereira"], "venue": "In NIPS,", "citeRegEx": "Gra\u00e7a et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gra\u00e7a et al\\.", "year": 2009}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Grandvalet and Bengio,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet and Bengio", "year": 2004}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["S. Hoi", "W. Liu", "Chang", "S.-F"], "venue": "In CVPR,", "citeRegEx": "Hoi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2008}, {"title": "GSML: A unified framework for sparse metric learning", "author": ["K. Huang", "Y. Ying", "C. Campbell"], "venue": "In ICDM,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Robust metric learning by smooth optimization", "author": ["K. Huang", "R. Jin", "Z. Xu", "C. Liu"], "venue": "In UAI,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Information theory and statistical mechanics", "author": ["E.T. Jaynes"], "venue": "Physical Review,", "citeRegEx": "Jaynes,? \\Q1957\\E", "shortCiteRegEx": "Jaynes", "year": 1957}, {"title": "Semi-supervised sparse metric learning using alternating linearization optimization", "author": ["W. Liu", "S. Ma", "D. Tao", "J. Liu", "P. Liu"], "venue": "In KDD,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "A general method for solving extremal problems (in Russian)", "author": ["B.T. Polyak"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Polyak,? \\Q1967\\E", "shortCiteRegEx": "Polyak", "year": 1967}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, 290:2323\u20132326,", "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis", "author": ["M. Sugiyama"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sugiyama,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama", "year": 2007}, {"title": "Semi-supervised local Fisher discriminant analysis for dimensionality reduction", "author": ["M. Sugiyama", "T. Id\u00e9", "S. Nakajima", "J. Sese"], "venue": "Machine Learning,", "citeRegEx": "Sugiyama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "J. Blitzer", "L. Saul"], "venue": "In NIPS,", "citeRegEx": "Weinberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E. Xing", "A. Ng", "M.I. Jordan", "S. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "An efficient algorithm for local distance metric learning", "author": ["L. Yang", "R. Jin", "R. Sukthankar", "Y. Liu"], "venue": "In AAAI,", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}, {"title": "Sparse metric learning via smooth optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": "In NIPS,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": ", {\u00b11}-valued labels that indicate the similarity/dissimilarity of data pairs (e.g., Weinberger et al., 2005; Davis et al., 2007); (c) Unsupervised type requiring no label information (e.", "startOffset": 78, "endOffset": 129}, {"referenceID": 20, "context": "To the best of our knowledge, all semi-supervised extensions employ off-theshelf techniques in type (c) such as principal component analysis (Yang et al., 2006; Sugiyama et al., 2010) or manifold embedding (Hoi et al.", "startOffset": 141, "endOffset": 183}, {"referenceID": 17, "context": "To the best of our knowledge, all semi-supervised extensions employ off-theshelf techniques in type (c) such as principal component analysis (Yang et al., 2006; Sugiyama et al., 2010) or manifold embedding (Hoi et al.", "startOffset": 141, "endOffset": 183}, {"referenceID": 9, "context": ", 2010) or manifold embedding (Hoi et al., 2008; Baghshah & Shouraki, 2009; Liu et al., 2010).", "startOffset": 30, "endOffset": 93}, {"referenceID": 13, "context": ", 2010) or manifold embedding (Hoi et al., 2008; Baghshah & Shouraki, 2009; Liu et al., 2010).", "startOffset": 30, "endOffset": 93}, {"referenceID": 4, "context": ", Fisher discriminant analysis1 (Fisher, 1936)), and the assistant one tries to identify and preserve the intrinsic geometric structure (e.", "startOffset": 32, "endOffset": 46}, {"referenceID": 7, "context": "on unlabeled data, which can achieve the sparsity of the posterior distribution (Gra\u00e7a et al., 2009), i.", "startOffset": 80, "endOffset": 100}, {"referenceID": 21, "context": "Furthermore, we employ mixed-norm regularization (Ying et al., 2009) to encourage the sparsity of the projection matrix, i.", "startOffset": 49, "endOffset": 68}, {"referenceID": 12, "context": "The maximum entropy principle (Jaynes, 1957) suggests that we should choose the probability distribution with the maximum entropy out of all distributions that match the data moments.", "startOffset": 30, "endOffset": 44}, {"referenceID": 14, "context": "Optimization (7) could be directly solved by the gradient projection method (Polyak, 1967), even though it is nonconvex.", "startOffset": 76, "endOffset": 90}, {"referenceID": 7, "context": "At the t-th E-Step, similarly to Gra\u00e7a et al. (2009), we have for each pair (xi, xj) \u2208 U that", "startOffset": 33, "endOffset": 53}, {"referenceID": 7, "context": "In this section, we discuss the sparsity issues, namely, we can obtain the posterior sparsity (Gra\u00e7a et al., 2009) by entropy regularization and the projection sparsity (Ying et al.", "startOffset": 94, "endOffset": 114}, {"referenceID": 21, "context": ", 2009) by entropy regularization and the projection sparsity (Ying et al., 2009) by trace-norm regularization.", "startOffset": 62, "endOffset": 81}, {"referenceID": 7, "context": "We can rewrite L2(A, \u03ba) as a soft posterior regularization (PR) objective (Gra\u00e7a et al., 2009).", "startOffset": 74, "endOffset": 94}, {"referenceID": 7, "context": "On the other hand, according to optimization (7) of Gra\u00e7a et al. (2009), the soft PR objective should take a form as", "startOffset": 52, "endOffset": 72}, {"referenceID": 21, "context": "Similarly to Ying et al. (2009), let P \u2208 Rm\u00d7m be a projection, and W = P>P be the metric induced from P .", "startOffset": 13, "endOffset": 32}, {"referenceID": 21, "context": "The equivalence of optimizations (6) and (15) is guaranteed by Lemma 1 of Ying et al. (2009).", "startOffset": 74, "endOffset": 93}, {"referenceID": 6, "context": "Moreover, there is another justification based on the information maximization principle (Gomes et al., 2010).", "startOffset": 89, "endOffset": 109}, {"referenceID": 5, "context": "Several excellent metric learning methods have been developed in the last decade, including neighborhood component analysis (NCA; Goldberger et al., 2004), large margin nearest neighbor classification (LMNN; Weinberger et al.", "startOffset": 124, "endOffset": 154}, {"referenceID": 18, "context": ", 2004), large margin nearest neighbor classification (LMNN; Weinberger et al., 2005), and information-theoretic metric learning (ITML; Davis et al.", "startOffset": 54, "endOffset": 85}, {"referenceID": 2, "context": ", 2005), and information-theoretic metric learning (ITML; Davis et al., 2007).", "startOffset": 51, "endOffset": 77}, {"referenceID": 20, "context": "A probabilistic GDM was designed intuitively as a baseline in the experimental part of Yang et al. (2006). It is a special case of our supervised part.", "startOffset": 87, "endOffset": 106}, {"referenceID": 20, "context": "Subsequently, local distance metric learning (LDM; Yang et al., 2006) is the pioneer of semi-supervised metric learning, which assumes that the eigenvectors of A are the principal components of training data.", "startOffset": 45, "endOffset": 69}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 115}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 147}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning.", "startOffset": 0, "endOffset": 179}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning. Liu et al. (2010) brings the element-wise sparsity to Hoi et al.", "startOffset": 0, "endOffset": 249}, {"referenceID": 9, "context": "Hoi et al. (2008) combines manifold regularization to the min-max principle of GDM based on Belkin & Niyogi (2001), and Baghshah & Shouraki (2009) shows that Roweis & Saul (2000) is also useful for semi-supervised metric learning. Liu et al. (2010) brings the element-wise sparsity to Hoi et al. (2008).", "startOffset": 0, "endOffset": 303}, {"referenceID": 10, "context": "Instead, we recommend Huang et al. (2009) and Huang et al.", "startOffset": 22, "endOffset": 42}, {"referenceID": 10, "context": "Instead, we recommend Huang et al. (2009) and Huang et al. (2010) for the latest reviews of sparse and robust metric learning respectively.", "startOffset": 22, "endOffset": 66}, {"referenceID": 19, "context": "We compared SERAPH with the Euclidean distance, four famous supervised and two representative semi-supervised metric learning methods6: global distance metric learning (GDM; Xing et al., 2002), neighborhood component analysis (NCA; Goldberger et al.", "startOffset": 168, "endOffset": 192}, {"referenceID": 5, "context": ", 2002), neighborhood component analysis (NCA; Goldberger et al., 2004), large margin nearest neighbor classification (LMNN; Weinberger et al.", "startOffset": 41, "endOffset": 71}, {"referenceID": 18, "context": ", 2004), large margin nearest neighbor classification (LMNN; Weinberger et al., 2005), information-theoretic metric learning (ITML; Davis et al.", "startOffset": 54, "endOffset": 85}, {"referenceID": 2, "context": ", 2005), information-theoretic metric learning (ITML; Davis et al., 2007), local distance metric learning (LDM; Yang et al.", "startOffset": 47, "endOffset": 73}, {"referenceID": 20, "context": ", 2007), local distance metric learning (LDM; Yang et al., 2006), and manifold Fisher discriminant analysis (MFDA; Baghshah & Shouraki, 2009).", "startOffset": 40, "endOffset": 64}], "year": 2012, "abstractText": "We propose a general information-theoretic approach called SERAPH (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, SERAPH is regularized by encouraging a low-rank projection induced from the metric. The optimization of SERAPH is solved efficiently and stably by an EMlike scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that SERAPH compares favorably with many well-known global and local metric learning methods.", "creator": "LaTeX with hyperref package"}}}