{"id": "1206.4615", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Levy Measure Decompositions for the Beta and Gamma Processes", "abstract": "We develop new representations for the Levy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on Levy processes, as these are of increasing importance in machine learning.", "histories": [["v1", "Mon, 18 Jun 2012 15:01:58 GMT  (238kb)", "http://arxiv.org/abs/1206.4615v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "stat.ME cs.LG math.ST stat.TH", "authors": ["yingjian wang", "lawrence carin"], "accepted": true, "id": "1206.4615"}, "pdf": {"name": "1206.4615.pdf", "metadata": {"source": "META", "title": "Le\u0301vy Measure Decompositions for the Beta and Gamma Processes", "authors": ["Yingjian Wang", "Lawrence Carin"], "emails": ["yw65@duke.edu", "lcarin@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "A prominent differentiation of non-parametric methods with respect to parametric approaches (s) indicates an increasing importance of these models; the application of stochastic processes rather than probability distributions; for example, a Gaussian process (Rasmussen & Williams, 2006) may be used to represent general smooth functions in a continuous space of covariates (e.g. time); in such processes, the non-parametric aspect concerns the number of characteristics / clusters that are allowed to be unrestricted (\"infinite\"), allowing the model to adjust the number of these entities as given and fu-appearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012."}, {"heading": "2. Background", "text": "Le'vy processes (Sato, 1999) and totally random metrics (Kingman, 1967) are two closely related concepts. Specifically, some Le'vy processes can be considered completely random metrics. In this section, brief reviews and connections to these two important concepts are presented."}, {"heading": "2.1. Le\u0301vy process", "text": "A Le vy process X (\u03c9) is a stochastic process with independent increments on a measuring space. Normally, a Le vy process is assumed to be one-dimensional, like the real line, in order to represent a stochastic process with variation over time. According to the Le vy Ito process (Sato, 1999), a Le vy process can be decomposed into a continuous Brownian motion with drift, and into a discrete part of a jump process. If a Le vy process X (\u03c9) has only the discrete part and its jumps are positive, then the characteristic function of the random variable X (A) is considered by: E {ejuX (A) = exp {R + \u00d7 A (ejup \u2212 1): (dp, dp): (1): a discriminatory process X (dp, dp): (1): a negative process that fulfills the integrability state (Sato, 1999): The expression of Le Jump (Le Jump: Le-equal to most of the current, non-lead) processes."}, {"heading": "2.2. Completely random measure", "text": "A random measurement (F) is called \"completely random\" when for all disjunction processes A1, A2 and F the random variables (A1) and A2 are independent of each other. A completely random measurement can be divided into three independent components: \u03a6 = \u03a6f + \u03a6d + \u03a6o (2), whereby \u03a6f = \u03b3f = hlf = hlf = hlf. An entirely random measurement can be divided into three independent components: \u03a6 = hlf + hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf = hlf)) randnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}, {"heading": "3. Beta process", "text": "A beta process (Hjort, 1990) is a Le vy process with beta-distributed steps; B-BP (c (\u03c9), \u00b5) is a beta process ifB (d\u03c9) \u0445 beta (c (\u03c9) \u00b5 (d\u03c9), c (\u03c9) (1 \u2212 \u00b5 (d\u03c9)))) (5), where \u00b5 is the basic measurement of the measurement space (\u044b, F) and a positive function c (\u03c9) for the concentration function. Expression (5) indicates that the steps of the beta process are independent, making it a special case of the Le-vy process family. The Le-vy measure of the beta process is \u03bd (d\u03c0, d\u03c9) = c (\u03c9) \u03c0 \u2212 1 (1 \u2212 \u03c0) c (\u03c9) \u2212 1d\u03c0\u00b5 (d\u0445) (6), whereby beta (0, c (\u043c) = c (\u043c) \u03c0 \u2212 1 \u2212 \u03c0) is an improper beta distribution, assuming that its integral process exceeds the poisal process (1)."}, {"heading": "3.1. Beta process Le\u0301vy measure decomposition", "text": "The infinite integral of improper beta distribution inspires a decomposition of improper distribution with an infinite number of self-distributions. The singularity of improper beta distribution is manifested by \u03c0 \u2212 1. Since the geometric series expansion is detailed with manipulation in the supplementary material, this results in the Le vy measurement of the decomposition theorem of the beta process: Theorem 1 For a beta process B (c) BP (c), \u00b5) with basic measurement and concentration c (6), we refer to the Poisson process as its underlying Poisson process and the Le vy measurement as a standalone measurement, then B measurement of the decay theorem of the beta process."}, {"heading": "3.2. The Le\u0301vy process Bk", "text": "It is interesting to examine the properties of Bk, such as expectation and variance. The designation Bk (d\u03c9) = 1 c (\u03c9) + k + 1\u00b5k (d\u03c9) as the base measure of Bk, in case B (A): E (Bk (A)) = B (d): A Bk (A) = Bk (A) Var (Bk (A))) = A 2 c (\u03c9) + K + 2 Bk (d\u03c9) (11) It is noteworthy that the Le-vy process Bk is no longer a beta process, since (5) is not satisfied. According to theorem 1, the jumps of Bk follow a correct beta distribution parameterized by the concentration function c (\u03c9) and the index k, and \u00b5k determines the places where the jumps take place. Since {Bk} \u00b2 k = 0 are independent, the index k is according to theorem 1:"}, {"heading": "3.3. Simulating the beta process", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1. Poisson superposition simulation", "text": "Theorem 1 shows that the underlying Poisson process of a beta process is an overlay of an infinite number of Poisson processes, each of which has a finite number of atoms. This perspective also represents a simulation method for the beta process: First, the Poisson process is sampled for all k = 0, 1, 2, \u00b7 \u00b7 \u00b7 (here we call the index k the \"round\" of the simulation); then we take the unification of the samples of each x k as a realization of the Poisson process. Implicitly applied with the Marker Theorem (Kingman, 1993), the simulation method of the beta process is as follows: For round k: 1: Sample the number of points for VK: nk, Poisson (VK) trection (VK) and VK (VK) as superdistribution."}, {"heading": "3.3.2. Related work", "text": "In (Thibaux & Jordan, 2007), the authors derive the simulation procedure described above for the homogeneous case in the context of the beta-Bernoulli process, which is presented here as a necessary result of the Le vy measurement decomposition. The same corrosive manipulation of theorem 1 can also be applied to the stable beta process (Teh & Go \ufffd ru \ufffd r, 2009), which results in: \u03bdk = Beta (1 \u2212 \u03c3, c (\u03c9) + \u03c3 + k) d\u03c0 \u00b7 (c (\u03c9) + k) \u0432 (c (\u03c9) + 1) \u0432 (c (\u03c9) + k + 1) \u0432 (c (\u03c9) + \u03c3) \u00b5 (d\u043c) (13) It is noteworthy that the decomposition method described in Theorem 1 is not the only Le vy measurement method for the beta process. The work of (Paisley & Jordan, 2012) and (Broderick al., 2011) shows that the faltering construction of a different Paisley process in 2010 represents a third Paisley process."}, {"heading": "3.4. Truncation analysis", "text": "Since the Poisson superposition simulation is performed in rounds, it is natural to analyze the distance between the true beta process B = > B and its circumcision \u2211 K = 0Bk, with circumcision in round K. One metric for this distance is the L1 standard: | | B \u2212 K \u2211 k = 0 Bk | | 1 = E | B \u2212 K \u2211 k = 0 Bk | = land use plan K + 1 (d\u043c).The expectation in (14) is the normalized measurement, which shows that B \u00b2 1 = 1. If B \u00b2 -K = 1 is bound, the B \u2212 K + 1 is reduced to cc \u2212 K + 1, indicating that the L1 distance decreases at a rate of O (1K). For the pitch-breaking construction of the beta process described in (Paisley et al., 2010), the L1 distance is: (cc + 1) K + 1.A further metric is the probability of the beta-L1 process."}, {"heading": "3.5. Posterior estimation", "text": "The aim of the conclusion is to estimate the beta process B from a series of observed data b with previous BP (c, \u00b5). Data b = b1: M are the same as in Section 3.4, which can be expressed as follows: bm = \u221e \u2211 i = 1 bi, m\u03b4\u03c9i, m = 1, 2, \u00b7 \u00b7, M (17), each bi, m \u00b2 {0, 1}."}, {"heading": "3.5.1. Posterior of Bk", "text": "Since B | b \u0445 BP (c + M, c\u00b5c + M + \u2211 M m = 1 bm c + M) (Thibaux & Jordan, 2007), the basic measure of B | b is a measure with positive masses assigned to individual atoms. Theorem 1 is still applicable to this mixed type of basic beta process leading to B \u2032 = \u00b2 k = 0 B \u00b2 k \u03bd \u00b2 k = beta (1, c + M + k) \u00b5 \u2032 k = c\u00b5 c + M + k + \u2211 M m = 1 bm c + M + k (18), with B \u2032, B \u2032 k, \u03bd \u2032 k and \u00b5 \u2032 k being the posterior counterparts to B, Bk, \u0441k and \u00b5k."}, {"heading": "3.5.2. Posterior estimation of \u03c0i:", "text": "Since each microk has a mass that results in M m = 1 bi, m c + M + k at the atomic spiral, each Bk contributes to Poisson (\u2211 M = 1 bi, m c + M + k) if the jumps occur after the distribution Beta (1, c + M + k) at the atomic spiral, the sum of which is \u03c0i. Thus, the rear estimate of \u03c0i (1, c + M + k) (19) results, from which it can be confirmed that E (\u03c0i | b) = 0 Hk \u2211 h = 1 bi, m c + M, the same as the rear processing scale without decomposition Beta (1, c + M + k) (19)."}, {"heading": "3.6. Relating the IBP and beta process", "text": "The study of the beta process through its Le-vy measurement discussed in this paper also reveals a correlation between the Indian Buffet Process (IBP) (Griffiths & Ghahramani, 2005) and the beta process through its Le-vy measurements. IBP with previous E-Beta measurement (c-N, c) can be considered a Le-vy process with the Le-vy measurement, which is given as follows: E-IBP = N-N = N-C-dp (21) here N is the same as K-in (Griffiths & Ghahramani, 2005). It can be proven that: E-IBP N-N = N-N (22), indicating that the beta process represents the limit of IBP with N-A increase; the detailed proof of (22) is presented in the Supplementary Material. Thus, IBP is like a \"mosaic\" approach to the beta process with N-increase."}, {"heading": "4. Gamma process", "text": "A gamma process (Applebaum, 2009) is a Le vy process with independent gamma increments. The gamma process is traditionally parameterized by a dimension and a scale function: G-P (\u03b1, \u03b8 (\u03c9)), where \u03b1 is the dimension on a measurement space (\u0432, F), and the scale \u03b8 (\u03c9) is a positive function. A gamma process can be intuitively defined by its increments on infinitesimal sets: G-P (\u03b1, \u03b8), where \u03b1 is the dimension on a measurement space (\u03b1, F), and the scale \u03b8 (\u03c9) is a positive function. The gamma process can also be expressed in the form of a base measure G0 and a concentration c (\u03c9), with c = 1 / th and G0 = dependence (Jordan., 2009), in order to align with other stochastic processes widely used in machine learning, such as the dilet process."}, {"heading": "4.1. Le\u0301vy measure decomposition", "text": "Like the beta process, the Le vy process of the gamma process is characterized by improper distribution. However, unlike the beta process, the gamma process G can be divided into two parts: G = 1 + 2 (1) and P (1). (26) The second term in (26) is a gamma process with the same dimension and half the scale of the gamma process G; the first term is a Le vy process with the Le vy process. (h) The second term in (26) is a gamma process with the same dimension. (h) and half the scale of the gamma process G; the first term is a Le vy process with the Le vy dimension. (h) The second term in (h) is a gamma process. (dp) 2)."}, {"heading": "4.2. Le\u0301vy processes \u0393k and \u0393kh", "text": "In order to gain further insight into the gamma process G in theorem 2, the expectations and deviations of the two groups A and B for each measurable quantity A and F are given: E (KH (A) = E (KH) \u03b1 (KH) (KH) (K + 1) h + 1E (KH (KH) = A (KH) \u03b1 (KH) k (K + 1) (28)) For the deviations of K and K: Var (KH (KH)) = (KH + 1) h + 2 (KH + 1) h + 2 (KH) A 2 (KH) \u03b1 (KH) \u03b1 (KH))) = [KH (KH) 1 \u2212 1 (KH + 1) 2 \u2022 A 2 (KH) (KH) (29) Since the Le vy processes are independent, it can be verified by analogy to (KH) that the expectation and deviation of the sum of K from the expectation of the deviation from this section is represented."}, {"heading": "4.3. Simulation of gamma process", "text": "Parallel to the simulation of the beta process in Section 3.3.1, a simulation method of the gamma process is presented: Simulation method: Sample of the Le vy process: 1: Sample of the score for \"kh\": nkh \"Poisson (\u03b3 / (k + 1) hh); 2: Sample of\" nkh \"points from\" \u03b1 \": \u03c9khi i.i.d., for\" i = 1, 2, \u00b7, nkh; 3: Sample \"kh\" (\u03c9khi) i.i.d. \"Gamma\" (h, \u03b8 (\u03c9khi) k + 1), for \"i = 1, 2, \u00b7 \u00b7, nkh\" where \"Q\" \u03b1 \"(d\u043c) is the mass of the form mea-safe. Then the union\" k \"= 1 s\" h = 1 (\u041akhi) h \", kh\" \"i = 1,\" \"\" nkh, \"\" \"\" nkh, \"\" \"\" \"\" nkh, \"\" \"\" \"\""}, {"heading": "4.4. Truncation analysis", "text": "Since in the simulation procedure in section 4.3 the index k and h both go to infinity, it is practical to analyze the distance between the true gamma process and the truncated one. To measure such a distance, we apply the L1 standard described in section 3.4: | | G \u2212 K \u00b2 k = 1 H \u00b2 h = 1 H \u00b2 kh | | 1 = E | G \u2212 K \u00b2 k = 1 H \u00b2 kh | (30), where the expectation in (30) is the normalized measure 1 = 1 K + 1 (31), indicating an O (1K) reduction rate as represented in (14). Noteworthy then is the situation in (30), where the expectation in (30) is the normalized measure 1 = 1 K + 1 (31) p \u00b2 h, indicating an O (1K) reduction rate as well as the truncated beta process in (14)."}, {"heading": "4.5. Generalized gamma process and symmetric gamma process", "text": "Theorem 2 can easily be extended to some variations of the gamma process. Here, we give the examples of the generalized gamma process (Brix, 1999) and the symmetric gamma process (C-inlar, 2010). The generalized gamma process extends the ordinary gamma process by adding a parameter 0 < \u03c3 < 1, whose Le vy measurement refers to 1\u044b (1 \u2212 \u03c3) p-value (\u03c9) dp\u03b1 (dp). Using the same decomposition process, it is then straightforward for the Le vy measure of the generalized gamma process to change in \u03bdkh = gamma (h \u2212 \u03c3, \u043c) k + 1) dp-value (k + 1) hh. The symmetric gamma process is a Le-vy process, whose increments are the differences of two gamma distribution variables with the same law, whose particularity is Le + 1 (dp)."}, {"heading": "5. Conclusions", "text": "The Le vy measurement of the decomposition of the beta and gamma processes offers new perspectives on the two widely used stochastic processes by providing insights into the subprocesses that constitute them, in this case Bk and Bk. And the decomposition regulations described here are far from the only ways of such decomposition. Theoretically elegant construction methods are derived from the proposed decomposition processes, which are directly applicable in practice.We have applied the proposed beta and gamma representations in numerical experiments, the details of which are omitted as this paper focuses on fundamental properties. However, to briefly summarize the experience with such representations, we consider, for example, the image inpainting problem considered in the beta process factor analysis model (Paisley & Carin, 2009)."}, {"heading": "Acknowledgements", "text": "The research reported here was supported by ARO, NGA, ONR and DARPA (MSEE programme)."}], "references": [{"title": "Processes and Stochastic Calculus", "author": ["Applebaum", "D. Levy"], "venue": null, "citeRegEx": "Applebaum and Levy,? \\Q2009\\E", "shortCiteRegEx": "Applebaum and Levy", "year": 2009}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "J. ACM,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Generalized gamma measures and shot-noise Cox processes", "author": ["A. Brix"], "venue": "Advances in Applied Probability,", "citeRegEx": "Brix,? \\Q1999\\E", "shortCiteRegEx": "Brix", "year": 1999}, {"title": "Beta processes, stick-breaking, and power laws", "author": ["T. Broderick", "M. Jordan", "J. Pitman"], "venue": "Bayesian analysis,", "citeRegEx": "Broderick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2011}, {"title": "Probability and Stochastics", "author": ["E. \u00c7inlar"], "venue": "Graduate Texts in Mathematics. Springer,", "citeRegEx": "\u00c7inlar,? \\Q2010\\E", "shortCiteRegEx": "\u00c7inlar", "year": 2010}, {"title": "Variational inference for the Indian buffet process", "author": ["F. Doshi", "K.T. Miller", "J. Van Gael", "Y.W. Teh"], "venue": "In AISTATS,", "citeRegEx": "Doshi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi et al\\.", "year": 2009}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T. Ferguson"], "venue": "The Annals of Statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2005}, {"title": "Nonparametric Bayes estimators based on beta processes in models for life history data", "author": ["N.L. Hjort"], "venue": "Annals of Statistics,", "citeRegEx": "Hjort,? \\Q1990\\E", "shortCiteRegEx": "Hjort", "year": 1990}, {"title": "Hierarchical models, nested models and completely random measures. In Frontiers of Statistical Decision Making and Bayesian Analysis: In Honor of James", "author": ["M.I. Jordan"], "venue": null, "citeRegEx": "Jordan.,? \\Q2009\\E", "shortCiteRegEx": "Jordan.", "year": 2009}, {"title": "Completely random measure", "author": ["J.F.C. Kingman"], "venue": "In Pacific Journal of Mathematics,", "citeRegEx": "Kingman,? \\Q1967\\E", "shortCiteRegEx": "Kingman", "year": 1967}, {"title": "Construction of dependent dirichlet processes based on poisson processes", "author": ["D. Lin", "E. Grimson", "J. Fisher"], "venue": "In NIPS, pp", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Dependent Nonparametric Processes", "author": ["S.N. MacEachern"], "venue": "Proceedings of the Section on Bayesian Statistical Science,", "citeRegEx": "MacEachern,? \\Q1999\\E", "shortCiteRegEx": "MacEachern", "year": 1999}, {"title": "Stick-breaking beta processes and the poisson process", "author": ["J. Paisley", "Blei D.M", "M.I. Jordan"], "venue": null, "citeRegEx": "Paisley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2012}, {"title": "Nonparametric factor analysis with beta process priors", "author": ["J. Paisley", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Paisley and Carin,? \\Q2009\\E", "shortCiteRegEx": "Paisley and Carin", "year": 2009}, {"title": "A stick-breaking construction of the beta process", "author": ["J. Paisley", "K. Zaas", "C. Woods", "G. Ginsburg", "L. Carin"], "venue": "In ICML, pp", "citeRegEx": "Paisley et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2010}, {"title": "The discrete infinite logistic normal distribution for mixed-membership modeling", "author": ["J. Paisley", "C. Wang", "D. Blei"], "venue": "In AISTATS,", "citeRegEx": "Paisley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["C. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "L\u00e9vy processes and infinitely divisible distributions", "author": ["K. Sato"], "venue": null, "citeRegEx": "Sato,? \\Q1999\\E", "shortCiteRegEx": "Sato", "year": 1999}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "Sethuraman,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman", "year": 1994}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Y.W. Teh"], "venue": "In Coling/ACL,", "citeRegEx": "Teh,? \\Q2006\\E", "shortCiteRegEx": "Teh", "year": 2006}, {"title": "Indian buffet processes with power-law behavior", "author": ["Y.W. Teh", "D. G\u00f6r\u00fcr"], "venue": "In NIPS,", "citeRegEx": "Teh and G\u00f6r\u00fcr,? \\Q2009\\E", "shortCiteRegEx": "Teh and G\u00f6r\u00fcr", "year": 2009}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "JASA, pp. 101:1566\u20131581,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Stickbreaking construction for the Indian buffet process", "author": ["Y.W. Teh", "D. G\u00f6r\u00fcr", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Nonparametric Bayesian Models for Machine Learning", "author": ["R. Thibaux"], "venue": "PhD thesis, EECS Dept.,", "citeRegEx": "Thibaux,? \\Q2008\\E", "shortCiteRegEx": "Thibaux", "year": 2008}, {"title": "Hierarchical beta processes and the Indian buffet process", "author": ["R. Thibaux", "M.I. Jordan"], "venue": "In AISTATS,", "citeRegEx": "Thibaux and Jordan,? \\Q2007\\E", "shortCiteRegEx": "Thibaux and Jordan", "year": 2007}, {"title": "Dependent Indian buffet processes", "author": ["S. Williamson", "P. Orbanz", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Williamson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Williamson et al\\.", "year": 2010}, {"title": "Non-parametric Bayesian dictionary learning for sparse image representations", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "G. Sapiro", "L. Carin"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Recently the idea of nonparametric methods has extended to feature learning and data clustering, with interest respectively in the beta-Bernoulli process (Thibaux & Jordan, 2007) and the Dirichlet process (Ferguson, 1973).", "startOffset": 205, "endOffset": 221}, {"referenceID": 18, "context": "In this paper we focus on L\u00e9vy processes (Sato, 1999), which are of increasing interest in machine learning.", "startOffset": 41, "endOffset": 53}, {"referenceID": 10, "context": "A family of L\u00e9vy processes, the pure-jump nondecreasing L\u00e9vy processes, also fit into the category of the completely random measure proposed by Kingman (Kingman, 1967).", "startOffset": 152, "endOffset": 167}, {"referenceID": 8, "context": "The beta process (Hjort, 1990) is an example of such a process, which is applied in nonparametric feature learning.", "startOffset": 17, "endOffset": 30}, {"referenceID": 22, "context": "Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al.", "startOffset": 87, "endOffset": 140}, {"referenceID": 20, "context": "Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al.", "startOffset": 87, "endOffset": 140}, {"referenceID": 1, "context": ", 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al., 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al.", "startOffset": 66, "endOffset": 85}, {"referenceID": 12, "context": ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).", "startOffset": 56, "endOffset": 117}, {"referenceID": 26, "context": ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).", "startOffset": 56, "endOffset": 117}, {"referenceID": 11, "context": ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).", "startOffset": 56, "endOffset": 117}, {"referenceID": 15, "context": "As examples of such work, (Thibaux & Jordan, 2007) and (Paisley et al., 2010) present explicit constructions for generating the beta process, (Teh et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 23, "context": ", 2010) present explicit constructions for generating the beta process, (Teh et al., 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.", "startOffset": 72, "endOffset": 90}, {"referenceID": 19, "context": ", 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.", "startOffset": 130, "endOffset": 148}, {"referenceID": 24, "context": ", 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.", "startOffset": 154, "endOffset": 169}, {"referenceID": 10, "context": "Apart from these specialized construction methods, in (Kingman, 1967) a general construction method for completely random measures is proposed, by first decomposing it into a sum of a countable number of \u03c3-finite measures, and then superposing the Poisson processes according to these sub-measures.", "startOffset": 54, "endOffset": 69}, {"referenceID": 15, "context": "\u2022 The decomposition of the beta process unifies the constructions in (Thibaux & Jordan, 2007), (Teh & G\u00f6r\u00fcr, 2009), and (with a different decomposing method) (Paisley et al., 2010), and a new generative construction for the gamma process and its variations is derived.", "startOffset": 158, "endOffset": 180}, {"referenceID": 18, "context": "L\u00e9vy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts.", "startOffset": 15, "endOffset": 27}, {"referenceID": 10, "context": "L\u00e9vy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts.", "startOffset": 59, "endOffset": 74}, {"referenceID": 18, "context": "By the L\u00e9vy-It\u00f4 decomposition (Sato, 1999), a L\u00e9vy process can be decomposed into a continuous Brownian motion with drift, and a discrete part of a pure-jump process.", "startOffset": 30, "endOffset": 42}, {"referenceID": 18, "context": "with \u03bd satisfying the integrability condition (Sato, 1999).", "startOffset": 46, "endOffset": 58}, {"referenceID": 10, "context": "According to (Kingman, 1967), \u03a6o is discrete with both random atoms and jumps.", "startOffset": 13, "endOffset": 28}, {"referenceID": 10, "context": "In (Kingman, 1967), it is noted that \u03a6o can be further split into a countable number of independent parts:", "startOffset": 3, "endOffset": 18}, {"referenceID": 8, "context": "A beta process (Hjort, 1990) is a L\u00e9vy process with beta-distributed increments; B \u223c BP(c(\u03c9), \u03bc) is a beta process if B(d\u03c9) \u223c Beta(c(\u03c9)\u03bc(d\u03c9), c(\u03c9)(1\u2212 \u03bc(d\u03c9))) (5) where \u03bc is the base measure on measure space (\u03a9,F) and a positive function c(\u03c9) the concentration function.", "startOffset": 15, "endOffset": 28}, {"referenceID": 3, "context": "The work of (Paisley & Jordan, 2012) and (Broderick et al., 2011) show that the stick-breaking construction of the beta process in (Paisley et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 15, "context": ", 2011) show that the stick-breaking construction of the beta process in (Paisley et al., 2010) is indeed a result of another way of decomposing the L\u00e9vy measure of the beta process.", "startOffset": 73, "endOffset": 95}, {"referenceID": 15, "context": "1 and make comparison with the construction of beta process in (Paisley et al., 2010).", "startOffset": 63, "endOffset": 85}, {"referenceID": 15, "context": "For the stick-breaking construction of beta process described in (Paisley et al., 2010), the L1 distance is: ( c c+1 ) .", "startOffset": 65, "endOffset": 87}, {"referenceID": 5, "context": "This metric was applied on the truncated Indian buffet process (Doshi et al., 2009) and truncated stick-breaking construction of the beta process (Paisley & Jordan, 2012), which indicates 1 4 \u222b |m\u221e(b)\u2212mK(b)|db \u2264 Pr(\u2203k > K, 1 \u2264 i \u2264 nk, 1 \u2264 m \u2264M, s.", "startOffset": 63, "endOffset": 83}, {"referenceID": 9, "context": "The gamma process can also be expressed in the form with a base measure G0 and a concentration c(\u03c9), with c = 1/\u03b8 and G0 = \u03b8\u03b1 (Jordan., 2009), to conform with other stochastic processes widely used in machine learning, such as the Dirichlet process.", "startOffset": 126, "endOffset": 141}, {"referenceID": 2, "context": "Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (\u00c7inlar, 2010).", "startOffset": 59, "endOffset": 71}, {"referenceID": 4, "context": "Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (\u00c7inlar, 2010).", "startOffset": 100, "endOffset": 114}, {"referenceID": 27, "context": "However, to briefly summarize experience with such representations, consider for example the image inpainting problem considered in (Zhou et al., 2009), based upon a beta process factor analysis model (Paisley & Carin, 2009).", "startOffset": 132, "endOffset": 151}, {"referenceID": 27, "context": "The model prioritized the first three dictionary elements as being pure colors, specifically red, green, and blue, with the important structured dictionary elements following (and no other pure-color dictionary elements, while in (Zhou et al., 2009) many \u2013 seemingly redundant \u2013 pure-color dictionary elements are inferred).", "startOffset": 230, "endOffset": 249}, {"referenceID": 27, "context": "sult given in (Zhou et al., 2009).", "startOffset": 14, "endOffset": 33}, {"referenceID": 16, "context": ", (Paisley et al., 2011)).", "startOffset": 2, "endOffset": 24}], "year": 2012, "abstractText": "We develop new representations for the L\u00e9vy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on L\u00e9vy processes, as these are of increasing importance in machine learning.", "creator": "LaTeX with hyperref package"}}}