{"id": "1206.4617", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Continuous Inverse Optimal Control with Locally Optimal Examples", "abstract": "Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.", "histories": [["v1", "Mon, 18 Jun 2012 15:02:28 GMT  (1793kb)", "http://arxiv.org/abs/1206.4617v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["sergey levine", "vladlen koltun"], "accepted": true, "id": "1206.4617"}, "pdf": {"name": "1206.4617.pdf", "metadata": {"source": "META", "title": "Continuous Inverse Optimal Control with Locally Optimal Examples", "authors": ["Sergey Levine", "Vladlen Koltun"], "emails": ["svlevine@stanford.edu", "vladlen@stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "Inverse Optimal Control (IOC) algorithms, also known as Inverse Amplification Learning (IRL), recover an unknown reward function in a Markov decision-making process (MDP) from expert demonstrations of relevant policy. This reward function can be used to conduct courses, generalize the expert's behavior toward new situations, or derive the expert's goals (Ng & Russell, 2000). Implementing the IOC in continuous, high-dimensional domains is a challenge because IOC algorithms tend to be much more accountable than the corresponding \"forward\" control methods. In this paper, we present an algorithm that efficiently handles deterministic MDPs by looking only at the form of learned reward function in the expert's neighborhood."}, {"heading": "2. Related Work", "text": "Most previous IOC methods solve the entire problem of forward development in the inner loop of an iterative process (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart, 2010). Such methods often use an arbitrary, possibly approximate forward solver, but this solver must be used several times during the learning process, making reward learning significantly more expensive than the forward problem. Dvijotham and Todorov avoid repeated calls to a forward solver by learning a value function directly (Dvijotham & Todorov, 2010). However, this requires value functional basics to impose the solution rather than the more frequent reward basics. Good value functional basics are difficult to construct and are not portable across domains."}, {"heading": "3. Background", "text": "We address deterministic, fixed horizon control tasks with continuous states x = (x1,.., xT) T, continuous actions u = (u1,.., uT) T and discrete time. Such tasks are characterized by a dynamic function F, which we define asF (xt \u2212 1, ut) = xt, and a reward function r (xt, ut). Given the initial state x0, the optimal actions are characterized by byu = arg max u (n) t (xt, ut).IOC aims to find a reward function r, under which the optimal actions match the demonstrations of the expert given by D = {(x (1) 0, u (1)))).The algorithm could also be presented with reward functions f: (xt, ut)."}, {"heading": "4. IOC with Locally Optimal Examples", "text": "To evaluate Equation 1 without calculating the partition function Z, we apply the Laplace approximation, which models the distribution locally as Gaussian (Tierney & Kadane, 1986). Note that this is not equivalent to modeling the reward function itself as Gaussian, since Equation 1 uses the sum of rewards along a path. In the context of the IOC, this corresponds to the assumption that the expert performs a local optimization when he selects the measures u rather than global planning. This assumption is strictly less restrictive than the assumption of global optimality. Using r (u) to approximate the sum of rewards along the path (x0, u), we can asp Equation 1 (u | x0) = he (u) [er (u) du (u) du (u) du (u), the probability is approached with a second magnitude Taylor expansion of r by u: r (u)."}, {"heading": "5. Efficient Likelihood Optimization", "text": "We can optimize Equation 2 directly with any optimization method, and the calculation is dominated by the linear system H \u2212 1g, so that the costs in path length T and action dimensionality are cubic. \u2212 We will therefore describe two approximate algorithms that evaluate probability in time linearly in T by linearizing dynamics. \u2212 This greatly speeds up the method with longer examples, although it should be noted that modern linear solvers are well optimized for symmetric matrices such as H, which makes it possible to evaluate probability without linearization for moderate length paths. To derive the approximate linear time resolution in H \u2212 1g, we first press g \u2212 and H separately for derivatives of r with respect to x and u block: g = r block without linearization with respect to mean length paths is quite feasible."}, {"heading": "5.1. Direct Likelihood Evaluation", "text": "We first describe a procedure for the direct evaluation of probability under the assumption that H + J = zeroed. First, we use the structure of J to evaluate Jg + zeroed in time linearly in T, which is necessary for the calculation of g. \u2212 This requires a simple recursion from t = T to 1: [Jg] t (g + z (t) g) z (t + 1) g (t + 1) g (t) g (t) g (3), where zg solves the product of g with the open diagonal elements of J. The linear system h = H \u2212 1gcan is solved with a stylistically similar recursion. First, we use the assumption that H + zeroed to the factor H = (H + JH) JD, where PJT = I. The nonzero blocks of P arePt, t = B \u2020 t Pt, t \u2212 tAt, and B \u2020 t is a pseudoinverse of potentially two linear systems."}, {"heading": "5.2. LQR-Based Likelihood Evaluation", "text": "While the approximate probability in Eq.2 does not include assumptions about the dynamics of the MDP, in Section 5.1 the algorithm linearizes the dynamics around the examples. This is consistent with the assumptions of the generally studied linear-square regulator (LQR) and proposes an alternative derivative of the algorithm as IOC in a linear-square system, with linear dynamics given by At and Bt, square reward matrices given by the diagonal blocks H and H, and linear reward vectors given by g and g. A complete derivative of the resulting algorithm is presented in Appendix B of the supplement and is similar to the MaxEnt LQR algorithm described by Ziebart (Ziebart, 2010), with an additional recursion to calculate the derivatives of the parameterized reward essians."}, {"heading": "6. Algorithms for IOC with Locally Optimal Examples", "text": "We can use the goal in Equation 2 to learn reward functions with a variety of representations, we will introduce a variant that learns reward as a linear combination of features, and a second variant that uses a Gaussian process to learn non-linear reward functions."}, {"heading": "6.1. Learning Linear Reward Functions", "text": "In the linear variant, the algorithm is provided with properties f, which depend on the state xt and the action ut. The reward is given by r (xt, ut) = \u03b8Tf (xt, ut), and the weights \u03b8 are learned. If g (k), g (k), H (k) and H (k) are allowed the gradients and Hessians of each property in relation to actions and states, the full gradients and Hessian sums of these quantities are weighted according to phenomena - e.g. g (k) k (k). The gradient of g (k) in relation to the respective property is simply g (k), and the gradients of the other matrices are given analogously. The probability gradients are then taken from Equation 4.In the evaluation of Equation 2, the log determinant of the negative Hessian property is not defined if the determinant is not positive."}, {"heading": "6.2. Learning Nonlinear Reward Functions", "text": "In the nonlinear variant of our algorithm, we represent the reward function as a Gaussian process (GP) that leads from attribute values to reward, as suggested by Levine. (Levine et al., 2011) The inputs of the Gaussian process are a series of inducing attribute points F = [f1.] T, and the noiseless outputs y at these points are learned. The location of the inducing points can be chosen in a variety of ways, but we follow Levine et al. and select the points that lie on the example paths that focus learning on the regions where the examples are most informative. In addition to the outputs y, we learn the hyperparameters that describe the GP core function (f i, f j) = \u03b2 exp (\u2212 12, k)."}, {"heading": "7. Evaluation", "text": "We evaluate our method on simulated robotic arm control, planar navigation and simulated driving. In the robotic arm task, the expert places continuous torque on each joint of an n-link planar robot arm. The reward depends on the position of the end effect. Each linkage has an angle and a speed, creating a state space of 2n dimensions. By changing the number of linkages, we can vary the dimensionality of the task. An example of a 4-link arm is shown in Figure 2. The complexity of this task makes it difficult to compare with previous work, so we also include a simple planar navigation task in which the expert takes continuous steps on a level as shown in Figure 3. Finally, we use human-made examples of a simulated driving task that shows how our method can learn complex strategies from human demonstrations in a more realistic domain. The reward function of the robotic arm navigation tasks and has optimal size."}, {"heading": "7.1. Locally Optimal Examples", "text": "To test how well each method handles locally optimal examples, we performed the navigation task with an increasing number of examples that were either global or locally optimal. As discussed above, globally optimal examples were determined by discretization, while locally optimal examples were calculated by optimizing actions from a random initialization. Each test was repeated eight times with random initial states for each example. Figure 4 shows that both variants of our algorithm converge with the right policy; the linear variant requires fewer examples because the characteristics provide a good linear basis for true reward; MaxEnt assumes global optimization and does not agree with the right policy if the examples are only locally optimal; it also suffers from discretization errors. OptVhat difficulty generalizing reward to invisible parts of government space because the value features of the reward do not impose a meaningful structure."}, {"heading": "7.2. Linear and Nonlinear Rewards", "text": "In the robotic arm task, we evaluated each method with both Gaussian grid characteristics and simple characteristics that only provide the position of the end effector and therefore do not form a linear basis for the true reward. Examples were globally optimal; the number of linkages was set to 2, resulting in a 4-dimensional state space. Only the nonlinear variant of our algorithm was able to successfully learn the reward from the simple characteristics, as shown in Figure 5. Even for the grid characteristics that form a linear basis for the reward, MaxEnt showed greater discretization errors due to the complex dynamics of this task, while OptV could not meaningfully generalize the reward due to the increased dimensionality of the task."}, {"heading": "7.3. High Dimensional Tasks", "text": "To evaluate the effect of dimensionality, we increased the number of robotic arm associations. As shown in Figure 6, the processing time of our methods scaled gracefully with the dimensionality of the task, while the quality of the reward did not noticeably deteriorate. OptV processing time increased exponentially due to the discretization of the action space. MaxEnt discretization was insoluble with more than two associations and is therefore not represented."}, {"heading": "7.4. Human Demonstrations", "text": "We evaluate how our method handles human demonstrations on a simulated driving task. Although driving policy was learned through previous IOC methods (Abbeel & Ng, 2004; Levine et al., 2011), its discrete formulation required a discrete simulator in which the agent makes simple decisions, such as selecting the lane to be changed. In contrast, our driving simulator is a fully continuous, second-order dynamic system. The actions correspond directly to the gas, pauses and steering of the simulated car, and state space includes position, orientation and linear and angular speeds. Therefore, earlier methods based on discretion cannot undo this domain. We used our nonlinear method to learn from sixteen 13-second examples of an aggressive driver cutting off other cars, an evasive driver who drives fast but retains a lot of clearance, and a tailgate that follows closely behind the other cars."}, {"heading": "8. Discussion and Future Work", "text": "We introduced an IOC algorithm designed for continuous, high-dimensional domains. Our method remains efficient in high-dimensional domains by using a local approach to the probability of the reward function. This approach also eliminates the global optimality requirement for expert demonstrations, allowing the method to learn the reward from examples that are optimal only locally. Local optimizations can be demonstrated more easily than global optimizations, especially in high-dimensional domains. As shown in our evaluation, previous methods do not match the underlying reward function if the examples are optimal locally, regardless of how many examples are provided. Since our algorithm relies on the derivatives of the reward functions to learn the reward function, we require that the characteristics are differentiable in terms of states and actions. These derivatives only need to be pre-calculated once, so it is quite practical to use finite differences when the analytical characteristics are not yet available for our derivatives."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their constructive comments. Sergey Levine was supported by NSF Graduate Research Fellowship DGE-0645962."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of ICML,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "Practical augmented lagrangian methods", "author": ["Birgin", "Ernesto G", "Mart\u0301\u0131nez", "Jos\u00e9 Mario"], "venue": "In Encyclopedia of Optimization,", "citeRegEx": "Birgin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Birgin et al\\.", "year": 2009}, {"title": "Linear Matrix Inequalities in System and Control Theory", "author": ["S. Boyd", "L. El Ghaoui", "E. Feron", "V. Balakrishnan"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 1994}, {"title": "Inverse optimal control with linearly-solvable MDPs", "author": ["Dvijotham", "Krishnamurthy", "Todorov", "Emanuel"], "venue": "In Proceedings of ICML,", "citeRegEx": "Dvijotham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dvijotham et al\\.", "year": 2010}, {"title": "Feature construction for inverse reinforcement learning", "author": ["Levine", "Sergey", "Popovi\u0107", "Zoran", "Koltun", "Vladlen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Levine et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2010}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["Levine", "Sergey", "Popovi\u0107", "Zoran", "Koltun", "Vladlen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "Andrew Y", "Russell", "Stuart J"], "venue": "In Proceedings of ICML,", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan", "Bagnell", "J. Andrew", "Zinkevich", "Martin A"], "venue": "In Proceedings of ICML,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan", "Silver", "David", "Bagnell", "J. Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Accurate approximations for posterior moments and marginal densities", "author": ["Tierney", "Luke", "Kadane", "Joseph B"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Tierney et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Tierney et al\\.", "year": 1986}, {"title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy", "author": ["Ziebart", "Brian D"], "venue": "PhD thesis,", "citeRegEx": "Ziebart and D.,? \\Q2010\\E", "shortCiteRegEx": "Ziebart and D.", "year": 2010}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew", "Bagnell", "J. Andrew", "Dey", "Anind K"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "We present two variants of our algorithm that learn the reward either as a linear combination of the provided features, as is common in prior work, or as a nonlinear function of the features, as in a number of recent methods (Ratliff et al., 2009; Levine et al., 2010; 2011).", "startOffset": 225, "endOffset": 274}, {"referenceID": 4, "context": "We present two variants of our algorithm that learn the reward either as a linear combination of the provided features, as is common in prior work, or as a nonlinear function of the features, as in a number of recent methods (Ratliff et al., 2009; Levine et al., 2010; 2011).", "startOffset": 225, "endOffset": 274}, {"referenceID": 7, "context": "Most prior IOC methods solve the entire forward control problem in the inner loop of an iterative procedure (Abbeel & Ng, 2004; Ratliff et al., 2006; Ziebart, 2010).", "startOffset": 108, "endOffset": 164}, {"referenceID": 2, "context": "More efficient IOC algorithms have been proposed for the special case of linear dynamics and quadratic rewards (LQR) (Boyd et al., 1994; Ziebart, 2010).", "startOffset": 117, "endOffset": 151}, {"referenceID": 11, "context": "\u201d We employ the maximum entropy IRL (MaxEnt) model (Ziebart et al., 2008), which is closely related to linearly-solvable MDPs (Dvijotham & Todorov, 2010).", "startOffset": 51, "endOffset": 73}, {"referenceID": 5, "context": "(Levine et al., 2011).", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "This prior is discussed in more detail in previous work (Levine et al., 2011).", "startOffset": 56, "endOffset": 77}, {"referenceID": 11, "context": "We compare the linear and nonlinear variants of our method with the MaxEnt IRL and OptV algorithms (Ziebart et al., 2008; Dvijotham & Todorov, 2010).", "startOffset": 99, "endOffset": 148}, {"referenceID": 5, "context": "Although driving policies have been learned by prior IOC methods (Abbeel & Ng, 2004; Levine et al., 2011), their discrete formulation required a discrete simulator where the agent makes simple decisions, such as choosing which lane to switch to.", "startOffset": 65, "endOffset": 105}], "year": 2012, "abstractText": "Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.", "creator": "LaTeX with hyperref package"}}}