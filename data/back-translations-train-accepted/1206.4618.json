{"id": "1206.4618", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Compact Hyperplane Hashing with Bilinear Functions", "abstract": "Hyperplane hashing aims at rapidly searching nearest points to a hyperplane, and has shown practical impact in scaling up active learning with SVMs. Unfortunately, the existing randomized methods need long hash codes to achieve reasonable search accuracy and thus suffer from reduced search speed and large memory overhead. To this end, this paper proposes a novel hyperplane hashing technique which yields compact hash codes. The key idea is the bilinear form of the proposed hash functions, which leads to higher collision probability than the existing hyperplane hash functions when using random projections. To further increase the performance, we propose a learning based framework in which the bilinear functions are directly learned from the data. This results in short yet discriminative codes, and also boosts the search performance over the random projection based solutions. Large-scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach.", "histories": [["v1", "Mon, 18 Jun 2012 15:03:10 GMT  (455kb)", "http://arxiv.org/abs/1206.4618v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["wei liu 0005", "jun wang 0006", "yadong mu", "sanjiv kumar", "shih-fu chang"], "accepted": true, "id": "1206.4618"}, "pdf": {"name": "1206.4618.pdf", "metadata": {"source": "META", "title": "Compact Hyperplane Hashing with Bilinear Functions", "authors": ["Wei Liu", "Jun Wang", "Yadong Mu", "Sanjiv Kumar", "Shih-Fu Chang"], "emails": ["wliu@ee.columbia.edu", "muyadong@ee.columbia.edu", "sfchang@ee.columbia.edu", "wangjun@us.ibm.com", "sanjivk@google.com"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Problem", "text": "First, let's repeat the well-known margin-based AL strategy proposed by (Tong & Koller, 2001). (For the convenience of expression, we append each data vector with a 1 and use a linear kernel.) Then, the SVM classifier displays the geometric relationship between w (x) = w (w) x, where the vector x (w) w (w) represents a data point and vector w (w) w (w). (Given a hyperplane query w (w) and a database with points X = xi) n i = 1, the active selection criterion prefers the most informative database point x = argminx (x, Pw), which represents the vector normal for the hyperplane Pw (w). (the minimum margin for the SVM decision level x. Note that D (x, Pw) is the hyperplane x."}, {"heading": "3. Randomized Hyperplane Hashing", "text": "In this section, we first briefly review the existing linear function-based randomized hash methods, then propose our bilateral randomized hash approach, and finally provide a theoretical analysis of the proposed bilinear hash function."}, {"heading": "3.1. Background \u2013 Linear Hash Functions", "text": "Jain et al. (Jain et al., 2010) developed two different families of randomized hash functions to attack the hyperplane hash problem, the first of which is Angle hyperplane hash (AH hash) A, one of which is ishA (z) = {[sgn (u z), sgn (v z)], z is a database point [sgn (u z), sgn (\u2212 v z)], z is a hyperplane normal (2), where z Rd represents an input vector, and u and v are both drawn independently of a standard Gaussian variant, i.e., u \u2212 N (0, Id \u00b7 d). Note that hA is a two-bit hash function that leads to the probability of a collision with a hyperplane (z-lezz), and a database point x: Pr [hA (w) = hA-hash."}, {"heading": "3.2. Bilinear Hash Functions", "text": "We propose a bilinear hash function as follows: (z) = sgn (u zz v), (6) where u, v Rd are two projection vectors. Our motivation for developing such a bilinear form stems from the following two requirements: 1) h should be invariant to the scale of z, which is motivated by the fact that z and \u03b2z (\u03b2 6 = 0) have the same point-to-ohyperplane angle; and 2) h should yield different hash bits for two perpendicularly arranged input vectors. The former is definitely true due to the bilinear formulation. We show in Lemma 1 that the latter is valid with constant probability if u, v is independent of the normal distribution. For the purpose of the hyperplane hash bits described above, the central role of bilinear hash functions is to find the query point w (the hyperplane normal) and the most desirable informative point (with dimmx = points unequal to the hash level) in terms of the hash functions."}, {"heading": "3.3. Theoretic Analysis", "text": "On the basis of the bilinear formula in eq. (6) we define a novel accidental functional family (3): (2). (2). (2). (3). (3). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5. (5). (5). (5). (5. (5). (5). (5. (5). (5). (5. (5). (5). (5). (5. (5). (5). (5). (5. (5). (5). (5). (5). (5. (5). (5). (5). (5. (5). (5). (5). (5). (5. (5). (5). (5). (5). (5). (5. (5). (5). (5). (5). (5). (5. (5). (5). (5. (5).). (5.). (5.). (5. (5.). (5.). (5. (5.). (5.). (5.). (5.). (5. (5). (5.).). (5.). (5.). (5.). (5.). (5.). (5.). (5.).). (5. (5.).). (5.). (5."}, {"heading": "4. Compact Hyperplane Hashing", "text": "Despite the higher collision probability of the proposed BH hash than AH hash and EH hash, it is still a randomized approach. The use of random projections in hB has two potential problems. (i) The probability of collision for parallel Pw and x with the retrieved database is not too high (only 1 / 2 according to Lemma 1). (ii) The hashing time is sublinear O (np) n to bind the approximation error of the retrieved neighbors, as shown in Theorem 2. AH hash and EHHash also suffer from the two problems. Although these randomized hyperplane hashing methods maintain bounded approximation errors, they require long hash codes and many (even hundreds) of hash tables for accuracy guaranteed."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "We conduct experiments with two publicly available data sets, including the 20 newsgroups text corpus and the 1.06 million subset called Tiny-1M of the 80 million tiny image collection. The first data set is version 22 of 20 newsgroups. It consists of 18,846 documents from 20 newsgroup categories. Each document is represented by a 26,214-dimensional tf-idf feature vector normalized from 2. The Tiny1M data set is a union of CIFAR-103 and one million tiny images sampled from the entire 80M image set. CIFAR-10 is a labeled subset of the 80M image set consisting of a total of 60,000 color images from 6,000 samples each. The other 1M images have no commented labels. In our experiments, we treat them as the \"other\" class that appears in CIFAR-10."}, {"heading": "5.2. Evaluations and Results", "text": "This year, it has reached the point where it is only half as much as it is half."}, {"heading": "6. Conclusions", "text": "We have dedicated ourselves to hyperplane hashing by proposing a specialized bilinear hash function that enables an efficient search of points near a hyperplane query. Even using random projections, the likelihood of a collision with the proposed hash function is higher than with existing randomized methods. By further learning the projections, we achieve compact but discriminatory codes that allow significant savings in both storage and time needed during the search. Extensive active learning experiments on two sets of data have shown the superior performance of our compact hyperplane hashing approach."}, {"heading": "Basri, R., Hassner, T., and Zelnik-Manor, L. Approximate", "text": "TPAMI, 33 (2): 266-278, 2011. Charikar, M. Similarity estimation techniques from rounding algorithms. In Proc. STOC, 2002."}, {"heading": "Datar, M., Immorlica, N., Indyk, P., and Mirrokni, V. S.", "text": "Localization-sensitive hash scheme based on p-stable distributions. In Proc. SoCG, 2004."}, {"heading": "Gionis, A., Indyk, P., and Motwani, R. Similarity search", "text": "In Proc. VLDB, 1999.Goemans, M. X. and Williamson, D. P. Improved approximation algorithms for maximum intersection and satisfaction problems through semi-defined programming. Journal of the ACM, 42 (6): 1115-1145, 1995."}, {"heading": "Jain, P., Vijayanarasimhan, S., and Grauman, K. Hashing", "text": "In NIPS 23, 2010. Li, P. and Konig, A. C. b-bit minwise hashing. In Proc. WWW, 2010."}, {"heading": "Liu, W., Wang, J., Kumar, S., and Chang, S.-F. Hashing", "text": "with graphics. In Proc. ICML, 2011."}, {"heading": "Liu, W., Wang, J., Ji, R., Jiang, Y.-G., and Chang, S.-F.", "text": "Supervised hashing with kernel. In Proc. CVPR, 2012.Nesterov, Y. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, 2003.Oliva, A. and Torralba, A. Modeling the shape of the scene: a holistic representation of the spatial hull. IJCV, 42 (3): 145-175, 2001.Tong, S. and Koller, D. Support vector machine active learning with applications to text classification. JMLR, 2: 45-66, 2001."}, {"heading": "Wang, J., Kumar, S., and Chang, S.-F. Semi-supervised", "text": "Hashing for large-scale search. TPAMI, 2012."}], "references": [{"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "In Proc. STOC,", "citeRegEx": "Charikar,? \\Q2002\\E", "shortCiteRegEx": "Charikar", "year": 2002}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "In Proc. SoCG,", "citeRegEx": "Datar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Datar et al\\.", "year": 2004}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In Proc. VLDB,", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM,", "citeRegEx": "Goemans and Williamson,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson", "year": 1995}, {"title": "Hashing hyperplane queries to near points with applications to large-scale active learning", "author": ["P. Jain", "S. Vijayanarasimhan", "K. Grauman"], "venue": "In NIPS", "citeRegEx": "Jain et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2010}, {"title": "b-bit minwise hashing", "author": ["P. Li", "A.C. K\u00f6nig"], "venue": "In Proc. WWW,", "citeRegEx": "Li and K\u00f6nig,? \\Q2010\\E", "shortCiteRegEx": "Li and K\u00f6nig", "year": 2010}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "Chang", "S.-F"], "venue": "In Proc. ICML,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Jiang", "Y.-G", "Chang", "S.-F"], "venue": "In Proc. CVPR,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Y. Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Nesterov,? \\Q2003\\E", "shortCiteRegEx": "Nesterov", "year": 2003}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "Oliva and Torralba,? \\Q2001\\E", "shortCiteRegEx": "Oliva and Torralba", "year": 2001}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "JMLR, 2:45\u201366,", "citeRegEx": "Tong and Koller,? \\Q2001\\E", "shortCiteRegEx": "Tong and Koller", "year": 2001}, {"title": "Semi-supervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "Chang", "S.-F"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "rithms from the Locality-Sensitive Hashing (LSH) family (Gionis et al., 1999)(Charikar, 2002)(Datar et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 0, "context": ", 1999)(Charikar, 2002)(Datar et al.", "startOffset": 7, "endOffset": 23}, {"referenceID": 1, "context": ", 1999)(Charikar, 2002)(Datar et al., 2004) which use random projections to convert input data into binary hash codes.", "startOffset": 23, "endOffset": 43}, {"referenceID": 6, "context": "The state-of-thearts include unsupervised hashing (Liu et al., 2011), semi-supervised hashing (Wang et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 11, "context": ", 2011), semi-supervised hashing (Wang et al., 2012), and supervised hashing (Liu et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 7, "context": ", 2012), and supervised hashing (Liu et al., 2012).", "startOffset": 32, "endOffset": 50}, {"referenceID": 4, "context": "In the literature, not much work has been done on the point-to-hyperplane problem except (Jain et al., 2010) which demonstrated the vital importance of such a problem in making SVM-based active learning feasible on massive data pools.", "startOffset": 89, "endOffset": 108}, {"referenceID": 4, "context": "Recently, hyperplane hashing schemes were proposed in (Jain et al., 2010) to cope with point-to-hyperplane search.", "startOffset": 54, "endOffset": 73}, {"referenceID": 4, "context": "In (Jain et al., 2010), two families of randomized hash functions were proved locality-sensitive to the angle between a database point and a hyperplane query; however, long hash bits and plentiful hash tables are required to cater for the theoretical guarantees.", "startOffset": 3, "endOffset": 22}, {"referenceID": 4, "context": "Actually, 300 bits and 500 tables were adopted in (Jain et al., 2010) to achieve reasonable performance, which incurs a heavy burden on both computation and storage.", "startOffset": 50, "endOffset": 69}, {"referenceID": 4, "context": "To derive provable hyperplane hashing like (Jain et al., 2010), this paper focuses on a slightly modified \u201cdistance\u201d |w \u22a4 x| \u2016w\u2016\u2016x\u2016 which is the sine of the point-to-hyperplane angle", "startOffset": 43, "endOffset": 62}, {"referenceID": 4, "context": "(Jain et al., 2010) devised two distinct families of randomized hash functions to attack the hyperplane hashing problem.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "We present the following theorem by adapting Theorem 1 in (Gionis et al., 1999) and Theorem 0.", "startOffset": 58, "endOffset": 79}, {"referenceID": 4, "context": "1 in the supplementary material of (Jain et al., 2010).", "startOffset": 35, "endOffset": 54}, {"referenceID": 8, "context": "For fast convergence, we adopt a pair of random projections (uj ,v 0 j ), which were used in hj , as a warm start and apply Nesterov\u2019s gradient method (Nesterov, 2003) to accelerate the gradient decent procedure.", "startOffset": 151, "endOffset": 167}, {"referenceID": 4, "context": "We compare four hashing methods including two randomized linear hashing schemes AH-Hash and EHHash (Jain et al., 2010), the proposed randomized bilinear hashing scheme BH-Hash, and the proposed learning-based bilinear hashing scheme that we call LBH-Hash.", "startOffset": 99, "endOffset": 118}, {"referenceID": 4, "context": "We also follow the dimension-sampling trick in (Jain et al., 2010) to accelerate EH-Hash\u2019s computation.", "startOffset": 47, "endOffset": 66}], "year": 2012, "abstractText": "Hyperplane hashing aims at rapidly searching nearest points to a hyperplane, and has shown practical impact in scaling up active learning with SVMs. Unfortunately, the existing randomized methods need long hash codes to achieve reasonable search accuracy and thus suffer from reduced search speed and large memory overhead. To this end, this paper proposes a novel hyperplane hashing technique which yields compact hash codes. The key idea is the bilinear form of the proposed hash functions, which leads to higher collision probability than the existing hyperplane hash functions when using random projections. To further increase the performance, we propose a learning based framework in which the bilinear functions are directly learned from the data. This results in short yet discriminative codes, and also boosts the search performance over the random projection based solutions. Large-scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach.", "creator": "LaTeX with hyperref package"}}}