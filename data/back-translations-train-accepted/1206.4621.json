{"id": "1206.4621", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Path Integral Policy Improvement with Covariance Matrix Adaptation", "abstract": "There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family - Cross-Entropy Methods and CMAES - at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for \"Path Integral Policy Improvement with Covariance Matrix Adaptation\". PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically.", "histories": [["v1", "Mon, 18 Jun 2012 15:05:32 GMT  (883kb)", "http://arxiv.org/abs/1206.4621v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["freek stulp", "olivier sigaud"], "accepted": true, "id": "1206.4621"}, "pdf": {"name": "1206.4621.pdf", "metadata": {"source": "META", "title": "Path Integral Policy Improvement with Covariance Matrix Adaptation", "authors": ["Freek Stulp", "Olivier Sigaud"], "emails": ["freek.stulp@ensta-paristech.fr", "olivier.sigaud@upmc.fr"], "sections": [{"heading": "1. Introduction", "text": "Most advances in the domain come from direct policy search methods based on the development of strategies. Copyright (s) on the first principles of stochastic control (PI2) is able to establish itself in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Copyright (s) on the first principles of stochastic control, and it is possible to surpass the gradient-based RL algorithms."}, {"heading": "2. Background and Related Work", "text": "We will now describe the CEM, CMAES and PI2 algorithms and their application to policy improvements."}, {"heading": "2.1. Cross-Entropy Method (CEM)", "text": "Considering an n-dimensional parameter vector of the CEM, Figure 1 becomes a cost function J: Rn 7 \u2192 R, the Cross-Entropy Method (CEM) for optimization searches for the global minimum with the following steps: Sample - Take K samples \u03b8k = 1... K from a distribution. Sort - Sort the samples in ascending order with respect to the evaluation of the cost function J (KEM). Note: The application parameters of the distribution parameters are recalculated based only on the first Ke \"Elite\" samples in the sorted list. Iterate - Return to the first step with the new distribution, up to convergence, or up to a certain number of iterations. A commonly used distribution is a multi-variable Gaussian distribution N (mean) and we can implement this (covariance matrix) so that these three steps are as in (1) - (5)."}, {"heading": "2.2. Covariance Matrix Adaptation - Evolution Strategy", "text": "The covariance matrix adaptation - evolution strategy (Hansen & Ostermeier, 2001) algorithm is very similar to CEM, but uses a more complex method to update the covariance matrix, as shown in Table 2.There are three differences to CEM: \u2022 The probabilities in CMAES do not have to be Pk = 1 / Ke as for CEM, but can be selected by the user as long as the constraints of Ke k = 1 Pk = 1 and P1 \u2265 \u00b7 PKe are met. Here, we use the default setting proposed by Hansen & Ostermeier (2001), i.e. Pk = ln (K + 1) \u2212 ln (K) \u2212 ln (k) \u2212 time methods performed by a distribution N, \u03c32\u0445PKe), i.e. the covariance matrix of the normal distribution is multiplied by a scalar step size."}, {"heading": "2.3. Policy Improvement with Path Integrals", "text": "A current trend in distribution policy is the use of parameterized strategies in combination with probability-weighted averaging; the PI2 algorithms are a current example of this approach. The use of parameterized strategies avoids the curse of dimensionality associated with (discrete) governmental spheres of action, and the use of probability weighting avoids having to estimate a gradient that can be difficult for noisy and discontinuous cost functions. PI2 algorithms are derived from the first principles of optimal control and take their name from the application of the Feynman-Kac problem to integrally transform the HamiltonJacobi-Bellman equations into a path that can be approached using Monte Carlo methods. PI2 algorithms are listed in Table 1."}, {"heading": "3.1. Evaluation Task", "text": "For evaluation purposes, we use a Viapoint task with a 10-DOF arm, the task is visualized and described in Figure 2. This Viapoint task originates from (Theodorou et al., 2010), where it is used to compare PI2 with PoWeR (Kober & Peters, 2011), NAC (Peters & Schaal, 2008) and REINFORCE (Williams, 1992).The goal of this task is expressed with the cost function in (24), where a represents the joint angles, x and y the coordinates of the end effector, and D = 10 the number of DOFs.The weighting term (D + 1 \u2212 d) penalizes DOFs closer to the origin, the underlying motivation being that wrist movements are less costly than shoulder movements for humans, cf. (Theodorou et al., 2010).J (\u03c4 ti) = 3 \u00b7 (xt \u2212 0.5) + yt \u2212 0.5 + + + 1 \u00b7 D = 1 \u00b7 D = 1 (1 \u00b7 D = 1 \u00b7 D = 1 \u00b7 D = 10D = 1 \u00b7 D = 1 \u00b7 D."}, {"heading": "3.2. Exploration Noise", "text": "A first difference between CEM / CMAES and PI2 is the way exploration noise is generated. In CEM and CMAES, time does not matter, so only one exploration vector \u03b8k is generated per experiment. In the stochastic optimal control from which PI2 is derived, \u03b8i represents a motor command at time i, and the stochasticity \u03b8i + i is caused by executing a command in the environment. When applying PI2 to DMPs, this stochasticity is more of a controlled noise to promote exploration, which the algorithm scans at the beginning of an experiment (\u03b8, \u03a3). Since this exploration noise is under our control, we do not need to vary it in every time step. In the work of Theodorou et al. (2010), for example, only one exploration vector \u0445k is generated at the beginning of an experiment, and the exploration is applied only to the basic DMP function, which has the highest activation."}, {"heading": "3.3. Definition of Eliteness", "text": ": they are either elite (Pk = 1 / Ke) or not (Pk = 0). PI2 considers elite ability rather as a continuous value inversely proportional to the costs of a trajectory. Probabilities in CMAES do not have to be Pk = 1 / Ke, as in CEM, but can be chosen by the user as long as the constraints Ke k = 1 Pk = 1 and P1 \u2265 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 These probabilities in CMAES do not have to be Pk = 1 / Ke, as in CEM, but can be chosen by the user as long as the constraints Ke = 1 Pk = 1 and P1 \u2265 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 PKe are met."}, {"heading": "3.4. Covariance Matrix Adaptation", "text": "We now turn to the most interesting and relevant difference between the algorithms. In CEM / CMAES, both averages and covariance of the distribution are updated, whereas PI2 only updates the mean. This is because in PI2 the form of the covariance matrix is limited by the relation \u03a3 = \u03bbR \u2212 1, where R is the (fixed) command cost matrix, and it is a parameter that is inversely proportional to the parameter h. This constraint is necessary to perform the derivation of PI2 (Theodorou et al., 2010).In this paper, we choose to ignore the constraints and apply the covariance matrix that we are updating to PI2. Since a covariance matrix update is calculated for each time step i (17), we must perform temporary averaging for the covariance matrix (19), just as we do for the mean matrix."}, {"heading": "4. Conclusion", "text": "In this paper, we examined the current state of the art of the Direct Policy Improvement Algorithm PI2 from the specific perspective of belonging to a family of methods based on the concept of probability-weighted averaging. We discussed similarities and differences between three algorithms in this family, namely PI2, CMAES and CEM. In particular, we demonstrated that the use of a probability-weighted averaging to update the covariance matrix, as performed in CEM and CMAES, enables PI2 to autonomously adjust the exploration size. The resulting PI2-CMA algorithm shows more consistent convergence under different starting conditions and facilitates the user's manual adjustment of exploration size parameters. Currently, we apply PI2-CMA to demanding tasks on a physically humanoid robot. Given the ability of PI2 to learn complex, high-dimensional tasks on real robots (Stal, Ip 2, IMA), we are convinced that PI2 can also be applied successfully."}, {"heading": "Acknowledgments", "text": "We thank the reviewers for their constructive suggestions for improvement. This work is supported by the French ANR program (ANR 2010 BLAN 0216 01), more at http: / / macsi.isir.upmc.fr"}], "references": [{"title": "Cross-entropy optimization of control policies with adaptive basis functions", "author": ["L. Busoniu", "D. Ernst", "Schutter", "B. De", "R. Babuska"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Busoniu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2011}, {"title": "Convex Optimization & Euclidean Distance Geometry", "author": ["J. Dattorro"], "venue": "Meboo Publishing USA,", "citeRegEx": "Dattorro,? \\Q2011\\E", "shortCiteRegEx": "Dattorro", "year": 2011}, {"title": "Completely derandomized self-adaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary Computation,", "citeRegEx": "Hansen and Ostermeier,? \\Q2001\\E", "shortCiteRegEx": "Hansen and Ostermeier", "year": 2001}, {"title": "Evolution strategies for direct policy search", "author": ["Heidrich-Meisnerm V", "C. Igel"], "venue": "In Proc. of the Int\u2019l Conference on Parallel Problem Solving from Nature,", "citeRegEx": "V. and Igel,? \\Q2008\\E", "shortCiteRegEx": "V. and Igel", "year": 2008}, {"title": "Movement imitation with nonlinear dynamical systems in humanoid robots", "author": ["A.J. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "In Proc. of the IEEE Int\u2019l Conference on Robotics and Automation (ICRA),", "citeRegEx": "Ijspeert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ijspeert et al\\.", "year": 2002}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "Machine Learning,", "citeRegEx": "Kober and Peters,? \\Q2011\\E", "shortCiteRegEx": "Kober and Peters", "year": 2011}, {"title": "Cross-entropy randomized motion planning", "author": ["M. Kobilarov"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "Kobilarov,? \\Q2011\\E", "shortCiteRegEx": "Kobilarov", "year": 2011}, {"title": "The CrossEntropy Method for fast policy search", "author": ["S. Mannor", "R.Y. Rubinstein", "Y. Gat"], "venue": "In Proceedings of the Int\u2019l Conference on Machine Learning,", "citeRegEx": "Mannor et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2003}, {"title": "Learning cost-efficient control policies with XCSF: Generalization capabilities and further improvement", "author": ["D. Marin", "J. Decock", "L. Rigoux", "O. Sigaud"], "venue": "In Proc. of Genetic and evolutionary computation,", "citeRegEx": "Marin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Marin et al\\.", "year": 2011}, {"title": "Exploring parameter space in reinforcement learning. Paladyn", "author": ["T. R\u00fcckstiess", "F. Sehnke", "T. Schaul", "D. Wierstra", "Y. Sun", "Schmidhuber"], "venue": "Journal of Behavioral Robotics,", "citeRegEx": "R\u00fcckstiess et al\\.,? \\Q2010\\E", "shortCiteRegEx": "R\u00fcckstiess et al\\.", "year": 2010}, {"title": "Learning to grasp under uncertainty", "author": ["F. Stulp", "E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "In Proceedings of the Int\u2019l Conference on Robotics and Automation,", "citeRegEx": "Stulp et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Stulp et al\\.", "year": 2011}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "Schaal"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Theodorou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Theodorou et al\\.", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}], "referenceMentions": [{"referenceID": 11, "context": "Scaling reinforcement learning (RL) methods to continuous state-action problems, such as humanoid robotics tasks, has been the focus of numerous recent studies (Kober & Peters, 2011; Theodorou et al., 2010).", "startOffset": 160, "endOffset": 206}, {"referenceID": 12, "context": "from first principles of stochastic optimal control, and is able to outperform gradient-based RL algorithms such as REINFORCE (Williams, 1992) and Natural Actor-Critic (Peters & Schaal, 2008) by an order of magnitude in terms of convergence speed and quality of the final solution (Theodorou et al.", "startOffset": 126, "endOffset": 142}, {"referenceID": 11, "context": "from first principles of stochastic optimal control, and is able to outperform gradient-based RL algorithms such as REINFORCE (Williams, 1992) and Natural Actor-Critic (Peters & Schaal, 2008) by an order of magnitude in terms of convergence speed and quality of the final solution (Theodorou et al., 2010).", "startOffset": 281, "endOffset": 305}, {"referenceID": 6, "context": "CEM has also been used in combination with sampling-based motion planning (Kobilarov, 2011).", "startOffset": 74, "endOffset": 91}, {"referenceID": 8, "context": "In (Marin et al., 2011), a CEM is extended to optimize a controller that generates trajectories to any point of the reachable space of the system.", "startOffset": 3, "endOffset": 23}, {"referenceID": 0, "context": "Busoniu et al. (2011) extend this work, and use CEM to learn a mapping from continuous states to discrete actions, where the centers and widths of the basis functions are automatically adapted.", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "R\u00fcckstiess et al. (2010) use Natural Evolution Strategies (NES), which has comparable results with CMAES, to di-", "startOffset": 0, "endOffset": 25}, {"referenceID": 12, "context": "The results above are compared with various gradient-based methods, such as REINFORCE (Williams, 1992) and NAC (Peters & Schaal, 2008).", "startOffset": 86, "endOffset": 102}, {"referenceID": 11, "context": "Also, we use Dynamic Movement Primitives as the underlying policy representation, which 1) enables us to scale to higher-dimensional problems, as demonstrated by (Theodorou et al., 2010); 2) requires us to perform temporal averaging, cf.", "startOffset": 162, "endOffset": 186}, {"referenceID": 11, "context": "PI is derived from first principles of optimal control, and gets its name from the application of the Feynman-Kac lemma to transform the HamiltonJacobi-Bellman equations into a so-called path integral, which can be approximated with Monte Carlo methods (Theodorou et al., 2010).", "startOffset": 253, "endOffset": 277}, {"referenceID": 4, "context": "So far, PI has mainly been applied to policies represented as Dynamic Movement Primitives (DMPs) (Ijspeert et al., 2002), where \u03b8 determines the shape of the movement.", "startOffset": 97, "endOffset": 120}, {"referenceID": 11, "context": "(Theodorou et al., 2010).", "startOffset": 0, "endOffset": 24}, {"referenceID": 11, "context": "When a cost function is compatible with both PoWeR and PI, they perform essentially identical (Theodorou et al., 2010).", "startOffset": 94, "endOffset": 118}, {"referenceID": 0, "context": "Whereas in other works the motivation for using CEM/CMAES for policy improvement is based on its empirical performance (Busoniu et al., 2011; HeidrichMeisner and Igel, 2008; R\u00fcckstiess et al., 2010) (e.", "startOffset": 119, "endOffset": 198}, {"referenceID": 9, "context": "Whereas in other works the motivation for using CEM/CMAES for policy improvement is based on its empirical performance (Busoniu et al., 2011; HeidrichMeisner and Igel, 2008; R\u00fcckstiess et al., 2010) (e.", "startOffset": 119, "endOffset": 198}, {"referenceID": 11, "context": "it is shown to outperform a particular gradientbased method), the PI derivation (Theodorou et al., 2010) demonstrates that there is a theoretically sound motivation for using methods based on probabilityweighted averaging, as this principle follows directly from first principles of stochastic optimal control.", "startOffset": 80, "endOffset": 104}, {"referenceID": 0, "context": "Note that any differences between PI and CEM/CMAESin general also apply to the specific application of CEM/CMAES to policy improvement, as done for instance by Busoniu et al. (2011) or Heidrich-Meisner and Igel (2008).", "startOffset": 160, "endOffset": 182}, {"referenceID": 0, "context": "Note that any differences between PI and CEM/CMAESin general also apply to the specific application of CEM/CMAES to policy improvement, as done for instance by Busoniu et al. (2011) or Heidrich-Meisner and Igel (2008). Before comparing the algorithms, we first present the evaluation task used in the paper.", "startOffset": 160, "endOffset": 218}, {"referenceID": 11, "context": "This viapoint task is taken from (Theodorou et al., 2010), where it is used to compare PI with PoWeR (Kober & Peters, 2011), NAC (Peters & Schaal, 2008), and REINFORCE (Williams, 1992).", "startOffset": 33, "endOffset": 57}, {"referenceID": 12, "context": ", 2010), where it is used to compare PI with PoWeR (Kober & Peters, 2011), NAC (Peters & Schaal, 2008), and REINFORCE (Williams, 1992).", "startOffset": 118, "endOffset": 134}, {"referenceID": 11, "context": "(Theodorou et al., 2010).", "startOffset": 0, "endOffset": 24}, {"referenceID": 11, "context": "In the work by Theodorou et al. (2010) for instance, only one exploration vector \u03b8k is generated at the beginning of a trial, and exploration is only applied to the DMP basis function that has the highest activation.", "startOffset": 15, "endOffset": 39}, {"referenceID": 11, "context": "Because choosing the weights is uncritical, we use the PI weighting scheme with h = 10, the default suggested by Theodorou et al. (2010), throughout the rest of this paper.", "startOffset": 113, "endOffset": 137}, {"referenceID": 11, "context": "This constraint is necessary to perform the derivation of PI (Theodorou et al., 2010).", "startOffset": 61, "endOffset": 85}, {"referenceID": 1, "context": "Temporal averaging over covariance matrices is possible, because 1) every positive-semidefinite matrix is a covariance matrix and vice versa 2) a weighted averaging over positivesemidefinite matrices yields a positive-semidefinite matrix (Dattorro, 2011).", "startOffset": 238, "endOffset": 254}, {"referenceID": 6, "context": "After each update, a small amount of base level exploration noise is added to the covariance matrix (\u03a3new \u2190 \u03a3new + 10I5) to avoid premature convergence, as suggested by Kobilarov (2011).", "startOffset": 169, "endOffset": 186}, {"referenceID": 10, "context": "Given the ability of PI to learn complex, high-dimensional tasks on real robots (Stulp et al., 2011), we are confident that PI-CMA can also successfully be applied to such tasks.", "startOffset": 80, "endOffset": 100}], "year": 2012, "abstractText": "There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI to other members of the same family \u2013 Cross-Entropy Methods and CMAES \u2013 at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI-CMA for \u201cPath Integral Policy Improvement with Covariance Matrix Adaptation\u201d. PI-CMA\u2019s main advantage is that it determines the magnitude of the exploration noise automatically.", "creator": "LaTeX with hyperref package"}}}