{"id": "1206.4622", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training", "abstract": "Item neighbourhood methods for collaborative filtering learn a weighted graph over the set of items, where each item is connected to those it is most similar to. The prediction of a user's rating on an item is then given by that rating of neighbouring items, weighted by their similarity. This paper presents a new neighbourhood approach which we call item fields, whereby an undirected graphical model is formed over the item graph. The resulting prediction rule is a simple generalization of the classical approaches, which takes into account non-local information in the graph, allowing its best results to be obtained when using drastically fewer edges than other neighbourhood approaches. A fast approximate maximum entropy training method based on the Bethe approximation is presented, which uses a simple gradient ascent procedure. When using precomputed sufficient statistics on the Movielens datasets, our method is faster than maximum likelihood approaches by two orders of magnitude.", "histories": [["v1", "Mon, 18 Jun 2012 15:05:52 GMT  (397kb)", "http://arxiv.org/abs/1206.4622v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.IR stat.ML", "authors": ["aaron defazio", "tib\u00e9rio s caetano"], "accepted": true, "id": "1206.4622"}, "pdf": {"name": "1206.4622.pdf", "metadata": {"source": "META", "title": "A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training", "authors": ["Aaron J. Defazio", "Tib\u00e9rio S. Caetano"], "emails": ["aaron.defazio@anu.edu.au", "tiberio.caetano@nicta.com.au"], "sections": [{"heading": "1. Introduction", "text": "The large scale and variability of the data has meant that traditional approaches were not applicable, especially those with square or cubic runtime.This has resulted in the majority of research taking two tracks: (1) latency factor models and (2) neighborhood models. Latency factor models embed both users, which appear in the proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, 2012. Copyright 2012 by the author (s).and objects in a low dimensional space from which predictions can be calculated using linear operations, including continuous approaches such as approximate matrix factorization (radio, 2006) and binary variable approaches, such as limited Boltzmann machines (Salakhutdinov et al., 2007).The second track, neighborhood models, is explored in this work."}, {"heading": "2. The Item Graph", "text": "We start with the introduction of the basics of our model. We get a set of users and items, along with a set of real reviews of items by users. The classic items neighborhood methods (Sarwar et al., 2001) learn a graph structure about items i = 0... N \u2212 1, along with a set of edge weights selected by users, so that when a query user u is presented, along with its ratings for the neighbors of the item i (ruj, j), the predicted rating of the item i isrui + \u2211 j (i) sji (ruj \u2212 \u00b5j), the assessment of items i (i) sji, in which the average ratings of this item are represented over all users. See Figure 1 for an example of an actual neighborhood for a movie referral system. To apply the above method, some principles or learning algorithms are needed to choose the neighborhood weights."}, {"heading": "3. The Item Field Model", "text": "In our case, the set of variables is simply the set of items whose values we treat as continuous variables in the range of 1 to 5. The most common characteristics that are used are simple tuples of variables (i, j), which can be equated with edges in the remaining items (RU). Most characteristics that are used are simple tuples of variables (i, j), which can be equated with edges in the graphical models, in our case the item graph. In addition, we will limit ourselves to the class of log-linear models that allow us to write the general form of distributions asP (r)."}, {"heading": "4. Prediction rule", "text": "The characteristic functions defined in the previous paragraph seem somewhat arbitrary at first. We will now show that they are additionally motivated by a simple association with existing neighbourhood methods. Consider the case of predicting a rating rui, in which user u knows all the rating ruj, j ne (i). These neighbours form the Markov totality of the node i. In this case, the conditional distribution is within the framework of the item field model: N rui; \u00b5i | \u2212 i, 1 \u03c32 = [i).ji, in which \u00b5i = \u00b5i \u2212 j (i).ji (ruj \u2212 j).j (i).j \u2212 ji is a univariate Gaussian distribution, the mean of which is given by a weighted sum of the same form as with traditional neighbourhood methods. In practice, we rarely have assessment information for the entire neighbourhood of each point, so this special case serves only to illustrate the linkage with existing predictions."}, {"heading": "5. Proposed Training Algorithm: Approximate Maximum Entropy Learning", "text": "The traditional way to learn an uncontrolled graphical model is with maximum probability. If exact conclusions are used, this is tantamount to maximum entropy training (Koller & Friedman, 2009), since the uncontrolled dual system of the others is no longer necessarily applied equally. In this case, the uncontrolled maximum entropy approach has been demonstrated to learn superior models in some cases (Granapathi et al, 2008). In the case of the item model, we will find that approximate maximum entropy learning is much faster, with comparable accuracy. We will first consider the case of a Gaussian model, with the variance characteristics described in Section 2, which is our limited Gaussian model based on an approximate maximum entropy approach."}, {"heading": "5.1. Missing Data & Kernel Functions", "text": "The proposed training methods use as input a sparse subset of a covariance matrix \u03a3, which contains the sufficient statistics required for the training. It should be noted that we do not assume that the covariance matrix is sparse, but that our training method only needs to query the entries in the subareas where the precision matrix is not assumed to be zero. As our samples are incomplete (we do not know all item ratings for all users), the true covariance matrix is unknown. For our purposes, we form a covariance matrix, assuming that the unevaluated items are evaluated by their item mean. More sophisticated methods of imputation are possible; we examined an ExpectationMaximization (EM) approach, which did not result in a significant improvement in the predictions made, but enabled better prediction covariances.Generally, instead of the covariance matrix, the core matrix could be used to enable the introduction of a metric."}, {"heading": "5.2. Conditional Random Field Variants", "text": "Many recent work in the field of cooperative filtering has concerned the handling of additional user metadata, such as age and gender information, which is normally collected by online systems (Stern et al., 2009). These attributes are inherently discrete, and so their integration as part of the MRF model leads to mixed discrete / continuous models. Approximate conclusions in such a model are no longer a simple linear algebra problem, and convergence becomes a problem. User attributes are better treated in a conditional random field (CRF) model, where conditional distributions relate exclusively to continuous item variables. Unfortunately, the optimization technique described above does not easily extend to CRF models. Approximate maximum entropy training using Difference-of-Convex methods has been successfully applied to CRF training courses (Granapathi et al., 2008), although such methods are slower than the maximum probability of extending the number of CRF calls, and the greater probability of extending the number of CRF calls during several hundred years of study."}, {"heading": "5.3. Maximum Likelihood Learning with Belief Propagation", "text": "An objective proportionality to the negative log probability for a Gaussian distribution according to the Bethe approximation can be derived from the Lagrangian approximation entropy (Eq.7) using the theory of duality. First, it should be noted that the Lagrangian method can be divided as follows: The dual method is then formed by maximizing in relation to C: The Bethe method is free energy (Yedidia et al., 2000)."}, {"heading": "6. Experiments", "text": "For our comparison, we tested two representative datasets."}, {"heading": "7. Related Work", "text": "Salakhutdinov et al. (2007) used a two-part graphical model, with binary hidden variables forming a part. This is essentially a latent factor model and, due to the hidden variables, requires different and less efficient training methods than the ones we use in this publication.In contrast to the sparse, non-two-part model we use. Multi-scale conditional random field models have also been applied to the more general social recommendation task with some success (Xin et al., 2009).Directed graphical models are often used as a modeling tool, as in Salakhutdinov & Mnih (2008).While undirected models can be applied in a similar way, the diagram structures we use in this thesis are far less rigid structure.Several papers suggest methods to learn the weights of a neighborhood chart (Koren, 2010) (Bell & Koren)."}, {"heading": "8. Conclusion", "text": "We have presented an undirected graphical model for collaborative filtering, which by its very nature generalizes the prediction rule of previous neighborhood methods by providing distributions of predictions instead of point estimates. We have detailed an efficient training algorithm based on the approximate principle of maximum entropy, which takes less than a second after pre-processing and is two orders of magnitude faster than a maximum probability approach. Our model has fewer parameters than other comparable models, which is beneficial for interpretability and training."}, {"heading": "Acknowledgements", "text": "NICTA is funded by the Australian Government, represented by the Department of Broadband, Communications and the Digital Economy, and the Australian Research Council through the ICT Centre of Excellence programme."}], "references": [{"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["Bell", "Robert", "Koren", "Yehuda"], "venue": "In ICDM,", "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "Nonlinear programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas", "year": 1995}, {"title": "Properties of bethe free energies and message passing in gaussian models", "author": ["Cseke", "Botond", "Heskes", "Tom"], "venue": null, "citeRegEx": "Cseke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cseke et al\\.", "year": 2011}, {"title": "Projected subgradient methods for learning sparse gaussians", "author": ["Duchi", "John", "Gould", "Stephen", "Koller", "Daphne"], "venue": "In UAI,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Constrained approximate maximum entropy learning of markov random fields", "author": ["Funk", "Simon"], "venue": "Try this at home,", "citeRegEx": "Funk and Simon.,? \\Q2006\\E", "shortCiteRegEx": "Funk and Simon.", "year": 2006}, {"title": "A constrained spreading activation approach to collaborative filtering", "author": ["Griffith", "Josephine", "ORiordan", "Colm", "Sorensen", "Humphrey"], "venue": "In KES,", "citeRegEx": "Griffith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Griffith et al\\.", "year": 2006}, {"title": "Applying associative retrieval techniques to alleviate the sparsity problem in collaborative filtering", "author": ["Huang", "Zan", "Chen", "Hsinchun", "Zeng", "Daniel"], "venue": "ACM TOIS,", "citeRegEx": "Huang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2004}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Factor in the neighbors: Scalable and accurate collaborative filtering", "author": ["Koren", "Yehuda"], "venue": "TKDD,", "citeRegEx": "Koren and Yehuda.,? \\Q2010\\E", "shortCiteRegEx": "Koren and Yehuda.", "year": 2010}, {"title": "Improved collaborative filtering via information transformation", "author": ["Lie", "Jian-Guo", "Wang", "Bing-Hong"], "venue": "International Journal of Modern Physics C,", "citeRegEx": "Lie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lie et al\\.", "year": 2009}, {"title": "Bayesian probabilistic matrix factorization using mcmc", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["Sarwar", "Badrul", "Karypis", "George", "Konstan", "Joseph", "Riedl", "John"], "venue": "In WWW10,", "citeRegEx": "Sarwar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sarwar et al\\.", "year": 2001}, {"title": "Gaussian belief propagation solver for systems of linear equations", "author": ["Shental", "Ori", "Bickson", "Danny", "Siegel", "Paul H", "Wolf", "Jack K", "Dolev"], "venue": null, "citeRegEx": "Shental et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shental et al\\.", "year": 2008}, {"title": "Matchbox: large scale online bayesian recommendations", "author": ["Stern", "David", "Herbrich", "Ralf", "Graepel", "Thore"], "venue": "In WWW,", "citeRegEx": "Stern et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2009}, {"title": "Correctness of belief propagation in gaussian graphical models of arbitrary topology", "author": ["Weiss", "Yair", "Freeman", "William T"], "venue": "Neural Computation,", "citeRegEx": "Weiss et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2001}, {"title": "A social recommendation framework based on multiscale continuous conditional random fields", "author": ["Xin", "King", "Irwin", "Deng", "Hongbo", "Lyu", "Michael R"], "venue": "In CIKM,", "citeRegEx": "Xin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xin et al\\.", "year": 2009}, {"title": "Bethe free energy, kikuchi approximations and belief propagation algorithms", "author": ["Yedidia", "Jonathan S", "Freeman", "William T", "Weiss", "Yair"], "venue": "Technical report, Mitsubishi electric research,", "citeRegEx": "Yedidia et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2000}, {"title": "L-bfgs-b: Algorithm 778: Fortran routines for large scale bound constrained optimization", "author": ["C. Zhu", "H.Byrd", "Richard", "Lu", "Peihuang", "Nocedal", "Jorge"], "venue": "ACM TOMS,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 11, "context": "These include continuous approaches, such as low-rank approximate matrix factorization (Funk, 2006) and binary variable approaches, such as restricted Boltzmann machines (Salakhutdinov et al., 2007).", "startOffset": 170, "endOffset": 198}, {"referenceID": 12, "context": "Rating predictions are performed under the assumption that users rate similar items similarly (for an item graph) or that similar users have similar preferences (for a user graph) using some form of weighted average (Sarwar et al., 2001).", "startOffset": 216, "endOffset": 237}, {"referenceID": 12, "context": "Classical item neighbourhood methods (Sarwar et al., 2001) learn a graph structure over items i = 0 .", "startOffset": 37, "endOffset": 58}, {"referenceID": 3, "context": "Structure learning is in principle possible in our model, using variants of recently proposed methods for covariance selection (Duchi et al., 2008).", "startOffset": 127, "endOffset": 147}, {"referenceID": 13, "context": "If the prediction variances are required, both the variances and the expected ratings can be computed using belief propagation, which often requires fewer iterations than the sparse solve operation (Shental et al., 2008).", "startOffset": 198, "endOffset": 220}, {"referenceID": 6, "context": "Such transitive diffusion has been explored previously for collaborative filtering, in a more ad-hoc fashion (Huang et al., 2004).", "startOffset": 109, "endOffset": 129}, {"referenceID": 14, "context": "Much recent work in Collaborative filtering has concerned the handling of additional user meta-data, such as age and gender information usually collected by online systems (Stern et al., 2009).", "startOffset": 172, "endOffset": 192}, {"referenceID": 17, "context": "The term inside of the minimization on the right is the Bethe free energy (Yedidia et al., 2000).", "startOffset": 74, "endOffset": 96}, {"referenceID": 18, "context": "We used the L-BFGS-B algorithm (Zhu et al., 1997) \u2013 a quasi-newton method that supports such constraints.", "startOffset": 31, "endOffset": 49}, {"referenceID": 14, "context": "As there is no standard test/training data split for this dataset, we took the approach from Stern et al. (2009), where all data for 90% of the users is used for training, and the remaining users have their ratings split into a 75% training set and 25% test set.", "startOffset": 93, "endOffset": 113}, {"referenceID": 14, "context": "Methods that learn a user dependent mapping from the real numbers into this interval have been explored in the literature (Stern et al., 2009).", "startOffset": 122, "endOffset": 142}, {"referenceID": 12, "context": "Comparisons are against our own implementation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochastic gradient descent for all factors) and the neighbourhood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective.", "startOffset": 90, "endOffset": 111}, {"referenceID": 12, "context": "Comparisons are against our own implementation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochastic gradient descent for all factors) and the neighbourhood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective.", "startOffset": 91, "endOffset": 167}, {"referenceID": 12, "context": "Comparisons are against our own implementation of a classical cosine neighbourhood method (Sarwar et al., 2001); a typical latent factor model (similar to Funk (2006) but with simultaneous stochastic gradient descent for all factors) and the neighbourhood method from Koren (2010) (the version without latent factors), which uses a non-linear least squares objective.", "startOffset": 91, "endOffset": 281}, {"referenceID": 10, "context": "Salakhutdinov et al. (2007) used a bipartite graphical model, with binary hidden variables forming one part.", "startOffset": 0, "endOffset": 28}, {"referenceID": 16, "context": "Multi-scale conditional random fields models have also been applied to the more general social recommendation task with some success (Xin et al., 2009).", "startOffset": 133, "endOffset": 151}, {"referenceID": 16, "context": "Multi-scale conditional random fields models have also been applied to the more general social recommendation task with some success (Xin et al., 2009). Directed graphical models are commonly used as a modelling tool, such as in Salakhutdinov & Mnih (2008). While undirected models can be used in a similar way, the graph structures we apply in this work are far less rigidly structured.", "startOffset": 134, "endOffset": 257}, {"referenceID": 5, "context": "Nonlocal neighbourhood methods have been explored using the concept of spreading activation, typically on the user graph (Griffith et al., 2006) or on a bipartite user-item graph (Lie & Wang, 2009).", "startOffset": 121, "endOffset": 144}], "year": 2012, "abstractText": "Item neighbourhood methods for collaborative filtering learn a weighted graph over the set of items, where each item is connected to those it is most similar to. The prediction of a user\u2019s rating on an item is then given by that rating of neighbouring items, weighted by their similarity. This paper presents a new neighbourhood approach which we call item fields, whereby an undirected graphical model is formed over the item graph. The resulting prediction rule is a simple generalization of the classical approaches, which takes into account non-local information in the graph, allowing its best results to be obtained when using drastically fewer edges than other neighbourhood approaches. A fast approximate maximum entropy training method based on the Bethe approximation is presented, which uses a simple gradient ascent procedure. When using precomputed sufficient statistics on the Movielens datasets, our method is faster than maximum likelihood approaches by two orders of magnitude.", "creator": "LaTeX with hyperref package"}}}