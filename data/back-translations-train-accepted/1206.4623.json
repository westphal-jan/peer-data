{"id": "1206.4623", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "On the Size of the Online Kernel Sparsification Dictionary", "abstract": "We analyze the size of the dictionary constructed from online kernel sparsification, using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator. Using this formula, we are able to connect the cardinality of the dictionary with the eigen-decay of the covariance operator. In particular, we show that under certain technical conditions, the size of the dictionary will always grow sub-linearly in the number of data points, and, as a consequence, the kernel linear regressor constructed from the resulting dictionary is consistent.", "histories": [["v1", "Mon, 18 Jun 2012 15:06:34 GMT  (382kb)", "http://arxiv.org/abs/1206.4623v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yi sun", "faustino j gomez", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1206.4623"}, "pdf": {"name": "1206.4623.pdf", "metadata": {"source": "META", "title": "On the Size of the Online Kernel Sparsification Dictionary", "authors": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "emails": ["yi@idsia.ch", "tino@idsia.ch", "juergen@idsia.ch"], "sections": [{"heading": "1. Introduction", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2. The Determinant of a Gram matrix", "text": "Let H be a separable Hilbert space endowed with an inner product < \u00b7, \u00b7 >, and P a distribution over H. Let's assume that it is a tensor product. Let's assume that it is the eigenvalues of C sorted in descending order, and then the (uncentered) covariance operator, where the samples are called tensor product. Let's assume that the eigenvalues of C are sorted in descending order, then the eigenvalues of (i, j) -th entry < (Theorem 2,1, Blanchard et al. 2007). Let's assume that detGk is the determinant of Gk."}, {"heading": "2.1. A Formula for the Expectation of the Gram Determinant", "text": "Before presenting our first main result (theorem 1), we introduce an additional notation: the elementary symmetrical polynomial 3 of the order k over n variables is defined, summing up over all k subsets of {1,.,., n}. We denote the infinite expansion of \u03bdn, k as\u03bdk (1, 2,.,.) = k! < i2 < i2 < ik.ik i2 whenever the infinite sum exists. For simplicity, k denote both the function and its respective values with default arguments (1, 2,.), and we only write down the arguments if they exist from (1, 2,.)."}, {"heading": "2.3. Bounding the Moments of the Gram Determinant", "text": "In this section, we will prove a simple result with respect to the moment E [(detGk) m], with the additional assumption that H is the reproducing Hilbert kernel (RKHS) associated with a limited Mercer kernel '(x, x \u2032). Note that H (m) for each m \u2265 1,' (m) (x, x \u2032) = ('(x, x \u2032) m is still a limited Mercer kernel. Let H (m) be the RKHS associated with' (m), and designate \u03bb (m) 1 \u2265 (m) 2 \u2265 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 the eigenvalues of the corresponding covariance operator in H (m). We have the following limit. Theorem 2 E [(detGk) m] \u2264 (m) 1, \u03bb (m) 2,.) for m = 2, 3 \u00b7 \u00b7 \u00b7 the eigenvalues of the corresponding covariance operator in H ()."}, {"heading": "3. Analyzing Online Kernel Sparsification", "text": "In OKS the dictionary D is initially empty. If a new sample is created, it is included in the dictionary. (From the dictionary ifdetGD (from the dictionary). (From the dictionary ifdetGD (from the dictionary). (From the dictionary ifdetGD (from the dictionary). (From the dictionary ifdetGD (from the dictionary). (From the dictionary ifdetGD (from the dictionary). (From the dictionary ifdetGD (from the dictionary). (From the dictionary ifdetGD (from the dictionary). (From the dictionary ifdetGD (from the dictionary). (From the dictionary. (from the dictionary. (ifdetGD.). (from the dictionary. (from the dictionary.). (from the dictionary. (from the dictionary. (ifdetGD). (from the dictionary.). (from the dictionary. (from the dictionary.). (from the dictionary. (ifde.). (from the dictionary.). (from the dictionary. (from the dictionary.). (from the dictionary. (from the dictionary. (ifde.). (.). (from the dictionary. (from the dictionary. (.). (from the dictionary. (if.). (from the dictionary. (.). (. (from the dictionary. (.). (from the dictionary. (. (.). (from the dictionary. (ifde. (.). (. (.). (from the dictionary. (. (.). (from the dictionary. (. (.). (.). (from the dictionary. (.). (from the dictionary. (. (.). (. (.). (from the dictionary. (.). (.. (...."}, {"heading": "4. Discussion", "text": "This paper presented a rigorous theoretical analysis of how the dictionary scales on the online nuclear sparsification scale in terms of the number of samples based on the properties of the determinant of the Gram matrix. This work should lead to a better understanding of OKS, both in terms of its computational complexity and the generalization capabilities associated with nuclear retrogressors. Below, three further points are discussed, which include a) the validity of assumption 1, b) how our results relate to the Nystro-m method and c) how the analysis can potentially be further developed."}, {"heading": "4.1. On Assumption 1", "text": "Under the mild condition that it is a Mercer nucleus within the meaning of definition 2,15 in Braun (2005) and subsequently in Theorem 3,26, it follows that the convergence of the spectrum in E [detGk] is not sufficient to hold theorem 1, and that the greater L1 convergence of the eigenspectrum is required. It is possible to drop assumption 1 entirely and base the discussion on Limn. \u2212 Instead, the convergence of the spectrum in E [detGk] is insufficient. Otherwise, according to the analysis by Gretton et al. (2009), we can create a sufficient convergence between assumption 1 and assumption 1 using the following extension of the Hoffman-Wielandt inequality (theorem [detGk] and a convergence in C [detGk]."}, {"heading": "4.2. Comparison with Nystro\u0308m Method", "text": "A similar approach to OKS to reduce the calculation costs of core methods is the Nystro \u00b2 m Method (Williams and Seeger, 2000), where the dictionary consists of a randomly selected subset of samples. A distinction between the two methods following the previous analysis is that the OKS dictionary fulfils the detGD > \u03b1 | D |, while the randomly selected subset D \u00b2 fulfils the detGD \u00b2 for larger D. Therefore, detGn detGD \u00b2 is detGD \u00b2.6We thank the anonymous reviewers for pointing this out. From an information-theoretical point of view, the detGndetGD \u00b2 protocol can be interpreted as conditional entropy (Cover and Thomas, 1988), suggesting that D \u00b2 collects less information about the data sets than the DtGD \u00b2.The theoretical study of the Nystro \u00b2 m Method by Drineas and Mahoney (2005) indicates that O (\u03b1 sample is needed to calculate the first 4k)."}, {"heading": "4.3. On Strengthening the Bound", "text": "The proof for Theorem 2 uses Markov's inequality to bind both P [detGk > \u03b1 k] and the probability of [detGk > \u03b1 (A) 6 = 0. In practice, this limit is hardly satisfactory. One possibility is to strengthen the limit by incorporating information from moments of higher order (Philips and Nelson, 1995), i.e. P [detGk > \u03b1] 6 = 0. However, the analysis of [m) i is generally difficult and remains an open research question. It is also possible to improve the second step by using concentration inequalities for configuration functions (Boucheron et al., 1999)."}], "references": [{"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "JMLR, 3:1\u201348,", "citeRegEx": "Bach and Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2002}, {"title": "The Hoffman-Wielandt inequality in infinite dimensions", "author": ["R. Bhatia", "L. Elsner"], "venue": "Proc. Indian Acad. Sci. (Math. Sci),", "citeRegEx": "Bhatia and Elsner.,? \\Q1994\\E", "shortCiteRegEx": "Bhatia and Elsner.", "year": 1994}, {"title": "Statistical properties of kernel principal component analysis", "author": ["G. Blanchard", "O. Bousquet", "L. Zwald"], "venue": "Machine Learning,", "citeRegEx": "Blanchard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2007}, {"title": "A sharp concentration inequality with applications", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "Technical Report 376,", "citeRegEx": "Boucheron et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 1999}, {"title": "Spectral properties of the kernel matrix and their relation to kernel methods in machine learning", "author": ["M.L. Braun"], "venue": "PhD thesis,", "citeRegEx": "Braun.,? \\Q2005\\E", "shortCiteRegEx": "Braun.", "year": 2005}, {"title": "Determinant inequalities via information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Cover and Thomas.,? \\Q1988\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 1988}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": null, "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Incremental sparsification for real-time online model learning", "author": ["N. Duy", "J. Peters"], "venue": "In AISTAT\u201910,", "citeRegEx": "Duy and Peters.,? \\Q2010\\E", "shortCiteRegEx": "Duy and Peters.", "year": 2010}, {"title": "Algorithms and representations for reinforcement learning", "author": ["Y. Engel"], "venue": "PhD thesis, Hebrew University,", "citeRegEx": "Engel.,? \\Q2005\\E", "shortCiteRegEx": "Engel.", "year": 2005}, {"title": "The kernel recursive least-squares algorithm", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Engel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2004}, {"title": "A fast, consistent kernel twosample test", "author": ["A. Gretton", "K. Fukumizu", "Z. Harchaoui", "B.K. Sriperumbudur"], "venue": "In NIPS\u201909,", "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "A distribution-free theory of nonparametric regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": null, "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2004}, {"title": "Testing for homogeneity with kernel Fisher discriminant analysis", "author": ["Z. Harchaoui", "F.R. Bach", "\u00c9. Moulines"], "venue": "In NIPS\u201908,", "citeRegEx": "Harchaoui et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Harchaoui et al\\.", "year": 2008}, {"title": "The strong law of large numbers for U-statistics", "author": ["W. Hoeffding"], "venue": "Technical Report 302, Department of statistics,", "citeRegEx": "Hoeffding.,? \\Q1961\\E", "shortCiteRegEx": "Hoeffding.", "year": 1961}, {"title": "Improved bound for the Nystr\u00f6m method and its application to kernel classification", "author": ["R. Jin", "T.-B. Yang", "M. Mahdavi", "Y.-F. Li", "Z.H. Zhou"], "venue": "Technical Report arXiv:1111.2262v3,", "citeRegEx": "Jin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2012}, {"title": "Random matrix approximation of spectra of integral operators", "author": ["V. Koltchinskii", "E. Gin\u00e9"], "venue": null, "citeRegEx": "Koltchinskii and Gin\u00e9.,? \\Q2000\\E", "shortCiteRegEx": "Koltchinskii and Gin\u00e9.", "year": 2000}, {"title": "The expected determinant of the random Gram matrix and its application to information retrieval", "author": ["J. Martin"], "venue": "URL http://dydan. rutgers.edu/Seminars/Slides/martin2.pdf", "citeRegEx": "Martin.,? \\Q2007\\E", "shortCiteRegEx": "Martin.", "year": 2007}, {"title": "Matrix analysis and applied linear algebra", "author": ["C.D. Meyer"], "venue": "SIAM: Society for Industrial and Applied Mathematics,", "citeRegEx": "Meyer.,? \\Q2001\\E", "shortCiteRegEx": "Meyer.", "year": 2001}, {"title": "A new look at Newton\u2019s inequalities", "author": ["C.P. Niculescu"], "venue": "Journal of Inequalities in Pure and Applied Mathematics,", "citeRegEx": "Niculescu.,? \\Q2000\\E", "shortCiteRegEx": "Niculescu.", "year": 2000}, {"title": "The moment bound is tighter than Chernoff\u2019s bound for positive tail probabilities", "author": ["T.K. Philips", "R. Nelson"], "venue": "The American Statistician,", "citeRegEx": "Philips and Nelson.,? \\Q1995\\E", "shortCiteRegEx": "Philips and Nelson.", "year": 1995}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Approximation theorems of mathematical statistics", "author": ["R.J. Serfling"], "venue": null, "citeRegEx": "Serfling.,? \\Q1980\\E", "shortCiteRegEx": "Serfling.", "year": 1980}, {"title": "Online kernel-based classification using adaptive projection algorithms", "author": ["K. Slavakis", "S. Theodoridis", "I. Yamada"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Slavakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Slavakis et al\\.", "year": 2008}, {"title": "An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels", "author": ["I. Steinwart", "D. Hush", "C. Scovel"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2006}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In NIPS\u201900,", "citeRegEx": "Williams and Seeger.,? \\Q2000\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2000}, {"title": "A sparse kernel-based least-squares temporal difference algorithm for reinforcement learning", "author": ["X. Xu"], "venue": "In Advances in Natural Computation,", "citeRegEx": "Xu.,? \\Q2006\\E", "shortCiteRegEx": "Xu.", "year": 2006}], "referenceMentions": [{"referenceID": 24, "context": "(Williams and Seeger, 2000), where a randomly selected subset is used.", "startOffset": 0, "endOffset": 27}, {"referenceID": 9, "context": "The second, which is the concern of this paper, is called Online Kernel Sparsification (OKS; Engel et al. 2004), where the dictionary is built up incrementally by incorporating new samples that cannot be represented well (in the least squares sense) using the current dictionary.", "startOffset": 87, "endOffset": 111}, {"referenceID": 7, "context": "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al.", "startOffset": 72, "endOffset": 94}, {"referenceID": 22, "context": "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006).", "startOffset": 111, "endOffset": 134}, {"referenceID": 8, "context": ", 2008) and reinforcement learning (Engel, 2005; Xu, 2006).", "startOffset": 35, "endOffset": 58}, {"referenceID": 25, "context": ", 2008) and reinforcement learning (Engel, 2005; Xu, 2006).", "startOffset": 35, "endOffset": 58}, {"referenceID": 7, "context": "Since being proposed, OKS has found numerous applications in regression (Duy and Peters, 2010), classification (Slavakis et al., 2008) and reinforcement learning (Engel, 2005; Xu, 2006). Despite this empirical success, however, the theoretical understanding of OKS is still lacking. Most of the theoretical analysis has been done by Engel et al. (2004), who showed that the constructed dictionary is guaranteed to represent major fraction of the leading eigenvectors of the Gram matrix (Theorem 3.", "startOffset": 73, "endOffset": 353}, {"referenceID": 21, "context": "is a U-statistic (Serfling, 1980) with kernel detGk.", "startOffset": 17, "endOffset": 33}, {"referenceID": 13, "context": "Since E [detGk] <\u221e, the law of large numbers for U-statistics (Hoeffding, 1961) asserts that", "startOffset": 62, "endOffset": 79}, {"referenceID": 16, "context": "An alternative proof may be derived using the generator function of E [detGk] (Martin, 2007).", "startOffset": 78, "endOffset": 92}, {"referenceID": 8, "context": "Note that our notation is equivalent to the form originally proposed by Engel et al. (2004) as", "startOffset": 72, "endOffset": 92}, {"referenceID": 5, "context": "From Theorem 5 in Cover and Thomas (1988), ( detGk+1 (\u03c61:k+1) \u03b1k+1 ) 1 k+1 \u2264 1 k + 1 \u2211", "startOffset": 18, "endOffset": 42}, {"referenceID": 4, "context": "15 in Braun (2005), and subsequently by Theorem 3.", "startOffset": 6, "endOffset": 19}, {"referenceID": 9, "context": "Otherwise, following the analysis by Gretton et al. (2009), we may provide sufficient conditions to Assumption 1 using the following extension of the Hoffman\u2013Wielandt inequality (Theorem 3, Bhatia and Elsner 1994) \u2211", "startOffset": 37, "endOffset": 59}, {"referenceID": 23, "context": "(2008), the convergence of \u2225\u2225\u2225C\u0303k \u2212 C\u2225\u2225\u2225 1 to zero can be established provided that i) H is a separable RKHS (e.g., an RKHS induced by a continuous kernel over a separable metric space; Steinwart et al. 2006) induced by some bounded kernel, and ii) the eigenspectrum of C satisfies \u2211 i \u03bb 1 2 i <\u221e.", "startOffset": 109, "endOffset": 208}, {"referenceID": 12, "context": "Using Proposition 12 in Harchaoui et al. (2008), the convergence of \u2225\u2225\u2225C\u0303k \u2212 C\u2225\u2225\u2225 1 to zero can be established provided that i) H is a separable RKHS (e.", "startOffset": 24, "endOffset": 48}, {"referenceID": 24, "context": "A similar approach to OKS for reducing the computational cost of kernel methods is the Nystr\u00f6m method (Williams and Seeger, 2000), where the dictionary consists of a subset of samples chosen at random.", "startOffset": 102, "endOffset": 129}, {"referenceID": 5, "context": "From an information theoretic point of view, log detGn detGD\u0303 can be interpreted as the conditional entropy (Cover and Thomas, 1988), which indicates that D\u0303 captures less information about the data sets.", "startOffset": 108, "endOffset": 132}, {"referenceID": 14, "context": "A recent study (Jin et al., 2012) shows that assuming bounded kernel, the spectral norm of the approximation error between the true and the approximated Gram matrix scales at a rate of O ( n |D| 1 2 ) , and in the case of \u03bbi \u223c i\u2212p, an", "startOffset": 15, "endOffset": 33}, {"referenceID": 6, "context": "The theoretical study of the Nystr\u00f6m method by Drineas and Mahoney (2005) suggests that O ( \u03b1\u22124k ) samples are needed to approximate the first k eigenvectors well, which is linear in k, irrespective of the sample size.", "startOffset": 47, "endOffset": 74}, {"referenceID": 19, "context": "One possibility is to strengthen the bound by incorporating information from higher order moments (Philips and Nelson, 1995), i.", "startOffset": 98, "endOffset": 124}, {"referenceID": 3, "context": "It is also possible to improve the second step, using concentration inequalities for configuration functions (Boucheron et al., 1999).", "startOffset": 109, "endOffset": 133}, {"referenceID": 3, "context": "By Theorem 2 in Boucheron et al. (1999), Zn concentrates sharply around E [Zn].", "startOffset": 16, "endOffset": 40}], "year": 2012, "abstractText": "We analyze the size of the dictionary constructed from online kernel sparsification, using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator. Using this formula, we are able to connect the cardinality of the dictionary with the eigen-decay of the covariance operator. In particular, we show that under certain technical conditions, the size of the dictionary will always grow sublinearly in the number of data points, and, as a consequence, the kernel linear regressor constructed from the resulting dictionary is consistent.", "creator": "LaTeX with hyperref package"}}}