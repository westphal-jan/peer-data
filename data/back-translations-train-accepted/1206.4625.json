{"id": "1206.4625", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Optimizing F-measure: A Tale of Two Approaches", "abstract": "F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario.", "histories": [["v1", "Mon, 18 Jun 2012 15:07:04 GMT  (378kb)", "http://arxiv.org/abs/1206.4625v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nan ye", "kian ming adam chai", "wee sun lee", "hai leong chieu"], "accepted": true, "id": "1206.4625"}, "pdf": {"name": "1206.4625.pdf", "metadata": {"source": "META", "title": "Optimizing F-Measures: A Tale of Two Approaches", "authors": ["Nan Ye", "Kian Ming A. Chai", "Wee Sun Lee", "Hai Leong Chieu"], "emails": ["yenan@comp.nus.edu.sg", "ckianmin@dso.org.sg", "leews@comp.nus.edu.sg", "chaileon@dso.org.sg"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to surpass themselves, both in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move, in terms of the way they move."}, {"heading": "2. Theoretical Analysis", "text": "We assume that there is a fixed but unknown distribution P (X, Y) that i.i.d. (X, Y) pairs generate during training and testing. We use X and Y to also mark their domains. In this work, Y = {0, 1}, with 0 for the negative or irrelevant class and 1 for the positive or relevant class. I (\u00b7) is the indicator function. Let us leave Dn = (x1, y1),..., (xn, yn)} to be a series of n (possibly non-i.i.d.) examples, and let us leave x and y denotes (x1,.., xn) and (y1,., yn) respectively. If the predicted labels are s = (s1,.., sn), then precision p (s, y) is the number of true positives over the number of predicted positives, and let us remember (s, y) is the number of true positives over the year (s, y)."}, {"heading": "2.1. Uniform Convergence and Consistency for EUM", "text": "Let us consider an arbitrary classifier: X 7 \u2192 y. Let us leave F\u03b2, n \u03b2 (\u03b2 \u03b2) = \u03b2 (\u03b2 \u03b2) = \u03b2 (\u03b2 \u03b2). Let Pij, n (\u03b8) be the empirical probability that a class i instance is observed and predicted as a class j of \u03b8; that is, Pij, n (\u03b8) = \u2211 n = 1 I (yk = i \u0432 (xk) = j) / n. ThenF\u03b2, n (\u03b8) = (1 + \u03b22) p11, n (\u03b8) \u03b22 (p11, n (\u03b8) + p11, n (\u03b8) + p01, n (\u03b8) Let pij (\u03b8) = E (Y = i) p11, n (\u03b8) p2 (p11, n). The probability that a class i instance is predicted as a class i instance is predicted as a class j."}, {"heading": "2.2. Optimality of Thresholding in EUM", "text": "Suppose we know the true conditional distribution P (Y | X). Suppose the class T of the probability threshold contains classifiers of the form I\u03b4 (x) = I (P (1 | x) > \u03b4), and the class T contains I (x) = I (P (1 | x). Suppose the class T of the probability threshold has the VC dimension 1, so that the empirical maximization of the F measure is consistent for this class. Although the class T \u00b2 does not contain all possible classifiers on X, an optimal classifier can be found in this class. Suppose that an optimal classification threshold = arg maxh \u00b2 T \u2032 F\u03b2 (h), so that the empirical maximization of the F measure for this class is consistent."}, {"heading": "2.3. An Asymptotic Equivalence Result", "text": "We will now examine the links between EUM-optimal classifiers and DTA-optimal classifiers when the true distribution P (\u03b2, Y) is known. (\u03b2, Y) By definition, a DTA-optimal classifier is better than an EUM-optimal classifier when tested on many i.i.d. test sets. However, we will give an asymptotic equivalence result for EUM-optimal classifiers and DTA-optimal classifiers on large i.i.d test sets if we consider only an optimal probability threshold as a representative EUM-optimal classification. In the following, we will let x = (x1,. xn) Xn be an i.d. sequence of observations. For all classifiers, we will let these properties apply (x) = (xi) i. All expectations designated by E (\u00b7) are taken under the conditional distribution P (y | x)."}, {"heading": "3. Algorithms", "text": "We first discuss approaches to EUM, then discuss DTA and present a new efficient prediction algorithm."}, {"heading": "3.1. Approximations to the EUM Approach", "text": "Accurate empirical optimization of F measurements for a parametric family is difficult due to its complex piecemeal linear nature (\u03b2 = \u03b2), and typically only approximations of F measurements are maximized. We will discuss three methods. However, given the optimality of the probability threshold for classifiers, it is natural to first learn an estimate p (Y | X) for P (Y | X) and then learn an optimal threshold g \u2212. If p (Y | X) is selected from a parametric family, the learned distribution follows an asymptotically normal convergence to the model with the lowest KL divergence to true distribution (White, 1982). We consider T to simplify the representation. Thus, if the model family is well specified, the resulting classifier asymptotically is optimal. We call this ML approximation a large convergence to the model with the lowest KL-White divergence to true distribution (1982)."}, {"heading": "3.2. Maximizing Expected F-measure", "text": "Considering a utility function U (s, y), the decision theory optimal prediction for x q = q can maximize the benefit of p (\u00b7 | x) (U (s, y)). Generally, the true distribution P is not known and is estimated. The approach, in which first the actual distributions are estimated using the maximum probability (ML) and then the decision theory optimal predictions are made, is called the MLE approach. We discuss the two steps in MLE, then we present an efficient algorithm for calculating the optimal predictions. First, the asymptotic convergence of ML (White, 1982) implies the MLE approach as asymptotically optimal when we expect sufficient training examples in a well-specified family. In practice, we will not know if the model family is well specified. Nevertheless, as we will see in Section 4.1, the MLE approach can provide results from the optimal prediction of i."}, {"heading": "3.2.1. A Quadratic Time Algorithm", "text": "We give an O (n3) time algorithm to calculate the optimal predictions, then we improve it to O (n2) if \u03b22 is rational, which is often the case. If we let F\u03b2; k (y) be the F\u03b2 measurement quantity, if the first k instances are predicted as positive, then we have F\u03b2; k (y) = (1 + \u03b2 2) \u2211 k i = 1 yi / [k + 2 \u2211 n = 1 yi]. Letf\u03b2; k = \u2211 y P (y) F\u03b2; k (y), and Si: j = \u03b1 j l = i yl. For the satisfaction of S1: k = k1 and Sk + 1: n = k2, their F\u03b2's (1 + \u03b22) are k1 / (k + \u03b22) k1 (k1) k1 / (k + k2)."}, {"heading": "4. Experiments", "text": "We show empirically that EUM can be more robust against model misspecifications, but DTA can be better for rare classes with small datasets and a common scenario of domain matching. We use one synthetic dataset, the Reuters 21578 dataset, and four multi-label classification datasets."}, {"heading": "4.1. Mixtures of Gaussians", "text": "We consider a mixture of Gaussians on D dimensions: P (X, Y) = \u03c0YN (X; \u00b5Y, \u0442Y), with \u043f1 = \u043f0 = ID, \u00b51 = (S + O) 1 / \u221a 4D and \u00b50 = \u2212 (S \u2212 O) 1 / \u221a 4D, where S and O are non-negative constants. Thus, S is the distance between the centers. We are determined only on S, O, D, \u03c01 and the number of training examples no. All instances are i.i.d. The optimal F1 achievable by a classifier can be calculated (see eq. 2), and it only depends on S and \u03c01. Ntr determines how close the estimated distribution is to the optimal model; and the number of test examples, Nts, affects the gap in performance between the threshold method and the expectation method (theorem 8).We train logistic regression (LR) models with three different representations: the coordinates only consists of 0."}, {"heading": "4.1.1. Effect of Model Quality", "text": "We use the default setting in the previous section, where \u03c01 = 0.5 and S = 4 have been changed to S = 2 as the true distribution, to generate a set of 3000 i.i.d. test instances. We make optimal predictions based on an assumed distribution identical to the true one, except that we vary \u03c01. For each \u03c01 we calculate the formula 1 and the formula Kullback-Leibler divergence (KL) for similar experiments, but instead use 0.1 and 0.9 as the true formula 1. Our choice of S = 2 instead of S = 4 for the true distribution has made the difference between the true and the assumed distribution more pronounced in the diagrams."}, {"heading": "4.1.2. Domain Adaptation", "text": "A common scenario is when P (X) changes, but P (Y | X) does not. Using the mixture of Gaussian with D = 10, S = 4, O = 0 and \u03c01 = 0.5, we generate 5000 i.i.d. training instances and 5000 test instances with P (Y | X) < 0.5. F1 values for turkey \u03b4, turkey E, ML\u03b4 and MLE (using R1) are 21%, 38%, 11% and 36%, respectively. Similar results are obtained under similar conditions."}, {"heading": "4.2. Text Classification", "text": "We evaluate the Reuters-21578 dataset 5 using the ModApte partition, which has 9603 training docu-5This, at http: / / www.daviddlewis.com / resources / testcollections / reuters21578 /.ments and 3299 test documents. We train two models: the Bayes multinomial standard model (NB) and an LR model using word event counting and a specified dummy attribute. Both models are regularized. For NB, we use the Laplace corrector with a class and word count. For LR, we use the Gaussian standard for the parameters. We only use subjects with at least C positive instances in both the train and test sets, and we vary C. Table 3 reports macro F1 values (averaged over topics), with ML.5 using 0.5 to threshold the probabilities, in Table 3, although NB generally does not provide good probabilities (MLE and MLE is still MLE)."}, {"heading": "4.3. Multilabel Datasets", "text": "We evaluate four standard multi-label classification datasets. 6 We train regulated LRs, with the regu-6 thesis available at http: / / mulan.sourceforge.net /.larization parameters for each class selected by double cross-validation. Macro F1 values are presented in Table 4. The bracketed values are determined by selecting the regularization parameter, which results in a model with minimal empirical KL divergence of test data. Each bracketed score is higher than its non-bracketed counterpart, so models that are closer to the true score are better for both MLE and ML\u03b4. Comparing the values for MLE with those for ML\u03b4 and F\u03b4, whether bracketed or not, we see that MLE performs better, especially for smaller C, suggesting that MLE is better for rare classes."}, {"heading": "5. Conclusion", "text": "We cited theoretical justifications and links to optimize F measures using EUM and DTA. We showed empirically that EUM seems to be more robust against model misspecifications, while DTA seems to be better with a good model for dealing with rare classes and a common domain adaptation scenario. Some important questions remain unanswered: the existence of interesting classifiers for which EUM can be performed accurately, the quantification of the impact of inaccurate models on optimal predictions, the determination of conditions under which one method is preferable to another, and practical methods for selecting the best method on a dataset. The results in this paper apply only to large datasets, and it is important to consider the arguments for a small number of cases. Experimenting with and analyzing other methods can also provide additional insights."}, {"heading": "Acknowledgement", "text": "This work is supported by DSO DSOL11102."}], "references": [{"title": "Expectation of F-measures: tractable exact computation and some empirical observations of its properties", "author": ["K.M.A. Chai"], "venue": "In SIGIR,", "citeRegEx": "Chai,? \\Q2005\\E", "shortCiteRegEx": "Chai", "year": 2005}, {"title": "Search-based structured prediction", "author": ["H. Daum\u00e9", "J. Langford", "D. Marcu"], "venue": "Machine learning,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "An exact algorithm for F-measure maximization", "author": ["K. Dembczynski", "W. Waegeman", "W. Cheng", "E. Hullermeier"], "venue": "In NIPS,", "citeRegEx": "Dembczynski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2011}, {"title": "A study on threshold selection for multi-label classification", "author": ["R.E. Fan", "C.J. Lin"], "venue": "Technical report,", "citeRegEx": "Fan and Lin,? \\Q2007\\E", "shortCiteRegEx": "Fan and Lin", "year": 2007}, {"title": "Maximum expected F-measure training of logistic regression models", "author": ["M. Jansche"], "venue": "In HLT/EMNLP,", "citeRegEx": "Jansche,? \\Q2005\\E", "shortCiteRegEx": "Jansche", "year": 2005}, {"title": "A maximum expected utility framework for binary sequence labeling", "author": ["M. Jansche"], "venue": "In ACL,", "citeRegEx": "Jansche,? \\Q2007\\E", "shortCiteRegEx": "Jansche", "year": 2007}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In ICML, pp", "citeRegEx": "Joachims,? \\Q2005\\E", "shortCiteRegEx": "Joachims", "year": 2005}, {"title": "Evaluating and optimizing autonomous text classification systems", "author": ["D.D. Lewis"], "venue": "In SIGIR, pp", "citeRegEx": "Lewis,? \\Q1995\\E", "shortCiteRegEx": "Lewis", "year": 1995}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schutze"], "venue": "CUP,", "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Reverse multi-label learning", "author": ["J. Petterson", "T. Caetano"], "venue": "In NIPS, pp. 1912\u20131920,", "citeRegEx": "Petterson and Caetano,? \\Q2010\\E", "shortCiteRegEx": "Petterson and Caetano", "year": 2010}, {"title": "Training conditional random fields with multivariate evaluation measures", "author": ["J. Suzuki", "E. McDermott", "H. Isozaki"], "venue": "In ACL, pp", "citeRegEx": "Suzuki et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2006}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "In HLT-NAACL,", "citeRegEx": "Sang and Meulder,? \\Q2003\\E", "shortCiteRegEx": "Sang and Meulder", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, 6(2):1453\u20131484,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Foundation of evaluation", "author": ["C.J. van Rijsbergen"], "venue": "Journal of Documentation,", "citeRegEx": "Rijsbergen,? \\Q1974\\E", "shortCiteRegEx": "Rijsbergen", "year": 1974}, {"title": "The nature of statistical learning theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}, {"title": "Maximum likelihood estimation of misspecified models", "author": ["H. White"], "venue": null, "citeRegEx": "White,? \\Q1982\\E", "shortCiteRegEx": "White", "year": 1982}, {"title": "A study of thresholding strategies for text categorization", "author": ["Y. Yang"], "venue": "In SIGIR, pp", "citeRegEx": "Yang,? \\Q2001\\E", "shortCiteRegEx": "Yang", "year": 2001}, {"title": "Bayesian online learning for multi-label and multi-variate performance measures", "author": ["X. Zhang", "T. Graepel", "R. Herbrich"], "venue": "In AISTATS, pp", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "F-measures are usually preferred to accuracies as standard performance measures in information retrieval (Manning et al., 2008), particularly, when relevant items are rare.", "startOffset": 105, "endOffset": 127}, {"referenceID": 16, "context": "Though simple, this method has been found to be effective and is commonly applied, for example, in text categorization (Yang, 2001).", "startOffset": 119, "endOffset": 131}, {"referenceID": 4, "context": "Joachims (2005) gave an efficient algorithm for maximizing a convex lower bound of F-measures for support vector machines, and showed it worked well on text classification.", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "Jansche (2005) gave an efficient algorithm to maximize a non-convex approximation to F-measures using logistic regression models, and showed it works well on a text summarization problem.", "startOffset": 0, "endOffset": 15}, {"referenceID": 7, "context": "The decision-theoretic approach (DTA), advocated by Lewis (1995), estimates a probability model first, and then computes the optimal predictions (in the sense of having highest expected F-measure) according to the model.", "startOffset": 52, "endOffset": 65}, {"referenceID": 0, "context": "Based on Lewis\u2019s characterization, Chai (2005) gave an O(n) time algorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA.", "startOffset": 35, "endOffset": 47}, {"referenceID": 0, "context": "Based on Lewis\u2019s characterization, Chai (2005) gave an O(n) time algorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA. Apparently unaware of Chai\u2019s work, Jansche (2007) solved the same problem in O(n) time.", "startOffset": 35, "endOffset": 223}, {"referenceID": 0, "context": "Based on Lewis\u2019s characterization, Chai (2005) gave an O(n) time algorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA. Apparently unaware of Chai\u2019s work, Jansche (2007) solved the same problem in O(n) time. For the general case when the labels are not necessarily independent, Dembczynski et al. (2011) gave an O(n) time algorithm given n+1 parameters of the label distribution, but the parameters can be expensive to compute.", "startOffset": 35, "endOffset": 357}, {"referenceID": 12, "context": "There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daum\u00e9 et al., 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al.", "startOffset": 85, "endOffset": 155}, {"referenceID": 10, "context": "There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daum\u00e9 et al., 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al.", "startOffset": 85, "endOffset": 155}, {"referenceID": 1, "context": "There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daum\u00e9 et al., 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al.", "startOffset": 85, "endOffset": 155}, {"referenceID": 17, "context": ", 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al., 2010; Petterson & Caetano, 2010).", "startOffset": 29, "endOffset": 93}, {"referenceID": 14, "context": "We now show that training to maximize the empirical F\u03b2 is consistent, using VC-dimension (Vapnik, 1995) to quantify the complexity of the classifier class.", "startOffset": 89, "endOffset": 103}, {"referenceID": 7, "context": "In general, optimal predictions may not satisfy this constraint (Lewis, 1995).", "startOffset": 64, "endOffset": 77}, {"referenceID": 15, "context": "using the maximum likelihood (ML) principle, then under very general conditions, the learned distribution follows an asymptotically normal convergence to the model with smallest KL-divergence to the true distribution (White, 1982).", "startOffset": 217, "endOffset": 230}, {"referenceID": 15, "context": "First, the asymptotic convergence of ML (White, 1982) implies the ML approach is asymptotically optimal when estimating with sufficient training examples in a well-specified family.", "startOffset": 40, "endOffset": 53}, {"referenceID": 14, "context": "To choose a class of the right complexity, one may follow the structural risk minimization principle (Vapnik, 1995).", "startOffset": 101, "endOffset": 115}, {"referenceID": 0, "context": "Chai (2005) used Gaussian process and obtained similar conclusion.", "startOffset": 0, "endOffset": 12}], "year": 2012, "abstractText": "F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario.", "creator": "LaTeX with hyperref package"}}}