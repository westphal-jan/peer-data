{"id": "1206.4627", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models", "abstract": "We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting.", "histories": [["v1", "Mon, 18 Jun 2012 15:07:39 GMT  (476kb)", "http://arxiv.org/abs/1206.4627v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jean honorio"], "accepted": true, "id": "1206.4627"}, "pdf": {"name": "1206.4627.pdf", "metadata": {"source": "CRF", "title": "Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models", "authors": ["Jean Honorio"], "emails": ["jhonorio@cs.sunysb.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to integrate themselves, in which they are able to live, in which they are able to live, in which they live, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they"}, {"heading": "2. Our Motivating Problem", "text": "In this section, we present the problem of learning sparse Ising models and discuss its characteristics. Our discussion will motivate a number of limitations and assumptions for a more general analysis of the convergence rate."}, {"heading": "2.1. Problem Setup", "text": "An Ising model is a Markov random field of binary variables with pair-by-pair interaction, first developed in statistical physics as a model for the energy of a physical system of interacting atoms (Koller & Friedman, 2009). Formally, the probability mass function (PMF) of an Ising model is parameterized by \u03b8 = (W, b) as: p\u03b8 (x) = 1 Z (W, b) e xTWx + bTx (1), where the domain for the binary variables x (1, + 1} N, W (RN \u00b7 N) is symmetrical with zero diagonal, b (RN) and partition function is defined as Z (W, b) x exTWx + bTx. For the clarity of convergence rate analysis, we also define the RM, where M = N2.In physical literature, W and b are referred to as ferromagnetic coupling and external."}, {"heading": "2.2. Bounds", "text": "Both are important components for the representation of convergence, and are largely used in optimization.In this work, we follow the original formulation of the problem set out in (Wainwright et al., 2006; Banerjee et al., 2008; Ho \ufffd fling & Tibshirani et al., 2007).We found that this problem has limitations for other stochastic optimization problems, such as SVMs (Shalev-Shwartz et al., 2007).First, we make some observations that will help us derive our limitations.The empirical second order moment we find ourselves in and the first order moment in eq. (2) are calculated from binary variables."}, {"heading": "2.3. Approximating the Gradient of the Log-Partition Function", "text": "Suppose we want the expression EP [xxT] in eq. (4), which is the gradient of the log partition function. Suppose we know the distribution from p\u03b8 (x) to a constant factor, i.e., p. \"(x) = exTWx + bTx. (Importance sampling extracts the S samples x (1),.., x.\" (S) from an experimental distribution with PMF q (x), calculates the importance (s) = p. \"(x) and produces the estimate (s) x.\" (s) x. \"(s) x.\" (x) x. \""}, {"heading": "3. Biased Stochastic Optimization", "text": "In this section, we will analyze the convergence rates of forward-backward splitting. Our results relate to any problem that meets the following widely used optimization assumptions: \u2022 the objective function consists of a smooth function L (\u03b8) and a non-smooth regulator R (\u03b8) \u2022 the optimal solution is limited, i.e. each visited point is located at a limited distance from the optimal solution, i.e. the non-smooth regulator disappears at zero, i.e. R (0) = 0We also require that the errors do not alter the Lipschitz continuity properties, i.e. that the Lipschitz continuity properties are not altered, i.e. that the Lipschitz continuity properties are not altered, i.e. that the convergence rates are not smooth (as discussed in Assumption 5)."}, {"heading": "3.1. Algorithm", "text": "We analyze forward-backward splitting (Duchi & Singer, 2009) for deterministic as well as biased stochastic errors, for non-increasing increments of the form \u03b7k-O (1kr) for 0 < r < 1. This method corresponds to the basic proximal gradient (Schmidt et al., 2011) for r = 0 (constant step size). We point out that FBS has gradient convergence for the smooth part of the objective function, while BasicPG O (1K) has convergence and accelerated PG O (1K2) convergence. Therefore, PG methods have a faster convergence, but are more prone to error. FBS performs gradient downgradient steps for the smooth part of the objective function, and (closed form) projection steps for the non-smooth part. Here, we assume that we have the same deterrent with each Iterk-side (unequal K gradient)."}, {"heading": "3.2. Convergence Rates for Deterministic Errors", "text": "In what follows, we analyze three different types of forward-backward splitting: robust, which gives the weighted average of all visited points using the step variables as in robust stochastic approximation (Nemirovski et al., 2009), based on what gives the average of all visited points as in (Duchi & Singer, 2009), or random, which gives a randomly chosen point. Here, we assume that for each iteration k we approximate the gradient with some deterministic errors (k). Our results in this subsection allow us to draw some conclusions not only with respect to FBS, but also with respect to the proximal gradation. To make our decisions for different step sizes O (1kr) for some 0 < r < r < r < r < r < r < 1, we use the harmonic numbers Hr, K = 1 kr and with it, K."}, {"heading": "3.3. Bounding the Error Term for Biased Stochastic Optimization", "text": "Subsequently, we will focus on the analysis of stochastic errors to see if better convergence rates can be achieved than those outlined in the previous paragraph. A formal analysis of the random selection criteria shows that the required number of random selection criteria for biased stochastic errors only requires a logarithmically increasing number of random selection criteria to become more convergent, i.e. there is no guarantee that accelerated selection criteria will be used in the biased stochastic selection criteria for FBS and basic PG. On the negative side, we have found that accelerated selection criteria are not guaranteed in the biased stochastic selection criteria.Further, we present our high probability of biased stochastic optimization. One way to tie the concept of error would be to rely on \"uniform convergence,\" i.e."}, {"heading": "4. Experimental Results", "text": "We illustrate our theoretical results with a small synthetic experiment (N = 15 variables), because we want to report the log probability for each iteration. We performed 10 repetitions. For each repetition, we generate edges in the basic truth model Wg with a density of 50%. The weight of each edge is generated uniformly according to the random principle of [\u2212 1; + 1]. We use bg = 0. Finally, we create a dataset with 50 samples. We use a \"Gibbs sampler\" by first finding the mean field distribution and then performing 5 Gibbs iterations. We use a step size factor \u03b2 = 1 and regulation parameters \u03c1 = 1 / 16. We also include a two-step algorithm by first learning the structure through \"1-regulated logistic regression\" (Wainwright et al al al al., 2006) and then the parameters by using FBS with the propagation of faith for the maximum diversification of our results will be incorporated into our contextual results."}], "references": [{"title": "Particle filtered MCMC-MLE with connections to contrastive divergence", "author": ["A. Asuncion", "Q. Liu", "A. Ihler", "P. Smyth"], "venue": null, "citeRegEx": "Asuncion et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2010}, {"title": "Estimate sequence methods: extensions and approximations", "author": ["M. Baes"], "venue": "IFOR internal report, ETH Zurich,", "citeRegEx": "Baes,? \\Q2009\\E", "shortCiteRegEx": "Baes", "year": 2009}, {"title": "Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data", "author": ["O. Banerjee", "L. El Ghaoui", "A. d\u2019Aspremont"], "venue": null, "citeRegEx": "Banerjee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2008}, {"title": "On the computational complexity of Ising spin glass models", "author": ["F. Barahona"], "venue": "Journal of Physics A: Mathematical, Nuclear and General,", "citeRegEx": "Barahona,? \\Q1982\\E", "shortCiteRegEx": "Barahona", "year": 1982}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "The Statistician,", "citeRegEx": "Besag,? \\Q1975\\E", "shortCiteRegEx": "Besag", "year": 1975}, {"title": "Complexity of inference in graphical models", "author": ["V. Chandrasekaran", "N. Srebro", "P. Harsha"], "venue": null, "citeRegEx": "Chandrasekaran et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2008}, {"title": "Smooth optimization with approximate gradient", "author": ["A. d\u2019Aspremont"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "d.Aspremont,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont", "year": 2008}, {"title": "Parallel tempering for training of restricted Boltzmann machines", "author": ["G. Desjardins", "A. Courville", "Y. Bengio", "P. Vincent", "O. Delalleau"], "venue": null, "citeRegEx": "Desjardins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "Stochastic first order methods in smooth convex optimization", "author": ["O. Devolder"], "venue": "CORE Discussion Papers 2012/9,", "citeRegEx": "Devolder,? \\Q2012\\E", "shortCiteRegEx": "Devolder", "year": 2012}, {"title": "First-order methods of smooth convex optimization with inexact oracle", "author": ["O. Devolder", "F. Glineur", "Y. Nesterov"], "venue": "CORE Discussion Papers 2011/2,", "citeRegEx": "Devolder et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Devolder et al\\.", "year": 2011}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": null, "citeRegEx": "Duchi and Singer,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer", "year": 2009}, {"title": "A convex upper bound on the log-partition function for binary graphical models", "author": ["L. El Ghaoui", "A. Gueye"], "venue": null, "citeRegEx": "Ghaoui and Gueye,? \\Q2008\\E", "shortCiteRegEx": "Ghaoui and Gueye", "year": 2008}, {"title": "Hybrid deterministicstochastic methods for data fitting", "author": ["M. Friedlander", "M. Schmidt"], "venue": null, "citeRegEx": "Friedlander and Schmidt,? \\Q2011\\E", "shortCiteRegEx": "Friedlander and Schmidt", "year": 2011}, {"title": "Markov chain Monte Carlo maximum likelihood", "author": ["C. Geyer"], "venue": "Computing Science and Statistics,", "citeRegEx": "Geyer,? \\Q1991\\E", "shortCiteRegEx": "Geyer", "year": 1991}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods", "author": ["H. H\u00f6fling", "R. Tibshirani"], "venue": null, "citeRegEx": "H\u00f6fling and Tibshirani,? \\Q2009\\E", "shortCiteRegEx": "H\u00f6fling and Tibshirani", "year": 2009}, {"title": "Accelerated gradient methods for stochastic optimization and online learning", "author": ["C. Hu", "J. Kowk", "W. Pan"], "venue": null, "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "On learning discrete graphical models using greedy methods", "author": ["A. Jalali", "C. Johnson", "P. Ravikumar"], "venue": null, "citeRegEx": "Jalali et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2011}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Efficient structure learning of Markov networks using `1-regularization", "author": ["S. Lee", "V. Ganapathi", "D. Koller"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Monte Carlo Strategies in Scientific Computing", "author": ["J. Liu"], "venue": null, "citeRegEx": "Liu,? \\Q2001\\E", "shortCiteRegEx": "Liu", "year": 2001}, {"title": "Inductive principles for restricted Boltzmann machine learning", "author": ["B. Marlin", "K. Swersky", "B. Chen", "N. de Freitas"], "venue": null, "citeRegEx": "Marlin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2010}, {"title": "Bayesian learning in undirected graphical models: Approximate MCMC algorithms", "author": ["I. Murray", "Z. Ghahramani"], "venue": null, "citeRegEx": "Murray and Ghahramani,? \\Q2004\\E", "shortCiteRegEx": "Murray and Ghahramani", "year": 2004}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Structure learning in Markov random fields", "author": ["S. Parise", "M. Welling"], "venue": null, "citeRegEx": "Parise and Welling,? \\Q2006\\E", "shortCiteRegEx": "Parise and Welling", "year": 2006}, {"title": "Optimum Monte Carlo sampling using Markov chains", "author": ["P. Peskun"], "venue": null, "citeRegEx": "Peskun,? \\Q1973\\E", "shortCiteRegEx": "Peskun", "year": 1973}, {"title": "Learning in Markov random fields using tempered transitions", "author": ["R. Salakhutdinov"], "venue": null, "citeRegEx": "Salakhutdinov,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov", "year": 2009}, {"title": "Learning deep Boltzmann machines using adaptive MCMC", "author": ["R. Salakhutdinov"], "venue": null, "citeRegEx": "Salakhutdinov,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov", "year": 2010}, {"title": "Convergence rates of inexact proximal-gradient methods for convex optimization", "author": ["M. Schmidt", "N. Le Roux", "F. Bach"], "venue": null, "citeRegEx": "Schmidt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2011}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": null, "citeRegEx": "Tieleman,? \\Q2008\\E", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "High dimensional graphical model selection using `1-regularized logistic regression", "author": ["M. Wainwright", "P. Ravikumar", "J. Lafferty"], "venue": null, "citeRegEx": "Wainwright et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2006}, {"title": "On the use of variational inference for learning discrete graphical models", "author": ["E. Yang", "P. Ravikumar"], "venue": null, "citeRegEx": "Yang and Ravikumar,? \\Q2011\\E", "shortCiteRegEx": "Yang and Ravikumar", "year": 2011}, {"title": "Estimation and annealing for Gibbsian fields", "author": ["L. Younes"], "venue": "Annales de l\u2019Institut Henri Poincare\u0301,", "citeRegEx": "Younes,? \\Q1988\\E", "shortCiteRegEx": "Younes", "year": 1988}], "referenceMentions": [{"referenceID": 31, "context": "Therefore a computationally tractable approach is to use sparseness promoting regularizers (Wainwright et al., 2006; Banerjee et al., 2008; H\u00f6fling & Tibshirani, 2009).", "startOffset": 91, "endOffset": 167}, {"referenceID": 2, "context": "Therefore a computationally tractable approach is to use sparseness promoting regularizers (Wainwright et al., 2006; Banerjee et al., 2008; H\u00f6fling & Tibshirani, 2009).", "startOffset": 91, "endOffset": 167}, {"referenceID": 31, "context": "For this reason, several researchers propose exact optimization of approximate objectives, such as `1-regularized logistic regression (Wainwright et al., 2006), greedy optimization of the conditional log-likelihoods (Jalali et al.", "startOffset": 134, "endOffset": 159}, {"referenceID": 17, "context": ", 2006), greedy optimization of the conditional log-likelihoods (Jalali et al., 2011), pseudo-likelihood (Besag, 1975) and a sequence of first-order approximations of the exact log-likelihood (H\u00f6fling & Tibshirani, 2009).", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": ", 2011), pseudo-likelihood (Besag, 1975) and a sequence of first-order approximations of the exact log-likelihood (H\u00f6fling & Tibshirani, 2009).", "startOffset": 27, "endOffset": 40}, {"referenceID": 2, "context": "Several convex upper bounds and approximations to the logpartition function have been proposed for maximum likelihood estimation, such as the log-determinant relaxation (Banerjee et al., 2008), the cardinality bound (El Ghaoui & Gueye, 2008), the Bethe entropy (Lee et al.", "startOffset": 169, "endOffset": 192}, {"referenceID": 19, "context": ", 2008), the cardinality bound (El Ghaoui & Gueye, 2008), the Bethe entropy (Lee et al., 2006; Parise & Welling, 2006), treereweighted approximations and general weighted freeenergy (Yang & Ravikumar, 2011).", "startOffset": 76, "endOffset": 118}, {"referenceID": 13, "context": "The use of stochastic maximum likelihood dates back to (Geyer, 1991; Younes, 1988), in which Markov chain Monte Carlo (MCMC) was used for approximating the gradient.", "startOffset": 55, "endOffset": 82}, {"referenceID": 33, "context": "The use of stochastic maximum likelihood dates back to (Geyer, 1991; Younes, 1988), in which Markov chain Monte Carlo (MCMC) was used for approximating the gradient.", "startOffset": 55, "endOffset": 82}, {"referenceID": 14, "context": "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al.", "startOffset": 216, "endOffset": 230}, {"referenceID": 30, "context": "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al.", "startOffset": 266, "endOffset": 282}, {"referenceID": 26, "context": "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al., 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al.", "startOffset": 298, "endOffset": 344}, {"referenceID": 7, "context": "For restricted Boltzmann machines (a very related graphical model) researchers have proposed a variety of approximation methods, such as variational approximations (Murray & Ghahramani, 2004), contrastive divergence (Hinton, 2002), persistent contrastive divergence (Tieleman, 2008), tempered MCMC (Salakhutdinov, 2009; Desjardins et al., 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al.", "startOffset": 298, "endOffset": 344}, {"referenceID": 27, "context": ", 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 0, "context": ", 2010), adaptive MCMC (Salakhutdinov, 2010) and particle filtering (Asuncion et al., 2010).", "startOffset": 68, "endOffset": 91}, {"referenceID": 21, "context": "Empirical results in (Marlin et al., 2010) suggests that stochastic maximum likelihood is superior to con-", "startOffset": 21, "endOffset": 42}, {"referenceID": 16, "context": "Most work in stochastic optimization assumes the availability of unbiased estimates (Duchi & Singer, 2009; Duchi et al., 2010; Hu et al., 2009; Nemirovski et al., 2009).", "startOffset": 84, "endOffset": 168}, {"referenceID": 23, "context": "Most work in stochastic optimization assumes the availability of unbiased estimates (Duchi & Singer, 2009; Duchi et al., 2010; Hu et al., 2009; Nemirovski et al., 2009).", "startOffset": 84, "endOffset": 168}, {"referenceID": 6, "context": "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d\u2019Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level.", "startOffset": 135, "endOffset": 189}, {"referenceID": 1, "context": "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d\u2019Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level.", "startOffset": 135, "endOffset": 189}, {"referenceID": 9, "context": "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d\u2019Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level.", "startOffset": 135, "endOffset": 189}, {"referenceID": 1, "context": "Additionally, other researchers have analyzed convergence rates in the presence of deterministic errors that do not decrease over time (d\u2019Aspremont, 2008; Baes, 2009; Devolder et al., 2011) and show convergence up to a constant level. Similarly, Devolder (2012) analyzed the case of stochastic errors with fixed bias and variance and show convergence up to a constant level.", "startOffset": 155, "endOffset": 262}, {"referenceID": 28, "context": "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al.", "startOffset": 43, "endOffset": 65}, {"referenceID": 28, "context": "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al.", "startOffset": 43, "endOffset": 95}, {"referenceID": 28, "context": "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al.", "startOffset": 43, "endOffset": 116}, {"referenceID": 28, "context": "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions.", "startOffset": 43, "endOffset": 139}, {"referenceID": 28, "context": "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse models under the `1 regularization. Friedlander & Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value.", "startOffset": 43, "endOffset": 539}, {"referenceID": 28, "context": "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse models under the `1 regularization. Friedlander & Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value. Friedlander & Schmidt (2011) proposes a growing sample-size strategy for approximating the gradient, i.", "startOffset": 43, "endOffset": 686}, {"referenceID": 28, "context": "Notable exceptions are the recent works of Schmidt et al. (2011); Friedlander & Schmidt (2011); Duchi et al. (2011). Schmidt et al. (2011) analyzed proximalgradient (PG) methods for deterministic errors of the gradient that decrease over time, for inexact projection steps and Lipschitz as well as strongly convex functions. In our work, we restrict our analysis to exact projection steps and do not assume strong convexity. Both assumptions are natural for learning sparse models under the `1 regularization. Friedlander & Schmidt (2011) provides convergence rates in expected value for PG with stochastic errors that decrease over time in expected value. Friedlander & Schmidt (2011) proposes a growing sample-size strategy for approximating the gradient, i.e. by picking an increasing number of training samples in order to better approximate the gradient. In contrast, our work is for NP-hard gradients and we provide bounds with high probability, by taking into account the bias and the variance of the errors. Duchi et al. (2011) analyzed mirror descent (a generalization that includes forward-backward splitting) and show convergence rates in expected value and with high probability with respect to the mixing time of the sampling distribution.", "startOffset": 43, "endOffset": 1036}, {"referenceID": 23, "context": "Regarding our contribution in optimization, we provide a convergence-rate analysis of deterministic errors for three different flavors of forward-backward splitting (FBS): robust (Nemirovski et al., 2009), basic and random (Duchi & Singer, 2009).", "startOffset": 179, "endOffset": 204}, {"referenceID": 28, "context": "FBS, but also PG (Schmidt et al., 2011).", "startOffset": 17, "endOffset": 39}, {"referenceID": 31, "context": "Wainwright et al. (2006)), convergence rates of parameter learning for fixed structures is up to now unknown.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "It is well known that, for an Ising model with arbitrary topology, computing the partition function Z is NPhard (Barahona, 1982).", "startOffset": 112, "endOffset": 128}, {"referenceID": 5, "context": "It is also NP-hard to approximate Z with high probability and arbitrary precision (Chandrasekaran et al., 2008).", "startOffset": 82, "endOffset": 111}, {"referenceID": 31, "context": "We replace the cardinality penalty by the `1norm regularizer as in (Wainwright et al., 2006; Banerjee et al., 2008; H\u00f6fling & Tibshirani, 2009).", "startOffset": 67, "endOffset": 143}, {"referenceID": 2, "context": "We replace the cardinality penalty by the `1norm regularizer as in (Wainwright et al., 2006; Banerjee et al., 2008; H\u00f6fling & Tibshirani, 2009).", "startOffset": 67, "endOffset": 143}, {"referenceID": 31, "context": "In this paper, we follow the original formulation of the problem given in (Wainwright et al., 2006; Banerjee et al., 2008; H\u00f6fling & Tibshirani, 2009), which does not regularize b.", "startOffset": 74, "endOffset": 150}, {"referenceID": 2, "context": "In this paper, we follow the original formulation of the problem given in (Wainwright et al., 2006; Banerjee et al., 2008; H\u00f6fling & Tibshirani, 2009), which does not regularize b.", "startOffset": 74, "endOffset": 150}, {"referenceID": 29, "context": "SVMs (Shalev-Shwartz et al., 2007).", "startOffset": 5, "endOffset": 34}, {"referenceID": 5, "context": "plexity results in (Chandrasekaran et al., 2008) imply that approximating those gradients with high probability and arbitrary precision is also NP-hard.", "startOffset": 19, "endOffset": 48}, {"referenceID": 25, "context": "In what follows, we characterize a family of samplers that includes importance sampling and MCMC as shown in (Peskun, 1973; Liu, 2001).", "startOffset": 109, "endOffset": 134}, {"referenceID": 20, "context": "In what follows, we characterize a family of samplers that includes importance sampling and MCMC as shown in (Peskun, 1973; Liu, 2001).", "startOffset": 109, "endOffset": 134}, {"referenceID": 25, "context": "The theoretical analysis implies that such constants B and V exist (Peskun, 1973; Liu, 2001) for importance sampling and MCMC.", "startOffset": 67, "endOffset": 92}, {"referenceID": 20, "context": "The theoretical analysis implies that such constants B and V exist (Peskun, 1973; Liu, 2001) for importance sampling and MCMC.", "startOffset": 67, "endOffset": 92}, {"referenceID": 5, "context": "Note that Definition 4 does not contradict the complexity results in (Chandrasekaran et al., 2008) that show that it is likely impossible to approximate Z (and therefore its gradient) with probability greater than 1 \u2212 \u03b4 and arbitrary precision \u03b5 in time polynomial in log 1\u03b4 and 1 \u03b5 .", "startOffset": 69, "endOffset": 98}, {"referenceID": 5, "context": "Therefore, Definition 4 cannot be used to obtain polynomial-time algorithms as the ones considered in (Chandrasekaran et al., 2008).", "startOffset": 102, "endOffset": 131}, {"referenceID": 28, "context": "This method is equivalent to basic proximal gradient (Schmidt et al., 2011) for r = 0 (constant step size).", "startOffset": 53, "endOffset": 75}, {"referenceID": 23, "context": "In what follows, we analyze three different flavors of forward-backward splitting: robust which outputs the weighted average of all visited points by using the step sizes as in robust stochastic approximation (Nemirovski et al., 2009), basic which outputs the average of all visited points as in (Duchi & Singer, 2009), or random which outputs a point chosen uniformly at random from the visited points.", "startOffset": 209, "endOffset": 234}, {"referenceID": 28, "context": "The convergence rates in Theorems 6, 7 and 8 lead to an error term A\u03b3,\u03be that is linear, while the error term is quadratic in the analysis of proximal gradient (Schmidt et al., 2011).", "startOffset": 159, "endOffset": 181}, {"referenceID": 28, "context": "As noted in (Schmidt et al., 2011), errors have a greater effect on the accelerated method than on the basic method.", "startOffset": 12, "endOffset": 34}, {"referenceID": 9, "context": "This observation suggests that, unlike in the error-free case, accelerated PG is not necessarily better than the basic method due to a higher sensitivity to errors (Devolder et al., 2011).", "startOffset": 164, "endOffset": 187}, {"referenceID": 28, "context": "The analysis of Schmidt et al. (2011) for deterministic errors implies that in order to have convergence, Table 1.", "startOffset": 16, "endOffset": 38}, {"referenceID": 28, "context": "For an informal (and incomplete) analysis of the results in (Schmidt et al., 2011) for biased stochastic optimization, consider each error bounded by its bias and variance \u2016\u03be\u20162 \u2264 B/Sk+c \u221a V/Sk for some c > 0 and an increasing number of random samples Sk that allows to obtain decreasing errors.", "startOffset": 60, "endOffset": 82}, {"referenceID": 31, "context": "We also include a two-step algorithm, by first learning the structure by `1-regularized logistic regression (Wainwright et al., 2006) and then learning the parameters by using FBS with belief propagation for gradient approximation.", "startOffset": 108, "endOffset": 133}], "year": 2012, "abstractText": "We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting.", "creator": " TeX output 2012.05.22:0015"}}}