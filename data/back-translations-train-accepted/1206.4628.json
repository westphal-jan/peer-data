{"id": "1206.4628", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Robust PCA in High-dimension: A Deterministic Approach", "abstract": "We consider principal component analysis for contaminated data-set in the high dimensional regime, where the dimensionality of each observation is comparable or even more than the number of observations. We propose a deterministic high-dimensional robust PCA algorithm which inherits all theoretical properties of its randomized counterpart, i.e., it is tractable, robust to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness -- a breakdown point of 50%. More importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large-scale real applications.", "histories": [["v1", "Mon, 18 Jun 2012 15:07:55 GMT  (699kb)", "http://arxiv.org/abs/1206.4628v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jiashi feng", "huan xu", "shuicheng yan"], "accepted": true, "id": "1206.4628"}, "pdf": {"name": "1206.4628.pdf", "metadata": {"source": "META", "title": "Robust PCA in High-dimension: A Deterministic Approach", "authors": ["Jiashi Feng", "Huan Xu", "Shuicheng Yan"], "emails": ["a0066331@nus.edu.sg", "mpexuh@nus.edu.sg", "eleyans@nus.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Related Work", "text": "The aforementioned hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary heretical hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary heredity hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary) hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary) hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary hereditary her"}, {"heading": "3. The Algorithm", "text": "In this section we will first formally explain the problem of the high-dimensional robust PCA. Then we will present the details of the proposed DHR-PCA algorithm and finally present the most important theoretical results on the performance guarantees of the algorithm."}, {"heading": "3.1. Problem Setup", "text": "In this subsection, we present the formal problem description of PCA for the high-dimensional data with contamination. Our setup, described below for the sake of completeness, largely follows that of Xu and al. (2010a). Given the n observations, there are t observations that are not corrupted, which are called authentic samples. The authentic samples zi-Rm are generated by a linear mapping: zi = Axi + ni. Here, noise ni of the normal distribution N (0, Im) is sampled; and the signal xi-Rd are i.e. samples of a random variable x with mean zero and variance Id. The matrix A-Rm \u00b7 d and the distribution of x are unknown. We assume that the number is absolutely continuous."}, {"heading": "3.2. Deterministic HR-PCA Algorithm", "text": "It is not to be expected that the deviation is large, that a correct main component is found, that the number of points with the greatest variance can be corrupted. Notice that the RVE is always set to the original observed quantity Y. We find that the RVE matches the robust L estimator, which is defined as the linear combination of the sequence: The RVE is always set to the original quantity Y. We find that the RVE matches the robust L estimator, which is defined as the linear combination of the sequence."}, {"heading": "4. Simulations", "text": "We dedicate this section experimentally to comparing the proposed DHR-PCA with HR-PCA = DHA. Since HRPCA has demonstrated superior robustness (against the dimensionality and number of outliers) over several robust PCA algorithms and standard PCA (Xu et al., 2010a), we skip simulations for them here.The numerical study aims to show that DHRPCA is much more efficient than HR-PCA, and now it achieves a competitive performance. We follow the data generation method in (Xu et al., 2010a) to randomly perform an m \u00b7 1 matrix and then generate its leading singular value towards a line with outliers with a uniform distribution over [\u2212 gap \u00b7 mag \u00b7 mag \u00b7 mag].mag \"thus represents the ratio between the size of the outliers and that of signal 10.. The value of the outliers is fixed."}, {"heading": "5. Proof of Theorem 1", "text": "In this section, we will outline the proof for theorem 1. In the following, let us fix d, m / n, \u03bb, t, and \u00b5. We can fix a \u03bb (0, 0.5) w.l.o.g. on the basis of the fact that if it is shown that a result is valid for \u03bb, it is also valid for \u03bb. The letter c is used to represent a constant, and is a constant that sinks to zero infinitely at n and m increments. Let us leave w1 (s),... wd (s) the candidate solution at stage s. Let us let Z and O be the indices of authentic samples or corrupted samples. Let us let Bd, {w, w, Rd, w, and Sd be its limit. Here, theorems 3 and 4 are directly adapted (Xu et al., 2010a)."}, {"heading": "5.1. Validity of the Robust Variance Estimator", "text": "First, we show that the following condition is most likely to apply."}, {"heading": "5.2. Finite Steps for a Good Solution", "text": "In this step, we show that the algorithm finds a good solution in a small number of steps. To prove this means to show that the algorithm either finds a good solution in each step, or the weight adjustment step reduces the weight of corrupt points more than the authentic points. Let \u03b1 (s) i specify the weight of the ith data point in the sth stage. These points include a good solution if the variance of the points projected on their span is mainly due to the authentic samples and not to the corrupt points. We call this \"good output event in step s\" by E (s), defined as: E (s) = {\u2211 i \u00b2 Z (s) i vi (s) i vi (s) i vi (s). The intuition is that there cannot be too many steps without finding a good solution."}, {"heading": "5.3. Bounds on the Solution Performance", "text": "The final part of the proof for these two main steps is the first step that we have completed, namely the optimal solution, w \u00b2 1,.. \u2212 \u2212 \u2212 d The output of the algorithm 1 and w1 (s),..., wd (s) the candidate solution at the stage s. We define H (w1,. \u2212 \u2212 w), d = 1, wTj A 2, and for notational simplification, leave H (w \u00b2 1,.., w \u00b2 d), Hs, H (w1), H (w1),. \u2212 wd (s), and H (w \u00b2 1, w \u00b2), w \u00b2 d). The statement of the finite sample and asymptotic theorems (theorem 1 and theorem 2, each lower than the expressed variance, E.V., which is the ratio H \u00b2 d."}, {"heading": "6. Conclusions", "text": "In this paper, we proposed a deterministic robust PCA algorithm for high-dimensional data corrupted by random outliers, alternating between a classical PCA and a reduction in weight coefficients at all data points. Theoretical analyses showed that the proposed algorithm is traceable, robust against corrupt points, easily nucleated, asymptotically consistent and reaches a maximum breakpoint of 50% - to the best of our knowledge the first deterministic algorithm to achieve these properties in a high-dimensional setup. More importantly, simulation results showed that the proposed algorithm improves computing efficiency over its randomized counterpart HR-PCA - in fact, the number of iterations needed to find a satisfactory solution appears to be constant, in sharp contrast to HR-PCA, whose number of iterations increases linearly with the number of outliers."}, {"heading": "Acknowledgements", "text": "H. Xu is partially supported by the National University of Singapore's R-265-000-384-133 Startup Scholarship. S. Yan is partially supported by the Singapore Ministry of Education under the MOE2010-T2-1-087 Research Scholarship."}], "references": [{"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": null, "citeRegEx": "Candes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2009}, {"title": "A fast algorithm for robust principal components based on projection pursuit", "author": ["C. Croux", "A. Ruiz-Gazen"], "venue": "In COMPSTAT: Proceedings in computational statistics,", "citeRegEx": "Croux and Ruiz.Gazen,? \\Q1996\\E", "shortCiteRegEx": "Croux and Ruiz.Gazen", "year": 1996}, {"title": "High breakdown estimators for principal components: the projection-pursuit approach revisited", "author": ["C. Croux", "A. Ruiz-Gazen"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Croux and Ruiz.Gazen,? \\Q2005\\E", "shortCiteRegEx": "Croux and Ruiz.Gazen", "year": 2005}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L. Ghaoui"], "venue": null, "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Breakdown properties of multivariate location estimators", "author": ["D.L. Donoho"], "venue": "Technical report,", "citeRegEx": "Donoho,? \\Q1982\\E", "shortCiteRegEx": "Donoho", "year": 1982}, {"title": "High-dimensional data analysis: The curses and blessings of dimensionality", "author": ["D.L. Donoho"], "venue": "AMS Math Challenges Lecture,", "citeRegEx": "Donoho,? \\Q2000\\E", "shortCiteRegEx": "Donoho", "year": 2000}, {"title": "A fast method for robust principal components with applications to chemometrics", "author": ["M. Hubert", "P.J. Rousseeuw", "S. Verboven"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Hubert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hubert et al\\.", "year": 2002}, {"title": "Robpca: a new approach to robust principal component analysis", "author": ["M. Hubert", "P.J. Rousseeuw", "K.V. Branden"], "venue": null, "citeRegEx": "Hubert et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hubert et al\\.", "year": 2005}, {"title": "Projection-pursuit approach to robust dispersion matrices and principal components: primary theory and monte carlo", "author": ["G. Li", "Z. Chen"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li and Chen,? \\Q1985\\E", "shortCiteRegEx": "Li and Chen", "year": 1985}, {"title": "Robust m-estimators of multivariate location and scatter", "author": ["R.A. Maronna"], "venue": "The annals of statistics,", "citeRegEx": "Maronna,? \\Q1976\\E", "shortCiteRegEx": "Maronna", "year": 1976}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": null, "citeRegEx": "Pearson,? \\Q1901\\E", "shortCiteRegEx": "Pearson", "year": 1901}, {"title": "Least median of squares regression", "author": ["P.J. Rousseeuw"], "venue": "Journal of the American statistical association,", "citeRegEx": "Rousseeuw,? \\Q1984\\E", "shortCiteRegEx": "Rousseeuw", "year": 1984}, {"title": "Robust regression and outlier detection", "author": ["P.J. Rousseeuw", "A.M. Leroy"], "venue": null, "citeRegEx": "Rousseeuw and Leroy,? \\Q1987\\E", "shortCiteRegEx": "Rousseeuw and Leroy", "year": 1987}, {"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.R. M\u00fcller"], "venue": "Artificial Neural Networks,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1997}, {"title": "Principal component analysis with contaminated data: The high dimensional case", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "In COLT,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "PCA is one of the most widely used data analysis methods (Pearson, 1901).", "startOffset": 57, "endOffset": 72}, {"referenceID": 5, "context": "Analyzing high dimensional data \u2013 data sets where the dimensionality of each observation is comparable to or even larger than the number of observations \u2013 has become a critical task in modern statistics and machine learning (Donoho, 2000).", "startOffset": 224, "endOffset": 238}, {"referenceID": 14, "context": "The work of Xu et al. (2010a) is among the first to analyze robust PCA algorithms in the high-dimensional setup.", "startOffset": 12, "endOffset": 30}, {"referenceID": 7, "context": "Robust PCA algorithms focusing on the low-dimensional setup (e.g., Rousseeuw, 1984; Croux & Ruiz-Gazen, 2005; Hubert et al., 2005) can be roughly categorized into two groups.", "startOffset": 60, "endOffset": 130}, {"referenceID": 9, "context": ", M -estimator (Maronna, 1976), S-estimator (Rousseeuw & Leroy, 1987), and Minimum Covariance Determinant (MCD) estimator (Rousseeuw, 1984).", "startOffset": 15, "endOffset": 30}, {"referenceID": 11, "context": ", M -estimator (Maronna, 1976), S-estimator (Rousseeuw & Leroy, 1987), and Minimum Covariance Determinant (MCD) estimator (Rousseeuw, 1984).", "startOffset": 122, "endOffset": 139}, {"referenceID": 6, "context": "The second group of algorithms directly maximize certain robust estimation of univariate variance for the projected observations and then obtain maximizers as the candidate principal components (Li & Chen, 1985; Croux & Ruiz-Gazen, 1996; 2005; Hubert et al., 2002).", "startOffset": 194, "endOffset": 264}, {"referenceID": 9, "context": ", M -estimator (Maronna, 1976), in the highdimensional regime their breakdown points will diminish and the results will be arbitrarily bad in presence of even few outliers.", "startOffset": 15, "endOffset": 30}, {"referenceID": 4, "context": "Second, widely used outlyingness indicators, including Mahalanobis distance and StahelDonoho outlyingness (Donoho, 1982) are no longer valid, due to a phenomenon termed \u2013 \u201cnoise explosion\u201d (Xu et al.", "startOffset": 106, "endOffset": 120}, {"referenceID": 7, "context": "This causes the algorithms relying on such outlyingness measures (Hubert et al., 2005) to collapse.", "startOffset": 65, "endOffset": 86}, {"referenceID": 11, "context": "The third problem is that the dimensionality may be larger than the number of data points and thus some robust estimators including Minimum Volume Ellipsoid (MVE) and Minimum Covariance Determinant (MCD) (Rousseeuw, 1984) become degenerated.", "startOffset": 204, "endOffset": 221}, {"referenceID": 0, "context": "(Candes et al., 2009) developed a framework to perform robust PCA using low-rank matrix decomposition.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Candes et al., 2009) developed a framework to perform robust PCA using low-rank matrix decomposition. Yet, their method focuses on the scenario that random entries of the observation matrix are arbitrarily corrupted, which differs from our setup where one corrupted data point may change the whole column of the observation matrix. The later setup is then investigated in Xu et al. (2010b). While their proposed method performs well under a small fraction of outliers, it breaks down for larger fraction of outliers \u2013 in particular, the breakdown point is far from 50%.", "startOffset": 1, "endOffset": 391}, {"referenceID": 14, "context": "Our setup, detailed below for completeness, largely follows that of Xu et al. (2010a).", "startOffset": 68, "endOffset": 86}, {"referenceID": 3, "context": "is a commonly used evaluation metric for the PCA algorithms (Xu et al., 2010a; d\u2019Aspremont et al., 2008).", "startOffset": 60, "endOffset": 104}, {"referenceID": 3, "context": ", 2010a; d\u2019Aspremont et al., 2008). It is always less than one, with equality achieved by a perfect recovery, i.e., the vectors w1, . . . ,wd have the same span as the true principal components {w\u03041, . . . , w\u0304d}. The distribution \u03bc affects the performance of the algorithms through its tail. We hence adapt the following tail weight function V : [0, 1] \u2192 [0, 1] from Xu et al. (2010a), which essentially represents how the tail of \u03bc\u0304 contributes to its variance,", "startOffset": 9, "endOffset": 386}, {"referenceID": 14, "context": "It has been shown in Xu et al. (2010a) that in expectation (and in probability), either the number of outliers will decrease faster, or the algorithm will find a good solution.", "startOffset": 21, "endOffset": 39}, {"referenceID": 13, "context": "satisfying k(x,y) = \u3008\u03c6(x), \u03c6(y)\u3009 for all x,y \u2208 R, we can perform dimension reduction without requiring the explicit form of \u03c6(\u00b7) in the kernel PCA (Sch\u00f6lkopf et al., 1997).", "startOffset": 147, "endOffset": 171}, {"referenceID": 14, "context": "We compare Theorem 5 with its randomized counterpart, Theorem 9 of Xu et al. (2010a). The latter states that for HR-PCA, E(s) succeeds with high probability for some s \u2264 (1 + )(1 + \u03ba)\u03bbn/\u03ba, where depends on \u03ba and \u03bb, and decreases to 0 when n \u2191 \u221e (for fixed \u03ba and \u03bb).", "startOffset": 67, "endOffset": 85}], "year": 2012, "abstractText": "We consider principal component analysis for contaminated data-set in the high dimensional regime, where the dimensionality of each observation is comparable or even more than the number of observations. We propose a deterministic high-dimensional robust PCA algorithm which inherits all theoretical properties of its randomized counterpart, i.e., it is tractable, robust to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness \u2013 a breakdown point of 50%. More importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large-scale real applications.", "creator": "LaTeX with hyperref package"}}}