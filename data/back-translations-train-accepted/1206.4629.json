{"id": "1206.4629", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Multiple Kernel Learning from Noisy Labels by Stochastic Programming", "abstract": "We study the problem of multiple kernel learning from noisy labels. This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples. Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem, and presenting a minimax formulation. We develop an efficient algorithm for solving the related convex-concave optimization problem with a fast convergence rate of $O(1/T)$ where $T$ is the number of iterations. Empirical studies on UCI data sets verify both the effectiveness of the proposed framework and the efficiency of the proposed optimization algorithm.", "histories": [["v1", "Mon, 18 Jun 2012 15:08:22 GMT  (216kb)", "http://arxiv.org/abs/1206.4629v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tianbao yang", "mehrdad mahdavi", "rong jin", "lijun zhang 0005", "yang zhou"], "accepted": true, "id": "1206.4629"}, "pdf": {"name": "1206.4629.pdf", "metadata": {"source": "META", "title": "Multiple Kernel Learning from Noisy Labelsby Stochastic Programming", "authors": ["Tianbao Yang", "Mehrdad Mahdavi", "Rong Jin", "Lijun Zhang", "Yang Zhou"], "emails": ["yangtia1@msu.edu", "mahdavim@msu.edu", "rongjin@msu.edu", "zljzju@zju.edu.cn", "young.zhou@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "Multiple Kernel Learning (MKL) (Lanckriet et al., 2004) has attracted a significant amount of interest in both machine learning and data mining communities due to the success of the kernel methods (Schoolhead & Smola, 2001). MKL aims to learn an optimal combination of multiple cores and a nonlinear classifier from the reproducing kernel Hilbert Appearing in the proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by author (s).Raum (RKHS), published with the combined kernel Hilbert Appearing in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Most of the previous studies on MKL focus on designing efficient algorithms to solve the associated optimization problem. Research has also been conducted to examine the effect of different regulators on more complex combinations, including more economical ones."}, {"heading": "2. Related Work", "text": "Our work is closely related to MKL. Various criteria have been developed to find the optimal core combination, such as maximum classification range (Lanckriet et al., 2004) and core alignment (Christianini et al., 2002; Cortes et al., 2010a). Among them, maximum margin MKL receives the most attention due to its empirical success and close relationship to support vector machines (SVMs). Many algorithms have been developed for maximum margin MKL by formulating the problem in SemiDefinite Programming (Lanckriet et al., 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high compression costs, these approaches are unable to handle large data sets and a large number of cores. A number of efficient algorithms based on alternating optimization has been proposed for ML."}, {"heading": "3. Multiple Kernel Learning From Noisy Labels", "text": "We first review a formulation of MKL based on the equivalence between MKL and Group Lasso (Xu et al., 2010; Bach, 2008), then describe the problem of MKL from noisy labels and present a stochastic programming framework to formulate it. Finally, we present an efficient algorithm to solve the associated convex concave optimization problem."}, {"heading": "3.1. Multiple Kernel Learning (MKL)", "text": "Let us be D = (xi, yi), i = 1, \u00b7 \u00b7, n = the training data, where xi-Rd denotes the ith instance and yi-Rd-1, 1} its binary name. We use y = (y1, \u00b7 \u00b7, yn) to represent the class assignment of all training examples. We use {\u03baj (\u00b7, \u00b7): Rd \u00b7 Rd 7 \u2192 R, j [m] the set of m-cores to be combined, and with H\u03baj the corresponding reproduction of the Hilbert Space Kernel (RKHS) equipped with \u03baj. We use u = (u1, \u00b7 \u00b7 \u00b7, to) the combination weights of the multiple cores to be combined, and with H\u03bau the corresponding reproduction of the Hilbert Space Kernel (RKHS)."}, {"heading": "3.2. A Stochastic Programming Framework for MKL from Noisy Labels", "text": "The key challenge is that we do not know which examples are incorrectly labeled. (1) To facilitate learning from the noisy labels, we assume that the noise level of the class assignments q (0, 1 / 2) will be a binary random variable that indicates whether the expected probability for any randomly selected example is incorrectly labeled. For a particular pair (x, y), we leave the probability for a correct label of x (1) or not (0), and p (x, y) = the probability for a correct label of x.Assumption 1. The noise level q is given, i.e., q = 1 \u2212 E (x, y).With Assumption 1, we set a correct label of x.Assumption 1, i.e."}, {"heading": "3.3. An Efficient Optimization Algorithm", "text": "In this section, we will introduce an efficient algorithm for solving the Minimax problem in (10). (We will first introduce an alternative formula for (10), i.e., min {fj} mj = 1 max \u00b2 Q\u03b1\u03bb2 m \u00b2 j = 1 x \u00b2 (1), where Q3 (1), where Q3 (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, 2, (2), (2, 2, 2, (2, 2, 2,), (2, (2), (2, 2, 2, 2, 2, 2, 2, (2), (2), (2, 2, (2), (2), (2), (2, (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2, (2, (2), (2), (2), (2), (2, (2), (2, (2), (2, (2), (2), (2, (2), (2), (2, (2), (2), (2, (2), (2, (2), (2), (), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2"}, {"heading": "4. Experiments", "text": "In this section, we simulate our experiments on UCI data sets to verify the effectiveness and efficiency of the proposed algorithms for MKL of noise labels. We select five sets of data from UCI repositories used in MKL studies to better understand the behavior of the proposed algorithm. Data set statistics are summarized in Table 1. We normalize the data by scaling each attribute to [0, 1], first subtracting each attribute from its minimum value, and then dividing it by the difference between the maximum and minimum value of the attribute. To generate label noise, we mirror the class label of each example with a probability of q."}, {"heading": "5. Conclusions", "text": "In this paper, we present a stochastic programming system for learning multiple cores from loud labels. We formulate the problem into a convex-concave optimization problem. We also present an efficient method for solving the associated convex-convex problem. Empirical studies in a simulated environment verify the effectiveness of the proposed frame and the efficiency of the optimization algorithm developed. For future work, we plan to apply the proposed approach to real problems with inherent noise in labels where noise may not be generated synthetically by independent random flipping. A related open problem would be how to gain knowledge of the noise level."}, {"heading": "Acknowledgement", "text": "This work is supported in part by the National Science Foundation (IIS-0643494), Office of Navy Research (ONR Award N00014-09-1-0663 and N0001412-1-0431)."}], "references": [{"title": "Consistency of the group lasso and multiple kernel learning", "author": ["Bach", "Francis R"], "venue": "JMLR, 9:1179\u20131225,", "citeRegEx": "Bach and R.,? \\Q2008\\E", "shortCiteRegEx": "Bach and R.", "year": 2008}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["Bach", "Francis R", "Lanckriet", "Gert R. G", "Jordan", "Michael I"], "venue": "In ICML,", "citeRegEx": "Bach et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2004}, {"title": "Robust Optimization", "author": ["Ben-Tal", "Aharon", "Ghaoui", "Laurent El", "Nemirovski", "Arkadi"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "Robust formulations for handling uncertainty in kernel matrices", "author": ["Bhadra", "Sahely", "Bhattacharya", "Sourangshu", "Bhattacharyya", "Chiranjib", "Ben-Tal", "Aharon"], "venue": "In ICML, pp", "citeRegEx": "Bhadra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bhadra et al\\.", "year": 2010}, {"title": "Support vector machines under adversarial label noise", "author": ["Biggio", "Battista", "Nelson", "Blaine", "Laskov", "Pavel"], "venue": "In ACML, pp", "citeRegEx": "Biggio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2011}, {"title": "Concentration Inequalities, pp. 208\u2013240", "author": ["S. Boucheron", "O. Bousquet", "G. Lugosi"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2004}, {"title": "L2 regularization for learning kernels", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "In UAI, pp", "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Two-stage learning kernel algorithms", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "In ICML, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Generalization bounds for learning kernels", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "In ICML, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "On kernel-target alignment", "author": ["Cristianini", "Nello", "Kandola", "Jaz", "Elisseeff", "Andre", "Shawe-Taylor", "John"], "venue": "In NIPS,", "citeRegEx": "Cristianini et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2002}, {"title": "Support vector machines on a budget", "author": ["Dekel", "Ofer", "Singer", "Yoram"], "venue": "In NIPS, pp", "citeRegEx": "Dekel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2006}, {"title": "Robust metric learning by smooth optimization", "author": ["Huang", "Kaizhu", "Jin", "Rong", "Xu", "Zenglin", "Liu", "ChengLin"], "venue": "In UAI,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Efficient and accurate lp-norm multiple kernel learning", "author": ["Kloft", "Marius", "Brefeld", "Ulf", "Sonnenburg", "Soeren", "Laskov", "Pavel", "M\u00fcller", "Klaus-Robert", "Zien", "Alexander"], "venue": "In NIPS,", "citeRegEx": "Kloft et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2009}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["Lanckriet", "Gert", "Cristianini", "Nello", "Bartlett", "Peter", "Ghaoui", "Laurent E"], "venue": "JMLR, 5:27\u201372,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["Lawrence", "Neil D", "Sch\u00f6lkopf", "Bernhard"], "venue": "In ICML, pp", "citeRegEx": "Lawrence et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2001}, {"title": "Learning the kernel function via regularization", "author": ["Micchelli", "Charles A", "Pontil", "Massimiliano"], "venue": null, "citeRegEx": "Micchelli et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Micchelli et al\\.", "year": 2005}, {"title": "Efficient methods in convex programming", "author": ["A. Nemirovski"], "venue": null, "citeRegEx": "Nemirovski,? \\Q1994\\E", "shortCiteRegEx": "Nemirovski", "year": 1994}, {"title": "Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["Nemirovski", "Arkadi"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski and Arkadi.,? \\Q2005\\E", "shortCiteRegEx": "Nemirovski and Arkadi.", "year": 2005}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Nesterov", "Yu"], "venue": "Technical report,", "citeRegEx": "Nesterov and Yu.,? \\Q2007\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2007}, {"title": "Putting semantic information extraction on the map: Noisy label models for fact extraction", "author": ["Pal", "Chris", "Mann", "Gideon", "Minerich", "Richard"], "venue": "In AAAI Workshop on Information Integration on the Web,", "citeRegEx": "Pal et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pal et al\\.", "year": 2007}, {"title": "A model for handling approximate, noisy or incomplete labeling in text classification", "author": ["Ramakrishnan", "Ganesh", "Chitrapura", "Krishna Prasad", "Krishnapuram", "Raghu", "Bhattacharyya", "Pushpak"], "venue": "In ICML,", "citeRegEx": "Ramakrishnan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ramakrishnan et al\\.", "year": 2005}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": null, "citeRegEx": "Robbins and Monro,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "A general and efficient multiple kernel learning algorithm", "author": ["Sonnenburg", "S\u00f6ren", "R\u00e4tsch", "Gunnar", "Sch\u00e4fer", "Christin"], "venue": "In NIPS, pp", "citeRegEx": "Sonnenburg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["Tseng", "Paul"], "venue": "Technical report,", "citeRegEx": "Tseng and Paul.,? \\Q2008\\E", "shortCiteRegEx": "Tseng and Paul.", "year": 2008}, {"title": "Robust support vector machine training via convex outlier ablation", "author": ["Xu", "Linli", "Crammer", "Koby", "Schuurmans", "Dale"], "venue": "In AAAI,", "citeRegEx": "Xu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2006}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Xu", "Zenglin", "Jin", "Rong", "Yang", "Haiqin", "King", "Irwin", "Lyu", "Michael R"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Learning from noisy side information by generalized maximum entropy model", "author": ["Yang", "Tianbao", "Jin", "Rong", "Jain", "Anil K"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Generalization bounds for learning the kernel problem", "author": ["Ying", "Yiming", "Campbell", "Colin"], "venue": "In COLT,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}, {"title": "Relaxed clipping: A global training method for robust regression and classification", "author": ["Yu", "Yaoliang", "Yang", "Min", "Xu", "Linli", "White", "Martha", "Schuurmans", "Dale"], "venue": "In NIPS,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "Multiple Kernel Learning (MKL) (Lanckriet et al., 2004) has attracted a significant amount of interest in both machine learning and data mining communities due to the success of kernel methods (Sch\u00f6lkopf & Smola, 2001).", "startOffset": 31, "endOffset": 55}, {"referenceID": 27, "context": ", hyperlink information (Yang et al., 2010)).", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "Using introduced binary random variables, we turn the deterministic constraint in MKL into a chance constraint (Ben-Tal et al., 2009), leading to a stochastic programming formulation.", "startOffset": 111, "endOffset": 133}, {"referenceID": 19, "context": "Unlike many previous studies (Lawrence & Sch\u00f6lkopf, 2001; Pal et al., 2007; Yang et al., 2010) on learning with noisy labeled data that make strong assumptions about the noise model (e.", "startOffset": 29, "endOffset": 94}, {"referenceID": 27, "context": "Unlike many previous studies (Lawrence & Sch\u00f6lkopf, 2001; Pal et al., 2007; Yang et al., 2010) on learning with noisy labeled data that make strong assumptions about the noise model (e.", "startOffset": 29, "endOffset": 94}, {"referenceID": 4, "context": "Notably, we do NOT assume the label noise of different examples are independent, a common assumption shared by most existing studies on learning from noisy labels (Biggio et al., 2011; Yang et al., 2010).", "startOffset": 163, "endOffset": 203}, {"referenceID": 27, "context": "Notably, we do NOT assume the label noise of different examples are independent, a common assumption shared by most existing studies on learning from noisy labels (Biggio et al., 2011; Yang et al., 2010).", "startOffset": 163, "endOffset": 203}, {"referenceID": 13, "context": "Various criteria have been developed to find the optimal kernel combination, such as maximum classification margin (Lanckriet et al., 2004) and kernel alignment (Cristianini et al.", "startOffset": 115, "endOffset": 139}, {"referenceID": 9, "context": ", 2004) and kernel alignment (Cristianini et al., 2002; Cortes et al., 2010a).", "startOffset": 29, "endOffset": 77}, {"referenceID": 13, "context": "Many algorithms have been developed for maxmargin MKL by formulating the problem into SemiDefinite Programming (Lanckriet et al., 2004), Second Order Cone Programming (Bach et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 23, "context": ", 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006).", "startOffset": 46, "endOffset": 71}, {"referenceID": 26, "context": "A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010).", "startOffset": 96, "endOffset": 141}, {"referenceID": 26, "context": "Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al.", "startOffset": 105, "endOffset": 150}, {"referenceID": 6, "context": ", 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al.", "startOffset": 17, "endOffset": 38}, {"referenceID": 12, "context": ", 2009), and lp norm (Kloft et al., 2009).", "startOffset": 21, "endOffset": 41}, {"referenceID": 11, "context": "Several recent studies (Huang et al., 2010) address the limitation of probabilistic approaches by exploring the robust optimization (Ben-Tal et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 2, "context": ", 2010) address the limitation of probabilistic approaches by exploring the robust optimization (Ben-Tal et al., 2009).", "startOffset": 96, "endOffset": 118}, {"referenceID": 25, "context": "Our study is particularly related to the recent work on robust SVM (Xu et al., 2006; Yu et al., 2011) in which a SVM classifier is learned in the presence of outliers.", "startOffset": 67, "endOffset": 101}, {"referenceID": 29, "context": "Our study is particularly related to the recent work on robust SVM (Xu et al., 2006; Yu et al., 2011) in which a SVM classifier is learned in the presence of outliers.", "startOffset": 67, "endOffset": 101}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels.", "startOffset": 40, "endOffset": 691}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels. Pal et al. (2007) present a probabilistic model for extracting location information for events with noisy training labels.", "startOffset": 40, "endOffset": 800}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels. Pal et al. (2007) present a probabilistic model for extracting location information for events with noisy training labels. Ramakrishnan et al. (2005) propose a Bayeisan model for learning with approximate, noisy or incomplete labels.", "startOffset": 40, "endOffset": 932}, {"referenceID": 1, "context": ", 2004), Second Order Cone Programming (Bach et al., 2004), and Semi-Infinite Linear Programming (Sonnenburg et al., 2006). Due to their high computational cost, these approaches are unable to handle large data sets and a large number of kernels. A number of efficient algorithms, based on alternating optimization, have been proposed for MKL (Rakotomamonjy et al., 2008; Xu et al., 2010). Besides efficient learning algorithms, various regularizers have been studied for MKL, including l1 norm (Rakotomamonjy et al., 2008; Xu et al., 2010), l2 norm (Cortes et al., 2009), and lp norm (Kloft et al., 2009). Our work is also related to learning with noisy labels. Lawrence & Sch\u00f6lkopf (2001) propose a probabilistic model for learning a kernel Fisher Discriminant from noisy labels. Pal et al. (2007) present a probabilistic model for extracting location information for events with noisy training labels. Ramakrishnan et al. (2005) propose a Bayeisan model for learning with approximate, noisy or incomplete labels. Yang et al. (2010) propose a generalized maximum entropy model for learning from noisy side information.", "startOffset": 40, "endOffset": 1035}, {"referenceID": 26, "context": "We first review a formulation of MKL based on the equivalence between MKL and group Lasso (Xu et al., 2010; Bach, 2008).", "startOffset": 90, "endOffset": 119}, {"referenceID": 5, "context": "The proposition follows directly from the Hoeffding\u2019s inequality (Boucheron et al., 2004).", "startOffset": 65, "endOffset": 89}, {"referenceID": 2, "context": ", \u03ben), where \u03bei = \u03be(xi, yi), we relax the deterministic constraint in (4) into a chance constraint (Ben-Tal et al., 2009)", "startOffset": 99, "endOffset": 121}, {"referenceID": 3, "context": "In (Bhadra et al., 2010), the authors introduce the chance constraints to handle the uncertainty in the elements of a kernel matrix, while the chance constraint in (5) is introduced to handle the uncertainty in the class labels.", "startOffset": 3, "endOffset": 24}, {"referenceID": 5, "context": "The proof follows immediately from the McDiarmid inequality (Boucheron et al., 2004).", "startOffset": 60, "endOffset": 84}, {"referenceID": 2, "context": "The idea of using the worst case analysis is closely related to robust optimization (Ben-Tal et al., 2009), which has been adopted by several recent studies, including budget SVM (Dekel & Singer, 2006), and robust metric learning (Huang et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 11, "context": ", 2009), which has been adopted by several recent studies, including budget SVM (Dekel & Singer, 2006), and robust metric learning (Huang et al., 2010).", "startOffset": 131, "endOffset": 151}, {"referenceID": 25, "context": "Note that an alternative approach is to consider the best case analysis (a strategy taken in robust SVM (Xu et al., 2006; Yu et al., 2011)) by minimizing the robust hinge loss, which can be defined as minp\u2208Q \u2211 i(pili+1\u2212pi), where li denotes the loss on ith example, to address the uncertainty arising from noisy labels.", "startOffset": 104, "endOffset": 138}, {"referenceID": 29, "context": "Note that an alternative approach is to consider the best case analysis (a strategy taken in robust SVM (Xu et al., 2006; Yu et al., 2011)) by minimizing the robust hinge loss, which can be defined as minp\u2208Q \u2211 i(pili+1\u2212pi), where li denotes the loss on ith example, to address the uncertainty arising from noisy labels.", "startOffset": 104, "endOffset": 138}, {"referenceID": 25, "context": "First, minimization over p will lead to a nonconvex optimization problem, as shown in (Xu et al., 2006), making it difficult, if not impossible, to develop an efficient learning algorithm to find the global optimum.", "startOffset": 86, "endOffset": 103}, {"referenceID": 26, "context": "We choose five data sets from UCI repository that have been used in MKL studies (Rakotomamonjy et al., 2008; Xu et al., 2010).", "startOffset": 80, "endOffset": 125}, {"referenceID": 26, "context": "To create multiple kernels, we follow the setup in (Xu et al., 2010) to generate Gaussian kernels with 10 different width {2\u22123, 2, \u00b7 \u00b7 \u00b7 , 26} for all features as well as for individual features, leading to a total of m = 10(d + 1) kernels for each data set, where d is the number of features.", "startOffset": 51, "endOffset": 68}, {"referenceID": 25, "context": "For the second baseline, we extend the idea of robust SVM (Xu et al., 2006) to MKL from noisy labels by using the robust hinge loss and minimizing over p \u2208 Q.", "startOffset": 58, "endOffset": 75}, {"referenceID": 26, "context": "We observe that for almost We do not compare many other MKL algorithms because (i) some algorithms (Xu et al., 2010) optimize the same objective as in (2), (ii) some algorithms (Cortes et al.", "startOffset": 99, "endOffset": 116}, {"referenceID": 6, "context": ", 2010) optimize the same objective as in (2), (ii) some algorithms (Cortes et al., 2009; Kloft et al., 2009) focus on different regularizations (e.", "startOffset": 68, "endOffset": 109}, {"referenceID": 12, "context": ", 2010) optimize the same objective as in (2), (ii) some algorithms (Cortes et al., 2009; Kloft et al., 2009) focus on different regularizations (e.", "startOffset": 68, "endOffset": 109}, {"referenceID": 9, "context": "l2 or lp), and (iii) some algorithms (Cristianini et al., 2002; Cortes et al., 2010a) use different criteria (e.", "startOffset": 37, "endOffset": 85}, {"referenceID": 16, "context": "erated mirror prox (AMP) method to the variational inequality method (VI) (Nemirovski, 1994) (i.", "startOffset": 74, "endOffset": 92}], "year": 2012, "abstractText": "We study the problem of multiple kernel learning from noisy labels. This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples. Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem, and presenting a minimax formulation. We develop an efficient algorithm for solving the related convex-concave optimization problem with a fast convergence rate of O(1/T ) where T is the number of iterations. Empirical studies on UCI data sets verify both the effectiveness and the efficiency of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}