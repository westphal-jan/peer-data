{"id": "1206.4630", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Efficient Decomposed Learning for Structured Prediction", "abstract": "Structured prediction is the cornerstone of several machine learning applications. Unfortunately, in structured prediction settings with expressive inter-variable interactions, exact inference-based learning algorithms, e.g. Structural SVM, are often intractable. We present a new way, Decomposed Learning (DecL), which performs efficient learning by restricting the inference step to a limited part of the structured spaces. We provide characterizations based on the structure, target parameters, and gold labels, under which DecL is equivalent to exact learning. We then show that in real world settings, where our theoretical assumptions may not completely hold, DecL-based algorithms are significantly more efficient and as accurate as exact learning.", "histories": [["v1", "Mon, 18 Jun 2012 15:08:38 GMT  (423kb)", "http://arxiv.org/abs/1206.4630v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rajhans samdani", "dan roth"], "accepted": true, "id": "1206.4630"}, "pdf": {"name": "1206.4630.pdf", "metadata": {"source": "META", "title": "Efficient Decomposed Learning for Structured Prediction", "authors": ["Rajhans Samdani", "Dan Roth"], "emails": ["rsamdan2@illinois.edu", "danr@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "There are several techniques proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004). Typical discriminatory structural learning algorithms (e.g. Collins (2002); Tsochantaridis et al., 2004). We refer to such learning methods as global learning (GL). Global inferences, and hence GL, can be slow for high-order models, expressive relationships between production variants. GL algorithms execute exact MAP conclusions as black boxes, which are an exaggeration of several problems, making learning slow."}, {"heading": "2. Problem Setting", "text": "Consider a structured prediction setting in which a ddimensional input x is drawn from a space X, and the output variable i = \u00b7 2004 is, w.l.o.g., a vector of binary markers {y1,.., yn}, drawn by Y {0, 1} n. Space Y can be specified by a series of declarative constraints, which as a form of specifying any domain knowledge about y.Inference: The labels in y are correlated and so it is advantageous to predict them simultaneously. As is typical, we express the prediction of all variables in y by using a scoring function f (x, y; w) = w \u00b7 p = w \u00b7 p."}, {"heading": "3. Structured Prediction: Learning", "text": "In this section, two modes of learning the parameter w from the training data D are discussed: global learning and local learning with their shortcomings. Global learning Given the follow-up procedure in (1) and the training data D, a popular discriminatory learning approach (Tsochantaridis et al., 2004; Taskar et al., 2004) is to minimize a convex SVM-style upper limit on the training data: l (w) = m \u2211 j = 1 max y y Y (f (xj, y; w) \u2212 f (xj, yj; w) learning + \u2206 (yj, y)) (2) The follow-up step in (2), which includes max, is performed globally across all labels of y and therefore we call this style Global Learning (GL) \u2212 f (xj, yj; w) \u2212 f (xj, yj; w) + \u0445 (yj, yj; w), the production step in (2), which includes L is performed globally across all labels and therefore we call this style, Global Learning (GL) -f (xj, yj; w) -f (xj, yj, yj; w) + \u0445 (yj, y; w), which reduces the production step from one global to another (L), from one global to another (L)."}, {"heading": "4. Decomposed Learning (DecL)", "text": "For a training instance (xj, yj), we use a maxmarginal formulation (taskar, yj, yj, yj, yj) for a subset of the output space that defines a \"neighborhood\" around yj, which is called the basic truth or gold production. The key idea behind the decomposed learning (DecL) is to learn w by distinguishing the parent label yj, capturing the structure of Y while it is much smaller. Figure 1 shows the general scheme for both GL and DecL, showing the similarities and differences. Let N = {nbr) | j (yj) j = 1, m} be the collection of neighborhoods for all training instances. To pursue the general idea behind our approach, we use a maxmarginal formulation (taskar, yj)."}, {"heading": "5. Theoretical Analysis", "text": "We present theoretical results to show some conditions under which DecL4D (1), DecL (2), DecL (2), DecL (2), DecL (4), D (4), D (4), D (4), D (4), D (3), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (5), D (5), D (5), D (5), D (5), D (5), D (5), D (5), D (5)."}, {"heading": "5.1. Exactness of DecL for Singleton Scoring Functions with Constraints", "text": "In this section we present accuracy results for DecL with singleton scoring function f (x, y; w) = \u2211 n = 1 yifi (x) = \u2211 n = 1 yiwi \u00b7 x, where space Y is specified by constraints. For example, Y can be specified by a collection of l logical constraints: Y = {y \u00b2 {0, 1} n | Ck (y) = 1, k = 1,..., l} where Ck is a logical function (e.g. OR) over binary variables in y. Y can also be specified by linear constraints over y, such as Y = {0, 1} n | Ay \u2264 b}. In several practical applications, the constraint structure has a certain symmetry and we can call Cor. 1 to provide accuracy guarantees for decompositions with specified sizes regardless of the number of variables."}, {"heading": "5.2. Exactness for Pairwise Markov Networks", "text": "() () () () () () () () () () () () () () () () () () () () () () () () ()) () () ()) () () () () () () () () () () () () () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () ()) () () () () () () () ()) () () () () () () ()) () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ("}, {"heading": "6. Experiments", "text": "In this section, we present experimental results under non-ideal conditions in the real world, which show that DecL is effective and robust. We show results on synthetic data as well as two real-life tasks of multi-label classification and information extraction. Where necessary, we perform exact conclusions with ILP. We show that DecL performs favorably in these tasks compared to GL and significantly shortens training time. We use appropriate LL approaches as competing baselines. In situations with limitations, we look at another baseline, LL + C, which uses limitations - which were ignored during learning - for conclusions during testing."}, {"heading": "6.1. Experiments on Synthetic Data", "text": "We first analyze DecL with simple decompositions - DecL-1,2,3 - with singleton scoring functions in a controlled synthetic setting, measuring performance and efficiency. We generate data with 10 binary output variables constrained by randomly generated linear constraints. We ensure that the resulting Y contains at least 50 outputs. Characteristics x are randomly sampled from a 20-dimensional space. We randomly generate singleton scoring functions and determine gold markings for each instance according to Equation 1 (hence we know that the data is divisible.) For learning, we use SVM-Struct (Tsochantaridis et al., 2004) to implement our algorithms. Our LL baseline ignores the limitations during learning and reduces it to learning 10 independent binary classifiers. We test on 200 instances and tune C, the regularity penalty parameter Lsochantaridis al, to implement our algorithms in 2004)."}, {"heading": "6.2. Multi Label Document Classification", "text": "We test different algorithms on a multi-label document classification task using the Reuters dataset (Lewis et al., 2004). We use a 6,000-instance section of data and reduce it to the 30 most common labels. We keep 3600 instances for training, 1200 for testing, and 1200 for validation. We model the scoring function as PMN using a complete diagram of all labels to capture interactions between all label pairs. We compare DecL-1,2,3 with GL and a Local Learning Approach (LL) that reduces the pair-wise components, reducing the problem to 30 independent binary classifiers. Again, we use SVM-Struct to learn the parameters for GL and DecL. We measure performance using a sample metric specified with F1 = 2ct + p, where t is the number of gold labels for that instance, p is the number of predicted labels, and c the number of correct data for GL and Decherded Labels, with GL-2 and Decherical-6 times faster than GL-6 and GL-6 times."}, {"heading": "6.3. Information Extraction: Sequence Tagging with Submodular Potentials", "text": "We test the effectiveness of our approach on two information transmission tasks inspired by our analysis of PMNs in Sec. 5.2. Our task is to identify the functional fields (e.g. \"author,\" \"facilties,\" \"roommates\") from quotations (McCallum et al., 2000) and advertisements (Grenager et al., 2005). We model this setting as HMM (a specific case of PMN) with different functional fields as hidden states and words as emissions. We add certain global constraints ranging from Chang et al. (2007) to HMM, which requires ILP-based inferences."}, {"heading": "7. Conclusion", "text": "We introduced Decomposed Learning (DecL) - a technique for efficient structural learning. DecL learns efficiently by conducting conclusions about a small part of the production space. We provided theoretical results that use structural characterizations, target parameters, and basic truth markers to dissect the production space in such a way that the resulting DecL is efficient and equivalent to exact learning. While common approach practice in structural learning involves the use of approximate MAP inferences without guarantees, our approach in these cases could offer a way to achieve significant improvements and be complemented with existing approximation techniques such as LP relaxation. In fact, our experimental results suggest that our algorithms are robust and work very well on real-world data. Recognition: This research is sponsored by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053, Defense Advanced Research Projects (DPA Machine Reading Program) AFRL Research."}], "references": [{"title": "Aggregation via Set Partitioning for Natural Language Generation", "author": ["R. Barzilay", "M. Lapata"], "venue": "In Proc. of HLT/NAACL,", "citeRegEx": "Barzilay and Lapata,? \\Q2006\\E", "shortCiteRegEx": "Barzilay and Lapata", "year": 2006}, {"title": "Spatial interaction and the statistical analysis of lattice systems", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Besag,? \\Q1974\\E", "shortCiteRegEx": "Besag", "year": 1974}, {"title": "On the statistical analysis of dirty pictures", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Besag,? \\Q1986\\E", "shortCiteRegEx": "Besag", "year": 1986}, {"title": "Markov random fields with efficient approximations", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "In CVPR,", "citeRegEx": "Boykov et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 1998}, {"title": "Guiding semisupervision with constraint-driven learning", "author": ["M. Chang", "L. Ratinov", "D. Roth"], "venue": "In ACL,", "citeRegEx": "Chang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2007}, {"title": "Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification", "author": ["Y. Choi", "C. Cardie"], "venue": "In EMNLP,", "citeRegEx": "Choi and Cardie,? \\Q2009\\E", "shortCiteRegEx": "Choi and Cardie", "year": 2009}, {"title": "Global inference for sentence compression: An integer linear programming approach", "author": ["J. Clarke", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Clarke and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Clarke and Lapata", "year": 2008}, {"title": "Constraint-based sentence compression: An integer programming approach", "author": ["Clarke", "James", "Lapata", "Mirella"], "venue": "In ACL,", "citeRegEx": "Clarke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2006}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In EMNLP,", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": null, "citeRegEx": "Crammer and Singer,? \\Q2002\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2002}, {"title": "Stochastic composite likelihood", "author": ["J.V. Dillon", "G. Lebanon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dillon and Lebanon,? \\Q2010\\E", "shortCiteRegEx": "Dillon and Lebanon", "year": 2010}, {"title": "Training structural svms when exact inference is intractable", "author": ["T. Finley", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Finley and Joachims,? \\Q2008\\E", "shortCiteRegEx": "Finley and Joachims", "year": 2008}, {"title": "Posterior regularization for structured latent variable models", "author": ["K. Ganchev", "J. Gra\u00e7a", "J. Gillenwater", "B. Taskar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ganchev et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "Unsupervised learning of field segmentation models for information extraction", "author": ["T. Grenager", "D. Klein", "C. Manning"], "venue": "In ACL,", "citeRegEx": "Grenager et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Grenager et al\\.", "year": 2005}, {"title": "Constraint classification for multiclass classification and ranking", "author": ["S. Har-Peled", "D. Roth", "D. Zimak"], "venue": "In NIPS,", "citeRegEx": "Har.Peled et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Har.Peled et al\\.", "year": 2003}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Dual decomposition for parsing with nonprojective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Structured learning with approximate inference", "author": ["A. Kulesza", "F. Pereira"], "venue": "In NIPS", "citeRegEx": "Kulesza and Pereira,? \\Q2008\\E", "shortCiteRegEx": "Kulesza and Pereira", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Polyhedral outer approximations with application to natural language parsing", "author": ["A.F.T. Martins", "N.A. Smith", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Martins et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2009}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["A. McCallum", "D. Freitag", "F. Pereira"], "venue": "In ICML,", "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Learning efficiently with approximate inference via dual losses", "author": ["O. Meshi", "D. Sontag", "T. Jaakkola", "A. Globerson"], "venue": "In ICML,", "citeRegEx": "Meshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2010}, {"title": "Learning and inference over constrained output", "author": ["V. Punyakanok", "D. Roth", "W. Yih", "D. Zimak"], "venue": "In IJCAI,", "citeRegEx": "Punyakanok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2005}, {"title": "Integer linear programming inference for conditional random fields", "author": ["D. Roth", "W. Yih"], "venue": "In ICML,", "citeRegEx": "Roth and Yih,? \\Q2005\\E", "shortCiteRegEx": "Roth and Yih", "year": 2005}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["D. Roth", "W. Yih"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "Roth and Yih,? \\Q2007\\E", "shortCiteRegEx": "Roth and Yih", "year": 2007}, {"title": "Semi-markov conditional random fields for information extraction", "author": ["Sarawagi", "Sunita", "Cohen", "William W"], "venue": "In NIPS,", "citeRegEx": "Sarawagi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi et al\\.", "year": 2004}, {"title": "More data means less inference: A pseudo-max approach to structured learning", "author": ["D. Sontag", "O. Meshi", "T. Jaakkola", "A. Globerson"], "venue": null, "citeRegEx": "Sontag et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2010}, {"title": "A joint model for extended semantic role labeling", "author": ["V. Srikumar", "D. Roth"], "venue": "In EMNLP,", "citeRegEx": "Srikumar and Roth,? \\Q2011\\E", "shortCiteRegEx": "Srikumar and Roth", "year": 2011}, {"title": "Piecewise training for structured prediction", "author": ["C. Sutton", "A. Mccallum"], "venue": "Machine Learning,", "citeRegEx": "Sutton and Mccallum,? \\Q2009\\E", "shortCiteRegEx": "Sutton and Mccallum", "year": 2009}, {"title": "Learning crfs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "In ECCV,", "citeRegEx": "Szummer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Szummer et al\\.", "year": 2008}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Samplerank: training factor graphs with atomic gradients", "author": ["M. Wick", "K. Rohanimanesh", "K. Bellare", "A. Culotta", "A. McCallum"], "venue": "In ICML,", "citeRegEx": "Wick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wick et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004).", "startOffset": 76, "endOffset": 141}, {"referenceID": 32, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004).", "startOffset": 76, "endOffset": 141}, {"referenceID": 31, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004).", "startOffset": 76, "endOffset": 141}, {"referenceID": 8, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004). Typical discriminative structural learning algorithms (e.g. Collins (2002); Tsochantaridis et al.", "startOffset": 77, "endOffset": 218}, {"referenceID": 8, "context": "Several techniques have been proposed for learning in structured prediction (Collins, 2002; Tsochantaridis et al., 2004; Taskar et al., 2004). Typical discriminative structural learning algorithms (e.g. Collins (2002); Tsochantaridis et al. (2004)) perform a global MAP inference over the entire (hence \u2018global\u2019) output space as an intermediate step.", "startOffset": 77, "endOffset": 248}, {"referenceID": 23, "context": "Such approaches can broadly be divided into those that relax the expressive interactions between output variables (Roth & Yih, 2005; Punyakanok et al., 2005; Sutton & Mccallum, 2009) during learning and those that relax the integrality constraints on assignments (Kulesza & Pereira, 2008; Finley & Joachims, 2008; Martins et al.", "startOffset": 114, "endOffset": 182}, {"referenceID": 20, "context": ", 2005; Sutton & Mccallum, 2009) during learning and those that relax the integrality constraints on assignments (Kulesza & Pereira, 2008; Finley & Joachims, 2008; Martins et al., 2009; Meshi et al., 2010).", "startOffset": 113, "endOffset": 205}, {"referenceID": 22, "context": ", 2005; Sutton & Mccallum, 2009) during learning and those that relax the integrality constraints on assignments (Kulesza & Pereira, 2008; Finley & Joachims, 2008; Martins et al., 2009; Meshi et al., 2010).", "startOffset": 113, "endOffset": 205}, {"referenceID": 15, "context": "Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning.", "startOffset": 46, "endOffset": 79}, {"referenceID": 33, "context": "Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning.", "startOffset": 46, "endOffset": 79}, {"referenceID": 1, "context": "The closest works to DecL are Pseudolikelihood-based techniques (Besag, 1974; Sontag et al., 2010) to learning; however, while Pseudolikelihood is consistent asymptotically, DecL aims to achieve equivalence to GL with a finite amount of data.", "startOffset": 64, "endOffset": 98}, {"referenceID": 27, "context": "The closest works to DecL are Pseudolikelihood-based techniques (Besag, 1974; Sontag et al., 2010) to learning; however, while Pseudolikelihood is consistent asymptotically, DecL aims to achieve equivalence to GL with a finite amount of data.", "startOffset": 64, "endOffset": 98}, {"referenceID": 13, "context": "Some of the MCMC-based contrastive techniques (Hinton, 2002; Wick et al., 2011) are conceptually similar to DecL in that they use approximate gradient steps for learning. Our work is also related in spirit to Meshi et al. (2010) who consider a Linear Programming relaxation of the entire inference and perform parameter updates after small message-passing inference steps.", "startOffset": 47, "endOffset": 229}, {"referenceID": 16, "context": "tree constraints in dependency parsing (Koo et al., 2010) and sometimes they are added declaratively (Roth & Yih, 2005; Clarke & Lapata, 2006; Barzilay & Lapata, 2006; Roth & Yih, 2007; Clarke & Lapata, 2008; Choi & Cardie, 2009; Ganchev et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 12, "context": ", 2010) and sometimes they are added declaratively (Roth & Yih, 2005; Clarke & Lapata, 2006; Barzilay & Lapata, 2006; Roth & Yih, 2007; Clarke & Lapata, 2008; Choi & Cardie, 2009; Ganchev et al., 2010).", "startOffset": 51, "endOffset": 201}, {"referenceID": 18, "context": "HMM and CRF (Lafferty et al., 2001)), in this paper, we consider PMNs in a max-margin setting a la Taskar et al.", "startOffset": 12, "endOffset": 35}, {"referenceID": 3, "context": "PMNs are used extensively in many structured prediction applications in computer vision (Boykov et al., 1998), computational biology (Meshi et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 22, "context": ", 1998), computational biology (Meshi et al., 2010), NLP, and information extraction (Lafferty et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 18, "context": ", 2010), NLP, and information extraction (Lafferty et al., 2001; Sarawagi & Cohen, 2004).", "startOffset": 41, "endOffset": 88}, {"referenceID": 17, "context": "HMM and CRF (Lafferty et al., 2001)), in this paper, we consider PMNs in a max-margin setting a la Taskar et al. (2004). PMNs are used extensively in many structured prediction applications in computer vision (Boykov et al.", "startOffset": 13, "endOffset": 120}, {"referenceID": 32, "context": "Global Learning Given the inference procedure in (1) and training data D, a popular discriminative learning approach (Tsochantaridis et al., 2004; Taskar et al., 2004) is to minimize an SVM-style convex upper bound on the loss over the training data:", "startOffset": 117, "endOffset": 167}, {"referenceID": 31, "context": "Global Learning Given the inference procedure in (1) and training data D, a popular discriminative learning approach (Tsochantaridis et al., 2004; Taskar et al., 2004) is to minimize an SVM-style convex upper bound on the loss over the training data:", "startOffset": 117, "endOffset": 167}, {"referenceID": 23, "context": "For instance, when highly expressive constraints are used over the structure, then dropping such constraints makes the structure more \u201clocal\u201d and faster to learn: for singleton functions (Punyakanok et al., 2005; Barzilay & Lapata, 2006), ignoring constraints reduces the problem to learning n independent binary classifiers wi; in case of sequential or tree-structured problems, the task reduces to learning with dynamic programming inference (Koo et al.", "startOffset": 187, "endOffset": 237}, {"referenceID": 16, "context": ", 2005; Barzilay & Lapata, 2006), ignoring constraints reduces the problem to learning n independent binary classifiers wi; in case of sequential or tree-structured problems, the task reduces to learning with dynamic programming inference (Koo et al., 2010; Roth & Yih, 2005).", "startOffset": 239, "endOffset": 275}, {"referenceID": 23, "context": "Refer to Punyakanok et al. (2005) for a detailed analysis and comparison of GL and LL for singleton scoring functions with constraints.", "startOffset": 9, "endOffset": 34}, {"referenceID": 31, "context": "To pursue the general idea behind our approach, we use a maxmargin formulation (Taskar et al., 2004) for learning over given data D = {(x,y)}j=1.", "startOffset": 79, "endOffset": 100}, {"referenceID": 14, "context": "This is exactly what techniques like multiclass SVM (Crammer & Singer, 2002) and constrained classification (Har-Peled et al., 2003) do.", "startOffset": 108, "endOffset": 132}, {"referenceID": 14, "context": "Interestingly, the one-vs-all technique ignores the given constraint (one-vs-all is a kind of LL technique) and may not be able to obtain linear separation even if the labels are pairwise linearly separable (Har-Peled et al., 2003).", "startOffset": 207, "endOffset": 231}, {"referenceID": 14, "context": "1 with DecL-2 and \u2206 = 0 (perceptron loss) yields constrained classification (Har-Peled et al., 2003) thus closing our loop on multi-class classification.", "startOffset": 76, "endOffset": 100}, {"referenceID": 32, "context": "Instead of subgradient-descent, DecL can also be used in a cutting-plane method (Tsochantaridis et al., 2004).", "startOffset": 80, "endOffset": 109}, {"referenceID": 1, "context": "Note that the Pseudolikelihood-based approaches (Besag, 1974; Dillon & Lebanon, 2010; Sontag et al., 2010) to structured prediction are asymptotically consistent; that is, they are equivalent to GL only in the limit of infinite data.", "startOffset": 48, "endOffset": 106}, {"referenceID": 27, "context": "Note that the Pseudolikelihood-based approaches (Besag, 1974; Dillon & Lebanon, 2010; Sontag et al., 2010) to structured prediction are asymptotically consistent; that is, they are equivalent to GL only in the limit of infinite data.", "startOffset": 48, "endOffset": 106}, {"referenceID": 2, "context": "For instance, in several computer vision tasks, neighboring pixels are more likely to carry the same label (Besag, 1986; Boykov et al., 1998); in infor-", "startOffset": 107, "endOffset": 141}, {"referenceID": 3, "context": "For instance, in several computer vision tasks, neighboring pixels are more likely to carry the same label (Besag, 1986; Boykov et al., 1998); in infor-", "startOffset": 107, "endOffset": 141}, {"referenceID": 30, "context": "Notably, graph cuts can be used for efficient learning over binary PMNs with submodular potentials (Szummer et al., 2008).", "startOffset": 99, "endOffset": 121}, {"referenceID": 32, "context": "For learning, we use SVM-Struct (Tsochantaridis et al., 2004) to implement our algorithms.", "startOffset": 32, "endOffset": 61}, {"referenceID": 19, "context": "We test various algorithms on a multi-label document classification task over the Reuters dataset (Lewis et al., 2004).", "startOffset": 98, "endOffset": 118}, {"referenceID": 27, "context": "Pseudomax (Sontag et al., 2010), performs badly.", "startOffset": 10, "endOffset": 31}, {"referenceID": 21, "context": "\u2018author\u2019, \u2018title\u2019, \u2018facilties\u2019, \u2018roommates\u2019) from citations (McCallum et al., 2000) and advertisements (Grenager et al.", "startOffset": 60, "endOffset": 83}, {"referenceID": 13, "context": ", 2000) and advertisements (Grenager et al., 2005) datasets.", "startOffset": 27, "endOffset": 50}, {"referenceID": 4, "context": "We add certain global constraints borrowed from Chang et al. (2007) to the HMM, which necessitate ILP-based inference.", "startOffset": 48, "endOffset": 68}, {"referenceID": 4, "context": "datasets by Chang et al. (2007). We also show average", "startOffset": 12, "endOffset": 32}, {"referenceID": 8, "context": "We perform discriminative learning using averaged structured perceptron (Collins, 2002).", "startOffset": 72, "endOffset": 87}, {"referenceID": 4, "context": "Our results compare favorably with the stateof-the-art supervised results reported on these datasets by Chang et al. (2007) (CRR07.", "startOffset": 104, "endOffset": 124}], "year": 2012, "abstractText": "Structured prediction is the cornerstone of several machine learning applications. Unfortunately, in structured prediction settings with expressive inter-variable interactions, exact inference-based learning algorithms, e.g. Structural SVM, are often intractable. We present a new way, Decomposed Learning (DecL), which performs efficient learning by restricting the inference step to a limited part of the structured spaces. We provide characterizations based on the structure, target parameters, and gold labels, under which DecL is equivalent to exact learning. We then show that in real world settings, where our theoretical assumptions may not completely hold, DecL-based algorithms are significantly more efficient and as accurate as exact learning.", "creator": "LaTeX with hyperref package"}}}