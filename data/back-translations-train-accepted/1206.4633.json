{"id": "1206.4633", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Fast Bounded Online Gradient Descent Algorithms for Scalable Kernel-Based Online Learning", "abstract": "Kernel-based online learning has often shown state-of-the-art performance for many online learning tasks. It, however, suffers from a major shortcoming, that is, the unbounded number of support vectors, making it non-scalable and unsuitable for applications with large-scale datasets. In this work, we study the problem of bounded kernel-based online learning that aims to constrain the number of support vectors by a predefined budget. Although several algorithms have been proposed in literature, they are neither computationally efficient due to their intensive budget maintenance strategy nor effective due to the use of simple Perceptron algorithm. To overcome these limitations, we propose a framework for bounded kernel-based online learning based on an online gradient descent approach. We propose two efficient algorithms of bounded online gradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD by maintaining support vectors using uniform sampling, and (ii) BOGD++ by maintaining support vectors using non-uniform sampling. We present theoretical analysis of regret bound for both algorithms, and found promising empirical performance in terms of both efficacy and efficiency by comparing them to several well-known algorithms for bounded kernel-based online learning on large-scale datasets.", "histories": [["v1", "Mon, 18 Jun 2012 15:13:13 GMT  (192kb)", "http://arxiv.org/abs/1206.4633v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["steven c h hoi", "jialei wang", "peilin zhao", "rong jin", "pengcheng wu"], "accepted": true, "id": "1206.4633"}, "pdf": {"name": "1206.4633.pdf", "metadata": {"source": "META", "title": "Fast Bounded Online Gradient Descent Algorithms forScalable Kernel-Based Online Learning", "authors": ["Peilin Zhao", "Jialei Wang", "Pengcheng Wu", "Rong Jin", "Steven C.H. Hoi"], "emails": ["zhao0106@ntu.edu.sg", "jl.wang@ntu.edu.sg", "wupe0003@ntu.edu.sg", "rongjin@cse.msu.edu", "chhoi@ntu.edu.sg"], "sections": [{"heading": null, "text": "Published in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by author (s) / owner (s)."}, {"heading": "1. Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2. Algorithms and Analysis", "text": "We look at online learning for classification. Our goal is to learn a function f: Rd \u2192 R from a sequence of learning examples. (xt, yt) We look at online learning as a sequence of learning examples. (xt, yt) We look at online learning as a sequence of learning examples. (xt, yt) We predict the class assignment for x. (f, x) We measure the class assignment for x (f, x) and the class assignment for x. (f, x) We assume that we have a constant loss that Lipschitz, as Lipschitz constant L. Let H be at RKHS has a core function. (Rd, x) We assume that we have a sequence for x."}, {"heading": "3. Experimental Results", "text": "In this section, we evaluate the empirical performance of the proposed algorithms for Bounded Online Gradient Descent (BOGD) learning algorithms by comparing them with the most advanced algorithms for online budget learning."}, {"heading": "3.1. Experimental Testbed", "text": "Table 1 shows the details of six binary class datasets used in our experiments. All of these datasets can be downloaded from the LIBSVM website 1 and the UCI Machine Learning Repository 2. These datasets were selected relatively randomly to cover a variety of datasets of different sizes."}, {"heading": "3.2. Baseline Algorithms and Setup", "text": "We refer to \"BOGD\" the proposed BOGD algorithm using uniform sampling (1958) and as \"BOGD + +\" the proposed BOGD algorithm using non-uniform sampling. We compare the two proposed BOGD algorithms with the following state-of-the-art algorithms for online budget learning: (i) \"RBP\" - the random budget perceptron algorithm (Cavallanti et al., 2007), (ii) \"Forgetron\" - the Forgetron algorithm (Dekel et al., 2005), (iii) \"Projectron\" - the Projectron perctron algorithm (Orabona et al., 2), \"Projectron perctron algorithm.\""}, {"heading": "3.3. Evaluation of Non-budget Algorithms", "text": "Table 2 summarizes the average performance of the two non-budget algorithms for kernel-based online learning. First, we note that OGD significantly exceeds Perceptron for all data sets, implying that a budget OGD algorithm may be more effective than the one based on the Perceptron algorithm. Second, the support vector size of OGD is generally much larger than that of Perceptron. Finally, the time cost of OGD is much higher than that of Perceptron, mainly due to the greater number of support vectors. Both the large number of support vectors and the high computing time motivate the development of budget OGD algorithms."}, {"heading": "3.4. Evaluation of Budget Algorithms", "text": "Table 3 summarizes the results of various online learning algorithms. First, we observe that RBP and Forgetron achieve similar performance in almost all cases. Furthermore, we find that Projectron + + achieves a lower error rate than Projectron for almost all datasets and for all budget sizes that are similar to the results in (Orabona et al., 2008). Second, compared to the basic algorithms for online budget learning, the proposed BOGD algorithm achieves comparable, sometimes better error rates, especially when the budget size is large, demonstrating the effectiveness of our framework. Of all the compared algorithms for online budget learning, we find that BOGD + + achieves the lowest error rates in most cases; if the budget is small, BOGD + achieves the best or closest results (except two datasets)."}, {"heading": "4. Conclusions", "text": "The basic idea of maintaining budget size is to remove a randomly selected support vector when the support vector overflows. Specifically, we proposed two efficient BOGD algorithms: (i) BOGD by randomly discarding a support vector using a uniform sample and (ii) BOGD + + by uneven sample. We conducted extensive empirical studies by comparing with several state-of-the-art algorithms, where our results showed that the proposed algorithms achieve the most promising performance in terms of both classification efficiency and computational efficiency."}, {"heading": "Acknowledgments", "text": "This work was partially supported by MOE Tier 1 Grant (RG33 / 11), Microsoft Research Grant (M4060936), National Science Foundation (IIS-0643494) and Office of Navy Research (ONR Award N00014-09-1-0663 and N00014-12-1-0431)."}], "references": [{"title": "Tracking the best hyperplane with a simple budget perceptron", "author": ["Cavallanti", "Giovanni", "Cesa-Bianchi", "Nicol\u00f2", "Gentile", "Claudio"], "venue": "Machine Learning,", "citeRegEx": "Cavallanti et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cavallanti et al\\.", "year": 2007}, {"title": "Implicit online learning with kernels", "author": ["Wang", "Shaojun", "Caelli", "Terry"], "venue": "In NIPS, pp", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Online classification on a budget", "author": ["Crammer", "Koby", "Kandola", "Jaz S", "Singer", "Yoram"], "venue": "In NIPS,", "citeRegEx": "Crammer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2003}, {"title": "Online passiveaggressive algorithms", "author": ["Crammer", "Koby", "Dekel", "Ofer", "Keshet", "Joseph", "ShalevShwartz", "Shai", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "The forgetron: A kernel-based perceptron on a fixed budget", "author": ["Dekel", "Ofer", "Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "In NIPS,", "citeRegEx": "Dekel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2005}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["Duchi", "John", "Singer", "Yoram"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2009}, {"title": "Online multiple kernel learning: Algorithms and mistake bounds", "author": ["Jin", "Rong", "Hoi", "Steven C. H", "Yang", "Tianbao"], "venue": "In ALT,", "citeRegEx": "Jin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2010}, {"title": "Online learning with kernels", "author": ["Kivinen", "Jyrki", "Smola", "Alex J", "Williamson", "Robert C"], "venue": "In NIPS, pp", "citeRegEx": "Kivinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2001}, {"title": "Sparse online learning via truncated gradient", "author": ["Langford", "John", "Li", "Lihong", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "The projectron: a bounded kernel-based perceptron", "author": ["Orabona", "Francesco", "Keshet", "Joseph", "Caputo", "Barbara"], "venue": "In ICML, pp", "citeRegEx": "Orabona et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2008}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1958}, {"title": "Pegasos: primal estimated sub-gradient solver for svm", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew"], "venue": "Math. Program.,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Online (and offline) on an even tighter budget", "author": ["Weston", "Jason", "Bordes", "Antoine"], "venue": "In AISTATS, pp", "citeRegEx": "Weston et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2005}, {"title": "Online gradient descent learning algorithms", "author": ["Ying", "Yiming", "Pontil", "Massimiliano"], "venue": "Found. Comput. Math.,", "citeRegEx": "Ying et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2008}, {"title": "Double updating online learning", "author": ["Zhao", "Peilin", "Hoi", "Steven C. H", "Jin", "Rong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML, pp", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "The goal of kernel-based online learning is to sequentially update a nonlinear kernel-based classifier given a sequence of training examples (Kivinen et al., 2001; Cheng et al., 2006; Crammer et al., 2006; Jin et al., 2010; Zhao et al., 2011).", "startOffset": 141, "endOffset": 242}, {"referenceID": 3, "context": "The goal of kernel-based online learning is to sequentially update a nonlinear kernel-based classifier given a sequence of training examples (Kivinen et al., 2001; Cheng et al., 2006; Crammer et al., 2006; Jin et al., 2010; Zhao et al., 2011).", "startOffset": 141, "endOffset": 242}, {"referenceID": 6, "context": "The goal of kernel-based online learning is to sequentially update a nonlinear kernel-based classifier given a sequence of training examples (Kivinen et al., 2001; Cheng et al., 2006; Crammer et al., 2006; Jin et al., 2010; Zhao et al., 2011).", "startOffset": 141, "endOffset": 242}, {"referenceID": 14, "context": "The goal of kernel-based online learning is to sequentially update a nonlinear kernel-based classifier given a sequence of training examples (Kivinen et al., 2001; Cheng et al., 2006; Crammer et al., 2006; Jin et al., 2010; Zhao et al., 2011).", "startOffset": 141, "endOffset": 242}, {"referenceID": 2, "context": "(Crammer et al., 2003) proposed a heuristic approach for online budget learning, which was further improved in (Weston & Bordes, 2005).", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "Forgetron (Dekel et al., 2005) is the first online budget learning algorithm that has guarantee on the number of mistakes.", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "Randomized Budget Perceptron (RBP) (Cavallanti et al., 2007) removes a randomly selected support vector when the number of support vectors exceeds the predefined budget.", "startOffset": 35, "endOffset": 60}, {"referenceID": 9, "context": "Unlike the strategy that discards one of support vectors to maintain the budget, Projectron (Orabona et al., 2008) adopts a projection strategy to bound the number of support vectors.", "startOffset": 92, "endOffset": 114}, {"referenceID": 7, "context": "In this paper, we develop a \u201cBounded Online Gradient Descent\u201d (BOGD) framework for online budget learning algorithms, based on the online gradient descent algorithms (Kivinen et al., 2001; Zinkevich, 2003; Ying & Pontil, 2008).", "startOffset": 166, "endOffset": 226}, {"referenceID": 8, "context": "Finally, it is important to distinguish the proposed work from sparse online learning (Langford et al., 2009; Duchi & Singer, 2009) whose goal is to learn a sparse linear classifier from a sequence of training examples.", "startOffset": 86, "endOffset": 131}, {"referenceID": 7, "context": "Similar to kernel-based online learning (Kivinen et al., 2001; Zinkevich, 2003) and the Pegasos algorithm (Shalev-Shwartz et al.", "startOffset": 40, "endOffset": 79}, {"referenceID": 11, "context": ", 2001; Zinkevich, 2003) and the Pegasos algorithm (Shalev-Shwartz et al., 2011), at each trial of online learning, given a received training example (xt, yt), we define the following loss function: Lt(f) = \u03bb 2 \u2016f\u2016H + l(ytf(xt)) (1)", "startOffset": 51, "endOffset": 80}, {"referenceID": 7, "context": "We first describe an online learning algorithm, similar to kernel-based online learning (Kivinen et al., 2001; Zinkevich, 2003), that minimizes the regret of \u2211T t=1 lt(ft) using the online gradient descent approach.", "startOffset": 88, "endOffset": 127}, {"referenceID": 7, "context": "Using the standard analysis of gradient descent (Kivinen et al., 2001; Zinkevich, 2003), it is not difficult to show for any f \u2208 \u03a9(\u03b7\u03b3),", "startOffset": 48, "endOffset": 87}, {"referenceID": 4, "context": "Second, although (13) shows a regret bound of O( \u221a T ) independent from B, it does not contradict the analysis presented in (Dekel et al., 2005).", "startOffset": 124, "endOffset": 144}, {"referenceID": 4, "context": "This is because we restrict the competitor f to the domain \u03a9(\u03b7\u03b3) while the analysis in (Dekel et al., 2005) considers any hypothesis in RHKS H as a competitor.", "startOffset": 87, "endOffset": 107}, {"referenceID": 4, "context": "Although this result may seem significantly worse than the one presented (Dekel et al., 2005), we emphasize that (14) is about regret bound while the result in (Dekel et al.", "startOffset": 73, "endOffset": 93}, {"referenceID": 4, "context": ", 2005), we emphasize that (14) is about regret bound while the result in (Dekel et al., 2005) is about mistake bound.", "startOffset": 74, "endOffset": 94}, {"referenceID": 0, "context": "We compare the two proposed BOGD algorithms with the following state-of-the-art algorithms for online budget learning: (i) \u201cRBP\u201d \u2014 the Random Budget Perceptron algorithm (Cavallanti et al., 2007), (ii) \u201cForgetron\u201d \u2014 the Forgetron algorithm (Dekel et al.", "startOffset": 170, "endOffset": 195}, {"referenceID": 4, "context": ", 2007), (ii) \u201cForgetron\u201d \u2014 the Forgetron algorithm (Dekel et al., 2005), (iii) \u201cProjectron\u201d \u2014 the Projectron algorithm (Orabona et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 9, "context": ", 2005), (iii) \u201cProjectron\u201d \u2014 the Projectron algorithm (Orabona et al., 2008), and (iv) \u201cProjectron++\u201d \u2014 the aggressive version of Projectron algorithm (Orabona et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 9, "context": ", 2008), and (iv) \u201cProjectron++\u201d \u2014 the aggressive version of Projectron algorithm (Orabona et al., 2008).", "startOffset": 82, "endOffset": 104}, {"referenceID": 7, "context": "We also compare the proposed algorithms to two non-budget online learning algorithms: (i) \u201cPerceptron\u201d \u2014 the classical Perceptron algorithm (Rosenblatt, 1958), and (ii) \u201cOGD\u201d \u2014 the Online Gradient Descent algorithm (Kivinen et al., 2001; Ying & Pontil, 2008).", "startOffset": 215, "endOffset": 258}, {"referenceID": 9, "context": "In addition, we also find that Projectron++ achieves a lower mistake rate than Projectron for almost all the datasets and for all budge sizes, which is similar to the results in (Orabona et al., 2008).", "startOffset": 178, "endOffset": 200}], "year": 2012, "abstractText": "Kernel-based online learning has often shown state-of-the-art performance for many online learning tasks. It, however, suffers from a major shortcoming, that is, the unbounded number of support vectors, making it nonscalable and unsuitable for applications with large-scale datasets. In this work, we study the problem of bounded kernel-based online learning that aims to constrain the number of support vectors by a predefined budget. Although several algorithms have been proposed in literature, they are neither computationally efficient due to their intensive budget maintenance strategy nor effective due to the use of simple Perceptron algorithm. To overcome these limitations, we propose a framework for bounded kernel-based online learning based on an online gradient descent approach. We propose two efficient algorithms of bounded online gradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD by maintaining support vectors using uniform sampling, and (ii) BOGD++ by maintaining support vectors using nonuniform sampling. We present theoretical analysis of regret bound for both algorithms, and found promising empirical performance in terms of both efficacy and efficiency by comparing them to several well-known algorithms for bounded kernel-based online learning on large-scale datasets. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "LaTeX with hyperref package"}}}