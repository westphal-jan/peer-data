{"id": "1206.4635", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Deep Mixtures of Factor Analysers", "abstract": "An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.", "histories": [["v1", "Mon, 18 Jun 2012 15:14:57 GMT  (3090kb)", "http://arxiv.org/abs/1206.4635v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yichuan tang", "ruslan salakhutdinov", "geoffrey e hinton"], "accepted": true, "id": "1206.4635"}, "pdf": {"name": "1206.4635.pdf", "metadata": {"source": "META", "title": "Deep Mixtures of Factor Analysers", "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"], "emails": ["tang@cs.toronto.edu", "rsalakhu@cs.toronto.edu", "hinton@cs.toronto.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Mixture of Factor Analysers", "text": "Factor analysis was first introduced in psychology as a latent variable model to find the \"underlying factor\" behind covariates; the latent variables are called factors and have a lower dimension than the covariates. Factor analyzers are linear models because the factor loads encompass a linear subspace within the vector space of the covariants. MFAs approach nonlinear manifolds by making local linear assumptions. Let x x x x x x x x x the D-dimensional data p: d \u2264 D} denote the d-dimensional latent variable and c {1,."}, {"heading": "3. Deep Mixtures of Factor Analysers", "text": "This boils down to the adaptation of conditional distributions p (x | z, c). However, as we show in our experimental results, this approach quickly leads to overfitting, especially when modelling high-dimensional data. An alternative is to replace the standard distributions p (x | z, c). The \"aggregated postcomponent\" is the empirical average over the data of the postdimensional data. An alternative is to replace the standard distribution layers p (z)."}, {"heading": "3.1. Inference", "text": "The exact conclusion in a collapsed DMFA model is of order O (CK), since the data probability must be calculated for each composite component. We can cause lower costs by using an approximate conclusion that is O (C + K). First, we calculate the rear p (z (1), c | x) = p (z (1) | x, c) p (c | x) using Equation 7. This rear end is exactly if we had a standard before z (1), but it is an approximation to the rear end of the DMFA model. The entropy of the rear p (c | x) should be very low in high dimensional spaces. Therefore, we make a point estimation by selecting component c with the maximum rear probability: c = arg max cp (c) p (22) p (1) | x) = c (1), c (2), c (c) (2) and (p)."}, {"heading": "3.2. Learning", "text": "A DMFA can be trained efficiently using a greedy layer-by-layer algorithm. However, the first layer of MFA is trained by default. We then use Eq. 23 to derive component c and the factors associated with that component for each training case {xn}. Then we freeze the parameters of the first layer and handle the first sampled algorithm 1 Learning DMFAsGiven data: X = {x1, x2,., xN}. / / Layer 1 Training Train 1st layer of MFA on X with C components and d factors with EM \u2192 MFA1. / / Layer 2 Training Create the dataset Yc for each of the C components. Yc."}, {"heading": "4. Experiments", "text": "We demonstrate the benefits of learning DMFAs on both low-dimensional and high-dimensional datasets, including facial images, natural image fields, and acoustic data. Toronto Face Database (TFD): The Toronto Face Database is a collection of aligned faces from a variety of (mostly) publicly available facial image databases (Susskind, 2011). From the original resolution of 108 \u00d7 108, we have resolutions of 48 \u00d7 48 or 24 \u00d7 24. We then randomly selected 30,000 images for training, 10,000 for testing. CIFAR-10: The CIFAR-10 datasets (Krizhevsky, 2009) consist of 60,000 32 \u00d7 32 \u00d7 3 color images from 10 object classes. There are 50,000 training images and 10,000 test images. Of 50,000 training images, 10,000 have been set aside for validation."}, {"heading": "4.1. Overfitting", "text": "The number of factors was adjusted to half of the input dimensionality, d (1) = D / 2 = 288. Fig. 2 shows the corresponding training and validation protocol probability. Next, we stacked a second MFA layer with five second layer components (Kc = 5) for each of the first layer components and d (2) = 50 second layer factors. The DMFA model (MFA2) improved by another 20 iterations over the course of the study (see red and blue lines in Fig. 2). For comparison, immediately after we first formed the two-layer MFA, we broke it into its equivalent flat representation and conducted additional training (Magenta and black lines in Fig. 2). Note that the flat MFA was retrofitted due to its additional capacity (5 times more parameters)."}, {"heading": "4.2. Qualitative Results", "text": "Next, we will show qualitative improvements of the samples from a DMFA compared to a standard MFA model. As a baseline, we first trained an MFA model on 30,000 24 x 24 facial images from the TFD with 288 factors and 100 components. Afterwards, we trained a DMFA with 20 components of the first layer and 5 components of the second layer for each of the 20 components of the first layer. The DMFA has the same number of parameters as the base model. The two-layer MFA (MFA2) shows better performance compared to the standard MFA by about 20 Nats on the test kit. Figure 3 shows further samples from the two models. Qualitatively, the DMFA seems to produce better samples compared to the flat MFA model."}, {"heading": "4.3. High Dimensional Data", "text": "Next, we will explore the benefits of DMFAs on the high-dimensional CIFAR and TIMIT datasets. We first trained an MFA model with the number of factors that relate to half of the input dimensionality; the number of mixing components was set at 20; for MFA2, 5 components with 50 latent factors were used; for the 3rd layer MFA, 3 factors with 30 latent factors were used; Table 1 shows the average formation and test likelihood; and we provide results for twotypes of RBM models commonly used in modeling high-dimensional real data, including image patches and language."}, {"heading": "4.4. Low Dimensional Data", "text": "Subsequently (Silva et al., 2011) we used 4 continuous datasets from the UCI repository and removed the discrete variables from all datasets. For the Parkinsons dataset, one variable was removed from each pair with a Pearson correlation coefficient greater than 0.98 (for details see (Silva et al., 2011)). Table 2 reports on the averaged test results by 10-fold cross-validation. Compared to the recently introduced Copula Networks, MFAs provide significantly better prediction performance. Adding a second layer resulted in significant improvements in model performance. The improvements by adding a second layer on all datasets were statistically significant due to the paired t test at p = 0.01."}, {"heading": "4.5. Natural Images", "text": "One important application of generative models is the task of image restoration, which can be formulated as an MAP estimation problem. As Zoran & Wei\u00df (2011) has confirmed, better prediction almost certainly leads to a better signal-to-noise ratio of the restored image. Furthermore, Zoran & Wei\u00df (2011) have shown that combining a mixture of Gaussian models trained on 8 x 8 fields with a patch-based denoising algorithm enabled them to achieve state-of-the-art results. Following their work, we trained a double-layer MFA on 8 x 8 fields from the Berkeley database. Two million training and 50,000 test fields were extracted from the 200 training and 100 test images, respectively. Table 3 shows results: The DMFA significantly improves Zoran & Wei\u00df's current state-of-the-art GMM model (2011) by about 2 natters, while it significantly outperforms other commonly used models such as PCA and ICA."}, {"heading": "4.6. Allocating more components to more popular factor analysers", "text": "We tested this hypothesis by first training an MFA model on 48 x 48 TFD surfaces, which achieved an average test log probability of 5159 Nats. For the two-layer MFA, instead of assigning 5 components to each of the first layer components, the result was Kc \u03c0c with min (Kc) = 2 and \u2211 C c Kc = 5 \u00b7 C. At constant learning hyperparameters, the resulting DMFA reached 5246 Nats on the test set. Compared to 5242 Nats of our predecessor model (c.f. Table 1), the new method resulted in a gain of 4 Nats. As a further alternative, a measurement for gas aggregation could be used to determine ceriality."}, {"heading": "5. Discussions", "text": "As density models, MFA's undirected RBM models significantly outperform real-world assessed data, and by using second-layer MFA's to model the aggregated rear layer of each first layer factor analyzer, we can achieve significant performance gains. Greater input dimensionality leads to greater gains in learning DMFA. Adding a third MFA layer, however, appears to be of little value. Another potential addition to our work is to train a mix of linear dynamic systems and then train an overarching mix of linear dynamic systems to model the aggregated rear layers of each component of the first layer."}, {"heading": "Acknowledgements", "text": "We thank Iain Murray for the discussions and Jakob Verbeek for sharing his MFA code, which was supported by NSERC & CIFAR."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.", "creator": "LaTeX with hyperref package"}}}