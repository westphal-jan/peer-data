{"id": "1206.4638", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Efficient Euclidean Projections onto the Intersection of Norm Balls", "abstract": "Using sparse-inducing norms to learn robust models has received increasing attention from many fields for its attractive properties. Projection-based methods have been widely applied to learning tasks constrained by such norms. As a key building block of these methods, an efficient operator for Euclidean projection onto the intersection of $\\ell_1$ and $\\ell_{1,q}$ norm balls $(q=2\\text{or}\\infty)$ is proposed in this paper. We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the problem. We show that the time complexity of our solution is $O(n+g\\log g)$ for $q=2$ and $O(n\\log n)$ for $q=\\infty$, where $n$ is the dimensionality of the vector to be projected and $g$ is the number of disjoint groups; we confirm this complexity by experimentation. Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage. We further show that embedded with our efficient projection operator, projection-based algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy.", "histories": [["v1", "Mon, 18 Jun 2012 15:16:28 GMT  (179kb)", "http://arxiv.org/abs/1206.4638v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["adams wei yu", "hao su", "fei-fei li"], "accepted": true, "id": "1206.4638"}, "pdf": {"name": "1206.4638.pdf", "metadata": {"source": "CRF", "title": "Efficient Euclidean Projections onto the Intersection of Norm Balls", "authors": ["Hao Su", "Wei Yu", "Li Fei-Fei"], "emails": ["haosu@cs.stanford.edu", "wyu@cs.hku.hk", "feifeili@cs.stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "The two types of frugal learning exist, for example, in the multitascular learning environment, indexing for each task, and there may be a large overlap of characteristics across multiple tasks (JP 2010), the two types of frugal learning exist simultaneously. (JP 2010) The two types of frugal learning exist in the multitascular learning environment, the index set for each task may be frugal, and there may be a large overlap of characteristics. (JP 2010) The two types of frugal learning exist in the multitascular learning environment, the index set for each task may be frugal, and there may be a large overlap of characteristics. (JP 2010) The two types of frugal learning exist in the multitascular learning environment, the index set for each task may have a large overlap of characteristics across multiple tasks. (JP 2010) The two types of frugal learning exist."}, {"heading": "2. Related Work", "text": "Much research has been done on efficient projection operators on standard spheres, but projection onto sphere 2 is simple, as we only need to move the point towards the origin. Linear time projection algorithms for standard 1 and standard 1,2 have been proposed by (Duchi et al., 2008; Liu & Ye, 2009) and (Schmidt et al., 2009) respectively, and the problem of projecting a point onto the intersection of convex sets has been studied for decades (Quattoni et al., 2009), in particular various alternative direction methods have been proposed (Han, 1988; Perkins, 2002; Schmidt & Murphy, 2010)."}, {"heading": "3. Notations and Definitions", "text": "We start by introducing the notation and definitions used on the whole paper. In view of the c-Rn standards, the c-norms are called i-norm = i-norm, i-norm = 1-norm, c-norm = 1-norm, c-norm = 1-norm, c-norm = 1-norm, c-norm = 2-norm, c-norm = 1-norm, c-norm = 1-norm, c-norm = 2-norm, c-norm = 1-norm, c-norm = 1-norm, c-norm = 1-norm, c-norm = 2-norm, c-norm = 1, c-norm = 1, c-norm = 1, c = 1, c = 1, c = 1, c = 1."}, {"heading": "4. Euclidean Projection on the", "text": "In this section we will present our approach to the Euclidean projection on the (1 + 1,2) normal ball and (1 + 1, 3) normal ball. For reasons of space, we leave most of the proofs in the appendix with the exception of theorem 1."}, {"heading": "4.1. Euclidean Projection on the", "text": "In this section, we first formulate the projection onto the standard ball as a convex optimization problem (Sec 4.1.1), parameterize the solution using dual variables, and provide an intuitive geometric interpretation (Sec 4.1.2). Finally, we determine the optimal dual variable values by determining the unique origin of a monotonous function using a bisection algorithm (Sec 4.1.3)."}, {"heading": "4.1.1. Problem Formulation", "text": "The Euclidean projection in Rn for the (1 + 1.2) standard ball can be formalized as follows: asmin x1 2-x 2-x 22 s.t. x-x-1,2 \u2264 \u03c41, x-x-1 \u2264 \u03c42 (3), where \u03c41 (\u03c42) indicates the radius of the standard ball. With the following sentence, we can reflect the point c on the positive orthotic by simply setting ci: | ci | and later restoring the original optimizer by setting x-i: = characters (ci) \u00b7 x-i. Therefore, we simply assume that ci \u2265 0, i = 1, 2,..., n is from now on. Suggestion 1 Let x-1 be the optimizer of the problem (3), then x-i-i-i-i-0, i = 1, 2,..., n."}, {"heading": "4.1.2. Parameterizing the Solution by Optimal Dual Variables", "text": "We can parameterise the solution x * using the optimal dual variables \u03bb * 1 > \u043c 2, as shown in the following example, so that the KKT system (Sec * 1 in the appendix) is satisfied (Bach and al *, 2011). Proposal 2 Assuming x *, \u03bb * 1 and \u03bb * 2 are the primary or dual solutions, then k = SGN (MAX (c * k \u2212 2), where e \u00b2 k is a vector of all 1s having the same dimension as c \u00b2 k))). The solution has an intuitive geometrical interpretation. x \u00b2 k is c \u00b2 k by the first translation c \u00b2 k, 0) (c \u00b2 k \u00b2) and the second by the first translation c \u00b2 e."}, {"heading": "4.1.3. Determining the Dual Variables", "text": "In this section, we will discuss how the variables are to be determined on a case-by-case basis. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "4.2. Euclidean Projection on the", "text": "The projection on the projection problem (1) for q = \u00b2 results as follows: min x, d1 2% c \u2212 x \u00b2 22s.t. Note that the above formulation differs from (Quattoni et al., 2009) in the additional term (i = 1, 2,... g). Similar to Sec 4.1.2, in the KKT system (in the appendix), the solution x \u00b2 0, the solution x \u00b2 0, the solution x \u00b2 0, the solution x \u00b2 and the optimal dual variable x \u00b2, the optimal solution x \u00b2, the optimal dual variable x \u00b2, the optimal dual variable x \u00b2, the optimal solution x \u00b2, the optimal dual solution x \u00b2, the optimal dual solution x \u00b2, the optimal dual variable x \u00b2, the ideal solution j \u00b2)."}, {"heading": "5. Experiments", "text": "In this section, we show the efficiency and effectiveness of the proposed projection algorithm in experiments using synthetic and real data. Due to the space limitation, we only show the result of 1 + 1.2, and the case of 1 + 1, \u00b2 is presented in the appendix."}, {"heading": "5.1. Efficiency of the Proposed Projection Algorithms", "text": "We first compare our methods with the Interior Point (IP) method and the alternative projection methods, which are also applicable to the solution of the Euclidean projection problem. < p > p > p > p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "5.2. Efficiency of the Projection-based Methods in Linear Regression with Composite Norm Constraints", "text": "In this section, we show that with our proposed projection operator, various projection-based methods q = six points per year can efficiently optimize the composite standards-related linear regression problem. These projection-based methods significantly exceed the basic method in terms of speed. We embed our projection operator in three types of projection-based optimization frames, including the Projected Gradient Method (PG), Nesterov's Optimal First Order Method (Nesterov), and the Projected QuasiNewton Method (PQN). We synthesize a small-format dataset and a medium-size dataset. For the small-format data, we perform the experimental conversion into (Friedman et al., 2010). We generate the coefficient vector w-R100 divided into ten blocks in which the last four blocks are split into all and the first six blocks have 10, 8, 6, 4, 2, and 1 non-zero entries."}, {"heading": "5.3. Classification Performance in Multi-task Learning", "text": "In this experiment, we show that the use of our efficient projection operator, with limited additional time costs, exceeds the compound q q standard regularizer in multi-task learning. In the multi-task learning environment, there are r > 1 response variables (each corresponding to a task) and a common set of p-characteristics. We assume that if the relevant characteristics for each task are sparse and there is a large overlap of these relevant characteristics between the tasks extracted from a collection of Dutch supply cards (Asuncion & Newman, 2007), both the thrift of each task and the thrift shared between the tasks will be restored. We use handwritten digit recognition as a test case. The input data are characteristics of handwritten digits (0-9) extracted from a collection of Dutch supply cards (Asuncion & Newman, 2007). This data set has been used by a number of papers as a reliable data set for handwriting algorithms (2010)."}], "references": [{"title": "Convex optimization with sparsity-inducing norms", "author": ["Bach", "Francis", "Jenatton", "Rodolphe", "Mairal", "Julien", "Obozinski", "Guillaume"], "venue": "Optimization for Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Efficient projections onto the l1-ball for learning in high dimensions", "author": ["Duchi", "John C", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In ICML, pp", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "An algorithm for restricted least squares regression", "author": ["Dykstra", "Richard L"], "venue": "JASA, 78(384):837\u2013842,", "citeRegEx": "Dykstra and L.,? \\Q1983\\E", "shortCiteRegEx": "Dykstra and L.", "year": 1983}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "Arxiv preprint arXiv10010736,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["Gabay", "Daniel", "Mercier", "Bertrand"], "venue": "Computers & Mathematics with Applications,", "citeRegEx": "Gabay et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Gabay et al\\.", "year": 1976}, {"title": "Efficient euclidean projections via piecewise root finding and its application in gradient", "author": ["Gong", "Pinghua", "Gai", "Kun", "Zhang", "Changshui"], "venue": "projection. Neurocomputing,", "citeRegEx": "Gong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2011}, {"title": "A successive projection method", "author": ["Han", "Shih-Ping"], "venue": "Mathematical Programming,", "citeRegEx": "Han and Shih.Ping.,? \\Q1988\\E", "shortCiteRegEx": "Han and Shih.Ping.", "year": 1988}, {"title": "A dirty model for multi-task learning", "author": ["Jalali", "Ali", "Ravikumar", "Pradeep D", "Sanghavi", "Sujay", "Ruan", "Chao"], "venue": "In NIPS, pp", "citeRegEx": "Jalali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2010}, {"title": "Efficient euclidean projections in linear time", "author": ["Liu", "Jun", "Ye", "Jieping"], "venue": "In ICML, pp", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Efficient l1/lq Norm Regularization", "author": ["Liu", "Jun", "Ye", "Jieping"], "venue": "Arxiv preprint arXiv:1009.4766,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Nesterov", "Yu"], "venue": "ReCALL,", "citeRegEx": "Nesterov and Yu.,? \\Q2007\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2007}, {"title": "A convergence analysis of dykstra\u2019s algorithm for polyhedral sets", "author": ["Perkins", "Chris"], "venue": "SIAM J. Numerical Analysis,", "citeRegEx": "Perkins and Chris.,? \\Q2002\\E", "shortCiteRegEx": "Perkins and Chris.", "year": 2002}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["Quattoni", "Ariadna", "Carreras", "Xavier", "Collins", "Michael", "Darrell", "Trevor"], "venue": "In ICML, pp", "citeRegEx": "Quattoni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Convex structure learning in log-linear models: Beyond pairwise potentials", "author": ["Schmidt", "Mark W", "Murphy", "Kevin P"], "venue": "In AISTATS, pp", "citeRegEx": "Schmidt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2010}, {"title": "Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton", "author": ["Schmidt", "Mark W", "van den Berg", "Ewout", "Friedlander", "Michael P", "Murphy", "Kevin P"], "venue": null, "citeRegEx": "Schmidt et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2009}, {"title": "Generalized proximity and projection with norms and mixed-norms", "author": ["Sra", "Suvrit"], "venue": "In Technique Report,", "citeRegEx": "Sra and Suvrit.,? \\Q2010\\E", "shortCiteRegEx": "Sra and Suvrit.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "For example, in the multi-task learning setting, the index set for features of each task may be sparse and there might be a large overlap of features across multiple tasks (Jalali et al., 2010).", "startOffset": 172, "endOffset": 193}, {"referenceID": 4, "context": "One natural approach is to formalize an optimization problem constrained by l1 norm and l1,q norm together, so that l1 norm induces the sparsity in features of each task and l1,q norm couples the sparsity across tasks(Friedman et al., 2010).", "startOffset": 217, "endOffset": 240}, {"referenceID": 15, "context": "Projection-based algorithms such as Projected Gradient (Bertsekas, 1999), Nesterov\u2019s optimal first-order method (Beck & Teboulle, 2009; Nesterov, 2007) and Projected Quasi-Newton (Schmidt et al., 2009) are major approaches to minimize a convex function with constraints.", "startOffset": 179, "endOffset": 201}, {"referenceID": 0, "context": "We choose q = 2 or \u221e because these two types of norms are the most widely used group-sparsity inducing norms(Bach et al., 2011).", "startOffset": 108, "endOffset": 127}, {"referenceID": 2, "context": "Linear time projection algorithms for l1 norm and l1,2 norm are proposed by (Duchi et al., 2008; Liu & Ye, 2009) and (Schmidt et al.", "startOffset": 76, "endOffset": 112}, {"referenceID": 15, "context": ", 2008; Liu & Ye, 2009) and (Schmidt et al., 2009), respectively.", "startOffset": 28, "endOffset": 50}, {"referenceID": 13, "context": "For the l1,\u221e norm, (Quattoni et al., 2009) proposes a method with O(n log n) complexity and (Sra, 2010) introduces a method with weak linear time complexity.", "startOffset": 19, "endOffset": 42}, {"referenceID": 6, "context": "Recently, (Gong et al., 2011) solves the projection onto the Intersection of Hyperplane and a Halfspace by PRF method in linear time via root finding in 1-D space.", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "1 in the appendix) is satisfied (Bach et al., 2011).", "startOffset": 32, "endOffset": 51}, {"referenceID": 13, "context": "Note that the formulation above differs from (Quattoni et al., 2009) in the additional term \u2225x\u22251 \u2264 \u03c42.", "startOffset": 45, "endOffset": 68}, {"referenceID": 13, "context": "For a given \u03bb2, \u2200i, j,max(ci,j \u2212 \u03bb2, 0) is determined, (Quattoni et al., 2009) shows that \u03bb1 and d \u2217 can be solved with time complexity O(n log n).", "startOffset": 55, "endOffset": 78}, {"referenceID": 4, "context": "For the small-sized data, we adopt the experiment setting in (Friedman et al., 2010).", "startOffset": 61, "endOffset": 84}, {"referenceID": 8, "context": "This dataset has been used by a number of papers as a reliable dataset for handwritten recognition algorithms(Jalali et al., 2010).", "startOffset": 109, "endOffset": 130}, {"referenceID": 8, "context": "Our performance is on par with state-of-the-art (Jalali et al., 2010).", "startOffset": 48, "endOffset": 69}], "year": 2012, "abstractText": "Using sparse-inducing norms to learn robust models has received increasing attention from many fields for its attractive properties. Projection-based methods have been widely applied to learning tasks constrained by such norms. As a key building block of these methods, an efficient operator for Euclidean projection onto the intersection of l1 and l1,q norm balls (q = 2 or \u221e) is proposed in this paper. We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the problem. We show that the time complexity of our solution is O(n + g log g) for q = 2 and O(n log n) for q = \u221e, where n is the dimensionality of the vector to be projected and g is the number of disjoint groups; we confirm this complexity by experimentation. Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage. We further show that embedded with our efficient projection operator, projectionbased algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy.", "creator": " TeX output 2012.05.19:1810"}}}