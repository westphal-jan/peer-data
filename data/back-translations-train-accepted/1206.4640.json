{"id": "1206.4640", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Stability of matrix factorization for collaborative filtering", "abstract": "We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design.", "histories": [["v1", "Mon, 18 Jun 2012 15:18:05 GMT  (423kb)", "http://arxiv.org/abs/1206.4640v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.NA cs.LG stat.ML", "authors": ["yu-xiang wang", "huan xu"], "accepted": true, "id": "1206.4640"}, "pdf": {"name": "1206.4640.pdf", "metadata": {"source": "META", "title": "Stability of Matrix Factorization for Collaborative Filtering", "authors": ["Yu-Xiang Wang", "Huan Xu"], "emails": ["yuxiangwang@nus.edu.sg", "mpexuh@nus.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have had in the past, and they will be able to play by the rules they have had in the past, and they will be able to play by the rules that they have had in the past, and they will be able to play by the rules that they have had in the past."}, {"heading": "2. Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Matrix Factorization with Missing Data", "text": "Let the user ratings of articles (such as movies) form a matrix Y in which each column corresponds to a user and each row corresponds to an element. So, the ijtest entry is the rating of item-i of user-j. The valid range of rating is [\u2212 k, + k. Y is assumed to be the rank-r matrix, so there is a factorization of this rating matrix Y = UV T, where Y-Rm \u00b7 n, U-Rm \u00b7 r, V-Rn \u00b7 r. Without loss of generality, we assume that m \u2264 n will run through the entire paper. Collaborative filtering is in the process of restoring the rating matrix of a fraction of the entries possibly corrupted by noise or error. That is, we observe Y-Rm \u00b7 r for (ij) the sampling set (assuming that it is uniformly random) and Y-rate is a corrupted copy of Y, and we want Y to restore this to restore the rating J (assuming that J is a suborder of the J)."}, {"heading": "2.2. Matrix Factorization as Subspace Fitting", "text": "As shown in Chen (2008), an alternative interpretation of collaborative filtering is the adjustment of the optimal r-dimensional subspace N to the collected data. That1In practice, this means that the user's preference for movies is influenced by no more than r latent factors. (3) where yi are the observed entries in the i column of Y, N is a m \u00b7 r matrix representing an orthonormal basis of N, Ni is the restriction of N to the observed entries in column i, and Pi = Ni (NTi Ni) \u2212 1NTi is the projection onto the chip (Ni).After the solution (3) we can estimate the full matrix in a column by the way in which (4) the column between the y space (NTi Ni) and the space (Ni) x x x x is calculated."}, {"heading": "2.3. Algorithms", "text": "We focus on the stability of the global optimal solution of problem (1). Since problem (1) is not convex, finding the global optimum is generally not trivial. Although this is certainly an important question, it goes beyond the scope of this paper. Instead, we briefly review some results on this aspect. Perhaps the simplest algorithm for (1) is the alternately least square method (ALS), which alternately minimizes the objective function via U and V to convergence. More sophisticated second-order algorithms such as Wiberg, Damped Newton and Levenberg Marquadt are proposed with better convergence rates, as in Okatani & Deguchi (2007). Specific variations for CF are investigated in Taka'cs et al. (2008) and Koren et al. (2009). From an empirical perspective, Mitra et al. (2010) reported that the global optimum is frequently achieved in Simulation and Chen (2008)."}, {"heading": "3. Stability", "text": "We show in this section that if a sufficient number of entries are evaluated, the global optimal solution of the factorization methods is stable in relation to noise - i.e., it restores a matrix that is \"close\" to the ground truth. This is measured by the Root Mean Square Error (RMSE): RMSE = 1 \u00b0 mn, Y \u00b2 -Y \u00b2 (6) Theorem 1. There is an absolute constant C, so it is likely that at least 1 \u2212 2 exp (\u2212 n), RMSE \u2264 1 \u00b0 (E), F \u00b2 E \u00b2, F \u00b2 F \u00b2 mn, Ck \u00b2 (n), 1 \u00b2 n, the last term is diminished, and RMSE \u00b2, which is essentially stable by the \"mean\" size of the entries of E \u00b2, i.e. the factorization methods compared with the factorization methods."}, {"heading": "3.1. Proof of Stability Theorem", "text": "First we explain briefly the idea of proof: The algorithm finds by definition the optimal rank R matrix, measured with regard to the root of the middle square (RMS) on the sampled entries. To show this, we have to specify a small RMS value on the entire matrix. To bind these RMS, we need the following theory. (Theorem 2) Let L (X) = 1 (X) = 1 (X \u2212 Y) x (X \u2212 Y) x (X \u2212 Y) x (X \u2212 F) the empirical and actual loss function. (In addition, the initial constraint maxi, j | Xi, j \u2212 k is assumed. Then the empirical and actual loss function Y applies to all rank r matrices X \u2212 X \u2212 X \u2212 Y."}, {"heading": "4. Subspace Stability", "text": "In this section, we examine the stability of the restored subspace using matrix factorization methods. Let's remember that matrix factorization methods assume that in the idealized noiseless case, each user's preference belongs to a subspace. If this subspace can be easily restored, we can predict the preferences of a new user without having to run the matrix factorization algorithms again. We analyze the latter, prediction error among individual users, in Section 5. To illustrate the difference between the stability of the restored matrix and that of the restored subspace, let's consider a concrete example in a movie recommendation where there are both honest users and malicious manipulators in the system. Let's say we get an output subspace N-by (3) and the missing evaluations are filled by (4). If N of the stability of the restored matrix is made by the other user's \"very close to the predicted stability,\" then all of the subspace is manipulated for the other great one."}, {"heading": "4.1. Subspace Stability Theorem", "text": "Let us designate the corresponding m \u00b7 r and n \u00b7 r orthonormal base matrix of vector spaces using N, M, N, N, M, M, M, etc. If Y is disturbed by additive error E and observed only on the basis of it, then there is a \"satisfactory\" value (N, N) and a \"satisfactory\" value (E)."}, {"heading": "4.2. Proof of Subspace Stability", "text": "The proof for theorem 3 has been provided, and the proof for this has been provided when a result applies to both the Frobenious norm and the spectral norm. We prove the two parts separately. Part 1: Canonical Angles. Let us prove the deviation of spaces spanned by the uppermost singular vectors of Y and Y. Our most important tools are Weyl's theorem and Wedin's theorem (Lemma F. 1 and F. 2 in Appendix F.), expressing singular value composition of Y and Y in block matrix form, which we imply in (F. 1) and (F. 2) of the theorem of Appendix M and the theorem of Wedin."}, {"heading": "5. Prediction Error of individual user", "text": "In this section, we analyze how reliably we can predict the ratings of a new user based on the subspace gained by matrix factorization methods. In particular, we link the prediction that y is the estimate of partial ratings using (4) to y. Without losing generality, we assume that observations occur in first hour entries, so that y = (y1 y2) is observed with y1 and y2 is unknown."}, {"heading": "5.1. Prediction of y With Missing data", "text": "Theorem 4: With all the above notations and definitions, and let N1 denote the restriction of N to the observed entries of y. Then, the prediction for y-N gnd is the smallest non-zero singular value of N1 (rth if N1 is not degenerated). Proof. by (4), and remember that only the first pm entries are observed, we have the smallest non-null value of N \u00b7 pinv (N1) y1: = (y1 \u2212 e, 1 y2 \u2212 e): = y + e. Let y be the vector we get by projecting y to the sub-space N, and denote-y-y-gny-y-y-y-y-y = (1-y-y, Y-y-y = 1 \u2212 y-y)."}, {"heading": "5.2. Bound on \u03c3min", "text": "To complete the above analysis, we now have \u03c3min. Note that \u03c3min can generally be arbitrarily close to zero if N is \"pointed.\" Therefore, in order to avoid such a situation, we place the proof of the following on the strong incoherence property introduced in Candes & Tao (2010) (see Appendix C for definition). Due to space constraints, we move the proof of the following to Appendix C.Proposition 1. If matrix Y for randomly generated matrices has a strong incoherence property with the parameter \u00b5, then consider the case where Y = UV, where U, V are two Gaussian random matrices of size m \u00d7 r and r \u00d7 n, as an example, where the random matrices of size m \u00b7 r \u2212 n are possible for randomly generated matrices."}, {"heading": "6. Robustness against manipulators", "text": "According to the empirical study Mobasher et al. (2006), matrix factorization as a model-based CF algorithm is more resilient to such attacks than similar CF algorithms like kNN. However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that a model-based recommendation system is robust, but rather is because common attack programs based on similarity do not exploit the vulnerability of the model-based approach. Our discovery is consistent with both Mobasher et al. (2006) and Cheng & Hurley (2010). In particular, we show that factorization methods are resilient to a class of common attack models, but are not as general.4 Hence N is also the orthonormal basis of any Y generated when G is its left multiplier."}, {"heading": "6.1. Attack models", "text": "Depending on the purpose, attackers can insert dummy profiles in many ways. Models of different attack strategies are examined in Mobasher et al. (2007). For simplicity, we suggest to divide the attack models into two different categories: Targeted Attack and Mass Attack.Targeted Attacks include average attacks (Lam & Riedl, 2004), segment attacks and bandwagon attacks (Mobasher et al., 2007).The common characteristic of targeted attacks is that they claim to be honest users in all ratings, except for a few targets of interest. Thus, any dummy user can be broken down into: e = egnd + s, where Egnd-N and s are sparse. Mass attacks include random attacks, hate attacks out of love (Mobasher et al., 2007), and others. The common feature of mass attacks is that they insert dummy users in such a way that many entries are manipulated."}, {"heading": "6.2. Robustness analysis", "text": "(Definition, injected user profiles are column-wise: each user corresponds to a corrupt column in the data matrix.) (Definition, injected user profiles are column-wise: each user corresponds to a corrupt column in the data matrix. (Definition, injected user profiles are column-wise: each user corresponds to a corrupt column in the data matrix.) (Definition, injected user: each user is a corrupt error that includes all dummy users.) Since we only care about predicting honest user ratings, we can, without loss of generality, consider the basic probability as [Y-Egnd] and the additive error to be [0-Egnd-Egnd]. Thus, the recovery error Z = [Y-Y-E-Egnd]. (Assuming that all conditions of Theorem 1 hold \"Targeted\" Attacks \"have an absolute constant 4SE)."}, {"heading": "6.3. Simulation", "text": "To verify our robustness paradigm, we ran simulations for both attack models. Y is generated by multiplying two 1000 x 10 Gaussian random matrix and an attacker is appended to the back of Y. Targeted attacks are generated by randomly selecting from a column of Y and assigning 2 \"shock\" or 2 \"nuclear targets\" to 1 or -1, respectively. Mass attacks are generated by evenly distributing them. Factorization is done using ALS. The results of the simulation are 5. This assumption serves to simplify the proof. It remains slightly below i.i.d. samples. Summarized in Figure 1 and Figure 2. Figure 1 compares the RMSE among two attack models. It shows that if the number of attackers increases, the RMSE under targeted attack remains small, while the RMSE increases significantly under random attack. Figure 2 compares RMSEE and RMSEY under random attack."}, {"heading": "7. Concluding discussions", "text": "This paper presented a comprehensive study on the stability of matrix factorization methods. Key findings include a near-optimal stability limit, a partial stability limit, and a worst-case limit for individual columns. Subsequently, the theory is applied to the notorious manipulator problem of collaborative filtering, leading to interesting insights into the inherent robustness of MF. Matrix factorization is an important tool for both matrix completion tasks and for PCAs with missing data. However, its practical success depends on its stability - the ability to tolerate noise and corruption. This work is a first attempt to understand the stability of matrix factorization, which we hope will guide the application of matrix factorization methods.We list some possible directions to expand this research in the future. On the theoretical front, the most important open question is under which conditions matrix factorization can achieve a solution close to global optimum."}, {"heading": "Acknowledgments", "text": "This research is supported in part by the National University of Singapore as part of a startup grant R-265000-384-133."}], "references": [{"title": "Spectral analysis of data", "author": ["Y. Azar", "A. Fiat", "A.R. Karlin", "F. McSherry", "J. Saia"], "venue": "In STOC, pp", "citeRegEx": "Azar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2001}, {"title": "Lessons from the netflix prize challenge", "author": ["R.M. Bell", "Y. Koren"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Bell and Koren,? \\Q2007\\E", "shortCiteRegEx": "Bell and Koren", "year": 2007}, {"title": "Matrix completion with noise", "author": ["E.J. Candes", "Y. Plan"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Candes and Plan,? \\Q2010\\E", "shortCiteRegEx": "Candes and Plan", "year": 2010}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Info. Theory,", "citeRegEx": "Candes and Tao,? \\Q2010\\E", "shortCiteRegEx": "Candes and Tao", "year": 2010}, {"title": "Optimization algorithms on subspaces: Revisiting missing data problem in low-rank matrix", "author": ["P. Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2008\\E", "shortCiteRegEx": "Chen", "year": 2008}, {"title": "Robustness analysis of modelbased collaborative filtering systems", "author": ["Z. Cheng", "N. Hurley"], "venue": "In AICS\u201909, pp", "citeRegEx": "Cheng and Hurley,? \\Q2010\\E", "shortCiteRegEx": "Cheng and Hurley", "year": 2010}, {"title": "Local operator theory, random matrices and banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook of the geometry of Banach spaces,", "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "Competitive recommendation systems", "author": ["P. Drineas", "I. Kerenidis", "P. Raghavan"], "venue": "In STOC, pp", "citeRegEx": "Drineas et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2002}, {"title": "Matrix completion from noisy entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "JMLR, 11:2057\u20132078,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Info. Theory,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "IEEE Tran. Computer,", "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Shilling recommender systems for fun and profit", "author": ["S.K. Lam", "J. Riedl"], "venue": "In WWW\u201904,", "citeRegEx": "Lam and Riedl,? \\Q2004\\E", "shortCiteRegEx": "Lam and Riedl", "year": 2004}, {"title": "Smallest singular value of random matrices and geometry of random polytopes", "author": ["A.E. Litvak", "A. Pajor", "M. Rudelson", "N. TomczakJaegermann"], "venue": "Advances in Mathematics,", "citeRegEx": "Litvak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Litvak et al\\.", "year": 2005}, {"title": "Large-scale matrix factorization with missing data under additional constraints", "author": ["K. Mitra", "S. Sheorey", "R. Chellappa"], "venue": null, "citeRegEx": "Mitra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitra et al\\.", "year": 2010}, {"title": "Model-based collaborative filtering as a defense against profile injection attacks", "author": ["B. Mobasher", "R. Burke", "J.J. Sandvig"], "venue": "In AAAI\u201906,", "citeRegEx": "Mobasher et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mobasher et al\\.", "year": 2006}, {"title": "Toward trustworthy recommender systems: An analysis of attack models and algorithm robustness", "author": ["B. Mobasher", "R. Burke", "R. Bhaumik", "C. Williams"], "venue": "ACM Tran. Inf. Tech.,", "citeRegEx": "Mobasher et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mobasher et al\\.", "year": 2007}, {"title": "On the wiberg algorithm for matrix factorization in the presence of missing components", "author": ["T. Okatani", "K. Deguchi"], "venue": null, "citeRegEx": "Okatani and Deguchi,? \\Q2007\\E", "shortCiteRegEx": "Okatani and Deguchi", "year": 2007}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Rudelson and Vershynin,? \\Q2009\\E", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2009}, {"title": "The smallest eigenvalue of a large dimensional wishart matrix", "author": ["J.W. Silverstein"], "venue": "The Annals of Probability,", "citeRegEx": "Silverstein,? \\Q1985\\E", "shortCiteRegEx": "Silverstein", "year": 1985}, {"title": "A unified view of matrix factorization models", "author": ["A. Singh", "G. Gordon"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Singh and Gordon,? \\Q2008\\E", "shortCiteRegEx": "Singh and Gordon", "year": 2008}, {"title": "Learning with matrix factorizations", "author": ["N. Srebro"], "venue": "PhD thesis, M.I.T.,", "citeRegEx": "Srebro,? \\Q2004\\E", "shortCiteRegEx": "Srebro", "year": 2004}, {"title": "Matrix perturbation theory", "author": ["G.W. Stewart", "J. Sun"], "venue": "Academic press New York,", "citeRegEx": "Stewart and Sun,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun", "year": 1990}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Adv. in AI,", "citeRegEx": "Su and Khoshgoftaar,? \\Q2009\\E", "shortCiteRegEx": "Su and Khoshgoftaar", "year": 2009}, {"title": "Investigation of various matrix factorization methods for large recommender systems", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "B. N\u00e9meth", "D. Tikk"], "venue": "In ICDMW\u201908,", "citeRegEx": "Tak\u00e1cs et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tak\u00e1cs et al\\.", "year": 2008}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm", "author": ["Z. Wen"], "venue": "Rice University CAAM Technical Report,", "citeRegEx": "Wen,? \\Q2010\\E", "shortCiteRegEx": "Wen", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Among various models proposed, matrix factorization is arguably the most widely applied method, due to its high accuracy, scalability (Su & Khoshgoftaar, 2009) and flexibility to incorporating domain knowledge (Koren et al., 2009).", "startOffset": 210, "endOffset": 230}, {"referenceID": 0, "context": "Azar et al. (2001) analyzed asymptotic performance of matrix factorization methods, yet under stringent assumptions on the fraction of observation and on the singular values.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Azar et al. (2001) analyzed asymptotic performance of matrix factorization methods, yet under stringent assumptions on the fraction of observation and on the singular values. Drineas et al. (2002) relaxed these assumptions but it requires a few fully rated users \u2013 a situation that rarely happens in practice.", "startOffset": 0, "endOffset": 197}, {"referenceID": 0, "context": "Azar et al. (2001) analyzed asymptotic performance of matrix factorization methods, yet under stringent assumptions on the fraction of observation and on the singular values. Drineas et al. (2002) relaxed these assumptions but it requires a few fully rated users \u2013 a situation that rarely happens in practice. Srebro (2004) considered the problem of the generalization error of learning a low-rank matrix.", "startOffset": 0, "endOffset": 324}, {"referenceID": 8, "context": "Recently, some alternative algorithms, notably StableMC (Candes & Plan, 2010) based on nuclear norm optimization, and OptSpace (Keshavan et al., 2010b) based on gradient descent over the Grassmannian, have been shown to be stable vis a vis noise (Candes & Plan, 2010; Keshavan et al., 2010a). However, these two methods are less effective in practice. As documented in Mitra et al. (2010); Wen (2010) and many others, matrix factorization methods typically outperform these two methods.", "startOffset": 128, "endOffset": 389}, {"referenceID": 8, "context": "Recently, some alternative algorithms, notably StableMC (Candes & Plan, 2010) based on nuclear norm optimization, and OptSpace (Keshavan et al., 2010b) based on gradient descent over the Grassmannian, have been shown to be stable vis a vis noise (Candes & Plan, 2010; Keshavan et al., 2010a). However, these two methods are less effective in practice. As documented in Mitra et al. (2010); Wen (2010) and many others, matrix factorization methods typically outperform these two methods.", "startOffset": 128, "endOffset": 401}, {"referenceID": 4, "context": "As pointed out in Chen (2008), an alternative interpretation of collaborative filtering is fitting the optimal r-dimensional subspace N to the sampled data.", "startOffset": 18, "endOffset": 30}, {"referenceID": 20, "context": "Specific variations for CF are investigated in Tak\u00e1cs et al. (2008) and Koren et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 9, "context": "(2008) and Koren et al. (2009). From an empirical perspective, Mitra et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 9, "context": "(2008) and Koren et al. (2009). From an empirical perspective, Mitra et al. (2010) reported that the global optimum is often obtained in simulation and Chen (2008) demonstrated satisfactory percentage of hits to global minimum from randomly initialized trials on a real data set.", "startOffset": 11, "endOffset": 83}, {"referenceID": 4, "context": "(2010) reported that the global optimum is often obtained in simulation and Chen (2008) demonstrated satisfactory percentage of hits to global minimum from randomly initialized trials on a real data set.", "startOffset": 76, "endOffset": 88}, {"referenceID": 8, "context": "We recall similar RMSE bounds for StableMC of Candes & Plan (2010) and OptSpace of Keshavan et al. (2010a):", "startOffset": 83, "endOffset": 107}, {"referenceID": 18, "context": "The main idea is to apply established results about the singular values of Gaussian random matrix G (e.g., Rudelson & Vershynin, 2009; Silverstein, 1985; Davidson & Szarek, 2001), then show that the orthogonal basis N of G is very close to G itself.", "startOffset": 100, "endOffset": 178}, {"referenceID": 12, "context": "We remark that the bound on singular values we used has been generalized to random matrices following subgaussian (Rudelson & Vershynin, 2009) and logconcave distributions (Litvak et al., 2005).", "startOffset": 172, "endOffset": 193}, {"referenceID": 13, "context": "According to the empirical study of Mobasher et al. (2006), matrix factorization, as a model-based CF algorithm, is more robust to such attacks compared to similarity-based CF algorithms such as kNN.", "startOffset": 36, "endOffset": 59}, {"referenceID": 4, "context": "However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommendation system is robust.", "startOffset": 12, "endOffset": 34}, {"referenceID": 4, "context": "However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommendation system is robust. Rather, it may due to the fact that that common attack schemes, effective to similarity based-approach, do not exploit the vulnerability of the model-based approach. Our discovery is in tune with both Mobasher et al. (2006) and Cheng & Hurley (2010).", "startOffset": 12, "endOffset": 357}, {"referenceID": 4, "context": "However, as Cheng & Hurley (2010) pointed out, it may not be a conclusive argument that model-based recommendation system is robust. Rather, it may due to the fact that that common attack schemes, effective to similarity based-approach, do not exploit the vulnerability of the model-based approach. Our discovery is in tune with both Mobasher et al. (2006) and Cheng & Hurley (2010). Specifically, we show that factorization methods are resilient to a class of common attack models, but are not so in general.", "startOffset": 12, "endOffset": 383}, {"referenceID": 14, "context": "Models of different attack strategies are surveyed in Mobasher et al. (2007). For convenience, we propose to classify the models of attack into two distinctive categories: Targeted Attack and Mass Attack.", "startOffset": 54, "endOffset": 77}, {"referenceID": 15, "context": "Targeted Attacks include average attack (Lam & Riedl, 2004), segment attack and bandwagon attack (Mobasher et al., 2007).", "startOffset": 97, "endOffset": 120}, {"referenceID": 15, "context": "Mass Attacks include random attack, love-hate attack (Mobasher et al., 2007) and others.", "startOffset": 53, "endOffset": 76}], "year": 2012, "abstractText": "We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design.", "creator": "LaTeX with hyperref package"}}}