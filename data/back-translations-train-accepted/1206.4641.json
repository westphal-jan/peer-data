{"id": "1206.4641", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Total Variation and Euler's Elastica for Supervised Learning", "abstract": "In recent years, total variation (TV) and Euler's elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler's elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods.", "histories": [["v1", "Mon, 18 Jun 2012 15:18:20 GMT  (267kb)", "http://arxiv.org/abs/1206.4641v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["tong lin", "hanlin xue", "ling wang", "hongbin zha"], "accepted": true, "id": "1206.4641"}, "pdf": {"name": "1206.4641.pdf", "metadata": {"source": "CRF", "title": "Total Variation and Euler\u2019s Elastica for Supervised Learning", "authors": ["Tong Lin", "Hanlin Xue", "Ling Wang", "Hongbin Zha"], "emails": ["tonglin123@gmail.com", "xuehl@cis.pku.edu.cn", "ling.wang.nj@gmail.com", "zha@cis.pku.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "The question at stake is to what extent it is actually a matter of a \"real\" or \"real\" loss, of a \"real\" or \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, of a \"real\" debt, \"of a\" real debt, \"of a\" real debt, \"of a\" real debt, \"of a\" real debt, \"of a\" real debt. \""}, {"heading": "2. Preliminaries", "text": "We briefly present the total variation and Euler's elasticity from the perspective of image processing and point to connections with earlier work in machine learning literature."}, {"heading": "2.1. Total Variation (TV)", "text": "The total variation of a 1D real-value function f is defined asV ab (f) = sup np \u2212 1 \u2211 i = 0 | f (xi + 1) \u2212 f (xi) |, where the upper hand goes over all ranges of the given interval [a, b]. If f \u00b2 (x) > 0, x \u00b2 [a, b] can be written, it is exactly f (b) \u2212 f (a) according to the basic theorem of calculation. The total variation has been widely used for image processing tasks such as denoization and inpainting. The groundbreaking work is Rudin, Osher and Fatemi's Image Denoising Model (Rudin et al., 1992): J = (I \u2212 I0) Production-Edit."}, {"heading": "2.2. Euler\u2019s Elastica (EE)", "text": "Euler (1744) first introduced elastic energy for a curve for modelling torsion-free elastic rods, then Mumford (Mumford, 1991) reintroduced elastics into computer vision, and later, elastic-based image information methods were developed (Chan et al., 2002; Masnou & Morel, 1998).A curve is called Eulers elastics when it comes to the equilibrium curve of elastic energy. Euler obtained the energy when investigating the steady shape of a thin and torsion-free rod under external forces, (1) where a and b stand for two positive constant weights, and Euler refers to the scalar curvature, and ds is the arc-length element. Euler obtained the energy when investigating a thin and torsion-free rod under external forces."}, {"heading": "3. The Proposed Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Problem Setup", "text": "The general problem of supervised learning can be described as follows: In the light of a training dataset {(x1, y1),... (xn, yn) with the data points xi and the corresponding target variables yi, the aim is to estimate an unknown function u (x) for a new point x. The difference between classification and regression lies only in the corresponding target values, with one discrete and the other continuous value. The widely used Tikhonov regulatory framework for supervised learning can be formulated as follows: min n \u00b2 i = 1L (u (xi), yi) + \u03bbS (u), (4) where L is a loss function and S (u) a smoothing term. In the literature, a variety of loss functions L was suggested: hinge loss for SVM, square loss for RLS, logistic loss for logistic regression, coughing losses, exponential loss, and others."}, {"heading": "3.2. Laplacian Regularization (LR)", "text": "A commonly used model using quadratic loss can be written asmin n \u2211 i = 1 (u (xi) \u2212 yi) 2 + \u03bbS (u). (5) When the RKHS standard is used for the smoothing term, the model is referred to as regularized smallest squares (RLS) (Rifkin, 2002). Another natural choice is the quadratic L2 standard of the gradient: S (u) = | bay u | 2, as proposed in (Belkin et al., 2006). (6) This LR model is widely used in image processing literature. Using variation calculations, minimization can be reduced to the following EulerLagrange partial differential equations (PDE) with a natural boundary condition along quadratic lines."}, {"heading": "3.3. Total Variation (TV) based Smoothing", "text": "A typical procedure consists of three steps: (a) set the functional learning problem under a continuous setting and design a suitable energy functionality; (b) derive the Euler Lagrange PDE from the variation calculation; (c) solve the PDE to discrete data points. (8) Note that in binary classification, the zero plane of u serves as the final decision boundary; the only difference between LR and TV is only the p-Sobolev regulator with p = 2 for LR and p = 1 for TV. Intuitively, LR punishes too many gradients at the edges, while television can allow sharp edges between two classes."}, {"heading": "3.4. Euler\u2019s Elastica (EE) based Smoothing", "text": "Elasticity-based supervised learning may be formulated as follows: JEE [] = u \u2212 y) 2dx + b\u03ba2 | \u0441u | dx, (9) where\u0442 = Elasticity-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-based counselling-counselling-based counselling-based counselling-counselling-on-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-counselling-councounselling-counselling-counselling-councouncounselling-councouncounselling-councouncouncouncounselling-councouncouncouncounselling-counselling-counselling-councouncouncouncouncounselling-councouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncouncoun"}, {"heading": "4. Algorithms", "text": "Unlike discrete methods such as SVM and Laplacian graphs, the proposed framework works in a continuous manner in which powerful mathematical analysis tools can make sense. Specifically, the variation calculation can be used to minimize energy functionality, leading to the Euler Long Range Method. As we mentioned in Section 3, the functional minimization of the LR can be converted into the solution of the PDE (7). Likewise, we get the following PDE for the TV model. \u00b7 (u \u2212 y) = 0, (11) and the PDE for the EE model. \u00b7 V \u2212 (u \u2212 y) = 0, (12) whereas V = 1, (a \u2212 u \u2212 u \u2212 u)."}, {"heading": "4.1. Radial Basis Function Approximation", "text": "The function approximation idea is based on the fact that a function u (x) can be expressed as the sum of the weighted basic function (x). The target function u can be expressed, for example, by using polynomials. The most common is the radial basic function (RBF), which is simple in expressions but has strong adaptability. The target function u can be expressed by u (x) = n \u00b2 i = 1 (x) (14) with a set of Gaussian RBF (x) = exp (\u2212 c | x \u2212 xi | 2), where {xi} are the training samples in supervised learning and c is a parameter. By using the RBF approximation, the problem can be reduced to determining the coefficient {wi}. Here are some analytical expressions to be used later:"}, {"heading": "4.2. Algorithm for LR", "text": "First, let us consider how to deal with the LR model by solving the linear elliptical PDE (7): \u2212 \u03bb \u0445 u + (u \u2212 y) = 0. By inserting (14) into the PDE and using the linearity of the laplac operator, the goal is to find a series of weights {wi}: \u2211 i [wi (\u03c6i \u2212 \u03bb \u03c6i)] = y.Let w: = (w1, w2,..., wm) T and y: = (y1, y2,... yn) T, where m is the number of basic functions and n is the number of training samples. Then, we have the following linear equation system: \u0394w = y, \u0441ij = \u0441ij (xi) \u2212 xi (xi).Numerically, in practice, we consider the following solution with the least squares as a solution to avoid badly posed problems: min w = 2 + \u03b7 | w | 2.The solution is simply given by (xi)."}, {"heading": "4.3. Algorithm for TV and EE models", "text": "Since the TV model is a special case of the EE model, we describe solutions for the more complicated EE model in this section. Two algorithms are developed to tackle non-linearity: (1) Gradient Descent Time Marching and (2) Lag-Linear Equation Iteration."}, {"heading": "4.3.1. Gradient Descent Time Marching", "text": "Using the variation calculation, we can determine the downward gradient for the desired function. By means of a matrix notation u (x) = 3 (x) = 3 (xi), the gradient of u is given as follows when it is fixed: 3 (k) \u2212 x = x1... x (k) x = x1... x (k) x = xn, where each iteration can be written as a small time step, and u (k) is renewed from w (k) \u2212 x (k) \u2212 x (k) x = x1... x (k) x (k) x = xn, where it is a small time step, and u (k) is renewed from w (k). The coefficients w are initialized as w (0) = (x) \u2212 x (x) x (T) x (1) x) x (T) x) x (T) x x) x (T) x) x (n) x (H) x (H) x) x (H) x (H) x (H) is initialized as a small time step."}, {"heading": "4.3.2. Lagged Linear Equation Iteration", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "5. Experimental Results", "text": "The two proposed approaches (TV and EE) will be compared with LR, SVM (with RBF cores) and BackPropagation Neural Networks (BPNN) to achieve binary classification, multi-class classification and regression in benchmark datasets."}, {"heading": "5.1. Binary Classification", "text": "The test data sets for the binary classification come from the libsvm website. Originally, these data sets are scaled to [0,1] and serve as a benchmark for testing the libsvm implementation. Here, we have downloaded seven data sets to evaluate the performance of our methods (TV and EE) using two types of implementations of the Gradient Descent Method (GD) and the Lagged Linear Equation Method (lagLE). The optimal parameters for each algorithm are selected by the grid search using 5x cross validation. To make the grid search more practical, only the two common parameters (c and \u03bb) for SVM, LR, TV and EE with the exception of BPNN are searched for. Empirically, the parameter \u03b7 is set as 1 for LR, and the parameter b is set as 0.01 for EE. Then, without BPNN, the two common parameters are searched, the logarithms of \u2212 10: 2. For each data set of PNE, we achieve the intersection of the PNE by randomly performing the PNE data set during the PNE classification of the five times the data sets of the PNE."}, {"heading": "5.2. Multi-class Classification", "text": "For multi-class tests, we collected data sets from the libsvm website and the UCI Machine Learning Repository, including commonly used small data sets and the handwritten digital USPS set. For USPS data, PCA is used to reduce the dimension to 30, and we randomly select 1000 samples for experiments. With the exception of BPNN, which has a built-in ability for multi-class tasks, almost all function learning approaches were originally designed for binary classification. To handle multi-class situations, one can usually be chosen against all or one against a strategy. If one is used against all, one must learn M functions to complete the multi-class task, with M being the number of classes. Recently (Varshney & Willsky, 2010), an efficient binary coding strategy was proposed to represent the decision boundary by using only log2M functions. In our experiments, the ones against all strategies are used. Exactly like the binary problems, we use parameters for each PNN to validate only 5 cross-diameter 1."}, {"heading": "5.3. Regression", "text": "We use seven regression data sets from the UCI Machine Learning Repository to validate the proposed methods against SVM, BPNN and LR. All data sets are scaled to [0,1]. Note that the Gradient Descent (GD) method for TV and EE is used here. We perform the same experimental settings by performing ten times a 5x cross-validation for each data set. Table 4 shows the regression results based on mean square errors (MSE). We can clearly see that both TV and EE achieve lower MSE on 6 data sets than SVM and LR. TV and EE also outperform BPNN on 5 data sets. Results showed excellent regression capability of our proposed methods."}, {"heading": "6. Conclusion", "text": "Due to the great success of total variation and Euler elastic models in the field of image processing, we extend these two models for supervised classification and regression to high-dimensional datasets. The TV regulator allows steeper edges close to the decision limits, while the elasticity smoothing term does not penalize smooth, even hyper surfaces of the target function. Compared to SVM and BPNN, our proposed methods have demonstrated competitiveness on commonly used benchmark datasets. In particular, TV and EE models achieve better performance on most datasets for binary classification and regression. Currently, a major drawback is the slow convergence speed of iteration procedures. Future work consists in investigating other suitable loss terms such as hinge losses, investigating other possibilities of basic functions, investigating the existence and uniformity of PDE solutions, and shortening runtime."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for their helpful suggestions. This work was supported by the National Basic Research Program of China (973 program number 2011CB302202) and the National Science Foundation of China (NSFC grant 61075119)."}], "references": [{"title": "Mathematical problems in image processing: partial differential equations and the calculus of variations", "author": ["G. Aubert", "P. Kornprobst"], "venue": null, "citeRegEx": "Aubert and Kornprobst,? \\Q2006\\E", "shortCiteRegEx": "Aubert and Kornprobst", "year": 2006}, {"title": "Fast newton-type methods for total variation regularization", "author": ["A. Barbero", "S. Sra"], "venue": "In ICML,", "citeRegEx": "Barbero and Sra,? \\Q2011\\E", "shortCiteRegEx": "Barbero and Sra", "year": 2011}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": "In ICML, pp", "citeRegEx": "Caruana and Niculescu.Mizil,? \\Q2006\\E", "shortCiteRegEx": "Caruana and Niculescu.Mizil", "year": 2006}, {"title": "Image processing and analysis: variational, PDE, wavelet, and stochastic methods", "author": ["T.F. Chan", "J. Shen"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "Chan and Shen,? \\Q2005\\E", "shortCiteRegEx": "Chan and Shen", "year": 2005}, {"title": "Euler\u2019s elastica and curvature-based inpainting", "author": ["T.F. Chan", "S.H. Kang", "J. Shen"], "venue": "SIAM Journal on Applied Mathematics, pp", "citeRegEx": "Chan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2002}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Machine learning:a review of classification and combining techniques", "author": ["S. Kotsiantis", "I. Zaharakis", "P. Pintelas"], "venue": "In Artificial Intelligence Review,", "citeRegEx": "Kotsiantis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kotsiantis et al\\.", "year": 2006}, {"title": "Level lines based disocclusion", "author": ["S. Masnou", "J.M. Morel"], "venue": "In ICIP, pp. 259\u2013263", "citeRegEx": "Masnou and Morel,? \\Q1998\\E", "shortCiteRegEx": "Masnou and Morel", "year": 1998}, {"title": "Elastica and computer vision", "author": ["D. Mumford"], "venue": "Center for Intelligent Control Systems,", "citeRegEx": "Mumford,? \\Q1991\\E", "shortCiteRegEx": "Mumford", "year": 1991}, {"title": "Semi-supervised learning with the graph laplacian: The limit of infinite unlabelled data", "author": ["B. Nadler", "N. Srebro", "X. Zhou"], "venue": "In NIPS,", "citeRegEx": "Nadler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nadler et al\\.", "year": 2009}, {"title": "Everything old is new again : a fresh look at historical approaches in machine learning", "author": ["R.M. Rifkin"], "venue": "PhD thesis,", "citeRegEx": "Rifkin,? \\Q2002\\E", "shortCiteRegEx": "Rifkin", "year": 2002}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Rudin et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 1992}, {"title": "An introduction to support vector machines and other kernel-based learning methods", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2000\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2000}, {"title": "A comprehensive introduction to differential geometry, volume 4", "author": ["M. Spivak"], "venue": "Publish or perish Berkeley,", "citeRegEx": "Spivak and Spivak,? \\Q1979\\E", "shortCiteRegEx": "Spivak and Spivak", "year": 1979}, {"title": "Classification using geometric level sets", "author": ["K.R. Varshney", "A.S. Willsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Varshney and Willsky,? \\Q2010\\E", "shortCiteRegEx": "Varshney and Willsky", "year": 2010}, {"title": "Regularization on discrete spaces", "author": ["D. Zhou", "B. Sch\u00f6lkopf"], "venue": "In Pattern Recognition,", "citeRegEx": "Zhou and Sch\u00f6lkopf,? \\Q2005\\E", "shortCiteRegEx": "Zhou and Sch\u00f6lkopf", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "Supervised learning (Bishop, 2006; Hastie T., 2009) infers a function that maps inputs to desired outputs under the guidance of training data.", "startOffset": 20, "endOffset": 51}, {"referenceID": 8, "context": "Existing methods can be roughly divided into statistics based and function learning based (Kotsiantis et al., 2006).", "startOffset": 90, "endOffset": 115}, {"referenceID": 3, "context": "The most successful classification and regression method is SVM (Bishop, 2006; Hastie T., 2009; Shawe-Taylor & Cristianini, 2000), whose cost function is composed of a hinge loss and a RKHS norm determined by a kernel.", "startOffset": 64, "endOffset": 129}, {"referenceID": 12, "context": "Replacing the hinge loss by a squared loss, the modified algorithm is called Regularized Least Squares (RLS) method (Rifkin, 2002).", "startOffset": 116, "endOffset": 130}, {"referenceID": 2, "context": "In addition, manifold regularization (Belkin et al., 2006) introduced a regularizer of squared gradient magnitude on manifolds.", "startOffset": 37, "endOffset": 58}, {"referenceID": 11, "context": "Its discrete version amounts to graph Laplacian regularization (Nadler et al., 2009; Zhou & Sch\u00f6lkopf, 2005), which approximates the original energy functional.", "startOffset": 63, "endOffset": 108}, {"referenceID": 13, "context": "The pioneering work is Rudin, Osher, and Fatemi\u2019s image denoising model (Rudin et al., 1992):", "startOffset": 72, "endOffset": 92}, {"referenceID": 2, "context": "on a manifold M (Belkin et al., 2006).", "startOffset": 16, "endOffset": 37}, {"referenceID": 10, "context": "Then Mumford (Mumford, 1991) reintroduced elastica into computer vision.", "startOffset": 13, "endOffset": 28}, {"referenceID": 6, "context": "Later, elastica based image inpainting methods were developed in (Chan et al., 2002; Masnou & Morel, 1998).", "startOffset": 65, "endOffset": 106}, {"referenceID": 10, "context": "According to (Mumford, 1991), the key link between the elastica and image inpainting relies on the the interpolation capability of elastica.", "startOffset": 13, "endOffset": 28}, {"referenceID": 12, "context": "If the RKHS norm is used for the smoothing term, the model is called regularized least squares (RLS) (Rifkin, 2002).", "startOffset": 101, "endOffset": 115}, {"referenceID": 2, "context": "Another natural choice is the squared L2-norm of the gradient: S(u) = |\u2207u|, as proposed in (Belkin et al., 2006).", "startOffset": 91, "endOffset": 112}, {"referenceID": 6, "context": "One can refer to (Chan et al., 2002) for details about the calculus of variations.", "startOffset": 17, "endOffset": 36}], "year": 2012, "abstractText": "In recent years, total variation (TV) and Euler\u2019s elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler\u2019s elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods.", "creator": " TeX output 2012.05.16:1551"}}}