{"id": "1206.4643", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty", "abstract": "We consider Markov decision processes under parameter uncertainty. Previous studies all restrict to the case that uncertainties among different states are uncoupled, which leads to conservative solutions. In contrast, we introduce an intuitive concept, termed \"Lightning Does not Strike Twice,\" to model coupled uncertain parameters. Specifically, we require that the system can deviate from its nominal parameters only a bounded number of times. We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies using this concept.", "histories": [["v1", "Mon, 18 Jun 2012 15:19:07 GMT  (390kb)", "http://arxiv.org/abs/1206.4643v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.GT cs.SY", "authors": ["shie mannor", "ofir mebel", "huan xu"], "accepted": true, "id": "1206.4643"}, "pdf": {"name": "1206.4643.pdf", "metadata": {"source": "META", "title": "Lightning Does Not Strike Twice:  Robust MDPs with Coupled Uncertainty", "authors": ["Shie Mannor", "Ofir Mebel"], "emails": ["shie@ee.technion.ac.il", "ofirmebel@gmail.com", "mpexuh@nus.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "2. Preliminaries", "text": "In this section we briefly explain our notations, some precursor states via MDP, and the setups we study are all dependent on the uncertainty we are exploring. < M > S, A, P, r >: Here T, possibly infinite, is the decision-maker making a reward of r (s, a), and the next state becomes with a probability of p (s, a). We use a subscript s to name the parameters for the state s. For example, rs is the reward vector for various actions in the state s, we assume that the original state is distributed accordingly (\u00b7), the initial state is with a probability p (s, a)."}, {"heading": "3. Non-Adaptive Uncertainty Model", "text": "In this section, we will focus on the non-adaptable case. As mentioned above, the uncertain parameters, although unknown, are set a priori and do not depend on the trajectory of the MDP. Likewise, the decision-maker must define a strategy and cannot adapt to the realization of the parameters, which means that the decision-maker does not observe whether or when the parameters deviate. Our goal is to find a strategy that works best among the least permissible parameter realizations, that is, to solve the following problem: maximum number of HR min (p, r), if or when the parameters deviate."}, {"heading": "3.1. Computational Complexity", "text": "In general, the non-adaptive case - problem 1 - is mathematically difficult. To make this statement formal, we consider the following \"yes / no\" decision problem. Decision problem. In view of a Markov decision process with | S | = n and | A | = m, D [0: n], Us for all s-S and \u03b2-R. Are there phenomena like this: p, r) and UDX (\u03c0, p, r) \u2265 \u03b2? (2) We refer to the decision problem by L (\u0448HR). Likewise, we can define L (\u043fMR) and L (\u0421MD). Next, we show that answering the decision problem is difficult even with very simple uncertainty setting U. This directly implies that it is difficult to find a strategy that (2) satisfies."}, {"heading": "3.2. Reward-Uncertainty Case", "text": "While problem 1 can generally be insoluble, we show that if only the reward parameters are subject to uncertainty, the problem can be solved in the polynomial time. First, we define the concept of the tractable uncertainty set: Definition 1. A quantity U is called tractable if in the polynomial time the following can be solved for each c: Minimize: c > x; s.t. x-U. With this formulation, we specify the main result of this section. The proof is given in Appendix C in the supplement material. Theorem 3. Suppose that only the reward is subject to uncertainty. If for all s-S, we are a tractable uncertainty set, then problem 1 can be solved in the polynomial time. If uncertainty sets are polyopes or ellipsoids, as is often the case in practice, we can convert problem 1 into simpler convex optimization problems such as Lquadratic problems or more reasonably solvable problems for large time."}, {"heading": "4. Adaptive Uncertainty Model", "text": "This section is dedicated to the adaptive case, i.e. the realization of the parameters - in particular the selection of different conditions - depends on the historical development of the MDP. Furthermore, the decision maker observes the parameter deviation retrospectively. In other words, at each decision stage, the decision maker chooses an action according to which the true parameters are realized and observed by the decision maker. The first case is that the decision maker is aware of the occurrence of parameter deviations, and his strategy may depend on such information. In practice, there are two cases in which the decision maker can certainly detect a deviation. In the first case, the parameter deviation is accompanied by external signals (e.g. lighting). Consider, for example, a problem in determining the route for a helicopter, the deviation occurs when a storm exists, and it is reasonable to assume that the representative can observe the storm if it really occurs. A similar example is the portfolio optimization, if the second one is not a market rotation, where the second is not a market rotation, or the second case is not a market rotation."}, {"heading": "4.1. Finite-horizon case", "text": "The number of decision stages (in contrast to states where D. Hence's parameters are limited, we are allowed to take different values for multiple visits.) The number of decision stages (in contrast to states where D. Hence's parameters are limited) is almost justified (in contrast to states where D. Hence's parameters are limited) and the number of decision stages (in contrast to states where D. Hence's parameters are limited) is almost justified."}, {"heading": "4.2. Discounted-Reward Infinite Horizon Case", "text": "Depending on whether the number of parameter deviations is also discounted, we formulate and examine the following two approaches: Setup A - Non-Discounted Deviates: max a1 - A min (p1, r1) - A min (p1, r1) - A min (p1, r1) - A min (p2, r2) - A min (p2, r2) - A min (p2, r2) - A min (pt, rt) - B - Discounted Deviates: max a1 - A min (p1, r1) - A min (p1, r1) - A min (p2, r2) - A min (p2, r2) - A min (p1) - V (p1) - V qtions (p1) - V max a2 - V (p2, r2) - D (p.Setup B - best."}, {"heading": "5. Continuous Deviations", "text": "Note that the previous approach, which limits the number of parameters that deviate, is not applicable to this approach. Therefore, we extend the previous results to the case of continuous deviation, i.e., the nature is allowed for each s deviation. If the realized parameter (p, r) is free of deviation, it is assumed that the nominal parameters are a budget of deviation b (p, r, p0, r0, U) free of deviation and that we are star-shaped in relation to (p0s, r 0 s). If the realized parameter (p, r) is free of deviation, then it is assumed that the nature is a budget of deviation b (p, p0, r0, U) such thatb (p, r0, r0, r0, U) free of deviation."}, {"heading": "6. Simulations", "text": "We present simulation results of the proposed method under a setup similar to our motivational example, i.e. the parameters deviate randomly from their nominal values with low probability. We use a Scenario2A set U is star shaped w.r.t. u0, if for each U-U the line segment between u and u0 also belongs to U. 3Note that both the case of the finite horizon and the two cases of the infinite horizon investigated can be formulated in this way. Based on the stochastic single product control problem described in Puterman (1994), the states represent the number of positions in the inventory, with the maximum capacity of the MAXSTOCK positions. Every day, an order for new positions is made at the cost of STOREPRICE. A price is paid for holding the positions in the inventory (both old and new), which is a function of the number of positions HOLDINGCOST."}, {"heading": "7. Conclusion", "text": "We proposed a new robust MDP framework, called \"Lightning Does Not Strike Twice,\" to model the case where uncertain parameters between different states are linked by a common pool of \"budgets\" of deviation, resulting in a comprehensible formulation that provides a flexible trade-off between risk and value that can be adjusted by matching the \"budget.\" 4Note that for d0 = 0 we get the nominal policy and for d0 = T the conservative, uncoupled uncertainty policy. 5The parameters chosen for the simulation are T = 100, MAXSTOCK = 20, STOREPRICE = 5, KUNDOMERPRICE = 50, NUMBERS = 10, HOLDINGCOST (n) = 2n2, PENALTY (n) = 7n2, for the time being the inventory is empty. An obstacle preventing robust MPs from being widely applied is that the conventional formulation must be a conservative one - the result of the DPs must be."}, {"heading": "Acknowledgments", "text": "S. Mannor was funded by the Israel Science Foundation (Contract 890015) and the Seventh Framework Programme of the European Union (FP7 / 20072013) under Funding Agreement No. 249254. H. Xu receives insufficient support from the National University of Singapore under the Startup Funding R-265-000-384-133."}], "references": [{"title": "Solving uncertain Markov decision problems", "author": ["A. Bagnell", "A. Ng", "J. Schneider"], "venue": "Technical Report CMU-RI-TR-01-25,", "citeRegEx": "Bagnell et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bagnell et al\\.", "year": 2001}, {"title": "Robust Optimization", "author": ["A. Ben-Tal", "L. El Ghaoui", "A. Nemirovski"], "venue": null, "citeRegEx": "Ben.Tal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 2009}, {"title": "Percentile optimization for Markov decision processes with parameter uncertainty", "author": ["E. Delage", "S. Mannor"], "venue": "Operations Research,", "citeRegEx": "Delage and Mannor,? \\Q2010\\E", "shortCiteRegEx": "Delage and Mannor", "year": 2010}, {"title": "Learning under ambiguity", "author": ["Epstein", "Larry G", "M. Schneider"], "venue": "Review of Economic Studies,", "citeRegEx": "Epstein et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Epstein et al\\.", "year": 2007}, {"title": "Competitive Markov Decision Process", "author": ["J. Filar", "K. Vrieze"], "venue": null, "citeRegEx": "Filar and Vrieze,? \\Q1996\\E", "shortCiteRegEx": "Filar and Vrieze", "year": 1996}, {"title": "Boundedparameter Markov decision processes", "author": ["R. Givan", "S. Leach", "T. Dean"], "venue": "Artificial Intelligence,", "citeRegEx": "Givan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Givan et al\\.", "year": 2000}, {"title": "Robust dynamic programming", "author": ["G. Iyengar"], "venue": "Mathematics of Operations Reseasrch,", "citeRegEx": "Iyengar,? \\Q2005\\E", "shortCiteRegEx": "Iyengar", "year": 2005}, {"title": "Bias and variance approximation in value function estimates", "author": ["S. Mannor", "D. Simeste", "P. Sun", "J. Tsitsiklis"], "venue": "Management Science,", "citeRegEx": "Mannor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2007}, {"title": "Robust control of Markov decision processes with uncertain transition matrices", "author": ["A. Nilim", "L. El Ghaoui"], "venue": "Operations Research,", "citeRegEx": "Nilim and Ghaoui,? \\Q2005\\E", "shortCiteRegEx": "Nilim and Ghaoui", "year": 2005}, {"title": "Markov Decision Processes", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q1994\\E", "shortCiteRegEx": "Puterman and L.", "year": 1994}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "In ICML, pp", "citeRegEx": "Strens,? \\Q2000\\E", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Distributionally robust Markov decision processes", "author": ["H. Xu", "S. Mannor"], "venue": "In NIPS, pp. 2505\u20132513,", "citeRegEx": "Xu and Mannor,? \\Q2010\\E", "shortCiteRegEx": "Xu and Mannor", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "Such deviation, termed \u201cparameter uncertainty,\u201d can cause the performance of the \u201coptimal\u201d policies to degrade significantly, as demonstrated in Mannor et al. (2007).", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": "Inspired by the so-called \u201crobust optimization\u201d framework (Ben-Tal & Nemirovski, 1998; Bertsimas & Sim, 2004; Ben-Tal et al., 2009), the common approach regards the MDP\u2019s uncertain parameters r and p as fixed but unknown elements of a known set U , often termed as the uncertainty set, and ranks solutions based on their performance under (respective) worst parameter realization.", "startOffset": 58, "endOffset": 131}, {"referenceID": 10, "context": "This is in sharp contrast to some other variants of MDP methods also aiming to mitigate conservativeness, including Bayesian reinforcement learning (Strens, 2000; Poupart, 2010), chance constrained MDP (Delage & Mannor, 2010), and distributionally robust MDP (Xu & Mannor, 2010), all assuming a-priori information on the distribution of the system parameters.", "startOffset": 148, "endOffset": 177}], "year": 2012, "abstractText": "We consider Markov decision processes under parameter uncertainty. Previous studies all restrict to the case that uncertainties among different states are uncoupled, which leads to conservative solutions. In contrast, we introduce an intuitive concept, termed \u201cLightning Does not Strike Twice,\u201d to model coupled uncertain parameters. Specifically, we require that the system can deviate from its nominal parameters only a bounded number of times. We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies.", "creator": "LaTeX with hyperref package"}}}