{"id": "1206.4648", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Two-Manifold Problems with Applications to Nonlinear System Identification", "abstract": "Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.", "histories": [["v1", "Mon, 18 Jun 2012 15:23:02 GMT  (7860kb)", "http://arxiv.org/abs/1206.4648v1", "ICML2012. arXiv admin note: text overlap witharXiv:1112.6399"]], "COMMENTS": "ICML2012. arXiv admin note: text overlap witharXiv:1112.6399", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["byron boots", "geoff gordon"], "accepted": true, "id": "1206.4648"}, "pdf": {"name": "1206.4648.pdf", "metadata": {"source": "META", "title": "Two-Manifold Problems with Applications to Nonlinear System Identification", "authors": ["Byron Boots", "Geoffrey J. Gordon"], "emails": ["beb@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Kernel PCA", "text": "Kernel PCA (Scho \u00bc lkopf et al., 1998) generalizes PCA to high or infinite dimensional input data, which are implicitly represented by a reproducing Hilbert kernel space (RKHS). If the kernel K (x, x) is sufficiently expressive, kernel PCA can find a structure that misses the regular PCA. Conceptually, our goal is to restore the \"characteristic function\" \u03c6 (x) = K (x, \u00b7) and define an infinitely high \"matrix\" with columns (xi), to restore the eigenvalues and eigenvectors of the centered covariance operator \u0445XX = 1 n\u0441H T. Here is H the centric matrix H = In \u2212 1centered matrix. For efficient calculation, we work with the Gram matrix G = 1nvectoring covariance operator H."}, {"heading": "2.2. Manifold Learning", "text": "The most important finding behind these methods is that large distances in the input space are often meaningless due to the large-scale curvature of the manifold; therefore, ignoring these distances can lead to a significant improvement in dimensionality reduction by \"unfolding\" the manifold. Interestingly, these algorithms can be considered special cases of kernel PCA in which the gram matrix G is constructed over the finite domain of the training data. (Ham et al., 2003) For example, in Laplacian Eigenmaps (LE) we can first calculate an adjectivity matrix W by the nearest neighbors: wi, j is peculiar if point i is one of the closest points."}, {"heading": "3. Bias and Instrumental Variables", "text": "Kernel eigenmap methods are very good at reducing dimensionality when the original data points sense a high-dimensional multiplicity relatively tightly, and when the noise in each sample is low compared to the local curvature of the multiplicity. In practice, however, observations are often loud and varied learning algorithms applied to these data sets usually result in distorted embeddings. See, for example, Figures 1-2, the \"noisy Swiss rolls.\" Our goal is therefore to design a more noise-resistant algorithm for the two-fold problem. We will begin by investigating PCA, a linear special case of diverse learning, and investigate why distorted embeddings occur in the presence of noise. Next, we will show how to overcome this problem in the linear case map, and then use the same ideas to generalize kernel PCA, a nonlinear algorithm, to generalize these eigen4 ideas in general terms."}, {"heading": "3.1. Bias in Finite-Dimensional Linear Models", "text": "Suppose xi is a noisy view of some underlying latent variable zi: xi = Mzi + i for a linear transformation M and i.i.d. zero-mean noise i.1 Without loss of generality, we assume xi and zi are centered and that Cov [zi] and M both have full column rank. In this case, PCA will not be able to restore Z on X in general: the expectation of GP = XX = 1 nXXT is M Cov [zi] M T + Cov [i], while we need M Cov [zi] M T to restore a transformation of M or Z. The unwanted term Cov [i] will generally affect all eigenvalues and eigenvectors of GP [XX], causing us to restore a distorted response even within the boundaries of infinite data."}, {"heading": "3.1.1. Instrumental Variables", "text": "We can fix this problem for linear embedding: instead of simple PCAs, we can use what could be called bedridden Y.1 Note that each i is a vector, and we do not assume that its coordinates are independent of each other (i.e., we do not assume that Cov [i] is spherical).2 sub-space PCA. This method finds a statistically consistent solution by using an instrumental variable (Pearl, 2000; Jolliffe, 2002), an observation that is correlated with the true latent variables but not with the noise in xi. Importantly, the selection of an instrumental variable is not merely a statistical tool, but rather an appraisal of the nature of the latent variable and the noise in the observations. We define noise as that part of the variability that is not correlated with the instrumental variable, and the signal that is that part that which is correlates."}, {"heading": "3.2. Bias in Nonlinear Models", "text": "We assume that noise observations xi = f (zi) + i, where zi is the desired low-dimensional latent variable, i is an i.i.d. noise term, and f is a smooth function with smooth inversion (so that f (zi) lies on a multiplicity). Our goal is to restore f and zi to identifiability. Kernel PCA (Sec. 2.1) is a common approach to this problem. In the (restrictive) realizable case, the kernel PCA gets the correct answer: that zi has the dimension PCA, that i has the variance zero, and that we have at least k independent samples. And suppose \u03c6 (f (z)) is a linear function of z. Then the gram matrix or the covariance \"matrix\" is a variance of PCA, and we can reconstruct a variance of the size k."}, {"heading": "3.2.2. Two-subspace PCA in RKHSs", "text": "We now assume that the kernel SVD = n (XH) are the square roots of the SVD. (D) We assume that our input factors are \"Matrices\" (D) and \"En\" (D). (D) We assume that we have \"Matrices\" (D) and \"En\" (D). (D) We assume that \"En\" (D) and \"En\" (D), \"En\" (D), \"En\" (D), \"En\" (D) and \"E\" (H), \"E\" (H), \"En\" (H), \"E\" (H), \"E\" (H), \"E\" (H) and \"E\" (H), \"E\" (H), \"E\" (H) and \"E.\""}, {"heading": "4. Two-Manifold Problems", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5. Two-Manifold Detailed Example: Nonlinear System Identification", "text": "A fundamental problem in machine learning and robotics is the identification of dynamic systems. This task requires two interrelated subtasks: 1) learning a low-dimensional state space, which is often known to be based on diversity; and 2) learning system dynamics. We propose to solve this problem by combining two-dimensional methods (for task 1) with spectral learning algorithms for nonlinear dynamic systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon et al., 2010; Hsu et al., 2009; Boots et al., 2010). Here, we focus on a concrete example: We show how to combine HSE-HMMs (Song et al., 2010), a powerful non-parametric approach to system identification, with diverse learning. We show that the resulting format can only include HSE-manifold HMSE-manifold HMSE systems (for example, HMSE-manifold HMSE-manifold HMSE-manifold HMSE-space)."}, {"heading": "5.1. Hilbert Space Embeddings of HMMs", "text": "The key idea behind the spectral learning of dynamic systems is that a good latent state is one that can predict the future. HSE HMMs implement this idea by finding a low-dimensional embedding of the conditional probability distribution of sequences of future observations and identifying the embedding coordinates as a state. Song et al. (2010) suggest identifying this low-dimensional state space as a subspace of an infinite dimensional RKHS. Intuitively, we might think that we could find the best state space by performing PCA or kernel PCA of sequences of future observations. That is, we would construct n sequences of future observations x1,....., xn Rd1 from a dynamic system, then construct a gram matrix GX whose (i, j) element is Kx that is a future state."}, {"heading": "5.2. Manifold HSE-HMMs", "text": "Unlike ordinary HSE HMMs, we are interested in modeling a dynamic system whose state space is on a low-dimensional manifold, even if that manifold is curved to take up a higher-dimensional subspace (an example is given in Section 5.3 below). We want to use this additional knowledge to narrow the learning algorithm and build a more precise model for a given amount of training data by replacing the SVD kernel with a two-dimensional method. That is, we learn centered gram matrices CX and CY for future and past observations, using a variety of methods such as LE or LLE (see Section 2.2). Then we apply an SVD to CXCY to restore the latent state space."}, {"heading": "5.3. Slotcar: A Real-World Dynamical System", "text": "To evaluate two-varied HSE HMMs, we look at the problem of tracking and predicting the position of a slot car with attached inertial measuring unit (IMU) racing around a track. Figure 3 (A) shows setup.We collected 3000 consecutive observations of 3D acceleration and angular speed at 10 Hz, while the slot car controlled the track by a constant policy (at different speeds).The goal was to learn a dynamic model of loud IMU data, and after filtering to predict current and future 2-dimensional locations. We used the first 2000 data points as training data and kept the last 500 data points for testing the learned models. We trained four models and evaluated these models for predictive accuracy, and, where appropriate, the learned latent states. We trained a 20-dimensional HMM with the spectral algorithm of Song et al."}, {"heading": "6. Related Work", "text": "In preparing this manuscript, we learned from the simultaneous and independent work of Mahadevan et al. (2011), which defines a particular two-dimensional algorithm, the Maximum Covariance Unfolding (MCU). We believe that the current work will help explain why two-dimensional methods such as MCU work well. A similar problem to the two-dimensional problem is Multidimensional Alignment (Ham et al., 2005; Wang & Mahadevan, 2009), which establishes connections between two or more data sets by aligning their underlying multiplicities. Our goal is another: we start from paired data where multidimensional alignments do not work; and we focus on learning algorithms that simultaneously discover multidimensional structures and connections between multiplicities (such as a highest-level learning problem defined between two multiplicities)."}, {"heading": "7. Conclusion", "text": "In this paper, we investigate two-dimensional problems where two sets of corresponding data points, generated from a single latent multiplicity and corrupted by noise, are located on or near two different higher-dimensional multiplicities. We design algorithms by relating two-dimensional problems to cross-covariance operators in RKHSs, and show that these algorithms result in a significant improvement over standard multidimensional learning approaches in the presence of noise. This is an appealing result: multidimensional learning algorithms typically assume that observations are (nearly) noiseless, an assumption rarely fulfilled in practice. In addition, we demonstrate the usefulness of two-dimensional problems by extending a newer dynamic system identification algorithm to learn a system with a multidimensional state space. The resulting algorithm learns a model that exceeds the state of the art in terms of prediction."}, {"heading": "Acknowledgements", "text": "Byron Boots and Geoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052. Byron Boots was supported by NSF grant number EEEC-0540865."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Belkin", "Mikhail", "Niyogi", "Partha"], "venue": "Neural Computation,", "citeRegEx": "Belkin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2002}, {"title": "Predictive state temporal difference learning", "author": ["Boots", "Byron", "Gordon", "Geoff"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Boots et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2010}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"], "venue": "In Proceedings of Robotics: Science and Systems VI,", "citeRegEx": "Boots et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2010}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["Boots", "Byron", "Siddiqi", "Sajid", "Gordon", "Geoffrey"], "venue": "In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI2011),", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "Linear leastsquares algorithms for temporal difference learning", "author": ["Bradtke", "Steven J", "Barto", "Andrew G"], "venue": "In Machine Learning,", "citeRegEx": "Bradtke et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke et al\\.", "year": 1996}, {"title": "Neighborhood smoothing embedding for noisy manifold learning", "author": ["Chen", "Guisheng", "Yin", "Junsong", "Li", "Deyi"], "venue": "In GrC, pp", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Theory and methods: Special invited paper: Dimension reduction and visualization in discriminant analysis (with discussion)", "author": ["Cook", "R. Dennis", "Yin", "Xiangrong"], "venue": "Australian and New Zealand Journal of Statistics,", "citeRegEx": "Cook et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cook et al\\.", "year": 2001}, {"title": "Consistency of kernel canonical correlation analysis", "author": ["K. Fukumizu", "F. Bach", "A. Gretton"], "venue": "Technical Report 942,", "citeRegEx": "Fukumizu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2005}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces", "author": ["Fukumizu", "Kenji", "Bach", "Francis R", "Jordan", "Michael I", "Williams", "Chris"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["Ham", "Jihun", "Lee", "Daniel D", "Mika", "Sebastian", "Schlkopf", "Bernhard"], "venue": null, "citeRegEx": "Ham et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2003}, {"title": "Semisupervised alignment of manifolds", "author": ["Ham", "Jihun", "Lee", "Daniel", "Saul", "Lawrence"], "venue": "10th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Ham et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2005}, {"title": "The most predictable criterion", "author": ["Hotelling", "Harold"], "venue": "Journal of Educational Psychology,", "citeRegEx": "Hotelling and Harold.,? \\Q1935\\E", "shortCiteRegEx": "Hotelling and Harold.", "year": 1935}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Zhang", "Tong"], "venue": "In COLT,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Principal component analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "Jolliffe,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe", "year": 2002}, {"title": "Spectral dimensionality reduction via maximum entropy", "author": ["Lawrence", "Neil D"], "venue": "In Proc. AISTATS,", "citeRegEx": "Lawrence and D.,? \\Q2011\\E", "shortCiteRegEx": "Lawrence and D.", "year": 2011}, {"title": "Sliced inverse regression for dimension reduction", "author": ["Li", "Ker-Chau"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li and Ker.Chau.,? \\Q1991\\E", "shortCiteRegEx": "Li and Ker.Chau.", "year": 1991}, {"title": "Maximum covariance unfolding: Manifold learning for bimodal data", "author": ["Mahadevan", "Vijay", "Wong", "Chi Wah", "Pereira", "Jose Costa", "Liu", "Tom", "Vasconcelos", "Nuno", "Saul", "Lawrence"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mahadevan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2011}, {"title": "Regression on manifolds using kernel dimension reduction", "author": ["Nilsson", "Jens", "Sha", "Fei", "Jordan", "Michael I"], "venue": "In ICML, pp", "citeRegEx": "Nilsson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nilsson et al\\.", "year": 2007}, {"title": "Causality: models, reasoning, and inference", "author": ["Pearl", "Judea"], "venue": null, "citeRegEx": "Pearl and Judea.,? \\Q2000\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 2000}, {"title": "Multivariate Reduced-rank Regression: Theory and Applications", "author": ["Reinsel", "Gregory C", "Velu", "Rajabather Palani"], "venue": null, "citeRegEx": "Reinsel et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Reinsel et al\\.", "year": 1998}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "Sam T", "Saul", "Lawrence K"], "venue": null, "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alex J", "M\u00fcller", "KlausRobert"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Reduced-rank hidden Markov models", "author": ["Siddiqi", "Sajid", "Boots", "Byron", "Gordon", "Geoffrey J"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "Algorithmic Learning Theory, Lecture Notes on Computer Science. Springer,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings of hidden Markov models", "author": ["L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola"], "venue": "In Proc. 27th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Tenenbaum", "Joshua B", "Silva", "Vin De", "Langford", "John"], "venue": "Science, 290:2319\u20132323,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Subspace Identification for Linear Systems: Theory, Implementation, Applications", "author": ["P. Van Overschee", "B. De Moor"], "venue": null, "citeRegEx": "Overschee and Moor,? \\Q1996\\E", "shortCiteRegEx": "Overschee and Moor", "year": 1996}, {"title": "A general framework for manifold alignment", "author": ["Wang", "Chang", "Mahadevan", "Sridhar"], "venue": "In Proc. AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["Weinberger", "Kilian Q", "Sha", "Fei", "Saul", "Lawrence K"], "venue": "Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "Weinberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2004}, {"title": "Robust local tangent space alignment via iterative weighted PCA", "author": ["Zhan", "Yubin", "Yin", "Jianping"], "venue": null, "citeRegEx": "Zhan et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Zhan et al\\.", "year": 1985}], "referenceMentions": [{"referenceID": 25, "context": "These kernel eigenmap methods include Isomap (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis & Saul, 2000), Laplacian Eigenmaps (LE) (Belkin & Niyogi, 2002), Maximum Variance Unfolding (MVU) (Weinberger et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 28, "context": ", 2000), Locally Linear Embedding (LLE) (Roweis & Saul, 2000), Laplacian Eigenmaps (LE) (Belkin & Niyogi, 2002), Maximum Variance Unfolding (MVU) (Weinberger et al., 2004), and Maximum Entropy Unfolding (MEU) (Lawrence, 2011).", "startOffset": 146, "endOffset": 171}, {"referenceID": 5, "context": "Several authors have attacked the problem of learning manifolds in the presence of noise using methods like neighborhood smoothing (Chen et al., 2008) and robust principal components analysis (Zhan & Yin, 2009; 2011), with some success when noise is limited.", "startOffset": 131, "endOffset": 150}, {"referenceID": 13, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression (Pearl, 2000), principal component analysis (PCA) (Jolliffe, 2002), and temporal difference learning (Bradtke & Barto, 1996).", "startOffset": 206, "endOffset": 222}, {"referenceID": 12, "context": "Subspace identification approaches to learning nonlinear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinite-dimensional) covariance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010).", "startOffset": 209, "endOffset": 266}, {"referenceID": 1, "context": "Subspace identification approaches to learning nonlinear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinite-dimensional) covariance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010).", "startOffset": 209, "endOffset": 266}, {"referenceID": 24, "context": "Subspace identification approaches to learning nonlinear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinite-dimensional) covariance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010).", "startOffset": 209, "endOffset": 266}, {"referenceID": 21, "context": "Kernel PCA (Sch\u00f6lkopf et al., 1998) generalizes PCA to high- or infinite-dimensional input data, represented implicitly using a reproducing-kernel Hilbert space (RKHS).", "startOffset": 11, "endOffset": 35}, {"referenceID": 21, "context": "\u03a3\u0302XX ; the eigenvectors of \u03a3\u0302XX are \u03a6Hvi\u03bb \u22121/2 i , where (\u03bbi,vi) are the eigenpairs of C (Sch\u00f6lkopf et al., 1998).", "startOffset": 89, "endOffset": 113}, {"referenceID": 9, "context": "Interestingly, these algorithms can be viewed as special cases of kernel PCA where the Gram matrix G is constructed over the finite domain of the training data in a particular way (Ham et al., 2003).", "startOffset": 180, "endOffset": 198}, {"referenceID": 9, "context": ") To relate LE to kernel PCA, Ham et al. (2003) showed that one can build a Gram matrix from L, G = L\u2020; the LE embedding is given by the top k eigenvectors (and optionally eigenvalues) of G.", "startOffset": 30, "endOffset": 48}, {"referenceID": 13, "context": "This method finds a statistically consistent solution through the use of an instrumental variable (Pearl, 2000; Jolliffe, 2002), an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi.", "startOffset": 98, "endOffset": 127}, {"referenceID": 23, "context": "The cross-covariance operator reduces to an ordinary cross-covariance matrix in the finite-dimensional case; in the infinite-dimensional case, it can be viewed as a kernel mean map descriptor (Smola et al., 2007) for the joint distribution P[X,Y ].", "startOffset": 192, "endOffset": 212}, {"referenceID": 24, "context": "The kernel SVD algorithm previously appeared as an intermediate step in (Song et al., 2010; Fukumizu et al., 2005); here we give a more complete description.", "startOffset": 72, "endOffset": 114}, {"referenceID": 7, "context": "The kernel SVD algorithm previously appeared as an intermediate step in (Song et al., 2010; Fukumizu et al., 2005); here we give a more complete description.", "startOffset": 72, "endOffset": 114}, {"referenceID": 24, "context": "The remainder of the proof follows from the proof of Theorem 1 in (Song et al., 2010) (the convergence of the empirical estimator of the kernel covariance oper-", "startOffset": 66, "endOffset": 85}, {"referenceID": 24, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 22, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 3, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 12, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 1, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 24, "context": "Here, we focus on a specific example: we show how to combine HSE-HMMs (Song et al., 2010), a powerful nonparametric approach to system identification, with manifold learning.", "startOffset": 70, "endOffset": 89}, {"referenceID": 24, "context": "Song et al. (2010) suggest finding this lowdimensional state space as a subspace of an infinite dimensional RKHS.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "From this subspace, we can proceed to identify the parameters of the system as in Song et al. (2010).", "startOffset": 82, "endOffset": 101}, {"referenceID": 24, "context": "First, we trained a 20-dimensional embedded HMM with the spectral algorithm of Song et al. (2010), using sequences of 150 consecutive observations and Gaussian RBF kernels.", "startOffset": 79, "endOffset": 98}, {"referenceID": 16, "context": "While preparing this manuscript, we learned of the simultaneous and independent work of Mahadevan et al. (2011). That paper defines one particular twomanifold algorithm, maximum covariance unfolding (MCU).", "startOffset": 88, "endOffset": 112}, {"referenceID": 10, "context": "A similar problem to the two-manifold problem is manifold alignment (Ham et al., 2005; Wang & Mahadevan, 2009), which builds connections between two or more data sets by aligning their underlying manifolds.", "startOffset": 68, "endOffset": 110}, {"referenceID": 8, "context": "Interconnected dimensionality reduction has been explored before in sufficient dimension reduction (SDR) (Li, 1991; Cook & Yin, 2001; Fukumizu et al., 2004).", "startOffset": 105, "endOffset": 156}, {"referenceID": 17, "context": "A related method is manifold kernel dimension reduction (Nilsson et al., 2007), which finds an embedding of covariates xi using a kernel eigenmap method, and then attempts to find a linear transformation of some of the dimensions of the embedded points to predict response variables yi.", "startOffset": 56, "endOffset": 78}], "year": 2012, "abstractText": "Recently, there has been much interest in spectral approaches to learning manifolds\u2014 so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.", "creator": "LaTeX with hyperref package"}}}