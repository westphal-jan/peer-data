{"id": "1206.4649", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Learning Efficient Structured Sparse Models", "abstract": "We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications.", "histories": [["v1", "Mon, 18 Jun 2012 15:23:19 GMT  (609kb)", "http://arxiv.org/abs/1206.4649v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["alexander m bronstein", "pablo sprechmann", "guillermo sapiro"], "accepted": true, "id": "1206.4649"}, "pdf": {"name": "1206.4649.pdf", "metadata": {"source": "META", "title": "Learning Efficient Structured Sparse Models", "authors": ["Pablo Sprechmann", "Alex Bronstein"], "emails": ["sprec009@umn.edu", "bron@cs.technion.ac.il", "guille@umn.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Structured Sparse Models", "text": "The underlying assumption of the sparse models is that the input vectors can be reconstructed precisely as a linear combination of some (usually learned) base vectors (factors or dictionary atoms) with a small number of non-zero coefficients. Structured sparse models also assume that the pattern of non-zero coefficients has a specific structure known as prioritization. (D) The group structure, G) as a collection of groups of atoms, G), is a dictionary of p m-dimensional atoms. We define groups of atoms by their indexes, G). Then we define a group structure, G, as a collection of atoms, G = G1,. (G), which is a group of atoms referring to the group of atoms."}, {"heading": "3. Optimization Algorithms", "text": "State-of-the-art approaches (1) are based on the family of proximal splitting methods (see (Bach et al., 2011) and references to them). Next, we briefly present proximal methods and an algorithm for solving hierarchical sparse coding problems (Tseng, 2001), which are used to construct trainable sparse coders."}, {"heading": "3.1. Forward-Backward Splitting", "text": "The forward-backward splitting method is designed for solving unlimited optimization problems where the cost function can be divided as min z-Rm f1 (z) + f2 (z), (3) where f1 is convex and differentiable with a 1 \u03b1 Lipschitz continuous gradient and f2 is convex extended and real and may not be smooth. (4) The forward-backward splitting method with fixed constant step clearly defines a series of iterates, zk + 1 = prox\u03b1f2 (z-1\u03b1 f1 (zk)), (proxf2 (z) = argmin u-Rm | | v | v | 22 + f2 (u) the convergence rates of these groups as converted."}, {"heading": "3.2. Proximal Operators", "text": "To simplify notation, we will henceforth formulate all derivatives for the case of hierarchical sparse encoding on two levels (HiLasso et al., 2010; Sprechmann et al., 2011), which covers the nature of hierarchical sparse models and generalization on several levels (Jenatton et al., 2011) or on a collaborative scheme (Sprechmann et al., 2011). The HiLasso model was introduced to promote thrift at the group and coefficient levels at the same time. In the face of a partition P = {G1,.., G | P |}, the group structure G | olze can be expressed as the union of two partitions: P and the amount of singlets. Thus, the regulator can promote sparseness both at the group and coefficient levels."}, {"heading": "3.3. Block-Coordinate Forward-Backward Algorithm", "text": "In algorithm 1, each iteration requires the update of all coefficient groups in partition P according to (7). You can choose a block coordinate strategy in which only one block is updated at a time (Tseng, 2001). In this paper, we refer to this algorithm as the Block Coordinate Forward Backward Algorithm (BCoFB) (see Bach et al., 2011) for a check of similar algorithms. Here, too, 1 / \u03b1 is the Lipshitz constant of the matching term and g is the index of the group in P, zk + 1 = zk, zk + 1g = pg \u2212 1\u03b1D T g (Dgp k \u2212 xg), (8) where 1 / \u03b1 is the Lipshitz constant of the matching term, zk + 1 = zk, zk + 1g = pg \u2212 1\u03b1D T (Dgp k k g \u2212 xg), and 1 / \u03b1 is the index of the group in P."}, {"heading": "4. Fast Structured Sparse Encoders", "text": "In fact, they will be able to reactivate the erroneous orders in order to reactivate them, and they will be able to reactivate themselves in order to reactivate them."}, {"heading": "4.1. Hierarchical Sparse Encoders", "text": "We are now expanding the idea of Gregor & LeCun to include hierarchical (structured) sparse code regressors. We are looking at a feed-forward architecture based on the BCoFB, in which each layer implements a single iteration of the BCoFB proximal method (algorithm 2), the encoder architecture shown in Figure 1. Each layer is essentially composed of the non-linear proximal operator \u03c0s, t followed by a group selector and a linear operation Sg corresponding to this group. Network parameters are initialized as in Algorithm 2. In the particular case of \u03b1 = 1 and s = 0, the CoD architecture is obtained."}, {"heading": "4.2. Alternative Training Objective Functions", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.3. Online Learning", "text": "Consequently, the network (and possibly the dictionary) can be trained on exactly the same data vectors that are fed to it for sparse encoding, allowing the proposed framework to be used in online learning applications. A complete online scenario for sparse modeling consists of (a) initializing the dictionary (e.g. by randomly sampling the originally observed training data vectors); (b) defining the dictionary in the training target and adapting the network parameters to the newly incoming data using an online learning algorithm (we use an online version of stochastic progression in small batches as described in Section 5); and (c) defining the sparse codes dictionary in the training target and adapting the dictionary using an online learning algorithm (e.g. (Mairal et al., 2009)."}, {"heading": "4.4. Supervised and Discriminative Learning", "text": "In the group or in the hierarchical lasso case, one can know the desired active groups for each data vector. In other cases, it is possible to integrate this information into the training target by using it individually for each training vector to promote the activation of a knowingly active group. In other cases, data vectors can be created in pairs with knowingly similar or dissimilar vectors, and the natural distance between the two groups can be reduced."}, {"heading": "5. Experimental Results", "text": "All NNs were implemented in Matlab with built-in GPU acceleration and run on state-of-the-art Intel Xeon E5620 CPU and NVIDIA Tesla C2070 GPU. Even with this non-optimized code, the propagation of 105 100-dimensional vectors through a 10-layer structured network using the proposed BCoFB architecture takes only 3.6 seconds, equivalent to 3.6\u00b5sec per vector per layer, several orders of magnitude faster than the exceptionally optimized SPAMS HiLasso code (Jenatton et al., 2011) that runs on the CPU. Such advantages of parallelization are possible due to the fixed datapath and complexity of the NN encoder compared to the iterative solution. In all experiments, the training was performed by gradient descent, backed by the Armijo rule."}, {"heading": "5.1. Classification", "text": "In this experiment, we evaluated the performance of the unstructured NN encoders in the MNIST digit classification task; the MNIST images were re-sampled to 17 x 17 (289-dimensional) fields; one set of ten dictionaries was formed for each class; the classification was performed by coding a test vector in each of the dictionaries and matching the label corresponding to the smallest value of the complete lasso lens; the following sparse encoders were compared: exact sparse codes (Exact Lasso), unstructured NN G-L and unstructured NN lasso (a CoD network trained with the lasso lens); ten networks were formed, one per class; all contained T = 5 CoD layers; the value = 0.1 was used in the lasso lens; dictionaries containing 100 (incomplete) and 289 (atomic) were used."}, {"heading": "5.2. Online Learning", "text": "In this experiment, we evaluated the online learning capabilities of the unstructured NN sparse encoders using 30 x 104 randomly localized 8 x 8 patches from three images of the Brodatz texture dataset (Randen & Husoy, 1999). Patches were arranged in three consecutive blocks of 104 patches from each image. The dictionary size was set to 64 atoms. \u03bb = 1 was used in the lasso lens. Online learning was performed in overlapping windows of 1 x 000 vectors with a step of 100 vectors. We compared standard online dictionary learning (Exact Lasso online) with unstructured NN lasso with dictionary adaptation in a given window (NN lasso online) initialized by the network parameters from the previous windows. In the latter case, the dictionary was initialized by a random subset of 64 NN from the first set of data vectors."}, {"heading": "5.3. Structured Coding", "text": "We first evaluate the performance of the structured sparse encoder in a speaker identification task, which was reproduced by (Sprechmann et al., 2011). In this application, the authors use HiLasso to automatically detect the current speaker in a given mixed signal. We repeat these experiments using the proposed efficient structured sparse encoder instead.The dataset consists of recordings of five different radio speakers, two women and three men. 25% of the samples were used for training, and the rest for testing. Within the test data, two sets of waveforms were created: one that contains isolated speakers, and another that contains all possible combinations of mixtures of twin speakers. The signals are divided into a series of overlapping local timeframes of 512 samples with 75% overlap, so that the characteristics of the signal remain stable in each frame. An 80-dimensional feature vector is obtained for each audio frame as its short-term power spectrum envelope."}, {"heading": "6. Conclusion", "text": "By combining ideas from convex optimization with multi-layered neural networks, we have developed a comprehensive framework for modern, economical modeling for real-time and large-scale applications, encompassing different objective functions, from reconstruction to classification, enabling different sparse coding structures from hierarchical to group-like, and addressing online learning scenarios. Simple implementation already achieves several orders of magnitude of acceleration compared to the state of the art at minimal cost of performance, opening the door to practical algorithms following the proven success of sparse modeling in various applications. An extension of the proposed approach to other structured, sparse modeling problems such as robust PCA and non-negative matrix factoring is available at http: / / www.eng.tau.ac.il / ~ bron / publications _ conferen.html and will be published due to space constraints elsewhere."}, {"heading": "Acknowledgement", "text": "This research was partially supported by ONR, NGA, ARO, NSF, NSSEFF and BSF."}], "references": [{"title": "Convex optimization with sparsity-inducing norms. In Optimization for Machine Learning", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": null, "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Img. Sci.,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Atomic decomposition by basis pursuit", "author": ["S. Chen", "D. Donoho", "M. Saunders"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Trans. on Inf. Theory,", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Average case analysis of multichannel sparse recovery using convex relaxation", "author": ["Y.C. Eldar", "H. Rauhut"], "venue": "IEEE Trans. on Inf. Theory,", "citeRegEx": "Eldar and Rauhut,? \\Q2010\\E", "shortCiteRegEx": "Eldar and Rauhut", "year": 2010}, {"title": "A note on the group lasso and a sparse group", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "lasso. Preprint,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "In In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "In ICML, pp", "citeRegEx": "Gregor and LeCun,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun", "year": 2010}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M.A. Ranzato", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": null, "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["K. Kavukcuoglu", "M.A. Ranzato", "Y. LeCun"], "venue": null, "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Coordinate descent optimization for `1 minimization with application to compressed sensing; a greedy algorithm", "author": ["Y. Li", "S. Osher"], "venue": "Inverse Problems and Imaging,", "citeRegEx": "Li and Osher,? \\Q2009\\E", "shortCiteRegEx": "Li and Osher", "year": 2009}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "In ICML,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "In CORE. Catholic University of Louvain, Louvain-la-Neuve,", "citeRegEx": "Nesterov,? \\Q2007\\E", "shortCiteRegEx": "Nesterov", "year": 2007}, {"title": "Filtering for texture classification: a comparative study", "author": ["T. Randen", "J.H. Husoy"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Randen and Husoy,? \\Q1999\\E", "shortCiteRegEx": "Randen and Husoy", "year": 1999}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Boureau", "Y-L", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "C-hilasso: A collaborative hierarchical sparse modeling framework", "author": ["P. Sprechmann", "I. Ram\u0131\u0301rez", "G. Sapiro", "Y.C. Eldar"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "Sprechmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2011}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Royal Stat. Society: Series B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl.,", "citeRegEx": "Tseng,? \\Q2001\\E", "shortCiteRegEx": "Tseng", "year": 2001}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "JMLR, 10:207\u2013244,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Learning sparse representations of high dimensional data on large scale dictionaries", "author": ["Z.J. Xiang", "H. Xu", "P.J. Ramadge"], "venue": "In NIPS,", "citeRegEx": "Xiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2011}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Royal Stat. Society, Series B,", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 13, "context": "Consequently, a significant amount of effort has been devoted to developing efficient optimization schemes (Beck & Teboulle, 2009; Li & Osher, 2009; Nesterov, 2007; Xiang et al., 2011).", "startOffset": 107, "endOffset": 184}, {"referenceID": 20, "context": "Consequently, a significant amount of effort has been devoted to developing efficient optimization schemes (Beck & Teboulle, 2009; Li & Osher, 2009; Nesterov, 2007; Xiang et al., 2011).", "startOffset": 107, "endOffset": 184}, {"referenceID": 8, "context": "Recent works have proposed to trade off precision in the sparse representation for computational speedup (Jarrett et al., 2009; Kavukcuoglu et al., 2010), by learning non-linear regressors capable of producing good approximations of sparse codes in a fixed amount of time.", "startOffset": 105, "endOffset": 153}, {"referenceID": 10, "context": "Recent works have proposed to trade off precision in the sparse representation for computational speedup (Jarrett et al., 2009; Kavukcuoglu et al., 2010), by learning non-linear regressors capable of producing good approximations of sparse codes in a fixed amount of time.", "startOffset": 105, "endOffset": 153}, {"referenceID": 6, "context": "dictionary is learned, the framework is related to recent efforts in producing NN based sparse representations, see (Goodfellow et al., 2009; Ranzato et al., 2007) and references therein.", "startOffset": 116, "endOffset": 163}, {"referenceID": 15, "context": "dictionary is learned, the framework is related to recent efforts in producing NN based sparse representations, see (Goodfellow et al., 2009; Ranzato et al., 2007) and references therein.", "startOffset": 116, "endOffset": 163}, {"referenceID": 6, "context": "It can be interpreted as an online trainable sparse auto-encoder (Goodfellow et al., 2009) with a sophisticated encoder and simple linear decoder.", "startOffset": 65, "endOffset": 90}, {"referenceID": 17, "context": "Several important structured sparsity settings can be cast as particular cases of (1): sparse coding, as mentioned above, which is often referred to as Lasso (Tibshirani, 1996) or basis pursuit (Chen et al.", "startOffset": 158, "endOffset": 176}, {"referenceID": 2, "context": "Several important structured sparsity settings can be cast as particular cases of (1): sparse coding, as mentioned above, which is often referred to as Lasso (Tibshirani, 1996) or basis pursuit (Chen et al., 1999; Donoho, 2006); group sparse coding, a generalization of the standard sparse coding to the cases in which the dictionary is sub-divided into groups that are known to be active or inactive simultaneously (Yuan & Lin, 2006), in this case G is a partition of {1, .", "startOffset": 194, "endOffset": 227}, {"referenceID": 3, "context": "Several important structured sparsity settings can be cast as particular cases of (1): sparse coding, as mentioned above, which is often referred to as Lasso (Tibshirani, 1996) or basis pursuit (Chen et al., 1999; Donoho, 2006); group sparse coding, a generalization of the standard sparse coding to the cases in which the dictionary is sub-divided into groups that are known to be active or inactive simultaneously (Yuan & Lin, 2006), in this case G is a partition of {1, .", "startOffset": 194, "endOffset": 227}, {"referenceID": 22, "context": ", p}; hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients (Zhao et al., 2009; Jenatton et al., 2011; Sprechmann et al., 2011).", "startOffset": 97, "endOffset": 164}, {"referenceID": 9, "context": ", p}; hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients (Zhao et al., 2009; Jenatton et al., 2011; Sprechmann et al., 2011).", "startOffset": 97, "endOffset": 164}, {"referenceID": 16, "context": ", p}; hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients (Zhao et al., 2009; Jenatton et al., 2011; Sprechmann et al., 2011).", "startOffset": 97, "endOffset": 164}, {"referenceID": 16, "context": "The groups in G form a hierarchy with respect to the inclusion relation (a tree structure), that is, if two groups overlap, then one is completely included in the other one; and collaborative sparse coding generalizing the concept of structured sparse coding to collections of input vectors by promoting given patterns of non-zero elements in the coefficient matrix (Eldar & Rauhut, 2010; Sprechmann et al., 2011).", "startOffset": 366, "endOffset": 413}, {"referenceID": 0, "context": "State-of-the-art approaches for solving (1) rely on the family of proximal splitting methods (see (Bach et al., 2011) and references therein).", "startOffset": 98, "endOffset": 117}, {"referenceID": 18, "context": "Next, we briefly introduce proximal methods and an algorithm for solving hierarchical sparse coding problems (Tseng, 2001) that will be used to construct trainable sparse encoders.", "startOffset": 109, "endOffset": 122}, {"referenceID": 13, "context": "Accelerated versions of proximal methods have been largely studied in the literature to improve their convergence rate (Beck & Teboulle, 2009; Nesterov, 2007).", "startOffset": 119, "endOffset": 158}, {"referenceID": 5, "context": "To simplify the notation, we will henceforth formulate all the derivations for the case of two-level hierarchical sparse coding, referred as HiLasso (Friedman et al., 2010; Sprechmann et al., 2011).", "startOffset": 149, "endOffset": 197}, {"referenceID": 16, "context": "To simplify the notation, we will henceforth formulate all the derivations for the case of two-level hierarchical sparse coding, referred as HiLasso (Friedman et al., 2010; Sprechmann et al., 2011).", "startOffset": 149, "endOffset": 197}, {"referenceID": 9, "context": "This captures the essence of hierarchical sparse models and the generalization to more layers (Jenatton et al., 2011) or to a collaborative scheme (Sprechmann et al.", "startOffset": 94, "endOffset": 117}, {"referenceID": 16, "context": ", 2011) or to a collaborative scheme (Sprechmann et al., 2011) is straightforward.", "startOffset": 37, "endOffset": 62}, {"referenceID": 9, "context": "The proximal operator of (5) can be expressed as (Jenatton et al., 2011; Sprechmann et al., 2011),", "startOffset": 49, "endOffset": 97}, {"referenceID": 16, "context": "The proximal operator of (5) can be expressed as (Jenatton et al., 2011; Sprechmann et al., 2011),", "startOffset": 49, "endOffset": 97}, {"referenceID": 18, "context": "One can choose a block coordinate strategy where only one block is updated at a time (Tseng, 2001).", "startOffset": 85, "endOffset": 98}, {"referenceID": 0, "context": "In this paper we will refer to this algorithm as Block-Coordinate Forward-Backward algorithm (BCoFB) (see (Bach et al., 2011) for a review on similar algorithms).", "startOffset": 106, "endOffset": 125}, {"referenceID": 8, "context": "In order to make sparse coding feasible in real time settings, it has been recently proposed to learn non-linear regressors capable of producing good approximations of sparse codes in a fixed amount of time (Jarrett et al., 2009; Kavukcuoglu et al., 2010).", "startOffset": 207, "endOffset": 255}, {"referenceID": 10, "context": "In order to make sparse coding feasible in real time settings, it has been recently proposed to learn non-linear regressors capable of producing good approximations of sparse codes in a fixed amount of time (Jarrett et al., 2009; Kavukcuoglu et al., 2010).", "startOffset": 207, "endOffset": 255}, {"referenceID": 12, "context": ", (Mairal et al., 2009)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "This is several orders of magnitude faster than the exceptionally optimized multithreaded SPAMS HiLasso code (Jenatton et al., 2011) executed on the CPU.", "startOffset": 109, "endOffset": 132}, {"referenceID": 16, "context": "We first evaluate the performance of the structured sparse encoders in a speaker identification task reproduced from (Sprechmann et al., 2011).", "startOffset": 117, "endOffset": 142}, {"referenceID": 16, "context": "An 80-dimensional feature vector is obtained for each audio frame as its short-time power spectrum envelope (refer to (Sprechmann et al., 2011) for details).", "startOffset": 118, "endOffset": 143}], "year": 2012, "abstractText": "We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we propose an efficient feed forward architecture derived from the iteration of the block-coordinate algorithm. This architecture approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, the proposed learnable sparse encoders are not only restricted to be approximants of the exact sparse code for a pre-given dictionary, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art exact optimization algorithms at minimal performance degradation, making the proposed framework suitable for real time and largescale applications.", "creator": "LaTeX with hyperref package"}}}