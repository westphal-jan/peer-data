{"id": "1206.4650", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Analysis of Kernel Mean Matching under Covariate Shift", "abstract": "In real supervised learning scenarios, it is not uncommon that the training and test sample follow different probability distributions, thus rendering the necessity to correct the sampling bias. Focusing on a particular covariate shift problem, we derive high probability confidence bounds for the kernel mean matching (KMM) estimator, whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel. By comparing KMM with the natural plug-in estimator, we establish the superiority of the former hence provide concrete evidence/understanding to the effectiveness of KMM under covariate shift.", "histories": [["v1", "Mon, 18 Jun 2012 15:23:37 GMT  (352kb)", "http://arxiv.org/abs/1206.4650v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yaoliang yu", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1206.4650"}, "pdf": {"name": "1206.4650.pdf", "metadata": {"source": "META", "title": "Analysis of Kernel Mean Matching under Covariate Shift", "authors": ["Yao-Liang Yu", "Csaba Szepesv\u00e1ri"], "emails": ["yaoliang@cs.ualberta.ca", "szepesva@cs.ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "In the traditional method of learning and testing, it is usually assumed that one draws from the same probability distribution (2004; David Suceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK 2012) (2004); in practice, however, this assumption can easily be violated for a variety of reasons, for example, due to sampling bias or non-stationary environment. Therefore, it is highly desirable to develop algorithms that remain effective under such distribution shifts. Under reasonable assumptions, this problem is hopeless if training and test distribution have nothing in common. On the other hand, if the two distributions are indeed related in a non-trivial way, then it is a fairly remarkable fact that effective adaptation is possible. Under reasonable assumptions, this problem has been attacked by researchers from statistics (Heckman, 1979; Shimodaira, 2000) and more recently by many researchers from machine learning, see for example ZadroznyAppearing in the 29th International Conference on Machine Learning, Scotland, 2012."}, {"heading": "2. Preliminaries", "text": "In this section, we formally present the problem of covariate shift that we are considering, followed by some relevant discussions."}, {"heading": "2.1. Problem Setup", "text": "The problem we are looking at in this paper is the estimation of the expected value EY te from the training sample {Xtri, Y tri tri}. We do not necessarily assume that the training and testing sample is drawn from the same probability measurement. The problem we are looking at in this paper is the estimation of the expected value EY te from the training sample {Xtri, Y tri}. We are i = 1 and the test sample {Xtei} nte i = 1. In particular, we would like to determine how quickly, say, the 1 \u2212 k."}, {"heading": "2.2. A Naive Estimator?", "text": "An immediate solution to estimating \u03b2 (x) is to estimate the two limits from the training sample (Xtri) or the test sample (Xtei). For example, if we know a third (Borel) measure Q (dx) (usually the Lebesgue measure for Rd), so that both dPtedQ (x) and dPtr dQ (x) exist, we can use standard density estimators to estimate it, and then set \u03b2 (x) = dPtedQ (x) / dPtr dQ (x). However, this naive approach is notoriously inferior, as density estimates in high dimensions are difficult, and in addition, a small estimation error in dPtrdQ (x) \u03b2 (x) could change significantly. Ours is little theoretical analysis of this seemingly naive approach."}, {"heading": "2.3. A Better Estimator?", "text": "It seems more attractive to estimate the RND \u03b2 (x) directly. In fact, a great deal of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009). Among the many references, we highlight the Kernel Mean Matching Algorithm (KMM), which was first proposed by Huang et al. (2007) and also forms the basis of this paper. KMM attempts to map the mean elements in a trait space induced by a kernel (\u00b7, \u00b7) on the X \u00d7 X domain: min \u03b2 i (\u03b2) {L (\u03b2)) and is also the basis of this trust. KMM attempts to map the mean elements in a trait space."}, {"heading": "2.4. Plug-in Estimator", "text": "Another natural approach is to estimate the regression function from the training sample and then insert it into the test set. We postpone the discussion and comparison regarding this estimator to Section 4.3."}, {"heading": "3. Motivation", "text": "Let us suppose that we have an overall set of classifiers, say, {fj} Nj = 1, all trained at the training test {(Xtri, Y tri) ntr i = 1. A useful task, therefore, is to compare the classifiers based on their generalization errors. Normally, this is done by putting the classifiers on a test sample {(Xtri, Y tei)} nt i = 1. It is not uncommon for the test sample to be drawn from a probability parameter other than the training sample, i.e. covariate shift has occurred. As it may be too expensive to retrain the classifiers when the test sample is available, we still like to1A in-depth background information on the theory of reproduction of nuclei that can be found at Aronszajn (1950), i.e. covariate shift has occurred."}, {"heading": "4. Theoretical Analysis", "text": "This section contains our main contribution, i.e. a theoretical analysis of the RMM estimator for EY te."}, {"heading": "4.1. The population version", "text": "I would like to thank all those who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles."}, {"heading": "4.2. The empirical version", "text": "NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NTT-NT-NTT-NT-NTT-NTT-NT-NTT-NT-NTT-NT-NTT-NT-NT-NTT-NT-NT-NT-NTT-NTT-NT-NT-NTT-NT-NTT-NT-NTT-NT-NT-NT-NT-NT-NT-NT-NTT-NT-NTT-NT-NT-NTT-NT-NT-NT-NT-NTT-NT-NT-NTT-NT-NT-NT-NTT-NT-NTT-NT-NTT-NT-NTT-NT-NT-NTT-NT-NT-NT-NT-NT-NT-NT-NT-NT-NT-NT-NT-NTT-NT-NT-NT-NT-NT-NT-NT-NT-"}, {"heading": "4.3. Discussion", "text": "It would appear that we are discussing a very natural question that the reader may already have: why not the regression function m (1) based on the data collected (1) based on the data collected (1) based on the data collected (1) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) based on the data collected (2) (2) based on the (2) based on the data collected (2) (2) based on the (2) based on the data collected."}, {"heading": "5. Conclusion", "text": "For the estimation of the expected value of the results of the test set at which a covariate shift has occurred, we have derived high probability limits for the confidence calculation of the mean value of the core (KMM), which are approximately O (n \u2212 1 2 tr + n \u2212 12) if the regression function is in the RKHS, and more generally O (n \u2212 s ntr \u00b7 ntentr + nte) if the regression function has a certain regularity measured by \u03b8. An extremely slow rate of about O (log \u2212 s ntr \u00b7 ntentr + nte) is also provided, which draws attention to the selection of the right core. Comparing the limits results that KMM is much superior to the plug-in estimator, which provides concrete evidence / understanding of the effectiveness of KMM under covariable shift. Although it is unclear to us whether it is possible to avoid an approximation of the regression function, we assume that the problem is optimal, and that in theory 2 it is the least likely to result."}, {"heading": "Acknowledgements", "text": "This work was supported by Alberta Innovates Technology Futures and NSERC."}], "references": [{"title": "Analysis of representations for domain adaptation", "author": ["Ben-David", "Shai", "Blitzer", "John", "Crammer", "Koby", "Pereira", "Fernando"], "venue": "In NIPS, pp", "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "Discriminative learning under covariate", "author": ["Bickel", "Steffen", "Br\u00fcckner", "Michael", "Scheffer", "Tobias"], "venue": "shift. JMLR,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Learning bounds for domain adaptation", "author": ["Blitzer", "John", "Crammer", "Koby", "Kulesza", "Alex", "Pereira", "Fernando", "Wortman", "Jennifer"], "venue": "In NIPS, pp", "citeRegEx": "Blitzer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2008}, {"title": "Sample selection bias correction theory", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Riley", "Michael", "Rostamizadeh", "Afshin"], "venue": "In ALT, pp", "citeRegEx": "Cortes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2008}, {"title": "Learning bounds for importance weighting", "author": ["Cortes", "Corinna", "Mansour", "Yishay", "Mohri", "Mehryar"], "venue": "In NIPS, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Learning theory: an approximation theory viewpoint", "author": ["Cucker", "Felipe", "Zhou", "Ding-Xuan"], "venue": null, "citeRegEx": "Cucker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cucker et al\\.", "year": 2007}, {"title": "Covariate Shift by Kernel Mean Matching, pp. 131\u2013160", "author": ["Gretton", "Arthur", "Smola", "Alexander J", "Huang", "Jiayuan", "Schmittfull", "Marcel", "Borgwardt", "Karsten M", "Sch\u00f6lkopf", "Bernhard"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Sample selection bias as a specification", "author": ["Heckman", "James J"], "venue": "error. Econometrica,", "citeRegEx": "Heckman and J.,? \\Q1979\\E", "shortCiteRegEx": "Heckman and J.", "year": 1979}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Huang", "Jiayuan", "Smola", "Alexander J", "Gretton", "Arthur", "Borgwardt", "Karsten M", "Sch\u00f6lkopf", "Bernhard"], "venue": "In NIPS,", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "A least-squares approach to direct importance estimation", "author": ["Kanamori", "Takafumi", "Hido", "Shohei", "Sugiyama", "Masashi"], "venue": "JMLR, 10:1391\u20131445,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Statistical analysis of kernel-based leastsquares density-ratio estimation", "author": ["Kanamori", "Takafumi", "Suzuki", "Taiji", "Sugiyama", "Masashi"], "venue": "Machine Learning,", "citeRegEx": "Kanamori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2012}, {"title": "Optimum bounds for the distributions of martingales in Banach spaces", "author": ["Pinelis", "Iosif"], "venue": "The Annals of Probability,", "citeRegEx": "Pinelis and Iosif.,? \\Q1994\\E", "shortCiteRegEx": "Pinelis and Iosif.", "year": 1994}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Shimodaira", "Hidetoshi"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira and Hidetoshi.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira and Hidetoshi.", "year": 2000}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["Smale", "Steve", "Zhou", "Ding-Xuan"], "venue": "Constructive Approximation,", "citeRegEx": "Smale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smale et al\\.", "year": 2007}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["Sriperumbudur", "Bharath K", "Gretton", "Arthur", "Fukumizu", "Kenji", "Sch\u00f6lkopf", "Bernhard", "Lanckriet", "Gert R. G"], "venue": null, "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["Steinwart", "Ingo"], "venue": null, "citeRegEx": "Steinwart and Ingo.,? \\Q2002\\E", "shortCiteRegEx": "Steinwart and Ingo.", "year": 2002}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["Sugiyama", "Masashi", "Nakajima", "Shinichi", "Kashima", "Hisashi", "Buenau", "Paul Von", "Kawanabe", "Motoaki"], "venue": "In NIPS,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "A note on application of integral operator in learning theory", "author": ["Sun", "Hongwei", "Wu", "Qiang"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Learning and evaluating classifiers under sample selection bias", "author": ["Zadrozny", "Bianca"], "venue": "In ICML", "citeRegEx": "Zadrozny and Bianca.,? \\Q2004\\E", "shortCiteRegEx": "Zadrozny and Bianca.", "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "(2004); Huang et al. (2007); Bickel et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al.", "startOffset": 8, "endOffset": 53}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al.", "startOffset": 8, "endOffset": 76}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al.", "startOffset": 8, "endOffset": 98}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al.", "startOffset": 8, "endOffset": 122}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al. (2009). We focus in this paper on the covariate shift assumption which was first formulated by Shimodaira (2000) and has been followed by many others.", "startOffset": 8, "endOffset": 146}, {"referenceID": 1, "context": "(2007); Bickel et al. (2009); BenDavid et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al. (2009). We focus in this paper on the covariate shift assumption which was first formulated by Shimodaira (2000) and has been followed by many others.", "startOffset": 8, "endOffset": 252}, {"referenceID": 9, "context": "A number of methods have been proposed to estimate the RND from finite samples, including kernel mean matching (KMM) (Huang et al., 2007), logistic regression (Bickel et al.", "startOffset": 117, "endOffset": 137}, {"referenceID": 1, "context": ", 2007), logistic regression (Bickel et al., 2009), Kullback-Leibler importance estimation (Sugiyama et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 17, "context": ", 2009), Kullback-Leibler importance estimation (Sugiyama et al., 2008), least-squares (Kanamori et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 10, "context": ", 2008), least-squares (Kanamori et al., 2009), and possibly some others.", "startOffset": 23, "endOffset": 46}, {"referenceID": 6, "context": "From the analyses we are aware of, such as (Gretton et al., 2009) on the confidence bound of the RND by KMM, (Kanamori et al.", "startOffset": 43, "endOffset": 65}, {"referenceID": 11, "context": ", 2009) on the confidence bound of the RND by KMM, (Kanamori et al., 2012) on the convergence rate of the least-squares estimate of the RND, and (Cortes et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 3, "context": ", 2012) on the convergence rate of the least-squares estimate of the RND, and (Cortes et al., 2008) on the distributional stability, they all assume that certain functions lie in the reproducing kernel Hilbert space (RKHS) induced by some user selected kernel.", "startOffset": 78, "endOffset": 99}, {"referenceID": 4, "context": "Recently, in a different setting, (Cortes et al., 2010) managed to replace this assumption with a bounded second moment assumption, at the expense of sacrificing the rate a bit.", "startOffset": 34, "endOffset": 55}, {"referenceID": 9, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 17, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 3, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 1, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 10, "context": "Indeed, a large body of work has been devoted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009).", "startOffset": 71, "endOffset": 195}, {"referenceID": 1, "context": ", 2008; Bickel et al., 2009; Kanamori et al., 2009). From the many references, we single out the kernel mean matching (KMM) algorithm, first proposed by Huang et al. (2007) and is also the basis of this paper.", "startOffset": 8, "endOffset": 173}, {"referenceID": 4, "context": "More details can be found in the paper of Gretton et al. (2009). A finite sample 1\u2212\u03b4 confidence bound for L\u0302(\u03b2) (similar as (10) below) is established in Gretton et al.", "startOffset": 42, "endOffset": 64}, {"referenceID": 4, "context": "More details can be found in the paper of Gretton et al. (2009). A finite sample 1\u2212\u03b4 confidence bound for L\u0302(\u03b2) (similar as (10) below) is established in Gretton et al. (2009). This bound is further transferred into a confidence bound for the generalization error of some family of loss minimization algorithms in Cortes et al.", "startOffset": 42, "endOffset": 176}, {"referenceID": 3, "context": "This bound is further transferred into a confidence bound for the generalization error of some family of loss minimization algorithms in Cortes et al. (2008), under the notion of distributional stability.", "startOffset": 137, "endOffset": 158}, {"referenceID": 6, "context": "Exhaustive experimental results on KMM can already be found in Gretton et al. (2009).", "startOffset": 63, "endOffset": 85}, {"referenceID": 15, "context": "Second, if the kernel k is characteristic (Sriperumbudur et al., 2010), meaning that the map \u222b X \u03a6(x)P(dx) from the space of probability measures to the RKHS H is injective, then we conclude \u03b2\u0302\u2217 = \u03b2 from (4) hence follows (5).", "startOffset": 42, "endOffset": 70}], "year": 2012, "abstractText": "In real supervised learning scenarios, it is not uncommon that the training and test sample follow different probability distributions, thus rendering the necessity to correct the sampling bias. Focusing on a particular covariate shift problem, we derive high probability confidence bounds for the kernel mean matching (KMM) estimator, whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel. By comparing KMM with the natural plug-in estimator, we establish the superiority of the former hence provide concrete evidence/understanding to the effectiveness of KMM under covariate shift.", "creator": "LaTeX with hyperref package"}}}