{"id": "1206.4651", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Is margin preserved after random projection?", "abstract": "Random projections have been applied in many machine learning algorithms. However, whether margin is preserved after random projection is non-trivial and not well studied. In this paper we analyse margin distortion after random projection, and give the conditions of margin preservation for binary classification problems. We also extend our analysis to margin for multiclass problems, and provide theoretical bounds on multiclass margin on the projected data.", "histories": [["v1", "Mon, 18 Jun 2012 15:24:01 GMT  (558kb)", "http://arxiv.org/abs/1206.4651v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["qinfeng shi", "chunhua shen", "rhys hill", "anton van den hengel"], "accepted": true, "id": "1206.4651"}, "pdf": {"name": "1206.4651.pdf", "metadata": {"source": "META", "title": "Is margin preserved after random projection?", "authors": ["Qinfeng Shi", "Chunhua Shen", "Anton van den Hengel"], "emails": ["javen.shi@adelaide.edu.au", "chunhua.shen@adelaide.edu.au", "rhys.hill@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "Difference between data classes is a key term in many existing classification algorithms, including support for vector machines (SVMs) (Cortes & Vapnik, 1995; Crammer & Singer, 2001) and boosting (Schapire & Freund, 1998) These classifiers are fundamentally involved in the identification and characterization of such margins and are described in terms of the accuracy and universality with which they do so. Random projections have attracted much attention in a number of areas, including signal processing (Donoho, 2006; Baraniuk et al., 2007) and clustering (Schulman, 2000), mainly due to the fact that distances under such transformations are preserved under certain circumstances (Dasgupta & Gupta, 2002). Random projections have also been applied to classification for a variety of purposes (Balcan et al., 2006; Duarte et al., 2007; Shi et al., multicing.)."}, {"heading": "2. Motivation and Definitions", "text": "A typical definition of the margin for a binary classification problem is the following: definition 1 (margin) The data set S = {(xi, y, yi, yi, yi, yi, {\u2212 1, + 1}) mi = 1 is denoted as linearly separable by margin \u03b3 if there is a unit length u, y. Rd exists, so that for all (x, y, y, y < u, x > \u03b3.The maximum (among all \u03b3) margin is\u03b3 = min (x, y). S y < u, x >. (1) Unfortunately, this margin is not obtained after a random projection, which we show by a counterexample, shown in Figure 1. We construct a dataset of 4 data points (x1, y1), (x2, y2), (x2, y2), u, x, x >. (x3), x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, (x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, (x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, (x, (x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "2.1. Error-allowed margin", "text": "Balcan et al. (2006) investigated the problem of margin maintenance in the context of a random projection for binary classification, deriving a formula for the probability that a margin under a given projection would be reduced by less than half. They provided two margin definitions below, using data set S as defined in Definition 1 and data distribution D. Definition 2 (Normalized Margin) A data set S linearly separable by margin \u03b3 if there is u-Rd, so that there is a margin for all (x, y), y < u, x > Margin and x-Margin. Definition 3 (Error-Allowed Margin) A data distribution D linearly separable by margin \u03b3 with a risk of error if there is u-Rd, so that this margin (x, y), up-Margin D (y < u, x > Margin, x-Allowed Margin). Definition 3 (Error-Allowed Margin) is linear by Margin \u03b3 with a risk of error, so that this margin (x, y), up-Margin D (y < u, x > Margin, x-Allowed Margin)."}, {"heading": "3. Margin Distortion and Preservation", "text": "(< R = >) < R = > D = > D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D"}, {"heading": "4. Angle Preservation", "text": "In order to prove the conservation of the angle as in Theorem 5, we will use the following tail limit, which also appears in another form in the simplified representation of JohnsonLindestrauss Lemma in Dasgupta & Gupta (2002). \u2212 W \u2212 W \u2212 W # 246; F & # 252; F & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W & # 160; W # 160; W # 160; W # 160; W & # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W & 160; W # 160; W # 160; W # 160; W & 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W # 160; W & # 160; W # 160; W # 160; W # 160; W & # 160; W # 160; W # 160; W # 160; W & # 160; W # 160; W # 160; W # 160; W # 160; W # 160 & # 160; W # 160; W # 160; W # 160; W # 160 & # 160; W # 160; W # 160"}, {"heading": "5. Experiments", "text": "The experiments described below provide an empirical confirmation of the above theoretical analysis and a demonstration of its application to SVM."}, {"heading": "5.1. Angle and inner product preservation condition", "text": "Figure 2 shows the results of simulations in which we randomly generate two vectors w and x > x > x > x > conservation, generating 2,000 random Gaussian matrices of the form given in Theorem 5. Each such matrix is used to project the data uniquely into n dimensions in which n = {30, 60, 90,..., 300}. We vary the probability {0.1, 0.3} and calculate the empirical probability of rejection for angle conservation P1 = 1 \u2212 Pr (((1 \u2212) \u2264 < R, R x > < R x < w, x > < w, x > < (1 +)) and the empirical probability of rejection for inner product conservation P2 = 1 \u2212 Pr (((((1 \u2212) \u2264 < R < R < R, R x > < w, x > < < < < < < < < < < < < < < < >; < < < < < >; < < p; p; b)."}, {"heading": "5.2. Margin preservation", "text": "Margins We created L-parallel hyperplanes, where L is the number of classes. Each class consists of 5 data points x-R100 from a hyperplane. We then created 100 random Gaussian matrices. We used the random matrices to project the data, and then calculated both the normalized margin and the unnormalized margin. We show the diagrams for both binary and multi-class cases (L = 3). As we can see in Figure 3, the empirical probability of rejection decreases (i.e. margins are more likely to be obtained) as the number of projections increases. Implications for SVMs As has been shown, the above results can be applied even in cases where the data is linear rejection probability (i.e. margins are more likely to be obtained) as the number of projections increases."}, {"heading": "6. Conclusion", "text": "In particular, we have shown that the preservation of the margin is closely related to the preservation of the acute angle (cosine) and the preservation of the internal product. We have seen that the normalized margin is more informative than the unnormalized margin. We have also provided theoretical support for classification methods that use random projections to achieve a classification of multicultural classes with a single model parameter. In contrast to previous work in the field (Balcan et al., 2006), we have shown that it is possible to provide limits for error-free margin preservation without requiring an infinite number of predictions, and this for arbitrary tolerances instead of only half of the original margin."}, {"heading": "Acknowledgments", "text": "This work is supported by the Australian Research Council DECRA grant DE120101161. We thank Anders Eriksson for the discussion on the counter example and Maria-Florina Balcan and Avrim Blum for the discussion on error-allowed margins."}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Achlioptas,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas", "year": 2003}, {"title": "An algorithmic theory of learning: Robust concepts and random projection", "author": ["Arriaga", "Rosa I", "Vempala", "Santosh"], "venue": "Machine Learning,", "citeRegEx": "Arriaga et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arriaga et al\\.", "year": 2006}, {"title": "Kernels as features: On kernels, margins, and low-dimensional mappings", "author": ["Balcan", "M.-F", "A. Blum", "S. Vempala"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "A simple proof of the restricted isometry principle for random matrices", "author": ["R.G. Baraniuk", "M. Davenport", "R. DeVore", "M.B. Wakin"], "venue": "Constructive Approximation,", "citeRegEx": "Baraniuk et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baraniuk et al\\.", "year": 2007}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Crammer and Singer,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2001}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Dasgupta and Gupta,? \\Q2002\\E", "shortCiteRegEx": "Dasgupta and Gupta", "year": 2002}, {"title": "Multiscale random projections for compressive classification", "author": ["M.F. Duarte", "M.A. Davenport", "M.B. Wakin", "J.N. Laska", "D. Takhar", "K.F. Kelly", "R.G. Baraniuk"], "venue": "In Proc. IEEE Int. Conf. Image Processing,", "citeRegEx": "Duarte et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Duarte et al\\.", "year": 2007}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Dimensionality reductions that preserve volumes and distance to affine spaces, and their algorithmic applications", "author": ["A. Magen"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "Magen,? \\Q2007\\E", "shortCiteRegEx": "Magen", "year": 2007}, {"title": "Boosting the margin: a new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Freund"], "venue": "Annals of Statistics,", "citeRegEx": "Schapire and Freund,? \\Q1998\\E", "shortCiteRegEx": "Schapire and Freund", "year": 1998}, {"title": "Clustering for edge-cost minimization", "author": ["L.J. Schulman"], "venue": "In Proc. Annual ACM Symp. Theory of Computing,", "citeRegEx": "Schulman,? \\Q2000\\E", "shortCiteRegEx": "Schulman", "year": 2000}, {"title": "S.V.N. Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A.J. Smola", "Vishwanathan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Shi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Rapid face recognition using hashing", "author": ["Q. Shi", "H. Li", "C. Shen"], "venue": "In Proc. IEEE Conf. Computer Vision & Pattern Recognition, San Francisco,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "A new benchmark dataset for handwritten character recognition", "author": ["L.J.P. van der Maaten"], "venue": "Technical Report TiCC TR 2009-002,", "citeRegEx": "Maaten,? \\Q2009\\E", "shortCiteRegEx": "Maaten", "year": 2009}, {"title": "The Nature of Statistical Learning", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}], "referenceMentions": [{"referenceID": 3, "context": "Random projections have attracted much attention within a range of fields including signal processing (Donoho, 2006; Baraniuk et al., 2007), and clustering (Schulman, 2000), largely due to the fact that distances are preserved under such transformations in certain circumstances (Dasgupta & Gupta, 2002).", "startOffset": 102, "endOffset": 139}, {"referenceID": 10, "context": ", 2007), and clustering (Schulman, 2000), largely due to the fact that distances are preserved under such transformations in certain circumstances (Dasgupta & Gupta, 2002).", "startOffset": 24, "endOffset": 40}, {"referenceID": 2, "context": "Random projections have also been applied to classification for a variety of purposes (Balcan et al., 2006; Duarte et al., 2007; Shi et al., 2009a;b; 2010).", "startOffset": 86, "endOffset": 155}, {"referenceID": 6, "context": "Random projections have also been applied to classification for a variety of purposes (Balcan et al., 2006; Duarte et al., 2007; Shi et al., 2009a;b; 2010).", "startOffset": 86, "endOffset": 155}, {"referenceID": 2, "context": "In this vein we build upon the work of Balcan et al. (2006) which provided a lower bound on the number of dimensions required if a random projection was to have a given probability of maintaining half of the original margin in the data.", "startOffset": 39, "endOffset": 60}, {"referenceID": 2, "context": "In this vein we build upon the work of Balcan et al. (2006) which provided a lower bound on the number of dimensions required if a random projection was to have a given probability of maintaining half of the original margin in the data. Although an important step, Balcan et al. (2006) do not solve the problem because the resulting formulation demands infinite many projections in order to guarantee the preservation of an error free margin.", "startOffset": 39, "endOffset": 286}, {"referenceID": 2, "context": "Balcan et al. (2006) showed that if the original data has normalised margin \u03b3 then as long as the number of projections n \u2265 c \u03b32 ln 1 \u03c1\u03b4 , (2)", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Applying chi-square distribution tail bound (Achlioptas, 2003) implies tight bounds on Pr ( \u2016R x\u2016 \u2264 (1 \u2212 )\u2016x\u2016 ) and", "startOffset": 44, "endOffset": 62}, {"referenceID": 8, "context": "This theorem shares similar insight as Magen (2007), in which Magen showed that random projections preserve volumes and distances to affine spaces.", "startOffset": 39, "endOffset": 52}, {"referenceID": 7, "context": "2009), for example, than the multiclass SVM (Crammer & Singer, 2001) algorithm in liblinear (Fan et al., 2008).", "startOffset": 92, "endOffset": 110}, {"referenceID": 14, "context": "Projecting features to a lower dimensional space can significantly reduce the model capacity such as VC dimension (Vapnik, 1995).", "startOffset": 114, "endOffset": 128}, {"referenceID": 2, "context": "In contrast to previous work in the area (Balcan et al., 2006) we have shown that it is possible to provide bounds on error free margin preservation without requiring an infinite number of projections, and have done so for arbitrary tolerances, rather than only for half of the original margin.", "startOffset": 41, "endOffset": 62}, {"referenceID": 1, "context": "In contrast to previous work in the area (Balcan et al., 2006) we have shown that it is possible to provide bounds on error free margin preservation without requiring an infinite number of projections, and have done so for arbitrary tolerances, rather than only for half of the original margin. In addition, all of the above has been achieved for multiclass rather than solely binary classifiers. It is worth pointing out that our error free margin is defined on a dataset as traditional margin concepts whereas Balcan et al. (2006)\u2019s error allowed margin is defined on a data distribution.", "startOffset": 42, "endOffset": 533}], "year": 2012, "abstractText": "Random projections have been applied in many machine learning algorithms. However, whether margin is preserved after random projection is non-trivial and not well studied. In this paper we analyse margin distortion after random projection, and give the conditions of margin preservation for binary classification problems. We also extend our analysis to margin for multiclass problems, and provide theoretical bounds on multiclass margin on the projected data.", "creator": "LaTeX with hyperref package"}}}