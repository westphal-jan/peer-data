{"id": "1206.4653", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Dimensionality Reduction by Local Discriminative Gaussians", "abstract": "We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-of-the-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data.", "histories": [["v1", "Mon, 18 Jun 2012 15:24:49 GMT  (662kb)", "http://arxiv.org/abs/1206.4653v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["nathan parrish", "maya r gupta"], "accepted": true, "id": "1206.4653"}, "pdf": {"name": "1206.4653.pdf", "metadata": {"source": "META", "title": "Dimensionality Reduction by Local Discriminative Gaussians", "authors": ["Nathan Parrish", "Maya R. Gupta"], "emails": ["nparrish@u.washington.edu", "gupta@ee.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "Dimensionality reduction is the mapping of high-dimensional data into a low-dimensional space while retaining as much of the information content of the data as possible. As a pre-processing step for supervised classification algorithms, dimensionality reduction achieves several important objectives. It reduces storage requirements and the complexity of the algorithm by improving the performance of learning algorithms by rejecting false or noisy features before training and testing. Dimensionality reduction can also protect against overmatching by reducing the number of parameters achieved by appearing in the proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, 2012. Author (s) / Owner Copyright 2012. We present a method for supervision reduction based on a local discriminatory Gaussian (LDG) criterion."}, {"heading": "2. Problem Formulation", "text": "We assume that a set of labeled training data (1) (1) where the indicator has a real function (1), from which we proceed (2). We want to find a matrix B * Rd * l, l < d so that the training pairs tested by the other n \u2212 1 can be separated. We measure this separability by the performance of a generative classifier. Let's measure the probability of the xi given class yi, which is estimated by the other n \u2212 1 tested training pairs. Then we measure this separability by the performance of a generative classifier. We measure the separation we have achieved by B: n (BTxi) p (BTxi | yi) p (yi) < max j p) p (BTxi) p (j) p (j) p (j) p (j) p (j) p (j)."}, {"heading": "2.1. LDG Solution", "text": "B, which is optimized (4), can be found with a self-decomposition. DefineV = n \u2211 i = 1 \u03c32i, yi \u0445 i, yi \u0445 T i, yiA = n \u2211 i = 1 m \u2211 j = 1 p (j) \u03c32i, j \u0445 i, j \u0445 T, j. Then (4) asB \u043a = arg min B \u0441\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441st\u0438\u0439 (5) as the smallest eigenvectors of the matrix (V \u2212 A) (5) s.t. BTB = I. Since both V and A are true symmetrical matrices, it is easy to show that the solution crosses A (5) as the smallest eigenvectors of the matrix (A \u2212 V) can be found in addition to the eigenvectors \u2212 V."}, {"heading": "3. Related Methods for Linear Dimensionality Reduction", "text": "Fisher's Discriminance Analysis (FDA) (Fisher, 1936) is a controlled technique that selects B to maximize the ratio of covariance between classes S (b) and covariance within class S (w).The solution is to select the uppermost eigenvectors of the generalized self-decomposition S (b) \u03bb = \u03bdS (w) \u03bb. FDA has two disadvantages: First, the FDA can work poorly on multimodal data where no single linear boundary separates the data by classes. Second, the covariance matrix is between classes m \u2212 1, so that the FDA can provide no more than m \u2212 1 dimensions. Local Fisher Discrimination Analysis (LFDA, Sugiyama, 2007) mitigates the disadvantages of the FDA. The LFDA generalizes the FDA by applying a weight reduction based on Pairwise Sample Distances between the class and covariance matrices within the class."}, {"heading": "4. Dimensionality Reduction Experiments", "text": "We conduct experiments to compare LDG with several different dimensionality reduction methods: PCA, FDA, LFDA, NCA, and information theoretical metric learning (ITML) (Davis et al., 2007) with feature selection using maximum relevance, minimum redundancy criteria via FDA-NN classification (Peng et al., 2005). For the NCA, LFDA, ITML, and MRMR feature selection, we use the code provided by the authors. We evaluate the performance of dimensionality reduction methods via k-NN classification with k = 3, as was done in (Weinberger & Saul, 2009). As a pre-processing step, we normalize the training data so that each feature has an average of zero and standard deviation. We select the number of neighbors we use for estimating local Gaussians for LDG dimensionality reduction by fiefold validation."}, {"heading": "5. LDG for Transfer Learning", "text": "In this section, we apply LDG dimensionality reduction to the transfer of learning objectives. In transfer learning, we would like to classify test data drawn from an unknown target domain distribution of feature vectors and class names, where we have very few training examples. However, we assume that we have many training examples from a source domain that differ from the target domain but are considered useful for learning. In our experiments, for example, we treat MNIST ratios as a source and USPS handletters as a target (see Figure 4). Let T = {(xi, yi)} nti = 1 be the target domain training data drawn from an unknown common distribution pT (x, y). Let S = {x ', y'ns + 1 be the source domain training data we draw from an unknown common distribution pS (x, y)."}, {"heading": "6. Related Methods for Transfer Learning", "text": "Let us consider dM (xi, x ') = (xi \u2212 x') TM (xi \u2212 x '), the square Mahalanobis distance, as the original ITML target: M * = arg min M 0 Tr (M) \u2212 log det (M) s.t. dM (xi, x') \u2264 u, if yi = y '(8) dM (xi, x') \u2265 v, if yi 6 = y '. For transfermetric learning, the authors suggest using objective function (8), but creating constraints only between examples from different areas, i.e. xi \u0432I and x \"\u0432S.\" In this way, they find an M that makes the distances between examples in the two areas small for data of the same class and large for data from different classes."}, {"heading": "7. Transfer Learning Experiments", "text": "We perform transfer learning experiments for two different classification problems. The first is to classify images by the category of the object found in the images, a thirty-class problem, with datasets from three areas: Amazon product images, images taken with a high-resolution DSLR camera, and images taken with a low-resolution webcam. Examples of the back-packs category for these three areas are shown in Figure 3. This dataset was first used by Saenko, et al., and we use the same pre-processing techniques described in (Saenko et al., 2010) to identify the images that result in 800 features per image. In the second problem, the two different domains show digital images in the MNIST and USPS datasets. The image features are the raw pixel values, and the only pre-processing we use is to increase the size of the NIST images to 16 pixels."}, {"heading": "8. Conclusions", "text": "We introduced LDG Dimensionality Reduction, a technique that maps the data to a space where classes are locally distributed to each training point. LDG is solved by a simple maximum eigenvalue substitution, and therefore scales better than iterative methods and LFDA for large datasets. In addition, we have shown that reducing the dimensionality of LDG can be applied to transfer learning problems with good results."}], "references": [{"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In Proc. International Conference on Machine Learning,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of Human Genetics,", "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Completely lazy learning", "author": ["E.K. Garcia", "S. Feldman", "M.R. Gupta", "S. Srivastava"], "venue": "IEEE Trans. Knowledge and Data Engineering,", "citeRegEx": "Garcia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2010}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "In Proc. Advances in Neural Information Processing Systems", "citeRegEx": "Globerson and Roweis,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis", "year": 2006}, {"title": "Neighbourhood components analysis", "author": ["A. Globerson", "S.T. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In Proc. Advances in Neural Information Processing Systems", "citeRegEx": "Globerson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2005}, {"title": "Comparison of discriminative training methods for speaker verification", "author": ["C. Ma", "E. Chang"], "venue": "In Proc. IEEE ICASSP 2003,", "citeRegEx": "Ma and Chang,? \\Q2003\\E", "shortCiteRegEx": "Ma and Chang", "year": 2003}, {"title": "Feature selection based on mutual information criteria of maxdependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F Long", "C. Ding"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In Proc. Computer Vision ECCV 2010,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis", "author": ["M. Sugiyama"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sugiyama,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama", "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "However, to reduce the model bias of assuming one Gaussian per class, we model p(xi|j) as locally Gaussian (Garcia et al., 2010).", "startOffset": 107, "endOffset": 128}, {"referenceID": 1, "context": "Fisher discriminant analysis (FDA) (Fisher, 1936) is a supervised technique that chooses B to maximize the ratio of the between-class covariance S to the within-class covariance S.", "startOffset": 35, "endOffset": 49}, {"referenceID": 8, "context": "Local Fisher discriminant analysis (LFDA) (Sugiyama, 2007) alleviates the drawbacks of FDA.", "startOffset": 42, "endOffset": 58}, {"referenceID": 4, "context": "Neighbourhood components analysis (NCA) (Globerson et al., 2005) is a dimensionality reduction technique that is based on a smooth approximation to the leave-one-out k-NN error.", "startOffset": 40, "endOffset": 64}, {"referenceID": 0, "context": "The approaches given in (Globerson & Roweis, 2006; Davis et al., 2007; Weinberger & Saul, 2009) propose convex optimization problems for finding M .", "startOffset": 24, "endOffset": 95}, {"referenceID": 0, "context": "However, we can perform dimensionality reduction by rewriting the Mahalanobis metric as M = L\u039bL and using a feature selection method on the resulting zi = \u039b Lxi as proposed in (Globerson & Roweis, 2006; Davis et al., 2007).", "startOffset": 176, "endOffset": 222}, {"referenceID": 0, "context": "We perform experiments to compare LDG to several different dimensionality reduction methods: PCA, FDA, LFDA, NCA, and information theoretic metric learning (ITML) (Davis et al., 2007) with feature selection using the maximum-relevance, minimum redundancy criterion (MRMR) (Peng et al.", "startOffset": 163, "endOffset": 183}, {"referenceID": 6, "context": ", 2007) with feature selection using the maximum-relevance, minimum redundancy criterion (MRMR) (Peng et al., 2005).", "startOffset": 96, "endOffset": 115}, {"referenceID": 2, "context": "We choose the number of neighbors used to estimate the local Gaussians for the LDG dimensionality reduction by fivefold cross-validation using a local QDA classifier (Garcia et al., 2010) on the original data.", "startOffset": 166, "endOffset": 187}, {"referenceID": 7, "context": "Of the dimensionality reduction techniques described in Section 3, only ITML has been adapted to the transfer learning scenario (Saenko et al., 2010).", "startOffset": 128, "endOffset": 149}, {"referenceID": 7, "context": ", and we use the same preprocessing techniques as described in (Saenko et al., 2010) to featurize the images, which results in 800 features per image.", "startOffset": 63, "endOffset": 84}, {"referenceID": 7, "context": "Finally, we compare to linear ITML for transfer learning as described in (Saenko et al., 2010) with MRMR feature selection.", "startOffset": 73, "endOffset": 94}], "year": 2012, "abstractText": "We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-ofthe-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data.", "creator": "LaTeX with hyperref package"}}}