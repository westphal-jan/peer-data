{"id": "1206.4655", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Modelling transition dynamics in MDPs with RKHS embeddings", "abstract": "We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as \\emph{embeddings} in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with least-squares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments.", "histories": [["v1", "Mon, 18 Jun 2012 15:25:58 GMT  (265kb)", "http://arxiv.org/abs/1206.4655v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["steffen gr\u00fcnew\u00e4lder", "guy lever", "luca baldassarre", "massimiliano pontil", "arthur gretton"], "accepted": true, "id": "1206.4655"}, "pdf": {"name": "1206.4655.pdf", "metadata": {"source": "META", "title": "Modelling transition dynamics in MDPs with RKHS embeddings", "authors": ["Steffen Gr\u00fcnew\u00e4lder", "Guy Lever", "Luca Baldassarre", "Arthur Gretton"], "emails": ["steffen@cs.ucl.ac.uk", "g.lever@cs.ucl.ac.uk", "l.baldassarre@cs.ucl.ac.uk", "m.pontil@cs.ucl.ac.uk", "arthur.gretton@gmail.com"], "sections": [{"heading": null, "text": "Published in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by author (s) / owner (s)."}, {"heading": "1. Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Preliminaries", "text": "We define by B (X) and Cb (X) the Banach areas with limited functions and limited continuous functions on X, each equipped with the sup standard. We look in particular at the problem where we control a series of deterministic measures (X). The aim is to find a policy measure that maximizes the expected sum of rewards achieved by the following steps: E [zi] t = 0 g trt + 1 (Xt, At). We define a reward rt + 1 = r (xt, at). We define a set of deterministic measures. (Xt) The aim is to find a policy that maximizes the expected sum of rewards achieved by the following steps. (Xt = 0 g, At). (X0 = x, At)."}, {"heading": "1.2. Overview of the approach", "text": "A number of recent studies have focused on the efficient evaluation of conditional expectations of functions that \"behave well\" in the sense that they belong to a reproducing Hilbert space (RKHS). These approaches have been particularly successful in performing conclusions in graphical models in which the model parameters are learned non-parametrically from data (Song et al., 2010b; 2009; 2011). The most important finding in this work is that conditional probabilities can be presented as functions in an RKHS, as conditional distribution embeddings. Conditional expectation of a function in the RKHS then becomes a linear operation in which we consider the inner product with the corresponding distribution embedding. Many methods for solving problems in MDPs require the calculation of functional expectations (value functions for example) in relation to transition dynamics, and so (approximations of) the operator f 7 \u2192 EX \u0445 P (\u00b7 x, a) [X)]]."}, {"heading": "1.3. Advantages of the approach", "text": "A direct core-based approach has a number of advantages. Firstly, such as density estimates, conditional embedding can be learned from a training sample: we do not have to deal with the problem of modelling system dynamics, such as the differential equations that regulate a robot arm. Unlike density estimates, however, distribution estimates do not scale badly with the dimension of the underlying space: the risk of a kernel density estimate increases as O (m \u2212 4 / (4 + d)) if the optimal bandwidth is used (Wasserman, 2006) [Sec. 6.5]. In contrast, the rate of convergence for conditional mean embedding is independent of the dimension of the underlying space (Song et al., 2010b). Secondly, the solution to many control problems involves a compilation of high-dimensional integrals to obtain expectations, which is prohibitively costly."}, {"heading": "1.4. Relation to existing methods", "text": "The methods include Kernel LSTD (Xu et al., 2005) and GPTD (Engel et al., 2005), which can be used to estimate (q-) values and differ mainly by the regulator (Taylor & Parr, 2009).Based on the q-value estimates, it is possible to optimize policy.Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011) Here, transition models (densities) are learned using Gaussian processes or grain density estimates, the use of which appreciation or policy optimization usually leads to a numerical solution, e.g. by means of an intermediate sample methodology. In contrast, we use cores to directly learn the expectation operators and avoid numerical integration. Finally, in (Paret al., 2008) an approach is proposed that is in contrast to an approximate representation of expectations."}, {"heading": "2. RKHS embeddings of transition probability kernels", "text": "Considering a fixed semi-definite distribution (p.s.d.) of the core K: Z \u00b7 Z \u2192 R (see e.g. Steinwart & Christmann, 2008, for details), we denote the number of reproducing kernels Hilbert Raum (RHKS), namely according to < \u00b7 K the inner product in HK. Due to the reproducing property of K in HK, we have h (x) = < K (x, \u00b7), h > K for all h-HK. We remember the idea of a universal kernel: a space of functions F RZ a kernel is a universal, if HK is universal in F, if HK is universal in F-HK."}, {"heading": "3. Application to MDPs", "text": "The learned embeddings are applied to MDPs by remembering (4) and defining an operator (x, a) which (x, a) (x, a) (x, a) (x, a) (x, a) (x, b) (x, a) (x, a) (x, a) (x, a) (x, b) (x) (x) (x, a) (x) (x) (x) (x) (x)) (if f / x) the quality of the approximation further depends on how well f can be defined by a low standard function in HL (x, a). This operator can be used instead of the true unknown operator (2) in any MDP method that implements such expectations as dynamic programming. As an example below, we analyze a value parameter, but similar considerations for other methods."}, {"heading": "4. Experiments", "text": "We performed three experiments using embedding in appreciation and policy optimization: the first experiment was an MDP with a fully observed discrete state space to demonstrate the convergence of value function with increasing training sample size; the second and third experiments evaluate our approach with respect to a classical control task and a task with high-dimensional states; in policy optimization we compare with LSPI (Lagoudakis & Parr, 2003), where we use the q-estimator (Engel et al., 2005), and for appreciation we compare with NPDP5 (Kroemer & Peters, 2011); we achieve better performance in all our experiments; we briefly address the choice of the regularization term; it can be shown that the conditional embedding 5We thank the authors for providing the Code.solve, \u00b5: = argmin-H [m \u00b2 H (x \u00b2) TEST'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S CODE'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S'S"}, {"heading": "4.1. Experiment 1", "text": "The first experiment is a navigation experiment in a 50 x 50 room. The reward is a Gaussian in the middle of the room. The agent has four actions: to the north, east, south or west. Each action has an 80% success rate and leads to random movement with a 20% probability. The state space is fully observed. We learn the conditional distribution, which consists of either 1000 or 5000 evenly tested transitions, uniformity ensuring that we avoid exploration artifacts. To determine the regulator, we have used a Gaussian nucleus and cross-validated the results are shown in Figure 1."}, {"heading": "4.2. Experiment 2", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.3. Experiment 3", "text": "Our last experiment is a high-dimensional task where sensor measurements are available and no state description is available; the environment consists of two rooms connected by a short corridor (Buhmer, 2012); the sensor measurements are images from a 3D renderer, and we aggregate four orientations (north, east, south and west) for a panorama, as the camera settings are unclear, especially near the walls; the agent's task is to reach a target in one of the rooms, using only the images to orient ourselves; training points were uniformly selected across the entrance area; we used a Gaussian core and validated the regulation parameter; the results for 4000 training points are shown in Figure 3. We compared the GP-based LSPI approach with the same core and settings for both approaches; the results are shown in Figure 2. Our method improves with increasing sample numbers; the GP-based LSPI approach would not be able to improve on this task and the DP would not be in the obvious position in this 3."}, {"heading": "5. Conclusions and Outlook", "text": "We have proposed a novel application of RKHS embedding to learning expectation operators associated with transition dynamics in MDPs, with particular emphasis on their use in dynamic programming methods. The approach avoids density estimates, sampling methods to evaluate integral or explicit models of the system, is computationally efficient, has cost linearly in the number of samples used in training (or even sublinear, with corresponding approximations), and has performance guarantees. Future work will focus on generalizing to more complex state and action spaces and extending the convergence results to continuous state spaces. Another important generalization concerns sample distribution, which is assumed to be iid, but in the non-iid case similar results can be expected."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the EPSRC # EP / H017402 / 1 (CARDyAL) and the European Union # FP7-ICT-270327 (Complacs) for their support."}], "references": [{"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bach and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2002}, {"title": "Mathematical issues in dynamic programming", "author": ["D. Bertsekas", "S. Shreve"], "venue": "Academic press,", "citeRegEx": "Bertsekas and Shreve,? \\Q1978\\E", "shortCiteRegEx": "Bertsekas and Shreve", "year": 1978}, {"title": "Robot Navigation using Reinforcement Learning and Slow Feature Analysis", "author": ["W. B\u00f6hmer"], "venue": "ArXiv,", "citeRegEx": "B\u00f6hmer,? \\Q2012\\E", "shortCiteRegEx": "B\u00f6hmer", "year": 2012}, {"title": "Reinforcement learning with gaussian processes", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "In ICML,", "citeRegEx": "Engel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2005}, {"title": "Modelling transition dynamics in mdps with rkhs embeddings", "author": ["S. Gr\u00fcnew\u00e4lder", "G. Lever", "L. Baldassarre", "M. Pontil", "A. Gretton"], "venue": "In arXiv,", "citeRegEx": "Gr\u00fcnew\u00e4lder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnew\u00e4lder et al\\.", "year": 2012}, {"title": "A non-parametric approach to dynamic programming", "author": ["O. Kroemer", "J. Peters"], "venue": "In NIPS,", "citeRegEx": "Kroemer and Peters,? \\Q2011\\E", "shortCiteRegEx": "Kroemer and Peters", "year": 2011}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation,", "citeRegEx": "Micchelli and Pontil,? \\Q2005\\E", "shortCiteRegEx": "Micchelli and Pontil", "year": 2005}, {"title": "Kernel-based reinforcement learning", "author": ["D. Ormoneit", "S. Sen"], "venue": "In Machine Learning,", "citeRegEx": "Ormoneit and Sen,? \\Q1999\\E", "shortCiteRegEx": "Ormoneit and Sen", "year": 1999}, {"title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning", "author": ["R. Parr", "L. Li", "G. Taylor", "C. Painter-Wakefield", "M.L. Littman"], "venue": "In ICML,", "citeRegEx": "Parr et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Parr et al\\.", "year": 2008}, {"title": "Gaussian processes in reinforcement learning", "author": ["C.E. Rasmussen", "M. Kuss"], "venue": "In NIPS. MIT Press,", "citeRegEx": "Rasmussen and Kuss,? \\Q2003\\E", "shortCiteRegEx": "Rasmussen and Kuss", "year": 2003}, {"title": "Methods of modern mathematical physics. Vol. 1: Functional Analysis", "author": ["M. Reed", "B. Simon"], "venue": null, "citeRegEx": "Reed and Simon,? \\Q1980\\E", "shortCiteRegEx": "Reed and Simon", "year": 1980}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "An upper bound on the loss from approximate optimal-value functions", "author": ["S.P. Singh", "R.C. Yee"], "venue": "Machine Learning,", "citeRegEx": "Singh and Yee,? \\Q1994\\E", "shortCiteRegEx": "Singh and Yee", "year": 1994}, {"title": "Kernels and regularization on graphs", "author": ["A.J. Smola", "R.I. Kondor"], "venue": "In COLT,", "citeRegEx": "Smola and Kondor,? \\Q2003\\E", "shortCiteRegEx": "Smola and Kondor", "year": 2003}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["L. Song", "J. Huang", "A.J. Smola", "K. Fukumizu"], "venue": "In ICML,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Hilbert space embeddings of hidden Markov models", "author": ["L. Song", "B. Boots", "S. Siddiqi", "G. Gordon", "A. Smola"], "venue": "In ICML,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Nonparametric tree graphical models", "author": ["L. Song", "A. Gretton", "C. Guestrin"], "venue": null, "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Kernel belief propagation", "author": ["L. Song", "A. Gretton", "D. Bickson", "Y. Low", "C. Guestrin"], "venue": "In AISTATS,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Algorithms for reinforcement learning", "author": ["C. Szepesvari"], "venue": null, "citeRegEx": "Szepesvari,? \\Q2009\\E", "shortCiteRegEx": "Szepesvari", "year": 2009}, {"title": "Kernelized value function approximation for reinforcement learning", "author": ["G. Taylor", "R. Parr"], "venue": "In ICML,", "citeRegEx": "Taylor and Parr,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Parr", "year": 2009}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": null, "citeRegEx": "Wendland,? \\Q2005\\E", "shortCiteRegEx": "Wendland", "year": 2005}, {"title": "Kernel least-squares temporal difference learning", "author": ["X. Xu", "T. Xie", "D. Hu", "X. Lu"], "venue": "International Journal of Information Technology,", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 20, "context": "(Szepesvari, 2009) for this background).", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "We require the following well-known result, which is proved in the Appendix for reference (Gr\u00fcnew\u00e4lder et al., 2012):", "startOffset": 90, "endOffset": 116}, {"referenceID": 4, "context": "Thus, the approach provides a framework for alleviating the curse of dimensionality in MDPs (particularly if, for example, sparsification of the embedding is considered, which we address briefly in the Appendix (Gr\u00fcnew\u00e4lder et al., 2012)).", "startOffset": 211, "endOffset": 237}, {"referenceID": 22, "context": "As a final advantage, the method applies wherever kernels may be defined, including on high dimensional or continuous state spaces, manifolds (kernels on the surface of a sphere (Wendland, 2005) are of particu-", "startOffset": 178, "endOffset": 194}, {"referenceID": 23, "context": "Methods include kernel LSTD (Xu et al., 2005) and GPTD (Engel et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005).", "startOffset": 17, "endOffset": 37}, {"referenceID": 9, "context": "Finally, in (Parr et al., 2008) a way is proposed to approximate expectations in a low dimensional state representation.", "startOffset": 12, "endOffset": 31}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al.", "startOffset": 18, "endOffset": 284}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011).", "startOffset": 18, "endOffset": 310}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011).", "startOffset": 18, "endOffset": 333}, {"referenceID": 3, "context": ", 2005) and GPTD (Engel et al., 2005). Both can be used to estimate (q-)values and they differ in this mainly through the regulariser (Taylor & Parr, 2009). Based on the q-value estimates it is possible to optimise the policy. Other related approaches include Rasmussen & Kuss (2003), Deisenroth et al. (2009), Ormoneit & Sen (1999) and Kroemer & Peters (2011). Here, transition models (densities) are learned with the help of Gaussian processes or kernel density estimates.", "startOffset": 18, "endOffset": 361}, {"referenceID": 15, "context": "Following (Song et al., 2009; 2010b) an estimate is", "startOffset": 10, "endOffset": 36}, {"referenceID": 15, "context": "Following Sriperumbudur et al. (2010), given any probability distribution P and p.", "startOffset": 10, "endOffset": 38}, {"referenceID": 15, "context": "These conditions require that the mapping (x, a) 7\u2192 EX\u223cP (\u00b7|(x,a))[f(X)] be an element of HK for all f \u2208 HL, and that the operator CY XC \u22123/2 XX be HilbertSchmidt, where CY X and CXX are covariance operators: see (Song et al., 2009) or Appendix D.", "startOffset": 213, "endOffset": 232}, {"referenceID": 4, "context": "1 for details (Gr\u00fcnew\u00e4lder et al., 2012).", "startOffset": 14, "endOffset": 40}, {"referenceID": 4, "context": "The following lemma is proved in the Appendix (Gr\u00fcnew\u00e4lder et al., 2012).", "startOffset": 46, "endOffset": 72}, {"referenceID": 4, "context": "The following result is proved in the Appendix (Gr\u00fcnew\u00e4lder et al., 2012): Proposition 3.", "startOffset": 47, "endOffset": 73}, {"referenceID": 4, "context": "(Sketch, see Appendix for full proof (Gr\u00fcnew\u00e4lder et al., 2012).", "startOffset": 37, "endOffset": 63}, {"referenceID": 3, "context": "In policy optimisation we compare to LSPI (Lagoudakis & Parr, 2003) where we use the q-value estimator from (Engel et al., 2005), and for value estimation we compare to NPDP (Kroemer & Peters, 2011).", "startOffset": 108, "endOffset": 128}, {"referenceID": 2, "context": "The environment consists of two rooms connected via a short corridor (B\u00f6hmer, 2012).", "startOffset": 69, "endOffset": 83}], "year": 2012, "abstractText": "We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as embeddings in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with leastsquares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "PDFSplit! (http://www.splitpdf.com)"}}}