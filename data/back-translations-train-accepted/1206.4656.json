{"id": "1206.4656", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Machine Learning that Matters", "abstract": "Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.", "histories": [["v1", "Mon, 18 Jun 2012 15:26:13 GMT  (233kb)", "http://arxiv.org/abs/1206.4656v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["kiri wagstaff"], "accepted": true, "id": "1206.4656"}, "pdf": {"name": "1206.4656.pdf", "metadata": {"source": "META", "title": "Machine Learning that Matters", "authors": ["Kiri L. Wagstaff"], "emails": ["kiri.l.wagstaff@jpl.nasa.gov"], "sections": [{"heading": "1. Introduction", "text": "The question is to what extent it has changed itself and the world as a whole."}, {"heading": "2. Machine Learning for Machine Learning\u2019s Sake", "text": "Our goal is not to point the finger at individuals or criticize them, but rather to initiate critical self-control and constructive, creative change. These problems do not interfere with all ML work, but they are frequent enough to justify our efforts to eliminate them. Nor is this about \"theory versus application.\" Theoretical work can be as inspired by real problems as applied work. Instead, criticism focuses on the limits of work that lies between theory and meaningful application: algorithmic advances accompanied by empirical studies that are disconnected from actual effect."}, {"heading": "2.1. Hyper-Focus on Benchmark Data Sets", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2.2. Hyper-Focus on Abstract Metrics", "text": "There are also problems with how we measure performance. Most commonly, an abstract evaluation metric (classification accuracy, root of mean square error or RMSE, F measure (van Rijsbergen, 1979), etc.) is used. These metrics are abstract in that they explicitly ignore or remove problem-specific details, usually so that numbers can be compared across domains. Is this seemingly obvious strategy providing us with useful information? It is acknowledged that the performance achieved by forming a modelM on data set X does not reflect the performance of other data sets drawn from the same problem, i.e., the training loss is an underestimation of test loss (Hastie et al., 2001). Strategies such as splitting X into training and test sets or cross-validation are aimed at estimating the expected performance of M when applied to all future data."}, {"heading": "2.3. Lack of Follow-Through", "text": "It is very difficult to identify a problem for which machine learning can offer a solution, to determine what data should be collected, to select or extract relevant characteristics, to select a suitable learning method, to select an evaluation method, to interpret the results, to involve domain experts, to publish the results of the relevant scientific community, to convince users to adopt the technique, and (only then) to have really made a difference (see Figure 1). An ML researcher may well feel exhausted or intimidated when considering this list of activities. However, each of these measures is a necessary part of a research program that seeks to have a real impact on the world outside of machine learning. Our field represents an additional barrier to impact. Generally speaking, only the activities in the middle row of Figure 1 are considered \"publishable\" in the ML community."}, {"heading": "3. Making Machine Learning Matter", "text": "Instead of following the letter of machine learning, can we rekindle its spirit? This is not just a matter of reporting on isolated applications. What we need is a fundamental change in the way we formulate, attack, and evaluate machine learning research projects."}, {"heading": "3.1. Meaningful Evaluation Methods", "text": "The first step is to define or select evaluation methods that, wherever possible, allow a direct measurement of the impact of ML innovation. In addition to the traditional measurements of performance, we can measure dollars saved, lives saved, time saved, effort reduced, quality of life increased, etc. Focusing our measurements on impact will help motivate the upstream restructuring of research efforts; they will guide us on how to select data sets, structure experiments, and define objective functions. At the very least, publications can report on how a given improvement in accuracy impacts the emerging problem area. Readers may wonder how this can be achieved if our goal is to develop generic methods that apply to all areas. However, as mentioned above, the common approach to using the same measurement in all areas is based on an unexplained and generally unfounded assumption that it is possible to equate an improvement of x% in one area with the other."}, {"heading": "3.2. Involvement of the World Outside ML", "text": "Many ML studies involve domain experts as collaborators who help define the ML problem and identify data for classification or regression tasks; they can also establish the missing link between an ML performance plot and its relevance to the problem area; this can help reduce the number of cases in which an ML system solves a partial problem that is of little interest to the relevant scientific community, or in which the performance of the ML system appears numerically good but is not reliable enough to ever be adopted; we could also solicit brief \"comments\" to accompany the publication of a new ML progress written by researchers with relevant domain expertise who have not been involved in ML research; they could provide an independent assessment of the performance, utility, and impact of the work; and, as an added benefit, it informs new communities of how and how well ML methods work."}, {"heading": "3.3. Eyes on the Prize", "text": "How many people, species, countries or square meters would be affected by a solution to the problem? What level of performance would be a meaningful improvement over the status quo? Warrick et al. (2010) provides an example of ML work that tackles all three aspects. Working with physicians and physicians, they developed a system to detect fetal hypoxia (oxygen deficiency) and enable emergency measures that literally save babies from brain injury or death. Following the publication of their results, which showed that 50% of cases of fetal hypoxia could be detected early enough for intervention, with an acceptable false positive rate of 7.5%, they are currently working on clinical trials as the next step toward widespread use."}, {"heading": "4. Machine Learning Impact Challenges", "text": "One way to guide research efforts is to articulate ambitious and meaningful challenges. In 1992, Carbonell articulated a list of challenges for the field, not to increase its impact, but instead to \"put the fun back into machine learning\" (Carbonell, 1992), including: 1. Discovering a new physical law leading to a published, related scientific article.2. Improving the 500 USCF / FIDE points in evaluating chess beyond a Class B level.3. Improving planning performance 100-fold in two different domains. 4. Investment returns of $1 million in a year.5. Exceeding a handcrafted NLP system on a task like translation.6. Outperform all handmade medical diagnostic systems with an ML solution used in at least two institutions and applied regularly. Because impact was not the guiding principle, these challenges extend far along that axis. An improved chess player might have the least active impact on the world during the following six live examples."}, {"heading": "5. Obstacles to ML Impact", "text": "Imagine that you get a grip on the problems you have in order to solve them. (...) You can't get a grip on them. (...) You can't get a grip on them. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place. (...) You can't put them in their place."}, {"heading": "6. Conclusions", "text": "Many researchers retire to their private studies with a copy of the dataset and work in isolation to achieve perfect algorithmic performance. Publishing results for the ML community is the end of the process. Success is usually not traced back to the original problem, or not in a form that can be used. However, these opportunities for real impact are widespread, and the fields of law, finance, politics, medicine, education, and more benefit from systems that can analyze, adapt, and take action (or at least recommend action). This paper highlights six examples of impact challenges and several real obstacles in the hope of stimulating a lively discussion about how ML can best make a difference."}, {"heading": "Acknowledgments", "text": "We thank Tom Dietterich, Terran Lane, Baback Moghaddam, David Thompson, and three insightful anonymous reviewers for their suggestions for this work, which was done during a sabbatical from the Jet Propulsion Laboratory."}], "references": [{"title": "Human development index: Methodology and measurement", "author": ["Anand", "Sudhir", "Sen", "Amartya K"], "venue": "Human Development Report Office,", "citeRegEx": "Anand et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Anand et al\\.", "year": 1994}, {"title": "The Netflix Prize", "author": ["Bennett", "James", "Lanning", "Stan"], "venue": "In Proc. of KDD Cup and Workshop,", "citeRegEx": "Bennett et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bennett et al\\.", "year": 2007}, {"title": "Machine learning: A maturing field", "author": ["Carbonell", "Jaime"], "venue": "Machine Learning,", "citeRegEx": "Carbonell and Jaime.,? \\Q1992\\E", "shortCiteRegEx": "Carbonell and Jaime.", "year": 1992}, {"title": "Yahoo! Learning to Rank Challenge overview", "author": ["Chapelle", "Olivier", "Chang", "Yi"], "venue": "JMLR: Workshop and Conference Proceedings,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Computational sustainability: Computational methods for a sustainable environment, economy, and society", "author": ["Gomes", "Carla P"], "venue": "National Academy of Engineering", "citeRegEx": "Gomes and P.,? \\Q2009\\E", "shortCiteRegEx": "Gomes and P.", "year": 2009}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "The meaning and use of the area under a receiver operating characteristic (ROC) curve", "author": ["J.A. Hanley", "B.J. McNeil"], "venue": null, "citeRegEx": "Hanley and McNeil,? \\Q1982\\E", "shortCiteRegEx": "Hanley and McNeil", "year": 1982}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Statistical phrase-based translation", "author": ["Koehn", "Philipp", "Och", "Franz Josef", "Marcu", "Daniel"], "venue": "In Proc. of the Conf. of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "The changing science of machine learning", "author": ["Langley", "Pat"], "venue": "Machine Learning,", "citeRegEx": "Langley and Pat.,? \\Q2011\\E", "shortCiteRegEx": "Langley and Pat.", "year": 2011}, {"title": "AUC: a misleading measure of the performance of predictive distribution models", "author": ["Lobo", "Jorge M", "Jimnez-Valverde", "Alberto", "Real", "Raimundo"], "venue": "Global Ecology and Biogeography,", "citeRegEx": "Lobo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lobo et al\\.", "year": 2008}, {"title": "CALO: Cognitive assistant that learns and organizes", "author": ["SRI International"], "venue": "http://caloproject.sri. com,", "citeRegEx": "International.,? \\Q2003\\E", "shortCiteRegEx": "International.", "year": 2003}, {"title": "Resolving confusion of tongues in statistics and machine learning: A primer for biologists and bioinformaticians", "author": ["M. van Iterson", "van Haagen", "H.H.B.M", "J.J. Goeman"], "venue": "Proteomics, 12:543\u2013549,", "citeRegEx": "Iterson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Iterson et al\\.", "year": 2012}, {"title": "A machine learning approach to the detection of fetal hypoxia during labor and delivery", "author": ["P.A. Warrick", "E.F. Hamilton", "R.E. Kearney", "D. Precup"], "venue": "In Proc. of the Twenty-Second Innovative Applications of Artificial Intelligence Conf.,", "citeRegEx": "Warrick et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Warrick et al\\.", "year": 2010}, {"title": "Ending Spam: Bayesian Content Filtering and the Art of Statistical Language Classification", "author": ["Zdziarski", "Jonathan A"], "venue": "No Starch Press, San Francisco,", "citeRegEx": "Zdziarski and A.,? \\Q2005\\E", "shortCiteRegEx": "Zdziarski and A.", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "tively solved spam email detection (Zdziarski, 2005) and machine translation (Koehn et al., 2003), two problems of global import.", "startOffset": 77, "endOffset": 97}, {"referenceID": 7, "context": ", training loss is an underestimate of test loss (Hastie et al., 2001).", "startOffset": 49, "endOffset": 70}, {"referenceID": 10, "context": ", extremely high false positive rates), and weighting false positives and false negatives equally, which may be inappropriate for a given problem domain (Lobo et al., 2008).", "startOffset": 153, "endOffset": 172}, {"referenceID": 5, "context": "It is easy to sit in your office and run a Weka (Hall et al., 2009) algorithm on a data set you downloaded from the web.", "startOffset": 48, "endOffset": 67}], "year": 2012, "abstractText": "Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field\u2019s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.", "creator": "LaTeX with hyperref package"}}}