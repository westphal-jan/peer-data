{"id": "1206.4659", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "abstract": "We present a max-margin nonparametric latent feature model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference.", "histories": [["v1", "Mon, 18 Jun 2012 15:27:56 GMT  (994kb)", "http://arxiv.org/abs/1206.4659v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jun zhu"], "accepted": true, "id": "1206.4659"}, "pdf": {"name": "1206.4659.pdf", "metadata": {"source": "CRF", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "authors": ["Jun Zhu"], "emails": ["dcszj@mail.tsinghua.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of us are able to outdo ourselves, and we are able to outdo ourselves, \"he said in an interview with The New York Times.\" I don't think we feel we can change the world. \""}, {"heading": "2. Latent Feature Relational Models", "text": "Suppose we have a N \u00b7 N Relational Link Matrix Y, where N is the number of entities. Let's consider the binary case in which the entry Yij = + 1 (or Yij = \u2212 1) indicates the presence (or absence) of a connection between entity i and entity j. We emphasize that all latent characteristic models presented below can be extended to deal with real or categorical Y. The goal of the link prediction is to learn a model from observed linkages in such a way that we can predict the values of unobserved entities of Y. In some cases, we have observed attributes that define the connection between i and j. In a latent characteristic model, each entity is associated with a vector \u00b5i RK, a point in a latent characteristic space (or latent social space). Then, the linkage is generally defined as asp (Yij = Xi\u00b5j, j, \u00b5j, \u00b5j)."}, {"heading": "3. Max-margin Latent Feature Models", "text": "Now we present the model of the latent maximum margin for link prediction. First, the basic concepts of MED are briefly reviewed (Jaakkola et al., 1999; Jebara, 2002)."}, {"heading": "3.1. MED", "text": "We consider the binary classification, where the response variable Y takes the values of {+ 1, \u2212 1}. Let X be an input vector and F (X; \u03b7) be a discriminating function parameterized by \u03b7. Let D = {(Xn, Yn)} Nn = 1 be a training set and h (x) = max (0, \u2212 x) define where there is a positive cost parameter. In contrast to standard SVMs, which estimate a single \u03b7, MED learns a distribution p (\u03b7) by having an entropically regulated risk combining problem with previous p0 (\u03b7) min p (\u03b7) KL (p) p0 (\u03b7) + CR (p (\u03b7))), where C is a positive constant; KL (p) q) is the KL divergence; R (p) min p; p; n h1 (N), the generative work is the positive one; C (xra) is the one; Xra (1) and Xp; n is the generative one; n is the variable one; n is the Xra (1)."}, {"heading": "3.2. MED Latent Feature Relational Model", "text": "We have used the same formulations as most general LFRM models. (...) We have the ability to define the latent discriminant function of which each line corresponds to an entity and each column. (...) The entity of the entity (...) means that the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (...) the entity (... the entity, the entity (...) the entity (...) the entity (... the entity, the entity, the entity (...) the entity (...) the entity (... the entity, the entity (...) the entity, the entity (...) the entity (... the entity, the entity, the entity (...) the entity (... the entity, the entity (...) the entity, the entity (...) the entity (... the entity, the entity, the entity (...) the entity, the entity (...) the entity (... the entity, the entity, the entity (...) the entity (...) the entity, the entity (... the entity, the entity, the entity (...) the entity, the entity, the entity (...) the entity, the... (..."}, {"heading": "3.2.1. Inference with Truncated Mean-Field", "text": "The above problem has beautiful properties. For example, Rj is a piece-by-piece linear functionality problem of p and f is linear. While sampling methods may lead to more accurate results, variational methods are generally more efficient and they also have a goal to monitor the convergence behavior. Here, we present a simple variation method to research such properties that prove to be good in practice. (9) Concretely, we make the truncated mean field assumptions of p (1, 2), p (3), p (2), K (2), k (2), p (2), p (3), p (3), p (3), p (2), p (3), p (3), p (3), p (3), p (3), p (3), p (4), p, 4, p, (4), p, 4, p (4), p, p, 4, (4), p, p (4, p, p, 4, p (4), p, p (4), p, p, p (4), p, p (4), p, p (4, p (4), p, p, p (4), p, p, p (4), p, p (4, p (4), p, p, p (4), p, p (4, p, p (4), p, p (4), p, p (4), p (4, p, p (4), p, p (4), p, p (4), p, p (4, p (4), p (4), p (4, p, p, p (4), p, p (4), p (4, p, p (4), p (4), p, p (4, p, p, p (4), p (4), p (4."}, {"heading": "3.3. The Fully-Bayesian Model", "text": "MedLFRM has a regularization parameter C, which normally plays an important role in large margin classifiers (especially on sparse and unbalanced datasets). To find a good value of C, cross-validation is a typical approach, but it could be mathematically expensive by comparing many candidates. Under the probability formula, we could offer an alternative way to control complexity, at least in part. We assume that the previous version is an isotropic normal distribution1 with common mean and precision, in which we present a full Bayesian MedLFRM model by introducing appropriate priorities for the hyperparameters. Normal-Gamma Prior: For simplification, we assume that the previous version is an isotropic normal distribution1 with common mean and precision."}, {"heading": "4. Experiments", "text": "We are now providing empirical studies on several real-world datasets to demonstrate the effectiveness of the Maxmargin principle in learning latent feature relational models, as well as the effectiveness of fully Bayesian methods in avoiding hyperparameter C tuning."}, {"heading": "4.1. Multi-relational Datasets", "text": "In fact, it is the case that most of us are able to move, move and move, \"he said in an interview with the Deutsche Presse-Agentur.\" It is not as if we are able to change the world, \"he said.\" But it is not as if we are able to change the world. \"He added,\" It is not as if we are able to change the world. \"He added,\" It is not as if we are able to change the world. \"He added,\" It is not as if we are able to change the world. \""}, {"heading": "4.2. Predicting NIPS coauthorship", "text": "The second experiments will be conducted on the basis of the co-authoring data constructed from the NIPS dataset, which contains a list of all papers and authors from NIPS 1-17. To better compare the symmetric co-author linkage data, we will use the same dataset as in (Miller et al., 2009), which contains 234 authors who publish with most other people2. In order to better match the symmetric co-author linkage data, we restrict our models to be symmetric, i.e., the rear mean of W is a symmetric matrix, as in (Miller et al., 2009). For MedLFRM and BayesMedLFRM LmmM LmmM, this symmetry limitation can easily be met if the SVM problems are solved (11) and (16). To determine the effects of the2The average probability of linkage formation on these data is about 0.02, also very unbalanced."}, {"heading": "4.3. Stability and Running Time", "text": "Figure 2 shows the change in objective function as well as the change in AUC values on the test data of the countries selected during the iterations for both MedLFRM and BayesMedLFRM. For MedLFRM, we report the results with the best C selected by cross-validation. We can see that the variational inference algorithms for both models converge quickly to a specific region. As we use a drop below the gradient to approximate the distribution of Z and the partial problems of the solution for p (VP) in practice, the objective function exhibits some disturbances, but within a relatively small interval. For the AUC values, we have similar observations, namely within several iterations, we could have very good link predictive activity. Again, the disturbance is maintained within a small region, which is reasonable for our approximate inference algorithms."}, {"heading": "5. Conclusions and Future Work", "text": "We have presented a discriminatory model of maximum latent margin for link prediction. Naturally, as part of a Bayesian-style maximum margin formulation, we incorporate Bayesian nonparametry ideas to automatically resolve the unknown dimensionality of a latent social space. Furthermore, we present a fully Bayesian formulation that can avoid tuning regularization constants. We have developed efficient variation methods for performing posterior inference. Empirical results on multiple real datasets seem to show the benefits inherited from both maximum margin learning and fully Bayesian methods.Our current analysis focuses on small static network snapshots. For future work, we are interested in learning more flexible latent function-relational models to deal with large dynamic networks and reveal more subtle network development patterns."}, {"heading": "Acknowledgements", "text": "JZ is supported by the research and development projects of the National Key Foundation 2012CB316301, a Starting Research Fund No. 553420003, the 221 Basic Research Plan for Young Faculties at Tsinghua University and a Research Fund No. 20123000007 from Microsoft Research Asia."}], "references": [{"title": "Mixed membership stochastic blockmodels", "author": ["E. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": "In NIPS, pp", "citeRegEx": "Airoldi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2008}, {"title": "Supervised random walks: predicting and recommending links in social networks", "author": ["L. Backstrom", "J. Leskovec"], "venue": "In WSDM,", "citeRegEx": "Backstrom and Leskovec,? \\Q2011\\E", "shortCiteRegEx": "Backstrom and Leskovec", "year": 2011}, {"title": "Choosing multiple parameters for support vector machines", "author": ["O. Chapelle", "V. Vapnik", "O. Bousquet", "S. Mukherjee"], "venue": "Machine Learning,", "citeRegEx": "Chapelle et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2002}, {"title": "Variational inference for the Indian buffet process", "author": ["F. Doshi-Velez", "K. Miller", "Gael", "J. Van", "Y.W. Teh"], "venue": "In AISTATS,", "citeRegEx": "Doshi.Velez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez et al\\.", "year": 2009}, {"title": "Bayesian approach to feature selection and parameter tuning for support vector machine classifiers", "author": ["C. Gold", "A. Holub", "P. Sollich"], "venue": "Neural Networks,", "citeRegEx": "Gold et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gold et al\\.", "year": 2005}, {"title": "Inference with normalgamma prior distributions in regression problems", "author": ["J.E. Griffin", "P.J. Brown"], "venue": "Bayesian Analysis,", "citeRegEx": "Griffin and Brown,? \\Q2010\\E", "shortCiteRegEx": "Griffin and Brown", "year": 2010}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2006\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2006}, {"title": "Latent space approaches to social network analysis", "author": ["P. Hoff", "A. Raftery", "M. Handcock"], "venue": "Journal of American Statistical Association,", "citeRegEx": "Hoff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2002}, {"title": "Modeling homophily and stochastic equivalence in symmetric relational data", "author": ["P.D. Hoff"], "venue": "In NIPS,", "citeRegEx": "Hoff,? \\Q2007\\E", "shortCiteRegEx": "Hoff", "year": 2007}, {"title": "Maximum entropy discrimination", "author": ["T. Jaakkola", "M. Meila", "T. Jebara"], "venue": "In NIPS,", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "Discriminative, generative and imitative learning", "author": ["T. Jebara"], "venue": "PhD Thesis,", "citeRegEx": "Jebara,? \\Q2002\\E", "shortCiteRegEx": "Jebara", "year": 2002}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J. Tenenbaum", "T. Griffithms", "T. Yamada", "N. Ueda"], "venue": "In AAAI,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Nonstationary kernel combination", "author": ["D.P. Lewis", "T. Jebara", "W.S. Noble"], "venue": "In ICML,", "citeRegEx": "Lewis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2006}, {"title": "The link prediction problem for social networks", "author": ["D. Liben-Nowell", "J.M. Kleinberg"], "venue": "In CIKM,", "citeRegEx": "Liben.Nowell and Kleinberg,? \\Q2003\\E", "shortCiteRegEx": "Liben.Nowell and Kleinberg", "year": 2003}, {"title": "Modeling dyadic data with binary latent factors", "author": ["E. Meeds", "Z. Ghahramani", "R. Neal", "S. Roweis"], "venue": "In NIPS,", "citeRegEx": "Meeds et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meeds et al\\.", "year": 2007}, {"title": "Nonparametric latent feature models for link prediction", "author": ["K. Miller", "T. Griffiths", "M. Jordan"], "venue": "In NIPS,", "citeRegEx": "Miller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2009}, {"title": "Estimation and prediction for stochastic blockstructures", "author": ["K. Nowicki", "T.A.B. Snijders"], "venue": "Journal of American Statistical Association,", "citeRegEx": "Nowicki and Snijders,? \\Q2001\\E", "shortCiteRegEx": "Nowicki and Snijders", "year": 2001}, {"title": "Stick-breaking construction of the Indian buffet process", "author": ["Y.W. Teh", "D. Gorur", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Maximum entropy discrimination markov networks", "author": ["J. Zhu", "E.P. Xing"], "venue": "JMLR, 10:2531\u20132569,", "citeRegEx": "Zhu and Xing,? \\Q2009\\E", "shortCiteRegEx": "Zhu and Xing", "year": 2009}, {"title": "MedLDA: Maximum margin supervised topic models for regression and classification", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}, {"title": "Infinite SVM: a Dirichlet process mixture of large-margin kernel machines", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}, {"title": "Infinite latent SVM for classification and multi-task learning", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "In NIPS,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 15, "context": "Often there is extra information about links and entities such as attributes and timestamps (Liben-Nowell & Kleinberg, 2003; Backstrom & Leskovec, 2011; Miller et al., 2009) that can be used to help with prediction.", "startOffset": 92, "endOffset": 173}, {"referenceID": 8, "context": "moid function) (Hoff, 2007; Miller et al., 2009) to define the link formation probability distribution.", "startOffset": 15, "endOffset": 48}, {"referenceID": 15, "context": "moid function) (Hoff, 2007; Miller et al., 2009) to define the link formation probability distribution.", "startOffset": 15, "endOffset": 48}, {"referenceID": 0, "context": "These latent feature models were shown to generalize latent class (Nowicki & Snijders, 2001; Airoldi et al., 2008) and latent distance (Hoff et al.", "startOffset": 66, "endOffset": 114}, {"referenceID": 7, "context": ", 2008) and latent distance (Hoff et al., 2002) models and are thus able to represent both homophily and stochastic equivalence, which are important properties commonly observed in real-world social network and relational data.", "startOffset": 28, "endOffset": 47}, {"referenceID": 15, "context": "The work (Miller et al., 2009) is an exception, which presents a nonparametric Bayesian method to automatically infer the unknown social dimension.", "startOffset": 9, "endOffset": 30}, {"referenceID": 9, "context": ", hinge-loss) that measures the quality of link prediction, under the principle of maximum entropy discrimination (MED) (Jaakkola et al., 1999; Jebara, 2002), which was introduced as an elegant framework to integrate max-margin learning and Bayesian generative modeling.", "startOffset": 120, "endOffset": 157}, {"referenceID": 10, "context": ", hinge-loss) that measures the quality of link prediction, under the principle of maximum entropy discrimination (MED) (Jaakkola et al., 1999; Jebara, 2002), which was introduced as an elegant framework to integrate max-margin learning and Bayesian generative modeling.", "startOffset": 120, "endOffset": 157}, {"referenceID": 15, "context": "First, like (Miller et al., 2009), we use nonparametric Bayesian techniques to automatically resolve the unknown dimension of a latent social space, and thus our work represents an attempt towards uniting Bayesian nonparametrics and max-margin learning, which have been largely treated as two isolated topics.", "startOffset": 12, "endOffset": 33}, {"referenceID": 7, "context": "For the latent distance model (Hoff et al., 2002), we have", "startOffset": 30, "endOffset": 49}, {"referenceID": 8, "context": "For the latent eigenmodel (Hoff, 2007), which generalizes the latent distance model and the latent class model for modeling symmetric relational data, we have", "startOffset": 26, "endOffset": 38}, {"referenceID": 15, "context": "The nonparametric latent feature relational model (LFRM) (Miller et al., 2009) leverages the recent advances in Bayesian nonparametrics to automatically infer the latent dimension.", "startOffset": 57, "endOffset": 78}, {"referenceID": 15, "context": "For modeling symmetric relational data, we usually constrain W to be symmetric (Miller et al., 2009).", "startOffset": 79, "endOffset": 100}, {"referenceID": 9, "context": "We first briefly review the basic concepts of MED (Jaakkola et al., 1999; Jebara, 2002).", "startOffset": 50, "endOffset": 87}, {"referenceID": 10, "context": "We first briefly review the basic concepts of MED (Jaakkola et al., 1999; Jebara, 2002).", "startOffset": 50, "endOffset": 87}, {"referenceID": 10, "context": "MED subsumes SVM as a special case and has been extended to incorporate latent variables (Jebara, 2002; Zhu et al., 2009) and to perform structured output prediction (Zhu & Xing, 2009).", "startOffset": 89, "endOffset": 121}, {"referenceID": 19, "context": "MED subsumes SVM as a special case and has been extended to incorporate latent variables (Jebara, 2002; Zhu et al., 2009) and to perform structured output prediction (Zhu & Xing, 2009).", "startOffset": 89, "endOffset": 121}, {"referenceID": 14, "context": "For finite sized matrices Z with K columns, we can define the prior as a Beta-Bernoulli process (Meeds et al., 2007).", "startOffset": 96, "endOffset": 116}, {"referenceID": 17, "context": "One elegant way to do that is the stick-breaking representation of IBP (Teh et al., 2007).", "startOffset": 71, "endOffset": 89}, {"referenceID": 10, "context": ", 2011a;b) to define the discriminant function using the expectation operator, instead of the traditional log-likelihood ratio of a Bayesian generative model with latent variables (Jebara, 2002; Lewis et al., 2006).", "startOffset": 180, "endOffset": 214}, {"referenceID": 12, "context": ", 2011a;b) to define the discriminant function using the expectation operator, instead of the traditional log-likelihood ratio of a Bayesian generative model with latent variables (Jebara, 2002; Lewis et al., 2006).", "startOffset": 180, "endOffset": 214}, {"referenceID": 3, "context": "For p(\u03bd), since the margin constraints are not dependent on \u03bd, we can get the same solutions as in (Doshi-Velez et al., 2009).", "startOffset": 99, "endOffset": 125}, {"referenceID": 4, "context": "the previous methods that were developed for estimating the hyper-parameters of SVM, by optimizing a logevidence (Gold et al., 2005) or an estimate of the generalization error (Chapelle et al.", "startOffset": 113, "endOffset": 132}, {"referenceID": 2, "context": ", 2005) or an estimate of the generalization error (Chapelle et al., 2002).", "startOffset": 51, "endOffset": 74}, {"referenceID": 15, "context": "We report the results of MedLFRM and BayesMedLFRM on the two benchmark datasets which were used in (Miller et al., 2009) to evaluate the performance of latent feature relational models.", "startOffset": 99, "endOffset": 120}, {"referenceID": 15, "context": "Depending on the input data, the latent features might not have interpretable meanings (Miller et al., 2009).", "startOffset": 87, "endOffset": 108}, {"referenceID": 11, "context": ", infinite relational model) (Kemp et al., 2006) and MMSB (i.", "startOffset": 29, "endOffset": 48}, {"referenceID": 0, "context": ", mixed membership stochastic block) (Airoldi et al., 2008), both of which were tested in (Miller et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 15, "context": ", 2008), both of which were tested in (Miller et al., 2009).", "startOffset": 38, "endOffset": 59}, {"referenceID": 15, "context": "As in (Miller et al., 2009), we consider two settings \u2013 \u201cglobal\u201d and \u201csingle\u201d.", "startOffset": 6, "endOffset": 27}, {"referenceID": 15, "context": "All the following results of MedLFRM and BayesMedLFRM are averages over 5 randomly initialized runs, again similar as in (Miller et al., 2009).", "startOffset": 121, "endOffset": 142}, {"referenceID": 15, "context": "To compare with LFRM, we use the same dataset as in (Miller et al., 2009), which contains 234 authors who had published with the most other people.", "startOffset": 52, "endOffset": 73}, {"referenceID": 15, "context": ", the posterior mean ofW is a symmetric matrix, as in (Miller et al., 2009).", "startOffset": 54, "endOffset": 75}, {"referenceID": 15, "context": "As in (Miller et al., 2009), we train the model on 80% of the data and use the remaining data for test.", "startOffset": 6, "endOffset": 27}, {"referenceID": 15, "context": "Table 2 shows the results, where the results of LFRM, IRM and MMSB were reported in (Miller et al., 2009).", "startOffset": 84, "endOffset": 105}], "year": 2012, "abstractText": "We present a max-margin nonparametric latent feature relational model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference.", "creator": "TeX"}}}