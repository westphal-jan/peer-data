{"id": "1206.4660", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Learning with Augmented Features for Heterogeneous Domain Adaptation", "abstract": "We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.", "histories": [["v1", "Mon, 18 Jun 2012 15:28:12 GMT  (146kb)", "http://arxiv.org/abs/1206.4660v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lixin duan", "dong xu", "ivor w tsang"], "accepted": true, "id": "1206.4660"}, "pdf": {"name": "1206.4660.pdf", "metadata": {"source": "CRF", "title": "Learning with Augmented Features for Heterogeneous Domain Adaptation", "authors": ["Lixin Duan", "Dong Xu", "Ivor W. Tsang"], "emails": ["S080003@ntu.edu.sg", "DongXu@ntu.edu.sg", "IvorTsang@ntu.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2. Kernel Learning for Heterogeneous Domain Adaptation", "text": "We define In as n \u00b7 n identity matrix and On \u00b7 m as n \u00b7 m matrix of all zeros. In addition, we define 0n, 1n \u0445 Rn as n \u00b7 1 column vectors of all zeros or all ones. The inequality a \u2264 b means ai \u2264 bi for i = 1,..., n. Furthermore, a \u0445 b denotes the elementary product between the vectors a and b, i.e. a \u0445 b = [a1b1,..., anbn] \u2032. And H 0 means that the matrix H is semi-definity.In this work, we assume that there is only one source domain and one target domain. For some given classes, we are provided with a series of labeled training samples {xsi, ysi \u2212 ns i = 1} from the source domain as well as a limited number of samples from the target domain (i = 1) and the target domain (i = 1)."}, {"heading": "2.1. Heterogeneous Feature Augmentation", "text": "Daume III (2007) proposed Feature Replication (FR) to expand the original Space Rd feature into a larger space Q R3d by replicating the source and target data for customization to homogeneous domains. Specifically, the feature mapping functions \u03c6s and \u03c6t for the source and target domains are defined as \u03c6s (x) = [x, \"x,\" 0d \"and \u03c6t (x) = [x,\" 0d, \"x\"] \u2032. Note that it does not make sense to use the method in (thumb \"III,\" 2007) directly for the HDA task by simply adding zeros to make the dimensions of the data from two domains equal, because there would be no correspondence between the heterogeneous properties in this case. To effectively use the heterogeneous properties from two domains, we first introduce a common space for the source and target data for our HDA task."}, {"heading": "2.2. Proposed Method", "text": "We define the weight vector w = [w \u2032 c, w \u2032 s, w \u2032 s, w \u2032 t = Q = Q > Q = Q = Q =. In addition, wc, ws and wt are also weight vectors defined for the common subspace, source domain and target domain. We then propose to learn the projection matrices P and Q as well as the weight vector w by functionally minimizing the structural risk of SVM. Formally, we present the formulation of our HFA method for the HDA problem as follows: dt P, Q min w, b, si, t i1, w 2 + C (ns)."}, {"heading": "2.3. Nonlinear Feature Transformation", "text": "Note that the size of the linear transformation measurement variable H is proportional to the feature dimension and is therefore not mathematically feasible for data with a very high dimension. In this section we will show that by applying kerelization the transformation measurement variable is independent of the feature dimension and only grows with the number of training data sets. As any feature mapping function can be used to derive a corresponding core space for the source and target data, we can replace its linear inner products with a core function k. Let us consider the number of proposed features (x s 1),"}, {"heading": "2.4. Detailed Solution", "text": "For our proposed HFA method, we develop an alternating optimization algorithm by iterative updating of \u03b1 and H to effectively solve the problem in (8). Specifically, when updating \u03b1 in (8), we repair by using the standard SVM with the kernel matrix KH. While we fix the SDP problem in (9) and solve it for H in (8) by SDP optimization, the optimization process is terminated when the value of the objective function converges in (8). To efficiently solve the SDP problem in (8), we also develop a simple projected gradient descent method to update H in (8). Let's define \u03b2s = [\u03b21,., \u03b2ns] and \u03b2t = [n] x x x. To efficiently solve the SDP problem in (9), we develop a simple projected gradient descent method to update H in (9), we define [1]."}, {"heading": "3. Related Work", "text": "The pioneering work (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) is limited to a few specific HDA tasks, as they require additional information to transfer the source knowledge to the target domain.To accomplish more general HDA tasks, other methods were proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011).Shi et al. (2010) suggested learning feature mapping matrices without using the valuable data label information.While Wang et al. (2011) used the class names of data, they assumed that the data should have a manifold structure; such manifold assumptions may not exist in the real world."}, {"heading": "4. Experiments", "text": "In this section, we evaluate our proposed HFA method for object recognition and multilingual text categorization. We focus on the problem of heterogeneous domain matching, where there is only one source domain and one target domain where only a limited number of marked target training examples are available. In addition, we assume that test data from the target domain will not be visible during the training phase."}, {"heading": "4.1. Setup", "text": "It is not as if it is a way of using the same protocols in the previous work (Kulis et al., 2011), in particular the SURF features (Bay et al., 2006) are extracted for all images. Images from Amazon and webcams are clustered into 800 visual words, representing the quantitative quantification of each image as an 800-dimensional histogram feature. Likewise, we represented each image from dslr as a 600-dimensional histogram feature. In the experiments dslr is used as the target domain."}, {"heading": "4.2. Classification Results", "text": "In fact, it is the case that most of them are not able to abide by the rules that they have imposed on themselves. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "4.3. Convergence Analysis", "text": "To analyze the convergence of the proposed algorithm 1 for our HFA method, we take a setting from each of the records as a storefront. We use the category \"Backpack\" and the source domain Amazon for the object record; for the multilingual Reuters record, the class \"C15\" is used together with the source domain English. Results show that algorithm 1 usually takes less than 80 (or 40) iterations before converging with the object record (or the multilingual Reuters record). We have similar observations for other categories / classes to the two records."}, {"heading": "5. Conclusions and Future Work", "text": "We have proposed a new method called Heterogeneous Feature Augmentation (HFA) for the adaptation of heterogeneous domains. In HFA, we extend the heterogeneous features from the source and target domains by using two newly proposed feature mapping functions each. In HFA, we propose to find the two projection matrices for the source and target data by using the standard SVM with hinged loss in both linear and non-linear cases. In addition, a so-called transformation metric is introduced into our formulated optimization problem of HFA so that it can be effectively solved by our developed alternating optimization algorithm. Promising results from HFA have been achieved on two benchmark data sets for object recognition and text classification."}, {"heading": "Acknowledgement", "text": "This work is supported by the Singapore National Research Foundation through its Interactive & Digital Media (IDM) Public Sector R & D Funding Initiative (managed by the IDM Programme Office) and AcRF Tier-1 Research Grant (RG15 / 08)."}], "references": [{"title": "Learning from multiple partially observed views \u2013 an application to multilingual text categorization", "author": ["M. Amini", "N. Usunier", "C. Goutte"], "venue": "In NIPS,", "citeRegEx": "Amini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2009}, {"title": "Surf: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "Gool", "L. Van"], "venue": "In ECCV,", "citeRegEx": "Bay et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bay et al\\.", "year": 2006}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In EMNLP,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In ACL,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Convex Optimization", "author": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Translated learning: Transfer learning across different feature spaces", "author": ["W. Dai", "Y. Chen", "Xue", "G.-R", "Q. Yang", "Y. Yu"], "venue": "In NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2009}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum\u00e9"], "venue": "In ACL,", "citeRegEx": "Daum\u00e9,? \\Q2007\\E", "shortCiteRegEx": "Daum\u00e9", "year": 2007}, {"title": "Visual event recognition in videos by learning from web data", "author": ["L. Duan", "D. Xu", "I.W. Tsang", "J. Luo"], "venue": "In CVPR,", "citeRegEx": "Duan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2010}, {"title": "Domain transfer multiple kernel learning", "author": ["L. Duan", "I.W. Tsang", "D. Xu"], "venue": "T-PAMI,", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Domain adaptation from multiple sources: A domain-dependent regularization approach. T-NNLS", "author": ["L. Duan", "D. Xu", "I.W. Tsang"], "venue": null, "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Learning from multiple outlooks", "author": ["M. Harel", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Harel and Mannor,? \\Q2011\\E", "shortCiteRegEx": "Harel and Mannor", "year": 2011}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Cross-domain sentiment classification via spectral feature alignment", "author": ["S.J. Pan", "X. Ni", "Sun", "J.-T", "Q. Yang", "Z. Chen"], "venue": "In WWW,", "citeRegEx": "Pan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Cross-language text classification using structural correspondence learning", "author": ["P. Prettenhofer", "B. Stein"], "venue": "In ACL,", "citeRegEx": "Prettenhofer and Stein,? \\Q2010\\E", "shortCiteRegEx": "Prettenhofer and Stein", "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In ECCV,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Transfer learning on heterogenous feature spaces via spectral transformation", "author": ["X. Shi", "Q. Liu", "W. Fan", "P.S. Yu", "R. Zhu"], "venue": "In ICDM,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "Heterogeneous domain adaptation using manifold alignment", "author": ["C. Wang", "S. Mahadevan"], "venue": "In IJCAI,", "citeRegEx": "Wang and Mahadevan,? \\Q2011\\E", "shortCiteRegEx": "Wang and Mahadevan", "year": 2011}, {"title": "Cross-lingual adaptation: An experiment on sentiment classifications", "author": ["B. Wei", "C. Pal"], "venue": "In ACL,", "citeRegEx": "Wei and Pal,? \\Q2010\\E", "shortCiteRegEx": "Wei and Pal", "year": 2010}, {"title": "Improving svm accuracy by training on auxiliary data sources", "author": ["P. Wu", "T.G. Dietterich"], "venue": "In ICML,", "citeRegEx": "Wu and Dietterich,? \\Q2004\\E", "shortCiteRegEx": "Wu and Dietterich", "year": 2004}, {"title": "Heterogeneous transfer learning for image clustering via the social web", "author": ["Q. Yang", "Y. Chen", "Xue", "G.-R", "W. Dai", "Y. Yu"], "venue": "ACL/IJCNLP,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Heterogeneous transfer learning for image classification", "author": ["Y. Zhu", "Y. Chen", "Z. Lu", "S.J. Pan", "Xue", "G.-R", "Y. Yu", "Q. Yang"], "venue": "In AAAI,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Domain adaptation methods have been successfully used for different research fields such as natural language processing and computer vision (Blitzer et al., 2006; 2007; Daum\u00e9 III, 2007; Duan et al., 2010; 2012b;a; Wu & Dietterich, 2004).", "startOffset": 140, "endOffset": 236}, {"referenceID": 7, "context": "Domain adaptation methods have been successfully used for different research fields such as natural language processing and computer vision (Blitzer et al., 2006; 2007; Daum\u00e9 III, 2007; Duan et al., 2010; 2012b;a; Wu & Dietterich, 2004).", "startOffset": 140, "endOffset": 236}, {"referenceID": 5, "context": "Thus, they cannot deal with the problem where the dimensions of data from the source and target domains are different, which is known as heterogeneous domain adaptation (HDA) (Dai et al., 2009; Yang et al., 2009).", "startOffset": 175, "endOffset": 212}, {"referenceID": 20, "context": "Thus, they cannot deal with the problem where the dimensions of data from the source and target domains are different, which is known as heterogeneous domain adaptation (HDA) (Dai et al., 2009; Yang et al., 2009).", "startOffset": 175, "endOffset": 212}, {"referenceID": 20, "context": "The same assumption was also used in (Yang et al., 2009; Zhu et al., 2011) for text-aid image clustering and classification.", "startOffset": 37, "endOffset": 74}, {"referenceID": 21, "context": "The same assumption was also used in (Yang et al., 2009; Zhu et al., 2011) for text-aid image clustering and classification.", "startOffset": 37, "endOffset": 74}, {"referenceID": 2, "context": "Based on structural correspondence learning (Blitzer et al., 2006), two methods (Prettenhofer & Stein, 2010; Wei & Pal, 2010) were recently proposed to extract the so-called pivot features from the source and target domains, which is specifically designed for the cross-language text classification task.", "startOffset": 44, "endOffset": 66}, {"referenceID": 3, "context": "Dai et al. (2009) proposed to learn a feature translator between the source and target domains by assuming that the data from both domains share co-occurrence attributes (i.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "For more general HDA tasks, Shi et al. (2010) proposed a method called Heterogeneous Spectral Mapping", "startOffset": 28, "endOffset": 46}, {"referenceID": 14, "context": "By kernelizing the method in (Saenko et al., 2010), Kulis et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "Harel and Mannor (2011) learned rotation matrices to match source data distributions to that of the target domain.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Harel and Mannor (2011) learned rotation matrices to match source data distributions to that of the target domain. However, this method does not use the valuable training labels, either. Wang et al. (2011) used the class labels of the training data to learn the manifold alignment by simultaneously maximizing the intradomain similarity and the inter-domain dissimilarity.", "startOffset": 0, "endOffset": 206}, {"referenceID": 10, "context": "Harel and Mannor (2011) learned rotation matrices to match source data distributions to that of the target domain. However, this method does not use the valuable training labels, either. Wang et al. (2011) used the class labels of the training data to learn the manifold alignment by simultaneously maximizing the intradomain similarity and the inter-domain dissimilarity. By kernelizing the method in (Saenko et al., 2010), Kulis et al. (2011) proposed to learn an asymmetric kernel transformation to transfer feature knowledge between the data from the source and target domains.", "startOffset": 0, "endOffset": 445}, {"referenceID": 12, "context": "Note that promising results have been shown by incorporating original features into feature augmentation (Daum\u00e9 III, 2007; Pan et al., 2010) to enhance the similarities between data from the same domain.", "startOffset": 105, "endOffset": 140}, {"referenceID": 12, "context": "Motivated by (Daum\u00e9 III, 2007; Pan et al., 2010), we also incorporate original features in this work and then augment any source and target domain samples x \u2208 Rs and x \u2208 Rt by using our newly proposed", "startOffset": 13, "endOffset": 48}, {"referenceID": 11, "context": "In order to effectively deal with high dimensional data, inspired by (Kulis et al., 2011), in the next subsection we will apply kernelization to the data from the source and target domains and show that (7) can be solved in a kernel space by learning a nonlinear transformation metric with its size independent of the feature dimension.", "startOffset": 69, "endOffset": 89}, {"referenceID": 11, "context": "1 in (Kulis et al., 2011).", "startOffset": 5, "endOffset": 25}, {"referenceID": 5, "context": "The pioneer works (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.", "startOffset": 18, "endOffset": 118}, {"referenceID": 20, "context": "The pioneer works (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.", "startOffset": 18, "endOffset": 118}, {"referenceID": 21, "context": "The pioneer works (Dai et al., 2009; Prettenhofer & Stein, 2010; Wei & Pal, 2010; Yang et al., 2009; Zhu et al., 2011) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.", "startOffset": 18, "endOffset": 118}, {"referenceID": 16, "context": "To handle more general HDA tasks, other methods have been proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011).", "startOffset": 108, "endOffset": 150}, {"referenceID": 16, "context": "To handle more general HDA tasks, other methods have been proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011). Shi et al. (2010) proposed to learn feature mapping matrices without using the valuable data label information.", "startOffset": 109, "endOffset": 170}, {"referenceID": 16, "context": "To handle more general HDA tasks, other methods have been proposed to explicitly discover a common subspace (Shi et al., 2010; Wang & Mahadevan, 2011). Shi et al. (2010) proposed to learn feature mapping matrices without using the valuable data label information. While Wang et al. (2011) used the class labels of data, they assumed the data should have a manifold structure.", "startOffset": 109, "endOffset": 289}, {"referenceID": 11, "context": "Recently, Kulis et al. (2011) proposed a nonlinear metric learning method to learn an asymmetric feature transformation for the source and target data with high dimensions.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "Object recognition: We employ a recently released dataset used in (Saenko et al., 2010; Kulis et al., 2011) for this task.", "startOffset": 66, "endOffset": 107}, {"referenceID": 11, "context": "Object recognition: We employ a recently released dataset used in (Saenko et al., 2010; Kulis et al., 2011) for this task.", "startOffset": 66, "endOffset": 107}, {"referenceID": 11, "context": "We follow the same protocols in the previous work (Kulis et al., 2011).", "startOffset": 50, "endOffset": 70}, {"referenceID": 1, "context": "Specifically, SURF features (Bay et al., 2006) are extracted for all the images.", "startOffset": 28, "endOffset": 46}, {"referenceID": 14, "context": "We strictly follow the setting in (Saenko et al., 2010; Kulis et al., 2011) and randomly select 20 (resp.", "startOffset": 34, "endOffset": 75}, {"referenceID": 11, "context": "We strictly follow the setting in (Saenko et al., 2010; Kulis et al., 2011) and randomly select 20 (resp.", "startOffset": 34, "endOffset": 75}, {"referenceID": 0, "context": "Text categorization: We use the Reuters multilingual dataset (Amini et al., 2009), which is collected by sampling parts of the Reuters RCV1 and RCV2 collections.", "startOffset": 61, "endOffset": 81}, {"referenceID": 11, "context": "Note it is not reported in (Kulis et al., 2011).", "startOffset": 27, "endOffset": 47}, {"referenceID": 11, "context": "Following (Kulis et al., 2011), we also report its results in this work.", "startOffset": 10, "endOffset": 30}, {"referenceID": 16, "context": "\u2022 HeMap (Shi et al., 2010): It finds the projection matrices for a common feature subspace as well as learns the optimal projected data from both domains.", "startOffset": 8, "endOffset": 26}, {"referenceID": 11, "context": "\u2022 ARC-t (Kulis et al., 2011): It uses the labeled training data from both domains to learn an asymmetric transformation metric between the different feature spaces.", "startOffset": 8, "endOffset": 28}, {"referenceID": 11, "context": "Evaluation metric: Following (Kulis et al., 2011), for each method we measure the classification accuracy over all categories/classes on both datasets.", "startOffset": 29, "endOffset": 49}, {"referenceID": 14, "context": "Object recognition: We report the mean and standard deviations of classification accuracies for all methods on the object dataset (Saenko et al., 2010) in Table 3.", "startOffset": 130, "endOffset": 151}, {"referenceID": 11, "context": "For KCCA and ARC-t, the numbers in the parentheses are the results reported in (Kulis et al., 2011).", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": "Both results of ARC-t implemented by ourselves and reported in (Kulis et al., 2011) are only comparable with those of SVM T, which shows that ARC-t is less effective for HDA on this dataset.", "startOffset": 63, "endOffset": 83}, {"referenceID": 0, "context": "Text categorization: Table 4 shows the mean and standard deviations of classification accuracies for all methods on the Reuters multilingual dataset (Amini et al., 2009) by using m = 20 labeled training samples per class from the target domain.", "startOffset": 149, "endOffset": 169}], "year": 2012, "abstractText": "We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.", "creator": " TeX output 2012.05.20:1505"}}}