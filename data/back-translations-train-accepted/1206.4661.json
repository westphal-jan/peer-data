{"id": "1206.4661", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Predicting accurate probabilities with a ranking loss", "abstract": "In many real-world applications of machine learning classifiers, it is essential to predict the probability of an example belonging to a particular class. This paper proposes a simple technique for predicting probabilities based on optimizing a ranking loss, followed by isotonic regression. This semi-parametric technique offers both good ranking and regression performance, and models a richer set of probability distributions than statistical workhorses such as logistic regression. We provide experimental results that show the effectiveness of this technique on real-world applications of probability prediction.", "histories": [["v1", "Mon, 18 Jun 2012 15:30:13 GMT  (218kb)", "http://arxiv.org/abs/1206.4661v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["aditya krishna menon", "xiaoqian jiang", "shankar vembu", "charles elkan", "lucila ohno-machado"], "accepted": true, "id": "1206.4661"}, "pdf": {"name": "1206.4661.pdf", "metadata": {"source": "META", "title": "Predicting accurate probabilities with a ranking loss", "authors": ["Aditya Krishna Menon", "Xiaoqian Jiang", "Shankar Vembu", "Charles Elkan", "Lucila Ohno-Machado"], "emails": ["AKMENON@UCSD.EDU", "XLJIANG@UCSD.EDU", "SHANKAR.VEMBU@UTORONTO.CA", "ELKAN@UCSD.EDU", "MACHADO@UCSD.EDU"], "sections": [{"heading": "1. Introduction", "text": "It is the problem of learning examples of labels, with the aim of categorizing future examples into one of several classes. However, many real-world applications require us to estimate the likelihood of an example with a particular label. For example, when studying the clicking behavior of ads in computational advertising, it is essential to model the likelihood of a click rather than just predicting whether it will be clicked (Richardson et al., 2007). Accurate probabilities are also essential for early assessment and admission to an ICU (Subbe et al., 2001)."}, {"heading": "2. Background and motivation", "text": "In classical terms, the reviewed learning literature is focused on the scenario in which we want to minimize the number of misclassified examples on test data. (...) Practical applications of machine learning models often have more complex limitations and requirements that require us to label the likelihood of an example. (...) Examples of such applications include: Building Meta-Classifiers, in which the results of a model are fed to a meta-classifier who uses additional domain knowledge to make a prediction. (...) Physicians prefer to use a classifier's prediction as evidence for their own decision-making processes (Manickam & Abidi, 1999). In such scenarios, it is essential that the classifier assess the confidence in his predictions as correct, which can be measured on the basis of probabilities; using predictions to take action, such as deciding whether or not to contact a person for a marketing campaign."}, {"heading": "3. Analysis of existing paradigms to learn accurate probabilities", "text": "We are now analyzing two important paradigms for probability estimation and examining their possible error modes."}, {"heading": "3.1. Optimization of a proper loss", "text": "A direct approach to predicting probabilities is to optimize a correct loss function on the training data based on a class of hypotheses, e.g. linear delimiters. Examples are logistic regression and linear regression (abbreviated to [0,1]), which are examples of the general linear model framework, where E [y | x] = f (wT x) is assumed for a linkage function f (\u00b7). The loss-dependent error magnitude L '(\u03b7, s) is a metric that allows us to choose between correct losses. For example, the discrepancy dimensions for square and logistic losses (Zhang, 2004) are Lsquare (\u03b7, s) = (\u03b7 \u2212 s) 2 + C1 (2) Llogistic (\u03b7, s) = KL (\u03b7, s) = 11 + e \u2212 s) + C2 (3), where KL denotes the kullback-Leibler divergence and C2 is independent of the prediction."}, {"heading": "3.2. Post-processing methods", "text": "Three popular techniques of this kind are flat scaling (Platt, 1999), binning (Zadrozny & Elkan, 2001), and isotonic regression (Zadrozny & Elkan, 2002).We focus on the latter because it is more flexible than the previous two approaches due to its non-parametric nature and has been shown to work empirically well for a number of input models (Niculescu-Mizil & Caruana, 2005).Isotonic regression is a non-parametric technique used to find a monotone that matches a set of target values. In a learning context, the method was used in (Zadrozny & Elkan, 2002) to learn meaningful probabilities from the results of an input model."}, {"heading": "3.3. Possible failure modes", "text": "There are at least two main reasons why the above paradigms do not provide exact probabilities: miscalculations of a maximum number of models based on parametric assumptions. In practice, simple models based on parametric assumptions are often incorrectly specified: for example, logistic regression assumes the parametric form \u03b7 (x) = 1 / (1 + e \u2212 wT x) for some w, but this assumption may not always be correct. While we cannot learn that logistic regression is flawed if we cannot represent it in our hypothesis class, Equation 1 says that the predictions of our model in expectation are close to \u03b7 (x) according to some discrepancy number measurements. It is possible that a model such as logistic regression can be flawed."}, {"heading": "4. Extracting probabilities from a ranker", "text": "For this reason, we will focus on this semi-parametric paradigm below. Our hope is to design a model that is at least as precise and not much more difficult to train than workhorses such as logistic regression. In order to obtain accurate estimates from isotonic regression, we need to specify what values we will feed in as input. We can therefore ask what characteristics such values should have in order to provide accurate probability estimations.We make the simple observation that isotonic regression interacts with the results of the input model only in one way: it uses them to enforce the monotonicity constraint on the output. Thus, isotonic regression will intuitively perform well if the (pictorial) order of the original values is good, and this should be our goal when we intuitively try to alienate this model."}, {"heading": "4.1. Isotonic regression and ranking performance", "text": "The real value that a model assigns to each example can be used to evaluate examples according to the reliability of a positive example (in fact).The pair order of precedence of a model can be measured by the range below the ROC curve (AUC), where the probability that a randomly drawn positive example has a higher value than a randomly drawn negative example. It is formally defined below. Definition 1. (Cle \u00b2 menc \u00b2 s \u00b2 s \u00b2 on et al., 2006) The AUC A (s \u00b2) of a model will have a higher value than a randomly drawn negative example. Definition 1. (Cle \u00b2 menc \u00b2 s \u00b2 on et al., 2006) The AUC (s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s) of a model. From now on, we think of a model that we represent as an equal ranker of examples. A natural quantity for investigation is the model that induces the Bayes optimal ranker."}, {"heading": "4.2. Our proposal: ranking loss + isotonic regression", "text": "The above examples suggest a natural idea: the direct optimization of the AUC on the training set and the post-processing of its results with isotonic regression. This can be seen as learning a model that performs well in ranking (due to the initial optimization of a ranking loss) as well as a good probability estimation performance (due to the isotonic regression that optimizes any proper loss).Appropriate handling of isotonic regression forces strict monotonicity, and thus its results will have the same AUC as the original model. On a finite training set with n + positive and n \u2212 negative examples, the empirical AUC Aemp can be calculated as logical regression."}, {"heading": "4.3. Justification of model", "text": "To argue that this model is learning something useful, we need to show two things: (a) the solution to the convex optimization problem of Equation 5 will (asymptotically), however, yield a Bayes-optimal ranking, provided the model is correctly specified, and (b) the isotonic regression on top of a Bayes-optimal ranking will recover (x). Point (a) can be determined if the underlying classification model uses a universal kernel (Cle) menc that the model is based on et al., 2006). For a linear kernel, this means that we can learn the optimal ranking if the underlying probability of the form c (wT x) is for some monotonous increasing c (\u00b7). Point (b) was specified in Section 4.1, and it is also the case that the isotonic regression estimation on a finite is consistent."}, {"heading": "4.4. Comparison to existing methods", "text": "The first step of our method is to maximize the paired rankings, and the isotonic regression steps lead to lower rankings (Richardson et al.) The idea of learning with good rankings and regressions was proposed in Sculley (2010)."}, {"heading": "5. Experimental results", "text": "Our experiments aim to investigate the conditions under which our method can improve performance against linear or logistic regression on both synthetic and real datasets."}, {"heading": "5.1. Methods compared", "text": "For comparison, we used the Combined Regression and Ranking Model (CRR) from Sculley (2010). We do not use CRR because this framework was explicitly designed to provide both a good ranking and a regression that we would like to compare with our approach; our hypothesis is that our method should provide the most accurate probabilities, while also providing the CRR model with an equitable ranking. Following Sculley (2010), we use the paired ranking framework (Herbrich et al., 2000; Joachims, 2002) with logistical losses to optimize AUC directly, which is naturally suitable for large-scale implementation using stochastic gradient descent. For this and the CRR model, we used the Sofia-ML-3 package to determine the benefits between the available models."}, {"heading": "5.2. Results on synthetic dataset", "text": "We will first examine the performance of our proposed method using a synthetic dataset to see the conditions under which we can expect it to improve performance over existing methods. In particular, we will examine the performance of various methods 3http: / / code.google.com / p / sofia-ml /, where the true probability model is Pr [y = 1 | x; w] = a1 [wT x < 0] + (1 \u2212 a) 1 [wT x \u2265 0], where 0 \u2264 a \u2264 12 controls the ground and the upper limit of the probability distribution. Such covered distributions result, for example, in the item response theory (Hambleton et al., 1991), where the probability of a student correctly answering a question is limited from below by the success rate of random guessing. Logistic regression is incorrectly specified for this relationship, although for a = 0 the sigmoid represents an appropriate approximation of capabilities, while for a = 12 the probability is independent of x and thus a number of points can be fully modelled by several methods."}, {"heading": "5.3. Results on real-world datasets", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6. Conclusion and future work", "text": "We investigated the principles behind predicting accurate probabilities and proposed a simple method to achieve them. Our method is based on the post-processing of the results of a model that optimizes a ranking loss through isotonic regression. It turns out that the model performs well empirically. In the future, it would be interesting to study the theoretical properties of the model more closely and to evaluate the model in other scenarios that require probability estimates."}, {"heading": "Acknowledgements", "text": "XJ and LOM were partly funded by the National Library of Medicine (R01LM009520) and NHLBI (U54 HL10846).5http: / / vikas.sindhwani.org / svmlin.html"}], "references": [{"title": "Generalization Bounds for the Area Under the ROC Curve", "author": ["S. Agarwal", "T. Graepel", "R. Herbrich", "S. Har-Peled", "D. Roth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Agarwal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2005}, {"title": "Statistical Inference under Order Restrictions: The Theory and Application of Isotonic Regression", "author": ["R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk"], "venue": null, "citeRegEx": "Barlow et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Barlow et al\\.", "year": 1972}, {"title": "The PAV-Algorithm optimized binary proper scoring", "author": ["N. Br\u00fcmmer", "J.D. Preez"], "venue": null, "citeRegEx": "Br\u00fcmmer and Preez,? \\Q2007\\E", "shortCiteRegEx": "Br\u00fcmmer and Preez", "year": 2007}, {"title": "On the Estimation of Parameters Restricted by Inequalities", "author": ["H.D. Brunk"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Brunk,? \\Q1958\\E", "shortCiteRegEx": "Brunk", "year": 1958}, {"title": "Loss Functions for Binary Class Probability Estimation: Structure and Applications", "author": ["A. Buja", "W. Stuetzle", "Y. Shen"], "venue": "Technical report, University of Pennsylvania,", "citeRegEx": "Buja et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2005}, {"title": "Ranking and empirical minimization of U-statistics", "author": ["S. Cl\u00e9men\u00e7on", "G. Lugosi", "N. Vayatis"], "venue": "The Annals of Statistics,", "citeRegEx": "Cl\u00e9men\u00e7on et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on et al\\.", "year": 2006}, {"title": "Bias Correction in Generalized Linear Models", "author": ["G.M. Cordeiro", "P. McCullagh"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Cordeiro and McCullagh,? \\Q1991\\E", "shortCiteRegEx": "Cordeiro and McCullagh", "year": 1991}, {"title": "Distribution-Free Maximum Likelihood Estimator of the Binary Choice", "author": ["S.R. Cosslett"], "venue": "Model. Econometrica,", "citeRegEx": "Cosslett,? \\Q1983\\E", "shortCiteRegEx": "Cosslett", "year": 1983}, {"title": "The effect of link misspecification on binary regression inference", "author": ["C. Czado", "T.J. Santner"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Czado and Santner,? \\Q1992\\E", "shortCiteRegEx": "Czado and Santner", "year": 1992}, {"title": "Incidence and predictors of microbiology results returning post-discharge and requiring follow-up", "author": ["R. El-Kareh", "C. Roy", "G. Brodsky", "M. Perencevich", "E.G. Poon"], "venue": "Journal of Hospital Medicine,", "citeRegEx": "El.Kareh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "El.Kareh et al\\.", "year": 2010}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["C. Elkan", "K. Noto"], "venue": "In KDD, pp", "citeRegEx": "Elkan and Noto,? \\Q2008\\E", "shortCiteRegEx": "Elkan and Noto", "year": 2008}, {"title": "Bias Reduction of Maximum Likelihood Estimates", "author": ["D. Firth"], "venue": "Biometrika, 80(1):27\u201338,", "citeRegEx": "Firth,? \\Q1993\\E", "shortCiteRegEx": "Firth", "year": 1993}, {"title": "A simple lexicographic ranker and probability estimator", "author": ["P. Flach", "E. Matsubara"], "venue": "In ECML, pp", "citeRegEx": "Flach and Matsubara,? \\Q2007\\E", "shortCiteRegEx": "Flach and Matsubara", "year": 2007}, {"title": "Variable Selection in Data Mining", "author": ["D.P. Foster", "R.A. Stine"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Foster and Stine,? \\Q2004\\E", "shortCiteRegEx": "Foster and Stine", "year": 2004}, {"title": "Fundamentals of Item Response Theory (Measurement Methods for the Social Science)", "author": ["R.K. Hambleton", "H. Swaminathan", "H.J. Rogers"], "venue": "Sage Publications,", "citeRegEx": "Hambleton et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Hambleton et al\\.", "year": 1991}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Herbrich et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2000}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In KDD, pp", "citeRegEx": "Joachims,? \\Q2002\\E", "shortCiteRegEx": "Joachims", "year": 2002}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In KDD, pp", "citeRegEx": "Joachims,? \\Q2006\\E", "shortCiteRegEx": "Joachims", "year": 2006}, {"title": "The Isotron Algorithm: HighDimensional Isotonic Regression", "author": ["A. Kalai", "R. Sastry"], "venue": "In COLT, pp", "citeRegEx": "Kalai and Sastry,? \\Q2009\\E", "shortCiteRegEx": "Kalai and Sastry", "year": 2009}, {"title": "Logistic Regression in Rare Events Data", "author": ["G. King", "L. Zeng"], "venue": "Political Analysis,", "citeRegEx": "King and Zeng,? \\Q2001\\E", "shortCiteRegEx": "King and Zeng", "year": 2001}, {"title": "Bipartite Ranking through Minimization of Univariate Loss", "author": ["W. Kotlowski", "K. Dembczynski", "E. H\u00fcllermeier"], "venue": "In ICML, pp", "citeRegEx": "Kotlowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kotlowski et al\\.", "year": 2011}, {"title": "Experienced Based Medical Diagnostics System Over The World Wide Web (WWW)", "author": ["S. Manickam", "S.S.R. Abidi"], "venue": "In AIAI,", "citeRegEx": "Manickam and Abidi,? \\Q1999\\E", "shortCiteRegEx": "Manickam and Abidi", "year": 1999}, {"title": "Maximum score estimation of the stochastic utility model of choice", "author": ["C.F. Manski"], "venue": "Journal of Econometrics,", "citeRegEx": "Manski,? \\Q1975\\E", "shortCiteRegEx": "Manski", "year": 1975}, {"title": "Predicting good probabilities with supervised learning", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "In ICML, pp", "citeRegEx": "Niculescu.Mizil and Caruana,? \\Q2005\\E", "shortCiteRegEx": "Niculescu.Mizil and Caruana", "year": 2005}, {"title": "Probabilistic Output for Support Vector Machines and Comarisons to Regularized Likelihood Methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Predicting clicks: Estimating the Click-Through Rate for New Ads", "author": ["M. Richardson", "E. Dominowska", "R. Ragno"], "venue": "In WWW,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "On Equivalence Relationships Between Classification and Ranking Algorithms", "author": ["C. Rudin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin,? \\Q2011\\E", "shortCiteRegEx": "Rudin", "year": 2011}, {"title": "A General Method for Comparing Probability Assessors", "author": ["M.J. Schervish"], "venue": "Annals of Statistics,", "citeRegEx": "Schervish,? \\Q1989\\E", "shortCiteRegEx": "Schervish", "year": 1989}, {"title": "Large Scale Learning to Rank", "author": ["D. Sculley"], "venue": "In NIPS Workshop on Advances in Ranking,", "citeRegEx": "Sculley,? \\Q2009\\E", "shortCiteRegEx": "Sculley", "year": 2009}, {"title": "Combined regression and ranking", "author": ["D. Sculley"], "venue": "In KDD,", "citeRegEx": "Sculley,? \\Q2010\\E", "shortCiteRegEx": "Sculley", "year": 2010}, {"title": "Detecting adversarial advertisements in the wild", "author": ["D. Sculley", "M.E. Otey", "M. Pohl", "B. Spitznagel", "J. Hainsworth", "Y. Zhou"], "venue": "In KDD, pp", "citeRegEx": "Sculley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sculley et al\\.", "year": 2011}, {"title": "Validation of a modified Early Warning Score in medical admissions", "author": ["C.P. Subbe", "M. Kruger", "P. Rutherford", "L. Gemmel"], "venue": "QJM: An International Journal of Medicine,", "citeRegEx": "Subbe et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Subbe et al\\.", "year": 2001}, {"title": "Learning and making decisions when costs and probabilities are both unknown", "author": ["B. Zadrozny", "C. Elkan"], "venue": "In KDD,", "citeRegEx": "Zadrozny and Elkan,? \\Q2001\\E", "shortCiteRegEx": "Zadrozny and Elkan", "year": 2001}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "In KDD,", "citeRegEx": "Zadrozny and Elkan,? \\Q2002\\E", "shortCiteRegEx": "Zadrozny and Elkan", "year": 2002}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["T. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Kernel Logistic Regression and the Import Vector Machine", "author": ["J. Zhu", "T. Hastie"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Zhu and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zhu and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "For example, when studying the click behaviour of ads in computational advertising, it is essential to model the probability of an ad being clicked, rather than just predicting whether or not it will be clicked (Richardson et al., 2007).", "startOffset": 211, "endOffset": 236}, {"referenceID": 31, "context": "Accurate probabilities are also essential for medical screening tools to trigger early assessment and admission to an ICU (Subbe et al., 2001).", "startOffset": 122, "endOffset": 142}, {"referenceID": 29, "context": "The model attempts to achieve good ranking (in an area under ROC sense) and regression (in a squared error sense) performance simultaneously, which is important in many real-world applications (Sculley, 2010).", "startOffset": 193, "endOffset": 208}, {"referenceID": 4, "context": "Then, we call a loss function ` Bayes consistent (Buja et al., 2005) if for every \u03b7 \u2208 [0,1], s\u2217(\u03b7) \u00b7 (\u03b7 \u2212 1 2 ) \u2265 0, meaning that we have the same sign as the optimal prediction under the 0-1 loss `(y, \u015d) = 1[y\u015d \u2264 0].", "startOffset": 49, "endOffset": 68}, {"referenceID": 4, "context": "In such cases, we call the corresponding probabilistic loss `P a proper (or Fisherconsistent) loss (Buja et al., 2005), and say that ` corresponds to a proper loss.", "startOffset": 99, "endOffset": 118}, {"referenceID": 24, "context": "The hinge loss of SVMs, `(y, \u015d) = max(0,1\u2212 (2y\u22121)\u015d), is Bayes consistent but does not correspond to a proper loss function, which is why SVMs do not output meaningful probabilities (Platt, 1999).", "startOffset": 181, "endOffset": 194}, {"referenceID": 34, "context": "For example, the discrepancy measures for square and logistic loss are (Zhang, 2004)", "startOffset": 71, "endOffset": 84}, {"referenceID": 34, "context": "Based on this, Zhang (2004) notes that logistic regression has difficulty when \u03b7(x)(1\u2212\u03b7(x)) \u2248 0 for some x, by virtue of requiring |\u015d(x)| \u2192 \u221e.", "startOffset": 15, "endOffset": 28}, {"referenceID": 24, "context": "Three popular techniques of this type are Platt scaling (Platt, 1999), binning (Zadrozny & Elkan, 2001), and isotonic regression (Zadrozny & Elkan, 2002).", "startOffset": 56, "endOffset": 69}, {"referenceID": 1, "context": ") If the input scores {\u015di} are sorted, then there is an O(n) algorithm to solve this problem, called pool adjacent violators (PAV) (Barlow et al., 1972).", "startOffset": 131, "endOffset": 152}, {"referenceID": 7, "context": "One natural scheme is a linear interpolation between the training scores (Cosslett, 1983).", "startOffset": 73, "endOffset": 89}, {"referenceID": 34, "context": "This model will be able to learn any measurable \u03b7(x) with a universal kernel (Zhang, 2004).", "startOffset": 77, "endOffset": 90}, {"referenceID": 11, "context": "It is possible to perform bias correction explicitly via a post-hoc modification of the learned parameters (King & Zeng, 2001), or implicitly by choosing a Jeffrey\u2019s prior regularizer (Firth, 1993).", "startOffset": 184, "endOffset": 197}, {"referenceID": 18, "context": "King and Zeng (2001) show that the constant in the O(\u00b7) depends on the imbalance in the classes, meaning that logistic regression can give biased probability estimates when attempting to model a rare event.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "(Cl\u00e9men\u00e7on et al., 2006) The AUC A (\u015d(\u00b7)) of a model \u015d : X \u2192 R is", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "Intuitively, we expect this optimal ranker to be \u03b7(x), or some (strictly) monotone transform c(\u00b7) thereof, and indeed this may be proven (Cl\u00e9men\u00e7on et al., 2006).", "startOffset": 137, "endOffset": 161}, {"referenceID": 27, "context": "(Schervish, 1989) We say that a model \u015d is calibrated if, for every \u03b1 \u2208 \u015d[X ], \u03b1 = Pr[y = 1|\u015d = \u03b1].", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "To maximize AUC, we may follow the pairwise ranking framework (Herbrich et al., 2000; Joachims, 2002), which", "startOffset": 62, "endOffset": 101}, {"referenceID": 16, "context": "To maximize AUC, we may follow the pairwise ranking framework (Herbrich et al., 2000; Joachims, 2002), which", "startOffset": 62, "endOffset": 101}, {"referenceID": 17, "context": "While the above loss function nominally requires O(n2) time to compute the gradient, clever algorithms can speed this up (Joachims, 2006).", "startOffset": 121, "endOffset": 137}, {"referenceID": 28, "context": "Empirically, it has been observed that stochastic gradient descent on the objective converges in a fraction of an epoch (Sculley, 2009).", "startOffset": 120, "endOffset": 135}, {"referenceID": 20, "context": "For example, Kotlowski et al. (2011) show that the ranking error (viz.", "startOffset": 13, "endOffset": 37}, {"referenceID": 5, "context": "Point (a) can be established if the underlying classification model uses a universal kernel (Cl\u00e9men\u00e7on et al., 2006).", "startOffset": 92, "endOffset": 116}, {"referenceID": 3, "context": "1, and it is further the case that the isotonic regression estimate on a finite training set is consistent, under mild regularity assumptions (Brunk, 1958).", "startOffset": 142, "endOffset": 155}, {"referenceID": 0, "context": "Since the empirical AUC is concentrated around the true AUC (Agarwal et al., 2005), the above is easily extended to a bound in terms of the true AUC.", "startOffset": 60, "endOffset": 82}, {"referenceID": 25, "context": "Good performance in both metrics is important in many applications, such as computational advertising (Richardson et al., 2007).", "startOffset": 102, "endOffset": 127}, {"referenceID": 25, "context": "Good performance in both metrics is important in many applications, such as computational advertising (Richardson et al., 2007). The idea of learning models with good ranking and regression performance was proposed in the combined regression and ranking (CRR) framework of Sculley (2010). A similar model for logistic loss was proposed by Ertekin and Rudin (2011).", "startOffset": 103, "endOffset": 288}, {"referenceID": 25, "context": "Good performance in both metrics is important in many applications, such as computational advertising (Richardson et al., 2007). The idea of learning models with good ranking and regression performance was proposed in the combined regression and ranking (CRR) framework of Sculley (2010). A similar model for logistic loss was proposed by Ertekin and Rudin (2011). The basic idea of such an approach is to simultaneously optimize the ranking and regression losses in a parametric manner, by minimizing a linear combination of both losses.", "startOffset": 103, "endOffset": 364}, {"referenceID": 25, "context": "Good performance in both metrics is important in many applications, such as computational advertising (Richardson et al., 2007). The idea of learning models with good ranking and regression performance was proposed in the combined regression and ranking (CRR) framework of Sculley (2010). A similar model for logistic loss was proposed by Ertekin and Rudin (2011). The basic idea of such an approach is to simultaneously optimize the ranking and regression losses in a parametric manner, by minimizing a linear combination of both losses. The hope is that this yields \u201cbest of both worlds\u201d performance in these objectives. Empirically, Sculley (2010) observed that generally the AUC obtained from such an approach was no worse than that of optimizing the ranking loss alone, while in some cases there was an improvement in the regression performance.", "startOffset": 103, "endOffset": 651}, {"referenceID": 25, "context": "Good performance in both metrics is important in many applications, such as computational advertising (Richardson et al., 2007). The idea of learning models with good ranking and regression performance was proposed in the combined regression and ranking (CRR) framework of Sculley (2010). A similar model for logistic loss was proposed by Ertekin and Rudin (2011). The basic idea of such an approach is to simultaneously optimize the ranking and regression losses in a parametric manner, by minimizing a linear combination of both losses. The hope is that this yields \u201cbest of both worlds\u201d performance in these objectives. Empirically, Sculley (2010) observed that generally the AUC obtained from such an approach was no worse than that of optimizing the ranking loss alone, while in some cases there was an improvement in the regression performance. By contrast, while we do make a parametric assumption for the ranking loss, our regression component is nonparametric and hence more powerful. Thus, in light of Sculley (2010)\u2019s finding, we expect to achieve equitable ranking performance to methods like CRR, and better regression performance.", "startOffset": 103, "endOffset": 1027}, {"referenceID": 30, "context": "mizes ranking performance; the idea is hinted at in (Sculley et al., 2011), but not discussed formally.", "startOffset": 52, "endOffset": 74}, {"referenceID": 22, "context": "Our approach is related to the single-index model (Manski, 1975) class of probabilities, Pr[y = 1|x] = f (wT x), where f (\u00b7) is an unknown link function, in contrast to a generalized linear model which assumes a specific link function.", "startOffset": 50, "endOffset": 64}, {"referenceID": 28, "context": "We also used the combined regression and ranking model (CRR) of Sculley (2010). We do not post-process CRR because that framework is explicitly designed with the aim of providing a good ranking as well as regression, which we would like to compare to our approach; our hypothesis is that our method should provide the most accurate probabilities, while additionally providing an equitable ranking to the CRR model.", "startOffset": 64, "endOffset": 79}, {"referenceID": 15, "context": "Following Sculley (2010), we use the pairwise ranking framework (Herbrich et al., 2000; Joachims, 2002) with logistic loss to optimize for AUC directly, which lends itself naturally to large-scale implementation using stochastic gradient descent.", "startOffset": 64, "endOffset": 103}, {"referenceID": 16, "context": "Following Sculley (2010), we use the pairwise ranking framework (Herbrich et al., 2000; Joachims, 2002) with logistic loss to optimize for AUC directly, which lends itself naturally to large-scale implementation using stochastic gradient descent.", "startOffset": 64, "endOffset": 103}, {"referenceID": 25, "context": "Following Sculley (2010), we use the pairwise ranking framework (Herbrich et al.", "startOffset": 10, "endOffset": 25}, {"referenceID": 14, "context": "item response theory (Hambleton et al., 1991), where the probability of a student answering a question correctly is bounded from below by the success rate of random guessing.", "startOffset": 21, "endOffset": 45}, {"referenceID": 9, "context": "The first dataset is from medical informatics (El-Kareh et al., 2010), where the goal is to predict follow-up errors on microbiology cultures.", "startOffset": 46, "endOffset": 69}, {"referenceID": 29, "context": "Note that logistic and linear regression are strong baselines, and that even small improvements in performance may be significant in practical applications (Sculley, 2010).", "startOffset": 156, "endOffset": 171}], "year": 2012, "abstractText": "In many real-world applications of machine learning classifiers, it is essential to predict the probability of an example belonging to a particular class. This paper proposes a simple technique for predicting probabilities based on optimizing a ranking loss, followed by isotonic regression. This semi-parametric technique offers both good ranking and regression performance, and models a richer set of probability distributions than statistical workhorses such as logistic regression. We provide experimental results that show the effectiveness of this technique on real-world applications of probability prediction.", "creator": "LaTeX with hyperref package"}}}