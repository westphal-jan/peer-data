{"id": "1206.4663", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "The Convexity and Design of Composite Multiclass Losses", "abstract": "We consider composite loss functions for multiclass prediction comprising a proper (i.e., Fisher-consistent) loss over probability distributions and an inverse link function. We establish conditions for their (strong) convexity and explore the implications. We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk.", "histories": [["v1", "Mon, 18 Jun 2012 15:30:52 GMT  (742kb)", "http://arxiv.org/abs/1206.4663v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mark d reid", "robert c williamson", "peng sun"], "accepted": true, "id": "1206.4663"}, "pdf": {"name": "1206.4663.pdf", "metadata": {"source": "META", "title": "The Convexity and Design of Composite Multiclass Losses", "authors": ["Mark D. Reid", "Robert C. Williamson", "Peng Sun"], "emails": ["MARK.REID@ANU.EDU.AU", "BOB.WILLIAMSON@ANU.EDU.AU", "SUNP08@MAILS.TSINGHUA.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "This presentation facilitates the understanding of multi-class losses, since it essentially distinguishes two different aspects: statistical and numerical (Vernet et al., 2011).The statistical properties are controlled by the correct loss.The link function is essentially only a parameterization. Choosing a suitable link can be helpful - for example, a non-convex self-loss can be made convex (and thus easier for numerical optimization) by selecting the appropriate link.In this paper we show how this is possible when a composite loss is convex, and how to convert an arbitrary self-loss convex convex convex convex. The results extend the results to binary composite losses due to Reid & Williamson (2010)."}, {"heading": "1.1. Previous Work", "text": "Adequate losses are the natural losses that must be used to estimate probability; the most important characteristic of a proper loss (see \u00a7 2.1 below) is that its expected value is always minimized by the distribution that defines expectation; they have been detailed in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012, if n = 2 (the \"binary case\"), where there is a holistic representation (Buja et al., 2005; Gneiting & Raftery, 2007; Reid & Williamson, 2011), and characterization (Reid & Williamson, 2010), if differentiable; the loss function theory makes clear how to ideally select a loss - taking into account one's own usefulness in relation to various erroneous predictions (Kiefer, 1987), (Berger, 1985, Section 2.4)."}, {"heading": "1.2. Key Contribution and Significance", "text": "The central point of the paper is the following: Multi-class losses are necessary for many problems. So far, they have typically been constructed as margin losses via a convex function f, which is applied to a generalized notion of \"margin.\" This is unsatisfactory from a design point of view because they confuse two distinct problems: the decision-theoretical notion of loss (which captures what is important to the end user (Berger, 1985)) and problems related to the ease of numerical optimization. In addition, margin losses are not particularly suitable for the non-symmetrical treatment of different classes, as is necessary in many applications. Fortunately, there is a way to neatly separate these two concerns by using a compound loss (Vernet et al., 2011), in which the statistical properties are controlled by choosing the correct loss, and the optimization properties by linking them. This leads to the natural question: Suppose that one remedies a proportionate loss (and thus the overall statistical properties should not be determined) as a loss of the configurability, and thus it is not a loss of the key."}, {"heading": "2. Formal Setup", "text": "Suppose that we are given data S = * xi, yi + i2 [m], so that yi 2 Y is the label corresponding to xi 2 X. These data follow a common distribution PX, Y on X [n]. We use EX, Y or EY | X to denote the expectation and conditional expectation in relation to PX, Y. Faced with a new observation x, we want to predict the probability of pi: = P (Y = i | X = x) of x belonging to class i, for i2 [n]. Classification of several classes requires the learner to predict the most likely class of x; that is, y = argmaxi2 [n] pi."}, {"heading": "2.1. Losses", "text": "A loss measures the quality of the forecast. Let Dn: = {(p1,.,., Y = q., pn): \u00c2i2 [n] pi = 1, and 0 pi 1, 8i 2 [n]} stands for the n-simplex estimate. For the estimate of the probability of losses, \": Dn! Rn +. For classification, the loss of q is 2 Dn (n]! Rn +. The partial losses\" i are the components of \"(q) = (q),\".., n (q) 0 and \"i (q) is the loss caused by the prediction of q 2."}, {"heading": "2.2. Matrix Differential Calculus", "text": "In order to differentiate the losses, we must include the n-simplex in a subset of Rn 1. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)"}, {"heading": "3. Derivatives of Composite Losses", "text": "In practice, predictions are usually vectors in Rn +, and in theory one could always select a parameterization of V in relation to a simple space, assuming a simple linkage with a simple linkage. (In practice, predictions are usually vectors in Rn +, and in theory one could always select a parameterization of V in relation to a particular linkage.) In practice, predictions are usually vectors in Rn +, and in theory one could assume a parameterization of V in relation to a simple space and a simple linkage with a particular linkage. (In practice, predictions are usually vectors in Rn +, and in theory one could assume a parameterization of V in relation to a particular space and a simple linkage with a particular linkage. (In practice, predictions are usually vectors in Rn +, and in theory a parameterization of V in relation to a particular space."}, {"heading": "4. Convexity", "text": "The convexity of a loss is desirable for the simplicity of the numerical optimization of an empirical risk. We will now consider when multi-class self-losses are convex, and give a characterization with respect to the corresponding Bayes risk, which, as we have seen, is the natural way to parameterize a loss. Results in this section are the multi-class generalization of the characterization of the convexity of binary self-losses (Reid & Williamson, 2010). In fact, we get more general results even in the binary case, because we are looking at strongly convex losses here. We will also show how any nonconvex self-loss can be made convex by appropriate choice of a connecting function, in particular: the canonical link.We define a loss if Dn + convex is convex convex when for all p 2 Dn, the map Dn 3 q 7! p0 \u2022 x convex is convex convex."}, {"heading": "4.1. Implications for Canonical Links", "text": "Remember that the canonical linkage y (HL) was chosen so that y (p) = DL (p) 0. This simplifies k (p) to the identity matrix In this case, the equation (17) is reduced to the following sequence. If \"l y 1 is defined in such a way that y = DL 0 then any map v 7!\" i (v) c is strongly convex if and only if \"HL\" (p) 1 < cIn \"or equivalent HL (p) 4 1c In.\" A direct consequence of this result is observance of the definition condition, which is always fulfilled if c = 0, since L \"is always a concave function. Thus, the use of a canonical linkage guarantees a composite loss (p) 4 1c."}, {"heading": "4.2. Implications for Binary Losses", "text": "In the binary case in which there is a contradiction, in which p = 2, (15) and (16) and the positivity of q = Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 22 Q = 22 Q = 22 Q = 22 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 21 21 21 K 21 21 21 21 21 K 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K (21 K = 21 K = 21 K = 21 K 21 K = 21 K 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 K = 21 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 18 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 18 Q = 21 Q = 21 Q = 21 Q = 21 Q = 18 Q = 21 Q = 18 Q = 18 Q = 18 Q = 18 Q = 21 Q = 18 Q = 18 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 18 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q = 21 Q ="}, {"heading": "5. Designing Losses", "text": "The theory developed above suggests that any choice of the correct loss function l and the linkage function y leads to a general loss function with properties (e.g. convexity) that depend entirely on their relationship to each other. Given these two \"knobs\" for parameterizing a loss function, we can begin to wonder what practical trade-offs are involved in selecting a composite loss as a substitute loss for a specific problem. We now propose a simple scheme for constructing loss families with the same Bayes risk. This is achieved by choosing the correct loss l and creating a parameterized family of linkage functions ya for parameters a 2 A. Since the Bayes risk is entirely determined by any composite loss l y 1a for a 2 A, we have Bayes risk L (p) = p0l (p). Thus, we are able to examine the effects of various decisions about composition loss on an underlying problem without fundamentally altering the problem."}, {"heading": "5.1. Parameterised Links", "text": "To construct a parametric family of linkages, we must first select a set of inverse linkage functions, which is the function of linkage functions B = {y 11,.., y 1 B} with a common domain, namely y 1 b: V! Dn for a common n and V. This collection is then called the basic set of linkage functions. We then take the convex linkages of B to form a series of inverse linkage functions Y 1 = conv (B). Each y 1 Y 1 is then identified with the unique A = DB in such a way that B = 1aby 1b = y1. For this construction, it is necessary to show that each such linkage is actually an inverse linkage function, that is, it is sufficient to assume that all base functions are strictly monotonic. A1Of course, this argument holds only in a point-by-point analysis."}, {"heading": "5.2. Experiments", "text": "To test the effects of link selection on the convergence rate, we performed a simple experiment using a basic multiclass boosting algorithm similar to the LKTreeBoost method (Friedman, 2001) for trees with two terminal nodes. In this experiment, l was set to gloss (i.e. li (p) = log pi), and two base links y 1exp and y 1sq correspond to the selection in the preceding subsection for the convex functions f (t) = e t and f (t) = (1 t) 2. For each loss, the increase in link functions y 1a = ay 1exp + (1 a) y 1sq was selected for a 2 {0,25,0,5,0,75,1} to construct composite losses a = l y 1a. For each loss, the upgrade was performed on the basis of data provided by i.i.i.i.i.i."}, {"heading": "6. Conclusion", "text": "Composite losses are a natural family of losses for the estimation and classification of multi-class losses, which allows the separation of concerns between statistical performance (l) and parameterization (y). We have shown that the requirement for the loss to split into a proper loss and an inverse linkage provides sufficient structure to obtain simple expressions for the gradient and derivatives of these losses, especially in the binary case or on the additional assumption that the linkage is canonical for the loss. We have used these results to create sufficient conditions for convexity and conditional numbers for composite losses and a general scheme for designing families of multi-class losses with the same Bayes risk. Preliminary experiments show that there are inherent trade-offs when designing multi-class losses that require further investigation."}, {"heading": "Acknowledgements", "text": "This work was supported by the Australian Research Council (ARC). NICTA is funded by the Australian governments, represented by the Department of Broadband, Communications and the Digital Economy, and by the ARC through the ICT Centre of Excellence program. Peng Sun visited ANU and NICTA while working on this paper."}], "references": [{"title": "Statistical Decision Theory and Bayesian Analysis", "author": ["Berger", "James O"], "venue": null, "citeRegEx": "Berger and O.,? \\Q1985\\E", "shortCiteRegEx": "Berger and O.", "year": 1985}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Loss functions for binary class probability estimation and classification: Structure and applications", "author": ["Buja", "Andreas", "Stuetzle", "Werner", "Shen", "Yi"], "venue": "Technical report,", "citeRegEx": "Buja et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Buja et al\\.", "year": 2005}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["Dietterich", "Thomas G", "Bakiri", "Ghulum"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1995}, {"title": "Notes on matrix calculus", "author": ["Fackler", "Paul K"], "venue": "North Carolina State University,", "citeRegEx": "Fackler and K.,? \\Q2005\\E", "shortCiteRegEx": "Fackler and K.", "year": 2005}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, pp. 1189\u20131232,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Gneiting", "Tilmann", "Raftery", "Adrian E"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gneiting et al\\.", "year": 2007}, {"title": "A framework for kernel-based multi-category classification", "author": ["Hill", "Simon I", "Doucet", "Arnaud"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hill et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2007}, {"title": "Fundamentals of Convex Analysis", "author": ["Hiriart-Urruty", "Jean-Baptiste", "Lemar\u00e9chal", "Claude"], "venue": null, "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 2001}, {"title": "Topics in Matrix Analysis", "author": ["Horn", "Roger A", "Johnson", "Charles A"], "venue": null, "citeRegEx": "Horn et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Horn et al\\.", "year": 1991}, {"title": "Admissibility and complete class results for the multinomial estimation problem with entropy and squared error loss", "author": ["Ighodaro", "Ayodele", "Santner", "Thomas", "Brown", "Lawrence"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Ighodaro et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Ighodaro et al\\.", "year": 1982}, {"title": "Introduction to Statistical Inference", "author": ["Kiefer", "Jack Carl"], "venue": "SpringerVerlag, New York,", "citeRegEx": "Kiefer and Carl.,? \\Q1987\\E", "shortCiteRegEx": "Kiefer and Carl.", "year": 1987}, {"title": "Fisher consistency of multicategory support vector machines", "author": ["Liu", "Yufeng"], "venue": "In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Liu and Yufeng.,? \\Q2007\\E", "shortCiteRegEx": "Liu and Yufeng.", "year": 2007}, {"title": "Matrix Differential Calculus with Applications in Statistics and Econometrics (revised edition)", "author": ["Magnus", "Jan R", "Neudecker", "Heinz"], "venue": null, "citeRegEx": "Magnus et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Magnus et al\\.", "year": 1999}, {"title": "Estimating multinomial cell probabilities under quadratic loss", "author": ["Nayak", "Tapan K", "Naik", "Dayanand N"], "venue": "Journal of the Royal Statistical Society. Series D (The Statistician),", "citeRegEx": "Nayak et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Nayak et al\\.", "year": 1989}, {"title": "Composite binary losses", "author": ["Reid", "Mark D", "Williamson", "Robert C"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reid et al\\.", "year": 2010}, {"title": "Information, divergence and risk for binary experiments", "author": ["Reid", "Mark D", "Williamson", "Robert C"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reid et al\\.", "year": 2011}, {"title": "Monotone operators in Banach space and nonlinear partial differential equations", "author": ["Showalter", "Ralph Edwin"], "venue": "American Mathematical Society,", "citeRegEx": "Showalter and Edwin.,? \\Q1997\\E", "shortCiteRegEx": "Showalter and Edwin.", "year": 1997}, {"title": "On the consistency of multiclass classification methods", "author": ["Tewari", "Ambuj", "Bartlett", "Peter L"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tewari et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tewari et al\\.", "year": 2007}, {"title": "Mixability is Bayes risk curvature relative to log loss", "author": ["van Erven", "Tim", "Reid", "Mark D", "Williamson", "Robert C"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory,", "citeRegEx": "Erven et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2011}, {"title": "Composite multiclass losses", "author": ["Vernet", "Elodie", "Williamson", "Robert C", "Reid", "Mark D"], "venue": "In NIPS2011,", "citeRegEx": "Vernet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vernet et al\\.", "year": 2011}, {"title": "Derivative operations on matrices", "author": ["Vetter", "William J"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Vetter and J.,? \\Q1970\\E", "shortCiteRegEx": "Vetter and J.", "year": 1970}, {"title": "Multicategory vertex discriminant analysis for high-dimensional data", "author": ["Wu", "Tong Tong", "Lange", "Kenneth"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Statistical analysis of some multi-category large margin classification methods", "author": ["Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang and Tong.,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2004}, {"title": "Coherence functions for multicategory margin-based classification methods", "author": ["Zhang", "Zhihua", "Jordan", "Michael I", "Li", "Wu-Jun", "Yeung", "DitYan"], "venue": "In Proceedings of the Twelfth Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}, {"title": "The margin vector, admissible loss and multi-class margin-based classifiers", "author": ["Zou", "Hui", "Zhu", "Ji", "Hastie", "Trevor"], "venue": null, "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}, {"title": "New multicategory boosting algorithms based on multicategory Fisher-consistent losses", "author": ["hastie/ Papers/margin.pdf. Zou", "Hui", "Zhu", "Ji", "Hastie", "Trevor"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Zou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 20, "context": "This representation makes the understanding of multiclass losses easier because crucially it seperates two distinct concerns: the statistical and the numerical (Vernet et al., 2011).", "startOffset": 160, "endOffset": 181}, {"referenceID": 20, "context": "This representation makes the understanding of multiclass losses easier because crucially it seperates two distinct concerns: the statistical and the numerical (Vernet et al., 2011). The statistical properties are controlled by the proper loss. The link function is essentially just a parametrisation. Choice of a suitable link can help \u2014 for example, a nonconvex proper loss can be made convex (and thus more amenable to numerical optimisation) by choice of the appropriate link. In this paper we show how this is possible, when a composite loss is convex, and how to convexify an arbitrary proper multiclass loss. The results extend the results on binary composite losses due to Reid & Williamson (2010).", "startOffset": 161, "endOffset": 706}, {"referenceID": 2, "context": "ied in detail when n = 2 (the \u201cbinary case\u201d) where there is an integral representation (Buja et al., 2005; Gneiting & Raftery, 2007; Reid & Williamson, 2011), and characterization (Reid & Williamson, 2010) when differentiable.", "startOffset": 87, "endOffset": 157}, {"referenceID": 10, "context": "(Ighodaro et al., 1982; Nayak & Naik, 1989).", "startOffset": 0, "endOffset": 43}, {"referenceID": 26, "context": "More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions).", "startOffset": 106, "endOffset": 214}, {"referenceID": 24, "context": "More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions).", "startOffset": 106, "endOffset": 214}, {"referenceID": 10, "context": "(Ighodaro et al., 1982; Nayak & Naik, 1989). Early approaches to multiclass losses used a simple reduction to binary (Dietterich & Bakiri, 1995). More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions). Zou et al. (2005) proposed a multiclass generalisation of \u201cadmissible losses\u201d (their name for classification calibration) for multiclass margin classification.", "startOffset": 1, "endOffset": 545}, {"referenceID": 10, "context": "(Ighodaro et al., 1982; Nayak & Naik, 1989). Early approaches to multiclass losses used a simple reduction to binary (Dietterich & Bakiri, 1995). More recently, other approaches to the design of losses for multiclass prediction have received attention (Zhang, 2004; Hill & Doucet, 2007; Tewari & Bartlett, 2007; Liu, 2007; Zou et al., 2008; Zhang et al., 2009), although none of these papers developed the connection to proper losses, and most restrict consideration to margin losses (which imply certain symmetry conditions). Zou et al. (2005) proposed a multiclass generalisation of \u201cadmissible losses\u201d (their name for classification calibration) for multiclass margin classification. Liu (2007) considered several multiclass generalisations of hinge loss (suitable for multiclass SVMs) and showed some of them were and others not Fisher consistent.", "startOffset": 1, "endOffset": 698}, {"referenceID": 20, "context": "Fortunately there is a way of neatly separating these two concerns through the use of a composite loss (Vernet et al., 2011) where the statistical properties are controlled by the choice of proper loss, and the optimisation properties via the link.", "startOffset": 103, "endOffset": 124}, {"referenceID": 19, "context": "Furthermore, since l is proper, Lemma 5 by van Erven et al. (2011) means we can use the relationship between a proper loss and its projected Bayes risk L\u0303 := L P 1 D to write Dl\u0303 ( p\u0303) =W ( p\u0303) \u00b7HL\u0303( p\u0303) (5) Dln( p\u0303) = y( p\u0303) \u00b7Dl\u0303 ( p\u0303) (6)", "startOffset": 47, "endOffset": 67}, {"referenceID": 20, "context": "Strict properness guarantees L\u0303 is strictly concave (Vernet et al., 2011) and Kachurovskii\u2019s theorem (Showalter, 1997) states that the derivative of a function is (strictly) monotonic if and only if the function is (strictly) convex.", "startOffset": 52, "endOffset": 73}, {"referenceID": 25, "context": "As an example, the class of Fisher-consistent margin losses proposed by Zou et al. (2008) provides a flexible starting point for designing sets of link functions as described above.", "startOffset": 72, "endOffset": 90}, {"referenceID": 5, "context": "In order to test the impact the choice of link has on the convergence rate we ran a simple experiment using a basic multiclass boosting algorithm, much like the LKTreeBoost method (Friedman, 2001) for trees with two terminal nodes.", "startOffset": 180, "endOffset": 196}], "year": 2012, "abstractText": "We consider composite loss functions for multiclass prediction comprising a proper (i.e., Fisherconsistent) loss over probability distributions and an inverse link function. We establish conditions for their (strong) convexity and explore the implications. We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk.", "creator": "LaTeX with hyperref package"}}}