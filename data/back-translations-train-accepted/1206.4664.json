{"id": "1206.4664", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Tighter Variational Representations of f-Divergences via Restriction to Probability Measures", "abstract": "We show that the variational representations for f-divergences currently used in the literature can be tightened. This has implications to a number of methods recently proposed based on this representation. As an example application we use our tighter representation to derive a general f-divergence estimator based on two i.i.d. samples and derive the dual program for this estimator that performs well empirically. We also point out a connection between our estimator and MMD.", "histories": [["v1", "Mon, 18 Jun 2012 15:31:13 GMT  (503kb)", "http://arxiv.org/abs/1206.4664v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["avraham ruderman", "mark d reid", "dario garc\u00eda-garc\u00eda", "james petterson"], "accepted": true, "id": "1206.4664"}, "pdf": {"name": "1206.4664.pdf", "metadata": {"source": "CRF", "title": "Tighter Variational Representations of f-Divergences via Restriction to Probability Measures", "authors": ["Avraham Ruderman", "Mark D. Reid", "Dar\u00edo Garc\u00eda-Garc\u00eda"], "emails": ["AVRAHAM.RUDERMAN@NICTA.COM.AU", "MARK.REID@ANU.EDU.AU", "DARIO.GARCIA@ANU.EDU.AU", "JAMES.PETTERSON@NICTA.COM.AU"], "sections": [{"heading": "1. Introduction", "text": "An important class of discrepancy measures between probability distributions is the family of f divergences (also known as Ali-Silvey (Ali & Silvey, 1966) or Csisz \u00b7 r divergences (Csisz\u00e1r, 1967)), including variation divergence, Hellinger distance and the well-known Kullback-Leibler (KL) divergence. Estimates of these variables from two i.i.d. samples can be used to test whether these samples come from similar distributions. Because of the convexity of the f function of the same name, which it defines, f -divergences can be expressed differently than the maximum of an optimization problem. This variation-related representation has recently been used for f -divergence estimation (Nguyen et al., 2010; Kanamori et al., 2011) homogeneity tests (Kanamori et al., 2011)."}, {"heading": "1.1. Convex Duality", "text": "For functions f: X \u2192 R defines the (fennel or convex) double F?: X? \u2192 R of a function is defined by double space X? (x?): \u00b7 > f: x < x > f (x), where < \u00b7 > is the double pairing of X and X?. In finite dimensional spaces such as Rd, double space is also Rd and < \u00b7 is the usual inner product. Double f????: X \u2192 R is only double f? (limited to X), that is, f? (x) = supx? (x) = supx? < x, x? f? (x?).For convex, lower semi-continuous functions f??"}, {"heading": "2. Estimation using RKHS methods", "text": "In view of two examples 3 Xn: = {x1,., xn} and Yn: = {y1,., yn} from P and Q respectively, we want the empirical measurements Pn: = 1n \u00b2 n = 1 \u00b2 and Qn: = 1 n \u00b2 n \u00b2 (IR1,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,.,...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2.1. Connections with Maximum Mean Discrepancy", "text": "The relation of the optimization program in (9) to the original f -divergence is imperative; the first term 1 n-Q = 1 f (n\u03b1i) is simply the empirical estimate of the f -divergence of the probability ratio dPdQ, since each n\u03b1i = \u03b1i 1 / n represents an estimate of dP dQ (yi) if Qn is considered uniform over y1. Since f (1) = 0, the minimization of this term alone would force \u03b1i = 1n for all i. In this sense, the first term can be regarded as a kind of generalized MaxEnt regulation; the second term can be regarded as a term that forces the \u03b1i terms to combine empirical means of the attribute vector-E (xi) and the computational model (yi)."}, {"heading": "3. Experiments", "text": "Theorem 1 shows that the restrictive limit of variation derived here is strictly stricter than that proposed by (Nguyen et al., 2010) (henceforth NWJ) for each function r-L1 (Q), unless r = dP / dQ concurs in this case and the optimum is achieved, which suggests that the optimization problem resulting from our narrower limit should lead to an estimator with a smaller bias. In this section, we present some empirical results supporting this improvement. We also conducted experiments to compare our method and Nguyen et as a method with other methods in the literature that are not based on varying representations of f-divergence. While these non-varying methods are not at the center of the experiments, we include them here as they may be of interest to others. However, in the context of the current work, we emphasize the superior performance of our estimator in relation to the NWJ density."}, {"heading": "3.1. Method", "text": "Both our estimator based on (9) and the M2 estimators of NWJ were implemented using the nonlinear convex optimization routine from the Python package Cvxopt to perform the optimization. Implementation of the estimator Wang et al. (Wang et al., 2009) was based on the cKDTree software selected by the SciPy library. Kanamori et al. (uLSIF) and Garc\u00eda et al. (f, l)) algorithms were implemented using the code provided by the respective authors 6. The method for selecting the parameters \u03bbn and \u03c3 for the NWJ estimator is not specified in (Nguyen et al., 2010). Therefore, for both NWJ and our estimator we have defined a shaped n = 1n (as discussed above) and a sample variance over Xn [1]."}, {"heading": "3.2. Results", "text": "Table 1 summarizes the application of all five estimators over 250 runs of the 1-d (odd lines) and 10-d experiments (even lines) for different selections of P and Q in the first column. Increasingly, the pairs of rows are ordered in the value of true KL divergence (shown in the second column) and are the same for both rows. The table lists the divergence estimates averaged over the different passes, as well as the empirical Mean Squared Error (MSE). The bold values for the MSE correspond to the lowest of the different estimators. Where the MSE of our estimator or that of NWJ is strictly lower than the other, we have set the MSE in italics. The last three columns are gray as they are not the main point of the experiments."}, {"heading": "3.3. Discussion", "text": "This is not surprising, since our estimator is based on a narrower boundary for divergence than theorem 1. In contrast, the difference is smaller because it behaves very differently from the variable estimators. Generally, their bias is significantly lower than that of both methods of variation when the real divergence is large. This is a natural conclusion, since the methods of variation presented here are intrinsically lower than the real divergences of the real divergences."}, {"heading": "4. Summary and Discussion", "text": "We have shown how narrower variational representations of f -divergences can be functionally derived by limiting the effective range of divergence to the set of probability scales. As much work in the literature is based on variational representations, this narrower version represents many potential applications. As an example, a dual program for f -divergence estimators was derived based on this narrower representation of density ratios within an RKHS H and arbitrary convex regulators, which aggravated and expanded the M2 estimator proposed in (Nguyen et al., 2010) and we demonstrated empirically the benefits of our analysis. We also gave a novel interpretation of the dual program with respect to MMD, which showed that our estimator can find an approximation r-H of the density ratio that attempts to simultaneously minimize the MMD between Pn and r-Qn and the empirical f-divergence."}], "references": [{"title": "A general class of coefficients of divergence of one distribution from another", "author": ["S.M. Ali", "S.D. Silvey"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Ali and Silvey,? \\Q1966\\E", "shortCiteRegEx": "Ali and Silvey", "year": 1966}, {"title": "Unifying divergence minimization and statistical inference via convex duality", "author": ["Y. Altun", "A. Smola"], "venue": "Learning theory, pp", "citeRegEx": "Altun and Smola,? \\Q2006\\E", "shortCiteRegEx": "Altun and Smola", "year": 2006}, {"title": "The concept of duality in convex analysis, and the characterization of the legendre transform", "author": ["S. Artstein-Avidan", "V. Milman"], "venue": "Annals of mathematics,", "citeRegEx": "Artstein.Avidan and Milman,? \\Q2009\\E", "shortCiteRegEx": "Artstein.Avidan and Milman", "year": 2009}, {"title": "On Bayesian bounds", "author": ["A. Banerjee"], "venue": "Proceedings of the 23rd International Conference on Machine learning,", "citeRegEx": "Banerjee,? \\Q2006\\E", "shortCiteRegEx": "Banerjee", "year": 2006}, {"title": "Convexity and optimization in Banach spaces, volume", "author": ["V. Barbu", "T. Precupanu"], "venue": null, "citeRegEx": "Barbu and Precupanu,? \\Q1986\\E", "shortCiteRegEx": "Barbu and Precupanu", "year": 1986}, {"title": "Parametric estimation and tests through divergences and the duality technique", "author": ["Broniatowski", "Michel", "Keziou", "Amor"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Broniatowski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Broniatowski et al\\.", "year": 2009}, {"title": "Information-type measures of difference of probability distributions and indirect observations", "author": ["I. Csisz\u00e1r"], "venue": "Studia Scientiarum Mathematicarum Hungarica,", "citeRegEx": "Csisz\u00e1r,? \\Q1967\\E", "shortCiteRegEx": "Csisz\u00e1r", "year": 1967}, {"title": "Large deviations techniques and applications, volume 38", "author": ["A. Dembo", "O. Zeitouni"], "venue": null, "citeRegEx": "Dembo and Zeitouni,? \\Q2009\\E", "shortCiteRegEx": "Dembo and Zeitouni", "year": 2009}, {"title": "Asymptotic evaluation of certain markov process expectations for large time", "author": ["MD Donsker", "Varadhan", "SRS"], "venue": "iv. Communications on Pure and Applied Mathematics,", "citeRegEx": "Donsker et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Donsker et al\\.", "year": 1983}, {"title": "Risk-based generalizations of fdivergences", "author": ["D. Garc\u00eda-Garc\u00eda", "U. von Luxburg", "R. SantosRodr\u00edguez"], "venue": "In Proceedings of the International Conference on Machine learning,", "citeRegEx": "Garc\u00eda.Garc\u00eda et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garc\u00eda.Garc\u00eda et al\\.", "year": 2011}, {"title": "A Kernel Method for the Two-SampleProblem", "author": ["A. Gretton", "K.M. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "Fundamentals of Convex Analysis", "author": ["Hiriart-Urruty", "Jean-Baptiste", "Lemar\u00e9chal", "Claude"], "venue": null, "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 2001}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "f-divergence estimation and two-sample homogeneity test under semiparametric density-ratio models", "author": ["T. Kanamori", "T. Suzuki", "M. Sugiyama"], "venue": "Information Theory, IEEE Transactions on (To appear),", "citeRegEx": "Kanamori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2011}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "Technical report, Department of Statistics, UC Berkeley,", "citeRegEx": "Nguyen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2007}, {"title": "On surrogate loss functions and f -divergences", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "Annals of Statistics,", "citeRegEx": "Nguyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2009}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X.L. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "On the estimation of alpha-divergences", "author": ["Poczos", "Barnabas", "Schneider", "Jeff"], "venue": "AISTATS", "citeRegEx": "Poczos et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poczos et al\\.", "year": 2011}, {"title": "Information, divergence and risk for binary experiments", "author": ["Reid", "Mark D", "Williamson", "Robert C"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Reid et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reid et al\\.", "year": 2011}, {"title": "Convex analysis, volume 28", "author": ["R.T. Rockafellar"], "venue": "Princeton Univ Pr,", "citeRegEx": "Rockafellar,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar", "year": 1997}, {"title": "Direct importance estimation for covariate shift adaptation", "author": ["M. Sugiyama", "T. Suzuki", "S. Nakajima", "H. Kashima", "P. von Bunau", "M. Kawanabe"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "Divergence estimation for multidimensional densities via k-nearestneighbor distances", "author": ["Q. Wang", "S.R. Kulkarni", "S. Verd\u00fa"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "An important class of discrepancy measures between probability distributions is the family of f -divergences (also known as Ali-Silvey (Ali & Silvey, 1966) or Csisz\u00b7r divergences (Csisz\u00e1r, 1967)).", "startOffset": 179, "endOffset": 194}, {"referenceID": 16, "context": "This variational representation has been recently used for f -divergence estimation (Nguyen et al., 2010; 2007; Kanamori et al., 2011) homogeneity testing (Kanamori et al.", "startOffset": 84, "endOffset": 134}, {"referenceID": 13, "context": "This variational representation has been recently used for f -divergence estimation (Nguyen et al., 2010; 2007; Kanamori et al., 2011) homogeneity testing (Kanamori et al.", "startOffset": 84, "endOffset": 134}, {"referenceID": 13, "context": ", 2011) homogeneity testing (Kanamori et al., 2011) and parameter estimation (Broniatowski & Keziou, 2009).", "startOffset": 28, "endOffset": 51}, {"referenceID": 3, "context": "This generalises similar observations for the specific case of the KL divergence, such as Banerjee\u2019s compression lemma (Banerjee, 2006).", "startOffset": 119, "endOffset": 135}, {"referenceID": 10, "context": "We show this dual program has a natural interpretation as a trade off between the minimisation of the f -divergence in question and the minimum mean discrepancy (MMD) (Gretton et al., 2008) between the empirical distributions.", "startOffset": 167, "endOffset": 189}, {"referenceID": 16, "context": "Experiments in section 3 confirm that the tighter variational form underlying our approach leads to better KL divergence estimates than the well known estimator of (Nguyen et al., 2010; 2007) which is based on the looser representation.", "startOffset": 164, "endOffset": 191}, {"referenceID": 16, "context": "The choice of order of the arguments P and Q is arbitrary and other authors, notably (Nguyen et al., 2010), define f -divergence in terms of dQ/dP .", "startOffset": 85, "endOffset": 106}, {"referenceID": 16, "context": "The dashed line and solid line represent our new expression and the expression used by (Nguyen et al., 2010) respectively as they vary over L\u221e(Q).", "startOffset": 87, "endOffset": 108}, {"referenceID": 16, "context": "As explored in (Nguyen et al., 2010; Altun & Smola, 2006) and in section 2 below, variational representations such as these readily admit approximation", "startOffset": 15, "endOffset": 57}, {"referenceID": 16, "context": "While our new expression and that of (Nguyen et al., 2010) have the same supremum, our expression is closer to the supremum at every point \u03c6 \u2208 L\u221e(Q).", "startOffset": 37, "endOffset": 58}, {"referenceID": 3, "context": "The first inequality is the well-known representation of the KL divergence in the large deviations literature (Donsker & Varadhan, 1983) which has been rediscovered in the PAC-Bayes community as the compression lemma (Banerjee, 2006).", "startOffset": 217, "endOffset": 233}, {"referenceID": 16, "context": "As in (Nguyen et al., 2010), our estimator is then defined via the dual representation of IRf,Qn that takes We assume that the samples are of equal size for simplicity.", "startOffset": 6, "endOffset": 27}, {"referenceID": 14, "context": "The proof techniques here are based on those in (Nguyen et al., 2007).", "startOffset": 48, "endOffset": 69}, {"referenceID": 19, "context": "By the infimal convolution theorem (Rockafellar, 1997) we therefore have", "startOffset": 35, "endOffset": 54}, {"referenceID": 16, "context": "The estimator M2 proposed in (Nguyen et al., 2010) uses square norm regularisation in a two sample setting and is therefore directly comparable to (9).", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "Following (Gretton et al., 2008), we can formalise the observation regarding the second term by considering the mean map \u03bc from distributions R over X to functions in an RKHS H defined by R 7\u2192 \u03bc [R] := Ex\u223cR [\u03a6(x)].", "startOffset": 10, "endOffset": 32}, {"referenceID": 15, "context": "This analysis also leads to an intuitive explanation why we should use the regularisation schedule \u03bbn = \u0398 ( n\u22121 ) as per (Nguyen et al., 2009).", "startOffset": 121, "endOffset": 142}, {"referenceID": 10, "context": "It was shown in (Gretton et al., 2008) that the MMD estimator", "startOffset": 16, "endOffset": 38}, {"referenceID": 16, "context": "Theorem 1 shows the restricted variational bound derived here is strictly tighter than the one proposed by (Nguyen et al., 2010) (henceforth NWJ) for every function r \u2208 L(Q) except when r = dP/dQ in which case they coincide and attain the optimum.", "startOffset": 107, "endOffset": 128}, {"referenceID": 21, "context": "\u2022 Wang et al (Wang et al., 2009): This estimator is based on nearest neighbour estimates of the two densities and does not make use of a variational representation.", "startOffset": 13, "endOffset": 32}, {"referenceID": 12, "context": "\u2022 Kanamori et al (Kanamori et al., 2009): This is a least-squares estimator for the density ratio, bypassing individual density estimations.", "startOffset": 17, "endOffset": 40}, {"referenceID": 20, "context": "We also experimented with another density ratio estimation method (Sugiyama et al., 2008), with very similar results.", "startOffset": 66, "endOffset": 89}, {"referenceID": 9, "context": "\u2022 Garc\u00eda et al (Garc\u00eda-Garc\u00eda et al., 2011): This estimator uses nearest neighbour misclassification rates and a reformulation of f -divergences in terms of risks.", "startOffset": 15, "endOffset": 43}, {"referenceID": 21, "context": "(Wang et al., 2009) estimator (henceforth WKV) was based on the cKDTree nearest neighbour routine from the SciPy library.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "The method for choosing the parameters \u03bbn and \u03c3 for the NWJ estimator are not specified in (Nguyen et al., 2010).", "startOffset": 91, "endOffset": 112}, {"referenceID": 16, "context": "This tightened and extended the M2 estimator proposed in (Nguyen et al., 2010) and we demonstrated empirically the benefits of our analysis.", "startOffset": 57, "endOffset": 78}, {"referenceID": 13, "context": "As future work we intend to investigate the impact of this tightened representation on other divergence estimators based on the looser representation such as (Kanamori et al., 2011), as well as to areas other than", "startOffset": 158, "endOffset": 181}, {"referenceID": 16, "context": "The work of (Nguyen et al., 2010) has already paved the way for this investigation.", "startOffset": 12, "endOffset": 33}], "year": 2012, "abstractText": "We show that the variational representations for f -divergences currently used in the literature can be tightened. This has implications to a number of methods recently proposed based on this representation. As an example application we use our tighter representation to derive a general f -divergence estimator based on two i.i.d. samples and derive the dual program for this estimator that performs well empirically. We also point out a connection between our estimator and MMD.", "creator": "TeX"}}}