{"id": "1206.4665", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Nonparametric variational inference", "abstract": "Variational methods are widely used for approximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approximations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data. Using multiple kernels allows the approximation to capture multiple modes of the posterior, unlike most other variational approximations. We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model. We obtain predictive performance as good as or better than more specialized variational methods and sample-based approximations. The method is easy to apply to more general graphical models for which standard variational methods are difficult to derive.", "histories": [["v1", "Mon, 18 Jun 2012 15:32:05 GMT  (655kb)", "http://arxiv.org/abs/1206.4665v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["samuel gershman", "matthew d hoffman", "david m blei"], "accepted": true, "id": "1206.4665"}, "pdf": {"name": "1206.4665.pdf", "metadata": {"source": "META", "title": "Nonparametric Variational Inference", "authors": ["Samuel J. Gershman", "Matthew D. Hoffman", "David M. Blei"], "emails": ["sjgershm@princeton.edu", "mdhoffma@cs.princeton.edu", "blei@cs.princeton.edu"], "sections": [{"heading": "1. Introduction", "text": "In this paper, we develop a new variable inference algorithm for complex probabilistic methods, our method can capture more meaningful distributions and apply it to a broader class of models. Variational inference methods define some restricted distribution families via the hidden variables and try to find the member of that family that comes closest to the rear model. The family is chosen in such a way that the problem of discovering the distribution q that best approximates the rear distribution components becomes a tractoral model. Variational methods are effective and widely used. Variational methods are effective and widely used. These methods usually find an unimodoric approximation of the rear distribution model. We find a non-model approximation of the rear distribution model."}, {"heading": "2. Variational inference", "text": "We consider the problem of calculating the rear distribution of the hidden variables. Therefore, this calculation is not analytically feasible for many interest models, since the denominator is difficult to calculate (Jordan et al., 1999; Beal, 2003). The goal behind the variational methods is to choose a member of this family who comes closest to the rear level. In the variational inference, the proximity is measured by Kullback-Leibler (KL), the divergence, KL [q) and KL [q) is the negative distribution. The goal is to choose a member of this family who is closest to the rear level."}, {"heading": "3. Nonparametric variational inference", "text": "Our algorithm is suitable for models with continuously weighted hidden random variables and does not require conjugation between pairs of variables. We select the distribution q (\u03b8) as an evenly weighted Gaussian mixture with isotropic covariances q (\u03b8) = 1N N \u00b2 n = 1 N (\u03b8; \u03c32nI), (5) where \u00b5n is the mean of the ninth Gaussian component and \u03c32n is its variance. We call this a \"non-parametric\" family: we make a weak series of assumptions about the shape of the rear component, since the Gaussian compound family can arbitrarily approximate complex rear components with a sufficient number of components. In addition, this family resembles kernel density estimators used in classical nonparametric statistics (Silverman, 1986), where \u00b5n plays the role of the center and bandwidth."}, {"heading": "3.1. The Evidence Lower Bound", "text": "If q is defined in the family of Eq. 5, we cannot calculate the ELBO F [q] (in general, there is no closed form expression either for the expectation of a nonlinear function under a Gaussian distribution or for the entropy of a mixture of Gaussians. However, we can approximate the ELBO values and optimize this approximation (see Lawrence, 2000; Honkela et al., 2007, for other approaches to this problem).First, we have to approximate the entropy term H [q], then we can approximate the expected log common Eq values [log p].We have to approximate entropy (the first term in Eq. 4) using Jensen's Inequality (Huber et al., 2008), H [q] = \u2212 setpoint q) q."}, {"heading": "3.2. Optimizing the ELBO", "text": "Our goal now is to maximize Eq. 10 with respect1When some parameters are limited, but one can also use nonlinear transformations to avoid an approximate parameterization within a limited range (e.g. the logistic function for variables in [0, 1]). In this case, one should add a protocol that approximates the distribution of ELBO, where J is the jacobic matrix of the first derivatives of the transformation.Algorithm 1 non-parametric variables in [0, 1] requires that the data y, number of components N: N be added randomly. Repetitions for n = 1 to N we do argmax\u00b5n L1 [q]. End for glory 21: N argmax\u03c321: N L2 q].until the change of L2 [q]."}, {"heading": "3.3. Relationship to other algorithms", "text": "The NPV target refers to several other methods. If there is a component N = 1, the entropy term log q1 does not depend on the mean \u00b51, and if \u03c3 2 1 becomes sufficiently small, the Hessian term goes from Eq. 10 to 0. Thus, the maximum of this function is the maximum a posteriori (MAP) solution. If N = 1 and \u03c31 \u2192 0 isL [q] = log p (y, \u00b5) + const. = log p (\u03b8 = \u00b5 | y) + const. This can be understood as a diagonalized laplace approximation (MacKay, 1995), i.e. where we ignore correlations between the dimensions of the MAP. If the Laplace approximation is allowed to vary, we have disadvantages: For example, it is not invariant to perform a repair if the mean and postmode of the MAP, i.e. if we ignore correlations between the dimensions of the MAP, the MAP."}, {"heading": "4. Related work", "text": "Approximate inference for non-conjugated models is an active area of research. Some authors have used numerical or Monte Carlo methods to approximate insoluble integrals. Thus, Lawrence et al. (2004) used importance samples to approximate the expectations required for inference in a Bayesian model of microarray images. Ihler et al. (2009) generalized particle filtering to approximate inference in factor diagrams with continuous variables. Honkela et al. (2007) used numerical quadratures to approximate expectations in a nonlinear factor analysis model. These techniques are useful but may fail in high dimensions. Several researchers use specialized approximations for certain classes of models, such as those with logistic nonlinearity (e.g. Jaakkola & Jordan, 2000; Khan et al., 2010). In contrast, our goal is to develop an algorithm for generalized inference in non-continuous modal jugation with Lawrence."}, {"heading": "5. Applications", "text": "In this section, we apply the NPV algorithm to several probabilistic models and compare its performance with other widely used methods."}, {"heading": "5.1. Logistic regression", "text": "We focus on a hierarchical logistic regression model and compare its accuracy with a standard variable treatment (Jaakkola & Jordan, 2000, henceforth \"YY\").Generative model.The observed data y = {c, X} consist of T binary class identifiers, ct {\u2212 1}, and K covariates for each datapoint, xt, RK. The hidden variables consist of K regression coefficients wk, R, and a precision parameter. We proceed from the following model (MacKay, 1995): p (\u03b1) = Gamma; a, b) (12) p (wk | \u03b1) = N (wk; 0, \u03b1 \u2212 1) p (13)."}, {"heading": "5.2. Topographic latent source analysis", "text": "The aim of these experiments is to understand the relationship between cognitive processes and brain activity. One reason for this problem is complicated, because the fMRI data has a spatial activity that is measured while a subject is performing a task (a grid of \"voxels\").The aim of these experiments is to understand the relationship between cognitive processes and brain activity. One reason for this is that the fMRI data is measured spatially. Brain activity is measured in 3D brain spaces (a grid of \"voxels\").Measurements performed near voxels are dependent.Gershman et al. (2011) develops a factoring model of spatial patterns in fMRI data, topographic latent source analysis (TLSA)."}, {"heading": "6. Discussion", "text": "Our algorithm is easy to apply to new probability models; all that is required is the probability function and its gradient (a requirement shared by many other algorithms, including MAP estimation and HMC).When applied to a hierarchical logistical regression model, we found that NPV exhibits a low loss of accuracy compared to a more specialized variation algorithm (Jaakkola & Jordan, 2000).We also demonstrated that using a nonlinear latent variable model of fMRI data, NPV can find an approximation of the rear NPV that improves predictive performance over the MAP estimate and has limitations on MCMC.NPV. First, it starts from a simple approximation family, which could be improved by introducing a full covariance matrix into component distributions."}], "references": [{"title": "Variational algorithms for approximate Bayesian inference", "author": ["M.J. Beal"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "Mathematical statistics: Basic ideas and selected topics, 2nd edn.(Vol", "author": ["P.J. Bickel", "K.A. Doksum"], "venue": null, "citeRegEx": "Bickel and Doksum,? \\Q2007\\E", "shortCiteRegEx": "Bickel and Doksum", "year": 2007}, {"title": "Approximating posterior distributions in belief networks using mixtures", "author": ["C.M. Bishop", "N. Lawrence", "T. Jaakkola", "M.I. Jordan"], "venue": null, "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "Split variational inference", "author": ["G. Bouchard", "O. Zoeter"], "venue": "In ICML, pp. 57\u201364", "citeRegEx": "Bouchard and Zoeter,? \\Q2009\\E", "shortCiteRegEx": "Bouchard and Zoeter", "year": 2009}, {"title": "Variational inference for largescale models of discrete choice", "author": ["M. Braun", "J. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Braun and McAuliffe,? \\Q2010\\E", "shortCiteRegEx": "Braun and McAuliffe", "year": 2010}, {"title": "A topographic latent source model for fMRI data", "author": ["S.J. Gershman", "D.M. Blei", "F. Pereira", "K.A. Norman"], "venue": null, "citeRegEx": "Gershman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2011}, {"title": "Blind separation of nonlinear mixtures by variational bayesian learning", "author": ["A. Honkela", "H. Valpola", "A. Ilin", "J. Karhunen"], "venue": "Digital Signal Processing,", "citeRegEx": "Honkela et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Honkela et al\\.", "year": 2007}, {"title": "On entropy approximation for gaussian mixture random vectors", "author": ["M.F. Huber", "T. Bailey", "H. Durrant-Whyte", "U.D. Hanebeck"], "venue": "In Multisensor Fusion and Integration for Intelligent Systems,", "citeRegEx": "Huber et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Huber et al\\.", "year": 2008}, {"title": "Particle-based variational inference for continuous systems", "author": ["A.T. Ihler", "A.J. Frank", "P. Smyth"], "venue": null, "citeRegEx": "Ihler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2009}, {"title": "Bayesian parameter estimation via variational methods", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "Statistics and Computing,", "citeRegEx": "Jaakkola and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 2000}, {"title": "Improving the mean field approximation via the use of mixture distributions", "author": ["T.S. Jaakola", "M.I. Jordan"], "venue": "In Learning Graphical Models", "citeRegEx": "Jaakola and Jordan,? \\Q1998\\E", "shortCiteRegEx": "Jaakola and Jordan", "year": 1998}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Variational bounds for mixed-data factor analysis", "author": ["M.E. Khan", "B. Marlin", "G. Bouchard", "K. Murphy"], "venue": "In NIPS", "citeRegEx": "Khan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2010}, {"title": "Variational inference in probabilistic models", "author": ["N.D. Lawrence"], "venue": "PhD thesis,", "citeRegEx": "Lawrence,? \\Q2000\\E", "shortCiteRegEx": "Lawrence", "year": 2000}, {"title": "Reducing the variability in cDNA microarray image processing by Bayesian inference", "author": ["N.D. Lawrence", "M. Milo", "M. Niranjan", "P. Rashbass", "S. Soullier"], "venue": null, "citeRegEx": "Lawrence et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2004}, {"title": "Probable networks and plausible predictions-a review of practical Bayesian methods for supervised neural networks. Network: Computation", "author": ["D.J.C. MacKay"], "venue": "Neural Systems,", "citeRegEx": "MacKay,? \\Q1995\\E", "shortCiteRegEx": "MacKay", "year": 1995}, {"title": "Fisher discriminant analysis with kernels", "author": ["S. Mika", "G. Ratsch", "J. Weston", "B. Scholkopf", "Mullers", "KR"], "venue": "In Neural Networks for Signal Processing IX,", "citeRegEx": "Mika et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mika et al\\.", "year": 1999}, {"title": "MCMC using Hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo", "citeRegEx": "Neal,? \\Q2011\\E", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Density Estimation for Statistics and Data Analysis", "author": ["B.W. Silverman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Silverman,? \\Q1986\\E", "shortCiteRegEx": "Silverman", "year": 1986}], "referenceMentions": [{"referenceID": 11, "context": "These methods usually find a unimodal approximation of the posterior, especially when the variational family is the commonly chosen mean-field family (Jordan et al., 1999; Beal, 2003).", "startOffset": 150, "endOffset": 183}, {"referenceID": 0, "context": "These methods usually find a unimodal approximation of the posterior, especially when the variational family is the commonly chosen mean-field family (Jordan et al., 1999; Beal, 2003).", "startOffset": 150, "endOffset": 183}, {"referenceID": 18, "context": "This family of distributions resembles classical kernel density estimators from nonparametric statistics (Silverman, 1986).", "startOffset": 105, "endOffset": 122}, {"referenceID": 5, "context": "Second, we show that it outperforms several MCMC methods for a non-conjugate matrix factorization model of brain activity data (Gershman et al., 2011).", "startOffset": 127, "endOffset": 150}, {"referenceID": 11, "context": "The idea behind variational methods is to approximate p(\u03b8|y) with a distribution q(\u03b8) that belongs to a constrained family of distributions, indexed by a variational parameter (Jordan et al., 1999; Beal, 2003).", "startOffset": 176, "endOffset": 209}, {"referenceID": 0, "context": "The idea behind variational methods is to approximate p(\u03b8|y) with a distribution q(\u03b8) that belongs to a constrained family of distributions, indexed by a variational parameter (Jordan et al., 1999; Beal, 2003).", "startOffset": 176, "endOffset": 209}, {"referenceID": 0, "context": "When p(\u03b8i) is chosen to be conjugate to p(y|\u03b8), the calculus of variations leads to closed-form coordinate ascent updates that converge to a local maximum of F [q] (Beal, 2003).", "startOffset": 164, "endOffset": 176}, {"referenceID": 18, "context": "Further, this family resembles kernel density estimators used in classical nonparametric statistics (Silverman, 1986), with \u03bcn playing the role of a kernel center and \u03c3 n playing the role of a bandwidth parameter.", "startOffset": 100, "endOffset": 117}, {"referenceID": 7, "context": "4) using Jensen\u2019s inequality (Huber et al., 2008),", "startOffset": 29, "endOffset": 49}, {"referenceID": 15, "context": "This can be understood as a diagonalized Laplace approximation (MacKay, 1995), i.", "startOffset": 63, "endOffset": 77}, {"referenceID": 0, "context": "The Laplace approximation has drawbacks: for example, it is not invariant to reparameterization, it performs badly when the mean and mode of the posterior are far apart, and it cannot capture multiple modes (Beal, 2003).", "startOffset": 207, "endOffset": 219}, {"referenceID": 11, "context": "For example, Lawrence et al. (2004) used importance sampling to approximate the expectations required for inference in a Bayesian model of microarray images.", "startOffset": 13, "endOffset": 36}, {"referenceID": 7, "context": "Ihler et al. (2009) generalized particle filtering for approximate inference in factor graphs with continuous variables.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Honkela et al. (2007) used numerical quadrature to approximate expectations in a nonlinear factor analysis model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Several researchers use specialized approximations for certain classes of models, such as those with logistic nonlinearities (e.g., Jaakkola & Jordan, 2000; Khan et al., 2010).", "startOffset": 125, "endOffset": 175}, {"referenceID": 2, "context": "Closely related to our method is the mixture meanfield (MMF) method (Bishop et al., 1998; Jaakola & Jordan, 1998; Lawrence, 2000), which models the posterior as a mixture of mean-field approximations.", "startOffset": 68, "endOffset": 129}, {"referenceID": 13, "context": "Closely related to our method is the mixture meanfield (MMF) method (Bishop et al., 1998; Jaakola & Jordan, 1998; Lawrence, 2000), which models the posterior as a mixture of mean-field approximations.", "startOffset": 68, "endOffset": 129}, {"referenceID": 2, "context": "Closely related to our method is the mixture meanfield (MMF) method (Bishop et al., 1998; Jaakola & Jordan, 1998; Lawrence, 2000), which models the posterior as a mixture of mean-field approximations. Recently, Bouchard & Zoeter (2009) revisited this approach using soft-binning functions.", "startOffset": 69, "endOffset": 236}, {"referenceID": 15, "context": "We assume the following model (MacKay, 1995):", "startOffset": 30, "endOffset": 44}, {"referenceID": 16, "context": "We evaluated NPV and JJ on 13 binary classification data sets compiled by Mika et al. (1999). The number of covariates in these data sets ranges from 2 to 60, and the number of observations ranges from 24 to 7400.", "startOffset": 74, "endOffset": 93}, {"referenceID": 16, "context": "Each point represents one of 13 data sets compiled by Mika et al. (1999). For NPV, N = 5 components were used (similar results were obtained with N = 10).", "startOffset": 54, "endOffset": 73}, {"referenceID": 17, "context": "We also fit the model using an MCMC algorithm, Hamiltonian (or Hybrid) Monte Carlo algorithm (HMC; Neal, 2011), which takes the same inputs as NPV (the log joint probability and its gradient).", "startOffset": 93, "endOffset": 110}, {"referenceID": 5, "context": "Following Gershman et al. (2011), we chose this function to be a radial basis function with parameters \u03c9k = {r\u0304k, \u03bbk}:", "startOffset": 10, "endOffset": 33}, {"referenceID": 5, "context": "We repeated this procedure for the MetropolisHastings (MH) sampler used in the original TLSA paper (Gershman et al., 2011).", "startOffset": 99, "endOffset": 122}, {"referenceID": 13, "context": "We re-emphasize here that TLSA is non-conjugate, and hence MMF cannot be applied without using specially-tailored approximations (Lawrence, 2000).", "startOffset": 129, "endOffset": 145}], "year": 2012, "abstractText": "Variational methods are widely used for approximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approximations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data. Unlike most other variational approximations, using multiple kernels allows the approximation to capture multiple modes of the posterior. We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model. We obtain predictive performance as good as or better than more specialized variational methods and MCMC approximations. The method is easy to apply to graphical models for which standard variational methods are difficult to derive.", "creator": "LaTeX with hyperref package"}}}