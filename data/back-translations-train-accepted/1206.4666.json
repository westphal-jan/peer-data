{"id": "1206.4666", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices", "abstract": "We present a Bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. A Gibbs sampler is derived to simulate samples of the common eigenvectors and the eigenvalues for these matrices. Several synthetic examples are used to illustrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms, which shows that the Gibbs sampler achieves the state-of-the-art performance on the examples considered. As a byproduct, the output of the Gibbs sampler could be used to estimate the log marginal likelihood, however we employ the approximation based on the Bayesian information criterion (BIC) which in the synthetic examples considered correctly located the number of common eigenvectors. We then succesfully applied the sampler to the source separation problem as well as the common principal component analysis and the common spatial pattern analysis problems.", "histories": [["v1", "Mon, 18 Jun 2012 15:32:46 GMT  (472kb)", "http://arxiv.org/abs/1206.4666v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "stat.CO cs.LG stat.ME", "authors": ["mingjun zhong", "mark a girolami"], "accepted": true, "id": "1206.4666"}, "pdf": {"name": "1206.4666.pdf", "metadata": {"source": "META", "title": "A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices", "authors": ["Mingjun Zhong", "Mark Girolami"], "emails": ["mingjun.zhong@gmail.com", "m.girolami@ucl.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "There are various applications for common diagnostic technology, such as Blind Source Separation (BSS) (BSS) (BSS) et al, 1997; Yeredor, 2002; Yeredor, 2002; Souloumiac, 2009).The standard diagonalization scheme finds a common diagonalization scheme in which a common diagonalization scheme is found. (WCkWT) There are various applications for common diagnostic technology, such as Blind Source Separation (BSS) (BSS) et al, 1997; Yeredor, 2002; Ziehe et al al al al al al, 2004), common principal component analysis (CPCS 1984)."}, {"heading": "2. The Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. The Data Likelihood", "text": "In view of the model in (1) and the assumed error distribution, the probability p (C) is given as follows: K = 1 | 2\u03c0\u03c32kI | \u2212 1 2 exp {\u2212 1 2\u03c32k | | Ck \u2212 B\u0445 kBT | | 2F}, with the Vec (\u00b7) representing the transformation of a matrix into a vector by concatenating the columns. Name xk = vec (Ck), A = (B 1) (1 B), where 1 denotes a column vector of size N \u00d7 1, where all elements are 1, uk = (\u03bb k 1, \u00b7 \u00b7 \u00b7 \u00b7 p = AukM) T and k = vec (Ek). Then the model \u00b7 \u00b7 K (according to the data 1 \u00b7 K) could be represented by the probability (2) and vice versa the K (2) by the probability p (2)."}, {"heading": "2.2. The Priors and Posteriors", "text": "5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "3. Sampling from the matrix Langevin-Bingham distribution", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. The vector Bingham distribution", "text": "In order to derive a simplified Bingham distribution from the matrix LB distribution, we have to draw samples from the following vector Bingham distribution: i = uniform sample (x-V) = uniform sample (x-V) = uniform sample (x-V) = uniform sample (x-V) = 1 (x-V) in sphere SM \u2212 1. The uniform measurement quantity dSM \u2212 1 (x) is invariant under orthogonal transformation. Therefore, the eigenvalue distribution could be applied to the entire orthogonal matrix O (M)."}, {"heading": "3.2. The matrix Langevin-Bingham distribution", "text": "We use the methods of Hoff (2009a) to simulate the matrix LB distribution. If M < N, we denote the matrix LB distribution of the following form: p (B | C, preservation), p (B), x (B), x (B), x (B), x (B). Since the columns of B are orthogonal to each other, we are statistically not independent of each other. To draw any random sample, we denote the rest of the columns from B to B \u2212 i. Denote Q with the size M \u00b7 (M \u2212 1) as the orthogonormal basis for the zero space of B \u2212 i. Then we could represent bi = Q\u03b2 = SM \u2212 2. Thus, the conditional distribution of \u03b2 of the given B \u2212 i has the following representation p \u2212 k."}, {"heading": "4. Simulation Results", "text": "In this section, we evaluate the proposed Gibbs samplers based on some data sets. In the first example, we compare the reject, slice, and grid sampling schemes to simulate the vector distribution based on a toy data set. In the second example, the Gibbs sampler was applied to data with sizes N = 10, M = 5, and K = 100 to search for common eigenvectors. We also looked at data with sizes N = 10, M = 10, and K = 100. In light of these results, we compare the Gibbs sampler with the Jacobi method, AC-DC, and FFDiag algorithms. Finally, we applied the Gibbs sampler to BSS, CPCA, and CSPA. Note that all Gibbs samplers were monitored for convergence in our experiments."}, {"heading": "4.1. Simulating vector Bingham distribution", "text": "In this experiment, we compare the three sample schemes, repulsion, disc and grid samples, to simulate the vector distributions p (x | A) and exp (xTAx). Randomly, a symmetrical real matrix A with a size of 10 \u00d7 10 was generated, which has 10 different eigenvalues. We calculated the effective sample sizes (ESS) for all methods shown in Table 1. Simulations of the log probability for the three methods are shown in Figure 1. As expected, all values of the log probability lie between the minimum and maximum eigenvalues of A. We conclude that all three schemes work similarly with respect to ESS."}, {"heading": "4.2. Gibbs sampler for joint diagonalization", "text": "In this section we evaluate the proposed Gibbs sampler for the common diagonalization problems on the basis of two data sets. K = 100 applies to the data under consideration. Matrices were generated according to Ck = B\u0432kBT + Ek, where BTB = I and each matrix Ck is size 10 x 10. All diagonal elements of the diagonal matrix \u0439k were generated randomly. For the first data set, matrix B is size 10 x 5. For the second data set, matrix B is size 10 x 10. Since Gaussian errors have been assumed, the matrix Ck must not be symmetrical. All algorithms considered here are designed to search for B and \u0451k. If B is called an estimate of B, we calculate Amari's performance index (API) (Amari et al., 1996) for matrix P = B \u2212 1B \u2212 If P is a mutation of the identity matrix, we calculate the API 1996 (Amari et)."}, {"heading": "4.2.1. The 10\u00d7 5 matrix B", "text": "For the first experiment, when M < N, we compared the performance of the Gibbs sampler based on rejection, slice, and raster sampling for sampling from the matrix LB distribution. We calculated the ESS for the three sampling schemes shown in Table 2. We see that both rejection and stratification sampling are more efficient with respect to ESS. We are now using the raster sampling scheme for the three schemes (see Figure 2), which shows that the raster sampling scheme performs better when looking for the modes of the model. We are now using the raster sampling scheme for the model, while N > M. The raster sampling scheme ESS was very small, suggesting that raster sampling may perform a random gait in the local mode of DC sampling. As our model is similar to the model of (Yeredor, 2002 sampling algorithm) we are comparing DC sampling with the raster."}, {"heading": "4.2.2. The 10\u00d7 10 matrix B", "text": "For the second experiment, we applied grid sampling, AC-DC, Jacobi and FFDiag methods to the dataset in which the matrix B is 10 x 10. The grid sampling scheme was used to sample the matrix B. The API statistics for grid sampling are in Table 4. The minimum API values for AC-DC, Jacobi method and FFDiag are in Table 5. These results show that the proposed method is comparable to the Jacobi method and exceeds both AC-DC and FFDiag in terms of the minimum API. In practice, however, we usually do not know the true B. Therefore, the MAP sample could be used for comparison purposes, which shows that the Gibbs sampler is comparable to the FFDiag and exceeds AC-DC."}, {"heading": "4.3. Applications to BSS", "text": "In this section, we apply the proposed approximate joint diagonalization scheme to a BSS problem. We use the \"ACsin10d\" data from the ICAlab benchmark datasets (http: / / www.bsp.brain.riken.jp / ICALAB /). This dataset contains 10 sine sources labeled S, and each source is scanned with 1000 uniformly distributed datasets. We randomly generated a mixing matrix A of size 10 x 10, and then we created the mixtures with X = AS + E, where E was the Gaussian error realizations with \u03c3 = 0.1. The data X was applied by multiplying the matrix W as defined in (Belouchrani et al., 1997), weighted, and then we obtained 100 covariance matrices C (\u03c4) = E {X (t + B) X (t + B) X (t) T) X (t) X (t) T) formatrix as defined in {0W = 1PI (1PI) and 1PI (1PI = 1PI)."}, {"heading": "4.4. Applications to CPCA and CSPA", "text": "In this example, we show that the Gibbs sampler could be applied to both CPCA and CSPA. Considering the K covariance matrices Ck, CPCA tries to find a common orthogonal matrix B, so that BCkB = \u043ek are diagonal (Flury, 1984). The columns of B are the common main components (CPC). However, in non-trivial problems, it would be unlikely that the \u0394k are exactly diagonal. We use the model (1) with Gaussian errors to infer the CPC. CPCA can be applied to signal processing, such as the CSPA, which has been applied to the pre-processing of EEG data in Brain Computer Interfaces (BCI) (Blankertz et al., 2008). We apply the Gibbs sampler first to a toy data set and then to some EEG data."}, {"heading": "4.4.1. A Synthetic Data", "text": "We use the procedure in (Blankertz et al., 2008) to generate data to prove the Gibbs sampler in CPCA and CSPA. Data for analysis are given according to the following linear mixing modelY = ASjwhere j = 1, 2 are class names, and S1 x N (0, x 1), where S1 = diag (0,1, 0,9) and S2 x N (0, x 2), whereby A2 = diag (0,9, 0,1). Since S1 and S2 come from different distributions, these are data from two classes. We can see that S1 exhibits the greatest deviation in one direction and S2 in the opposite direction. The linear mixing matrix is called A, and the observations Y 1 and 2 are data classified as Class 1 and 2.Essentially, the CSPA patterns are sought, whereby CSPA identifies the common spatial patterns as A, and the data are called Class 1 and 2."}, {"heading": "4.4.2. EEG Data", "text": "The data used in this study corresponds to the EEG dataset I\u03b2 of the BCI competition III (Blankertz et al., 2006). This dataset collects the EEG signals recorded for three subjects who had to perform eigenimages, i.e. for imaginary left or right movements. Therefore, the two classes to be identified were left and right. Before performing the CSPA, we extract characteristics from these EEG signals. We opt for the use of self-performance characteristics (BP) and finally, we have four characteristics for each subject designated as C3-\u03b1, C3-\u03b2, C4-\u03b1 and C4-\u03b2. Therefore, any data point that has four characteristics is designated as one of the two classes. We do the CSPA for these data points. We refer to the two class data as X1 and X2. In the first step, we have the data as usual by using a whitening matrix class W."}, {"heading": "5. Conclusions", "text": "All required conditional distributions are known standard distributions. We compared the Gibbs sampler with some other common diagonalization algorithms and showed that the Gibbs sampler provides the most advanced performance in the small examples we considered. We also applied the algorithm to BSS, CPCA, and CSPS. The Gibbs sampler could be extended to include cases where the diagonalizatioin matrix is not orthogonal."}, {"heading": "Acknowledgments", "text": "MG is grateful for the financial support of the Engineering and Physical Sciences Research Council (EPSRC) UK, the grants EP / J007617 / 1, EP / E052029 / 2, EP / H024875 / 2. MZ is supported by the basic research of the central universities in China."}], "references": [{"title": "A new learning algorithm for blind source separation", "author": ["Amari", "S.-I", "A. Cichocki", "H.H. Yang"], "venue": "In NIPS,", "citeRegEx": "Amari et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Amari et al\\.", "year": 1996}, {"title": "A blind source separation technique based on second order statistics", "author": ["A. Belouchrani", "Meraim", "K. Abed", "Cardoso", "J.-F", "E. Moulines"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "Belouchrani et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Belouchrani et al\\.", "year": 1997}, {"title": "Optimizing spatial filters for robust eeg single-trial analysis", "author": ["B. Blankertz", "R. Tomioka", "S. Lemm", "M.Kawanabe", "K.-R.Mller"], "venue": "IEEE Signal Proc. Magazine,", "citeRegEx": "Blankertz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blankertz et al\\.", "year": 2008}, {"title": "Jacobi angles for simultaneous diagonalization", "author": ["Cardoso", "J.-F", "A. Souloumiac"], "venue": "SIAM J. Mat. Anal. Appl.,", "citeRegEx": "Cardoso et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cardoso et al\\.", "year": 1996}, {"title": "maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Common principal components in k groups", "author": ["B. Flury"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Flury,? \\Q1984\\E", "shortCiteRegEx": "Flury", "year": 1984}, {"title": "Simultaneous similarity of matrices", "author": ["S. Friedland"], "venue": "Adv. in Math.,", "citeRegEx": "Friedland,? \\Q1983\\E", "shortCiteRegEx": "Friedland", "year": 1983}, {"title": "Inference from iterative simulation using multiple sequences", "author": ["A. Gelman", "D. Rubin"], "venue": "Statist. Sci.,", "citeRegEx": "Gelman and Rubin,? \\Q1992\\E", "shortCiteRegEx": "Gelman and Rubin", "year": 1992}, {"title": "Simulation of the matrix Bingham-von Mises-Fisher distribution, with applications to multivariate and relational data", "author": ["P.D. Hoff"], "venue": "J. Comput. Graph. Statist.,", "citeRegEx": "Hoff,? \\Q2009\\E", "shortCiteRegEx": "Hoff", "year": 2009}, {"title": "A hiearchical eigenmodel for pooled covariance estimation", "author": ["P.D. Hoff"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Hoff,? \\Q2009\\E", "shortCiteRegEx": "Hoff", "year": 2009}, {"title": "The von mises-fisher matrix distribution in orientation statistics", "author": ["C.G. Khatri", "K.V. Mardia"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Khatri and Mardia,? \\Q1977\\E", "shortCiteRegEx": "Khatri and Mardia", "year": 1977}, {"title": "The quantitative extraction and topographic mapping of the abnormal components in the clinical EEG. Electroencephalogr", "author": ["Z.J. Koles"], "venue": "Clin. Neurophysiol.,", "citeRegEx": "Koles,? \\Q1991\\E", "shortCiteRegEx": "Koles", "year": 1991}, {"title": "Sampling from compositional and directional distributions", "author": ["A. Kume", "S.G. Walker"], "venue": "Statist. and Comput.,", "citeRegEx": "Kume and Walker,? \\Q2006\\E", "shortCiteRegEx": "Kume and Walker", "year": 2006}, {"title": "Joint approximate diagonalization of positive definite matrices", "author": ["Pham", "D.-T"], "venue": "SIAM J. on Matrix Anal. and Appl.,", "citeRegEx": "Pham and D..T.,? \\Q2001\\E", "shortCiteRegEx": "Pham and D..T.", "year": 2001}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Statist.,", "citeRegEx": "Schwarz,? \\Q1978\\E", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "Nonorthogonal joint diagonalization by combining givens and hyperbolic rotations", "author": ["A. Souloumiac"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "Souloumiac,? \\Q2009\\E", "shortCiteRegEx": "Souloumiac", "year": 2009}, {"title": "Joint diagonalization via subspace fitting techniques", "author": ["A. van der Veen"], "venue": "In ICASSP, pp. 2773\u20132776,", "citeRegEx": "Veen,? \\Q2001\\E", "shortCiteRegEx": "Veen", "year": 2001}, {"title": "Non-orthogonal joint diagonalization in the least-squares sense with application in blind source separation", "author": ["A. Yeredor"], "venue": "IEEE Trans. on Sig. Proc.,", "citeRegEx": "Yeredor,? \\Q2002\\E", "shortCiteRegEx": "Yeredor", "year": 2002}, {"title": "Bayesian methods to detect dye-labelled dna oligonucleotides in multiplexed raman spectra", "author": ["M. Zhong", "M. Girolami", "K. Faulds", "D. Graham"], "venue": "J. R. Statist. Soc. C,", "citeRegEx": "Zhong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2011}, {"title": "A fast algorithm for joint diagonalization with nonorthogonal transformations and its application to blind source separation", "author": ["A. Ziehe", "P. Laskov", "G. Nolte", "Muller", "K-R"], "venue": null, "citeRegEx": "Ziehe et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ziehe et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Belouchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 17, "context": "There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Belouchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 19, "context": "There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Belouchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 5, "context": ", 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 52, "endOffset": 65}, {"referenceID": 11, "context": ", 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al., 2008).", "startOffset": 110, "endOffset": 147}, {"referenceID": 2, "context": ", 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al., 2008).", "startOffset": 110, "endOffset": 147}, {"referenceID": 17, "context": "In this paper we restrict ourselves to the case BB = I, however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009).", "startOffset": 127, "endOffset": 180}, {"referenceID": 19, "context": "In this paper we restrict ourselves to the case BB = I, however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009).", "startOffset": 127, "endOffset": 180}, {"referenceID": 15, "context": "In this paper we restrict ourselves to the case BB = I, however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009).", "startOffset": 127, "endOffset": 180}, {"referenceID": 17, "context": "This model is a generalization of the subspace fitting models of (van der Veen, 2001; Yeredor, 2002), where here the errors Ek are represented explicitly.", "startOffset": 65, "endOffset": 100}, {"referenceID": 17, "context": "Our approach is most similar to the subspace fitting models (van der Veen, 2001; Yeredor, 2002) although we also treat the error variances as parameters to be inferred.", "startOffset": 60, "endOffset": 95}, {"referenceID": 5, "context": "Flurry (Flury, 1984) and Hoff (Hoff, 2009b) proposed CPCA for a series of covariance matrices whose eigenvalues were assumed positive, a restriction which we relax.", "startOffset": 7, "endOffset": 20}, {"referenceID": 4, "context": "Point estimate methods such as the expectationmaximisation (EM) algorithm (Dempster et al., 1977) or the AC-DC algorithm (Yeredor, 2002) could be derived to estimate these parameters.", "startOffset": 74, "endOffset": 97}, {"referenceID": 17, "context": ", 1977) or the AC-DC algorithm (Yeredor, 2002) could be derived to estimate these parameters.", "startOffset": 31, "endOffset": 46}, {"referenceID": 17, "context": "Based on some data, we compared the Gibbs sampler to the Jacobi method (Cardoso & Souloumiac, 1996), AC-DC (Yeredor, 2002) and the FFDiag algorithm (Ziehe et al.", "startOffset": 107, "endOffset": 122}, {"referenceID": 19, "context": "Based on some data, we compared the Gibbs sampler to the Jacobi method (Cardoso & Souloumiac, 1996), AC-DC (Yeredor, 2002) and the FFDiag algorithm (Ziehe et al., 2004).", "startOffset": 148, "endOffset": 168}, {"referenceID": 0, "context": "Across all the results the Gibbs sampler is comparable to both the Jacobi and FFDiag methods whilst outperforming the AC-DC algorithm in terms of the Amari performance index (Amari et al., 1996).", "startOffset": 174, "endOffset": 194}, {"referenceID": 8, "context": "We employ the procedures of Hoff (2009a) to simulate the matrix LB distribution.", "startOffset": 28, "endOffset": 41}, {"referenceID": 0, "context": "If B\u0302 is denoted as the estimation of B, for comparison purposes we calculate Amari\u2019s performance index (API) (Amari et al., 1996) for the matrix P = B\u0302\u22121B.", "startOffset": 110, "endOffset": 130}, {"referenceID": 17, "context": "Since our model is similar to that of (Yeredor, 2002), we compare grid sampling to the AC-DC algorithm.", "startOffset": 38, "endOffset": 53}, {"referenceID": 14, "context": "As the first step to model selection, we used the Gibbs sampling output to approximate the log marginal likelihood of various models using the BIC (Schwarz, 1978).", "startOffset": 147, "endOffset": 162}, {"referenceID": 18, "context": "unbiased estimation schemes) could be used for model selection as in (Zhong et al., 2011)", "startOffset": 69, "endOffset": 89}, {"referenceID": 1, "context": "The data X were whitened by multiplying the matrix W as defined in (Belouchrani et al., 1997) and then we obtain 100 covariance matrices C(\u03c4) = E { X\u0303(t+ \u03c4)X\u0303(t) } .", "startOffset": 67, "endOffset": 93}, {"referenceID": 5, "context": "Given K covariance matrices Ck, CPCA seeks to find a common orthogonal matrix B such that BCkB = \u039bk are diagonal (Flury, 1984).", "startOffset": 113, "endOffset": 126}, {"referenceID": 2, "context": "CPCA can be applied to signal processing, such as the CSPA which has been applied to the preprocessing of EEG data in Brain Computer Interfaces (BCI) (Blankertz et al., 2008).", "startOffset": 150, "endOffset": 174}, {"referenceID": 2, "context": "We employ the procedure in (Blankertz et al., 2008) to generate data in demonstrating the Gibbs sampler in CPCA and CSPA.", "startOffset": 27, "endOffset": 51}], "year": 2012, "abstractText": "We present a Bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. A Gibbs sampler is derived to simulate samples of the common eigenvectors and the eigenvalues for these matrices. Several synthetic examples are used to illustrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms, which shows that the Gibbs sampler achieves the state-of-theart performance on the examples considered. As a byproduct, the output of the Gibbs sampler could be used to estimate the log marginal likelihood, however we employ the approximation based on the Bayesian information criterion (BIC) which in the synthetic examples considered correctly located the number of common eigenvectors. We then succesfully applied the sampler to the source separation problem as well as the common principal component analysis and the common spatial pattern analysis problems.", "creator": "LaTeX with hyperref package"}}}