{"id": "1206.4667", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation", "abstract": "Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.", "histories": [["v1", "Mon, 18 Jun 2012 15:33:05 GMT  (377kb)", "http://arxiv.org/abs/1206.4667v1", "ICML2012"], ["v2", "Wed, 18 Jul 2012 18:54:06 GMT  (84kb,D)", "http://arxiv.org/abs/1206.4667v2", "ICML2012, fixed citations to use correct tech report number"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IR", "authors": ["kendrick boyd", "jesse davis", "david page", "v\u00edtor santos costa"], "accepted": true, "id": "1206.4667"}, "pdf": {"name": "1206.4667.pdf", "metadata": {"source": "META", "title": "Unachievable Region in Precision-Recall Spaceand Its Effect on Empirical Evaluation", "authors": ["Kendrick Boyd"], "emails": ["boyd@cs.wisc.edu", "vsc@dcc.fc.up.pt", "jesse.davis@cs.kuleuven.be", "page@biostat.wisc.edu"], "sections": [{"heading": "1. Introduction", "text": "The PR curves illustrate the trade-off between the proportion of positive examples that are truly positive (precision) as a function of the proportion of correctly classified positives (recall) Davis Davis. In particular, PR analysis is preferred to ROC analysis when there is a large skew in the class distribution. In this situation, even a relatively low false positive rate can produce a large number of false positives and thus low precision (Davis & Goadrich, 2006). Many applications arearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner is explicitly characterized by a large skew in the class distribution. In information retrieval (IR), only a few documents are relevant to a given query. In medical diagnoses, only a small percentage of the population has a specific disease at any given time."}, {"heading": "2. Achievable and Unachievable Points in PR Space", "text": "Finally, in Theorems 1 and 2, we present our central theoretical contributions formalizing the notion of the unreachable region in the PR space. We assume familiarity with precision, callback, and confusion matrices (see Davis and Goadrich (2006) for an overview). We use p for precision, r for retrieval, and tp, fp, fn, tn for the number of true positives, false pos-1To be absolutely true in the ROC space, fractions must be allowed for tp, fp, fn, tn. Fractions can be considered in an advanced data set.itives, false negatives, and true negatives, and respectably. Consider a dataset D with n = pos + neg examples, where pos is the number of positive examples, and neg the number of negative examples."}, {"heading": "2.1. Unachievable Points in PR Space", "text": "It is easy to show that, as in the ROC space, any valid confusion matrix, where tp > 0 defines a single and unique point in the PR space. In the PR space, both retrieval and precision depend on the tp cell of the confusion matrix, as opposed to the true positive and false positive rate used in the ROC space. This dependence, together with the fact that a particular dataset contains a fixed number of negative and positive examples, limits the precision that is possible for a particular memory. To illustrate this effect, we consider a dataset with pos = 100 and neg = 200. Table 1 (a) shows a valid confusion matrix with r = 0.2 and p = 0.2. Let's keep the accuracy constant and at the same time increase the recollection."}, {"heading": "2.2. Unachievable Region in PR Space", "text": "Theorem 1 has a limitation that every reachable point in the PR space must meet. Unlike a given skew = 1, there are many unreachable points, and we refer to this collection of points as the unreachable region of the PR space. This section examines the characteristics of the unreachable region. (1) There are no assumptions about the performance of a model. Let's consider a model that gives the worst possible ranking, in which every negative example is rankedahead of any positive example. Building a PR curve based on this ranking means setting a PR point to (0, 0) and a second PR point to (1, posn). Davis and Goadrich (2006) provide the correct method for interpolation between points in the PR space; interpolation is not linear in the PR space, but linear between the corresponding points in the ROC space. Interpolation between the two known points gives intermediate points with recall ri = i n and precision."}, {"heading": "3. PR Space Metrics that Account for Unachievable Region", "text": "The unreachable region represents a lower limit on AUCPR, and it is important to develop evaluation metrics that take this into account. We believe that any metric A \u00b2 that replaces AUCPR should meet at least the following two characteristics. First, A \u00b2 should refer to AUCPR. Suppose AUCPR was used to measure the performance of the classifiers C1,.., Cn on a single test set. If AUCPR (Ci, testD) > AUCPR (Cj, testD), then A \u00b2 (Ci, testD) > A \u2032 (Cj, testD), as a test set, the skew of testD affects each model equally. Note that this property may not be appropriate or desirable when summarizing results across multiple sets of tests, as in cross-validation, because each test set may have a different skew."}, {"heading": "4. Discussion and Recommendations", "text": "We believe that all practitioners who use PR space-based assessment results (e.g. PR curves, AUCPR, AP, F1) should be aware of the unreachable region and its potential impact on their analyses. Visually looking at the PR curve or looking at an AUCPR value often results in an intuitive sense of the quality of an algorithm or the difficulty of a task or a dataset. If the imbalance is extremely large, the effect of the very small, unreachable region is negligible for PR analysis. However, there are many cases where the imbalance is closer to 0.5 and the unreachable range is not insignificant. At \u03c0 = 0.1, AUCPRMIN \u2248 0.05, and it increases the closer it gets to 0.5. AUCPR is used in many applications where there is a context in which the impact on PR curves exists and in which the PR approach becomes increasingly clear."}, {"heading": "4.1. Aggregation for Cross-Validation", "text": "In fact, it is the case that we will be able to get to grips with the problems mentioned in order to solve them."}, {"heading": "4.2. Aggregation among Different Tasks", "text": "This setting differs from cross-validation because each task is not assumed to have the same underlying distribution; while the tasks may be unrelated (Tang et al., 2009), they often come from the same domain. For example, the tasks could be the truth values of different predicates in a relational domain (Kok & Domingos, 2010; Mihalkova & Mooney, 2007) or different queries in an IR environment (Manning et al., 2008). Often, researchers report a single, aggregated score by averaging the results in the different tasks. However, the tasks can have potentially very different skews, and thus different minimal AUCPR values that provide (some) control for Skew is preferred to the average AUCPR. In SRL, researchers often evaluate algorithms by setting the average AUCPR predicates over a variety of tasks in a single dataset."}, {"heading": "4.3. Downsampling", "text": "Stamping is common when learning from highly distorted tasks. Stamping often alters the skew of the traction set (e.g. by subsampling the negatives to facilitate learning, or using data from case control studies) so that it does not reflect the true skew. PR analyses are often applied to the sampled data sets (Sonnenburg et al., 2006; Natarajan et al., 2011; Sutskever et al., 2010).The sensitivity of AUCPR and the values associated with it makes it important to identify and, if possible, quantify the effects of stamping on evaluation metrics. The different size of the unreachable region provides an explanation and quantification of some dependencies of PR curves and AUCPR on slope. Therefore, AUCNPR, which adapts to the unattainable region, should be more stable than AUCPR on changes in the skew."}, {"heading": "1:1 0.851 0.785 0.330 0.316", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1:2 0.740 0.680 0.329 0.315", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1:3 0.678 0.627 0.343 0.329", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1:4 0.701 0.665 0.314 0.299", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1:5 0.599 0.560 0.334 0.320", "text": "Table 3 shows the values of AUCPR and AUCNPR of a test set that has been sampled to the same angle as the tension set and to the original (i.e. unsampled) test set. AUCNPR has a smaller variance than AUCPR. However, there is still a significant difference between the values of the downsampled test set and the original test set. As expected, the difference increases when the ratio is 1: 1 negative. At this ratio, even the AUCNPR value on the downsampled data is more than twice as high as the value on the original slope. This is a huge difference and it is disconcerting that it is obtained simply by changing the slope of the dataset. An interesting area for future research is the study of scoring metrics that are either less sensitive to slopes or allow simple and accurate transformations that facilitate comparisons between different slopes."}, {"heading": "4.4. F1 Score", "text": "A commonly used evaluation metric for a single point in the PR space is the F\u03b2 family, F\u03b2 = (1 + \u03b22) pr2p + rwhere \u03b2 > 0 is a parameter for controlling the relative importance of retrieval and precision (Manning et al., 2008). However, the most commonly used parameter is the F1 score (\u03b2 = 1), which is the harmonious mean of precision and retrieval. We focus our discussion on the F1 score, but similar analyses apply to F\u03b2. \"Figure 4 shows contours of the F1 score across the PR space. While the unreachable region of the PR space does not set limits on the F1 score based on skew, there is still a subtle interaction between skew and F1. Since F1 combines precision and retrieval in a single score, it inevitably loses information. One aspect of this information loss is that PR points with the same F1 score have very different relationships with the unreachable region. Viewing each A, each of 45 points has a B and a very different score."}, {"heading": "5. Conclusion", "text": "By accurately describing this unreachable region in Theorems 1 and 2, we deepen our understanding of the effects of downsampling and the impact of the minimal PR curve on F measurement and score aggregation."}, {"heading": "Acknowledgments", "text": "We thank Jude Shavlik and the anonymous reviewers for their insightful comments and suggestions, and Stanley Kok for providing the results of the LSM algorithm. We thank you for our financial support. KB is funded by NIH 5T15LM007359. VC by ERDF by Progr. COMPETE, the Portuguese government by FCT, proj. HORUS ref. PTDC / EIA-EIA / 100897 / 2008, and the EU Sev. Fram. Progr. FP7 / 2007-2013 by Progr. 288147. JD by Forschungsfonds K.U. Leuven (CREA / 11 / 015 and OT / 11 / 051), EU FP7 Marie Curie Career Integration Grant (# 294068) and FWO-Vlaanderen (G.0356.12). DP by NIGMS grant R01GM097618-01, NLgrant 0R0400-Career Integration Grant Center (G.0356.12)."}], "references": [{"title": "Nonparametric estimation of the precision-recall curve", "author": ["S. Cl\u00e9men\u00e7on", "N. Vayatis"], "venue": "ICML", "citeRegEx": "Cl\u00e9men\u00e7on and Vayatis,? \\Q2009\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on and Vayatis", "year": 2009}, {"title": "The relationship between precision-recall and ROC curves", "author": ["J. Davis", "M. Goadrich"], "venue": "ICML", "citeRegEx": "Davis and Goadrich,? \\Q2006\\E", "shortCiteRegEx": "Davis and Goadrich", "year": 2006}, {"title": "An integrated approach to learning Bayesian networks of rules", "author": ["J. Davis", "E. Burnside", "I. Dutra", "C.D. Page", "V. Costa"], "venue": "In Machine Learning: ECML", "citeRegEx": "Davis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2005}, {"title": "Apples-to-apples in crossvalidation studies: pitfalls in classifier performance measurement", "author": ["G. Forman", "M. Scholz"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "Forman and Scholz,? \\Q2010\\E", "shortCiteRegEx": "Forman and Scholz", "year": 2010}, {"title": "Gleaner: creating ensembles of first-order clauses to improve recall-precision curves", "author": ["M. Goadrich", "L. Oliphant", "J. Shavlik"], "venue": "Machine Learning,", "citeRegEx": "Goadrich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goadrich et al\\.", "year": 2006}, {"title": "Predicting outcome for collaborative featured article nomination in wikipedia", "author": ["M. Hu", "E. Lim", "R. Krishnan"], "venue": "In International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Learning Markov logic networks using structural motifs", "author": ["S. Kok", "P. Domingos"], "venue": "ICML", "citeRegEx": "Kok and Domingos,? \\Q2010\\E", "shortCiteRegEx": "Kok and Domingos", "year": 2010}, {"title": "Temporal information extraction", "author": ["X. Ling", "D.S. Weld"], "venue": "AAAI", "citeRegEx": "Ling and Weld,? \\Q2010\\E", "shortCiteRegEx": "Ling and Weld", "year": 2010}, {"title": "Comparing evaluation metrics for sentence boundary detection", "author": ["Y. Liu", "E. Shriberg"], "venue": "In ICASSP 2007,", "citeRegEx": "Liu and Shriberg,? \\Q2007\\E", "shortCiteRegEx": "Liu and Shriberg", "year": 2007}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schtze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Bottom-up learning of Markov logic network structure", "author": ["L. Mihalkova", "R.J. Mooney"], "venue": "ICML", "citeRegEx": "Mihalkova and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Mihalkova and Mooney", "year": 2007}, {"title": "Gradient-based boosting for statistical relational learning: The relational dependency network case", "author": ["S. Natarajan", "T. Khot", "K. Kersting", "B. Gutmann", "J. Shavlik"], "venue": "Machine Learning,", "citeRegEx": "Natarajan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2011}, {"title": "ARTS: accurate recognition of transcription starts in human", "author": ["S. Sonnenburg", "A. Zien", "G. R\u00e4tsch"], "venue": "Bioinformatics (Oxford, England),", "citeRegEx": "Sonnenburg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J. Tenenbaum"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2010}, {"title": "SVMs modeling for highly imbalanced classification. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["Y. Tang", "Y. Zhang", "N.V. Chawla", "S. Krasser"], "venue": "IEEE Transactions on,", "citeRegEx": "Tang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2009}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "SIGIR", "citeRegEx": "Yue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "For example, IR systems are frequently judged by their mean average precision, or MAP (not to be confused with the same acronym for \u201cmaximum a posteriori\u201d), which is an approximation of the mean AUCPR over the queries (Manning et al., 2008).", "startOffset": 218, "endOffset": 240}, {"referenceID": 2, "context": "Similarly, AUCPR often serves as an evaluation criteria for statistical relational learning (SRL) (Kok & Domingos, 2010; Davis et al., 2005; Sutskever et al., 2010; Mihalkova & Mooney, 2007) and information extraction (IE) (Ling & Weld, 2010; Goadrich et al.", "startOffset": 98, "endOffset": 190}, {"referenceID": 13, "context": "Similarly, AUCPR often serves as an evaluation criteria for statistical relational learning (SRL) (Kok & Domingos, 2010; Davis et al., 2005; Sutskever et al., 2010; Mihalkova & Mooney, 2007) and information extraction (IE) (Ling & Weld, 2010; Goadrich et al.", "startOffset": 98, "endOffset": 190}, {"referenceID": 4, "context": ", 2010; Mihalkova & Mooney, 2007) and information extraction (IE) (Ling & Weld, 2010; Goadrich et al., 2006).", "startOffset": 66, "endOffset": 108}, {"referenceID": 15, "context": "Additionally, some algorithms, such as SVM-MAP (Yue et al., 2007) and SAYU (Davis et al.", "startOffset": 47, "endOffset": 65}, {"referenceID": 2, "context": ", 2007) and SAYU (Davis et al., 2005), explicitly optimize the AUCPR of the learned model.", "startOffset": 17, "endOffset": 37}, {"referenceID": 1, "context": "We assume familiarity with precision, recall, and confusion matrices (see Davis and Goadrich (2006) for an overview).", "startOffset": 74, "endOffset": 100}, {"referenceID": 1, "context": "Davis and Goadrich (2006) provide the correct method for interpolating between points in PR space; interpolation is non-linear in PR space but is linear between the corresponding points in ROC space.", "startOffset": 0, "endOffset": 26}, {"referenceID": 11, "context": ", by subsampling the negative examples (Natarajan et al., 2011; Sutskever et al., 2010)) increases the minimum AUCPR by approximately 0.", "startOffset": 39, "endOffset": 87}, {"referenceID": 13, "context": ", by subsampling the negative examples (Natarajan et al., 2011; Sutskever et al., 2010)) increases the minimum AUCPR by approximately 0.", "startOffset": 39, "endOffset": 87}, {"referenceID": 5, "context": "1 (Hu et al., 2009; Sonnenburg et al., 2006; Liu & Shriberg, 2007).", "startOffset": 2, "endOffset": 66}, {"referenceID": 12, "context": "1 (Hu et al., 2009; Sonnenburg et al., 2006; Liu & Shriberg, 2007).", "startOffset": 2, "endOffset": 66}, {"referenceID": 14, "context": "While the tasks may be unrelated (Tang et al., 2009), often they come from the same domain.", "startOffset": 33, "endOffset": 52}, {"referenceID": 9, "context": "For example, the tasks could be the truth values of different predicates in a relational domain (Kok & Domingos, 2010; Mihalkova & Mooney, 2007) or different queries in an IR setting (Manning et al., 2008).", "startOffset": 183, "endOffset": 205}, {"referenceID": 6, "context": "Results are for the LSM algorithm from Kok and Domingos (2010). The range of scores shows the difficulty and skews of the prediction tasks vary greatly.", "startOffset": 39, "endOffset": 63}, {"referenceID": 12, "context": "PR analysis is frequently used on the downsampled data sets (Sonnenburg et al., 2006; Natarajan et al., 2011; Sutskever et al., 2010).", "startOffset": 60, "endOffset": 133}, {"referenceID": 11, "context": "PR analysis is frequently used on the downsampled data sets (Sonnenburg et al., 2006; Natarajan et al., 2011; Sutskever et al., 2010).", "startOffset": 60, "endOffset": 133}, {"referenceID": 13, "context": "PR analysis is frequently used on the downsampled data sets (Sonnenburg et al., 2006; Natarajan et al., 2011; Sutskever et al., 2010).", "startOffset": 60, "endOffset": 133}, {"referenceID": 2, "context": "To explore this, we used SAYU (Davis et al., 2005) to learn a model for the advisedBy task in the", "startOffset": 30, "endOffset": 50}, {"referenceID": 9, "context": "where \u03b2 > 0 is a parameter to control the relative importance of recall and precision (Manning et al., 2008).", "startOffset": 86, "endOffset": 108}], "year": 2012, "abstractText": "Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.", "creator": "LaTeX with hyperref package"}}}