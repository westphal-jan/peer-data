{"id": "1206.4668", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Approximate Principal Direction Trees", "abstract": "We introduce a new spatial data structure for high dimensional data called the \\emph{approximate principal direction tree} (APD tree) that adapts to the intrinsic dimension of the data. Our algorithm ensures vector-quantization accuracy similar to that of computationally-expensive PCA trees with similar time-complexity to that of lower-accuracy RP trees.", "histories": [["v1", "Mon, 18 Jun 2012 15:33:25 GMT  (880kb)", "http://arxiv.org/abs/1206.4668v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["mark mccartin-lim", "andrew mcgregor 0001", "rui wang 0003"], "accepted": true, "id": "1206.4668"}, "pdf": {"name": "1206.4668.pdf", "metadata": {"source": "META", "title": "Approximate Principal Direction Trees", "authors": ["Mark McCartin-Lim", "Andrew McGregor", "Rui Wang"], "emails": ["markml@cs.umass.edu", "mcgregor@cs.umass.edu", "ruiwang@cs.umass.edu"], "sections": [{"heading": null, "text": "APD trees use a small number of PowerMethod iterations to find partitioning levels for recursive partitioning of data, so they represent a natural trade-off between runtime and accuracy achieved by RP and PCA trees. Our theoretical results ensure a) strong performance regardless of the convergence rate of the power method, and b) that O (log d) iterations are sufficient to establish the warranty of PCA trees when the intrinsic dimension is d. We demonstrate this trade-off and the effectiveness of our data structure both on the CPU and on the GPU."}, {"heading": "1. Introduction", "text": "This year it is so far that it will only take a few days to reach an agreement."}, {"heading": "2. Preliminary Definitions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Average Diameter", "text": "For a given set of points, we measure their similarity in terms of the average distance between the points within the set. We will use the following notation: Definition 2.1 (Average diameter of a set of points)."}, {"heading": "2.2. Local Covariance Dimension", "text": "Several possible definitions of the intrinsic dimension are discussed in (Verma et al., 2009).The one they use is in1See, e.g. (Dasgupta & Freund, 2008).Their analysis of RP trees and PCA trees, local covariance dimension, is based on a statistical representation of the data and is therefore well suited for modelling data from machine learning problems. We analyze APD trees using the same definition.Remember that the covariance matrix C-RD-D of a series of points S = {x1,.., xn-RD with mean (S) = 0, can be written as C-XTX, where X-Rn \u00b7 D is the matrix whose i-th line corresponds to xi. The idea behind the local covariance dimension is as follows. The eigenvectors of the covariance matrix form an ormal basis for the data."}, {"heading": "3. The APD Construction", "text": "Before discussing the partitioning rule used in APD trees, we review a general template for possible rules. This template is presented in Algorithm 1. If we assume that S corresponds to the set of points assigned to a particular node in the tree, then it is obvious that this meta-algorithm can be applied recursively to create a balanced binary meeting point. If S is considered an \"outlier,\" we use a sphere to partition points that are close to the center of the data away from those that are not. We will discuss this case further in Section 3.2. Otherwise, we will use a hyperplane to partition the data. If S is considered an outlier, we will use a sphere to partition points that are close to the center of the data, away from those that are not."}, {"heading": "3.1. The New Splitting Rule", "text": "The new splitter rule that we propose allows us to achieve an accuracy similar to that of the PCA rule without the computational effort. It is based on the power method (Burden & Faires, 2010), a well-known technique for approximating eigenvectors. See Algorithm 2. The technique translates nicely into a parallel algorithm that we can implement on the GPU, as we will see in Section 5.Theorem 3.1. For i [D], we leave pi-RD and i > 0 the (normalized) eigenvectors and eigenvalues of the ecovariance matrix C. If s = D i = 1 \u03b2ipi is the initial vector used in the APD splitter rule, then the vector that is isp = Cts-Cts-Cts-Cs-Cs-T-T-T-T-T-T-T-T-T-T-T-T-T becomes the vector that is defined by the splitter rule."}, {"heading": "3.2. Fast outlier detection", "text": "The tree construction meta algorithm has a special case because S contains outliers, i.e. if the average distance between the points is significantly smaller than the maximum distance between two points, this is important because even if we would find a good hyperplane to divide our data, the average diameter of the resulting partitions is still influenced by the outlier. Following (Dasgupta & Freund, 2008; Verma et al., 2009) we say that S has outliers if the maximum depends on the relative size of the average diameter and the maximum diameter: Definition 3.1 (outlier). For a user-definable parameter c > 0, we say that S contains outliers if \u03942 (S) > c \u00b2 a (S), where the maximum diameter depends on the relative size of the average diameter and the maximum diameter: Definition 3.1 (outlier)."}, {"heading": "3.3. Comparison between splitting rules", "text": "We can now specify our main theoretical result for the APD splitting rule and compare it with the analog results for RP and PCA trees. Henceforth, we assume that there are no outliers, i.e. there are constants c1, c2, (0, 1) in such a way that if S has a local covariance dimension (d, c1): 1. RP rule: If p is selected uniformly randomly from the unity sphere in RD, then E [\u2206 2a (S1, S2)] < (1 \u2212 c2 / d) has such a covariance dimension (d, c1). 2. PCA rule: p is the main property vector of i. (S1, S2) & lt."}, {"heading": "4. Theoretical Analysis of APD trees", "text": "To prove this, we must quantity V (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p) = 1 x (S, p)."}, {"heading": "5. Experimental Results", "text": "We compare the quality of APD trees with those of RRP trees and PCA trees by measuring vector quantification errors (VQ errors). In vector quantification, the goal is to capture all vectors (or points) in a certain number of representative vectors (or points). This may have happened with a spatial partitioning tree belonging to each partition, namely by the average of the individual points in that partition. Consequently, the VQ error is defined as an average square error. In particular, when S1, S2, S2, S6, S6, S3, S4, S4, S5, S5, S5, S5, S5, S6, S6, S6, S6, S6, S6, S6, S6, S6, S6, S6, S6, 7, 7, 7, 7, S7, 7, S7, 7, S7, 7, 7, S7, 7, S7, 7, 7, S7, 7, S7, 7, S7, 7, 7, S7, 7, 7, S7, 7, 7, S6, 7, 7, 7, S6, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, S6, S6, 7, 7, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, 7, 7, 7, 7, S6, 7, 7, 7, 7, S6, 7, 7, 7, 7,"}, {"heading": "6. Conclusion", "text": "We introduced the APD tree, a new spatial data structure for high-dimensional data. APD trees use a small number of power siterations to achieve computing efficiency (comparable to RP trees) and high quality (comparable to PCA trees).The approach is insensitive to the convergence properties of the power method and is well suited for GPU computation."}, {"heading": "Acknowledgments", "text": "This work is partially supported by NSF grants CCF0953754 and CCF-1025120."}], "references": [{"title": "The hardness of k-means clustering", "author": ["Dasgupta", "Sanjoy"], "venue": "Technical Report CS2008-0916,", "citeRegEx": "Dasgupta and Sanjoy.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Sanjoy.", "year": 2008}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Dasgupta", "Sanjoy", "Freund", "Yoav"], "venue": "In STOC,", "citeRegEx": "Dasgupta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2008}, {"title": "A probabilistic theory of pattern recognition", "author": ["L. Devroye", "L. Gyorfi", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 1996}, {"title": "Learning the structure of manifolds using random projections", "author": ["Freund", "Yoav", "Dasgupta", "Sanjoy", "Kabra", "Mayank", "Verma", "Nakul"], "venue": "In NIPS, pp", "citeRegEx": "Freund et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2008}, {"title": "Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees", "author": ["D.T. Lee", "C.K. Wong"], "venue": "Acta Informatica,", "citeRegEx": "Lee and Wong,? \\Q1977\\E", "shortCiteRegEx": "Lee and Wong", "year": 1977}, {"title": "Power iteration clustering", "author": ["Lin", "Frank", "Cohen", "William W"], "venue": "In ICML, pp", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "A note on a method for generating points uniformly on n-dimensional spheres", "author": ["M.E. Muller"], "venue": "Comm. Assoc. Comput. Mach.,", "citeRegEx": "Muller,? \\Q1958\\E", "shortCiteRegEx": "Muller", "year": 1958}, {"title": "Which spatial partition trees are adaptive to intrinsic dimension", "author": ["Verma", "Nakul", "Kpotufe", "Samory", "Dasgupta", "Sanjoy"], "venue": "In UAI, pp", "citeRegEx": "Verma et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Verma et al\\.", "year": 2009}, {"title": "When rank trumps precision: Using the power method to compute Google\u2019s PageRank", "author": ["R.S. Wills"], "venue": "PhD thesis, North Carolina State University,", "citeRegEx": "Wills,? \\Q2007\\E", "shortCiteRegEx": "Wills", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "This is shown to be true even when the intrinsic dimensionality of the data is low (Verma et al., 2009).", "startOffset": 83, "endOffset": 103}, {"referenceID": 7, "context": "Recently, random-projection (RP) trees (Dasgupta & Freund, 2008) and PCA trees (Verma et al., 2009) have been proposed as alternatives to k-d trees that adapt to intrinsic dimensionality.", "startOffset": 79, "endOffset": 99}, {"referenceID": 7, "context": "However, PCA trees perform significantly better than RP trees at reducing the average diameter, as demonstrated in (Verma et al., 2009).", "startOffset": 115, "endOffset": 135}, {"referenceID": 8, "context": ", Google\u2019s PageRank (Wills, 2007)) and for spectral clustering (Lin & Cohen, 2010).", "startOffset": 20, "endOffset": 33}, {"referenceID": 7, "context": "Local Covariance Dimension Several possible definitions of intrinsic dimension are discussed in (Verma et al., 2009).", "startOffset": 96, "endOffset": 116}, {"referenceID": 2, "context": "The tree produced by this meta-algorithm is a hybrid of a BSP tree and a sphere tree (Devroye et al., 1996).", "startOffset": 85, "endOffset": 107}, {"referenceID": 7, "context": "Following (Dasgupta & Freund, 2008; Verma et al., 2009), we say that S has outliers if the maximum depends on the relative size of the average diameter and the maximum diameter: Definition 3.", "startOffset": 10, "endOffset": 55}, {"referenceID": 7, "context": "The following results were shown in (Dasgupta & Freund, 2008) and (Verma et al., 2009).", "startOffset": 66, "endOffset": 86}, {"referenceID": 7, "context": "Specifically, if we can prove a lower bound for V (S, p) when p is chosen according to the APD splitting rule, then we can appeal to the following variant of a proposition from (Verma et al., 2009).", "startOffset": 177, "endOffset": 197}, {"referenceID": 6, "context": "renormalizing (Muller, 1958); and b) the square of a variable with standard normal distribution has the \u03c7distribution with one degree of freedom.", "startOffset": 14, "endOffset": 28}, {"referenceID": 3, "context": "We try to closely replicate the experiments done in (Freund et al., 2008), using the same kind of datasets and the same parameters.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "As in (Freund et al., 2008), the synthetic dataset consists of 10,000 points, each a 1,000-d vector and generated as follows: choose a peak value p uniformly randomly from [0, 1], and then generates the coordinates of the point from the normal distribution N(p, 1).", "startOffset": 6, "endOffset": 27}], "year": 2012, "abstractText": "We introduce a new spatial data structure for high dimensional data called the approximate principal direction tree (APD tree) that adapts to the intrinsic dimension of the data. Our algorithm ensures vector-quantization accuracy similar to that of computationally-expensive PCA trees with similar time-complexity to that of loweraccuracy RP trees. APD trees use a small number of powermethod iterations to find splitting planes for recursively partitioning the data. As such they provide a natural trade-off between the running-time and accuracy achieved by RP and PCA trees. Our theoretical results establish a) strong performance guarantees regardless of the convergence rate of the powermethod and b) that O(log d) iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is d. We demonstrate this trade-off and the efficacy of our data structure on both the CPU and GPU.", "creator": "LaTeX with hyperref package"}}}