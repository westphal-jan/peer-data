{"id": "1206.4669", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Sparse Additive Functional and Kernel CCA", "abstract": "Canonical Correlation Analysis (CCA) is a classical tool for finding correlations among the components of two random vectors. In recent years, CCA has been widely applied to the analysis of genomic data, where it is common for researchers to perform multiple assays on a single set of patient samples. Recent work has proposed sparse variants of CCA to address the high dimensionality of such data. However, classical and sparse CCA are based on linear models, and are thus limited in their ability to find general correlations. In this paper, we present two approaches to high-dimensional nonparametric CCA, building on recent developments in high-dimensional nonparametric regression. We present estimation procedures for both approaches, and analyze their theoretical properties in the high-dimensional setting. We demonstrate the effectiveness of these procedures in discovering nonlinear correlations via extensive simulations, as well as through experiments with genomic data.", "histories": [["v1", "Mon, 18 Jun 2012 15:34:07 GMT  (275kb)", "http://arxiv.org/abs/1206.4669v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sivaraman balakrishnan", "kriti puniyani", "john d lafferty"], "accepted": true, "id": "1206.4669"}, "pdf": {"name": "1206.4669.pdf", "metadata": {"source": "META", "title": "Sparse Additive Functional and Kernel CCA", "authors": ["Sivaraman Balakrishnan", "Kriti Puniyani", "John Lafferty"], "emails": ["sbalakri@cs.cmu.edu", "kpuniyan@cs.cmu.edu", "lafferty@galton.uchicago.edu"], "sections": [{"heading": "1. Introduction", "text": "It is a classical method for searching for correlations between the components of two random vectors X-Rp1 and Y-Rp2."}, {"heading": "2. Sparse additive kernel CCA", "text": "We will now deal with the question of whether we will be able to create a reproductive system in which we can rely on a reproductive system. We assume that E [fj] = 0 and E [gj] = 0 for all fj [fj], and gj [fj] [fj] s that in practice we will always work with centered gram matrices (see Bach & Jordan (2003)., Denote of F = 1 fj [fj]."}, {"heading": "3. Sparse additive functional CCA", "text": "We formulate an optimization problem with respect to sparse additional CCA (SA-FCCA), and from this we derive a scalable backfitting procedure for this problem. Here, we work directly via the Hilbert ranges L2 (x) and L2 (y). We will consider the subspace of \u00b5 (xj) as measurable functions with the usual internal product. < fj > = E (fj) f \"j\" j \"j\" j \"j\" j \"j\" j \"j\" j \"j.\" and similar to Tk \"for the functions of y.\" To force the functionality, we consider the functions in a second order. We further assume that the functions are uniformly limited, and the measures are supported by a compact subset of Euclidean space with Lebesgue measurements. For a fixed uniform base with reference to haveFj. \""}, {"heading": "4. Marginal Thresholding", "text": "The above formulations of SA-KCCA and SA-FCCA are not jointly convex, but biconvex. Therefore, iterative optimization algorithms cannot be guaranteed to achieve the globally optimal solution. To solve this problem, we first perform the algorithms without sparsity limitations. The resulting non-sparse function collections are then used as initializations for the algorithm that includes sparsity penalties. While such initialization works well with small dimension problems, the performance of the estimator decreases (Figure 1). In order to extend the algorithms to the high-dimensional scenario 2-2, we propose marginal thresholds 2 as a screening method to reject irrelevant variables and to apply the SA-FCCA and SA-KCCA models to the reduced dimensionality problem (Figure 1). In order to extend the algorithms to the high-dimensional scenario 2-2-2, we propose marginal thresholds 2-2 as a screening method to reject irrelevant variables 2-2-2 and to apply the SA-FCCA and SA-KCCA models to the reduced dimensionality problem 2-2-2-2-2-2. To extend the algorithms to the high-dimensionality scenario 2-2-2, we propose 2-2-threshold values as a screening method to reject irrelevant variables 2-2-2-2-2-2 and apply the SA-FCCA and SA-2-2-2-2-2-2 models to the reduced dimensionality problem 2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-1-1-1-1-1-2-2-1-1-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-"}, {"heading": "5. Main theoretical results", "text": "The theoretical characterization of these procedures, however, is based on uniform large variations in the covariance between functions. In this section, we will assume all univariate spaces in which the univariate spaces are identical. In the case of RKHS, we limit our attention to functions in a constant radius in the Hilbert space associated with a reproducing kernel K. In the functional case, univariate space is a second order in which the integrative area of the second derivative is limited by a constant. With some misuse of notation, we will denounce these spaces. We are interested in controlling the quantitative spaces."}, {"heading": "6. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Non-linear correlations", "text": "We compare SA-FCCA and SA-KCCA with two models, sparsely additive linear CCA (SCCA) (Witten et al., 2009) and Kernel CCA (KCCA) (Bach & Jordan, 2003).Figure 1 shows the performance of each model when running on data with n = 150 samples in p1 = 15, p2 = 15 dimensions in which only one relevant variable is present in X and Y (the remaining dimensions are Gaussian random noise).We report on two metrics to measure whether the correct correlations are captured by the various methods.SinceKCCA uses all the data dimensions to find correlations using the estimated functions and b) precision and callback in identifying the correct variables involved in the correlation estimation.Each result is averaged over 10 repetitions of the experiment.CCCCA uses all the data dimensions to find correlations, its precision and callback corrections in identifying the correct variables involved in the correlation between the correlation between both CCA and the relevant corte.If both methods are high, SinceKCCA uses all the data dimensions to find correlations, its accuracy and its callback-back-back-back-back-back-back-back-back-back-back-back correlations will be high."}, {"heading": "6.2. Marginal thresholding", "text": "We now test the efficiency of the threshold by conducting an experiment for n = 150, p1 = 150, p2 = 150. We generate several relevant variables such as: fi (Xi) = cos (\u03c02 Xi), i {1, 3}, fi (Xi) = X2i, i {2, 4} Yj = 4 \u2211 i = 1; i6 = jfi (Xi) + N (0, 0.12) j. We repeat the experiment by generating data ten times and the results in Table 2. The bandwidth in the different methods was determined using a uniform distribution and standardized before calculating fi (Xi). Each fi (Xi) is standardized before calculating Yj. We repeat the experiment by dividing data in Table 2. CCC. The bandwidth in the different methods was selected using a plug-in estimator for the mean distance between the points in a single dimension."}, {"heading": "6.3. Application to DLBCL data", "text": "We applied our nonlinear CCA models to a dataset of comparative genomic hybridization (CGH) and gene expression measurements from 203 diffuse large B-cell lymphomas (DLBCL) (Lenz, 2008). We received 1500 CGH measurements from chromosomes 1 of the data and 1500 gene expression measurements from genes on chromosomes 1 and 2 of the data. Data were standardized and winsorized so that the data is within twice the mean absolute deviation. We found that the model recorded interesting nonlinear relationships between CGH and gene expression data to reduce the dimensionality of the problem, and then performed SA-FCCA. Permutation tests were used to select a suitable bandwidth and economy parameter, as described in Witten et al. (2009). We found that the model had interesting nonlinear relationships between CGH and gene expression data that do not necessarily influence gene expression."}, {"heading": "7. Discussion", "text": "For situations where covariate grouping is known, Chen & Liu (2012) have proposed weak-group linear CCA. These extensions all have natural non-parametric analogies that would be interesting to study. As in the case of regression (Koltchinskii & Yuan, 2010), the KCCA formulation considered in this paper can also be generalized to include multiple nuclei and nuclei via groups of variables in a straightforward manner. As the threshold of marginal correlations is crossed, one can imagine exploiting the structure in the correlations, especially in the (p1 \u00d7 p2) marginal correlation matrix, we are looking for a double-glossy high entry in the matrix structure that could allow a weaker interpretation."}, {"heading": "Acknowledgements", "text": "Research supported in part by NSF funding IIS-1116730, AFOSR contract FA9550-09-1-0373 and NIH funding R01-GM093156-03."}], "references": [{"title": "Kernel independent component analysis", "author": ["Bach", "Francis R", "Jordan", "Michael I"], "venue": null, "citeRegEx": "Bach et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2003}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "JMLR, 3:463\u2013482,", "citeRegEx": "Bartlett and Mendelson,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson", "year": 2002}, {"title": "Estimating optimal transformations for multiple regression and correlation", "author": ["Breiman", "Leo", "Friedman", "Jerome H"], "venue": "JASA, 80(391):pp", "citeRegEx": "Breiman et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1985}, {"title": "Sure independence screening in generalized linear models with NP-dimensionality", "author": ["Fan", "Jianqing", "Song", "Rui"], "venue": "Ann. Statist.,", "citeRegEx": "Fan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2010}, {"title": "Persistence in highdimensional linear predictor selection and the virtue of overparametrization", "author": ["Greenshtein", "Eitan", "Ritov", "Ya\u2019acov"], "venue": null, "citeRegEx": "Greenshtein et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greenshtein et al\\.", "year": 2004}, {"title": "Behaviour and convergence of the constrained covariance", "author": ["A. Gretton", "A. Smola", "O. Bousquet", "R. Herbrich", "B. Schoelkopf", "N. Logothetis"], "venue": "Technical Report 130, MPI for Biological Cybernetics,", "citeRegEx": "Gretton et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2004}, {"title": "Generalized additive models", "author": ["Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "Statistical Science, 1(3):pp", "citeRegEx": "Hastie et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 1986}, {"title": "Relations between two sets of variates", "author": ["Hotelling", "Harold"], "venue": "Biometrika, 28(3/4):pp", "citeRegEx": "Hotelling and Harold.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling and Harold.", "year": 1936}, {"title": "On consistency and sparsity for principal components analysis in high dimensions", "author": ["Johnstone", "Iain M", "Lu", "Arthur Yu"], "venue": "JASA, 104(486):682\u2013693,", "citeRegEx": "Johnstone et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Johnstone et al\\.", "year": 2009}, {"title": "Functional aggregation for nonparametric regression", "author": ["Juditsky", "Anatoli", "Nemirovski", "Arkadii"], "venue": "Ann. Statist.,", "citeRegEx": "Juditsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Juditsky et al\\.", "year": 2000}, {"title": "Sparsity in multiple kernel learning", "author": ["Koltchinskii", "Vladimir", "Yuan", "Ming"], "venue": "Ann. Statist.,", "citeRegEx": "Koltchinskii et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koltchinskii et al\\.", "year": 2010}, {"title": "Molecular subtypes of diffuse large Bcell lymphoma arise by distinct genetic pathways", "author": ["Lenz", "G. et. al"], "venue": "Proc. Natl. Acad. Sci. U.S.A.,", "citeRegEx": "Lenz and al.,? \\Q2008\\E", "shortCiteRegEx": "Lenz and al.", "year": 2008}, {"title": "High-dimensional additive modeling", "author": ["Meier", "Lukas", "van de Geer", "Sara", "B\u00fchlmann", "Peter"], "venue": "Ann. Statist.,", "citeRegEx": "Meier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2009}, {"title": "Genomewide sparse canonical correlation of gene expression with genotypes", "author": ["E Parkhomenko", "D Tritchler", "J. Beyene"], "venue": "BMC Proc,", "citeRegEx": "Parkhomenko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Parkhomenko et al\\.", "year": 2007}, {"title": "Minimax-optimal rates for sparse additive models over kernel classes via convex programming", "author": ["Raskutti", "Garvesh", "Wainwright", "Martin J", "Yu", "Bin"], "venue": null, "citeRegEx": "Raskutti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raskutti et al\\.", "year": 2010}, {"title": "Sparse additive models", "author": ["Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry"], "venue": "JRSSB (Statistical Methodology),", "citeRegEx": "Ravikumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2009}, {"title": "Extensions of sparse canonical correlation analysis with applications to genomic data", "author": ["D.M. Witten", "R.J. Tibshirani"], "venue": "Stat Appl Genet Mol Biol,", "citeRegEx": "Witten and Tibshirani,? \\Q2009\\E", "shortCiteRegEx": "Witten and Tibshirani", "year": 2009}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["Witten", "Daniela M", "Tibshirani", "Robert", "Hastie", "Trevor"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "Witten et al. (2009) present examples of recent studies involving such data.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "This has motivated different approaches to sparse CCA, which regularizes (1) by suitable sparsity-inducing l1 penalties (Witten et al., 2009; Witten & Tibshirani, 2009; Parkhomenko et al., 2007; Chen & Liu, 2012).", "startOffset": 120, "endOffset": 212}, {"referenceID": 13, "context": "This has motivated different approaches to sparse CCA, which regularizes (1) by suitable sparsity-inducing l1 penalties (Witten et al., 2009; Witten & Tibshirani, 2009; Parkhomenko et al., 2007; Chen & Liu, 2012).", "startOffset": 120, "endOffset": 212}, {"referenceID": 15, "context": "Recently, several authors have shown how sparse additive models for regression can be efficiently estimated even when p > n (Ravikumar et al., 2009; Koltchinskii & Yuan, 2010; Meier et al., 2009; Raskutti et al., 2010).", "startOffset": 124, "endOffset": 218}, {"referenceID": 12, "context": "Recently, several authors have shown how sparse additive models for regression can be efficiently estimated even when p > n (Ravikumar et al., 2009; Koltchinskii & Yuan, 2010; Meier et al., 2009; Raskutti et al., 2010).", "startOffset": 124, "endOffset": 218}, {"referenceID": 14, "context": "Recently, several authors have shown how sparse additive models for regression can be efficiently estimated even when p > n (Ravikumar et al., 2009; Koltchinskii & Yuan, 2010; Meier et al., 2009; Raskutti et al., 2010).", "startOffset": 124, "endOffset": 218}, {"referenceID": 16, "context": "Returning to the nonconvex sparse CCA problem, Witten et al. (2009) and Parkhomenko et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 13, "context": "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations.", "startOffset": 11, "endOffset": 37}, {"referenceID": 13, "context": "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations.", "startOffset": 11, "endOffset": 148}, {"referenceID": 13, "context": "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations. As we show in simulations, both approaches can lead to poor results, even in the linear case. To address this issue, we propose and study a simple marginal thresholding step to reduce the dimensionality, in the spirit of the diagonal thresholding of Johnstone & Lu (2009) and the SURE screening of Fan & Song (2010).", "startOffset": 11, "endOffset": 456}, {"referenceID": 13, "context": "(2009) and Parkhomenko et al. (2007) suggest using the solution to the nonsparse version of the problem to initialize sparse CCA; Chen & Liu (2012) use several random initializations. As we show in simulations, both approaches can lead to poor results, even in the linear case. To address this issue, we propose and study a simple marginal thresholding step to reduce the dimensionality, in the spirit of the diagonal thresholding of Johnstone & Lu (2009) and the SURE screening of Fan & Song (2010). This results in a three step procedure where after preprocessing we use the nonsparse version of our problem to determine a good initialization for the sparse formulation.", "startOffset": 11, "endOffset": 500}, {"referenceID": 15, "context": "As with the group LASSO, constraining \u2211 j \u2016fj\u20162 encourages sparsity amongst the functions fj Ravikumar et al. (2009). As stated, this is an infinite dimensional optimization problem over Hilbert spaces.", "startOffset": 93, "endOffset": 117}, {"referenceID": 17, "context": "This leads us to a strategy of biconvex optimization that mirrors the linear algorithm of Witten et al. (2009); specifically, initialize by solving the problem without the sparsity constraints, fix \u03b1 and optimize for \u03b2 and vice-versa until convergence.", "startOffset": 90, "endOffset": 111}, {"referenceID": 17, "context": "Instead, in the spirit of \u201cdiagonal penalized CCA\u201d of Witten et al. (2009) we constrain the sum of the variances of the individual fjs.", "startOffset": 54, "endOffset": 75}, {"referenceID": 5, "context": "1 is proved via a Rademacher symmetrization argument of Bartlett & Mendelson (2002) (see also Gretton et al. (2004)) while Lemma 5.", "startOffset": 94, "endOffset": 116}, {"referenceID": 5, "context": "1 is proved via a Rademacher symmetrization argument of Bartlett & Mendelson (2002) (see also Gretton et al. (2004)) while Lemma 5.2 is based on a bound on the bracketing integral of the Sobolev space (see Ravikumar et al. (2009)).", "startOffset": 94, "endOffset": 230}, {"referenceID": 17, "context": "We compare SA-FCCA and SA-KCCA with two models, sparse additive linear CCA (SCCA) (Witten et al., 2009) and kernel CCA (KCCA) (Bach & Jordan, 2003).", "startOffset": 82, "endOffset": 103}, {"referenceID": 17, "context": "The sparsity and smoothness parameters for all methods were tuned using permutation tests, as described in Witten et al. (2009), assuming that Cf = Cg = C, and \u03b3f = \u03b3g = \u03b3.", "startOffset": 107, "endOffset": 128}, {"referenceID": 17, "context": "Permutation tests were used to pick an appropriate bandwidth and sparsity parameter, as described in Witten et al. (2009). We found that the model picked interesting non-linear relationships between CGH and gene expression data.", "startOffset": 101, "endOffset": 122}], "year": 2012, "abstractText": "Canonical Correlation Analysis (CCA) is a classical tool for finding correlations among the components of two random vectors. In recent years, CCA has been widely applied to the analysis of genomic data, where it is common for researchers to perform multiple assays on a single set of patient samples. Recent work has proposed sparse variants of CCA to address the high dimensionality of such data. However, classical and sparse CCA are based on linear models, and are thus limited in their ability to find general correlations. In this paper, we present two approaches to high-dimensional nonparametric CCA, building on recent developments in high-dimensional nonparametric regression. We present estimation procedures for both approaches, and analyze their theoretical properties in the high-dimensional setting. We demonstrate the effectiveness of these procedures in discovering nonlinear correlations via extensive simulations, as well as through experiments with genomic data.", "creator": "LaTeX with hyperref package"}}}