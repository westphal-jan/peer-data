{"id": "1206.4672", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Efficient Active Algorithms for Hierarchical Clustering", "abstract": "Advances in sensing technologies and the growth of the internet have resulted in an explosion in the size of modern datasets, while storage and processing power continue to lag behind. This motivates the need for algorithms that are efficient, both in terms of the number of measurements needed and running time. To combat the challenges associated with large datasets, we propose a general framework for active hierarchical clustering that repeatedly runs an off-the-shelf clustering algorithm on small subsets of the data and comes with guarantees on performance, measurement complexity and runtime complexity. We instantiate this framework with a simple spectral clustering algorithm and provide concrete results on its performance, showing that, under some assumptions, this algorithm recovers all clusters of size ?(log n) using O(n log^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects. Through extensive experimentation we also demonstrate that this framework is practically alluring.", "histories": [["v1", "Mon, 18 Jun 2012 15:35:20 GMT  (3381kb)", "http://arxiv.org/abs/1206.4672v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["akshay krishnamurthy", "sivaraman balakrishnan", "min xu", "aarti singh"], "accepted": true, "id": "1206.4672"}, "pdf": {"name": "1206.4672.pdf", "metadata": {"source": "CRF", "title": "Efficient Active Algorithms for Hierarchical Clustering", "authors": ["Akshay Krishnamurthy", "Sivaraman Balakrishnan"], "emails": ["akshaykr@cs.cmu.edu", "sbalakri@cs.cmu.edu", "minx@cs.cmu.edu", "aarti@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to establish ourselves in the world, and that we are able to assert ourselves in the world, that we are able to change the world, \"he said in an interview with the\" New York Times. \""}, {"heading": "2. Related Work", "text": "In fact, most of us are able to move to another world, where we can move to another world, where we can move to another world, where we can move to another world."}, {"heading": "3. Main Results", "text": "Before proceeding with our main results, we first clarify some notations and introduce a hierarchical cluster model that we will analyze. (We refer to A as any flat cluster algorithm that takes as its parameter a dataset and a natural number k, indicating the number of clusters to produce.) Throughout the paper, k will denote the number of clusters at each split, and we assume that k is known and fixed in the hierarchy. We let n be the number of objects in a dataset and define s as parameters for our algorithms that affect the number of measurements used by our algorithm, with smaller s implying fewer measurements than clusters. The parameter s reflects a compromise between the measurement overhead and the statistical accuracy of our algorithms; increasing s increases the algorithm 1 ActiveCluster (A, s, {xi} ni = 1, k) when n s = 1 Draw back."}, {"heading": "3.1. An Active Clustering Framework", "text": "We assume that we see only similarities between the objects in the individual clusters for which there are the most similarities. In this recursive phase, where we do not observe measurements between the clusters, i.e. up to partition Cj, we observe only similarities between the objects in the small subsets. We observe similarities between the objects in the objects, i.e. up to partition Cj, we observe only similarities between the objects in the objects in Cj. These results in an active algorithm that focuses on solving the higher resolution."}, {"heading": "R1 recovers all clusters of size at least s with probability 1\u2212o(n2e\u2212cs), for some constant c = c(\u03b7, \u03b3).", "text": "This probability of success is 1 \u2212 o (1) as long as: s \u2265 max 1 c1 log n 4 (1 + \u03b7) 2 log n24 1 + 4 log (4C\u03b7kn) = 0 (log (nk))) 1) R2 uses O (ns log n) similarity measurements. R3 runs in the time O (nAs + ns log n), where A runs on adatasets of size s in the time O (As). At a high level, the theorem says that the clustering guarantee for a flat, non-active algorithm, A, can be converted into a hierarchical clustering guarantee for an active version of A. And that this active algorithm enjoys significantly reduced measurement and runtime complexity. The only property required by A. is that it covers a flat clustering situation with a very high probability. While the probability of success seems oddly high, we will show that for a fairly intuitive model we enjoy a simple cluster algorithm guarantee cluster algorithms."}, {"heading": "3.2. Active Spectral Clustering", "text": "It is not possible that we see ourselves in a position to implement such a system if we are able to implement such a system. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it. (...) We are not in a position to implement it."}, {"heading": "3.4. Some Practical Considerations", "text": "Our algorithm, as mentioned above, has some shortcomings that allow theoretical analysis but are undesirable for practical applications. Specifically, we are therefore developing a variant of active spectra, called HeurSpec, with multiple heuristics. First, we are using the popular eigengap heuristics, in which the number of clusters k is chosen in such a way that the gap in the eigenvalues \u03bbk + 1 \u2212 \u03bbk of the lapis large. Second, we propose discarding all undersampled objects with a low degree (if limited to the sample) in the hope of removing underrepresented clusters from the sample. On average, when an object is very similar to any of the clusters represented in the sample, we create a new cluster for that object. We expect that these two heuristics will help us restore small clusters by directly comparing the effectiveness of these hay spectra."}, {"heading": "4. Simulations", "text": "In this section, we present experiments that verify our theoretical results. With Theorem 2, we expect that ActiveSpectral will be the first function of ActiveSpectral for a constant amount of noise \u03c3, which means that it will restore all sufficiently large splits with a high probability. In comparison, Balakrishnan et. al. (2011), we show that SpectralCluster can tolerate noise with n. (A) that in fact the noise tolerance of SpectralCluster grows with n, while ActiveSpectral shows the probability of successfully restoring the first slit in a noisy HBM as a function of various n in Figure 2 (a) shows that the noise tolerance of SpectralCluster grows with n, while 2 (c) shows that ActiveSpectral enjoys constant noise tolerance. (B) and 2 (d) indicate that similar guarantees for remedies and Aveveveans improves activeMeans.Our theory also predicts the statistical performance of Spectral, which increases the activity of Spectral."}, {"heading": "5. Real World Experiments", "text": "In these cases we can report on the outlier fraction, as we did in the simulation."}, {"heading": "6. Discussion", "text": "Our findings in this paper, which show that a family of active hierarchical cluster algorithms has strong performance guarantees, raise several interesting questions. We have shown that ActiveSpectral has reasonable statistical performance, but can other algorithms be activated while retaining statistical properties? Second, are there principled ways to circumvent a state of balance even when subsampling objects? Finally, is there a theoretically justified approach to estimating the number of clusters, k? Another direction does not relate to clustering, but to the recently popular matrix completion problem. With hierarchically structured matrices, our findings imply that an active algorithm can restore high-level (rank n / log n) matrices using O (n log2 n) similarities, an improvement over non-active approaches. Active algorithms can therefore provide impressive guarantees of matrix completion and associated problems we hope to address in the future."}, {"heading": "Acknowledgements", "text": "This research is partially supported by AFOSR with funding FA9550-10-1-0382 and NSF with funding IIS1116458. AK is partially supported by an NSF Graduate Research Fellowship."}], "references": [{"title": "Fast computation of low rank matrix approximations", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In STOC,", "citeRegEx": "Achlioptas and McSherry,? \\Q2001\\E", "shortCiteRegEx": "Achlioptas and McSherry", "year": 2001}, {"title": "Noise thresholds for spectral clustering", "author": ["S. Balakrishnan", "M. Xu", "A. Krishnamurthy", "A. Singh"], "venue": "In NIPS,", "citeRegEx": "Balakrishnan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2011}, {"title": "Robust hierarchical clustering", "author": ["M.F. Balcan", "P. Gupta"], "venue": "In COLT,", "citeRegEx": "Balcan and Gupta,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Gupta", "year": 2010}, {"title": "Better streaming algorithms for clustering problems", "author": ["M. Charikar", "L. O\u2019Callaghan", "Panigrahy", "Rina"], "venue": "In STOC,", "citeRegEx": "Charikar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2003}, {"title": "Phylogenetic Clustering with R package phyclust", "author": ["W.C. Chen"], "venue": "URL http://thirteen-01.stat.iastate. edu/snoweye/phyclust/", "citeRegEx": "Chen,? \\Q2010\\E", "shortCiteRegEx": "Chen", "year": 2010}, {"title": "Global versus local methods in nonlinear dimensionality reduction", "author": ["V. de Silva", "J.B. Tenenbaum"], "venue": "In NIPS,", "citeRegEx": "Silva and Tenenbaum,? \\Q2002\\E", "shortCiteRegEx": "Silva and Tenenbaum", "year": 2002}, {"title": "Active Clustering: Robust and Efficient Hierarchical Clustering using Adaptively Selected Similarities", "author": ["B. Eriksson", "G. Dasarathy", "A. Singh", "R. Nowak"], "venue": null, "citeRegEx": "Eriksson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 2011}, {"title": "Fast montecarlo algorithms for finding low-rank approximations", "author": ["A.M. Frieze", "R. Kannan", "S. Vempala"], "venue": "J. ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "An iterative improvement procedure for hierarchical clustering", "author": ["D. Kauchak", "S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "Kauchak and Dasgupta,? \\Q2003\\E", "shortCiteRegEx": "Kauchak and Dasgupta", "year": 2003}, {"title": "Noun phrases in context 500 dataset, 2009. URL http://www.cs.cmu.edu/~tom/10709_ fall09/RTWdata.html", "author": ["T. Mitchell"], "venue": null, "citeRegEx": "Mitchell,? \\Q2009\\E", "shortCiteRegEx": "Mitchell", "year": 2009}, {"title": "Using population mixtures to optimize the utility of genomic databases: linkage disequilibrium and association study design in India", "author": ["T.J. Pemberton", "M. Jakobsson", "D.F. Conrad", "G. Coop", "J.D. Wall", "J.K. Pritchard", "P.I. Patel", "N.A. Rosenberg"], "venue": "Ann. Hum. Genet.,", "citeRegEx": "Pemberton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pemberton et al\\.", "year": 2008}, {"title": "Spectral Clustering and the High-Dimensional Stochastic Block Model", "author": ["K. Rohe", "S. Chatterjee", "B. Yu"], "venue": "Technical Report 791,", "citeRegEx": "Rohe et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rohe et al\\.", "year": 2010}, {"title": "Fast and accurate k-means for large datasets", "author": ["M. Shindler", "A. Meyerson", "A. Wong"], "venue": "In NIPS,", "citeRegEx": "Shindler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shindler et al\\.", "year": 2011}, {"title": "Active Clustering of Biological Sequences", "author": ["K. Voevodski", "M.F. Balcan", "H. R\u00f6glin", "S. Teng", "Y. Xia"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Voevodski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "By appealing to previous statistical guarantees (Balakrishnan et al., 2011), we can show that this algorithm has desirable theoretical properties, both in terms of statistical and computational performance.", "startOffset": 48, "endOffset": 75}, {"referenceID": 6, "context": "There is a large body of work on hierarchical and partitional clustering algorithms, many coming with various theoretical guarantees, but only few algorithms attempt to minimize the number of pairwise similarities used (Eriksson et al., 2011; Balcan & Gupta, 2010; Shamir & Tishby, 2011).", "startOffset": 219, "endOffset": 287}, {"referenceID": 6, "context": "There is a large body of work on hierarchical and partitional clustering algorithms, many coming with various theoretical guarantees, but only few algorithms attempt to minimize the number of pairwise similarities used (Eriksson et al., 2011; Balcan & Gupta, 2010; Shamir & Tishby, 2011). Along this line, the work of Eriksson et. al. (2011) and Shamir and Tishby (2011) is closest in flavor to ours.", "startOffset": 220, "endOffset": 342}, {"referenceID": 6, "context": "There is a large body of work on hierarchical and partitional clustering algorithms, many coming with various theoretical guarantees, but only few algorithms attempt to minimize the number of pairwise similarities used (Eriksson et al., 2011; Balcan & Gupta, 2010; Shamir & Tishby, 2011). Along this line, the work of Eriksson et. al. (2011) and Shamir and Tishby (2011) is closest in flavor to ours.", "startOffset": 220, "endOffset": 371}, {"referenceID": 13, "context": "Recently (Voevodski et al., 2012) proposed an active algorithm for flat k-way clustering that selects O(k) landmarks and partitions the objects using distances to these landmarks.", "startOffset": 9, "endOffset": 33}, {"referenceID": 5, "context": "This idea of selecting landmarks bears strong resemblence to the first phase of our active clustering algorithm and also has connections to the Landmark MDS algorithm of de Silva and Tenenbaum (2002). These approaches are tied to specific algorithms, while our framework is much more general.", "startOffset": 173, "endOffset": 200}, {"referenceID": 3, "context": "A related direction is the body of work on efficient streaming and online algorithms for approximating the k-means and k-medians objectives (See for example (Charikar et al., 2003; Shindler et al., 2011)).", "startOffset": 157, "endOffset": 203}, {"referenceID": 12, "context": "A related direction is the body of work on efficient streaming and online algorithms for approximating the k-means and k-medians objectives (See for example (Charikar et al., 2003; Shindler et al., 2011)).", "startOffset": 157, "endOffset": 203}, {"referenceID": 13, "context": "As with (Voevodski et al., 2012), the guarantees for these algorithms do not immediately translate into an exact recovery guarantee, making it challenging to transform these approaches into hierarchical clustering algorithms.", "startOffset": 8, "endOffset": 32}, {"referenceID": 7, "context": "While there have been advances in this direction, the majority of these require the entire similarity matrix or graph to be known a priori (Frieze et al., 2004).", "startOffset": 139, "endOffset": 160}, {"referenceID": 6, "context": "This type of balancedness parameter has been used in previous analyses of clustering algorithms (Eriksson et al., 2011; Balakrishnan et al., 2011), and it is common to assume that the clustering is not too unbalanced.", "startOffset": 96, "endOffset": 146}, {"referenceID": 1, "context": "This type of balancedness parameter has been used in previous analyses of clustering algorithms (Eriksson et al., 2011; Balakrishnan et al., 2011), and it is common to assume that the clustering is not too unbalanced.", "startOffset": 96, "endOffset": 146}, {"referenceID": 6, "context": "This is related to the Tight-Clustering condition used in (Eriksson et al., 2011) and less stringent than earlier results which assume that within- and between-cluster similarities are constant and bounded in expectation (Rohe et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 11, "context": ", 2011) and less stringent than earlier results which assume that within- and between-cluster similarities are constant and bounded in expectation (Rohe et al., 2010).", "startOffset": 147, "endOffset": 166}, {"referenceID": 1, "context": "We emphasize that K2 subsumes many of the assumptions of previous clustering analyses (for example (Balakrishnan et al., 2011; Rohe et al., 2010)).", "startOffset": 99, "endOffset": 145}, {"referenceID": 11, "context": "We emphasize that K2 subsumes many of the assumptions of previous clustering analyses (for example (Balakrishnan et al., 2011; Rohe et al., 2010)).", "startOffset": 99, "endOffset": 145}, {"referenceID": 1, "context": "In this section, we turn a simple spectral algorithm (See pseudocode in Algorithm 2) into an active clustering algorithm, using the analysis from (Balakrishnan et al., 2011).", "startOffset": 146, "endOffset": 173}, {"referenceID": 1, "context": "The algorithm operates on hierarchically structured similarity matrices refered to as the noisy Hierarchical Block Matrices (again from (Balakrishnan et al., 2011)).", "startOffset": 136, "endOffset": 163}, {"referenceID": 6, "context": "One way to measure this is the outlier fraction metric between the clustering returned by an algorithm and the true hierarchy (Eriksson et al., 2011).", "startOffset": 126, "endOffset": 149}, {"referenceID": 9, "context": "The datasets are: The set of articles from NIPS volumes 0 through 12 from (Roweis, 2002), a subset of NPIC500 co-occurence data from the Readthe-Web project (Mitchell, 2009) which we call RTW, a SNP dataset from the HGDP (Pemberton et al.", "startOffset": 157, "endOffset": 173}, {"referenceID": 10, "context": "The datasets are: The set of articles from NIPS volumes 0 through 12 from (Roweis, 2002), a subset of NPIC500 co-occurence data from the Readthe-Web project (Mitchell, 2009) which we call RTW, a SNP dataset from the HGDP (Pemberton et al., 2008), and a synthetic phylogeny dataset produced using phyclust (Chen, 2010).", "startOffset": 221, "endOffset": 245}, {"referenceID": 4, "context": ", 2008), and a synthetic phylogeny dataset produced using phyclust (Chen, 2010).", "startOffset": 67, "endOffset": 79}], "year": 2012, "abstractText": "Advances in sensing technologies and the growth of the internet have resulted in an explosion in the size of modern datasets, while storage and processing power continue to lag behind. This motivates the need for algorithms that are efficient, both in terms of the number of measurements needed and running time. To combat the challenges associated with large datasets, we propose a general framework for active hierarchical clustering that repeatedly runs an off-the-shelf clustering algorithm on small subsets of the data and comes with guarantees on performance, measurement complexity and runtime complexity. We instantiate this framework with a simple spectral clustering algorithm and provide concrete results on its performance, showing that, under some assumptions, this algorithm recovers all clusters of size \u03a9(log n) using O(n log n) similarities and runs in O(n log n) time for a dataset of n objects. Through extensive experimentation we also demonstrate that this framework is practically alluring.", "creator": "Preview"}}}