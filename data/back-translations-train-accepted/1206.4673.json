{"id": "1206.4673", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Group Sparse Additive Models", "abstract": "We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. Previous works either study the group sparsity in the parametric setting (e.g., group lasso), or address the problem in the non-parametric setting without exploiting the structural information (e.g., sparse additive models). In this paper, we present a new method, called group sparse additive models (GroupSpAM), which can handle group sparsity in additive models. We generalize the l1/l2 norm to Hilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block coordinate descent algorithm for constructing the estimate. We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models, and also conduct a comparative experiment on a real breast cancer dataset.", "histories": [["v1", "Mon, 18 Jun 2012 15:35:38 GMT  (470kb)", "http://arxiv.org/abs/1206.4673v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["junming yin", "xi chen", "eric p xing"], "accepted": true, "id": "1206.4673"}, "pdf": {"name": "1206.4673.pdf", "metadata": {"source": "META", "title": "Group Sparse Additive Models", "authors": ["Junming Yin", "Xi Chen", "Eric P. Xing"], "emails": ["junmingy@cs.cmu.edu", "xichen@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it will be in a position to achieve the objectives I have mentioned."}, {"heading": "2. Related Work", "text": "Lin & Zhang (2006) proposed the COSSO estimator, which uses the sum of reproducing kernel Hilbert space (RKHS) norms of component functions as a regulatory penalty for simultaneous variable selection and model fitting into smoothing spline ANOVA models. Ravikumar et al. (2009) introduced a sparse version of additive models called SpAM, penalizing the sum of L2 (\u00b5) norms of component functions. While these methods have proven effective in estimating economical non-parametric models, none of them are able to take advantage of structural information. Liu et al. (2009a) extended SpAM task setting; however, the group structure is imposed on the tasks rather than the covariates considered here. Finally, it is important to emphasize that our work is substantially different from another line of research that also mixes groups and so-added models."}, {"heading": "3. Background", "text": "We begin by reviewing some important concepts of non-parametric regression and (sparse) additive models to set the stage for our method."}, {"heading": "3.1. Smoothing for Nonparametric Regression", "text": "Nonparametric regression deals with the estimation of regression functionality (X) = m (X1,..., Xp) = E [Y | X1,.., Xp] from a set of n data samples {(X (i), y (i), y (i), y (R), i = 1., n), without any parametric form of m (X), such as the linearity in X1,..., Xp. We can write Y = m (X) +, where E [] = 0.In the case of a single covariate (p = 1), m (X) = E [Y | X], x] is the orthogonal projection of Y on the linear space of all measurable functions of X and can be written."}, {"heading": "3.2. Additive Models", "text": "Although it is easy to generalize one-dimensional smoothing to the p-dimensional case, it is known that smoothing breaks up into high dimensions due to the curse of dimensionality, which motivates the study of additive models (Hastie & Tibshirani, 1990), m (X1,..., Xp) = \u03b1 + p \u00b2 j = 1 fj (Xj), (4) where f1,.., fp are one-dimensional smooth component functions, one for each covariate. For simplicity and identification, we assume that \u03b1 = 0 and fj \u00b2 Hj are such that E [fj (Xj)] = 0 are for each additive model in the population to minimizeL (f) = 1 2 E [(Y \u2212 p \u00b2) smoof = 1 fj (Xj) 2], (5) above {f \u00b2. The minimizers of (5) (j = j = j = j = j = j) 6 (j = j)."}, {"heading": "3.3. SpAM", "text": "Ravikumar et al. (2009) proposed a new approach called SpAM for component selection in high-dimensional additive models, the idea being to impose a limitation on the index set of non-zero component functions by regularization: min f: fj Hj L (f) + 0 (f), (8), where \u03bb > 0 is the regularization parameter and \u0442 (f) = \u2211 p j = 1% fj behaves like a \"1 sphere across different components to promote functional sparseness; the stationary state of (8) is given by byfj = [1 \u2212 \u03bb PjRj] + PjRj, (9), where Rj = Y \u2212 \u2211 k 6 = j fk is the partial remainder and [\u00b7] + = max {\u00b7, 0}. An example version of the algorithm can be obtained by inserting sampling estimates in (9)."}, {"heading": "4. GroupSpAM", "text": "In this section, we assume that this is a problem that can only be solved if it is a problem defined at the level of GroupSpAM, defined at the level of GroupSpAM. (f) However, the GroupSpAM group is defined at the level of GroupSpAM group. (f) The GroupSpAM group is defined at the level of GroupSpAM group. (f) The GroupSpAM group is defined at the level of GroupSpAM group. (f) The GroupSpAM group is defined at the level of GroupSpAM group. (f) The GroupSpAM group is defined at the level of GroupSpAM group. (The GroupSpAM group is defined at the level of GroupSpAM group. (The GroupSpAM group is defined at the level of Groupree group.) The GroupSpAM group is defined at the level of GroupAM group. (S) The GroupSpAM group is defined at the level of GroupSpAM group."}, {"heading": "5. GroupSpAM with Overlap", "text": "In this section we allow overlaps between the groups in G and we are interested in the estimation of an additive model, the support of which is a unification of groups. We take the approach of Jacob et al. (2009) by introducing a series of latent functions hg = {hgj-Hj} j-g, one sentence for each group, and solve the following optimization problem in f, {hg} g-GL (f) + tically-g-G-G, dg-G-G, i.e. g: j-g hgj = fj, j = 1,.., p. (17) The idea is to break down each original component function as the sum of a series of latent functions and then apply the functional group penalty to the decomposition. Consequently, the covariants, which are not in a selected group, are removed and the resulting support is a unification of groups. As soon as we eliminate f from (17), the problem Tomin {hg} g-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-G-17) reduces the problem"}, {"heading": "6. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Simulation Study", "text": "We generate covariants with compound symmetry covariance structure as follows: each covariate Xj = (Wj + tU) / (1 + t), j = 1,.., p, where W1,..., Wp and U are i.i.d of Uni (-2,5,2,5). For different covariates Xj and Xk, corr (Xj, Xk) = t2 / (1 + t2). The setting t = 0 corresponds to the case of independent covariants. The sample size n = 150, and the dimension of the covariants p = 200 and 1000. We then generate responses from an additive model in Rp with two groups of relevant component functions, each four (Table 1): Y = m (X) + = 8 j = 1 fj fj (Xj) +, where the individual covariants p = 200 and 1000. We generate responses from an additive model in Rp with two groups of relevant component functions."}, {"heading": "6.2. Breast Cancer Data", "text": "In our second experiment, we use GroupSpAM, which overlaps with a breast cancer dataset (van de Vijver et al., 2002), to demonstrate the potential benefit of using additive models with group sparseness in a real-life problem, which consists of gene expression measurements for 8,141 genes collected from 295 breast cancer tumors (78 metastatic and 217 non-metastatic), and we are interested in finding a sparse group of genes that can distinguish the two types of tumors. Instead of looking at individual genes independently, a more powerful way is to build a predictive model that takes into account their signaling information. Some genes in a biological pathway are known to perform the seminal functionality in the cell, so we are more likely to be involved in the phenomenon under investigation in a group way. Furthermore, each gene can participate in multiple ways in the analysis, and the phenomenon under investigation may possibly depend on gene behavior in a complex way."}, {"heading": "7. Conclusions", "text": "In this paper, we propose a new method for variable selection in non-parametric additive models where there is a potentially overlapping group structure between the covariates, developing an efficient optimization algorithm and achieving promising results on both simulated and real data. An interesting future direction is to design new functional penalties to include richer structures between the covariates (e.g. hierarchical tree structure), and to investigate the asymptotic properties of the method, such as model selection and predictive consistency."}, {"heading": "Acknowledgments", "text": "This work is supported by NIH 1R01GM087694, NIH 1R01GM093156 and a Ray and Stephanie Lane Research Fellowship to JY. We would like to thank Guillaume Obozinski for the helpful discussion on MKL and the anonymous reviewers for their valuable comments."}, {"heading": "Appendix: Proof of Theorem 1", "text": "Proof. If we write L (f) in (5) as a function that depends only on fg, we get L (fg) = 1 2 E [(Rg) = 1 2 E [(Rg) = 1 2 E [(Rg) + 1 (Xj))) 2].If we consider a disturbance of L (fg) along the direction \u03b7g = (fg) j, L (fg + g) = 1 2 E [(Rg) j (Xj) + 1 (Xj))) 2].The first adjustment of L (fg) \u2212 L (fg) \u2212 L (fg) is an adjustment of J (Xj) \u2212 G (Xj) \u2212 G (Rg) \u2212 Rg) = 1 E [g) = 1 (Xj), E [fj), the last adjustment of L (Xj) \u2212 fj (Rj) and the last adjustment (Rj)."}], "references": [{"title": "Consistency of the group lasso and multiple kernel learning", "author": ["F. Bach"], "venue": "JMLR, 9:1179\u20131225,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Sure independence screening for ultrahigh dimensional feature space. JRSSB", "author": ["J. Fan", "J. Lv"], "venue": null, "citeRegEx": "Fan and Lv,? \\Q2008\\E", "shortCiteRegEx": "Fan and Lv", "year": 2008}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Technical report, Stanford University,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Generalized Additive Models", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "The benefit of group sparsity", "author": ["J. Huang", "T. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Huang and Zhang,? \\Q2010\\E", "shortCiteRegEx": "Huang and Zhang", "year": 2010}, {"title": "Variable selection in nonparametric additive models", "author": ["J. Huang", "J. Horowitz", "F. Wei"], "venue": "Annals of Statistics,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J. Vert"], "venue": "In ICML", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Component selection and smoothing in multivariate nonparametric regression", "author": ["Y. Lin", "H. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Lin and Zhang,? \\Q2006\\E", "shortCiteRegEx": "Lin and Zhang", "year": 2006}, {"title": "Nonparametric regression and classification with joint sparsity constraints", "author": ["H. Liu", "J. Lafferty", "L. Wasserman"], "venue": "In NIPS", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "HighDimensional additive modeling", "author": ["L. Meier", "S. van de Geer", "P. Buehlmann"], "venue": "Annals of Statistics,", "citeRegEx": "Meier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2009}, {"title": "Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles", "author": ["A Subramanian"], "venue": "PNAS, 102(43):15545\u201315550,", "citeRegEx": "Subramanian,? \\Q2005\\E", "shortCiteRegEx": "Subramanian", "year": 2005}, {"title": "Regression shrinkage and selection via the lasso. JRSSB", "author": ["R. Tibshirani"], "venue": null, "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "A gene-expression signature as a predictor of survival in breast cancer", "author": ["M van de Vijver"], "venue": "N. Engl. J. Med.,", "citeRegEx": "Vijver,? \\Q1999\\E", "shortCiteRegEx": "Vijver", "year": 1999}, {"title": "Model selection and estimation in regression with grouped variables. JRSSB", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "`1-regularized methods, such as lasso (Tibshirani, 1996), are among the most widely used approaches for variable selection in linear models.", "startOffset": 38, "endOffset": 56}, {"referenceID": 0, "context": "While some recent progress has been made on this problem by proposing various functional penalties (Lin & Zhang, 2006; Bach, 2008; Ravikumar et al., 2009), none of these methods is capable of exploiting the structural information among the covariates that may exist as prior knowledge in many applications.", "startOffset": 99, "endOffset": 154}, {"referenceID": 2, "context": "In this sense, GroupSpAM can be viewed as a nonparametric extension of the generalized group lasso (Friedman et al., 2010), which allows non-orthonormal covariates within a group, to additive models.", "startOffset": 99, "endOffset": 122}, {"referenceID": 8, "context": "Liu et al. (2009a) extended SpAM to the multi-task setting; however, the group structure is imposed on the tasks instead of on the covariates being considered here.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Finally, it is important to emphasize that our work is substantially different from another line of research that also mixes group lasso and additive models (Bach, 2008; Meier et al., 2009; Huang et al., 2010).", "startOffset": 157, "endOffset": 209}, {"referenceID": 10, "context": "Finally, it is important to emphasize that our work is substantially different from another line of research that also mixes group lasso and additive models (Bach, 2008; Meier et al., 2009; Huang et al., 2010).", "startOffset": 157, "endOffset": 209}, {"referenceID": 5, "context": "Finally, it is important to emphasize that our work is substantially different from another line of research that also mixes group lasso and additive models (Bach, 2008; Meier et al., 2009; Huang et al., 2010).", "startOffset": 157, "endOffset": 209}, {"referenceID": 6, "context": "We adopt the approach of Jacob et al. (2009) by introducing a set of latent functions h = {hgj \u2208 Hj}j\u2208g, one set for each group, and solve the following optimization problem min f ,{h}g\u2208G L(f) + \u03bb \u2211 g\u2208G \u221a dg\u2016h\u2016", "startOffset": 25, "endOffset": 45}, {"referenceID": 6, "context": "Table 3 shows the results of GroupSpAM, SpAM and group lasso with overlap (Jacob et al., 2009) based on the balanced loss function by a 3-fold cross validation.", "startOffset": 74, "endOffset": 94}], "year": 2012, "abstractText": "We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. Previous works either study the group sparsity in the parametric setting (e.g., group lasso), or address the problem in the nonparametric setting without exploiting the structural information (e.g., sparse additive models). In this paper, we present a new method, called group sparse additive models (GroupSpAM), which can handle group sparsity in additive models. We generalize the `1/`2 norm to Hilbert spaces as the sparsityinducing penalty in GroupSpAM. Moreover, we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block coordinate descent algorithm for constructing the estimate. We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models, and also conduct a comparative experiment on a real breast cancer dataset.", "creator": "LaTeX with hyperref package"}}}