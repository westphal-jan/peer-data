{"id": "1206.4676", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Clustering by Low-Rank Doubly Stochastic Matrix Decomposition", "abstract": "Clustering analysis by nonnegative low-rank approximations has achieved remarkable progress in the past decade. However, most approximation approaches in this direction are still restricted to matrix factorization. We propose a new low-rank learning method to improve the clustering performance, which is beyond matrix factorization. The approximation is based on a two-step bipartite random walk through virtual cluster nodes, where the approximation is formed by only cluster assigning probabilities. Minimizing the approximation error measured by Kullback-Leibler divergence is equivalent to maximizing the likelihood of a discriminative model, which endows our method with a solid probabilistic interpretation. The optimization is implemented by a relaxed Majorization-Minimization algorithm that is advantageous in finding good local minima. Furthermore, we point out that the regularized algorithm with Dirichlet prior only serves as initialization. Experimental results show that the new method has strong performance in clustering purity for various datasets, especially for large-scale manifold data.", "histories": [["v1", "Mon, 18 Jun 2012 15:36:49 GMT  (339kb)", "http://arxiv.org/abs/1206.4676v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NA stat.ML", "authors": ["zhirong yang", "erkki oja"], "accepted": true, "id": "1206.4676"}, "pdf": {"name": "1206.4676.pdf", "metadata": {"source": "META", "title": "Clustering by Low-Rank Doubly Stochastic Matrix Decomposition", "authors": ["Zhirong Yang", "Erkki Oja"], "emails": ["zhirong.yang@aalto.fi", "erkki.oja@aalto.fi"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Clustering by DCD", "text": "Suppose the similarities between n data samples are pre-calculated and specified in a nonnegative symmetric matrix A. This matrix can be considered as the (weighted) affinity of an undirected similarity graph, where each node corresponds to a data sample (data node).A cluster analysis algorithm takes such input and divides the data nodes into r disjunct subsets. In the probable cluster analysis, we want to find P (k | i), the probability of assigning the ith sample to the smallest cluster, where i = 1,..., n and k = 1,..., r. In the following, i, j and v stand for data sample (node) indices, while k and l stand for cluster indices."}, {"heading": "2.1. Learning objective", "text": "Some of our work was inspired by the AnchorGraph (Liu et al., 2010), which was used in large approximative graph constructions based on a two-step random walk between data nodes through a series of anchor nodes. Note that AnchorGraph is not a cluster method. If we add the data nodes to the input similarity graph, the cluster that assigns probabilities can be regarded as a single-step random walk from data nodes to augmented cluster nodes. Without preference to any particular samples, we establish uniform previous P (i) = 1 / n across the data nodes. By doing this, the reverse random margin probabilities can be calculated from the Bayes formula P."}, {"heading": "2.2. Probabilistic model", "text": "The optimization target has an analogous statistical model with the PLSI. Dropping the constant terms from the DKL (A | | A) corresponds to the goal of maximizing the ijAij log. (7) This can be identified as the log probability of the following generative model if Aij are integers: for t = 1,..., T, add one to the entry (i, j). Multinomial (1 n A, 1), the probability of which is given by p (A) = T = 1 n A-ij (1 n A-ij) Aij. The above model simply uses uniform previous rows of W. It does not prevent the use of informative priors or complexity control. A natural choice for probabilities is the dirichlet distribution (\u03b1 > 0) p (Wi1,."}, {"heading": "2.3. Optimization", "text": "The optimization problem with dirichlet before W is equivalent to minimizingJ (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"W\" (W) = \"(W) =\" W \"(W) =\" W \"(W) (W) =\" W \"(W) (W) =\" (W \"(W) =\" W (W) = \"W\" (W) = \"(W\" (W) = W \"(W) =\" (W) = \"W\" (W \"(W) =\" (W) = \"W\" (W \"(W) =\" (W \"(W) =\" W \"(W\") = \"(W\" (W \") =\" W \"(W\") = \"W\" (W \"(W\") = \"(W\") = \"W\" (W \"(W\") = \"(W\") = \"(W\" (W \") =\" W \"(W\") = \"W\" (W \"(W\" (W \") = W\") = \"(W\") = \"(W\" (W \") =\" W \"(W\" (W \") = W\" (W \"(W\") = W \") =\" (W \"(W\") = \"W\" (W \") = W\" (W \") = W\" (W \") =\") = \"W\" (W \") =\" W \"(W\" (W \"(W\") = W \"(W\" (W \") = W\") = \"(W\") = \"(W\" (W \") =\" (W \") = W\" (W \") = W\" (W"}, {"heading": "2.4. Initialization", "text": "The optimization problems of many cluster analysis methods, including ours, are non-convex. Usually, finding the global optimum is very expensive or even NP-hard. If local optimizers are used, the optimization path can easily get stuck in bad local optima if the algorithm assumes an arbitrary random guess, the first can only be applied to vector data and could be slow for large-scale high-dimensional data. Here, we use the second initialization method. While the original Ncut NP is hard, the relaxed Ncut problem can be solved more efficiently using spectral methods (Shi & Malik, 2000)."}, {"heading": "3. Related Work", "text": "Our method intersects with several other approaches to machine learning. Here we discuss some of these directions and show the connections and our new contributions."}, {"heading": "3.1. Topic Model", "text": "An early topic model was PLSI (Hofmann, 1999), which maximizes the following log probability for symmetrical inputs. (14) However, both objectives can be expressed equally by non-negative low approximation using KL divergence. The big difference is the decomposition form of the approximate matrix. There are two ways to model the hierarchy between latent variables and the observed variables. Topical model uses the purely generative path, while our method uses the discriminatory way. PLSI gives the clustering results indirectly. One should apply the Bayes formula to evaluate P (k | i) and P (k)."}, {"heading": "3.2. Nonnegative Matrix Factorization", "text": "Research at the NMF also opened the door to multiplicative majorization minimization algorithms to optimize against non-negative matrices. In the original NMF, a non-negative matrix X is approached by a product of two low-grade matrices W andH. Later researchers found that more constraints or normalizations should be imposed on factorization matrices to achieve the desired performance. Orthogonality is a popular choice (see Ding et al., 2006) for highly sparse factorization matrices, especially the cluster indicator matrix. However, orthogonality constraints seem more exclusive than other constraints or privileges. In practice, orthogonality favors euclimatic distance."}, {"heading": "3.3. AnchorGraph", "text": "DCD uses the same matrix decomposition as AnchorGraph. However, there are several key differences between the two methods. First, AnchorGraph is not intended for clustering, but for building graph input. AnchorGraph has no learning objective that captures the global structure of data such as clusters. Each line of the decomposing matrix in AnchorGraph is learned individually and encodes only the local information. There is no learning about the decomposing matrix as a whole. Furthermore, anchors are either selected from data samples or pre-learned by e.g. kmeans. In contrast, cluster nodes are virtual in our formulation. They are not vectors and do not require physical storage."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Compared methods", "text": "We compared our method with eight other clustering algorithms that can take a symmetrical nonnegative sparse matrix as input, ranging from classical to state-of-the-art methods with various principles: diagram sections including Normalized Cut (Ncut) (Shi & Malik, 2000), Nonnegative Spectral Cut (NSC) (Ding et al., 2008) and 1-Spectral Ratio Cheeger Cut (1-Spec) (Hein & Buehler, 2010); nonnegative matrix factorization including Projective NMF (PNMF) (Yang & Oja, 2010) and 1-Spectral Ratio Cheeger Cut (1-Spec) (Ding et al., 2006), and Leftstostic Decomposition (LSD) that MSD) (Arora et al., 2011); subject models including Probabilistic Latent Semantic Indexing (PLI) (Actualizations), Hofmann (SM) and Interaction (We) (1999)."}, {"heading": "4.2. Datasets", "text": "Specifically, we focus on data that lies within a curved manifold. Therefore, we have selected 15 such data sets that are publicly available from a variety of areas; the data sources are stated in the supplementary document.The statistics of the selected data sets are summarized in Table 1. In short, Amazon are book resemblances according to amazon.com purchase records; votes are congressional ballots of two different parties; ORL, PIE, YaleB are face images collected under different conditions; COIL20 are small toy images; Isolet and LegReco are handwritten English letter images; Webkb4 and 7 sectors are text document collections; Mfeat, USPS, PenDigits, MNIST are handwritten digitalis.We have pre-processed the above data sets to generate similarity graphs, except Amazon, which is already in sparse chart format. We extracted the scattering characteristics (mallard, or Airibit) for their respective features."}, {"heading": "4.3. Results", "text": "The cluster performance of the compared methods is evaluated by clustering purity = 1n r \u2211 k = 1 max 1 \u2264 l \u2264 r nlk (15), where nlk is the number of data samples in cluster k belonging to the ground truth class l. Greater purity generally corresponds to a better cluster result. The lustful puritans for the compared methods are in Table 2.Our method has a strong performance in terms of cluster purity. Note that the DCD wins 12 out of 15 selected data sets. Even for the other three data sets, DCD is the first or second runner-up, with puritties associated with the winner or very close to the winner. The new method is particularly advantageous for large data sets. Note that the data sets in Table 2 are sorted by their size. We can see that there are some other winners or joint winners for smaller data sets, for example, LSD for the PIE faces or 1-Spec for the Mfeat numbers for this very small CD with a similar CD."}, {"heading": "5. Conclusions", "text": "We have presented a new cluster method based on a low-level non-negative approach with three important initial contributions: (1) a novel approach to decomposition of the approach matrix derived from a two-step random process; (2) a relaxed majorization 1 see http: / / yann.lecun.com / exdb / mnist / minimization algorithm to find better approach matrices; (3) a strategy that uses pre-initialization regulation with the dirichlet. Experimental results have shown that our method works robustly for several selected datasets and can improve cluster purity for large, diverse datasets. There are other dimensions that influence cluster performance. Our practice shows that initialization could play an important role because most current algorithms are only local optimizers. Using Dirichlet before is just one way to smooth out the objective functional space, allowing others to achieve better primer prioritization."}, {"heading": "Acknowledgment", "text": "This work was financially supported by the Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN, grant number 251170; Zhirong Yang also under decision number 140398)."}], "references": [{"title": "Clustering by left-stochastic matrix factorization", "author": ["R. Arora", "M. Gupta", "A. Kapila", "M. Fazel"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Arora et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2011}, {"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Teh", "Y.-W"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Asuncion et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2001}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "In International conference on Knowledge discovery and data mining (SIGKDD),", "citeRegEx": "Ding et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2006}, {"title": "Convex and seminonnegative matrix factorizations", "author": ["C. Ding", "T. Li", "M.I. Jordan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ding et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2010}, {"title": "An inverse power method for nonlinear eigenproblems with applications in 1-spectral clustering and sparse pca", "author": ["M. Hein", "T. B\u00fchler"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hein and B\u00fchler,? \\Q2010\\E", "shortCiteRegEx": "Hein and B\u00fchler", "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In International Conference on Research and Development in Information Retrieval (SIGIR),", "citeRegEx": "Hofmann,? \\Q1999\\E", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["W. Liu", "J. He", "Chang", "S.-F"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Group invariant scattering", "author": ["S. Mallat"], "venue": "Communications in Pure and Applied Mathematics,", "citeRegEx": "Mallat,? \\Q2012\\E", "shortCiteRegEx": "Mallat", "year": 2012}, {"title": "Estimating a dirichlet distribution", "author": ["T. Minka"], "venue": null, "citeRegEx": "Minka,? \\Q2000\\E", "shortCiteRegEx": "Minka", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Component models for large networks", "author": ["J. Sinkkonen", "J. Aukia", "S. Kaski"], "venue": "ArXiv e-prints,", "citeRegEx": "Sinkkonen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sinkkonen et al\\.", "year": 2008}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "In International Conference on Research and Development in Informaion Retrieval (SIGIR),", "citeRegEx": "Xu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2003}, {"title": "Linear and nonlinear projective nonnegative matrix factorization", "author": ["Z. Yang", "E. Oja"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "Yang and Oja,? \\Q2010\\E", "shortCiteRegEx": "Yang and Oja", "year": 2010}, {"title": "Unified development of multiplicative algorithms for linear and quadratic nonnegative matrix factorization", "author": ["Z. Yang", "E. Oja"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Yang and Oja,? \\Q2011\\E", "shortCiteRegEx": "Yang and Oja", "year": 2011}, {"title": "Doubly stochastic normalization for spectral clustering", "author": ["R. Zass", "A. Shashua"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zass and Shashua,? \\Q2006\\E", "shortCiteRegEx": "Zass and Shashua", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Symmetric PLSI with the same Bayesian treatment is called Interaction Component Model (ICM) (Sinkkonen et al., 2008).", "startOffset": 92, "endOffset": 116}, {"referenceID": 1, "context": "NMF was originally applied to vectorial data, where Ding et al. (2010) have shown that NMF is equivalent to the classical k-means method.", "startOffset": 52, "endOffset": 71}, {"referenceID": 1, "context": "NMF was originally applied to vectorial data, where Ding et al. (2010) have shown that NMF is equivalent to the classical k-means method. Later NMF was applied to the (weighted) graph given by the pairwise similarities. For example, Ding et al. (2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al.", "startOffset": 52, "endOffset": 252}, {"referenceID": 0, "context": "(2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al. (2011) proposed Left Stochastic Decomposition that approximates a similarity matrix based on Euclidean distance and a left-stochastic matrix.", "startOffset": 80, "endOffset": 100}, {"referenceID": 0, "context": "(2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al. (2011) proposed Left Stochastic Decomposition that approximates a similarity matrix based on Euclidean distance and a left-stochastic matrix. Another stream in the same direction is topic modeling. Hofmann (1999) gave a generative model in Probabilistic Latent Semantic Indexing (PLSI) for counting data, which is essentially equivalent to NMF using Kullback-Leibler (KL) divergence and Tri-factorizations.", "startOffset": 80, "endOffset": 306}, {"referenceID": 0, "context": "(2008) presented Nonnegative Spectral Cuts by using a multiplicative algorithm; Arora et al. (2011) proposed Left Stochastic Decomposition that approximates a similarity matrix based on Euclidean distance and a left-stochastic matrix. Another stream in the same direction is topic modeling. Hofmann (1999) gave a generative model in Probabilistic Latent Semantic Indexing (PLSI) for counting data, which is essentially equivalent to NMF using Kullback-Leibler (KL) divergence and Tri-factorizations. Bayesian treatment of PLSI by using Dirichlet prior was later introduced by Blei et al. (2001). Symmetric PLSI with the same Bayesian treatment is called Interaction Component Model (ICM) (Sinkkonen et al.", "startOffset": 80, "endOffset": 595}, {"referenceID": 7, "context": "Some of our work was inspired by the AnchorGraph (Liu et al., 2010) which was used in large approximative graph construction based on a two-step random walk between data nodes through a set of anchor nodes.", "startOffset": 49, "endOffset": 67}, {"referenceID": 6, "context": "An early topic model was PLSI (Hofmann, 1999) which maximizes the following log-likelihood for symmetric input A: \u2211", "startOffset": 30, "endOffset": 45}, {"referenceID": 11, "context": "Bayesian treatment for the symmetric version of PLSI leads to Interaction Component Model (Sinkkonen et al., 2008).", "startOffset": 90, "endOffset": 114}, {"referenceID": 1, "context": "The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al., 2009) or clustering purity in our experiments (see Section 4).", "startOffset": 137, "endOffset": 160}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance. The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al.", "startOffset": 0, "endOffset": 183}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance. The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al., 2009) or clustering purity in our experiments (see Section 4). Hofmann (1999); Asuncion et al.", "startOffset": 0, "endOffset": 345}, {"referenceID": 1, "context": "Asuncion et al. (2009) found that wrongly chosen parameters can lead to only mediocre or even poor performance. The automatic hyperparameters updating method proposed by Minka (2000) does not necessarily lead to good solution in terms of perplexity (Asuncion et al., 2009) or clustering purity in our experiments (see Section 4). Hofmann (1999); Asuncion et al. (2009) suggested to select the hyperparameters using the smallest approximation error for some heldout matrix entries, which is however more costly and might weaken or even break the cluster structure.", "startOffset": 0, "endOffset": 369}, {"referenceID": 0, "context": "Recently Arora et al. (2011) proposed a symmetric NMF using leftstochastic factorizing matrices called LSD.", "startOffset": 9, "endOffset": 29}, {"referenceID": 3, "context": ", 2008), and 1-Spectral ratio Cheeger cut (1-Spec) (Hein & B\u00fchler, 2010); nonnegative matrix factorization including Projective NMF (PNMF) (Yang & Oja, 2010), Symmetric 3-Factor Orthogonal NMF (ONMF) (Ding et al., 2006), and LeftStochastic Decomposition (LSD) (Arora et al.", "startOffset": 200, "endOffset": 219}, {"referenceID": 0, "context": ", 2006), and LeftStochastic Decomposition (LSD) (Arora et al., 2011); topic models including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) and Interaction Component Model (ICM) (Sinkkonen et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 6, "context": ", 2011); topic models including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) and Interaction Component Model (ICM) (Sinkkonen et al.", "startOffset": 78, "endOffset": 93}, {"referenceID": 11, "context": ", 2011); topic models including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) and Interaction Component Model (ICM) (Sinkkonen et al., 2008).", "startOffset": 132, "endOffset": 156}, {"referenceID": 9, "context": "The hyperparameters in ICM are automatically adjusted by using Minka\u2019s method (Minka, 2000).", "startOffset": 78, "endOffset": 91}, {"referenceID": 8, "context": "We extracted the scattering features (Mallat, 2012) for image data except Isolet and Mfeat which have their own feature representation.", "startOffset": 37, "endOffset": 51}], "year": 2012, "abstractText": "Clustering analysis by nonnegative lowrank approximations has achieved remarkable progress in the past decade. However, most approximation approaches in this direction are still restricted to matrix factorization. We propose a new low-rank learning method to improve the clustering performance, which is beyond matrix factorization. The approximation is based on a twostep bipartite random walk through virtual cluster nodes, where the approximation is formed by only cluster assigning probabilities. Minimizing the approximation error measured by Kullback-Leibler divergence is equivalent to maximizing the likelihood of a discriminative model, which endows our method with a solid probabilistic interpretation. The optimization is implemented by a relaxed Majorization-Minimization algorithm that is advantageous in finding good local minima. Furthermore, we point out that the regularized algorithm with Dirichlet prior only serves as initialization. Experimental results show that the new method has strong performance in clustering purity for various datasets, especially for large-scale manifold data.", "creator": "LaTeX with hyperref package"}}}