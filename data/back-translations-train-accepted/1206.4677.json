{"id": "1206.4677", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching", "abstract": "In real-world classification problems, the class balance in the training dataset does not necessarily reflect that of the test dataset, which can cause significant estimation bias. If the class ratio of the test dataset is known, instance re-weighting or resampling allows systematical bias correction. However, learning the class ratio of the test dataset is challenging when no labeled data is available from the test domain. In this paper, we propose to estimate the class ratio in the test dataset by matching probability distributions of training and test input data. We demonstrate the utility of the proposed approach through experiments.", "histories": [["v1", "Mon, 18 Jun 2012 15:37:07 GMT  (352kb)", "http://arxiv.org/abs/1206.4677v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["marthinus christoffel du plessis", "masashi sugiyama"], "accepted": true, "id": "1206.4677"}, "pdf": {"name": "1206.4677.pdf", "metadata": {"source": "META", "title": "Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching", "authors": ["Marthinus Christoffel du Plessis", "Masashi Sugiyama"], "emails": ["CHRISTO@SG.CS.TITECH.AC.JP", "SUGI@CS.TITECH.AC.JP"], "sections": [{"heading": "1. Introduction", "text": "Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006). However, this de facto standard assumption is often violated in real-world problems caused by intrinsic sample selection bias or inevitable non-stationarity (Heckman, 1979; Quin-oneroCandela et al., 2009; Sugiyama & Kawanabe, 2012).In classification scenarios, changes in class balance are often observed - for example, the ratio between men and women in the real world is almost fifty-fifty (test theorem), whereas training samples collected in a research laboratory to be dominated by male data. Such a situation is referred to as class change, and the bias caused by different class balances can be systematically adjusted by a reweighting or resampling of class balances, if the test balance in 2002 is known as the Elascan (Elascan)."}, {"heading": "2. Problem Formulation and Existing Method", "text": "In this section we formulate the problem of semi-supervised estimation of classes and review an existing method (Saerens et al., 2001)."}, {"heading": "2.1. Problem Formulation", "text": "Suppose that the class-related density of the training data p (x | y) and the test data p (x | y) are the same: p (x | y) = p \u2032 (x | y). (1) Note that the training and test joint densities p (x, y) and p \u2032 (x, y) as well as the training and test entry densities p (x) and p \u2032 (x) are generally different from this viewpoint. The aim of this work is to estimate p \u2032 (y) from labeled training samples {(xi, yi)} ni = 1 independently of p (x, y) and unlabeled tests."}, {"heading": "2.2. Existing Method", "text": "We give a brief overview of an existing method for semi-supervised estimation of the previous class (Saerens et al., 2001) based on the algorithm of expectation maximization (EM) (Dempster et al., 1977). In the algorithm, the estimates of the previous and next class (y) are iteratively updated as follows: 1. Achieve an estimate of the preceding probability of the next class (Sugiyama, 2010).2. Achieve an estimate of the previous and next class (xi, yi), from the training data (xi, yi), ni = 1 (Kernel logistic regression (Hastie et al., 2001).2. Achieve an estimate of the previous class - y (y), p (y), from the designated training data p p (x), p (Bayerix)."}, {"heading": "3. Reformulation of the EM Algorithm as Distribution Matching", "text": "In this section, we show that the above EM algorithm can be interpreted as corresponding to the problem of test input density (= x). Based on the assumption that the class-related densities for education and test data are unchanged (see Eq. (1), we model the test input density p (x). (x) Based on the assumption that the class-related densities for education and test data are unchanged (see Eq. (1), we can compare the test input density p (x) with the test input density p (x). (x) Based on the assumption that the class-related densities are a coefficient corresponding to p (y). (y): c)."}, {"heading": "4. Class-Prior Estimation by Direct Divergence Minimization", "text": "The analysis in the previous section motivates us to explore a more direct way to learn coefficients {\u03b8y} cy = 1. That is, in the face of an estimator of a divergence from p \u2032 to q \u2032, coefficients {\u03b8y} cy = 1 are learned, so that the divergence estimator is minimized. In this section, we first review a general framework for approximating f divergences (Ali & Silvey, 1966; Csisza \u0301 r, 1967) by means of Legendre-fennel convex duality (Keziou, 2003; Nguyen et al., 2010). Subsequently, we review two specific methods of divergence estimation for KL divergence and Pearson (PE) divergence (Pearson, 1900). Finally, we propose to use the PE divergence estimator to determine the coefficients {\u03b8y} cy = 1."}, {"heading": "4.1. Framework of f -Divergence Approximation", "text": "An f -divergence (Ali & Silvey, 1966; Csisza \u0301 r, 1967) from p \"to q\" is a general divergence measure defined by a convex function f \"in such a way that f\" (1) = 0 asDf \"(p,\" q \") can be undercut as follows (Keziou, 2003; Nguyen et al., 2010): Df\" (p, \"q\") = maxr \"(x) r\" (x) - x \"p\" (x), dx \"(x), dx\" (9), where f \"is the convex conjugation of f.\" The maximum is reached if and only if r \"(x) / p\" (x)."}, {"heading": "4.2. KL-Divergence Approximation", "text": "With f (u) = \u2212 log u for u > 0 and + \u221e for u \u2264 0, the f -naive divergence is reduced to the KL divergence. In this f, the convex conjugation is given by f * (v) = \u2212 1 \u2212 log (\u2212 v) for v < 0 and + \u221e for v \u2265 0. However, if \u2212 \u03b1 \"is considered \u03b1 ', an empirical approximation of equation (9) under (4) and (10) is given as follows (Nguyen et al., 2010): KL (p \u00b2 q \u2032) \u2248 max {\u03b1') b '= 0 [\u2212 c \u00b2 y = 1 adventy = i: yb \u2211' (xi) + 1n \u2032 n \u2032 n \u2032 n \u2032 i = 1 log (b \u00b2) = optimized properties (b \u00b2) = optimized properties (x \u00b2)."}, {"heading": "4.3. PE-Divergence Approximation", "text": "As an alternative to the KL divergence, we consider the PE divergence defined by PE (p \u00b2 q \u00b2): = 1 2 \u00b2 (q \u00b2 (x) p \u00b2 (x) 2 p \u00b2 (x) dx, (11), which is a square-loss variant of the KL divergence and a f-divergence with f (u) = (t \u2212 1) 2 / 2.For this f, the convex conjugate is given by f \u00b2 (v) = v2 / 2 + v. Then an empirical approximation of equation (9) below (4) and (10) is given as follows (Kanamori et al., 2009a): PE (p \u00b2 q \u00b2) = max \u03b1 [\u2212 12 \u03b1 > G \u00b2 \u03b1 + \u03b1 > H \u00b2 \u00b2 \u00b2 = optimized properties \u2212 1 \u00b2 (1 \u00b2), whereas the [4] and (10) optimized properties can be considered positive."}, {"heading": "4.4. Learning Class Ratios by PE Divergence Matching", "text": "As shown above, the KL and PE divergences without density estimation can be systematically estimated using Legendre-fennel-convex duality, including the PE divergence estimator, explicitly expressed as P-E (\u03b8): = \u2212 1 2 \u03b8 > H ratios (G ratios) \u2212 1 G ratios (G ratios + \u03bbR) \u2212 1 H divergence estimator, explicitly expressed as P ratios, is more useful for the following reasons: PE divergence proved to be more robust against outliers than the KL divergence estimators, based on performance divergence analyses (Basu et al., 1998; Sugiyama et al., 2012). This is a useful property in practical data analysis suffering from high noise and outliers. Furthermore, it has been shown that the above PE divergence estimator is the minimum conditional number among a general class of divergence estimators (which means that PE is the most efficient)."}, {"heading": "5. Experiments", "text": "In this section we report on experimental results."}, {"heading": "5.1. Setup", "text": "The following five methods are compared: \u2022 EM-KLR: The method of Saerens et al. (2001) (see Section 2.2). The class-rear probability of the training data set is estimated using \"2-punished logistic regression with Gaussian nuclei.\" The L-BFGS quasi-Newton implementation included in the \"minFunc\" package is used for logistic regression training (Schmidt, 2005). \u2022 KL-KDE: The KL divergence estimator based on kernel density estimation (KDE). Class-wise input density is estimated by KDE with Gaussian nuclei. Core widths are estimated using probability cross validation (Silverman, 1986). \u2022 PE-KDE: The PE divergence estimator based on KDE. Class-wise input density is estimated by KDE with Gaussian nuclei. Kernel widths are estimated using the least square cross validation (1986)."}, {"heading": "5.2. Benchmark Datasets", "text": "We select 10 samples from each of the two classes for the training dataset and 50 samples for the test dataset. Samples of the test dataset are selected with the probability \u2082 from the first class and (1 \u2212 \u043d) from the second class, where \u03b8 * = 0.1, 0.2, 0.3, 0.4, 0.5. The average squared error of the estimated class ratio is given in Figure 1. This shows that methods based on KL and PE deviations generally perform better than EM-KLR, which means that our reformulation of the EM algorithm as distribution matching (see Section 3) helps to obtain accurate estimates of the class ratio. Among KL-KDE methods, they tend to perform better than KL-DR. This is because in KL-KDE we did not estimate the first term as distribution matrix, which is negative entropy and a constant."}, {"heading": "5.3. Real-World Application", "text": "Finally, we show the usefulness of the proposed approach to a real problem of classification of military vehicles using geophone recordings (Duarte & Hu, 2004).In this task of vehicle classification, a class change is inevitable, since the type of vehicles passing through varies according to time (e.g. day and night).n samples are taken from each of the designated classes for training with the previous uniform class, whereas 100 samples with probabilities p = [0.6 0.1 0.3] are taken from each of the classes for the test set. Due to the prohibitive cost of calculating KL-DR, this experiment was not factored.In Figure 3, we plot the \"2 distance between the true and estimated class before classification and the misclassification rate based on incession-weighted logistical regressions (Hastie al., 2001) compared to 1000 training runs as the most accurate number of training runs."}, {"heading": "6. Conclusion", "text": "In this paper, we discussed the problem of estimating the test class ratios in the context of semi-supervised learning composition. First, we showed that the EM-based estimator introduced in Saerens et al. (2001) can be considered as an indirect agreement with the test entry distribution by a linear combination of classic input distributions. Based on this view, we proposed to use an explicit and possibly more precise divergence estimator based on the density ratio estimation (Kanamori et al., 2009a) for learning test class priorities.The proposed method showed several beautiful properties such as high robustness to noise and outliers, superior numerical stability and excellent computing efficiency.Through experiments, we demonstrated that the class ratios estimated by the proposed method are more accurate than competing methods that can be translated into better precision."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their helpful comments. MCdP was supported by the MEXT scholarship, MS by AOARD and JST PRESTO."}], "references": [{"title": "A general class of coefficients of divergence of one distribution from another", "author": ["S.M. Ali", "S.D. Silvey"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Ali and Silvey,? \\Q1966\\E", "shortCiteRegEx": "Ali and Silvey", "year": 1966}, {"title": "Robust and efficient estimation by minimising a density power divergence", "author": ["A. Basu", "I.R. Harris", "N.L. Hjort", "M.C. Jones"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Basu et al\\.", "year": 1998}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Estimating class priors in domain adaptation for word sense disambiguation", "author": ["Y.S. Chan", "H.T. Ng"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics,", "citeRegEx": "Chan and Ng,? \\Q2006\\E", "shortCiteRegEx": "Chan and Ng", "year": 2006}, {"title": "SemiSupervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "Zien", "A. (eds"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "AUC optimization and the two-sample problem", "author": ["S. Cl\u00e9men\u00e7on", "N. Vayatis", "M. Depecker"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cl\u00e9men\u00e7on et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on et al\\.", "year": 2009}, {"title": "AUC optimization vs. error rate minimization", "author": ["C. Cortes", "M. Mohri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cortes and Mohri,? \\Q2004\\E", "shortCiteRegEx": "Cortes and Mohri", "year": 2004}, {"title": "Information-type measures of difference of probability distributions and indirect observation", "author": ["I. Csisz\u00e1r"], "venue": "Studia Scientiarum Mathematicarum Hungarica,", "citeRegEx": "Csisz\u00e1r,? \\Q1967\\E", "shortCiteRegEx": "Csisz\u00e1r", "year": 1967}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Vehicle classification in distributed sensor networks", "author": ["M.F. Duarte", "Y.H. Hu"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "Duarte and Hu,? \\Q2004\\E", "shortCiteRegEx": "Duarte and Hu", "year": 2004}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Elkan,? \\Q2001\\E", "shortCiteRegEx": "Elkan", "year": 2001}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Sample selection bias as a specification", "author": ["J.J. Heckman"], "venue": "error. Econometrica,", "citeRegEx": "Heckman,? \\Q1979\\E", "shortCiteRegEx": "Heckman", "year": 1979}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Condition number analysis of kernel-based density ratio estimation", "author": ["T. Kanamori", "T. Suzuki", "M. Sugiyama"], "venue": "Technical report,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Statistical analysis of kernel-based least-squares density-ratio estimation", "author": ["T. Kanamori", "T. Suzuki", "M. Sugiyama"], "venue": "Machine Learning,", "citeRegEx": "Kanamori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2012}, {"title": "Dual representation of \u03c6-divergences and applications", "author": ["A. Keziou"], "venue": "Comptes Rendus Mathe\u0301matique,", "citeRegEx": "Keziou,? \\Q2003\\E", "shortCiteRegEx": "Keziou", "year": 2003}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Kullback and Leibler,? \\Q1951\\E", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Support vector machines for classification in nonstandard situations", "author": ["Y. Lin", "Y. Lee", "G. Wahba"], "venue": "Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2002}, {"title": "The EM algorithm and extensions. Wiley series in probability and statistics: Applied probability and statistics", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan,? \\Q1997\\E", "shortCiteRegEx": "McLachlan and Krishnan", "year": 1997}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "author": ["K. Pearson"], "venue": null, "citeRegEx": "Pearson,? \\Q1900\\E", "shortCiteRegEx": "Pearson", "year": 1900}, {"title": "Dataset Shift in Machine Learning", "author": ["J. Qui\u00f1onero-Candela", "M. Sugiyama", "A. Schwaighofer", "Lawrence", "N. (eds"], "venue": null, "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2009}, {"title": "Regularized leastsquares classification. Advances in Learning Theory: Methods, Model and Applications", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": "NATO Science Series III: Computer and Systems Sciences,", "citeRegEx": "Rifkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2003}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar,? \\Q1970\\E", "shortCiteRegEx": "Rockafellar", "year": 1970}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure", "author": ["M. Saerens", "M. Patrice", "C. Decaestecker"], "venue": "Neural Computation,", "citeRegEx": "Saerens et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2001}, {"title": "minFunc\u2014Unconstrained differentiable multivariate optimization in MATLAB", "author": ["M. Schmidt"], "venue": null, "citeRegEx": "Schmidt,? \\Q2005\\E", "shortCiteRegEx": "Schmidt", "year": 2005}, {"title": "Density Estimation: For Statistics and Data Analysis", "author": ["B.W. Silverman"], "venue": null, "citeRegEx": "Silverman,? \\Q1986\\E", "shortCiteRegEx": "Silverman", "year": 1986}, {"title": "Superfast-trainable multi-class probabilistic classifier by least-squares posterior fitting", "author": ["M. Sugiyama"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Sugiyama,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama", "year": 2010}, {"title": "Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation", "author": ["M. Sugiyama", "M. Kawanabe"], "venue": null, "citeRegEx": "Sugiyama and Kawanabe,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama and Kawanabe", "year": 2012}, {"title": "Direct importance estimation for covariate shift adaptation", "author": ["M. Sugiyama", "T. Suzuki", "S. Nakajima", "H. Kashima", "P. von B\u00fcnau", "M. Kawanabe"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "Density ratio matching under the Bregman divergence: A unified framework of density ratio estimation", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1998\\E", "shortCiteRegEx": "Vapnik", "year": 1998}], "referenceMentions": [{"referenceID": 33, "context": "Introduction Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006).", "startOffset": 125, "endOffset": 174}, {"referenceID": 12, "context": "Introduction Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006).", "startOffset": 125, "endOffset": 174}, {"referenceID": 2, "context": "Introduction Most supervised learning algorithms assume that training and test data follow the same probability distribution (Vapnik, 1998; Hastie et al., 2001; Bishop, 2006).", "startOffset": 125, "endOffset": 174}, {"referenceID": 13, "context": "However, this de facto standard assumption is often violated in realworld problems, caused by intrinsic sample selection bias or inevitable non-stationarity (Heckman, 1979; Qui\u00f1oneroCandela et al., 2009; Sugiyama & Kawanabe, 2012).", "startOffset": 157, "endOffset": 230}, {"referenceID": 11, "context": "Such a situation is called a class-prior change, and the bias caused by differing class balances can be systematically adjusted by instance re-weighting or resampling if the class balance in the test dataset is known (Elkan, 2001; Lin et al., 2002).", "startOffset": 217, "endOffset": 248}, {"referenceID": 19, "context": "Such a situation is called a class-prior change, and the bias caused by differing class balances can be systematically adjusted by instance re-weighting or resampling if the class balance in the test dataset is known (Elkan, 2001; Lin et al., 2002).", "startOffset": 217, "endOffset": 248}, {"referenceID": 6, "context": ", through maximization of the area under the ROC curve (Cortes & Mohri, 2004; Cl\u00e9men\u00e7on et al., 2009).", "startOffset": 55, "endOffset": 101}, {"referenceID": 5, "context": "In this paper, we focus on the latter scenario under a semi-supervised learning setup (Chapelle et al., 2006), where no labeled data is available from the test domain.", "startOffset": 86, "endOffset": 109}, {"referenceID": 9, "context": "(2001) is a seminal paper on this topic, which proposed to estimate the class ratio by the expectationmaximization (EM) algorithm (Dempster et al., 1977)\u2014 alternately updating the test class-prior and class-posterior probabilities from some initial estimates until convergence.", "startOffset": 130, "endOffset": 153}, {"referenceID": 12, "context": "In this procedure, the class-wise input distributions are approximated via classposterior estimation, for example, by kernel logistic regression (Hastie et al., 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 145, "endOffset": 166}, {"referenceID": 29, "context": ", 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 36, "endOffset": 52}, {"referenceID": 21, "context": "Recently, KL divergence estimation based on direct density-ratio estimation has been shown to be promising (Nguyen et al., 2010; Sugiyama et al., 2008).", "startOffset": 107, "endOffset": 151}, {"referenceID": 31, "context": "Recently, KL divergence estimation based on direct density-ratio estimation has been shown to be promising (Nguyen et al., 2010; Sugiyama et al., 2008).", "startOffset": 107, "endOffset": 151}, {"referenceID": 22, "context": "Furthermore, a squared-loss variant of the KL divergence called the Pearson (PE) divergence (Pearson, 1900) can also be approximated in the same way, with an analytic solution that can be computed efficiently (Kanamori et al.", "startOffset": 92, "endOffset": 107}, {"referenceID": 2, "context": ", 2001; Bishop, 2006). However, this de facto standard assumption is often violated in realworld problems, caused by intrinsic sample selection bias or inevitable non-stationarity (Heckman, 1979; Qui\u00f1oneroCandela et al., 2009; Sugiyama & Kawanabe, 2012). In classification scenarios, changes in class balance are often observed\u2014for example, the male-female ratio is almost fifty-fifty in the real-world (test set), whereas training samples collected in a research laboratory tends to be dominated by male data. Such a situation is called a class-prior change, and the bias caused by differing class balances can be systematically adjusted by instance re-weighting or resampling if the class balance in the test dataset is known (Elkan, 2001; Lin et al., 2002). However, the class ratio in the test dataset is often unknown Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s). in practice. A possible approach to coping with this problem is to learn a classifier so that the performance for all possible class balances is improved, e.g., through maximization of the area under the ROC curve (Cortes & Mohri, 2004; Cl\u00e9men\u00e7on et al., 2009). Another, possibly more direct approach is to estimate the class ratio in the test dataset and use the estimates for instance re-weighting or resampling. In this paper, we focus on the latter scenario under a semi-supervised learning setup (Chapelle et al., 2006), where no labeled data is available from the test domain. Saerens et al. (2001) is a seminal paper on this topic, which proposed to estimate the class ratio by the expectationmaximization (EM) algorithm (Dempster et al.", "startOffset": 8, "endOffset": 1585}, {"referenceID": 26, "context": "Problem Formulation and Existing Method In this section, we formulate the problem of semisupervised class-prior estimation and review an existing method (Saerens et al., 2001).", "startOffset": 153, "endOffset": 175}, {"referenceID": 26, "context": "Existing Method We give a brief overview of an existing method for semisupervised class-prior estimation (Saerens et al., 2001), which is based on the expectation-maximization (EM) algorithm (Dempster et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 9, "context": ", 2001), which is based on the expectation-maximization (EM) algorithm (Dempster et al., 1977).", "startOffset": 71, "endOffset": 94}, {"referenceID": 12, "context": "Obtain an estimate of the training class-posterior probability, p\u0302(y|x), from training data {(xi, yi)}ni=1, for example, by kernel logistic regression (Hastie et al., 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 151, "endOffset": 172}, {"referenceID": 29, "context": ", 2001) or its squared-loss variant (Sugiyama, 2010).", "startOffset": 36, "endOffset": 52}, {"referenceID": 26, "context": "However, this was not recognized in Saerens et al. (2001) since the algorithm was derived via the incomplete data EM method.", "startOffset": 36, "endOffset": 58}, {"referenceID": 8, "context": "In this section, we first review a general framework of approximating the f -divergences (Ali & Silvey, 1966; Csisz\u00e1r, 1967) via Legendre-Fenchel convex duality (Keziou, 2003; Nguyen et al.", "startOffset": 89, "endOffset": 124}, {"referenceID": 17, "context": "In this section, we first review a general framework of approximating the f -divergences (Ali & Silvey, 1966; Csisz\u00e1r, 1967) via Legendre-Fenchel convex duality (Keziou, 2003; Nguyen et al., 2010).", "startOffset": 161, "endOffset": 196}, {"referenceID": 21, "context": "In this section, we first review a general framework of approximating the f -divergences (Ali & Silvey, 1966; Csisz\u00e1r, 1967) via Legendre-Fenchel convex duality (Keziou, 2003; Nguyen et al., 2010).", "startOffset": 161, "endOffset": 196}, {"referenceID": 22, "context": "Then we review two specific methods of divergence estimation for the KL divergence and the Pearson (PE) divergence (Pearson, 1900).", "startOffset": 115, "endOffset": 130}, {"referenceID": 8, "context": "Framework of f -Divergence Approximation An f -divergence (Ali & Silvey, 1966; Csisz\u00e1r, 1967) from p\u2032 to q\u2032 is a general divergence measure defined by a convex function f such that f(1) = 0 as", "startOffset": 58, "endOffset": 93}, {"referenceID": 25, "context": "It was shown that the f -divergence can be lower-bounded via Legendre-Fenchel convex duality (Rockafellar, 1970) as follows (Keziou, 2003; Nguyen et al.", "startOffset": 93, "endOffset": 112}, {"referenceID": 17, "context": "It was shown that the f -divergence can be lower-bounded via Legendre-Fenchel convex duality (Rockafellar, 1970) as follows (Keziou, 2003; Nguyen et al., 2010):", "startOffset": 124, "endOffset": 159}, {"referenceID": 21, "context": "It was shown that the f -divergence can be lower-bounded via Legendre-Fenchel convex duality (Rockafellar, 1970) as follows (Keziou, 2003; Nguyen et al., 2010):", "startOffset": 124, "endOffset": 159}, {"referenceID": 21, "context": "This provides a non-parametric divergence estimator (Nguyen et al., 2010; Sugiyama et al., 2008; Kanamori et al., 2012).", "startOffset": 52, "endOffset": 119}, {"referenceID": 31, "context": "This provides a non-parametric divergence estimator (Nguyen et al., 2010; Sugiyama et al., 2008; Kanamori et al., 2012).", "startOffset": 52, "endOffset": 119}, {"referenceID": 16, "context": "This provides a non-parametric divergence estimator (Nguyen et al., 2010; Sugiyama et al., 2008; Kanamori et al., 2012).", "startOffset": 52, "endOffset": 119}, {"referenceID": 21, "context": "(9) under (4) and (10) is given as follows (Nguyen et al., 2010):", "startOffset": 43, "endOffset": 64}, {"referenceID": 31, "context": "A similar approach, which directly estimates the inverted ratio p\u2032(x)/q\u2032(x) with the same model (10), is also known (Sugiyama et al., 2008):", "startOffset": 116, "endOffset": 139}, {"referenceID": 31, "context": "Tuning parameters possibly included in the basis function such as the kernel width can be systematically optimized by cross-validation (Sugiyama et al., 2008).", "startOffset": 135, "endOffset": 158}, {"referenceID": 31, "context": "The KL-divergence estimator obtained above was proved to possess superior convergence properties both in parametric and non-parametric setups (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 142, "endOffset": 186}, {"referenceID": 21, "context": "The KL-divergence estimator obtained above was proved to possess superior convergence properties both in parametric and non-parametric setups (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 142, "endOffset": 186}, {"referenceID": 1, "context": "is more useful for our purpose of learning class ratios, because of the following reasons: The PE-divergence was shown to be more robust against outliers than the KLdivergence, based on power divergence analysis (Basu et al., 1998; Sugiyama et al., 2012).", "startOffset": 212, "endOffset": 254}, {"referenceID": 32, "context": "is more useful for our purpose of learning class ratios, because of the following reasons: The PE-divergence was shown to be more robust against outliers than the KLdivergence, based on power divergence analysis (Basu et al., 1998; Sugiyama et al., 2012).", "startOffset": 212, "endOffset": 254}, {"referenceID": 27, "context": "The L-BFGS quasi-Newton implementation included in the \u2018minFunc\u2019 package is used for logistic regression training (Schmidt, 2005).", "startOffset": 114, "endOffset": 129}, {"referenceID": 28, "context": "The kernel widths are estimated using likelihood cross-validation (Silverman, 1986).", "startOffset": 66, "endOffset": 83}, {"referenceID": 28, "context": "The kernel widths are estimated using least-squares cross-validation (Silverman, 1986).", "startOffset": 69, "endOffset": 86}, {"referenceID": 26, "context": "\u2022 EM-KLR: The method of Saerens et al. (2001) (see Section 2.", "startOffset": 24, "endOffset": 46}, {"referenceID": 27, "context": "For the optimization, the L-BFGS with projection implementation \u2018minFuncBC\u2019 is used (Schmidt, 2005).", "startOffset": 84, "endOffset": 99}, {"referenceID": 24, "context": "Figure 2 shows misclassification rates for a regularized leastsquares classifier (Rifkin et al., 2003) with instance weighting.", "startOffset": 81, "endOffset": 102}, {"referenceID": 12, "context": "In Figure 3, we plot the `2-distance between the true and estimated class priors and the misclassification rate based on instance-weighted kernel logistic regression (Hastie et al., 2001) averaged over 1000 runs as functions of the number of training samples.", "startOffset": 166, "endOffset": 187}, {"referenceID": 23, "context": "We first showed that the EM-based estimator introduced in Saerens et al. (2001) can be regarded as indirectly matching the test input distribution by a linear combination of classwise input distributions.", "startOffset": 58, "endOffset": 80}], "year": 2012, "abstractText": "In real-world classification problems, the class balance in the training dataset does not necessarily reflect that of the test dataset, which can cause significant estimation bias. If the class ratio of the test dataset is known, instance re-weighting or resampling allows systematical bias correction. However, learning the class ratio of the test dataset is challenging when no labeled data is available from the test domain. In this paper, we propose to estimate the class ratio in the test dataset by matching probability distributions of training and test input data. We demonstrate the utility of the proposed approach through experiments.", "creator": "LaTeX with hyperref package"}}}