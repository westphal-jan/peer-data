{"id": "1206.4679", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Factorized Asymptotic Bayesian Hidden Markov Models", "abstract": "This paper addresses the issue of model selection for hidden Markov models (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. As with FAB in mixture models, FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion (FIC). It inherits, from FAB for mixture models, several desirable properties for learning HMMs, such as asymptotic consistency of FIC with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower FIC bound through the iterative optimization. Further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. Experimental results shows that FAB outperforms states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in terms of model selection accuracy and computational efficiency.", "histories": [["v1", "Mon, 18 Jun 2012 15:37:59 GMT  (599kb)", "http://arxiv.org/abs/1206.4679v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ryohei fujimaki", "kohei hayashi"], "accepted": true, "id": "1206.4679"}, "pdf": {"name": "1206.4679.pdf", "metadata": {"source": "META", "title": "Factorized Asymptotic Bayesian Hidden Markov Models", "authors": ["Ryohei Fujimaki", "Kohei Hayashi"], "emails": ["rfujimaki@sv.nec-labs.com", "hayashi.kohei@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is as if it were a matter of a way as it has been observed in the USA in recent years. (...) In fact, it is as if it were a matter of a way as it has developed in the USA and in Europe. (...) It is as if it were a way as it has developed in the USA and in the USA. (...) It is as if it were a way as it has developed in the USA and in the USA and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of the United States of the United States and in the United States of the United States of the United States and in the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States and in the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United States of the United"}, {"heading": "2. Preliminaries", "text": "Let X = X1,. \u03b2, XT and Z = Z1,., ZT be each sequence of observed and hidden random variables. Zt = (Zt1,. Z t K) is an indicator vector, and Ztk = 1 if X t is generated from the k-th hidden state, and Ztk = 0 else. We call the number of hidden states K. Let's assume that we observe the length of the n-th sequences2 and designate it as xN = x1., xN. The n-th sequence is called xn = x n,., x Tn, where Tn is the length of the n-th sequence. We designate the sequence of latent variables according to xN = z1,."}, {"heading": "3. FIC for HMMs", "text": "The following lower limit of hidden states, A1, and A2, which does not apply the hidden variables directly, is not directly used by us as a model for time-dependent hidden variables (e.g. HMMs).Based on the Markov status of hidden states, A1, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A2, A3, 3, 3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3, A3,"}, {"heading": "4. FAB for HMMs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. FAB Lower Bound", "text": "Since both xN and zN are defined and therefore not available in practice, we cannot evaluate q q q = q (q = q) ourselves. Instead, FAB maximizes the lower limit of FIChmm. As is done in (Fujimaki & Morinaga, 2012) similarly, we use two q q q = q q (q = q = q) to derive the lower limit. First, based on the definition of the ML estimator, log p (xN, zN \u2212 n). Second, based on the concavity of the logarithm function, log (n, Tn, t = 1 z t nk)."}, {"heading": "4.2. Iterative Optimization with FAB Forward-Backward Algorithm", "text": "Since their simultaneous optimization would be intractable, FABhmm works on the basis of iterations of two partial steps (V-step and M-step). Let's leave the extrapolation (i-1) on the basis of (12). FABhmm then optimizes q by maximizing in (9). The conditions in (9) dependence on q can be decompiled with respect to the sequences, and therefore we can optimize the differences with respect to the sequences independently."}, {"heading": "4.3. Convergence and Stopping Criterion", "text": "Let us define the lower limit of the FIC as follows: FIC (i) LB (x N, M) \u2261 G (q (i), q (i) = q (i), xN, \u03b8 (i)). (17) Then, as with blending models (Fujimaki & Morinaga, 2012), the FABhmm is guaranteed to increase monotonous FIC (i) LB (xN, M) by the VM iterations, which can be summarized in the following theory. (18) We apply FIC (i) LB (x N, M) \u2212 FIC (i \u2212 1) LB (xN, M) \u2265 FIC (x N, M) LB (18)."}, {"heading": "4.4. Automatic Hidden State Selection", "text": "An interesting property of the asymptotic exposed regulator \u03b4tk (10) q is a shrinkage effect for selecting the number of hidden states that we specified in the previous section, and thus, despite our asymptotic ignorance of priorities, it offers an excessive mitigation. In (13), f t (i) nk with a small \u03b4 t (i \u2212 1) k value is largely regulated, without reference to observation and previous paths. While b t (i) nk has no explicit regulating effect, a noteworthy fact is that any next path b + 1 (i) nj with a large incret (i + 1) j value makes only a small contribution to updating b t (i) nk. Then, in (14), the k-th hidden state with a small \u043c t (i \u2212 1) k value for the smallest states b (i \u2212 1) k value of the smallest size, Tk (n), n (tnt) q (n)."}, {"heading": "4.5. Discussion: Comparison with VB and BIC", "text": "Here we compare three approximation inference methods (FAB, VB and BIC) and discuss their differences. FAB approaches marginal log probabilities by (19). Let us then calculate normalization constants for forward-looking algorithms of ML and VB estimates as \"q\" (ML) and \"t\" (V B), respectively \"t\" (V B), variable free energy FV B and BIC4 respectively (see (Beal, 2003) for VB free energy): \"Ms B = N, Tn\" n, t = 1 Log \"tn\" (V B) + \"n\" Log \"(\u03b1) Log\" r \"n\" (\u03b1) + \"d\" r \"(\u03b2) q\" (\u03b2) + \"K\" k \"k\" (1 Log \"),\" t \"c\" n \"(B),\" c \"t\" n \"(B),\" c \"n\" (B), \"V\" (B \"(\" \"),\" c \",\" \"\" n."}, {"heading": "5. Experiments and Discussion", "text": "We performed simulations using artificial data to examine basic behaviors of HMMs with FAB (FABHMMs) and evaluated them against real ebook data. FABHMM was compared with VBHMM, iHMM and HMMs with ML estimation and BIC model selection (MLHMMs). We implemented FABHMM and MLHMM from Python, while we used Matlab software for iHMM5 and VBHM6."}, {"heading": "5.1. Simulations with Artificial Data", "text": "The true model had four hidden states, either Gaussian emission probabilities with averages (\u2212 4, \u2212 1, 2, 3) and variances (0.5, 0.5, 0.5) or categorical emission probabilities with 8 alphabet and a transition probability described as follows: (0 1 0 0 1 0 0 0 0 0 0 0 0) / 3, with the left and right matrices corresponding to transition and categorical emission probabilities. We set the initial probability of the first state as one and the remainders as zero."}, {"heading": "T FAB ML iHMM1 iHMM2 VB", "text": "The results below are the averages of ten runs. Table 1 shows that FABHMM estimated the true number of hidden states almost perfectly for both types of emissions. Surprisingly, although FAB is an asymptotic method, it outperformed the others with relatively small data sizes. The iHMMs also performed well, but their results were somehow influenced by a choice of hyperpriorities.VBHMM and MLHMM were significantly inferior to the others in terms of model selection performance."}, {"heading": "5.2. E-Book Character Sequences", "text": "Next, we evaluated the feasibility of text predictions. We prepared six books7, Alice's Adventures in Wonderland (Alice), THe Art of War (Sunzi), The Metamorphosis (Kafka), The Republic (Plato), The United States Declaration of Independence (DOI) and The Adventures of Sherlock Holmes (Sherl). In this context, each letter in the texts was treated as a categorical observation, the letters contained some special characters, and the number of categorical alphabets varied from 32 to 50 in these records. Here, we compared FAB with iHMMM1 and VBHMM. Because these data are more complicated than artificial data, we used the next 5000 letters for testing. For fair comparison, we eliminated the alphabets from the test records that did not appear in the training sets. Here, we compared FAB with iHMMMMM1 and VBHMM. Because these data are more complicated than artificial data, we set the MMMMMM max performance for MHMMMMMMM and MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMAMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMU performance and MMMMMMMMMMMMMMMU and MMMMMMMMMMMMMMMMM"}, {"heading": "Data K Time PLL K Time PLL K Time PLL", "text": "This means that FABHMM could get more compact HMM representations than iHMM, which is usually desirable in practice. Table 2 shows the training times, the estimated number of hidden states and PLLs (T = 5000), for all data, comparable in terms of PLLs, FABHMM and iHMM, and worst in terms of VBHMM. FABHMM's training times were approximately three or four times faster, which is consistent with the results in the previous paragraph. These results point to clear advantages of FABHMM in terms of accuracy of model selection over VBHMM, and in terms of computational efficiency over iHMM. Finally, we stress that FABHMM has no hyperparameters in its criterion, and all of the above strict selection procedures were carried out automatically."}, {"heading": "6. Summary and Future Work", "text": "We have theoretically demonstrated several desirable properties (asymptotic FIC consistency, automatic shrinkage effect, monotonous increase in the FIC lower limit, etc.) and experimentally demonstrated that FABHMM outperforms Bayesian and non-parametric state variations in terms of accuracy of model selection and computational efficiency. We have several problems for future studies: first, it is interesting to expand the FAB for more flexible HMM families, as the iHMM was extended for factorial HMM (Ghahramani & Jordan, 1997; van Gael et al., 2009); second, how both FABmm and FABhmm are designed for discrete hidden variables, and therefore the FAB is still an open problem for continuous hidden variable models; third, logically, for example, details of FIC convergence (fast convergence) should be investigated."}, {"heading": "Beal, M. J., Ghahramani, Z., and Rasmussen, C. E. The", "text": "Infinite hidden Markov model. In NIPS, 2002."}, {"heading": "Bratieres, S., van Gael, J., Vlachos, A., and Ghahramani,", "text": "Z. Scaling the iHMM: Parallelization versus Hadoop. In SMLA, 2010. Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum probability from incomplete data on the EM algorithm. Journal of the Royal Statistical Society, B39 (1): 1-38, 1977."}, {"heading": "Fujimaki, Ryohei and Morinaga, Satoshi. Factorized", "text": "Asymptotic Bayesian inference for hybrid models. In AISTATS, 2012."}, {"heading": "Fujimaki, Ryohei, Sogawa, Yasuhiro, and Morinaga,", "text": "Satoshi. Online heterogeneous hybrid modeling with edge and copula selection. In KDD, 2011. Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models. Machine Learning, 29: 245-273, 1997. Kurihara, Kenichi, Welling, Max, and Teh, Yee Whye. Collapsed variational dirichlet process mixture models. In IJCAI, pp. 2796-2801, 2007.MacKay, D. J. C. Ensemble learning for hidden markov models. Technical report, 1997.Rabiner, L. R. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of IEEE, 77: 257-86, 1989."}, {"heading": "Robert, Christian P., Rydn, Tobias, and Titterington,", "text": "D.M. Bayesian inference in hidden markov models through reversible jump markov chain monte carlo. Journal of the Royal Statistical Society, Series B, 62: 57-75, 2000.Schwarz, Gideon. Estimating the dimension of a model. The Annals of Statistics, 6 (2): 461-464, 1978.Teh, Yee Whye, Newman, Dave, and Welling, Max. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In NIPS, pp. 1378-1385, 2006.van Gael, J., Saatci, Y, Teh, Y.-W., and Ghahramani, Z. Beam sampling for the infinite hidden Markov model. In ICML, 2008.van Gael, J., Teh, Y.-W., and Ghahramani, Z. The infinite factorial hidden Markov model. In NIPS, 2009."}, {"heading": "Yamazaki, K. and Watanabe, S. Algebraic geometry and", "text": "stochastic complexity of hidden Markov models. Neurocomputing, 69: 62-84, 2005."}], "references": [{"title": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes", "author": ["L.E. Baum"], "venue": "Inequalities, 3:1\u20138,", "citeRegEx": "Baum,? \\Q1972\\E", "shortCiteRegEx": "Baum", "year": 1972}, {"title": "Variational Algorithms for Approximate Bayesian Inference", "author": ["M.J. Beal"], "venue": "PhD thesis,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "The infinite hidden Markov model", "author": ["M.J. Beal", "Z. Ghahramani", "C.E. Rasmussen"], "venue": "In NIPS,", "citeRegEx": "Beal et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Beal et al\\.", "year": 2002}, {"title": "Scaling the iHMM: Parallelization versus Hadoop", "author": ["S. Bratieres", "J. van Gael", "A. Vlachos", "Z. Ghahramani"], "venue": "In SMLA,", "citeRegEx": "Bratieres et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bratieres et al\\.", "year": 2010}, {"title": "Maximum likelihood from imcomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Factorized asymptotic bayesian inference for mixture models", "author": ["Fujimaki", "Ryohei", "Morinaga", "Satoshi"], "venue": "In AISTATS,", "citeRegEx": "Fujimaki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fujimaki et al\\.", "year": 2012}, {"title": "Online heterogeneous mixture modeling with marginal and copula selection", "author": ["Fujimaki", "Ryohei", "Sogawa", "Yasuhiro", "Morinaga", "Satoshi"], "venue": "In KDD,", "citeRegEx": "Fujimaki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fujimaki et al\\.", "year": 2011}, {"title": "Factorial hidden markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Ghahramani and Jordan,? \\Q1997\\E", "shortCiteRegEx": "Ghahramani and Jordan", "year": 1997}, {"title": "Collapsed variational dirichlet process mixture models", "author": ["Kurihara", "Kenichi", "Welling", "Max", "Teh", "Yee Whye"], "venue": "In IJCAI, pp", "citeRegEx": "Kurihara et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kurihara et al\\.", "year": 2007}, {"title": "Ensemble learning for hidden Markov models", "author": ["D.J.C. MacKay"], "venue": "Technical report,", "citeRegEx": "MacKay,? \\Q1997\\E", "shortCiteRegEx": "MacKay", "year": 1997}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of IEEE,", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Bayesian inference in hidden markov models through reversible jump markov chain monte carlo", "author": ["Robert", "Christian P", "Rydn", "Tobias", "D.M. Titterington"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Robert et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Robert et al\\.", "year": 2000}, {"title": "Estimating the dimension of a model", "author": ["Schwarz", "Gideon"], "venue": "The Annals of Statistics,", "citeRegEx": "Schwarz and Gideon.,? \\Q1978\\E", "shortCiteRegEx": "Schwarz and Gideon.", "year": 1978}, {"title": "A collapsed variational bayesian inference algorithm for latent dirichlet allocation", "author": ["Teh", "Yee Whye", "Newman", "Dave", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Beam sampling for the infinite hidden Markov model", "author": ["J. van Gael", "Y Saatci", "Teh", "Y.-W", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "Gael et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gael et al\\.", "year": 2008}, {"title": "The infinite factorial hidden Markov model", "author": ["J. van Gael", "Teh", "Y.-W", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Gael et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gael et al\\.", "year": 2009}, {"title": "Algebraic geometry and stochastic complexities of hidden Markov models", "author": ["K. Yamazaki", "S. Watanabe"], "venue": null, "citeRegEx": "Yamazaki and Watanabe,? \\Q2005\\E", "shortCiteRegEx": "Yamazaki and Watanabe", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": "Markov chain Monte Carlo methods (MCMCs) (Robert et al., 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques.", "startOffset": 41, "endOffset": 62}, {"referenceID": 9, "context": ", 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques.", "startOffset": 48, "endOffset": 74}, {"referenceID": 1, "context": ", 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques.", "startOffset": 48, "endOffset": 74}, {"referenceID": 2, "context": "In terms of modeling, infinite HMMs (iHMMs) employ a hierarchical Dirichlet process prior in order to express an infinite number of hidden states (Beal et al., 2002).", "startOffset": 146, "endOffset": 165}, {"referenceID": 2, "context": "(2008) uses a beam sampling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al., 2002).", "startOffset": 104, "endOffset": 123}, {"referenceID": 3, "context": "Although the beam sampling technique considerably reduces the computational cost of MCMC inference, it is still higher than that of HMMs using variational non-parametric Bayesian inference (VBHMMs), while acceleration of iHMMs has been discussed from the viewpoints of parallelization (Bratieres et al., 2010).", "startOffset": 285, "endOffset": 309}, {"referenceID": 1, "context": ", 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analytically intractable marginal log-likelihoods, using respectively, sampling and variational approximation techniques. The former has an advantage over the latter in approximation accuracy but has a disadvantage in computational efficiency, and thus the choice of appropriate inference algorithm has been decided on the basis of a trade-off between accuracy and computational efficiency. In terms of modeling, infinite HMMs (iHMMs) employ a hierarchical Dirichlet process prior in order to express an infinite number of hidden states (Beal et al., 2002). In them, the number of components is determined on the basis of mild prior knowledge expressed by a few hyperparameters. The state-of-the-art inference of iHMMs proposed by van Gael et al. (2008) uses a beam sampling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al.", "startOffset": 63, "endOffset": 861}, {"referenceID": 4, "context": "The iterative optimization of FABhmm can be seen as a natural generalization of the expectation-maximization (EM) algorithm (Dempster et al., 1977), and, interestingly, unique regularizers appear as exponentiated update terms in our FAB forward-backward algorithm.", "startOffset": 124, "endOffset": 147}, {"referenceID": 10, "context": "A standard parameter inference follows the EM algorithm with a specific expectation step known as either the forward-backward algorithm (Rabiner, 1989) or the Baum-Welch algorithm (Baum, 1972).", "startOffset": 136, "endOffset": 151}, {"referenceID": 0, "context": "A standard parameter inference follows the EM algorithm with a specific expectation step known as either the forward-backward algorithm (Rabiner, 1989) or the Baum-Welch algorithm (Baum, 1972).", "startOffset": 180, "endOffset": 192}, {"referenceID": 6, "context": "This can be seen as an HMM-extension of the so-called \u201cheterogeneous mixture models\u201d (Fujimaki et al., 2011).", "startOffset": 85, "endOffset": 108}, {"referenceID": 1, "context": "Notably, these two regularizers contain dependencies between parameters (\u03b2k and \u03c6k) and hidden states, which the variational lower bound of VB methods usually ignore on their variational distributions (Beal, 2003).", "startOffset": 201, "endOffset": 213}, {"referenceID": 1, "context": "We can solve this problem using a way similar to that for the variational free energy for VBHMMs (Beal, 2003).", "startOffset": 97, "endOffset": 109}, {"referenceID": 1, "context": "Variational free energy FV B and BIC can then be respectively computed as follows (see (Beal, 2003) for VB free energy):", "startOffset": 87, "endOffset": 99}], "year": 2012, "abstractText": "This paper addresses the issue of model selection for hidden Markov models (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. As with FAB in mixture models, FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion (FIC). It inherits, from FAB for mixture models, several desirable properties for learning HMMs, such as asymptotic consistency of FIC with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower FIC bound through the iterative optimization. Further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. Experimental results shows that FAB outperforms states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in terms of model selection accuracy and computational efficiency.", "creator": "LaTeX with hyperref package"}}}