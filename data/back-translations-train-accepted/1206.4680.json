{"id": "1206.4680", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Fast Prediction of New Feature Utility", "abstract": "We study the new feature utility prediction problem: statistically testing whether adding a new feature to the data representation can improve predictive accuracy on a supervised learning task. In many applications, identifying new informative features is the primary pathway for improving performance. However, evaluating every potential feature by re-training the predictor with it can be costly. The paper describes an efficient, learner-independent technique for estimating new feature utility without re-training based on the current predictor's outputs. The method is obtained by deriving a connection between loss reduction potential and the new feature's correlation with the loss gradient of the current predictor. This leads to a simple yet powerful hypothesis testing procedure, for which we prove consistency. Our theoretical analysis is accompanied by empirical evaluation on standard benchmarks and a large-scale industrial dataset.", "histories": [["v1", "Mon, 18 Jun 2012 15:38:18 GMT  (875kb)", "http://arxiv.org/abs/1206.4680v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG math.ST stat.TH", "authors": ["hoyt a koepke", "mikhail bilenko"], "accepted": true, "id": "1206.4680"}, "pdf": {"name": "1206.4680.pdf", "metadata": {"source": "CRF", "title": "Fast Prediction of New Feature Utility", "authors": ["Hoyt Koepke", "Mikhail Bilenko"], "emails": ["hoytak@stat.washington.edu", "mbilenko@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "In many cases, this is an approach that involves identifying the causes of disease."}, {"heading": "2. Related Work", "text": "The task addressed in this paper - efficient estimation of the predictive benefit of new traits without retraining - is related, but differs from three known problems: the selection of traits (Guyon & Elisseeff, 2003), the acquisition of active traits (Saar-Tsechansky et al., 2009), and the extraction of traits (Krupka et al., 2008) While these tasks include the estimation of traits, they have different objectives. Many techniques for these problems are based on retraining, while our motivation avoids them. Unlike our setting, where the goal is to efficiently test new traits, the feature selection aims to remove unnecessary existing traits (Guyon & Elisseeff, 2003)."}, {"heading": "3. New Feature Utility & Independence", "text": "We consider the default setting of inductive learning, where training data is a set of samples of random variable pairs (Xi, Yi) from an unknown common distribution function PX, Y, according to data examples described by characteristic values Xi and predictive targets Yi. Learning corresponds to looking for a predictor function f0 from a function class FX that minimizes the expected loss EL (f (X), Y) for a given loss function L that encodes the applicable error measurement: f0 = argmin f FXEL (f (X), Y) (1) The new function forecasting problem can be positioned as determining whether the addition of an additional random variable, X. \"F,\" it can lead to a reduction of the expected loss in data representation if the predictor has been retrained with it. We refer to the function class for predictors on the resulting representation as FX, X. \"F,\" it subsummarizes the function classes FX and FX, \"the function classes are reduced to zero."}, {"heading": "4. A Consistent Feature Utility Test", "text": "In this section, we present a theoretical description of our approach and demonstrate that there is an accurate test of whether a new feature X can improve predictive accuracy. (D) The key part of the evidence detailed in Section 4.2 is the use of the predictor class and the loss function. (D) Then we show a sequence of equivalent formulations of our problem in the context of the actual common distribution of current predictions. (D) The goal we achieve in statement T4 of Theorem 4.2 is a formulation that can be tested precisely in the limited sample situation. (D) This formulation combined with a way of handling the optimization component of the bootstrap test leads to the practical algorithms in Section 4.3. (D)"}, {"heading": "4.1. Equivalent Tests", "text": "In this section, we show that testing (H \u2032 1) is equivalent to testing for the existence of a function that is positively correlated with the loss gradient, which makes it possible to develop a consistent bootstrap algorithm for it. When evaluating a new function, we are interested in finding a function in FX \u2032 that most improves the expected loss. In terms of condition (L3), defineg0 = argmax g \u00b2 FX \u00b2: std (G \u2032) = 1E g (X \u2032). (3) as a function that most closely aligns with a direction of improvement, the following theorem links improvements to this function. Theorem 4.2: Suppose the loss function L and the prediction class F (L1) - (L3) and (F1) (F4). Let's leave f0 = argmin f (FX) EL (F), Y) and g0."}, {"heading": "4.2. A Consistent Hypothesis Test", "text": "The above evidences work for random variables in relation to their actual distributions; the bridge between these and a practical algorithm is effective. As already discussed, we are interested in evaluating the performance of our predictor on the actual distribution, which requires a consistent test to determine whether (H) 1) 1) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1 (H) 1) is equivalent. In the next theorem, we will define an accurate hypotheses test of (H) 1) and prove its consistency. The last step that is required is a good estimator, the f0 (H) f0 (H) 1) generated by the minimal risk predictor f0 (FX).This is because we can show the bias in the bootstrap to be controlled for the standard deviation of n (H)."}, {"heading": "4.3. A Feature Evaluation Algorithm", "text": "The execution of the test requires a transformation that maximizes the inner product between the function and the negative loss gradient. (...) The following theorem allows you to do this by square error minimization for an appropriately weighted expectation. (...) Assumed, F meets assumptions (F1) and (F2), and let this be defined as in (L3). (...) Letf 1 = argmin g. \"(...) Assumed, F.\" (...) Assumed, F. (...) Assumptions (...). (...). (...) Assumptions (...). (...) Assumptions (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.... (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (.). (...). (. (.). (.).). (.). (.).).). (.). (...). (...). (. (.).). (...). (...). (. (.). (.). (.).). (.).). ("}, {"heading": "5. Experimental Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "We evaluate the proposed approach based on three learning tasks: calibrated binary classification, regression, and ranking. Standard loss functions are used for each task: cross-entropy (log loss) for calibrated classification, squared loss for regression, and NDCG for ranking (Ja \ufffd rvelin & Keka \ufffd la \ufffd inen, 2002). Despite the fact that NDCG is discontinuous, it fulfills assumptions (L1) - (L3), and its meaningful functional grade estimates can be approximated by aggregating paired cost differences as described in (Burges, 2010). For classification and regression, we use standard real-task benchmarks from the UCI Collection, Adult, and Housing. For ranking, we use a large-scale industrial search engine file, Web Ranking. While using thousands of individual features, they are grouped into several dozen different sources of information."}, {"heading": "5.2. Methodology", "text": "The accuracy of the functionality forecast is evaluated on the basis of the actual error improvements achieved by retraining with the new feature. Experimental procedure can be summarized as follows: 1. In view of the data set X, which consists of d features, X (1).. X (d) performs an evaluation with all the features included in order to obtain a complete data loss L (f). 2. For each feature X (i), the evaluation is performed on the basis of a lowered data set which excludes the feature, with the corresponding predictors f (\u00ac i) 0, i = 1.. d. Accuracy difference between Li = L (f) \u2212 L (f (\u00ac i) 0) defines the actual benefit of Feature X (i) for each predictor trained. For each trained predictor, the values of the loss gradients per instance are determined."}, {"heading": "5.3. Results and Discussion", "text": "Pro-dataset charts in Figure 1 illustrate predicted vs. actual benefits for each feature, shown as percentages of the range achieved over all features, with actual benefits based on loss reduction due to added features, and predicted benefits based on results achieved through Algorithm 1. In other words, the feature with the highest actual and predicted benefits appears on horizontal or vertical axes at 100%. Features for which the actual benefit is significant at p < 0.05 (via validation folds) are demarked. As the results show, the proposed method identifies the features that produce actual accuracy gains with very high memory value: Any features that are determined as insignificant actually do not result in significant curve gains. While some of the features identified as relevant do not actually produce any appreciable accuracy gains, this is expected to be: While a feature that may have a pre-value for a learning algorithm, the underlying motivation to have a specific one may be algorithm."}, {"heading": "6. Future Work", "text": "Although this work has shown that the problem of predicting feature utility can be solved by positioning it as a hypothesis test in the functional space, it would be interesting to see alternative algorithms for the problem that have been designed using information theory formulations. Another potentially fruitful direction for future work is the development of semi-supervised methods that can use unlabeled data to improve new feature utility estimates, as these are abundant in large areas. Furthermore, the design of modifications of the described approach to feature selection, extraction and active feature value acquisition could provide new efficient methods for these tasks, since the general idea of exploiting the outputs of an existing predictor is clearly relevant to these problems. Finally, another attractive future direction of work is the creation of new feature utility prediction algorithms that override the \"black box\" assumption and utilize properties of a particular learning algorithm or predictor class, possibly leading to better performance."}, {"heading": "7. Conclusions", "text": "The approach is general and supports many common learning tasks and loss functions where the problem is reduced to a quadratic regression. This can only be done for the new features in isolation or in conjunction with existing features. The resulting algorithm allows easy parallelization, making it suitable for large areas. Empirical evaluations showed the correctness of the approach for multiple learning tasks. Recognition: The authors thank Tom Finley for helping with ranking experiments and anonymous reviewers for helpful feedback. This work was done while the first author was visiting Microsoft Research."}], "references": [{"title": "Permutation tests for linear models", "author": ["M.J. Anderson", "J. Robinson"], "venue": "Australian & New Zealand Journal of Statistics,", "citeRegEx": "Anderson and Robinson,? \\Q2001\\E", "shortCiteRegEx": "Anderson and Robinson", "year": 2001}, {"title": "Scaling Up Machine Learning: Parallel and Distributed Approaches", "author": ["R. Bekkerman", "M. Bilenko", "J. Langford"], "venue": null, "citeRegEx": "Bekkerman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bekkerman et al\\.", "year": 2012}, {"title": "From RankNet to LambdaRank to LambdaMART: An overview", "author": ["C.J.C. Burges"], "venue": "Technical Report MSR-TR2010-82, Microsoft Research,", "citeRegEx": "Burges,? \\Q2010\\E", "shortCiteRegEx": "Burges", "year": 2010}, {"title": "Concentration inequalities of the crossvalidation estimator for empirical risk minimiser", "author": ["M. Cornec"], "venue": "Arxiv preprint arXiv:1011.0096,", "citeRegEx": "Cornec,? \\Q2010\\E", "shortCiteRegEx": "Cornec", "year": 2010}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE PAMI,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Model selection by bootstrap penalization for classification", "author": ["M. Fromont"], "venue": "Machine Learning,", "citeRegEx": "Fromont,? \\Q2007\\E", "shortCiteRegEx": "Fromont", "year": 2007}, {"title": "Consistent nonparametric tests of independence", "author": ["A. Gretton", "L. Gy\u00f6rfi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton and Gy\u00f6rfi,? \\Q2010\\E", "shortCiteRegEx": "Gretton and Gy\u00f6rfi", "year": 2010}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "C.H. Teo", "L. Song", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "Correlation-based feature selection for machine learning", "author": ["M.A. Hall"], "venue": "PhD thesis, The University of Waikato,", "citeRegEx": "Hall,? \\Q1999\\E", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Testing conditional independence using maximal nonlinear conditional correlation", "author": ["T.M. Huang"], "venue": "Annals of Statistics,", "citeRegEx": "Huang,? \\Q2010\\E", "shortCiteRegEx": "Huang", "year": 2010}, {"title": "Cumulated Gain-based evaluation of IR techniques", "author": ["J\u00e4rvelin", "Kalervo", "Kek\u00e4l\u00e4inen", "Jaana"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Estimating attributes: analysis and extensions of RELIEF", "author": ["I. Kononenko"], "venue": "In Proceedings of ECML,", "citeRegEx": "Kononenko,? \\Q1994\\E", "shortCiteRegEx": "Kononenko", "year": 1994}, {"title": "Learning to select features using their properties", "author": ["E. Krupka", "A. Navot", "N. Tishby"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Krupka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krupka et al\\.", "year": 2008}, {"title": "Budgeted learning of N\u00e4\u0131ve-Bayes classifiers", "author": ["D.J. Lizotte", "O. Madani", "R. Greiner"], "venue": "In Proceedings of UAI,", "citeRegEx": "Lizotte et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lizotte et al\\.", "year": 2003}, {"title": "Boosting algorithms as gradient descent in function space", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "In NIPS,", "citeRegEx": "Mason et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mason et al\\.", "year": 1999}, {"title": "Permutation tests for studying classifier performance", "author": ["M. Ojala", "G.C. Garriga"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ojala and Garriga,? \\Q2010\\E", "shortCiteRegEx": "Ojala and Garriga", "year": 2010}, {"title": "Active feature-value acquisition", "author": ["M. Saar-Tsechansky", "P. Melville", "F. Provost"], "venue": "Management Science,", "citeRegEx": "Saar.Tsechansky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Saar.Tsechansky et al\\.", "year": 2009}, {"title": "Testing conditional independence via rosenblatt transforms", "author": ["K. Song"], "venue": "Annals of Statistics,", "citeRegEx": "Song,? \\Q2009\\E", "shortCiteRegEx": "Song", "year": 2009}, {"title": "Supervised feature selection via dependence estimation", "author": ["L. Song", "A. Smola", "A. Gretton", "K.M. Borgwardt", "J. Bedo"], "venue": "In Proceedings of ICML,", "citeRegEx": "Song et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Song et al\\.", "year": 2007}, {"title": "G.R.G. Hilbert space embeddings and metrics on probability measures", "author": ["B.K. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "Lanckriet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "A nonparametric hellinger metric test for conditional independence", "author": ["L. Su", "H. White"], "venue": "Econometric Theory,", "citeRegEx": "Su and White,? \\Q2008\\E", "shortCiteRegEx": "Su and White", "year": 2008}, {"title": "Asymptotic statistics", "author": ["A.W. Van der Vaart"], "venue": null, "citeRegEx": "Vaart,? \\Q2000\\E", "shortCiteRegEx": "Vaart", "year": 2000}, {"title": "Weak convergence and empirical processes", "author": ["A.W. Van der Vaart", "J.A. Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner,? \\Q1996\\E", "shortCiteRegEx": "Vaart and Wellner", "year": 1996}], "referenceMentions": [{"referenceID": 18, "context": "The task addressed in this paper \u2013 efficient estimation of predictive utility for new features without re-training \u2013 is related yet distinct from three known problems: feature selection (Guyon & Elisseeff, 2003), active feature acquisition (Saar-Tsechansky et al., 2009), and feature extraction (Krupka et al.", "startOffset": 240, "endOffset": 270}, {"referenceID": 14, "context": ", 2009), and feature extraction (Krupka et al., 2008).", "startOffset": 32, "endOffset": 53}, {"referenceID": 13, "context": "Representatives include wrapper approaches that utilize multiple rounds of re-training with feature subsets, methods that use prediction results for instances with permuted or distorted feature values (Breiman, 2001; Kononenko, 1994), and filter techniques that rely on joint statistics of features and class labels (Song et al.", "startOffset": 201, "endOffset": 233}, {"referenceID": 20, "context": "Representatives include wrapper approaches that utilize multiple rounds of re-training with feature subsets, methods that use prediction results for instances with permuted or distorted feature values (Breiman, 2001; Kononenko, 1994), and filter techniques that rely on joint statistics of features and class labels (Song et al., 2007).", "startOffset": 316, "endOffset": 335}, {"referenceID": 15, "context": "Feature acquisition aims to incrementally select individual feature values for addition to the dataset via estimating their expected utility, and can be viewed as a feature-focused variant of active learning (Lizotte et al., 2003; Saar-Tsechansky et al., 2009).", "startOffset": 208, "endOffset": 260}, {"referenceID": 18, "context": "Feature acquisition aims to incrementally select individual feature values for addition to the dataset via estimating their expected utility, and can be viewed as a feature-focused variant of active learning (Lizotte et al., 2003; Saar-Tsechansky et al., 2009).", "startOffset": 208, "endOffset": 260}, {"referenceID": 14, "context": "Feature extraction methods attempt to construct new joint features that combine individual attributes by evaluating their dependency structure (Della Pietra et al., 1997; Krupka et al., 2008).", "startOffset": 143, "endOffset": 191}, {"referenceID": 6, "context": "On the theoretical side, several approaches have used bootstrapping or permutation tests to assess predictive value of features (Fromont, 2007; Anderson & Robinson, 2001; Ojala & Garriga, 2010).", "startOffset": 128, "endOffset": 193}, {"referenceID": 21, "context": "In particular, there has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperumbudur et al., 2010; Song et al., 2007; Gretton et al., 2008).", "startOffset": 112, "endOffset": 181}, {"referenceID": 20, "context": "In particular, there has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperumbudur et al., 2010; Song et al., 2007; Gretton et al., 2008).", "startOffset": 112, "endOffset": 181}, {"referenceID": 8, "context": "In particular, there has been active work for kernel methods that use the Hilbert-Schmidt Independence Criteria (Sriperumbudur et al., 2010; Song et al., 2007; Gretton et al., 2008).", "startOffset": 112, "endOffset": 181}, {"referenceID": 11, "context": "While our approach also relies on functional analysis techniques, it provides an alternative that does not rely on kernels, instead using standard correlation methods, similarly in spirit to (Huang, 2010).", "startOffset": 191, "endOffset": 204}, {"referenceID": 11, "context": "However, this ideal test is expensive to perform (Huang, 2010; Song, 2009; Su & White, 2008).", "startOffset": 49, "endOffset": 92}, {"referenceID": 19, "context": "However, this ideal test is expensive to perform (Huang, 2010; Song, 2009; Su & White, 2008).", "startOffset": 49, "endOffset": 92}, {"referenceID": 3, "context": "Also, one can use methods known to asymptotically reduce the bias, such as k-fold cross validation (Cornec, 2010).", "startOffset": 99, "endOffset": 113}, {"referenceID": 1, "context": "The method scales well to large-data tasks as the Nbootstrap + 1 evaluations of K can be easily parallelized, X \u2032 is typically lower-dimensional than X, and efficient distributed algorithms for least-squares regression are well-studied (Bekkerman et al., 2012).", "startOffset": 236, "endOffset": 260}, {"referenceID": 16, "context": "It is important to note that training a regressor g to maximize correlation with the loss function gradient is central to AnyBoost and MART views of boosting as gradient descent in function space (Mason et al., 1999; Friedman, 2001).", "startOffset": 196, "endOffset": 232}, {"referenceID": 5, "context": "It is important to note that training a regressor g to maximize correlation with the loss function gradient is central to AnyBoost and MART views of boosting as gradient descent in function space (Mason et al., 1999; Friedman, 2001).", "startOffset": 196, "endOffset": 232}, {"referenceID": 2, "context": "Despite the fact that NDCG is discontinuous, it satisfies assumptions (L1)-(L3), and its pointwise functional gradient estimates can be approximated by aggregating pairwise cost differentials as described in (Burges, 2010).", "startOffset": 208, "endOffset": 222}, {"referenceID": 2, "context": "WebSearch Ranking 26 741,325 NDCG \u03bb-estimates (Burges, 2010)", "startOffset": 46, "endOffset": 60}, {"referenceID": 5, "context": "Gradient boosted trees were used for all tasks (Friedman, 2001), using training loss corresponding to each task.", "startOffset": 47, "endOffset": 63}, {"referenceID": 2, "context": "For ranking, the LambdaMART tree boosting algorithm that optimizes NDCG was used (Burges, 2010).", "startOffset": 81, "endOffset": 95}, {"referenceID": 10, "context": "Figure 2 illustrates the performance of \u03c7 Statistic, Information Gain Ratio, and Correlation-based Feature Selection (CFS) (Guyon & Elisseeff, 2003; Hall, 1999) for the Adult dataset.", "startOffset": 123, "endOffset": 160}], "year": 2012, "abstractText": "We study the new feature utility prediction problem: statistically testing whether adding a feature to the data representation can improve the accuracy of a current predictor. In many applications, identifying new features is the main pathway for improving performance. However, evaluating every potential feature by re-training the predictor can be costly. The paper describes an efficient, learner-independent technique for estimating new feature utility without re-training based on the current predictor\u2019s outputs. The method is obtained by deriving a connection between loss reduction potential and the new feature\u2019s correlation with the loss gradient of the current predictor. This leads to a simple yet powerful hypothesis testing procedure, for which we prove consistency. Our theoretical analysis is accompanied by empirical evaluation on standard benchmarks and a large-scale industrial dataset.", "creator": "TeX"}}}