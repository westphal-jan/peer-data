{"id": "1206.4682", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Copula-based Kernel Dependency Measures", "abstract": "The paper presents a new copula based method for measuring dependence between random variables. Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution. We prove that this approach has several advantageous properties. Similarly to Shannon mutual information, the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables. This is important in many applications, for example in feature selection. The estimator is consistent, robust to outliers, and uses rank statistics only. We derive upper bounds on the convergence rate and propose independence tests too. We illustrate the theoretical contributions through a series of experiments in feature selection and low-dimensional embedding of distributions.", "histories": [["v1", "Mon, 18 Jun 2012 15:40:32 GMT  (266kb)", "http://arxiv.org/abs/1206.4682v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG math.ST stat.ML stat.TH", "authors": ["barnab\u00e1s p\u00f3czos", "zoubin ghahramani", "jeff g schneider"], "accepted": true, "id": "1206.4682"}, "pdf": {"name": "1206.4682.pdf", "metadata": {"source": "CRF", "title": "Copula-based Kernel Dependency Measures", "authors": ["Barnab\u00e1s P\u00f3czos", "Zoubin Ghahramani", "Jeff Schneider"], "emails": ["bapoczos@cs.cmu.edu", "zoubin@eng.cam.ac.uk", "schneide@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Measuring dependence between random variables is a major problem in statistics, information theory and machine learning with a wide range of applications in science and technology. The most well-known measure of dependence is the Shannon metric, which has lately found numerous applications. Although this is the most popular estimate for dependence, it is only one of many existing ones. In particular, it is a special case of the re-metric used by the author (s) / owner (s). Other interesting metrics for dependence include the maximum correlation coefficient (Tsallis, 1988).Published in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Copyright 2012 by the author (s). Other interesting metrics for mutual metrics include the maximum correlation coefficient (Tsallis, 1959), kernel mutual information (Gretton et al, 2003), the generalized variance and kernel correlation analysis Bach (2002)."}, {"heading": "2. Maximum Mean Discrepancy", "text": "In this section we review some important properties of Maximum Mean Discrepancy (MMD) = Q = universal Q functions (2001), which are used to measure the distance between distributions (Borgwardt et al., 2006; Fortet & Mourier, 1953).One appealing property of this size is that it is efficiently distributed by independent and identically distributed (i.i.d.) samples.Definition 1. Let F be a class of functions, P, Q are probability distributions. The MMD between P and Q on function class F is defined as: M [F, P, Q] samples.Definition 1. F (EX) \u2212 P is a class of functions Q [f (Y)."}, {"heading": "3. The Copula of Distributions", "text": "In the following, we will consider some important properties of the copula of multivariate distributions that we will use in our paper (Nelsen, 1998). Copula plays an important role in investigating the dependence between random variables. X1,.., Xd border variables are independent of each other if and only if the copula distribution is the multivariate equal distribution. Conversely, we can measure the dependence of X1,.. \u2212, Xd random variables by measuring how far the copula distribution is removed from the even distribution. The copula contains all the information we need to measure dependence, and it is invariant on any non-linear, strictly increasing transformation of border variables. The copula can be defined by the scular theorem (Sklar, 1959) as follows. Let X = (X1,.,., Xd) d distribution."}, {"heading": "4. Dependence Estimation", "text": "Let's use the (U1,.., Ud) function (0, 1) as a random variable with uniform distribution to the d-dimensional unit Cube, U \u0445 d [0, 1] d. We define the dependence between continuous random variables X1,.., Xd as the MMD distance between the common copula and the d-dimensional uniform distribution: I (X1,., Xd). = M (F, PZ, PU).Definition 4. Let x1, x2, R. A function g: R \u2192 R increases strictly if g (x1) < g (x2) for all x1 < x2.It is easy to see that I (X1,., Xd). 0, and I (X1,., Xd)."}, {"heading": "5. Feature Selection", "text": "The I (X) dependency measurement defined above is invariant in order to strictly increase the transformations of the boundary variables. In this section we discuss the advantages of this property in the feature selection problem. If we want to select d-real evaluation characteristics {X1,.., Xd} and a target value Y, an obvious approach is to select the h-characteristics that together have the highest dependence on Y. Unfortunately, this subset selection problem is very difficult. Therefore, several approximation methods and heuristics have been proposed. For example, according to the so-called maximality criterion (Peng & Ding, 2005), our goal is to select a feature, the S (X1,., Xd) and S (Xd), that maximizes the average dependence between the characteristics and the target."}, {"heading": "6. Numerical Illustrations", "text": "We illustrate the theoretical contributions of this paper with a series of numerical experiments showing the properties of the copula-based kernel dependency measurement. To apply this approach, we need to generate m sample points from the product distributions of the marginals. Let's allow \u03c4i (1: m), (1 \u2264 i \u2264 d) independent random permutations of {1,..., m}. Then we can consider Q [X1: m]. = (X1\u03c41 (1: m), X 2 \u03c42 (1: m),..., Xd\u03c4d (1: m) Tals samples from the \u0394di = 1 PXi distribution. In other words, if X1: m is stored in a d \u00d7 m dimensional sample matrix and we independently mute the elements of each row, then the distributions of the two PXi distributions (they remain independent of the other marginal size), but Mi = Mvty."}, {"heading": "6.1. Feature Selection", "text": "In this experiment we show that I (X) can achieve a better performance in selecting the characteristics than MMD without copula transformation (M (F, PX, \u0435d i = 1 PXi). The task in this experiment was to select the characteristic between X1 and X2 that contains the most information about Y. This characteristic is, of course, X1 (a) and Figure 3 (b), since Y is a deterministic function of it and X2 is independent of Y; it contains no information about Y. 300 sample points from the common branches of (X1, Y) and (X2, Y) are shown in Figure 3 (b) and Figure 3 (b) respectively. Empirical copula transformed points of (Y, X1) and (Y, X2) are shown in Figure X."}, {"heading": "6.2. Feature Standardization", "text": "A commonly used step in the preprocessing of characteristics is the standardization of characteristics, i.e. the linear transformation of the characteristics towards zero-mean variance and unit variance. One might wonder whether this simple transformation can solve the problem of section 6.1. Below we will show an example where we have only two characteristics with zero-mean variance, and the method of selecting characteristics of the MMD, which is non-invariant to the strictly increasing transformations of the characteristics, selects a characteristic that is actually independent of the target value. Let the variables be standardized so that they have zero mean and standard variance 1. We have 4,000 i.d. observations from our observed characteristics X1 and X2. The task again was to select the characteristic that contains the most information about Y. The solution to this problem Y is then the standard variation 1. We have 4,000 i.d. observations between our characteristics X1 and X2."}, {"heading": "6.3. Housing Dataset", "text": "The dataset contains 506 cases of 14 real-world evaluation criteria. The attributes include various characteristics, including the per capita crime rate by city, full property tax rates per $10,000, average number of rooms per apartment, percentage of lower status of the population, median home value in $1,000, etc. Our goal is to predict some of these characteristics and select the most important characteristics for this prediction. As the datasets have very different characteristics, it is highly unimportant how to scale them for the characteristic selection if the applied dependence is non-invariant in order to increase the transformations of the margins. However, this is not a problem for our proposed dependence on the measurement. In this experiment, our goal was to predict the \"median value of homes in $1,000.\""}, {"heading": "7. Discussion and Conclusion", "text": "We introduced a new RKHS-based dependence measure based on the copula of continuous distributions. We have shown that the dependence measure is invariant to strictly increasing transformations of boundary variables, and this property is important for feature selection and low-dimensional embedding of distributions. We have also proposed estimators that are almost certainly consistent and robust, use only ranking statistics and do not suffer from the curse of dimensionality. We derived upper limits for convergence rates and illustrated the theory through a series of numerical experiments."}], "references": [{"title": "Kernel independent component analysis", "author": ["Bach", "Francis R"], "venue": "JMLR, 3:1\u201348,", "citeRegEx": "Bach and R.,? \\Q2002\\E", "shortCiteRegEx": "Bach and R.", "year": 2002}, {"title": "Integrating structured biological data by kernel maximum mean discrepancy", "author": ["K. Borgwardt", "A. Gretton", "M. Rasch", "H. Kriegel", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "Bioinformatics, 22(14):e49\u2013e57,", "citeRegEx": "Borgwardt et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Borgwardt et al\\.", "year": 2006}, {"title": "Weak Dependence: With Examples and Applications, volume 190 of Lecture", "author": ["J. Dedecker", "P. Doukhan", "G. Lang", "J.R. Leon", "S. Louhichi", "C. Prieur"], "venue": "Notes in Statistics. Springer,", "citeRegEx": "Dedecker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dedecker et al\\.", "year": 2007}, {"title": "Combinatorial Methods in Density Estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye and Lugosi,? \\Q2001\\E", "shortCiteRegEx": "Devroye and Lugosi", "year": 2001}, {"title": "Mutual information is critically dependent on prior assumptions: would the correct estimate of mutual information please identify", "author": ["A. Fernandes", "G. Gloor"], "venue": "itself? Bioinformatics,", "citeRegEx": "Fernandes and Gloor,? \\Q2010\\E", "shortCiteRegEx": "Fernandes and Gloor", "year": 2010}, {"title": "Convergence de lar\u00e9paration empirique vers la r\u00e9paration th\u00e9orique", "author": ["R. Fortet", "E. Mourier"], "venue": "Ann. Scient. E\u0301cole Norm,", "citeRegEx": "Fortet and Mourier,? \\Q1953\\E", "shortCiteRegEx": "Fortet and Mourier", "year": 1953}, {"title": "The kernel mutual information", "author": ["A. Gretton", "R. Herbrich", "A. Smola"], "venue": "In Proc. ICASSP,", "citeRegEx": "Gretton et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2003}, {"title": "Measuring statistical dependence with HilbertSchmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Sch\u00f6lkopf"], "venue": "In ALT, pp", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "An Introduction to Copulas (Lecture Notes in Statistics)", "author": ["R. Nelsen"], "venue": null, "citeRegEx": "Nelsen,? \\Q1998\\E", "shortCiteRegEx": "Nelsen", "year": 1998}, {"title": "Estimation of renyi entropy and mutual information based on generalized nearest-neighbor graphs", "author": ["D. P\u00e1l", "B. P\u00f3czos", "Szepesv\u00e1ri", "Cs"], "venue": "In NIPS,", "citeRegEx": "P\u00e1l et al\\.,? \\Q2010\\E", "shortCiteRegEx": "P\u00e1l et al\\.", "year": 2010}, {"title": "Feature selection based on mutual information: Criteria of max-dependency, maxrelevance, and min-redundancy", "author": ["H. Peng", "C. Ding"], "venue": null, "citeRegEx": "Peng and Ding,? \\Q2005\\E", "shortCiteRegEx": "Peng and Ding", "year": 2005}, {"title": "On measures of dependence", "author": ["A. R\u00e9nyi"], "venue": "Acta. Math. Acad. Sci. Hungar,", "citeRegEx": "R\u00e9nyi,? \\Q1959\\E", "shortCiteRegEx": "R\u00e9nyi", "year": 1959}, {"title": "On measure of entropy and information", "author": ["A. R\u00e9nyi"], "venue": "In 4th Berkeley Symposium on Math., Stat., and Prob.,", "citeRegEx": "R\u00e9nyi,? \\Q1961\\E", "shortCiteRegEx": "R\u00e9nyi", "year": 1961}, {"title": "On nonparametric measures of dependence for random variables", "author": ["B. Schweizer", "E. Wolff"], "venue": "The Annals of Statistics,", "citeRegEx": "Schweizer and Wolff,? \\Q1981\\E", "shortCiteRegEx": "Schweizer and Wolff", "year": 1981}, {"title": "Fonctions de rpartition n dimensions et leurs marges", "author": ["A. Sklar"], "venue": "Publ. Inst. Statist. Univ. Paris,", "citeRegEx": "Sklar,? \\Q1959\\E", "shortCiteRegEx": "Sklar", "year": 1959}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": null, "citeRegEx": "Steinwart,? \\Q2001\\E", "shortCiteRegEx": "Steinwart", "year": 2001}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G.J. Sz\u00e9kely", "M.L. Rizzo", "N.K. Bakirov"], "venue": "Annals of Statistics,", "citeRegEx": "Sz\u00e9kely et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sz\u00e9kely et al\\.", "year": 2007}, {"title": "Possible generalization of boltzmann-gibbs statistics", "author": ["C. Tsallis"], "venue": "J. Statist. Phys.,", "citeRegEx": "Tsallis,? \\Q1988\\E", "shortCiteRegEx": "Tsallis", "year": 1988}], "referenceMentions": [{"referenceID": 12, "context": "In particular, it is a special case of the R\u00e9nyi-\u03b1 (R\u00e9nyi, 1961) and Tsallis-\u03b1 mutual information (Tsallis, 1988).", "startOffset": 51, "endOffset": 64}, {"referenceID": 17, "context": "In particular, it is a special case of the R\u00e9nyi-\u03b1 (R\u00e9nyi, 1961) and Tsallis-\u03b1 mutual information (Tsallis, 1988).", "startOffset": 98, "endOffset": 113}, {"referenceID": 11, "context": "Other interesting dependence measures include the maximal correlation coefficient (R\u00e9nyi, 1959), kernel mutual information (Gretton et al.", "startOffset": 82, "endOffset": 95}, {"referenceID": 6, "context": "Other interesting dependence measures include the maximal correlation coefficient (R\u00e9nyi, 1959), kernel mutual information (Gretton et al., 2003), the generalized variance and kernel canonical correlation analysis (Bach, 2002), the Hilbert-Schmidt independence criterion (Gretton et al.", "startOffset": 123, "endOffset": 145}, {"referenceID": 7, "context": ", 2003), the generalized variance and kernel canonical correlation analysis (Bach, 2002), the Hilbert-Schmidt independence criterion (Gretton et al., 2005), the Schweizer-Wolff measure (Schweizer & Wolff, 1981), and the distance based correlation (Sz\u00e9kely et al.", "startOffset": 133, "endOffset": 155}, {"referenceID": 16, "context": ", 2005), the Schweizer-Wolff measure (Schweizer & Wolff, 1981), and the distance based correlation (Sz\u00e9kely et al., 2007).", "startOffset": 99, "endOffset": 121}, {"referenceID": 9, "context": "For example, the bound on the convergence rate of the R\u00e9nyi and Tsallis information estimator (P\u00e1l et al., 2010) suffers from the curse of dimensionality.", "startOffset": 94, "endOffset": 112}, {"referenceID": 9, "context": "For example, the bound on the convergence rate of the R\u00e9nyi and Tsallis information estimator (P\u00e1l et al., 2010) suffers from the curse of dimensionality. The available reproducing kernel based dependence measures are not invariant to strictly increasing transformation of the Xi marginal random variables. The estimator of Sz\u00e9kely et al. (2007) is not robust; one single large enough outlier can arbitrarily ruin the estimator.", "startOffset": 95, "endOffset": 346}, {"referenceID": 1, "context": "In this section we review some important properties of the Maximum Mean Discrepancy (MMD), which is a quantity used to measure the distance between distributions (Borgwardt et al., 2006; Fortet & Mourier, 1953).", "startOffset": 162, "endOffset": 210}, {"referenceID": 1, "context": "This is stated formally in the following lemma (Borgwardt et al., 2006).", "startOffset": 47, "endOffset": 71}, {"referenceID": 1, "context": "An unbiased estimator for M[F , P,Q] (when m = n) has also been derived in Borgwardt et al. (2006):", "startOffset": 75, "endOffset": 99}, {"referenceID": 8, "context": "Below we review a few important properties of the copula of multivariate distributions that we will use in our work (Nelsen, 1998).", "startOffset": 116, "endOffset": 130}, {"referenceID": 14, "context": "The copula can be defined by the Sklar\u2019s theorem (Sklar, 1959) as follows.", "startOffset": 49, "endOffset": 62}, {"referenceID": 2, "context": ", F\u0302(Xm)) \u2208 R is called the empirical copula (Dedecker et al., 2007).", "startOffset": 45, "endOffset": 68}], "year": 2012, "abstractText": "The paper presents a new copula based method for measuring dependence between random variables. Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution. We prove that this approach has several advantageous properties. Similarly to Shannon mutual information, the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables. This is important in many applications, for example in feature selection. The estimator is consistent, robust to outliers, and uses rank statistics only. We derive upper bounds on the convergence rate and propose independence tests too. We illustrate the theoretical contributions through a series of experiments in feature selection and low-dimensional embedding of distributions.", "creator": " TeX output 2012.05.17:1636"}}}