{"id": "1206.4683", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Marginalized Denoising Autoencoders for Domain Adaptation", "abstract": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.", "histories": [["v1", "Mon, 18 Jun 2012 15:40:50 GMT  (514kb)", "http://arxiv.org/abs/1206.4683v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["minmin chen", "zhixiang eddie xu", "kilian q weinberger", "fei sha"], "accepted": true, "id": "1206.4683"}, "pdf": {"name": "1206.4683.pdf", "metadata": {"source": "META", "title": "Marginalized Denoising Autoencoders for Domain Adaptation", "authors": ["Minmin Chen", "Zhixiang (Eddie) Xu", "Kilian Q. Weinberger", "Fei Sha"], "emails": ["MC15@CSE.WUSTL.EDU", "XUZX@CSE.WUSTL.EDU", "KILIAN@WUSTL.EDU", "FEISHA@USC.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it is a purely reactionary project, in which it is a reactionary, reactionary and reactionary solution."}, {"heading": "2. Notation and Background", "text": "It is. (...) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) We assume that our data comes from two areas, namely from the United States, from which we come. (.) We assume that they are both human beings. (.) We assume that they are both human beings. (.) We assume that they are both human beings. (.) We assume that they are both human beings who are able to identify themselves. (.) \"(.)\" (.) \"(.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (. (.) It is. (.) It is. (.) It is. (. (.) It is. (.) It is. (. (.) It is. (. (.) It is. (.) It is. (. (.) It is. (.) It is. (. (.) It is."}, {"heading": "3. SDA with Marginalized Corruption", "text": "In this section, we present a modified version of SDA that maintains its strong learning capabilities and addresses the above concerns through multiple order-of-magnitude acceleration, fewer meta-parameters, faster model selection, and layer convexity."}, {"heading": "3.1. Single-layer Denoiser", "text": "We take the inputs q = = q q = Q,.., xn from D = DS-DT and damage them by randomly removing characteristics - each characteristic is set to 0 with the probability p \u2265 0. Let's call the corrupt version of xi x-i. (Unlike the two-stage encoder and decoder in SDA, we reconstruct the corrupt inputs with a single figure W: Rd \u2192 Rd, which minimizes the square reconstruction loss12n n n n. (Unlike the two-stage version of xi \u2212 d). (1) To simplify the notation, we assume that a constant property is added to the input, xi = [xi; 1], and a corresponding bias within the figure W = [W, b]. The constant property is never corrupted. The solution to (1) depends on which characteristics of each input are randomly corrupted. To reduce the variance, we can be multiplied within the figure."}, {"heading": "3.2. Marginalized Denoising Autoencoder", "text": "Ideally, we would like the matrices P and Q to converge with their expected values as defined in (3), as m becomes very large. (5) In the rest of this section, we calculate the expectations of these two matrices, where m \u2192 \u221e, we derive the expectations of Q and P and the corresponding figure W asW = E [P] E [Q] \u2212 1. (5) For the rest of this section, we calculate the expectations of these two matrices. For the moment, we focus on E [Q] = n \u00b2 i = E [x \u00b2 ix > i]. (6) An extra-diagonal input into the matrix \u00b2 ix \u00b2 > i is unspoiled if the two characteristics \u03b1 and \u03b2 both \"survived\" corruption, which is explicitly probable."}, {"heading": "3.3. Nonlinear feature generation and stacking", "text": "In SDAs, nonlinearity is injected by the nonlinear encoder function h (\u00b7), which is learned together with the reconstruction weights W. Such an approach makes the training procedure highly nonconvex and requires iterative procedures to learn the model parameters. To get the solution in closed form from the linear mapping in Section 3.2, we insert nonlinearity into our learned representation by weights W. A nonlinear squeeze function is applied to the output of each mDA. Several choices are possible, including sigmoid, hyperbolic tangent, TANH (), or the rectifier function mSQM (n)."}, {"heading": "3.4. mSDA for Domain Adaptation", "text": "One observation reported in Glorot et al., 2011 is that when multiple domains are available, sharing SDA unattended pre-training across all domains is beneficial compared to pre-training at source and destination alone. We observe a similar trend in our approach. The results reported in Section 5 are based on characteristics learned from data from all available domains. Once an mSDA is trained, the results of all levels after squeezing, Tanh (Wtht \u2212 1), combined with the original characteristics h0, are linked together and form the new representation. All inputs are transformed into the new feature space. A linear Support Vector Machine (SVM) (Chang & Lin, 2011) is then trained on the transformed source inputs and tested on the target domain. There are two meta parameters in mDA: Corruption is almost completely explained by the ability to form the SDA source data and the SVM data based on the SDA."}, {"heading": "4. Extension for High Dimensional Data", "text": "In practice, a work-around means truncating the input data to the most common characteristics (Glorot et al., 2011). Unfortunately, this prevents SDAs from using important information found in less common characteristics. (As we show in Section 5, these less common characteristics lead to significantly better results.) High dimensionality also poses a challenge for mSDA, as the system of linear equations in (5) complexity O (d3) becomes too costly. (In this section, we describe how this calculation can be combined with a simple subdivision into sub-problems of O (r 3). We combine the concept of \"pivot characteristics\" by Blitzer et al. (2006) and the use of the most common characteristics of Glorot et al. (2011)."}, {"heading": "5. Results", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "5.1. Comparison with Related Work", "text": "In the first series of experiments, we use the setting of (Glorot et al., 2011) on the small Amazon benchmark set. The input data are reduced to just the 5, 000 most common terms of uniques and bigrams as features. Figure 1 presents a detailed comparison of transfer losses in the twelve areas of customization tasks with the various methods mentioned. A linear SVM trained on the characteristics of SDA and mSDA clearly outperforms all other methods. For multiple tasks, the transfer loss goes into the negative - in other words, an SVM on the transformed source data has higher accuracy than a trained on the original target data. This is a strong indication that the learned new representation bridges between domains. It is worth noting that in ten of the twelve tasks mSDA achieves a low transfer loss from the SDA group, SDA achieves a lower transfer loss than SDA.Timing."}, {"heading": "5.2. Further Analysis", "text": "In addition to the comparison with previous work, we also analyze various other aspects of mSDA. Low frequency characteristics of DA. Before work, we often limit input data to the most common characteristics (Glorot et al., 2011). We use the modification of Section 4 to scale mSDA (5 layers) to high dimensions and include less common unigrams and bigrams in input (small Amazon quantity). In the case of SDA, we make the first layer a dimensionality that transforms d dimensions to 5000. The left graph in Figure 3 shows the performance of mSDA and SDA as input dimensionality increases (words are selected in decreasing order of their frequency).The transfer ratio is calculated relative to the baseline with d = 5000 characteristics. Both algorithms clearly benefit from having more characteristics up to 30, 000. mSDA corresponds to the transfer ratio of SDA consistent and increasing dimensionality."}, {"heading": "5.3. General Trends", "text": "In summary, some general trends can be observed in all experiments: 1. With one layer, mSDA is up to three orders of magnitude faster, but slightly less meaningful than the original SDA. This is due to the fact that mSDA has no hidden layer. 2. There is a clear trend that additional \"deep\" layers significantly improve the results (here up to five layers). With additional layers, mSDA structures achieve (and exceed) the accuracy of a single-layer SDA and still achieve several hundred-fold acceleration. 3. The mSDA functions help with a variety of classification tasks, domain classification and mood analysis, and can be trained very efficiently on high-dimensional data."}, {"heading": "6. Discussion and Conclusion", "text": "Although mSDA primarily marginalizes corruption in SDA education, the two algorithms differ in several fundamental ways: First, mSDA layers have no hidden nodes - this allows for a closed-loop solution with significant acceleration, but may have limitations that need to be studied; second, mSDA has only two free meta-parameters that control both the amount of noise and the number of layers to be stacked, which greatly simplifies model selection; and finally, taking advantage of the analytical traceability of linear regression, the parameters of an mDA are trained to optimally decode all sorts of corrupt training inputs - arguably \"an infinite number.\" This is practically impracticable for SDA. We hope that our work on mSDA will inspire future research on efficient SDA training beyond domain adaptation, and will have an impact on a variety of research issues."}, {"heading": "Acknowledgements", "text": "KQW, MC, ZX were supported by NSF IIS-1149882 and NIH U01 1U01NS073457-01. FS was supported by NSF IIS-0957742, DARPA CSSG N10AP20019 and D11AP00278."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural networks,", "citeRegEx": "Baldi and Hornik,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": null, "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "A Theory of Learning from Different Domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "Wortman", "Jenn"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2009}, {"title": "Theano: a CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Pattern Recognition and Machine Learning", "author": ["Bishop", "Christopher"], "venue": null, "citeRegEx": "Bishop and Christopher.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and Christopher.", "year": 2006}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 Conference on EMNLP, pp", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on IST,", "citeRegEx": "Chang and Lin,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin", "year": 2011}, {"title": "Co-Training for Domain Adaptation", "author": ["M. Chen", "K.Q. Weinberger", "J.C. Blitzer"], "venue": "In NIPS,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Automatic Feature Decomposition for Single View Co-training", "author": ["M. Chen", "K.Q. Weinberger", "Y. Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["III H. Daume"], "venue": "In ACL,", "citeRegEx": "Daume,? \\Q2007\\E", "shortCiteRegEx": "Daume", "year": 2007}, {"title": "Large-Scale Learning of Embeddings with Reconstruction Sampling", "author": ["Y. Dauphin", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Dauphin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2011}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Correcting Sample Selection Bias by Unlabeled Data", "author": ["J. Huang", "A.J. Smola", "A. Gretton", "K.M. Borgwardt", "B. Scholkopf"], "venue": "In NIPS", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "IEEE Conference on, pp", "author": ["K. Kavukcuoglu", "M.A. Ranzato", "R. Fergus", "Le-Cun", "Y. Learning invariant features through topographic filter maps. In CVPR"], "venue": "1605\u20131612. IEEE, 2009.", "citeRegEx": "Kavukcuoglu et al\\.,? 2009", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Unsupervised Feature Learning for Audio Classification using Convolutional Deep Belief Networks", "author": ["Lee", "Honglak", "Largman", "Yan", "Pham", "Peter", "Ng", "Andrew Y"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Evigan: a hidden variable model for integrating gene evidence for eukaryotic gene", "author": ["Liu", "Qian", "Mackey", "Aaron", "Roos", "David", "Pereira", "Fernando"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "Domain Adaptation with Multiple Sources", "author": ["T. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In NIPS", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Reranking and selftraining for parser adaptation", "author": ["D. McClosky", "E. Charniak", "M. Johnson"], "venue": "In ACL, pp", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hintont", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information processing & management,", "citeRegEx": "Salton and Buckley,? \\Q1988\\E", "shortCiteRegEx": "Salton and Buckley", "year": 1988}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In ICML,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Feature hashing for large scale multitask learning", "author": ["K.Q. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "In ICML", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Topic-bridged PLSA for cross-domain text classication", "author": ["G. Xue", "W. Dai", "Q. Yang", "Y. Yu"], "venue": "In SIGIR,", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 12, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 24, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 25, "context": "Domain adaptation (Ben-David et al., 2009; Huang et al., 2007; Weinberger et al., 2009; Xue et al., 2008) aims to generalize a classifier that is trained on a source domain, for which typically plenty of training data is available, to a target domain, for which data is scarce.", "startOffset": 18, "endOffset": 105}, {"referenceID": 15, "context": "Examples are computational biology (Liu et al., 2008), natural language processing (Daume III, 2007; McClosky et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 17, "context": ", 2008), natural language processing (Daume III, 2007; McClosky et al., 2006) and computer vision (Saenko et al.", "startOffset": 37, "endOffset": 77}, {"referenceID": 21, "context": ", 2006) and computer vision (Saenko et al., 2010).", "startOffset": 28, "endOffset": 49}, {"referenceID": 12, "context": "Recent work has investigated several techniques for alleviating the difference: instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 101, "endOffset": 143}, {"referenceID": 16, "context": "Recent work has investigated several techniques for alleviating the difference: instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 101, "endOffset": 143}, {"referenceID": 5, "context": ", 2011b) and learning joint target and source feature representations (Blitzer et al., 2006; Glorot et al., 2011; Xue et al., 2008).", "startOffset": 70, "endOffset": 131}, {"referenceID": 11, "context": ", 2011b) and learning joint target and source feature representations (Blitzer et al., 2006; Glorot et al., 2011; Xue et al., 2008).", "startOffset": 70, "endOffset": 131}, {"referenceID": 25, "context": ", 2011b) and learning joint target and source feature representations (Blitzer et al., 2006; Glorot et al., 2011; Xue et al., 2008).", "startOffset": 70, "endOffset": 131}, {"referenceID": 23, "context": "The authors propose to learn robust feature representations with stacked denoising autoencoders (SDA) (Vincent et al., 2008).", "startOffset": 102, "endOffset": 124}, {"referenceID": 14, "context": "The outputs of their intermediate layers are then used as input features for SVMs (Lee et al., 2009).", "startOffset": 82, "endOffset": 100}, {"referenceID": 5, "context": "(2011) demonstrate that using SDA-learned features in conjunction with linear SVM classifiers yields record performance on the benchmark tasks of sentiment analysis across different product domains (Blitzer et al., 2006).", "startOffset": 198, "endOffset": 220}, {"referenceID": 10, "context": "Recently, Glorot et al. (2011) proposed a new approach that falls into the third category.", "startOffset": 10, "endOffset": 31}, {"referenceID": 10, "context": "Recently, Glorot et al. (2011) proposed a new approach that falls into the third category. The authors propose to learn robust feature representations with stacked denoising autoencoders (SDA) (Vincent et al., 2008). Denoising autoencoders are one-layer neural networks that are optimized to reconstruct input data from partial and random corruption. These denoisers can be stacked into deep learning architectures. The outputs of their intermediate layers are then used as input features for SVMs (Lee et al., 2009). Glorot et al. (2011) demonstrate that using SDA-learned features in conjunction with linear SVM classifiers yields record performance on the benchmark tasks of sentiment analysis across different product domains (Blitzer et al.", "startOffset": 10, "endOffset": 539}, {"referenceID": 5, "context": "They are significantly slower to train than competing algorithms (Blitzer et al., 2006; Chen et al., 2011a; Xue et al., 2008), primarily because of their reliance on iterative and numerical optimization to learn model parameters.", "startOffset": 65, "endOffset": 125}, {"referenceID": 25, "context": "They are significantly slower to train than competing algorithms (Blitzer et al., 2006; Chen et al., 2011a; Xue et al., 2008), primarily because of their reliance on iterative and numerical optimization to learn model parameters.", "startOffset": 65, "endOffset": 125}, {"referenceID": 3, "context": "Consequently, even a highly optimized implementation (Bergstra et al., 2010) may require hours (even days) of training time.", "startOffset": 53, "endOffset": 76}, {"referenceID": 11, "context": "We follow the setup of Glorot et al. (2011) and focus on the problem of domain adaptation throughout this paper.", "startOffset": 23, "endOffset": 44}, {"referenceID": 20, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 13, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 14, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 23, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 19, "context": "Various forms of autoencoders have been developed in the deep learning literature (Rumelhart et al., 1986; Baldi & Hornik, 1989; Kavukcuoglu et al., 2009; Lee et al., 2009; Vincent et al., 2008; Rifai et al., 2011).", "startOffset": 82, "endOffset": 214}, {"referenceID": 23, "context": "In this work, as in Vincent et al. (2008), we use the latter and set a fraction of the features of each input to zero.", "startOffset": 20, "endOffset": 42}, {"referenceID": 23, "context": "The stacked denoising autoencoder (SDA) of Vincent et al. (2008) stacks several DAs together to create higher-level representations, by feeding the hidden representation of the t DA as input into the (t + 1) DA.", "startOffset": 43, "endOffset": 65}, {"referenceID": 14, "context": "For example, Lee et al. (2009) demonstrate that the hidden representations computed by either all or partial layers of a convolutional neural network (CNN) make excellent features for classification with SVMs.", "startOffset": 13, "endOffset": 31}, {"referenceID": 11, "context": "It is shown that SDAs are able to disentangle hidden factors which explain the variations in the input data, and automatically group features in accordance with their relatedness to these factors (Glorot et al., 2011).", "startOffset": 196, "endOffset": 217}, {"referenceID": 3, "context": "Training with (stochastic) gradient descent is slow and hard to parallelize (although a dense-matrix GPU implementation exists (Bergstra et al., 2010) and an implementation based on reconstruction sampling exists (Dauphin Y.", "startOffset": 127, "endOffset": 150}, {"referenceID": 11, "context": "One observation reported in (Glorot et al., 2011) is that if multiple domains are available, sharing the unsupervised pre-training of SDA across all domains is beneficial compared to pre-training on the source and target only.", "startOffset": 28, "endOffset": 49}, {"referenceID": 11, "context": "In practice, a work-around is to truncate the input data to the r dmost common features (Glorot et al., 2011).", "startOffset": 88, "endOffset": 109}, {"referenceID": 5, "context": "We combine the concept of \u201cpivot features\u201d from Blitzer et al. (2006) and the use of most-frequent features from Glorot et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 5, "context": "We combine the concept of \u201cpivot features\u201d from Blitzer et al. (2006) and the use of most-frequent features from Glorot et al. (2011). Instead of learning a single mapping W \u2208 Rd\u00d7(d+1) to reconstruct all corrupted features, we learn multiple mappings but only reconstruct the r d", "startOffset": 48, "endOffset": 134}, {"referenceID": 5, "context": "We evaluate mSDA on the Amazon reviews benchmark data sets (Blitzer et al., 2006) together with several other algorithms for representation learning and domain adaptation.", "startOffset": 59, "endOffset": 81}, {"referenceID": 11, "context": "For simplicity (and comparability), we follow the convention of (Chen et al., 2011b; Glorot et al., 2011) and only consider the binary classification problem whether a review is positive (higher than 3 stars) or negative (3 stars or lower).", "startOffset": 64, "endOffset": 105}, {"referenceID": 5, "context": "To counter the effect of class- and size-imbalance, a more controlled smaller dataset was created by Blitzer et al. (2006), which contains reviews of four types of products: books, DVDs, electronics, and kitchen appliances.", "startOffset": 101, "endOffset": 123}, {"referenceID": 5, "context": "Besides these two baselines, we evaluate the efficacy of a linear SVM trained on features learned by mSDA and two alternative feature learning algorithms, Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and", "startOffset": 196, "endOffset": 218}, {"referenceID": 11, "context": "1-layer1 SDA (Glorot et al., 2011).", "startOffset": 13, "endOffset": 34}, {"referenceID": 11, "context": "Following Glorot et al. (2011), we evaluate our results with the transfer error e(S, T ) and the in-domain error e(T, T ).", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "Following Glorot et al. (2011), we evaluate our results with the transfer error e(S, T ) and the in-domain error e(T, T ). The transfer error e(S, T ) denotes the classification error of a classifier trained on the labeled source data and tested on the unlabeled target data. The in-domain error e(T, T ) denotes the classification error of a classifier that is trained on the labeled target data and tested on the unlabeled target data. Similar to Glorot et al. (2011) we measure the performance of a domain adaptation algorithm in terms of the transfer loss, defined as e(S, T )\u2212eb(T, T ), where eb(T, T ) defines the in-domain error of the baseline.", "startOffset": 10, "endOffset": 470}, {"referenceID": 11, "context": "In the first set of experiments, we use the setting from (Glorot et al., 2011) on the small Amazon benchmark set.", "startOffset": 57, "endOffset": 78}, {"referenceID": 11, "context": "Prior work often limits the input data to the most frequent features (Glorot et al., 2011).", "startOffset": 69, "endOffset": 90}, {"referenceID": 1, "context": "Ben-David et al. (2007) suggest the Proxy-A-distance (PAD) as a measure of how different two domains are from each other.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Ben-David et al. (2007) suggest the Proxy-A-distance (PAD) as a measure of how different two domains are from each other. The metric is defined as 2(1 \u2212 2 ), where is the generalization error of a classifier (a linear SVM in our case) trained on the binary classification problem to distinguish inputs between the two domains. The right plot in Figure 3 shows the PAD before and after mSDA is applied. Surprisingly, the distance increases in the new representation \u2014 i.e. distinguishing between two domains becomes easier with the mSDA features. We explain this effect through the fact that mSDA is unsupervised and learns a generally better representation for the input data. This helps both tasks, distinguishing between domains and sentiment analysis (e.g. in the electronicdomain mSDA might interpolate the feature \u201cdvd player\u201d from \u201cblue ray\u201d, both are not particularly relevant for sentiment analysis but might help distinguish the review from the book domain.). Glorot et al. (2011) observe a similar effect with the representations learned with SDA.", "startOffset": 0, "endOffset": 990}], "year": 2012, "abstractText": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters \u2014 in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.", "creator": "LaTeX with hyperref package"}}}