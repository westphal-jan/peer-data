{"id": "1206.4686", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Discriminative Probabilistic Prototype Learning", "abstract": "In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where each original input datapoint is described by a set of vectors and their associated outputs may be given by soft labels indicating, for example, class probabilities. We represent an input datapoint as a mixture of probabilities over the corresponding set of feature vectors where each probability indicates how likely each vector is to belong to an unknown prototype pattern. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. We show that our model can be seen as a probabilistic generalization of learning vector quantization (LVQ). We apply our method to the problems of shape classification, hyperspectral imaging classification and people's work class categorization, showing the superior performance of our method compared to the standard prototype-based classification approach and other competitive benchmark methods.", "histories": [["v1", "Mon, 18 Jun 2012 15:42:34 GMT  (350kb)", "http://arxiv.org/abs/1206.4686v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["edwin v bonilla", "antonio robles-kelly"], "accepted": true, "id": "1206.4686"}, "pdf": {"name": "1206.4686.pdf", "metadata": {"source": "META", "title": "Discriminative Probabilistic Prototype Learning", "authors": ["Edwin V. Bonilla", "Antonio Robles-Kelly"], "emails": ["edwin.bonilla@nicta.com.au", "Antonio.Robles-Kelly@nicta.com.au"], "sections": [{"heading": "1. Introduction", "text": "A fundamental problem in the field of machine learning is to find useful characterizations of the input so that we can achieve a better generalization, as described in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner (s).pabilities with our learning algorithms. We refer to this problem as that of learning representations. This has been a long-standing goal in machine learning and has been dealt with over the years from different perspectives. In fact, one of the simplest and oldest attempts to address this problem was given by Rosenblatt (1962) with the Perceptron algorithm for classification problems. In such an approach it was proposed that we have non-linear mappings of the inputs so that the representation obtained allows us to distinguish between classes with a simple linear function."}, {"heading": "2. Problem Setting", "text": "In this paper, we are interested in multi-level classification problems, for which an input point is characterised by a series of characteristic vectors (1),.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3. Parameter Learning", "text": "In this section we are interested in the parameters of our discriminatory probabilistic prototype framework. These parameters are: the velocity parameter \u03b2, the vocabulary or the centers {\u00b5k} Kk = 1 and the parameters of the discriminatory classifier \u0442. This could be done by a number of optimization methods, including simulated annealing and the Markov chain in Monte Carlo. Here we propose a direct gradient-based optimization of the data log probability."}, {"heading": "3.1. Direct Likelihood Maximization with Gradient-Based Methods", "text": "Based on iid data, the log probability of the model parameters for which the data are given can be expressed as follows: (1) (1) (2) (2) (2) (2) (2) (3) (3) (3) (2) (3) (3) (3) (3) (3) (3) (3) (3) (4) (4) (4) (4) (4) (4) (4) (5) = (5) (5) (5) (N) (n) (6)) (5) = (5) (n) (n)) (5) = (5) (n) (n) (4)) (5) = (n) (6) (5)) (P) (yn) refers to the soft designations associated with input n, e.g. the empirical class probabilities."}, {"heading": "3.2. Discriminative Parametric Model: Softmax Classifier", "text": "For a parametric model such as the Softmax classifier: P (y = i | z, \u044b) = exp (\u03b8 i) T z) \u2211 Cj = 1 exp (\u03b8 j) T z) def = \u03c3i (z; \u044b) (16) the local log probability concepts can be written as follows: Ln = C \u2211 j = 1P (yn = j) log \u03c3j (zn; \u044b). (17) However, the corresponding derivatives would represent the prototype representations as follows:"}, {"heading": "3.3. Relation to LVQ", "text": "Learning Vector Quantization (LVQ, Kohonen, 1990) is a prototype-based learning algorithm that uses the information of the class name to adapt the prototypes (i.e. cluster centers).The idea is that the prototypes move to the training examples in their respective class and away from the training examples with different names. As with the k-mean algorithm, the mapping of the data points to the prototypes would correspond to the rule in Equation (1).If we were to update the prototypes in our model by gradient ascendence, this update would be: \"(new)\" = \u00b5 (old) \"+ \u03b7 \u00b5'L,\" (20) where \u03b7 is the learning rate. By expanding the equation (11), replacing equations (12) and (18), and adopting hard labels, i.e. P labels (yn) is 1 for just one label and zero for everyone else, we should: \"probability\" (new)."}, {"heading": "4. Experiments and Results", "text": "In this section, we present results on synthetic data, shape classification, hyperspectral image classification and working class categorization. In all our experiments, we apply our prototypical learning approach, in which we iterate the learning of prototype parameters {\u00b5 '} K' = 1, \u03b2 and model parameters \u043a to model probability by means of coordinate ascent. We have initialized cluster centers that use k-means and vary the size of the vocabulary, i.e. K, according to the experimental vehicle. To illustrate the behavior of our algorithm and show its performance in terms of competitive benchmarks, the first three problems investigated (synthetic data, shape classification, hyper-spectral image classification) consider the frequent case of hard labels, and our last experiment on working class categorization examines the use of class probabilities as soft labels."}, {"heading": "4.1. Illustration on Synthetic Data", "text": "These experiments are based on a set of 20 data points, each consisting of a set of Mn vectors in a 2-D space. Here, we compare the basic model determined by k-mean clusters (with K = 2) with our discriminatory model. Figure 1 shows the original data points in two-dimensional space together with the cluster centers (above). We see that the cluster centers (Panel b) learned through our discriminatory approach are quite different, although the clusters obtained with k-means (Panel a) are very close to those used to generate the data. In the lower panel, we show the prototype representation provided by both methods, and we find that the representation learned through our method is much more differentiated, since it takes class names into account."}, {"heading": "4.2. MPEG DataSet", "text": "Our first real dataset is provided by the MPEG-7 CE-1 Part B database (Latecki et al., 2000), which we will call mpeg-7 dataset. It contains 1400 binary shapes organized into 70 classes, each of which consists of 20 images. We sampled 1 in every 10 pixels on the contours of the shape and we built a fully interconnected graph whose edge weights are given by the Euclidean distances between each pair of pixel locations. These weights are then normalized in the interval [0, 1]. Characteristic vectors are given by the frequency of histograms of these distances for each node. In our experiments, we used 10 containers for the frequency of the histogram composition and set K = 200. For shape categorization purposes, we compare our method with three alternatives."}, {"heading": "4.3. SPECTRAL Dataset", "text": "This application looks at hyper-spectral images from the real world scenes that encompass different types of materials. We have commented on these images at the pixel level taking into account 10 different classes (C = 10): tree trunk, light poles, shadows on grass, grass, road, white line on the road, leaves, sky and white regions in the sky. We looked at 24 different images from which we have extracted 1, 746, 708 data points with their corresponding labels. To characterize an input data point with a series of vectors (Sn), we have considered neighborhood information according to 7 x 7 windows. Therefore, for each data point we have 49 feature vectors to subsume the data to include 1000 training instances and 16643 test instances across 10 different partitions. Note that our method effectively learns the prototypes that can be used to represent the spectra for the objects in the scene as a linear combination of the information signatures of their constituents."}, {"heading": "4.4. Test Likelihoods on MPEG-7 and SPECTRAL", "text": "Now we turn our attention to evaluating the methods from a probable standpoint by reporting the probability of the models on the test data. In Figure 2 (a) we show the log probability of the test data for our probable prototype approach and the baseline. The bar corresponds to the mean across the five studies and the segments contain two standard errors. Note that the log probability of our approach is significantly higher than that of the standard approach. This is because the above performance measurements take into account the correctness of the classifier labels without error probability. Therefore, our method not only provides better performance than the alternatives, but also better probability estimates. In Figure 2 (b) we show similar results for the spectral dataset. As with the mpeg-7 dataset, the log probability of the test data for the spectral data supplied by our approach is greater than the variability and lower."}, {"heading": "4.5. ADULT Dataset", "text": "Indeed, in recent years, we have been able to show that we are able to solve the world's problems by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by putting them in check, by putting them in check, and by putting them in check."}, {"heading": "5. Related Work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "6. Conclusions", "text": "We have presented a probabilistic model for discriminatory latent representation learning that is a relaxed version of the popular approach to prototype-based classification, and from an application standpoint, our method can be considered a discriminatory technique that can be used for geoscience separation and remote sensing (Bergman, 2006), and can also be applied to other problems, such as population modeling, where labels and proportions of these can be associated with entities. Generally, our method requires a model for a probabilistic discriminatory classifier, and to demonstrate the utility of our approach we have used a Softmax classifier. However, in principle, our approach can be used with any discriminatory and probabilistic classifier, including non-parametric methods. In the future, we aim to explore the combination of non-parametric methods with our prototypical learning approach, and to investigate better parameter learning optimization algorithms, e.g. based on mid-range approximations."}, {"heading": "Acknowledgments", "text": "We thank Sarah Namin and Lars Petersson for providing the spectral dataset. NICTA is funded by the Australian Government, represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence Programme."}], "references": [{"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE TPAMI,", "citeRegEx": "Belongie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Belongie et al\\.", "year": 2002}, {"title": "Scaling learning algorithms towards AI. In Large-Scale Kernel Machines", "author": ["Bengio", "Yoshua", "LeCun", "Yann"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Learning a compact code for novelcategory recognition", "author": ["Andrew. Picodes"], "venue": "In NIPS", "citeRegEx": "Picodes,? \\Q2011\\E", "shortCiteRegEx": "Picodes", "year": 2011}, {"title": "Some unmixing problems and algorithms in spectroscopy and hyperspectral imaging", "author": ["M. Bergman"], "venue": "In Applied Imagery and Pattern Recognition Workshop,", "citeRegEx": "Bergman,? \\Q2006\\E", "shortCiteRegEx": "Bergman", "year": 2006}, {"title": "Error-correcting tournaments", "author": ["Beygelzimer", "Alina", "Langford", "John", "Ravikumar", "Pradeep"], "venue": "In International conference on Algorithmic Learning Theory,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "GTM: The generative topographic mapping", "author": ["Bishop", "Christopher M", "Svens\u00e9n", "Markus", "Williams", "Christopher K. I"], "venue": "Neural Computation,", "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "Learning mid-level features for recognition", "author": ["Boureau", "Y-Lan", "Bach", "Francis", "LeCun", "Yann", "Ponce", "Jean"], "venue": "In CVPR,", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "In CVPR,", "citeRegEx": "Fei.Fei and Perona,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei and Perona", "year": 2005}, {"title": "Kernel codebooks for scene categorization", "author": ["Gemert", "Jan C", "Geusebroek", "Jan-Mark", "Veenman", "Cor J", "Smeulders", "Arnold W"], "venue": "In ECCV,", "citeRegEx": "Gemert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gemert et al\\.", "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey", "Salakhutdinov", "Ruslan"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improved versions of learning vector quantization", "author": ["Kohonen", "Teuvo"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Kohonen and Teuvo.,? \\Q1990\\E", "shortCiteRegEx": "Kohonen and Teuvo.", "year": 1990}, {"title": "Shape descriptors for non-rigid shapes with a single closed contour", "author": ["L.J. Latecki", "R. Lakamper", "U. Eckhardt"], "venue": "In CVPR,", "citeRegEx": "Latecki et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Latecki et al\\.", "year": 2000}, {"title": "Supervised learning of quantizer codebooks by information loss minimization", "author": ["Lazebnik", "Svetlana", "Raginsky", "Maxim"], "venue": "IEEE TPAMI, pp", "citeRegEx": "Lazebnik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2009}, {"title": "Principles of neurodynamics; perceptrons and the theory of brain mechanisms", "author": ["Rosenblatt", "Frank"], "venue": "Spartan Books,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1962\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1962}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["Scholkopf", "Bernhard", "Smola", "Alexander J"], "venue": null, "citeRegEx": "Scholkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Scholkopf et al\\.", "year": 2001}, {"title": "Soft learning vector quantization", "author": ["Seo", "Sambu", "Obermayer", "Klaus"], "venue": "Neural Computation,", "citeRegEx": "Seo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2002}, {"title": "Shape matching and modeling using skeletal context", "author": ["J. Xie", "P. Heng", "M. Shah"], "venue": "Pattern Recognition,", "citeRegEx": "Xie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "Neural networks and their back-propagation algorithm (Rumelhart et al., 1986) became popular because they offered an automatic way of learning flexible representations by introducing the so-called hidden layers and hidden units into multilayer Perceptron architectures.", "startOffset": 53, "endOffset": 77}, {"referenceID": 11, "context": "Our first real dataset is given by the MPEG-7 CE-1 Part B database (Latecki et al., 2000), which we will refer to as the mpeg-7 dataset.", "startOffset": 67, "endOffset": 89}, {"referenceID": 0, "context": "These are the shape and skeletal contexts in Belongie et al. (2002) and Xie et al.", "startOffset": 45, "endOffset": 68}, {"referenceID": 0, "context": "These are the shape and skeletal contexts in Belongie et al. (2002) and Xie et al. (2008), respectively.", "startOffset": 45, "endOffset": 90}, {"referenceID": 0, "context": "Probabilistic Prototype Standard Prototype Shape Context (Belongie et al., 2002) Skeletal Context (Xie et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 17, "context": ", 2002) Skeletal Context (Xie et al., 2008) 85.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": "In geosciences and process control this is known as unmixing (Bergman, 2006).", "startOffset": 61, "endOffset": 76}, {"referenceID": 5, "context": "Bishop et al., 1998). The work by Seo & Obermayer (2002) is related to ours in their attempt to generalize LVQ but their probabilistic model is inherently generative (see their equation 4).", "startOffset": 0, "endOffset": 57}, {"referenceID": 7, "context": "For example, Gemert et al. (2008) replace histograms with kernel density estimation (KDE) in the construction of codebooks and use the extracted features in conjunction with SVMs (nonprobabilistic approach) for classification.", "startOffset": 13, "endOffset": 34}, {"referenceID": 6, "context": "On a similar vein, Boureau et al. (2010) propose the learning of mid-level features for recognition in computer vision.", "startOffset": 19, "endOffset": 41}, {"referenceID": 3, "context": "From the application viewpoint, our method can be viewed as a discriminative technique that can be used for unmixing in geosciences and remote sensing (Bergman, 2006).", "startOffset": 151, "endOffset": 166}], "year": 2012, "abstractText": "In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where an input datapoint is described by a set of feature vectors and its associated output may be given by soft labels indicating, for example, class probabilities. We represent an input datapoint as a K-dimensional vector, where each component is a mixture of probabilities over its corresponding set of feature vectors. Each probability indicates how likely a feature vector is to belong to one-out-of-K unknown prototype patterns. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. We show that our model can be seen as a probabilistic generalization of learning vector quantization (LVQ). We apply our method to the problems of shape classification, hyperspectral imaging classification and people\u2019s work class categorization, showing the superior performance of our method compared to the standard prototype-based classification approach and other competitive benchmarks.", "creator": "LaTeX with hyperref package"}}}