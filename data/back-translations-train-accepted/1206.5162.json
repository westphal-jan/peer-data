{"id": "1206.5162", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2012", "title": "Fast Variational Inference in the Conjugate Exponential Family", "abstract": "We present a general method for deriving collapsed variational inference algo- rithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound.", "histories": [["v1", "Fri, 22 Jun 2012 14:36:15 GMT  (34kb,D)", "https://arxiv.org/abs/1206.5162v1", null], ["v2", "Tue, 4 Dec 2012 19:35:34 GMT  (54kb,D)", "http://arxiv.org/abs/1206.5162v2", "Accepted at NIPS 2012"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james hensman", "magnus rattray", "neil d lawrence"], "accepted": true, "id": "1206.5162"}, "pdf": {"name": "1206.5162.pdf", "metadata": {"source": "CRF", "title": "Fast Variational Inference in the Conjugate Exponential Family", "authors": ["James Hensman", "Magnus Rattray"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A popular solution [King and Lawrence, 2006, Teh et al., 2007, Kurihara et al., 2007, Sung et al., 2008, La'zaro-Gredilla and Titsias, 2011, La'zaro-Gredilla et al., 2011] is to analytically marginalize a portion of the varying approximate distribution and remove it from optimization. In this paper, we provide a unifying framework for broken conclusions in the general class of models composed of conjugate exponential graphs (CEGs).First, we review the corpus of previous work with a precise and unifying derivation of the broken boundaries. We describe how the applicability of broken quantities to certain CEGs can be determined by a simple D separation."}, {"heading": "2 The Marginalised Variational Bound", "text": "However, the advantages of analytically marginalizing a subset of variables in boundaries of variation seem well understood: several different approaches have been proposed in the context of specific models. In Dirichlet process mixing models, Kurihara et al. [2007] proposed a broken down approach in which both cluster parameters and mixing weights were marginalized, again with some approximations. Teh et al. [2007] proposed a broken down inference method for latent dirichlet allocation (LDA). In this paper, we unify all these results from the perspective of the \"KL-corrected boundary\" [King and Lawrence, 2006]. This lower boundary of model evidence is also an upper limit for the original boundary of variation; the difference between the two boundaries is given by a cullback-Leibler divergence."}, {"heading": "2.1 Variational Inference", "text": "Suppose we have a probabilistic model for data, D, because the parameters (and / or latent variables) q (Q), X, Z, the form p (D, X, Z) = p (D, Z) p (X) p (X). In variable bayes (see, for example, Bishop [2006]), we approach the trailing p (Z, X | D) by a distribution q (Z, X) dZ dX to derive a lower limit on the model proof L, which serves as an objective function in variational optimization: p (D) \u2265 L = q (Z, X) lnp (D, X) dZ dX. (1) For the traceability of the center field (MF) approach, q factorises through its variables, q (Z) q (X) q (X) q (X) q) q (X) q (X)."}, {"heading": "3 Partial Equivalence of the Bounds", "text": "We can restore LMF from LKL by re-applying Jensen's inequality, LKL = ln-q (X) p (X) p (X) exp {L1} dX-q (X) ln {p (X) q (X) exp {L1} dX, (6) which can be rearranged to obtain the center boundary, LKL-q (X) q (Z) ln {p (D | Z, X) p (Z) p (X) q (X) dX dZ, (7), and consequently LKL = LMF + KL (q (X) | q (X) and1 LKL \u2265 LMF. For a given q (Z) p (X) p (X) p (X) dX dZ are the same after q (X) is updated using the center method: The approximation values are ultimately the same. The advantage of the new boundary is to reduce the number of parameters in the optimization."}, {"heading": "3.1 Gradients", "text": "Consider the gradient of the KL corrected equation with respect to the parameters of q (Z): \u2202 LKL \u2202 \u03b8z = exp (5). To find the gradient of the center boundary, we note that it can be written with respect to our conditional boundary (3) asLMF = Eq (X) [L1 + ln p (X) \u2212 ln q (X)], pointing out that it has to be written with respect to our conditional boundary (3) asLMF = Eq (X) [L1 + ln p (X) \u2212 ln q (X)]. This results in the boundaries being equal, LMF = LKL, but also their gradients with respect to the CZ.Sato [2001] has shown that the variable actualization equation can be interpreted as a gradient."}, {"heading": "3.2 Curvature of the Bounds", "text": "King and Lawrence [2006] have shown empirically that the KLC boundary could lead to faster convergence, because the boundaries differ in their curvature: The curvature of the KLC boundary allows an optimizer to take greater steps. We now derive analytical expressions for the curvature of both boundaries. For the middle field boundary we have \u2202 2LMF pending 2z = Eq (X) [6], (10) and for the KLC boundary, with some manipulations of (4) and use (5): 6 2LKL \u043d [7] z = e \u2212 LKL 2eLKL \u043d [7] z = e \u2212 LKL \u043d [7] z \u2212 2LKL Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol Protocol"}, {"heading": "3.3 Relationship to Collapsed VB", "text": "For example, Sung et al. [2008] proposed a latent variable model in which the model parameters were marginalized, and Teh et al. [2007] proposed a non-parametric theme model in which the proportions of the document collapsed, leading to improved inference or faster convergence. The KLC-bound derivative we provided also marginalizes parameters, but after a variable approximation. The difference between the two approaches is distilled in these expressions: lnEp (X) [exp {Eq (Z) [ln p (D | X, Z)] Eq (Z) [ln {Ep (X) [p (D | X, Z)]] (12), where the left expression appears in the KLC boundary, and the right expression appears in the boundary for collapsed variable bayes, with the rest of the boundary being the same."}, {"heading": "3.4 Applicability", "text": "We select the variables that break the dependency structure of the diagram to enable the analytical calculation of the integral in (4). Assuming the appropriate conjugate exponential structure for the model, we are left with the requirement to select a subset that induces the appropriate factorization, which induced factorizations are discussed in detail in Bishop [2006]. These are factorizations in the approximate posterior range resulting from the shape of the variable approximation and the structure of the model. These factorizations allow the application of KLC-bound and can be identified by a simple d-separation test. The d-separation test involves verifying the independence between the marginalized variables (X in the above) conditioned by the observed data D and the approved 2Kurihara et al models."}, {"heading": "4 Riemannian Gradient Based Optimisation", "text": "Sato [2001] and Hoffman et al. [2012] showed that the VBEM process performs a gradient increase in the space of natural parameters. Using the KLC, which is bound to break down the problem, gradient methods seem to be a natural choice for optimization, since there are fewer parameters to deal with, and we have shown that the calculation of the gradients is simple (the variation update equations contain the model gradients). It turns out that the KLC limit is particularly suitable for Rieman or natural gradient methods, because the information geometry of the exponential family construction (s) we optimize leads to a simple expression for the natural gradient. Previous studies of natural gradients for variable bayes [Honkela et al., 2010, Kuusela et al., 2009] required the inversion of the Fisher information at each step (ours not), while the KEM steps are bound to some of the ribbon parameters and are only vectors in nature."}, {"heading": "4.1 Variable Transformations", "text": "We can calculate the natural gradient of our collapsed boundary by looking at the updated equations of the non-collapsed problem as described above. However, if we want to apply more powerful optimization methods such as conjugated gradient ascent, it is helpful to parameterize the natural parameters in an unrestricted manner. [Amari and Nagaoka, 2007] gives the natural gradient: g = G (\u03b8) \u2212 1 x x x x x x x x x x x x x x x x x x x x x x x x x. (14) For exponential family distributions without x x x x x x x."}, {"heading": "4.2 Steepest Ascent is Coordinate Ascent", "text": "Sato [2001] showed that the VBEM algorithm was a gradient-based algorithm. In fact, VBEM consists of unit steps in the direction of the natural gradient of the canonical parameters. Equation (9) and Sato's work [2001] show that the gradient of the KLC boundary can be determined by taking into account the standard intermediate field update for the non-collapsed parameter Z. We confirm these relationships in the supplementary material for the models examined in the next section. After confirming that the VB-E step corresponds to the steepest gradient ascent, we are now investigating whether the procedure could be improved by using conjugate gradients."}, {"heading": "4.3 Conjugate Gradient Optimization", "text": "One idea to solve some of the problems associated with the steepest rise is to ensure that each gradient step (geometrically) is conjugated with the previous one. Honkela et al. [2010] applied conjugated gradients to the standard mean field boundary, so that due to its different curvature we expect much faster convergence for the KLC boundary. Since VBEM uses a stride length of 1 for optimization, we have also applied this stride length to conjugated gradients. In the natural conjugation method, the search direction at the ith iteration is determined by si = \u2212 g-i + \u03b2si \u2212 1. Empirically, the Fletcher-Reeves method worked well for us: \u03b2FR = < g-i < g-i < g < i < i < g-i < g-i < g-i \u2212 1, g-i \u2212 1 > \u2212 1 (16), whereby Rieltmann can be defined by G \u00b7 > the inner product."}, {"heading": "5 Experiments", "text": "For the empirical investigation of possible accelerations, we selected a series of probability models. We provide derivatives of the bound and more complete explanations of the models in the supplementary material. In each experiment, it was assumed that the algorithm converged when the change of the boundary or the Riemann gradient below 10 \u2212 6. Comparisons between optimization methods always used the same initial conditions (or a set of initial conditions) for each method. First, we reproduce the mixture of Gaussians described by Honkela et al. [2010]."}, {"heading": "5.1 Mixtures of Gaussians", "text": "For a mixture of Gaussians, using the d separation rule, we select the cluster mapping (latent) variables for X, which are parameterized by the softmax function for unrestricted optimization. Our model includes a fully Bayesian treatment of cluster parameters and mixing ratios, the approximate rear distributions of which appear as (5). Full details of the algorithm derivation are given in the supplementary material. A nice feature is that we can deduce from the discussion of an expression for the natural gradient without a matrix. In Honkela et al. [2010], the data is drawn from a mixture of five two-dimensional Gaussians with equal weights, each unit having spherical covariance. The centers of the components are at (0, 0) and (\u00b1 R, \u00b1 R)."}, {"heading": "5.2 Topic Models", "text": "Deferred Dirichlet Allocation (LDA) [Lead et al., 2003] is a popular approach to extracting topics from documents. To demonstrate the KLC binding, we applied it to 200 papers from the 2011 NIPS conference. PDFs were pre-processed with Pdftotext, removing non-alphabetic characters and roughly filtered words by popularity to form a vocabulary size of 2000.4. We selected the latent variables for topic assignment for parameterization, topic breakdown, and document proportions. Conjugate gradient optimization was compared to the standard VBEM approach. We used twelve random initializations, with each algorithm assuming every initial condition. Topic and document distributions were treated with fixed, non-informative priorities."}, {"heading": "5.3 RNA-seq alignment", "text": "The BitSeq method [Glaus et al., 2012] is based on a probabilistic model and uses Gibbs sampling as an approximate conclusion; the sampler may suffer from particularly slow convergence due to the size of the problem, which has six million latent variables for the data considered here. We implemented a variable version of their model and optimized it using VBEM and our Collapsed Riemannian Method. We applied the model to the data of human microRNA described in Xu et al. [2010]. The model was initialized and optimized using four random starting conditions using standard VBEM and the conjugate gradient versions of the algorithm. Polack-Ribie re-conjugation method provided very poor performance for this problem and often gave a negative conjugation of Gibaus: We leave it here. Solutions for the other methods were only executed within approximately 46 seconds."}, {"heading": "6 Discussion", "text": "Under very general conditions (conjugated exponential family), we have shown the equivalence of broken limits of variation and marginalized limits of variation with the KL-corrected perspective of King and Lawrence [2006]. We have provided a concise derivation of these limits by combining several strands of work and laying the foundations for a much broader application. If the broken variables are bundled in the standard MF, the KLC limit is identical to the MF limit. Sato [2001] has shown that the rise of the MF limit (as of VBEM updates) is equivalent to the rise of the MF limit. This implies the standard variation in terms of value and gradients."}, {"heading": "Acknowledgements", "text": "The authors thank Michalis Titsias for his helpful comment on an earlier draft and Peter Glaus for his help with the C + + implementation of the RNAseq alignment algorithm. This work was funded by the EU FP7-CBBE Project Ref 289434 and BBSRC grant number BB / 1004769 / 1."}, {"heading": "7 Conjugate gradient algorithms", "text": "There are several methods for approximating the parameter \u03b2 in the conjugate gradient algorithm: We used the methods Polack-Ribie re, Fletch-Reeves or Hestenes-Boot: \u03b2PR = < g-i, g-g-i \u2212 1 > i < g-i \u2212 1, g-i \u2212 1 > i \u2212 1 \u03b2FR = < g-i, g-i > i < g-i \u2212 1, g-i \u2212 1 > i \u2212 1 > i \u2212 1\u03b2HS = < g-i, g-g-i \u2212 1 > i < g-i \u2212 1, g-g-i \u2212 1 > i \u2212 1 (17) where < \u00b7, \u00b7 > i denotes the inner product in the Rieman geometry given by g-i > G (A) g."}, {"heading": "8 Mixture of Gaussians", "text": "A MoG model is defined as follows: We have a set of N D-dimensional vectors Y = {yn} Nn = 1. The probability is p (Y | \u03b7, L) = K \u00b2 k = 1 N \u00b2 n = 1 N (yn | \u00b5k, preservation \u2212 1 k)'nk (18), where L is a collection of binary latent variables indicating cluster membership, L = {'nk} Nn = 1 Kk = 1 and \u03b7 is a collection of cluster parameters. Previous value over L is obtained by a multinomic distribution of components that in turn have a funnel advance with uniform concentrations, which is for simplicity's sake: p (L \u00b2 k) = K \u00b2 k = 1 N \u00b2 n = 1 Kendall'nkk, p (\u03c0) = RD \u00b2 k \u2212 1k (19), which represents a K-dimensional vector with elements."}, {"heading": "8.1 Applying the KLC bound", "text": "The first task in applying the KLC is now to choose which variables to parameterise HL and which to marginalise. From the graphical representation of the MoG problem in Figure 2, we can see that it is possible to select the variables the other way round: the parameterisation and marginalisation of L, but the parameterisation of the latent variables makes the implementation a little easier. We use a factorised multinomial distribution q (L) to approximate the variables to p (L), parameterised with the softmax functions soq (L) = N = 1 K = 1 r'nknk, rnk = e. We are now ready to apply the procedure described above."}, {"heading": "9 Latent Dirichlet Allocation", "text": "The latent dirichlet assignment is a popular topic model. See Lead et al. [2003] for a thorough introduction. Suppose we have D documents, K topics, and a vocabulary of size V. The dth document contains Nd words Wd = {wdn} Ndn = 1, and each word is used as a binary vector wdn = 0, 1} V. Each word is associated with a latent variable \"dn,\" which assigns the word to a topic, i.e. \"dn = 0, 1} K. We use W to represent the collection of all words, W = {Wd} Dd = 1, and L to represent the collection of all latent variables L = {'dn} Ndn = 1} Dd = 1. Each document has a related vector of theme proportions, and each topic is represented by a vector of word proportions."}, {"heading": "9.1 The collapsed bound", "text": "To derive the collapsed boundary, we use a similar d separation test as for the blending model to select the latent variables as parameters (non-collapsed) nodes. See Figure 3.To continue, we assume a factoring multinomial disadvantage for L: q (L) = D-D = 1 Nd-D = 1 Nd-K = 1 r'dnk (29) is subject to the constraint K-K = 1'dnk = 1, which we apply by a soft maximum repair measurement rdnk = e. (30) We assume that we derive the conditional boundary ln p (W-D)."}, {"heading": "9.2 Topics found by LDA", "text": "For the sake of completeness, here are some topics found by the LDA on the NIPS conference data."}, {"heading": "10 BitSeq Model", "text": "The generative model for an RNA-seq test is as follows: We assume that the experiment consists of a stack of RNA fragments, with the frequency of transcript Tm fragments in the test \u03b8m. The sequencer then randomly selects a fragment from the stack, so that the probability of selecting a fragment corresponding to transcript Tm is \u03b8m. By introducing a convenient affiliation vector'n for each read transcript, we can select Writep (\"nm = 1\" m = 1 \"nmm\" (34), where \"nm, 0, 1\" is a binary variable that indicates whether the ninth fragment comes from the smallest transcript (\"nm = 1\") and is subject to M m = 1 \"nm = 1. We use L to represent the collection of all alignment variables. Both transcripts are variables that can be derived from the main target."}, {"heading": "10.1 The collapsed bound", "text": "Figure 4 shows a graphic representation of the BitSeq model. It is clear that the parameterization of the latent variables allows us to collapse the conditional limit as usual, or vice versa. If we again select the latent variables for parameterization, X = {L}, Z = {\u03b8}, we first find the conditional limit as follows: ln p (R | T, \u03b8) = ln p (R | L, T) p (L | \u03b8) dL \u2265 Eq (L) [ln p (R | L, T) + ln p (L | \u03b8) \u2212 ln q (L) = 1 m \u00b2 m \u00b2 n \u00b2 m \u00b2 m \u00b2 m \u00b2 (rn | Tm) + ln \u00b2 m \u00b2 L1 (37) It is clear that this limit adapts to the previous limit for 2001 so that we can marginalize it: ln p (R | T) > LKL = n \u00b2 m \u00b2 m \u00b2 m \u00b2 m \u00b2 m \u00b2 (l \u00b2)."}], "references": [{"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Y. Teh"], "venue": "arXiv preprint arXiv:1205.2662,", "citeRegEx": "Asuncion et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2012}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Identifying differentially expressed transcripts from RNA-seq data with biological variation", "author": ["P. Glaus", "A. Honkela", "M. Rattray"], "venue": null, "citeRegEx": "Glaus et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Glaus et al\\.", "year": 2012}, {"title": "Stochastic variational inference", "author": ["M. Hoffman", "D. Blei", "C. Wang", "J. Paisley"], "venue": "arXiv preprint arXiv:1206.7051,", "citeRegEx": "Hoffman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2012}, {"title": "Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes", "author": ["A. Honkela", "T. Raiko", "M. Kuusela", "M. Tornio", "J. Karhunen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Honkela et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Honkela et al\\.", "year": 2010}, {"title": "Fast variational inference for Gaussian process models through KL-correction", "author": ["N. King", "N.D. Lawrence"], "venue": "Machine Learning: ECML", "citeRegEx": "King and Lawrence.,? \\Q2006\\E", "shortCiteRegEx": "King and Lawrence.", "year": 2006}, {"title": "Collapsed variational Dirichlet process mixture models", "author": ["K. Kurihara", "M. Welling", "Y.W. Teh"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Kurihara et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kurihara et al\\.", "year": 2007}, {"title": "A gradient-based algorithm competitive with variational Bayesian EM for mixture of Gaussians", "author": ["M. Kuusela", "T. Raiko", "A. Honkela", "J. Karhunen"], "venue": "In Neural Networks,", "citeRegEx": "Kuusela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kuusela et al\\.", "year": 2009}, {"title": "Variational heteroscedastic Gaussian process regression", "author": ["M. L\u00e1zaro-Gredilla", "M.K. Titsias"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "L\u00e1zaro.Gredilla and Titsias.,? \\Q2011\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla and Titsias.", "year": 2011}, {"title": "Overlapping mixtures of Gaussian processes for the data association problem", "author": ["M. L\u00e1zaro-Gredilla", "S. Van Vaerenbergh", "N. Lawrence"], "venue": "Pattern Recognition,", "citeRegEx": "L\u00e1zaro.Gredilla et al\\.,? \\Q2011\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla et al\\.", "year": 2011}, {"title": "Online model selection based on the variational Bayes", "author": ["M.A. Sato"], "venue": "Neural Computation,", "citeRegEx": "Sato.,? \\Q2001\\E", "shortCiteRegEx": "Sato.", "year": 2001}, {"title": "Latent-space variational Bayes", "author": ["J. Sung", "Z. Ghahramani", "S. Bang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Sung et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2008}, {"title": "A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation", "author": ["Y.W. Teh", "D. Newman", "M. Welling"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Transcriptome and targetome analysis in MIR155 expressing cells using RNA-seq", "author": ["G. Xu"], "venue": "RNA, pages 1610\u20131622,", "citeRegEx": "Xu,? \\Q2010\\E", "shortCiteRegEx": "Xu", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "In this paper we unify all these results from the perspective of the \u2018KL corrected bound\u2019 [King and Lawrence, 2006].", "startOffset": 90, "endOffset": 115}, {"referenceID": 6, "context": "In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach using both truncated stick-breaking and symmetric priors.", "startOffset": 36, "endOffset": 59}, {"referenceID": 6, "context": "In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach using both truncated stick-breaking and symmetric priors. Sung et al. [2008] proposed \u2018latent space variational Bayes\u2019 where both the clusterparameters and mixing weights were marginalised, again with some approximations.", "startOffset": 36, "endOffset": 166}, {"referenceID": 6, "context": "In Dirichlet process mixture models Kurihara et al. [2007] proposed a collapsed approach using both truncated stick-breaking and symmetric priors. Sung et al. [2008] proposed \u2018latent space variational Bayes\u2019 where both the clusterparameters and mixing weights were marginalised, again with some approximations. Teh et al. [2007] proposed a collapsed inference procedure for latent Dirichlet allocation (LDA).", "startOffset": 36, "endOffset": 329}, {"referenceID": 6, "context": "In this paper we unify all these results from the perspective of the \u2018KL corrected bound\u2019 [King and Lawrence, 2006]. This lower bound on the model evidence is also an upper bound on the original variational bound, the difference between the two bounds is given by a Kullback Leibler divergence. The approach has also been referred to as the marginalised variational bound by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011].", "startOffset": 91, "endOffset": 405}, {"referenceID": 6, "context": "In this paper we unify all these results from the perspective of the \u2018KL corrected bound\u2019 [King and Lawrence, 2006]. This lower bound on the model evidence is also an upper bound on the original variational bound, the difference between the two bounds is given by a Kullback Leibler divergence. The approach has also been referred to as the marginalised variational bound by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011]. The connection between the KL corrected bound and the collapsed bounds is not immediately obvious.", "startOffset": 91, "endOffset": 441}, {"referenceID": 1, "context": "Bishop [2006]) we approximate the posterior p(Z,X|D) by a distribution q(Z,X).", "startOffset": 0, "endOffset": 14}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011].", "startOffset": 0, "endOffset": 248}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011]. The resulting bound is not dependent on q(X).", "startOffset": 0, "endOffset": 284}, {"referenceID": 6, "context": "King and Lawrence [2006] substituted the expression for the optimal distribution (for example q(X)) back into the bound (1), eliminating one set of parameters from the optimisation, an approach that has been reused by L\u00e1zaro-Gredilla et al. [2011], L\u00e1zaro-Gredilla and Titsias [2011]. The resulting bound is not dependent on q(X). King and Lawrence [2006] referred to this new bound as \u2018the KL corrected bound\u2019.", "startOffset": 0, "endOffset": 356}, {"referenceID": 6, "context": "giving us the bound of King and Lawrence [2006] & L\u00e1zaro-Gredilla et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 6, "context": "giving us the bound of King and Lawrence [2006] & L\u00e1zaro-Gredilla et al. [2011]. Note that one set of parameters was marginalised after the variational approximation was made.", "startOffset": 23, "endOffset": 80}, {"referenceID": 10, "context": "Sato [2001] has shown that the variational update equation can be interpreted as a gradient method, where each update is also a step in the steepest direction in the canonical parameters of q(Z).", "startOffset": 0, "endOffset": 12}, {"referenceID": 5, "context": "Honkela et al. [2010] looked to rectify this weakness by applying a conjugate gradient algorithm to the mean field bound.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "2 Curvature of the Bounds King and Lawrence [2006] showed empirically that the KLC bound could lead to faster convergence because the bounds differ in their curvature: the curvature of the KLC bound enables larger steps to be taken by an optimizer.", "startOffset": 26, "endOffset": 51}, {"referenceID": 6, "context": "In this result the first term is equal to (10), and the second two terms combine to be always positive semi-definite, proving King and Lawrence [2006]\u2019s intuition about the curvature of the bound.", "startOffset": 126, "endOffset": 151}, {"referenceID": 12, "context": "For example, Sung et al. [2008] proposed a latent variable model where the model parameters were marginalised, and Teh et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 12, "context": "For example, Sung et al. [2008] proposed a latent variable model where the model parameters were marginalised, and Teh et al. [2007] proposed a nonparametric topic model where the document proportions were collapsed.", "startOffset": 13, "endOffset": 133}, {"referenceID": 12, "context": "Sung et al. [2008] propose a first order approximation to the expectation of the form Eq(Z) [ f(Z) ] \u2248 f(Eq(Z) [ Z ] ), which reduces the right expression to the that on the left.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "The form of these corrections would need to be determined on a case by case basis, and has in fact been shown to be less effective than those methods unified here [Asuncion et al., 2012].", "startOffset": 163, "endOffset": 186}, {"referenceID": 0, "context": "These induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in the approximate posterior which arise from the form of the variational approximation and from the structure of the model.", "startOffset": 61, "endOffset": 75}, {"referenceID": 0, "context": "These induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in the approximate posterior which arise from the form of the variational approximation and from the structure of the model. These factorisations allow application of KLC bound, and can be identified using a simple d-separation test as Bishop discusses. The d-separation test involves checking for independence amongst the marginalised variables (X in the above) conditioned on the observed data D and the approximated 2Kurihara et al. [2007] and Teh et al.", "startOffset": 61, "endOffset": 543}, {"referenceID": 0, "context": "These induced factorisations are discussed in some detail in Bishop [2006]. They are factorisations in the approximate posterior which arise from the form of the variational approximation and from the structure of the model. These factorisations allow application of KLC bound, and can be identified using a simple d-separation test as Bishop discusses. The d-separation test involves checking for independence amongst the marginalised variables (X in the above) conditioned on the observed data D and the approximated 2Kurihara et al. [2007] and Teh et al. [2007] suggest a further second order correction and assume that that q(Z) is Gaussian to obtain tractability.", "startOffset": 61, "endOffset": 565}, {"referenceID": 12, "context": "This allowed Sung et al. [2008] to derive a general form for latent variable models, though our formulation is general to any conjugate exponential graph.", "startOffset": 13, "endOffset": 32}, {"referenceID": 4, "context": "Sato [2001] and Hoffman et al. [2012] showed that the VBEM procedure performs gradient ascent in the space of the natural parameters.", "startOffset": 16, "endOffset": 38}, {"referenceID": 11, "context": "2 Steepest Ascent is Coordinate Ascent Sato [2001] showed that the VBEM algorithm was a gradient based algorithm.", "startOffset": 39, "endOffset": 51}, {"referenceID": 11, "context": "2 Steepest Ascent is Coordinate Ascent Sato [2001] showed that the VBEM algorithm was a gradient based algorithm. In fact, VBEM consists of taking unit steps in the direction of the natural gradient of the canonical parameters. From equation (9) and the work of Sato [2001], we see that the gradient of the KLC bound can be obtained by considering the standard meanfield update for the non-collapsed parameter Z.", "startOffset": 39, "endOffset": 274}, {"referenceID": 5, "context": "Honkela et al. [2010] applied conjugate gradients to the standard mean field bound, we expect much faster convergence for the KLC bound due to its differing curvature.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "We note from Kuusela et al. [2009] that this can be simplified since g\u0303>Gg\u0303 = g\u0303>GG\u22121g = g\u0303>g, and other conjugate methods, defined in the supplementary material, can be applied similarly.", "startOffset": 13, "endOffset": 35}, {"referenceID": 5, "context": "First we recreate the mixture of Gaussians example described by Honkela et al. [2010].", "startOffset": 64, "endOffset": 86}, {"referenceID": 5, "context": "In Honkela et al. [2010] data are drawn from a mixture of five two-dimensional Gaussians with equal weights, each with unit spherical covariance.", "startOffset": 3, "endOffset": 25}, {"referenceID": 2, "context": "2 Topic Models Latent Dirichlet allocation (LDA) [Blei et al., 2003] is a popular approach for extracting topics from documents.", "startOffset": 49, "endOffset": 68}, {"referenceID": 3, "context": "The BitSeq method [Glaus et al., 2012] is based on a probabilistic model and uses Gibbs sampling for approximate inference.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "All the variational approaches represent an improvement over a Gibbs sampler, which takes approximately one week to run for this data [Glaus et al., 2012].", "startOffset": 134, "endOffset": 154}, {"referenceID": 3, "context": "The BitSeq method [Glaus et al., 2012] is based on a probabilistic model and uses Gibbs sampling for approximate inference. The sampler can suffer from particularly slow convergence due to the large size of the problem, which has six million latent variables for the data considered here. We implemented a variational version of their model and optimised it using VBEM and our collapsed Riemannian method. We applied the model to data described in Xu et al. [2010], a study of human microRNA.", "startOffset": 19, "endOffset": 465}, {"referenceID": 6, "context": "Under very general conditions (conjugate exponential family) we have shown the equivalence of collapsed variational bounds and marginalized variational bounds using the KL corrected perspective of King and Lawrence [2006]. We have provided a succinct derivation of these bounds, unifying several strands of work and laying the foundations for much wider application of this approach.", "startOffset": 197, "endOffset": 222}, {"referenceID": 6, "context": "Under very general conditions (conjugate exponential family) we have shown the equivalence of collapsed variational bounds and marginalized variational bounds using the KL corrected perspective of King and Lawrence [2006]. We have provided a succinct derivation of these bounds, unifying several strands of work and laying the foundations for much wider application of this approach. When the collapsed variables are updated in the standard MF bound the KLC bound is identical to the MF bound in value and gradient. Sato [2001] has shown that coordinate ascent of the MF bound (as proscribed by VBEM updates) is equivalent to steepest", "startOffset": 197, "endOffset": 528}, {"referenceID": 6, "context": "We have shown that it is always less negative than that of the original variational bound allowing much larger steps in the variational parameters as King and Lawrence [2006] suggested.", "startOffset": 150, "endOffset": 175}], "year": 2012, "abstractText": "We present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic inference using our bound.", "creator": "TeX"}}}