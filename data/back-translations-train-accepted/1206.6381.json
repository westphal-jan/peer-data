{"id": "1206.6381", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Shortest path distance in random k-nearest neighbor graphs", "abstract": "Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R^d. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (669kb)", "http://arxiv.org/abs/1206.6381v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Mon, 9 Jul 2012 08:36:42 GMT  (578kb,D)", "http://arxiv.org/abs/1206.6381v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["morteza alamgir", "ulrike von luxburg"], "accepted": true, "id": "1206.6381"}, "pdf": {"name": "1206.6381.pdf", "metadata": {"source": "CRF", "title": "Shortest path distance in random k-nearest neighbor graphs", "authors": ["Morteza Alamgir", "Ulrike von Luxburg"], "emails": ["morteza@tuebingen.mpg.de", "ulrike.luxburg@tuebingen.mpg.de"], "sections": [{"heading": "1. Introduction", "text": "The shortest distance is the most basic distance between points in this graph, and it is widely used in computer science and in machine learning. In this paper we want to understand the geometry derived from the shortest distance in the random geometric graph. (...) It is about the way in which we look at the shortest distance between the fixed points in this graph. (...) It is about the way in which we describe the shortest path between the individual points in this graph-type and way. (...) It is about the behavior of the shortest path between the fixed points in this graph-type and way: 1. It is about the distance between the distances we can show the shortest path between the points in the graph-type and way we can assign the shortest path between the fixed points in the graph-type and way, how we assign the shortest path between the fixed points in this graph-type and way. (...) It is about the behavior of the shortest path between the fixed points in this graph-type of distance, how we can show the distance between the points in this graph-type and way Distance we can assign the distance between the distance in each other. (...) It is about the distance in this way that distance we can show the distance between the distance between the fixed points in this graph-type of points in this graph."}, {"heading": "2. Basic definitions", "text": "Consider a closed, connected subset X Rd, which is equipped with a density function p in relation to the Lebesgue measure. To simplify the representation, let us assume for the rest of the paper that the density p is associated with the Lipschitz constant L and is limited from 0 to 0. To simplify the notation later, we define the abbreviation x (x, y). We denote the Euclidean volume of the unit sphere in Rd. Assume the finite dataset X1,..., Xn was drawn i.d after p. We build a geometric diagram G = (V, E), which has the data points as linkages and linkages that are close."}, {"heading": "3. Shortest paths in unweighted graphs", "text": "In this section we examine the behaviour of the shortest path in the family of unweighted kNN graphs."}, {"heading": "4. Shortest paths in weighted graphs", "text": "In this section we discuss both questions from the introduction. We also extend our results from the previous section to include weighted kNN charts and \u03b5 charts."}, {"heading": "4.1. Weight assignment problem", "text": "Consider a graph based on the i.i.d. sample X1,..., Xn-X from the density p. We are given a positive scalar function f, which is only a function of the density: f (x) = f (p). We want to assign edge weights in such a way that the graph SP distance to the f distance converges into X. It is generally known that the f length of a curve \u03b3: [a, b] \u2192 X leads to subintervals [xi, xi + 1] by a Riemann sum over a division of [a, b]."}, {"heading": "4.2. Limit distance problem", "text": "Consider a weighted graph based on the i.i.d. sample X1,..., Xn-X from the density p. We get an increasing edge weight function h: R + \u2192 R +, which assigns the weight h to the edge (x, y). We are interested in finding the limit of the graph SP distance in relation to the edge weight function h, as the sample size goes into infinity. In particular, we are looking for a distance function f, so that the SP distance converges with the f distance. Suppose we know the solution f = f (p (x)) of this problem. To guarantee the convergence of the distances, f \u00b2 should assign the weighting of the shape of wij \u00b2 f (p (Xi)), Xi \u2212 Xj \u00b2. This would mean that p (Xi) = f (p (x)), h (\"Xi \u2212 Xj\"), Xi (\"problem\" Xi \"is closely related to Xj \u2212 that Xj is."}, {"heading": "4.2.1. Subadditive weights", "text": "A function h (x) is called a subadditive if we formally confirm this statement. (x) A function h (x) is called a subadditive if we (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x x x x) x x x x x x x (x) x x x (x) x x x x x x (x) x (x) x x (x) x (x) x x (x) x x (x) x x (x) x (x) x (x) x x (x) x (x) x x x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x (x) x (x) x (x) x x (x) x (x) x (x x) x) x (x (x x x x"}, {"heading": "4.2.2. Supperadditive weights", "text": "A function h is called a superadditive when we get an intuition about the behavior of the SP for a superadditive h (x + y). Examples are f (x) = xa; a > 1 and f (x) = xex. To get an intuition about the behavior of the SP for a superadditive h, we take an example of three vertices x, y, z, all connected in the diagram and sitting on a straight line, so that the SP will take many \"small\" edges instead of a few \"long\" edges. Therefore, we do not expect much difference between superadditive-weighted kNN graphs and \u03b5 graphs: the long edges in the kNN graph will not be used anyway."}, {"heading": "5. Consequences in applications", "text": "In this section we will examine the consequences of our results on the multiple embedding by isomap and on a particular semi-supervised learning method. There are two cases in which we do not expect a drastic difference between the SP in the weighted and unweighted kNN graph: (1) If the underlying density p is almost uniform. (2) If the intrinsic dimensionality of our data d is high. The latter is because at the q distance the underlying density is in the form of p (x) 1 / d, with the exponent flattening the distribution for large d."}, {"heading": "5.1. Isomap", "text": "Isomap is a widely used method for embedding low-dimensional manifolds (Tenenbaum et al., 2000). The main idea is to use metric, multidimensional scaling on the matrix of paired geodesic distances. The use of Euclidean edge length as weighting leads to convergence of SP distance to geodesic distance. But what would be the effect of using isomap on unweighted diagrams? Our results of the last section already indicate that there is not much difference between unweighted and weighted \u03b5 diagrams for isomap. In the case of kNN diagrams, however, we differ because weighted and unweighted shortest paths measure different sizes. The effect of applying isomap on unweighted kNN diagrams can be easily demonstrated by the following simulation: We stitch 2000 points in R2 from a distribution showing two uniform high-density squares surrounded by a low density region."}, {"heading": "5.2. Semi-supervised learning", "text": "Our work is closely related to some of the literature on semi-supervised learning (SSL): in regularization-based approaches, the underlying density is either implicitly exploited, as was attempted in laplac regularization (Zhu et al., 2003, but see Nadler et al., 2009; Alamgir & von Luxburg, 2011 and Zhou & Belkin, 2011), or more explicitly as in authoritative regularization (Bousquet et al., 2004). Alternatively, new distance functions are defined on the basis of data that take into account the density of the unlabeled points."}, {"heading": "6. Conclusions and outlook", "text": "We have seen in this paper that the shortest path on unweighted kNN graphs has a very strange borderline behavior: it prefers to go through low-density regions and even takes large detours to avoid the high-density regions. In hindsight, this result seems obvious, but most people are surprised when they first hear about it. In particular, we believe it is important to spread this insight among machine learners who routinely use unweighted kNN graphs as a simple, robust alternative to \u03b5 graphs. In some ways, unweighted \u03b5 graphs and unweighted kNN graphs behave as \"duals\": While grades in \u03b5 graphs reflect the underlying density, they are independent of the density in kNN graphs. While the shortest path in \u03b5 graphs is independent of the underlying density and convergent to the climatic abbreviations, the N may be the shortest path to calculating density."}], "references": [{"title": "Phase transition in the familiy of p-resistances", "author": ["M. Alamgir", "U. von Luxburg"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Alamgir and Luxburg,? \\Q2011\\E", "shortCiteRegEx": "Alamgir and Luxburg", "year": 2011}, {"title": "Semi-supervised learning with density based distances", "author": ["A. Bijral", "N. Ratliff", "N. Srebro"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Bijral et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bijral et al\\.", "year": 2011}, {"title": "Measure based regularization", "author": ["O. Bousquet", "O. Chapelle", "M. Hein"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Shortest path through random points", "author": ["Hwang", "S.J", "Damelin S. B", "A.O. Hero"], "venue": "In Preprint available at Arxiv,", "citeRegEx": "Hwang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2012}, {"title": "Semi-supervised learning with the graph Laplacian: The limit of infinite unlabelled data", "author": ["B. Nadler", "N. Srebro", "X. Zhou"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Nadler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nadler et al\\.", "year": 2009}, {"title": "A strong law for the longest edge of the minimal spanning tree", "author": ["M. Penrose"], "venue": "Ann. of Prob.,", "citeRegEx": "Penrose,? \\Q1999\\E", "shortCiteRegEx": "Penrose", "year": 1999}, {"title": "Estimating and computing density based distance metrics", "author": ["Sajama", "A. Orlitsky"], "venue": "In International Conference on Machine learning (ICML),", "citeRegEx": "Sajama and Orlitsky,? \\Q2005\\E", "shortCiteRegEx": "Sajama and Orlitsky", "year": 2005}, {"title": "Supplementary material to \u201dA Global Geometric Framework for Nonlinear Dimensionality Reduction", "author": ["J. Tenenbaum", "V. de Silva", "J. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Hitting times, commute distances and the spectral gap in large random geometric graphs", "author": ["U. von Luxburg", "M. Hein", "A. Radl"], "venue": "In Preprint available at Arxiv,", "citeRegEx": "Luxburg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Luxburg et al\\.", "year": 2010}, {"title": "Semi-supervised learning by higher order regularization", "author": ["X. Zhou", "M. Belkin"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Zhou and Belkin,? \\Q2011\\E", "shortCiteRegEx": "Zhou and Belkin", "year": 2011}, {"title": "SemiSupervised Learning Using Gaussian Fields and Harmonic Functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "In International Conference of Machine Learning (ICML),", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "Tenenbaum et al. (2000) discuss the case of \u03b5- and kNN graphs when p is uniform and D is the geodesic distance.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) discuss the case of \u03b5- and kNN graphs when p is uniform and D is the geodesic distance. Sajama & Orlitsky (2005) extend these results to \u03b5-graphs from a general density p by introducing edge weights that depend on an explicit estimate of the underlying density.", "startOffset": 0, "endOffset": 137}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) discuss the case of \u03b5- and kNN graphs when p is uniform and D is the geodesic distance. Sajama & Orlitsky (2005) extend these results to \u03b5-graphs from a general density p by introducing edge weights that depend on an explicit estimate of the underlying density. In a recent preprint, Hwang & Hero (2012) consider completely connected graphs whose vertices come from a general density p and whose edge weights are powers of distances.", "startOffset": 0, "endOffset": 328}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) answer the question for a very special case with h(x) = x and uniform p.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) answer the question for a very special case with h(x) = x and uniform p. Hwang & Hero (2012) study the case h(x) = x, a > 1 for arbitrary density p.", "startOffset": 0, "endOffset": 117}, {"referenceID": 8, "context": "The following proposition is a direct adaptation of Proposition 31 from von Luxburg et al. (2010). Proposition 6 (Bounding R p,k and R max p,k ) Given \u03bb < 1/2 define rlow and rup as", "startOffset": 76, "endOffset": 98}, {"referenceID": 7, "context": "The proof idea is a generalization of the covering argument in the proof of the sampling lemma in Tenenbaum et al. (2000). We first construct a covering of X that consists of balls with approximately the same probability mass.", "startOffset": 98, "endOffset": 122}, {"referenceID": 7, "context": "Isomap is a widely used method for low dimensional manifold embedding (Tenenbaum et al., 2000).", "startOffset": 70, "endOffset": 94}, {"referenceID": 2, "context": ", 2009; Alamgir & von Luxburg, 2011 and Zhou & Belkin, 2011), or more explicitly as in measure based regularization (Bousquet et al., 2004).", "startOffset": 116, "endOffset": 139}, {"referenceID": 1, "context": ", 2009; Alamgir & von Luxburg, 2011 and Zhou & Belkin, 2011), or more explicitly as in measure based regularization (Bousquet et al., 2004). Alternatively, one defines new distance functions on the data that take the density of the unlabeled points into account. Here, the papers by Sajama & Orlitsky (2005) and Bijral et al.", "startOffset": 117, "endOffset": 308}, {"referenceID": 1, "context": "Here, the papers by Sajama & Orlitsky (2005) and Bijral et al. (2011) are most related to our paper.", "startOffset": 49, "endOffset": 70}, {"referenceID": 1, "context": "Here, the papers by Sajama & Orlitsky (2005) and Bijral et al. (2011) are most related to our paper. Both papers suggest different ways to approximate the density based distance from the data. In Sajama & Orlitsky (2005) it is achieved by estimating the underlying density while in Bijral et al.", "startOffset": 49, "endOffset": 221}, {"referenceID": 1, "context": "Here, the papers by Sajama & Orlitsky (2005) and Bijral et al. (2011) are most related to our paper. Both papers suggest different ways to approximate the density based distance from the data. In Sajama & Orlitsky (2005) it is achieved by estimating the underlying density while in Bijral et al. (2011), the authors omit the density estimation and use an approximation.", "startOffset": 49, "endOffset": 303}], "year": 2012, "abstractText": "Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs.", "creator": "TeX"}}}