{"id": "1206.6382", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains", "abstract": "In this paper, we present a novel framework incorporating a combination of sparse models in different domains. We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse Gaussian independence model (with a sparse covariance matrix). We provide efficient methods for decomposition of the data into two domains, \\viz Markov and independence domains. We characterize a set of sufficient conditions for identifiability and model consistency. Our decomposition method is based on a simple modification of the popular $\\ell_1$-penalized maximum-likelihood estimator ($\\ell_1$-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples $n$ scales as $n = \\Omega(d^2 \\log p)$, where $p$ is the number of variables and $d$ is the maximum node degree in the Markov model. Our conditions for recovery are comparable to those of $\\ell_1$-MLE for consistent estimation of a sparse Markov model, and thus, we guarantee successful high-dimensional estimation of a richer class of models under comparable conditions. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (195kb)", "http://arxiv.org/abs/1206.6382v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["majid janzamin", "animashree anandkumar"], "accepted": true, "id": "1206.6382"}, "pdf": {"name": "1206.6382.pdf", "metadata": {"source": "CRF", "title": "High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains", "authors": ["Majid Janzamin", "Animashree Anandkumar"], "emails": ["mjanzami@uci.edu", "a.anandkumar@uci.edu"], "sections": [{"heading": null, "text": "Keywords: High-dimensional covariance estimation, sparse selection of graphic models, sparse covariance phenomenon in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by author (s) / owner (s). Models, sparseness, convex optimization."}, {"heading": "1. Introduction", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country,"}, {"heading": "2. Background and Problem Statement", "text": "Notation: For each vector v-Rp and each real number a-Rp, the notation refers to the \"a-norm of the vector v-Rp\" given by \"v-Rp\": = (\u2211 p-1-vi-a) 1a. For each matrix U-Rp-p, the operator norm is given by \"| | U-Rp-p\": = max-z-a = 1-Uz-b for the parameters a, b-T-1. Specifically, we use the \"o-norm of the operator, which is equivalent to\" | | U-Z = 1,..., p-p-j = 1-Uij-Uz-b. \"We also use the\" xi-element norm \"to refer to the maximum absolute value of the inputs of the matrix U-R."}, {"heading": "2.1. Gaussian Graphical Models", "text": "A Gaussian graphical model is a family of common Gaussian distributions which, in accordance with a given graph, have the factor J = J. If one considers a graph G = (V, E), with V = {1,.., p}, one considers a vector of the Gaussian random variable X = [X1, X2,., Xp], each node i-V being associated with a Gaussian scalar random variable Xi. A Gaussian graphical model Markov on G has a probability density function (pdf), which can be defined as fX (x) exp [\u2212 12x T Jx + hTx], where J is a positively defined symmetric matrix whose savings pattern corresponds to the property of the graph G. More precisely, J (i, j) = 0 iff (i, j) Suff (i, j).The matrix variant is known as a concentration matrix or potentix."}, {"heading": "2.2. Problem Statement", "text": "We now give a detailed description of our problem, which consists of the problem of covariance, but which we cannot solve. (note) We assume that there are no unambiguous parameters for the identification of the model parameters. (note) However, we assume that a parametric model that is not identifiable is identifiable in relation to a measurement if there are two different parameters that are not identifiable. (note) So we have no hope of evaluating the model parameters from observed data. (note) We belong to the family of standard exponential distributions (Wainwright & Jordan, 2008). (note) It is also possible, and as such, in the minimal form. (note)"}, {"heading": "3. Analysis under Exact Statistics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Assumptions under Exact Statistics", "text": "We first provide a number of sufficient conditions under which we can guarantee that the decomposition of \u03a3 \u043c in (2) the concentration matrix J \u043a M and the residual matrix \u03a3 R. We impose the following limitations on the two matrices: (A.0) J \u0445 M is a positive definitive matrix: J \u043a M 0. (A.1) Diagonal entries of J \u0445 M are limited from above, i.e., the support of their off-diagonal entries is sufficient (.R) ij 6 = 0-0-0-0-0-1-0-1-1-1 Diagonal entries of \u03a3 R are zero-1-1-0-0-1-1-1-1-1-1-1-1-2-2-2-2-Diagonal entries of \u0445\u0430-R, and the support of their off-diagonal entries is sufficient (.R) ij 6 = 0-2-1-1-1-1 of their diagonal entries are fulfilled (.R) and the diagonal entries of \u0445\u0430-2-1-1-0 (.R) are fulfilled."}, {"heading": "3.2. Formulation of the Optimization Program", "text": "We now propose a method based on convex optimization for obtaining (J \u043a M, \u03a3 \u043c R) on the basis of the covariance matrix \u03a3 in (2). Consider the following program: (\u03a3) M, \u03a3 \"1, off denotes the\" 1 norm of off-diagonal entries, which is the sum of absolute values of off-diagonal entries, and (\u00b7) d denotes diagonal entries. Intuitively, the parameter \u03bb \"1, off denotes the\" 1 norm of off-diagonal entries, which is the sum of absolute values of off-diagonal entries, and (\u00b7 log \") d denotes diagonal entries. Intuitively, the parameter \u03bb imposes a penalty for large residual covariances, and under favorable conditions, the thrift in the residual matrix can be favored. The program in (4) can be reformulated."}, {"heading": "3.3. Guarantees and main results", "text": "The main result of the decomposition is as follows: The proofs can be found in the extended version of arXiv.Theorem 1 (Uniqueness of decomposition). Under (A.0) - (A.3) the optimal solutions of the primary dual optimization programs (6) and (4) are indicated by a covariance matrix \u03a3 *, if we set the parameter \u03bb = (J * J * M * M) = (J * R) in the optimization program in (4), and the decomposition is unique. Thus, we find that the proposed optimization programs in (4) and (6) uniquely restore the Markov concentration matrix J * M and the residual covariance matrix \u0432 * R under conditions (A.0) - (A.3)."}, {"heading": "4. Sample Analysis of the Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Optimization Program", "text": "(5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5)"}, {"heading": "4.2. Assumptions under Sample Statistics", "text": "(7), (7), (7), (7), (7), (7), (7), (7), (7), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8, 8, 8, 8, (8, 8, 8, 8, 8, 8, 8, 8, (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8, 8, 8, (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8), (8, (8), (8), (8), (8), (8), (8), (8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8 (8, 8, 8, 8, 8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8, 8 (8, 8, 8), 8, 8 (8, 8, 8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8 (8, 8, 8, 8), 8, 8, 8, 8, 8 (8, 8, 8, 8, 8, 8, 8 (8), 8, 8, 8, 8, 8, 8, 8, 8 (8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "4.3. Guarantees and Main Results", "text": "We are now ready to provide the main result.Theorem 2: Consider a Gaussian distribution with covariance matrix. (G) Consider the covariance matrix. (G) Consider the results of the Gaussian model. (G) Consider the results. (G) Consider the covariance matrix. (G) Consider the results. (G) Consider the results. (G) Consider the results. (G) Consider the unique optimal solutions of the original pair (7) and (G). (G) Consider the covariance matrix. (G) Consider the results. (G) Consider the covariance matrix. (G) Consider the results. (G) Consider the results. (G) Consider the results. (G) Consider the results. (G) Consider the results. (G) (G) Consider the results. (G) Consider the results. (G)"}, {"heading": "5. Experiments", "text": "In this section, we provide experimental results for the proposed algorithm. We refer to our proposed optimization program as the \"1 +\" J-J method and compare it to the well-known \"1 method,\" which is a special case of the proposed algorithm if \"2 +.\" The optimization programs are implemented by YALMIP (Lofberg, 2004) and SDPT3 (Toh et al., 1999) The underlying graph for the Markov part is an 8 \u00d7 8 2-D network structure (4-nearest neighboring network structure). We randomly select the non-zero open diagonal entries in J-M (according to the grid borders) from {\u2212 0.5, 0.5}. Then, we make sure that J-M is positively defined by selecting some uniform diagonal weighting."}, {"heading": "6. Conclusion", "text": "In this paper, we have presented an in-depth study of convex optimization methods and guarantees for high-dimensional covariance decomposition into sparse Markov and Independence ranges. We offer consistency guarantees for estimation in both Markov and Independence ranges, and establish efficient sample complexity results for our methodology. These results open up many future research directions. An important aspect is the relaxation of the savings constraints imposed in both ranges and the development of new methods to enable the decomposition of such models. Further considerations include the extension to discrete models and other models for the residual covariance matrix (e.g. low-grade matrices). Such results will drive the envelope of efficient models for high-dimensional estimation. It is worth noting that in many scenarios it is important to include latent variables, within our framework it is difficult to include both latent variables and marginal dependencies, and we provide learning and guarantees in the future."}], "references": [{"title": "An Introduction to Multivariate Statistical Analysis", "author": ["T.W. Anderson"], "venue": null, "citeRegEx": "Anderson,? \\Q1984\\E", "shortCiteRegEx": "Anderson", "year": 1984}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Fundamentals of statistical exponential families: with applications in statistical decision theory", "author": ["L.D. Brown"], "venue": "Lecture Notes-Monograph Series, Institute of Mathematical Statistics,", "citeRegEx": "Brown,? \\Q1986\\E", "shortCiteRegEx": "Brown", "year": 1986}, {"title": "Gaussian multiresolution models: Exploiting sparse Markov and covariance structure", "author": ["M.J. Choi", "V. Chandrasekaran", "A.S. Willsky"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Choi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2010}, {"title": "On a dualization of graphical gaussian models", "author": ["G. Kauermann"], "venue": "Scandinavian journal of statistics,", "citeRegEx": "Kauermann,? \\Q1996\\E", "shortCiteRegEx": "Kauermann", "year": 1996}, {"title": "Graphical models: Clarendon Press", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Yalmip: A toolbox for modeling and optimization in matlab", "author": ["J. Lofberg"], "venue": "In IEEE international symposium on Computer Aided Control Systems Design (CACSD),", "citeRegEx": "Lofberg,? \\Q2004\\E", "shortCiteRegEx": "Lofberg", "year": 2004}, {"title": "WalkSums and Belief Propagation in Gaussian Graphical Models", "author": ["D.M. Malioutov", "J.K. Johnson", "A.S. Willsky"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Malioutov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Malioutov et al\\.", "year": 2006}, {"title": "High-dimensional covariance estimation by minimizing `1-penalized log-determinant divergence", "author": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Sdpt3 - a matlab software package for semidefinite programming", "author": ["K.C. Toh", "M.J. Todd", "R.H. Tutuncu"], "venue": "Optimization Methods and Software,", "citeRegEx": "Toh et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Toh et al\\.", "year": 1999}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Finding the sample covariance matrix based on observed data is straightforward and widely used (Anderson, 1984).", "startOffset": 95, "endOffset": 111}, {"referenceID": 4, "context": "A natural mechanism to achieve this is to impose a sparsity constraint on the covariance matrix, which implies that the variables under consideration satisfy marginal independence, corresponding to the zero pattern of the covariance matrix (Kauermann, 1996) (and we refer to such models as independence models).", "startOffset": 240, "endOffset": 257}, {"referenceID": 5, "context": "widespread acceptance in recent years (Lauritzen, 1996).", "startOffset": 38, "endOffset": 55}, {"referenceID": 8, "context": "The set of sufficient conditions for successful recovery are based on the so-called notion of mutual incoherence, which controls the dependence between different sets of variables, See (Ravikumar et al., 2011).", "startOffset": 185, "endOffset": 209}, {"referenceID": 8, "context": "Our consistency proofs borrow ideas from (Ravikumar et al., 2011), and at the same time, require new ideas to carefully control the errors in the two domains, viz.", "startOffset": 41, "endOffset": 65}, {"referenceID": 3, "context": "The idea that a combination of Markov and independence models can provide good model-fitting is not by itself new, see (Choi et al., 2010).", "startOffset": 119, "endOffset": 138}, {"referenceID": 7, "context": "This is because the Markov components of the estimated models tend to be more walk summable (Malioutov et al., 2006), since some of the correlations can be \u201ctransferred\u201d to the residual matrix.", "startOffset": 92, "endOffset": 116}, {"referenceID": 5, "context": "The local and global Markov properties are equivalent for nondegenerate Gaussian distributions (Lauritzen, 1996).", "startOffset": 95, "endOffset": 112}, {"referenceID": 2, "context": "Under nondegeneracy conditions, it is also in the minimal form, and as such is identifiable (Brown, 1986).", "startOffset": 92, "endOffset": 105}, {"referenceID": 2, "context": "Based on the results for exponential families (Brown, 1986), \u0393(i,j),(k,l) = Cov{XiXj , XkXl}, and hence it can be interpreted as an edge-based alternative to the usual covariance matrix \u03a3M .", "startOffset": 46, "endOffset": 59}, {"referenceID": 8, "context": "Remark 2 (Comparison with sparse graphical model selection): The high dimensional covariance estimation problem which is investigated in (Ravikumar et al., 2011) involves similar mutual incoherence conditions and gives similar consistency results.", "startOffset": 137, "endOffset": 161}, {"referenceID": 8, "context": "Regarding the final result, sample complexity and convergence rate of estimated models are exactly the same as results in (Ravikumar et al., 2011) with only some minor differences in coefficients.", "startOffset": 122, "endOffset": 146}, {"referenceID": 6, "context": "The optimization programs are implemented by YALMIP (Lofberg, 2004) and SDPT3 (Toh et al.", "startOffset": 52, "endOffset": 67}, {"referenceID": 9, "context": "The optimization programs are implemented by YALMIP (Lofberg, 2004) and SDPT3 (Toh et al., 1999) packages for MATLAB.", "startOffset": 78, "endOffset": 96}, {"referenceID": 7, "context": "It is shown by Malioutov et al. (2006) that if a model is walk-summable then the mean estimates under LBP converge and are accurate.", "startOffset": 15, "endOffset": 39}], "year": 2012, "abstractText": "In this paper, we present a novel framework incorporating a combination of sparse models in different domains. We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse Gaussian independence model (with a sparse covariance matrix). We provide efficient methods for decomposition of the data into two domains, viz., Markov and independence domains. We characterize a set of sufficient conditions for identifiability and model consistency. Our decomposition method is based on a simple modification of the popular `1-penalized maximumlikelihood estimator (`1-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples n scales as n = \u03a9(d log p), where p is the number of variables and d is the maximum node degree in the Markov model. Our conditions for recovery are comparable to those of `1-MLE for consistent estimation of a sparse Markov model, and thus, we guarantee successful high-dimensional estimation of a richer class of models under comparable conditions. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}