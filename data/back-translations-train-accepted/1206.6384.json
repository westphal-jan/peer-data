{"id": "1206.6384", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization", "abstract": "We describe novel subgradient methods for a broad class of matrix optimization problems involving nuclear norm regularization. Unlike existing approaches, our method executes very cheap iterations by combining low-rank stochastic subgradients with efficient incremental SVD updates, made possible by highly optimized and parallelizable dense linear algebra operations on small matrices. Our practical algorithms always maintain a low-rank factorization of iterates that can be conveniently held in memory and efficiently multiplied to generate predictions in matrix completion settings. Empirical comparisons confirm that our approach is highly competitive with several recently proposed state-of-the-art solvers for such problems.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (374kb)", "http://arxiv.org/abs/1206.6384v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["haim avron", "satyen kale", "shiva prasad kasiviswanathan", "vikas sindhwani"], "accepted": true, "id": "1206.6384"}, "pdf": {"name": "1206.6384.pdf", "metadata": {"source": "META", "title": "Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization", "authors": ["Haim Avron", "Satyen Kale", "Shiva Prasad Kasiviswanathan"], "emails": ["haimav@us.ibm.com", "sckale@us.ibm.com", "spkasivi@us.ibm.com", "vsindhw@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "It is about the question of whether it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way in which it is about a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a way in which it is about a way in which it is about a way and a way in which it is about a way and a way and a way and a way it is about a way and a way and a way and a way it is about a way and a way and a way it is about a way and a way it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a"}, {"heading": "1.1. Preliminaries", "text": "We use i: j to denote the set {i,., j}, and [n] = 1: n. Vectors are always column vectors and are denoted by bold letters. We use 0m \u00b7 n to denote a m \u00b7 n matrix of all zeros. Given R [m] and C [n], we denote by XR, C the submatrix of X, consisting of rows in R \u2212 n columns in C. For a matrix X, Rm \u00b7 n with m \u2265 n, \u03c3 (X) = (X), (X), \u03c3n (X) is the vector of the singular values of X \u2212 n, consisting of entries in non-increasing order. The core (trace) norm of a matrix x X is the sum of the singular values of X."}, {"heading": "2. Stochastic Subgradient Descent", "text": "s leave F (X) = f (X) + f (X) + f). Instead of (1) solving this problem, we solve this problem immediately (1). The reason why we use this sentence K is to ensure that our iterations are limited. This is ensured by projecting to K if we ever step outside of K. The projection to K is defined as follows: Definition 1 (projection operator for K). The reason why we use this sentence K is to ensure that our iterations are limited. (X) The projection to K if we ever step outside of K. The projection to K is defined as follows: Definition 1 (projection operator for K). Let us define K (P) = argminQ (K)."}, {"heading": "Then Y is a probing matrix.", "text": "Because of the linearity of expectation, we have for each matrix A E [AY >] = AE [Y >] = A. Therefore, in our algorithms we use G (F (X) Y > as an unbiased estimator of G (F (X)). Using an unbiased estimator instead of an exact subgradient (i.e. using SSGD instead of a subgradient) is advantageous only if there is a certain arithmetical advantage. Our exploratory technique has two potential advantages over using an exact subgradient. First, it is more efficient for each matrix A to calculate the product G (F (X)) than it is for G (F (X) to actually calculate an unbiased estimator."}, {"heading": "2.1. Basic SSGD Algorithm", "text": "Using the above ingredients, we will now describe a basic SSGD algorithm for the solution (2) > Y-rate (1) > Y-rate (1) = G (X (t)). We will use g (t) = G (F (t)))) Y > Y > Y > as an unbiased estimator of G (F (X (t))). The Basic SSGD algorithm describes the complete procedure. Basic SSGD is not efficient in terms of runtime and memory requirements. Subsections below yield more efficient variants. The Basic SSGD algorithm: f, \u03bb, T, step sizes \u03b7 (1), \u03b7 (T \u2212 1), and kInitialize X (0) \u00b7 n for t = 0 to T \u2212 1 doGenerate n probing matrix g (t)."}, {"heading": "2.2. Decreasing the Iteration Cost of", "text": "We have a compact SVD definition of X (t) = U (t), if the ranks of X (t) (t) and V (t) are small, we can do much better. Let r (t) the rankings of X (t) and X (t) the rankings of X (t) and Y (t) recognize that we have a compact SVD definition of X (t). We will now show how to create the SVD definition of X (t) without explicitly forming X (t + 1)."}, {"heading": "2.3. Enforcing Low-Rank Iterates", "text": "The updated algorithm of Lemma 2.4 could be used in Algorithm Basic SSGD to go from the SVD of X (t) to SVD of X (t + 1). (D) The discussion in the previous section shows that if the iterate (t) s in Algorithm Basic SSGD are low, then the iterations are fast. This suggests that the idea of explicitly truncating the least singular values of the iterates is to ensure that the iterations always remain low. In this section we formalize this idea.Let Mr = Mm \u00b7 nr denote the set of m-by-n matrices of the ranking at most r. We assume that rank (Xopt) the ranks r, i.e., Xopt Mr, for some parameters r. (X) Then minimizing F (X) via K Mr yields the same optimal solution."}, {"heading": "3. Application: Matrix Completion", "text": "In the low-rank matrix completion problem, we get a series of low-rank indices. Minimizing the rank is difficult, so a popular approach to solving the matrix completion problem is as follows. Let's be the projection of the matrix set onto the index set. That is, (Pump (X)) ij = Xij, if (i, j), and otherwise 0. Let Z be the matrix containing the known values at their correct position and 0 in all other positions. The problem to be solved is the minimum size X (X) \u2212 Z (X) \u2212 2F + \u03b2 (X). (3) For reasons that will emerge later, we have decided to solve the problem with two parameters (\u03b1 and \u03b2) instead of a single parameter. Other variants are possible, such as the use of \"1 loss instead of 2\" or \"loss.\""}, {"heading": "Heuristics for warm-starting the algorithm and", "text": "Our algorithm can start from any matrix with rank up to r, as long as we have a compact SVD of that matrix. The initial SVD is also useful for setting \u03b1 and \u03b2 in a scale-free way. The nuclear standard serves as regularizer, so we expect \u03b2-Xopt values to be a certain size smaller than \u03b1f (Xopt). We do not know the values of \"Xopt\" and \"f\" (Xopt), so we use the values of \"X\" (0) and \"f\" (0) instead. First, we set \"X-Z-2F,\" which makes the value of \"f\" and \"f\" (Xopt) between 0 and \"1.\" We then find \u03b2 such that \u03b2-X (0) and \"f\" are set."}, {"heading": "4. Experimental Results", "text": "The first dataset (MovieLens 10M, Partition-rb) has about 107 ratings from 69878 users on 10677 movies; the second dataset (Netflix) has about 108 ratings from 480189 users on 17770 movies; the ratings are on a holistic scale from 1 to 5; we have divided each dataset into training and test sets, as Jaggi & Sulovsky did (2010); we have processed the datasets as follows: For each row and column of the training matrix Z, we calculate the mean of the rating i, and we score the mean rating of the column j; we subtract the average rating of the column j."}, {"heading": "Setting the parameters and sensitivity to", "text": "In Figure 1, we simply examine the same parameters without additional experimental results. We have sensitivity to the value of the parameters R, \u03b4, and \u03bd. The best choice of the ranking (r) is 11. However, the increase of the ranking from 11 to 15 results in a significant increase of the RMSE value by 0.75%, and the decrease of the ranking to 7 causes only an increase of 0.38% in RMSE. The best choice of the ranking results in an increase of 0.48%. The best choice of the ranking is 0.009%, and the RMSE value increases more disadvantageously than underestimated. Setting the ranking to 0.1% in the RMSE value only leads to an increase of 0.48%. The best choice of the ranking of rankings is 0.009, and the RMSE value increases fairly smoothly away from this value. We set the values to 0.015, and 0.005 based on preliminary observations on moviegoers, we try to complete 10M without any adjustment."}, {"heading": "Anderson, Michael, Ballard, Grey, Demmel, James, and", "text": "Keutzer, Kurt. Communication-Avoiding QR Decomposition for GPUs. In IPDPS, 2011."}, {"heading": "Avron, Haim, Kale, Satyen, Kasiviswanathan, Shiva, and", "text": "Sindhwani, Vikas. Efficient and Practical Stochastic Gradient Descent for Nuclear Standard Regulation (full version) Technical Report, IBM T. J. Watson Research Center, Yorktown Heights, NY, 2012."}, {"heading": "Cai, J.F., Candes, E.J., and Zhen, Z. A Singular Value", "text": "Thresholding Algorithm for Matrix Completion. SIAM J. Optimization, 20 (4), 2010."}, {"heading": "Choi, J., Dongarra, J.J., Pozo, R., and Walker, D.W.", "text": "ScaLAPACK: a Scalable Linear Algebra Library for Distributed Memory Concurrent Computers. In Frontiers of Massively Parallel Computation, 1992. Clarkson, Kenneth L. Coresets, sparse greedy approximation and the Frank Wolfe algorithm. In SODA, pp. 922-931, 2008."}, {"heading": "Constantine, Paul G. and Gleich, David F. Tall and", "text": "thin QR factorizations in MapReduce architectures. In MapReduce, pp. 43-50, 2011."}, {"heading": "Demmel, James, Grigori, Laura, Hoemmen, Mark,", "text": "and Langou, Julien. Communication-Otimal Parallel and Sequential QR and LU Factorizations. Arxiv, abs / 0808.2664, 2008."}, {"heading": "Frank, Marguerite and Wolfe, Philip. An Algorithm", "text": "for square programming. Naval Research Logistics Quarterly, 3 (1-2): 95-110, 1956.Hazan, Elad. Sparse Approximate Solutions to Semidefined Programs. In LATIN, 2008.Hoemmen, Mark. Communication-Avoiding Krylov Subspace Methods. Dissertation, University of California, Berkeley, 2010."}, {"heading": "Jaggi, Martin and Sulovsky\u0301, Marek. A Simple Algorithm", "text": "for Nuclear Norm Regularized Problems. In ICML, pp. 471-478, 2010."}, {"heading": "Ji, S. and Ye, J. An Accelerated Gradient Method for Trace", "text": "Standards minimization. In ICML, 2009."}, {"heading": "Mazumder, Rahul, Hastie, Trevor, and Tibshirani, Robert.", "text": "Spectral regularization algorithms for learning large incomplete matrices. J. Mach. Learn. Res., 99: 2287- 2322, 2010."}, {"heading": "Natarajan, B K. Sparse Approximate Solutions to Linear", "text": "Systems. SICOMP, 24 (2): 227-234, 1995."}, {"heading": "Recht, Benjamin and Re\u0301, Christopher. Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion. Optimization Online, 2011.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Shalev-Shwartz, Shai, Gonen, Alon, and Shamir, Ohad.", "text": "Great convex minimization with a low-rank constraint. In ICML, pp. 329-336, 2011."}, {"heading": "Watson, G.A. Characterization of the subdifferential of", "text": "Some Matrix Standards. Linear Algebra and its Applications, 170 (0): 33 - 45, 1992."}], "references": [{"title": "Communication-Avoiding QR Decomposition for GPUs", "author": ["Anderson", "Michael", "Ballard", "Grey", "Demmel", "James", "Keutzer", "Kurt"], "venue": "In IPDPS,", "citeRegEx": "Anderson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2011}, {"title": "Efficient and practical stochastic subgradient descent for nuclear norm regularization (full version)", "author": ["Avron", "Haim", "Kale", "Satyen", "Kasiviswanathan", "Shiva", "Sindhwani", "Vikas"], "venue": "Technical report, IBM T. J. Watson Research", "citeRegEx": "Avron et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2012}, {"title": "A Singular Value Thresholding Algorithm for Matrix Completion", "author": ["J.F. Cai", "E.J. Candes", "Z. Zhen"], "venue": "SIAM J. Optimization,", "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "ScaLAPACK: a Scalable Linear Algebra Library for Distributed Memory Concurrent Computers", "author": ["J. Choi", "J.J. Dongarra", "R. Pozo", "D.W. Walker"], "venue": "In Frontiers of Massively Parallel Computation,", "citeRegEx": "Choi et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Choi et al\\.", "year": 1992}, {"title": "Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm", "author": ["Clarkson", "Kenneth L"], "venue": "In SODA, pp", "citeRegEx": "Clarkson and L.,? \\Q2008\\E", "shortCiteRegEx": "Clarkson and L.", "year": 2008}, {"title": "Tall and skinny QR factorizations in MapReduce architectures", "author": ["Constantine", "Paul G", "Gleich", "David F"], "venue": "In MapReduce,", "citeRegEx": "Constantine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Constantine et al\\.", "year": 2011}, {"title": "Communication-Otimal Parallel and Sequential QR and LU Factorizations", "author": ["Demmel", "James", "Grigori", "Laura", "Hoemmen", "Mark", "Langou", "Julien"], "venue": "Arxiv, abs/0808.2664,", "citeRegEx": "Demmel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Demmel et al\\.", "year": 2008}, {"title": "An Algorithm for Quadratic Programming", "author": ["Frank", "Marguerite", "Wolfe", "Philip"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Frank et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1956}, {"title": "Sparse Approximate Solutions to Semidefinite Programs", "author": ["Hazan", "Elad"], "venue": "In LATIN,", "citeRegEx": "Hazan and Elad.,? \\Q2008\\E", "shortCiteRegEx": "Hazan and Elad.", "year": 2008}, {"title": "Communication-Avoiding Krylov Subspace Methods", "author": ["Hoemmen", "Mark"], "venue": "PhD thesis, University of California, Berkeley,", "citeRegEx": "Hoemmen and Mark.,? \\Q2010\\E", "shortCiteRegEx": "Hoemmen and Mark.", "year": 2010}, {"title": "A Simple Algorithm for Nuclear Norm Regularized Problems", "author": ["Jaggi", "Martin", "Sulovsk\u00fd", "Marek"], "venue": "In ICML, pp", "citeRegEx": "Jaggi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2010}, {"title": "An Accelerated Gradient Method for Trace Norm Minimization", "author": ["S. Ji", "J. Ye"], "venue": "In ICML,", "citeRegEx": "Ji and Ye,? \\Q2009\\E", "shortCiteRegEx": "Ji and Ye", "year": 2009}, {"title": "Spectral Regularization Algorithms for Learning Large Incomplete Matrices", "author": ["Mazumder", "Rahul", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Sparse Approximate Solutions to Linear Systems", "author": ["Natarajan", "B K"], "venue": "SICOMP,", "citeRegEx": "Natarajan and K.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan and K.", "year": 1995}, {"title": "Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion", "author": ["Recht", "Benjamin", "R\u00e9", "Christopher"], "venue": "Optimization Online,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Large-Scale Convex Minimization with a Low-Rank Constraint", "author": ["Shalev-Shwartz", "Shai", "Gonen", "Alon", "Shamir", "Ohad"], "venue": "In ICML, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Characterization of the subdifferential of some matrix norms", "author": ["G.A. Watson"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Watson,? \\Q1992\\E", "shortCiteRegEx": "Watson", "year": 1992}], "referenceMentions": [{"referenceID": 2, "context": ", the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al.", "startOffset": 40, "endOffset": 58}, {"referenceID": 12, "context": ", 2010), Soft-Impute (Mazumder et al., 2010), accelerated Proximal Gradient approach (Ji & Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which requires repeated calls to an SVD solver.", "startOffset": 21, "endOffset": 44}, {"referenceID": 2, "context": ", the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al., 2010), accelerated Proximal Gradient approach (Ji & Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which requires repeated calls to an SVD solver. In particular, if the iterates need to pass through a region where the spectrum is dense, these techniques can potentially become prohibitively expensive. An alternative approach is proposed by Jaggi & Sulovsk\u00fd (2010) who map (1) to the problem of optimizing a convex function over the set of positive semi-definite matrices with unit trace, for which the Sparse-SDP solver of Hazan (2008) is invoked.", "startOffset": 41, "endOffset": 527}, {"referenceID": 2, "context": ", the Singular Value Thresholding (SVT) (Cai et al., 2010), Soft-Impute (Mazumder et al., 2010), accelerated Proximal Gradient approach (Ji & Ye, 2009) and related efforts, all involve applying a soft-thresholding operator on the singular values of an iterate, which requires repeated calls to an SVD solver. In particular, if the iterates need to pass through a region where the spectrum is dense, these techniques can potentially become prohibitively expensive. An alternative approach is proposed by Jaggi & Sulovsk\u00fd (2010) who map (1) to the problem of optimizing a convex function over the set of positive semi-definite matrices with unit trace, for which the Sparse-SDP solver of Hazan (2008) is invoked.", "startOffset": 41, "endOffset": 699}, {"referenceID": 12, "context": "\u25e6 We apply our method to matrix completion and show that it compares favorably to state-of-the-art techniques for solving (1) (Jaggi & Sulovsk\u00fd, 2010; Mazumder et al., 2010; Shalev-Shwartz et al., 2011).", "startOffset": 126, "endOffset": 202}, {"referenceID": 15, "context": "\u25e6 We apply our method to matrix completion and show that it compares favorably to state-of-the-art techniques for solving (1) (Jaggi & Sulovsk\u00fd, 2010; Mazumder et al., 2010; Shalev-Shwartz et al., 2011).", "startOffset": 126, "endOffset": 202}, {"referenceID": 16, "context": "It is well known that UrankV > rank is a subgradient of \u2016X\u2016\u2217 (Watson, 1992), so we find that a subgradient of F (X) = f(X) + \u03bb\u2016X\u2016\u2217 is", "startOffset": 61, "endOffset": 75}, {"referenceID": 1, "context": "(All missing proofs appear in the full version of the paper (Avron et al., 2012).", "startOffset": 60, "endOffset": 80}, {"referenceID": 1, "context": "For a pseudo-code description, see full version of the paper (Avron et al., 2012).", "startOffset": 61, "endOffset": 81}, {"referenceID": 3, "context": "By using a highly tuned package like ScaLAPACK (Choi et al., 1992) good speedups should be attainable with little effort.", "startOffset": 47, "endOffset": 66}, {"referenceID": 6, "context": "Recent research on QR factorization has shown how to implement it efficiently on communication-bound massive parallel machines (Demmel et al., 2008), MapReduce clusters (Constantine & Gleich, 2011), and GPUs (Anderson et al.", "startOffset": 127, "endOffset": 148}, {"referenceID": 0, "context": ", 2008), MapReduce clusters (Constantine & Gleich, 2011), and GPUs (Anderson et al., 2011).", "startOffset": 67, "endOffset": 90}, {"referenceID": 1, "context": "For a pseudo-code description, see full version of the paper (Avron et al., 2012).", "startOffset": 61, "endOffset": 81}, {"referenceID": 12, "context": "Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)).", "startOffset": 59, "endOffset": 82}, {"referenceID": 12, "context": "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as \u201cSoft-Impute\u201d.", "startOffset": 94, "endOffset": 117}, {"referenceID": 12, "context": "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as \u201cSoft-Impute\u201d. Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (\u03bb) where the results on a value are used as a warmstart for the next value. The results where examined (RMSE measured) at the different points along the regularization path, and the running time at any point \u03bbi is the sum of time needed to run the algorithm at \u03bbi and the running time of \u03bbi\u22121 (because of the warm start). The path used for MovieLens was (\u03bb0/2, \u03bb0/4, \u03bb0/8, \u03bb0/10), and the path used for Netflix was (\u03bb0/250, \u03bb0/300), where \u03bb0 is the spectral norm of the input matrix Z. Note that both Jaggi & Sulovsk\u00fd (2010) and Mazumder et al.", "startOffset": 94, "endOffset": 896}, {"referenceID": 12, "context": "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as \u201cSoft-Impute\u201d. Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (\u03bb) where the results on a value are used as a warmstart for the next value. The results where examined (RMSE measured) at the different points along the regularization path, and the running time at any point \u03bbi is the sum of time needed to run the algorithm at \u03bbi and the running time of \u03bbi\u22121 (because of the warm start). The path used for MovieLens was (\u03bb0/2, \u03bb0/4, \u03bb0/8, \u03bb0/10), and the path used for Netflix was (\u03bb0/250, \u03bb0/300), where \u03bb0 is the spectral norm of the input matrix Z. Note that both Jaggi & Sulovsk\u00fd (2010) and Mazumder et al. (2010) apply additional heuristics and/or post-processing to their basic algorithm.", "startOffset": 94, "endOffset": 923}, {"referenceID": 12, "context": "The second algorithm that we compared to is the soft singular value thresholding algorithm of Mazumder et al. (2010). We refer to this algorithm as \u201cSoft-Impute\u201d. Here, we used the MATLAB code provided by the authors (see (Mazumder et al., 2010)). We used the path-following strategy suggested by the code, i.e., we set a path of values for the regularization parameter (\u03bb) where the results on a value are used as a warmstart for the next value. The results where examined (RMSE measured) at the different points along the regularization path, and the running time at any point \u03bbi is the sum of time needed to run the algorithm at \u03bbi and the running time of \u03bbi\u22121 (because of the warm start). The path used for MovieLens was (\u03bb0/2, \u03bb0/4, \u03bb0/8, \u03bb0/10), and the path used for Netflix was (\u03bb0/250, \u03bb0/300), where \u03bb0 is the spectral norm of the input matrix Z. Note that both Jaggi & Sulovsk\u00fd (2010) and Mazumder et al. (2010) apply additional heuristics and/or post-processing to their basic algorithm. Additionally, Mazumder et al. (2010) measure only time spent on SVD computations.", "startOffset": 94, "endOffset": 1037}, {"referenceID": 12, "context": "the discrepancy between the results we show here and the results reported in (Jaggi & Sulovsk\u00fd, 2010) and (Mazumder et al., 2010).", "startOffset": 106, "endOffset": 129}, {"referenceID": 12, "context": "Mazumder et al. (2010) report 0.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "The GECO algorithm proposed in Shalev-Shwartz et al. (2011) is not a solver for nuclear norm regularized problems, but uses a greedy method, with optimality guarantees, to optimize f under explicit low-rank constraints.", "startOffset": 31, "endOffset": 60}], "year": 2012, "abstractText": "We describe novel subgradient methods for a broad class of matrix optimization problems involving nuclear norm regularization. Unlike existing approaches, our method executes very cheap iterations by combining low-rank stochastic subgradients with efficient incremental SVD updates, made possible by highly optimized and parallelizable dense linear algebra operations on small matrices. Our practical algorithms always maintain a low-rank factorization of iterates that can be conveniently held in memory and efficiently multiplied to generate predictions in matrix completion settings. Empirical comparisons confirm that our approach is highly competitive with several recently proposed state-of-the-art solvers for such problems.", "creator": "LaTeX with hyperref package"}}}