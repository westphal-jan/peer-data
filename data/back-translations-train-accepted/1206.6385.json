{"id": "1206.6385", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Improved Estimation in Time Varying Models", "abstract": "Locally adapted parameterizations of a model (such as locally weighted regression) are expressive but often suffer from high variance. We describe an approach for reducing the variance, based on the idea of estimating simultaneously a transformed space for the model, as well as locally adapted parameterizations in this new space. We present a new problem formulation that captures this idea and illustrate it in the important context of time varying models. We develop an algorithm for learning a set of bases for approximating a time varying sparse network; each learned basis constitutes an archetypal sparse network structure. We also provide an extension for learning task-driven bases. We present empirical results on synthetic data sets, as well as on a BCI EEG classification task.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (631kb)", "http://arxiv.org/abs/1206.6385v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ME stat.ML", "authors": ["doina precup", "philip bachman"], "accepted": true, "id": "1206.6385"}, "pdf": {"name": "1206.6385.pdf", "metadata": {"source": "META", "title": "Improved Estimation in Time Varying Models", "authors": ["Philip Bachman", "Doina Precup"], "emails": ["PHIL.BACHMAN@GMAIL.COM", "DPRECUP@CS.MCGILL.CA"], "sections": [{"heading": "1. Introduction", "text": "Locally adjusted parameters can produce flexible representations of relatively rigid components; locally weighted regression serves as a canonical example of this approach. Such models reduce bias but increase variance because they have a reduced effective sample size for each estimate. We address this problem with a transformed (constrained) space in which local parameters are found. A common approach to improving effectiveness in machine learning is to transform the data into an alternative representation in front of the model in a way that amplifies useful information, while the mitigation of this approach is exemplary."}, {"heading": "2. A General Problem Formulation", "text": "The problem examined in this paper arises as a generalization of the following optimization: B \u043c = \u03b2 \u03b2 = \u03b2 B [\"f, X, B) (1), where the loss measures\" the goodness \"of the adaptation of model f to the data (x1, y1),..., (xm, ym), givena set of parameterizations B = {\u03b21,..., \u03b2m \u00b2), and an optimal set of parameterizations B is sought.The idea of using multiple model parameters is not often explored in machine learning. As motivation for this perspective, we begin by expressing the standard linear regression in the form of (1), in which case f measures the residuals produced by a parameter vector: f (x, y), \u03b2, y, \u03b2 \u2212 yFor a set of parameter vectors \u03b2i, is proportional to the log liquids by which they are normally distributed."}, {"heading": "3. Illustrations of the Problem Formulation", "text": "Let us consider as a first example the execution of a locally weighted regression analogous to that in (\u03b2 \u03b22), but with the local parameterizations of f, which are limited to a linear subspace. Letg (\u03b2 3) = A\u03b2 \u03b2 w, where A is the matrix of parameters of g. We can (5) describe (arg min A min B \u00b2 m \u00b2 p = 1 m \u00b2 j = 1 k (\u03b2 \u00b2 xi, xj) (x T j \u00b2 w i \u2212 yj) 2 (6), in which we now divide each \u03b2 \u00b2 i into a localization subcomponent \u03b2 \u00b2 xi and a coefficient subcomponent \u03b2 \u00b2 w i. If xTj A\u03b2 \u00b2 w i is considered as a reduction of xj in the subspace spanned by the columns of A, followed by a linear regression in this subspace, the objective in (6) is closely linked to the methods of linear dimension reduction for regression based on non-sparing."}, {"heading": "4. Learning Compact Representations of", "text": "Time Varying Network StructureIn this section, we use our new problem formulation to derive a novel algorithm for estimating time-varying network structures, using a time-dependent, sparse combination of learned base structures. By using an analogy between our algorithm and sparse encoding (Olshausen & Field, 1996), we then extend our algorithm to learning task-driven base structures, guided by the work in (Mairal et al., 2011). We begin by reviewing existing work on estimating network structures before describing the new algorithms."}, {"heading": "4.1. Sparse Network Structure Estimation", "text": "In recent years, much effort has gone into developing effective methods for estimating sparsely structured Gaussian models. (GGM) A Gaussian Graphic Model (GGM) explains a series of m-dimensional observations X = 1Note that we describe the covariances \u03b2-x1,..., xm}, xi-Rn with a series of n wells (each corresponding to a dimension) and a series of edges, each describing the strength of the relationship between their occurring wells. A GGM implies a covariance and is equivalent to modelling X with a normal distribution N (~ 0, \u03a3). Typically, observations are standardized before estimating a GGM to have means 0.Many existing methods that focus on estimating its structure, i.e. the pattern of zero / not zero edges. These methods typically work with the precision matrix (i.e."}, {"heading": "4.2. Estimating Network Structures as Combinations of Basis Structures", "text": "We reformulate the optimization in a (12) manner similar to the way in which we generalize the locally weighted regression from (3) to the form (6). (...) Our proposed estimation method each revolves around the following optimization: (...) The optimization of (...) is a regulatory term, and (...) controls the strength of regulation. (...) We estimate the strength of regulation. (...) In view of this, we estimate that the optimization of (13) is the optimization of (...), and (...) controls the strength of regulation. (...) We estimate that the optimization of (...) is the optimization of (...) on the basis of (...) we optimize (...) on the basis of (...) we optimize the optimization of (...)"}, {"heading": "4.3. Supervised Basis Structure Learning", "text": "We can adapt the work of (Mairal et al., 2011) to enable our algorithm to learn task-driven sets of base network structures. We consider the task of minimizing differentiable supervised loss functions, which can be described as follows: Ls (X, B, w) = T (n) = 1 \"s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) n (n) s (n) s (n) n (n) s (n) s (n (n) s (n) n (n (n) s (n) n (n (n) s (n) s (n (n) n (n) s (n (n) s (n (n) n (n) n (n (n) s (n (n) n (n) n (n (n) n (n) s (n (n) n (n) n (n (n) n (n) n (n (n) n (n) n (n) n (n (n) n) n (n (n) n (n) n (n (n) n) n (n) n) n (n (n) n (n (n) n (n (n) n) n (n) n (n) n (n (n (n) n) n (n) n (n) n (n (n (n) n) n) n (n (n (n) n (n (n) n) n (n) n (n) n (n (n) n (n) n) n (n () n () n () n (n) n (n () n () n () n (n (n () n () n) n (n (n) n) n (n () n () n (n (n) n (n (n ("}, {"heading": "5. Synthetic Network Analysis", "text": "This section presents tests based on simulated observation sequences showing the ability of our algorithm to recover recurring elements of time-varying network structures. We generated each observation sequence by capturing the observation sequence at a given time from a normal distribution of N (0, \u0442t) in which we were a convex combination of four covariance matrix bases (an example set of trajectories is shown in Fig. 1). We generated each of these observation sequences by symmetrically removing two-thirds of the off-diagonal entries (the ones with the smallest order of magnitude) from a random covariance matrix with eigenvalues universally distributed in (0, 1), and rescaled diagonal entries to ensure a positive definition."}, {"heading": "6. BCI EEG Analysis", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7. Conclusion", "text": "With the help of this formulation, we developed a novel algorithm for learning representations of sparse structures in time-varying networks with recurring structural motifs. We used tests with synthetic data to show that our algorithm behaves as desired under suitable conditions, while an application to BCI-EEG data showed the potential value of our algorithm under real conditions. We plan to apply our approach to other types of tasks, such as the analysis of time-varying weather and traffic patterns, in addition to investigating alternative methods for parameter transformation that go beyond the linear transformations considered in this paper. Our algorithm is easily extendable to estimate time-varying structures in dynamic Bayesian networks."}], "references": [{"title": "Recovering time-varying networks of dependencies in social and biological studies", "author": ["A. Ahmed", "E.P. Xing"], "venue": null, "citeRegEx": "Ahmed and Xing,? \\Q2009\\E", "shortCiteRegEx": "Ahmed and Xing", "year": 2009}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["B. Boots", "G. Gordon"], "venue": "In AAAI,", "citeRegEx": "Boots and Gordon,? \\Q2011\\E", "shortCiteRegEx": "Boots and Gordon", "year": 2011}, {"title": "Likelihood-based sufficient dimension reduction", "author": ["R.D. Cook", "L. Forzani"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cook and Forzani,? \\Q2009\\E", "shortCiteRegEx": "Cook and Forzani", "year": 2009}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Technical report, Stanford University,", "citeRegEx": "Friedman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2009}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": null, "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural Networks,", "citeRegEx": "Hyv\u00e4rinen and Oja,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Oja", "year": 2000}, {"title": "On time varying undirected graphs", "author": ["M. Kolar", "E.P. Xing"], "venue": "In AISTATS,", "citeRegEx": "Kolar and Xing,? \\Q2011\\E", "shortCiteRegEx": "Kolar and Xing", "year": 2011}, {"title": "Sparsistent Learning of Varying-coefficient Models with Structural Changes", "author": ["M. Kolar", "L. Song", "E.P. Xing"], "venue": "In NIPS 22,", "citeRegEx": "Kolar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolar et al\\.", "year": 2009}, {"title": "Disclda: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "In NIPS", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2008}, {"title": "Graphical Models", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Regularizing common spatial patterns to improve bci designs: Unified theory and new algorithms", "author": ["F. Lotte", "C. Guan"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "Lotte and Guan,? \\Q2011\\E", "shortCiteRegEx": "Lotte and Guan", "year": 2011}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Mairal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2011}, {"title": "High-dimensional graphs and variable selection with the lasso", "author": ["N Meinshausen", "P. B\u00fchlmann"], "venue": "Annals of Statistics,", "citeRegEx": "Meinshausen and B\u00fchlmann,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and B\u00fchlmann", "year": 2006}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "Olshausen and Field,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1996}, {"title": "Supervised dimensionality reduction using mixture models", "author": ["Sajama", "A. Orlitsky"], "venue": "In ICML,", "citeRegEx": "Sajama and Orlitsky,? \\Q2005\\E", "shortCiteRegEx": "Sajama and Orlitsky", "year": 2005}, {"title": "Exploring regression structure using nonparametric functional estimation", "author": ["A.M. Samarov"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Samarov,? \\Q1993\\E", "shortCiteRegEx": "Samarov", "year": 1993}, {"title": "Characterization of four-class motor-imagery eeg data for the bcicompetition", "author": ["A. Schl\u00f6gl", "F. Lee", "H. Bischof", "G. Pfurtscheller"], "venue": "Journal of Neural Engineering,", "citeRegEx": "Schl\u00f6gl et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schl\u00f6gl et al\\.", "year": 2005}, {"title": "Reduced-rank hidden markov models", "author": ["S. Siddiqi", "B. Boots", "G. Gordon"], "venue": "In AISTATS,", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "Keller: estimating timevarying interactions between genes", "author": ["L. Song", "M. Kolar", "E.P. Xing"], "venue": "Bioinformatics, 25:i128\u2013", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Time-varying dynamic bayesian networks", "author": ["L. Song", "M. Kolar", "E.P. Xing"], "venue": "In NIPS 22,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, 290:2319\u20132323,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statiscal Society. Series B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Highdimensional graphical model selection using l1-regularized logistic regression", "author": ["M.J. Wainwright", "P. Ravikumar", "J. Lafferty"], "venue": "In NIPS", "citeRegEx": "Wainwright et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2007}, {"title": "An adaptive estimation of dimension reduction space", "author": ["Y. Xia", "H. Tong", "W.K. Li", "Zhu", "L-X"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Xia et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2002}, {"title": "Time varying undirected graphs", "author": ["S. Zhou", "J. Lafferty", "L. Wasserman"], "venue": "Machine Learning,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "(Tenenbaum et al., 2000), and dimension reduction for regression (Fukumizu et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": ", 2000), and dimension reduction for regression (Fukumizu et al., 2004; Cook & Forzani, 2009).", "startOffset": 48, "endOffset": 93}, {"referenceID": 9, "context": "Examples of the first approach include DiscLDA (Lacoste-Julien et al., 2008) and supervised dimensionality reduction using Bayesian mixture models (Sajama & Orlitsky, 2005), which seek useful linear reductions of the parameters of a generative model.", "startOffset": 47, "endOffset": 76}, {"referenceID": 18, "context": "The second approach includes the application of spectral methods to learning transformed representations of HMMs (Siddiqi et al., 2010) and PSRs (Boots & Gordon, 2011).", "startOffset": 113, "endOffset": 135}, {"referenceID": 16, "context": "If one views xj A\u03b2\u0302 w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002).", "startOffset": 284, "endOffset": 317}, {"referenceID": 24, "context": "If one views xj A\u03b2\u0302 w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002).", "startOffset": 284, "endOffset": 317}, {"referenceID": 12, "context": "Through an analogy between our algorithm and sparse coding (Olshausen & Field, 1996), we then extend our algorithm to learning of task-driven basis structures, guided by the work in (Mairal et al., 2011).", "startOffset": 182, "endOffset": 203}, {"referenceID": 10, "context": "The use of self-regression for network structure estimation is based on the following results (Lauritzen, 1996):", "startOffset": 94, "endOffset": 111}, {"referenceID": 25, "context": "Value estimation methods estimate each entry in \u03a3\u22121 (Zhou et al., 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 3, "context": ", 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al., 2008; Song et al., 2009b; Kolar & Xing, 2011)).", "startOffset": 84, "endOffset": 147}, {"referenceID": 22, "context": "The sparsity assumption can be incorporated into the self-regression process by using sparsifying regression techniques, such as the well known Lasso (Tibshirani, 1996).", "startOffset": 150, "endOffset": 168}, {"referenceID": 23, "context": "\u03a3\u22121 under suitable conditions (Meinshausen & B\u00fchlmann, 2006; Wainwright et al., 2007; Kolar & Xing, 2011).", "startOffset": 30, "endOffset": 105}, {"referenceID": 8, "context": "A recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011).", "startOffset": 128, "endOffset": 229}, {"referenceID": 25, "context": "A recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011).", "startOffset": 128, "endOffset": 229}, {"referenceID": 8, "context": "This second assumption distinguishes KELLER from methods such as (Ahmed & Xing, 2009) and (Kolar et al., 2009), which assume abrupt changes in the network structure.", "startOffset": 90, "endOffset": 110}, {"referenceID": 12, "context": "We use this form to meet the assumptions required for the task-driven dictionary learning described in (Mairal et al., 2011), used in the further extension of our algorithm.", "startOffset": 103, "endOffset": 124}, {"referenceID": 4, "context": "When optimizing B\u0302 with \u00c2 held fixed, we compute the optimal \u03b2\u0302t for each t via elastic-net regressions solved with the publicly available, highly optimized glmnet package (Friedman et al., 2009).", "startOffset": 172, "endOffset": 195}, {"referenceID": 12, "context": "We can adapt the work of (Mairal et al., 2011) to enable our algorithm to learn task-driven sets of basis network structures.", "startOffset": 25, "endOffset": 46}, {"referenceID": 12, "context": "However, Mairal et al. (2011) show that if elastic-net regularization is used to produce each \u03b2\u0302t from the (xt, Dt), the gradient of the perinstance supervised loss `s w.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "BCI) motor imagery experiment available as task 3a from BCI competition III (Schl\u00f6gl et al., 2005; Blankertz et al., 2006).", "startOffset": 76, "endOffset": 122}], "year": 2012, "abstractText": "Locally adapted parameterizations of a model (such as locally weighted regression) are expressive but often suffer from high variance. We describe an approach for reducing this variance, based on the idea of estimating simultaneously a transformed space for the model and locally adapted parameterizations expressed in the new space. We present a new problem formulation that captures this idea and illustrate it in the important context of time varying models. We develop an algorithm for learning a set of bases for approximating a time varying sparse network; each learned basis constitutes an archetypal sparse network structure. We also provide an extension for learning task-specific bases. We present empirical results on synthetic data sets, as well as on a BCI EEG classification task.", "creator": "LaTeX with hyperref package"}}}