{"id": "1206.6388", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Canonical Trends: Detecting Trend Setters in Web Data", "abstract": "Much information available on the web is copied, reused or rephrased. The phenomenon that multiple web sources pick up certain information is often called trend. A central problem in the context of web data mining is to detect those web sources that are first to publish information which will give rise to a trend. We present a simple and efficient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to a trend before others. We validate our approach on real data collected from influential technology news feeds.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (633kb)", "http://arxiv.org/abs/1206.6388v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.SI stat.ML", "authors": ["felix bie\u00dfmann", "jens-michalis papaioannou", "mikio l braun", "andreas harth"], "accepted": true, "id": "1206.6388"}, "pdf": {"name": "1206.6388.pdf", "metadata": {"source": "CRF", "title": "Canonical Trends: Detecting Trend Setters in Web Data", "authors": ["Felix Bie\u00dfmann", "Jens-Michalis Papaioannou", "Mikio Braun", "Andreas Harth"], "emails": ["felix.biessmann@tu-berlin.de", "jensmicha@googlemail.com", "mikio.braun@tu-berlin.de", "harth@kit.edu"], "sections": [{"heading": "1. Introduction", "text": "Several commercial surveys are based on the temporal variation of web sources for data collection. The news domain is a prime example of an industry in which time plays a role. Sources that break a story gain reputation and economic value. We therefore look at the problem of identifying forward-looking news sources based on temporal correlations found in data. Our definition of a trend-setter is simple. For example, if a single1See http: / / www.google.com / trends / correlate / Appearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Author (s) / owner (s).web source publishes content that later dominates the content of a pool of other websites, we consider this source to be a trend-setter; this approach is similar to the causative power of websites based on analytics such as Sindano and hhi."}, {"heading": "2. Related Work", "text": "Subsequently, we will consider whether we will be able to find a solution that will enable us to find a solution that meets the needs of the people."}, {"heading": "3. Canonical Trends", "text": "For our approach, we extract from each web source f 2 {1, 2,.., F} in our collection of F-Web sources the corresponding features xf (t) 2 RW at times t = {0, 1,.., T}. For the sake of simplicity, we assume here regularly sampled times. In our application example, we will extract the feature-of-words features, see Section 6.2.1, but our approach is easily applicable to other feature representations such as n-grams or collections of hyperlinks. After the feature extraction, we store the multivariate feature time series in a sparse matrixXf = [x f (t = 1),... x f (t = T) 2 RW-T. (1) We are not interested in the dynamics of a single web source, but rather in the temporal variation of manynodes in the web graph. The common time series of all web sources can simply be represented as the Yf-Xf functions."}, {"heading": "4. Canonical Trend Prediction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5. Canonical Trend Algorithm", "text": "Informally, our approach consists of three steps: 1. Extracting the feature matrix X f for each feed2. Temporarily embedding individual news feeds X f3. Kernel CCA between X f and all other feeds Y fIn the following, we describe steps two and three in detail. Data acquisition and feature extraction are described in Section 6.2.1.3. Or invariant nonlinear transformations in the case of kernel CCA."}, {"heading": "5.1. Temporal Embedding", "text": "The time embedding takes place by creating a new representation X-f for each feed f, into which we insert copies of the data in Xf, which are reset with a time delay X-f = 264 X-f, = N... Xf, = 1375 2 RWN T. (7)"}, {"heading": "5.2. Kernel CCA", "text": "eiD rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "5.3. Model evaluation for time series", "text": "In order to obtain meaningful predictive accuracy, we apply a 10-fold cross-validation: We divide the available data into training and test data, estimate it, and calculate the predictive accuracy in Equivalent 5 on test data. When cross-validating time series data, special care must be taken. In contrast to standard classification settings, where you can simply randomly select a certain subset of data, the time dependencies in time series data overlap with N samples. Therefore, we discarded the first N samples from the training block adjacent to the test data block, ensuring that no data point we tested was used to train the KCCA model. We estimated the optimal time delay and regulation parameters using the 10-fold cross-validation (nested within the test data block), 4 hours, 2 hours, 2 hours, and 5 hours."}, {"heading": "5.4. Comparison with other approaches", "text": "The relevant contribution of the CT algorithm is that it maximizes the co-variation of individual web sources X and other web sources Y f. An alternative approach to topic recognition is latent semantic analysis (LSA) (Deerwester et al., 1990), factorizing only a single matrix of BoW characteristics. In LSA, the strongest topic area vy, f 2 RW, is actually the subspace in the BoW space, here the row space of Yf, which captures the most variance eargmax characteristics, f (v > y, f Y > f v y, f), s.t. v > y, f = 1. (11) The strongest topic v x, f in the single feed-BoW space Xf is analogously determined by the temporal relationship between LSA and CT."}, {"heading": "6. Results", "text": "First, we illustrate our approach using a toy data set. Then, we present some results on real-world data obtained from technology news feeds."}, {"heading": "6.1. Canonical Trends: A toy data example", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6.2. Trend setter detection in News feeds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1. Data Collection", "text": "We collected data from 96 news feeds5 in 2011. Bag-of-Word (BoW) features were extracted using standard tools for processing natural language6. After the removal of stopwords and the pedigree of our BoW dictionary, the feature time series contained W '105 words. Time series of each word were then stored in sparse matrices Xf 2 RW'T, with f = {1,.., F = 96} news feed and t = {1,..., T} denoting time in hours. Timestamp 5http: / / beta.wunderfacts.com / 6http: / / www.nltk.org / of all news sources set to CET. For the sake of clarity of the results presented here, we focus on the month of October 2011."}, {"heading": "6.2.2. Canonical Trends in News feeds", "text": "In practice, however, the canonical trends are very similar. Figure 2, at the top, shows the mean and 25th / 75th percentiles of all canonical trends in October 2011. Percentiles are very close to the median trend, indicating a great similarity of canonical trends. Reports of Steve Jobs \"death mark a pronounced peak in the first week, which is reflected in all canonical trends. Also, note that the trends clearly reflect weekly publication activity on news feeds, five peaks per week and a low point reflecting the weekend. Our results show that the temporal dynamics in the canonical subspace can be easily interpreted and authentically reflect the effects of relevant information cascades in large web graphics."}, {"heading": "6.2.3. Canonical Trend Prediction", "text": "We examined how well we can predict trends in a pool of web sources from a single web source. Table 1 shows predictive accuracy as a canonical correlation (see eq. 5) for the ten best predictors, i.e. the trendsetter news feeds, summarized as 25th / 50th / 75th percentiles of cross-validation folds. Using the information published at t's, the content of all other news sites is compared to all other feeds in more than 50% of the cases tested, the listed news feeds could predict the general trend in time t with high accuracy. For example, the website http: / / businessinsider.com will track the content of all other news sites in the database of more than 50% of the cases tested with a correlation coe cient of 0.8."}, {"heading": "7. Conclusion and Outlook", "text": "By applying the kernel trick, we can authentically exploit the complete multivariate structure of temporal dependencies in the canonical subspace of web graph characteristics such as the BoW representation. Both the trends detected and the characteristics learned by the algorithm reflect the true effects of information cascades in time-evolving graphs. Future work will include empirical evaluations to investigate the temporal correlation not only by BoW characteristics, but also by auxiliary data, such as the frequency of retweets along the lines of (Lerman & Hogg, 2010), which predict the popularity of content based on early user interest. Another useful feature representation could be called entities along the lines of (Gabrilovich et al, 2004)."}, {"heading": "Acknowledgements", "text": "We would like to thank Klaus-Robert Mueller and Manuel GomezRodriguez for their helpful discussions. M.B. and F.B. would like to thank them for their support of this project through the BMBF project ALICE \"Autonomous Learning in Complex Environments\" (01IB10003B)."}], "references": [{"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["A Aizerman", "EM Braverman", "L. Rozonoer"], "venue": "Automation and Remote Control,", "citeRegEx": "Aizerman et al\\.,? \\Q1964\\E", "shortCiteRegEx": "Aizerman et al\\.", "year": 1964}, {"title": "Asymptotic theory for canonical correlation analysis", "author": ["Anderson", "TW"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Anderson and TW.,? \\Q1999\\E", "shortCiteRegEx": "Anderson and TW.", "year": 1999}, {"title": "tkCCA and its application in multimodal neuronal data analysis", "author": ["F Bie\u00dfmann", "FC Meinecke", "A Gretton", "A Rauch", "G Rainer", "NK Logothetis", "M\u00fcller", "KR"], "venue": "Machine Learning Journal,", "citeRegEx": "Bie\u00dfmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bie\u00dfmann et al\\.", "year": 2010}, {"title": "Indexing by Latent Semantic Analysis", "author": ["SC Deerwester", "ST Dumais", "TK Landauer", "GW Furnas", "Harshman", "RA"], "venue": "Journal of the American Society of Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Statistical consistency of kernel CCA", "author": ["K Fukumizu", "FR Bach", "A. Gretton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2007}, {"title": "ICA using kernel canonical correlation analysis", "author": ["C Fyfe", "P. Lai"], "venue": "In Proc. Int. Workshop on Independent Component Analysis,", "citeRegEx": "Fyfe and Lai,? \\Q2000\\E", "shortCiteRegEx": "Fyfe and Lai", "year": 2000}, {"title": "Newsjunkie: providing personalized newsfeeds via analysis of information novelty", "author": ["E Gabrilovich", "S Dumais", "E. Horvitz"], "venue": "In Proceedings of the WWW,", "citeRegEx": "Gabrilovich et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2004}, {"title": "Uncovering the temporal dynamics of di\u21b5usion networks", "author": ["M Gomez Rodriguez", "D Balduzzi", "B. Sch\u00f6lkopf"], "venue": "Proceedings of ICML pp", "citeRegEx": "Rodriguez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2011}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, 28(3):321\u2013377,", "citeRegEx": "Hotelling,? \\Q1936\\E", "shortCiteRegEx": "Hotelling", "year": 1936}, {"title": "Using a model of social dynamics to predict popularity of news", "author": ["K Lerman", "T. Hogg"], "venue": "In Proceedings of the WWW,", "citeRegEx": "Lerman and Hogg,? \\Q2010\\E", "shortCiteRegEx": "Lerman and Hogg", "year": 2010}, {"title": "Memetracking and the dynamics of the news cycle", "author": ["J Leskovec", "L Backstrom", "J. Kleinberg"], "venue": "In Proceedings of KDD, pp", "citeRegEx": "Leskovec et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2009}, {"title": "On wiener-granger causality, information and canonical correlation", "author": ["Otter", "PW"], "venue": "Economics Letters,", "citeRegEx": "Otter and PW.,? \\Q1991\\E", "shortCiteRegEx": "Otter and PW.", "year": 1991}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine,", "citeRegEx": "Pearson,? \\Q1901\\E", "shortCiteRegEx": "Pearson", "year": 1901}, {"title": "Graphscope: parameter-free mining of large time-evolving graphs", "author": ["J Sun", "C Faloutsos", "S Papadimitriou", "Yu", "PS"], "venue": "In Proceedings of SIGKDD,", "citeRegEx": "Sun et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2007}, {"title": "Modeling information di\u21b5usion in implicit networks", "author": ["J Yang", "J. Leskovec"], "venue": "In Proceedings of the ICDM, pp", "citeRegEx": "Yang and Leskovec,? \\Q2010\\E", "shortCiteRegEx": "Yang and Leskovec", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "The authors of (Sun et al., 2007) use the temporal dynamics within a communication network graph to partition the nodes of the graph into groups.", "startOffset": 15, "endOffset": 33}, {"referenceID": 10, "context": "Other approaches towards network data graphs evolving over time investigate the di\u21b5usion of influential items, so called memes (Leskovec et al., 2009; Yang & Leskovec, 2010; Gomez Rodriguez et al., 2011).", "startOffset": 127, "endOffset": 203}, {"referenceID": 10, "context": "In (Leskovec et al., 2009; Yang & Leskovec, 2010) the authors focus on di\u21b5usion of n-grams in blogs and news media.", "startOffset": 3, "endOffset": 49}, {"referenceID": 10, "context": "Despite a number of similarities between (Leskovec et al., 2009; Yang & Leskovec, 2010; Gomez Rodriguez et al., 2011) and our method we emphasize an important di\u21b5erence: All of the above approaches require that the relevant items of information are selected prior to the analysis.", "startOffset": 41, "endOffset": 117}, {"referenceID": 10, "context": "For example in (Leskovec et al., 2009; Yang & Leskovec, 2010) the authors analyze a large data set containing millions of n-grams.", "startOffset": 15, "endOffset": 61}, {"referenceID": 10, "context": "in (Leskovec et al., 2009).", "startOffset": 3, "endOffset": 26}, {"referenceID": 8, "context": "y can be computed simultaneously using canonical correlation analysis (CCA) (Hotelling, 1936).", "startOffset": 76, "endOffset": 93}, {"referenceID": 4, "context": "The mathematical properties of CCA are as well understood (Jordan, 1875) as its statistical convergence criteria (Anderson, 1999; Fukumizu et al., 2007).", "startOffset": 113, "endOffset": 152}, {"referenceID": 2, "context": "(tkCCA), that can deal with high dimensional data, small sample sizes and time delayed non-linear dependencies between data (Bie\u00dfmann et al., 2010).", "startOffset": 124, "endOffset": 147}, {"referenceID": 0, "context": "However using the well known kernel trick (Aizerman et al., 1964) we can e ciently compute CCA in kernel space.", "startOffset": 42, "endOffset": 65}, {"referenceID": 3, "context": "An alternative approach for topic detection is latent semantic analysis (LSA) (Deerwester et al., 1990) in which only a single matrix of BoW features is factorized.", "startOffset": 78, "endOffset": 103}, {"referenceID": 12, "context": "Informally the relationship between LSA and CT is similar to the relationship between principal component analysis (PCA) (Pearson, 1901) and CCA: PCA maximizes the variance within one web source X", "startOffset": 121, "endOffset": 136}], "year": 2012, "abstractText": "Much information available on the web is copied, reused or rephrased. The phenomenon that multiple web sources pick up certain information is often called trend. A central problem in the context of web data mining is to detect those web sources that are first to publish information which will give rise to a trend. We present a simple and e cient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to a trend before others. We validate our approach on real data collected from influential technology news feeds.", "creator": "Preview"}}}