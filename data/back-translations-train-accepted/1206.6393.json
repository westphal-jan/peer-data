{"id": "1206.6393", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Local Loss Optimization in Operator Models: A New Insight into Spectral Learning", "abstract": "This paper re-visits the spectral method for learning latent variable models defined in terms of observable operators. We give a new perspective on the method, showing that operators can be recovered by minimizing a loss defined on a finite subset of the domain. A non-convex optimization similar to the spectral method is derived. We also propose a regularized convex relaxation of this optimization. We show that in practice the availabilty of a continuous regularization parameter (in contrast with the discrete number of states in the original method) allows a better trade-off between accuracy and model complexity. We also prove that in general, a randomized strategy for choosing the local loss will succeed with high probability.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (354kb)", "http://arxiv.org/abs/1206.6393v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["borja balle", "ariadna quattoni", "xavier carreras"], "accepted": true, "id": "1206.6393"}, "pdf": {"name": "1206.6393.pdf", "metadata": {"source": "META", "title": "Local Loss Optimization in Operator Models: A New Insight into Spectral Learning", "authors": ["Borja Balle"], "emails": ["bballe@lsi.upc.edu", "aquattoni@lsi.upc.edu", "carreras@lsi.upc.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to outdo themselves by putting themselves at the center of attention."}, {"heading": "2. Weighted Automata and Hankel Matrices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Preliminaries and Notation", "text": "Let us say that the pair (U) is a basis for the A-series (WA = > WA), and use \u03bb to denote the empty string. The Hankel matrix of a function f: 44 x - R over strings is a bi-infinite matrix Hf: 44 x - R with its entries indexed by prefixes and suffixes: Hf (u, v) = f (uv). The rank of f is defined as rank (Hf) = rank (Hf), which in principle can be infinite. Given the prefixes and suffixes of U, V and suffixes of M, we define the Hankel sub-block H: U \u00b7 V \u2192 R of Hf as H (u uv) = f (Hf). Note that if | U | = p, V = s we have H-Rp \u00b7 s. Generally, U and V have a rank (Hf)."}, {"heading": "2.2. Probability Distributions over Strings", "text": "Throughout the paper, it is assumed that some sub-blocks of the Hankel matrix Hf are known, either accurately or roughly. Obviously, in practice, it only makes sense to consider targets for which (approximate values) these sub-blocks can be obtained effectively, such as examples taken from a probability distribution, such as by querying an oracle. In general, most of the spectral methods discussed in Section 1 are used to learn probability distributions defined by a form of finite state machine. In these cases, the entries of H are probabilities, and usually a sample from the corresponding distribution is used to obtain empirical estimates of these probabilities, which will result in an approximate Hankel subblock H. Although we will not fix a specific probability model, it is worth noting that our results are applied seamlessly to most of the settings that have been cosisted to date. Specifically, we can deal with the following two settings: when defining probabilities over certain formulas Hexlities in WA, and when mixing them with other formulas Hexlities in WA."}, {"heading": "2.3. Duality between WA and Factorizations", "text": "We remember the following result (see (Beimel et al., 2000): rank (f) = rank (f) = rank (f) = rank (f) = rank (f) = rank (f) = rank (f) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3. Learning WA via Loss Minimization", "text": "In spirit, our algorithm is similar to the spectral method in the sense that we are a function for & # 252; r & # 252; r & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & & # 10; & & & & # 10; & & & & # 10; & & & # 10; & & # 10; & & & # 10; & & # 10; & # 10; & # 10; & & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & # 10; & & & # 10; & & # 10; & & # 10; & & # 10; & # 10; & & & # 10; & & # 10; & & & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & & & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & & & # 10; & # 10; & & & & # 10; & & # 10; & & # 10; & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & & # 10; & & & # 10; & & # 10; & & # 10; & & & # 10; & & & & & # 10; & & & & # 10; & & # 10; & & & & # 10; & # 10; & & & & # 10; & & & & & # 10; & & & & & &"}, {"heading": "3.1. A Convex Local Loss", "text": "The main idea for obtaining a convex optimization similar to (SO) will be to remove X, as we have already seen that it is the only source of non-convexity in optimization (SO). However, the new convex objective must include a term that forces optimization in a manner similar to (SO). First, we must note that the choice of n effectively limits the maximum ranking of operators Ba. Once this maximum ranking is established, X can be interpreted as enforcing a common \"semantic space\" between the different operators Ba, by ensuring that each of them works on a statespace that is controlled by the same projection of H. In addition, the constraint onX controls its norm, thus ensuring that the operators Ba also controls their norm to be in the order of Ha and H."}, {"heading": "4. Choosing the Local Loss", "text": "We have already discussed why it is important in practice to have methods to find a basis. In this section we will show a basic result on the basis, namely that simple randomized strategies for selecting a basis are highly likely to be successful. Furthermore, our result indicates limits on the number of examples required to find a basis that is polyphonic from some parameters of the target function f: \u03a3 \u00b2 R and the sample distribution D. We will start with a well-known folkloristic result on the existence of a minimum base. This means that in principle all methods of learning WA from sub-blocks of the Hankel matrix can work with a block whose size is only square in the number of states of the target. Proposal 4. For each f: Antonio \u2022 R of the ranking r, there is a basis (U, V) of the f | V | = V) of the sub-blocks of the Hankel matrix theory."}, {"heading": "5. Experimental Results", "text": "We performed synthetic and real experiments by comparing the SVD and the convex optimization methods. For the synthetic experiments, we created random PNFAs with literacy variables ranging from 2 to 10 symbols and a random number of states in the same range. For each random target model, we then selected k training sequences and trained models using SVD and CO. Results are shown with respect to L1 errors in true distribution (all results are averages of 10 sampling rounds). We determined that all subranges of length are 1, followed by Hsu and al., 2009 shows the learning curves for three target models. Each model was randomly selected from a series of models that have the smallest singular value of H."}, {"heading": "6. Conclusion", "text": "In this paper, we have attempted to facilitate the understanding and applicability of spectral approaches to learning weighted automata. Specifically, we have made the following contributions: (1) formulate weighted automata learning as a local loss minimization; (2) show that under certain conditions the standard SVD approach is an optimizer of this local loss; (3) suggest a convex relaxation that allows fine-tuning the complexity / accuracy compromise; (4) provide a verifiable correct method to estimate the extent of local loss function from samples; and (5) demonstrate in synthetic experiments that the convex relaxation method is more robust under certain conditions than the SVD approach."}, {"heading": "A. Technical Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1. Proof Sketch for Theorem 2", "text": "Let A = > \u03b1 > 1, \u03b1 \u221e, {Aa} \"be a WA with n states. Suppose that (U, V) is a basis for fA and write H = PS for the factorization induced by A on this Hankel sub-block. For each m and each pair of matrices N-Rm \u00b7 n and M-Rn \u00b7 m, so that PMN = P, the WA B = NAM = \u03b1 \u03b1 > 1 M, N\u03b1, {NAaM} fB = fA. Lemma 7. Let f: \u0440 \u2022 R be a function of the finite rank r and assume that (U, V) is a basis for f. Then the matrix H = [Ha1,.., Ham] has the rank r.Now the following three facts can be established."}, {"heading": "A.2. Proof of Theorem 5", "text": "We use the following result from (Vershynin, 2012).Theorem 8 (Corollary 5.52) in (Vershynin, 2012).We use the following result from (Vershynin, 2012).Theorem 8 (Corollary 5.52) in (Vershynin, 2012).We assume that a probability distribution in Rd with complete covariance matrix C and in a centered Euclidean ball of radius R. Even if N-K (\u03c31 / 2) R2 log (1 / 3) the matrix C (1 / 3) participates with full probability in the distribution.Here K is a universal constant. Consider the prefixes produced by algorithm 1 on input an i.d. random sample S = (x1)., xN) drawn by D. We write U = (u1, uN) for the prefixes produced by algorithms."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the EU's PASCAL2 NoE (FP7-ICT-216886) and a Google Research Award. B.B. was supported by an FPU scholarship (AP2008-02064) from the Spanish Ministry of Education. A.Q. and X.C. were supported by the Spanish government (JCI-2009-04240, RYC-2008-02223) and the E.C. (XLike FP7-288342)."}, {"heading": "Balle, B., Quattoni, A., and Carreras, X. A spectral", "text": "Learning algorithm for finite-state converters. ECML-PKDD, 2011."}, {"heading": "Beimel, A., Bergadano, F., Bshouty, N.H., Kushilevitz, E.,", "text": "and Varricchio, S. Learning functions represented as a variety of automatons. JACM, 2000."}, {"heading": "Boots, B., Siddiqi, S., and Gordon, G. Closing the learning", "text": "Planning loop with predictive state representations. I. J. Robotic Research, 2011."}, {"heading": "Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum", "text": "Probability through incomplete data on the EM algorithm. Journal of the Royal Statistical Society, 1977.Hsu, D., Kakade, S. M., and Zhang, T. A spectral algorithm for learning hidden Markov models. In Proc. of COLT, 2009."}, {"heading": "Liu, G., Sun, J., and Yan, S. Closed-form solutions to a", "text": "Category of Minimization Problems in Nuclear Standards. NIPS Workshop on Low-Rank Methods for Large-Scale Machine Learning, 2010."}, {"heading": "Luque, F.M., Quattoni, A., Balle, B., and Carreras, X.", "text": "Spectral learning in the non-deterministic analysis of dependencies. EACL, 2012.Parikh, A.P., Song, L., and Xing, E.P. A spectral algorithm for latent tree models. ICML, 2011."}, {"heading": "Siddiqi, S.M., Boots, B., and Gordon, G.J. Reduced-rank", "text": "hidden markov models. AISTATS, 2010."}, {"heading": "Song, L., Boots, B., Siddiqi, S., Gordon, G., and Smola,", "text": "A. Hilbert Space Embedings of hidden markov models. ICML, 2010. Vershynin, R. Introduction to the non-asymptotic analysis of random matrices, Band Compressed Sensing, Theory and Applications, Chapter 5. CUP, 2012."}], "references": [{"title": "Quadratic weighted automata: Spectral algorithm and likelihood maximization", "author": ["R. Bailly"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bailly,? \\Q2011\\E", "shortCiteRegEx": "Bailly", "year": 2011}, {"title": "A spectral learning algorithm for finite state transducers", "author": ["B. Balle", "A. Quattoni", "X. Carreras"], "venue": "ECML\u2013 PKDD,", "citeRegEx": "Balle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2011}, {"title": "Learning functions represented as multiplicity automata", "author": ["A. Beimel", "F. Bergadano", "N.H. Bshouty", "E. Kushilevitz", "S. Varricchio"], "venue": null, "citeRegEx": "Beimel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Beimel et al\\.", "year": 2000}, {"title": "Closing the learning planning loop with predictive state representations. I", "author": ["B. Boots", "S. Siddiqi", "G. Gordon"], "venue": "J. Robotic Research,", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "In Proc. of COLT,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Closed-form solutions to a category of nuclear norm minimization problems", "author": ["G. Liu", "J. Sun", "S. Yan"], "venue": "NIPS Workshop on Low-Rank Methods for Large-Scale Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Spectral learning in non-deterministic dependency parsing", "author": ["F.M. Luque", "A. Quattoni", "B. Balle", "X. Carreras"], "venue": null, "citeRegEx": "Luque et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Luque et al\\.", "year": 2012}, {"title": "A spectral algorithm for latent tree graphical models", "author": ["A.P. Parikh", "L. Song", "E.P. Xing"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "Reduced-rank hidden markov models", "author": ["S.M. Siddiqi", "B. Boots", "G.J. Gordon"], "venue": "AISTATS,", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "Hilbert space embeddings of hidden markov models", "author": ["L. Song", "B. Boots", "S. Siddiqi", "G. Gordon", "A. Smola"], "venue": null, "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Introduction to the non-asymptotic analysis of random matrices, volume Compressed Sensing, Theory and Applications, chapter", "author": ["R. Vershynin"], "venue": "CUP,", "citeRegEx": "Vershynin,? \\Q2012\\E", "shortCiteRegEx": "Vershynin", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Most of these methods can be interpreted as instances of the Expectation\u2013 Maximization algorithm (Dempster et al., 1977).", "startOffset": 97, "endOffset": 120}, {"referenceID": 5, "context": "It is the so-called spectral learning method, introduced by (Hsu et al., 2009) in the context of HMM and also applied to many other models such as Reduced Rank HMM (Siddiqi et al.", "startOffset": 60, "endOffset": 78}, {"referenceID": 9, "context": ", 2009) in the context of HMM and also applied to many other models such as Reduced Rank HMM (Siddiqi et al., 2010), Kernelized HMM (Song et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 10, "context": ", 2010), Kernelized HMM (Song et al., 2010), Predictive State Representations (Boots et al.", "startOffset": 24, "endOffset": 43}, {"referenceID": 3, "context": ", 2010), Predictive State Representations (Boots et al., 2011), Latent Tree Graphical Models (Parikh et al.", "startOffset": 42, "endOffset": 62}, {"referenceID": 8, "context": ", 2011), Latent Tree Graphical Models (Parikh et al., 2011), Finite States Transducers (Balle et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 1, "context": ", 2011), Finite States Transducers (Balle et al., 2011), and Quadratic Weighted Automata (Bailly, 2011).", "startOffset": 35, "endOffset": 55}, {"referenceID": 0, "context": ", 2011), and Quadratic Weighted Automata (Bailly, 2011).", "startOffset": 41, "endOffset": 55}, {"referenceID": 7, "context": "Furthermore, in the latter case our model encompasses the settings where a sample is used to estimate probabilities of words f(x), prefixes f(x\u03a3\u2217), or substrings f(\u03a3\u2217x\u03a3\u2217), since it is not difficult to see that when f is given by some WA with n states, there exists another WA with n states computing prefix and substring probabilities (Luque et al., 2012).", "startOffset": 335, "endOffset": 355}, {"referenceID": 2, "context": "We recall the following result (see (Beimel et al., 2000)): rank(f) = r < \u221e if and only if f = fA for some WA A with r states and for any WA A such that fA = f then |A| \u2265 r.", "startOffset": 36, "endOffset": 57}, {"referenceID": 5, "context": "The spectral algorithm of (Hsu et al., 2009) can be easily derived using Lemma 1.", "startOffset": 26, "endOffset": 44}, {"referenceID": 6, "context": "1 in (Liu et al., 2010) and the observation that when \u03c4 \u2192 \u221e optimization (CO) is equivalent to minB\u03a3 \u2016B\u03a3\u2016\u2217 s.", "startOffset": 5, "endOffset": 23}, {"referenceID": 5, "context": "We fixed the set of prefixes and suffixes to be all substrings of length 1, following (Hsu et al., 2009).", "startOffset": 86, "endOffset": 104}, {"referenceID": 11, "context": "We use the following result from (Vershynin, 2012).", "startOffset": 33, "endOffset": 50}, {"referenceID": 11, "context": "52 in (Vershynin, 2012)).", "startOffset": 6, "endOffset": 23}], "year": 2012, "abstractText": "This paper re-visits the spectral method for learning latent variable models defined in terms of observable operators. We give a new perspective on the method, showing that operators can be recovered by minimizing a loss defined on a finite subset of the domain. This leads to a derivation of a non-convex optimization similar to the spectral method. We also propose a regularized convex relaxation of this optimization. In practice our experiments show that a continuous regularization parameter (in contrast with the discrete number of states in the original method) allows a better trade-off between accuracy and model complexity. We also prove that in general, a randomized strategy for choosing the local loss succeeds with high probability.", "creator": "LaTeX with hyperref package"}}}