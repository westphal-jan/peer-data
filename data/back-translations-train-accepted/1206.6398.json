{"id": "1206.6398", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Learning Parameterized Skills", "abstract": "We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (1054kb)", "http://arxiv.org/abs/1206.6398v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Mon, 3 Sep 2012 16:05:45 GMT  (740kb,DS)", "http://arxiv.org/abs/1206.6398v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["bruno castro da silva", "george konidaris", "andrew g barto"], "accepted": true, "id": "1206.6398"}, "pdf": {"name": "1206.6398.pdf", "metadata": {"source": "META", "title": "Learning Parameterized Skills", "authors": ["Bruno Castro da Silva", "George Konidaris", "Andrew G. Barto"], "emails": ["bsilva@cs.umass.edu", "gdk@csail.mit.edu", "barto@cs.umass.edu"], "sections": [{"heading": "1. Introduction", "text": "One approach to dealing with the complexity of applying reinforcement learning to solve high-dimensional control problems is the definition or discovery of hierarchically structured strategies. The most common hierarchical reinforcement learning formalism is the option framework (Sutton et al., 1999), in which high-level options (also referred to as skills) define time-extended strategies that can be used directly in learning and planning, but abstract the details of low control. One of the motivating principles underlying hierarchical restraint appears in the proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Copyright 2012 by the author (s) / owner is the idea that sub-problems recur so that acquired or designed options can be reused in a variety of tasks and contexts. However, the option framework, as it is usually formulated, defines an option as a single policy."}, {"heading": "2. Setting", "text": "We assume that the MDPs have dynamics and reward functions that are so similar that they can be considered variations of the same task. Formally, the goal of such an agent is to maximize the maximum reward over the entire distribution of possible MDPs. We assume that the MDPs have dynamics and reward functions that are so similar that they can be considered variations of the same task. Formally, the goal of such an agent is to maximize the following task parameters that are drawn from a task parameter vector that comes out of a | T | dimensional continuous space T, J (\u03c0) = E {E \u00b2 K t = 0 rt | \u03c0, \u0432} the expected yield that is achieved in the execution of political tasks, while in the task setting the reverse and P (\u0432) is a probability density function that describes the probability that the task will occur. In addition, we currently define using parameters that are defined as skills."}, {"heading": "2.1. Assumptions", "text": "We assume that the agent must solve tasks that are drawn from a distribution P (\u03c4). Suppose we get a set K of pairs {\u03c4, \u03b8\u03c4} where \u03c4 is a | T | -dimensional vector of task parameters derived from Equation 2.We begin by highlighting the fact that the probability density function P induces a (possibly infinite) set of competence policies for solving tasks in support of P, each corresponding to a vector that maximizes quantity in the equation. We begin by highlighting the fact that the probability density function P induces a (possibly infinite) set of competence policies for solving tasks in support of P, each of which corresponds to a vector. These policies lie in a N-dimensional space that contains sample policies that can be used to solve tasks that are drawn from P. Since the tasks in support of P are related to the assumption that the policy is based on some structure of this policy, it is reasonable to assume that the policy is drawn from this structure; in particular, that the policy is a solution to the structure of this space."}, {"heading": "3. Overview", "text": "Our method goes further by collecting exemplary task cases and their solution strategies, and using them to train a family of independent non-linear regression models that map task parameters to political parameters. However, since the strategies for different subgroups of T may be in different, fragmented multiplicities, it is necessary to first estimate how many such low-dimensional surfaces exist, before separately training a series of regression models for each of them. Formally, our method consists of four steps: 1) draw | K | sample tasks from P and construct K, the task instances and the associated learned political parameters. 2) use K to estimate the geometry and topology of political space, in particular the number D of low-dimensional surfaces embedded in the politics of skill; 3) train a classifier that forms elements of T on [1,........,,..........] on a series of mangages (4)."}, {"heading": "4. The Dart Throwing Domain", "text": "In the field of dart throwing, a simulated planar, underactuated robotic arm is entrusted with the task of learning a parameterized strategy to accurately throw darts at targets around it (Figure 4).The base of the arm is fastened to a wall in the middle of a 3-meter high and 4-meter wide space.The arm consists of three interconnected links and a single motor that applies a torque to only the second joint, making this a difficult nonlinear and 1) final step. This last step assumes that the political characteristics are roughly independent of the task; if not, it is possible to alternatively train a series of D multivariate nonlinear regression models: i, i, i, i, i, i [1,., D].Each maps elements of T to complete political parameterizations, with the parameterization of the arm completed by the RN, and can be used to construct them."}, {"heading": "5. Learning Parameterized Skills for Dart Throwing", "text": "In order to implement the method outlined in Section 3, we need to define methods that represent a policy; 2) learn a policy from experience; 3) analyze and estimate the topology of political space; 3) determine the number of underdimensional surfaces on which competences lie; 4) construct non-linear classification mechanisms; and 5) construct non-linear regression models; in this section, we describe the specific algorithms and techniques chosen to address the problem of the Dart throwing domain. We discuss our results in Section 6. Our methods are guided directly by the characteristics of the domain. As the following experiments involve a multi-common robotic arm, we choose a political representation that is particularly well suited to robotics: Dynamic Movement Primitives (Schaal et al al al)."}, {"heading": "6. Experiments", "text": "Before discussing the performance of parameterized skills in this area, we have some empirically measured characteristics of their political space. In particular, we describe the different characteristics of the induced space of politics that we have generated to accomplish the task. We have similarly engaged in the processing of goals in the corresponding positions. Measures to accomplish this task have been elaborated with the help of Powell's; the learning algorithm has been configured to perform a policy update every 20 years. In our simulations, this criterion corresponds to the moment when the robotic arm executes a policy that has been achieved within 5 centimeters of the intended goal. In the order in which we launch the policy, we initiate strategies for the following goals."}, {"heading": "7. Related Work", "text": "The simplest solution for learning tasks in RL is that they turn out to be part of the state descriptor and treat the entire class of tasks as a single MDP. This approach has several shortcomings: 1) Learning and generalizing tasks is long enough to create all the necessary prerequisites; 2) The number of basic functions needed to determine the value of policies must be increased."}, {"heading": "8. Conclusions and Future Work", "text": "The idea behind our method is to capture a small number of task instances and generalize them to new problems by combining classifiers and non-linear regression models. In practice, this approach is effective because it exploits the intrinsic structure of political space and because qualification policies for similar tasks are typically based on low-dimensional diversity. Our framework enables the construction of effective parameterized skills and is also capable of identifying the number of qualitatively different strategies needed to solve a given task distribution, which can be broadened in several important directions. First, the question of how to actively select training tasks to improve the general willingness of a parameterized skill in the face of future task divestment needs to be addressed. Another important open problem is how to deal properly with a non-stationary task distribution. If a new task distribution is known accurately, it may be possible to manipulate the parameterized skills from the K."}, {"heading": "Acknowledgments", "text": "This research was partly funded by the 7th Framework Programme of the European Community (FP7 / 20072013), Funding Agreement No. ICT-IP-231722, project \"IM-CLeVeR - Intrinsically Motivated CumulativeLearning Versatile Robots.\""}], "references": [{"title": "Structure learning in a sensorimotor association", "author": ["D. Braun", "S. Waldert", "A. Aertsen", "D. Wolpert", "C. Mehring"], "venue": "task. PLoS ONE,", "citeRegEx": "Braun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Braun et al\\.", "year": 2010}, {"title": "Learning powerful kicks on the Aibo ERS-7: The quest for a striker", "author": ["M. Hausknecht", "P. Stone"], "venue": "In RoboCup2010: Robot Soccer World Cup XIV,", "citeRegEx": "Hausknecht and Stone,? \\Q2011\\E", "shortCiteRegEx": "Hausknecht and Stone", "year": 2011}, {"title": "Movement imitation with nonlinear dynamical systems in humanoid robots", "author": ["A. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation,", "citeRegEx": "Ijspeert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ijspeert et al\\.", "year": 2002}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kober and Peters,? \\Q2008\\E", "shortCiteRegEx": "Kober and Peters", "year": 2008}, {"title": "Imitation and reinforcement learning", "author": ["J. Kober", "J. Peters"], "venue": "IEEE Robotics & Automation Magazine,", "citeRegEx": "Kober and Peters,? \\Q2010\\E", "shortCiteRegEx": "Kober and Peters", "year": 2010}, {"title": "Building portable options: Skill transfer in reinforcement learning", "author": ["G. Konidaris", "A. Barto"], "venue": "In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Konidaris and Barto,? \\Q2007\\E", "shortCiteRegEx": "Konidaris and Barto", "year": 2007}, {"title": "Value-function-based transfer for reinforcement learning using structure mapping", "author": ["Y. Liu", "P. Stone"], "venue": "In Proceedings to the Twenty-First National Conference on Artificial Intelligence,", "citeRegEx": "Liu and Stone,? \\Q2006\\E", "shortCiteRegEx": "Liu and Stone", "year": 2006}, {"title": "Learning movement primitives", "author": ["S. Schaal", "J. Peters", "J. Nakanishi", "A. Ijspeert"], "venue": "In Proceedings of the Eleventh International Symposium on Robotics Research. Springer,", "citeRegEx": "Schaal et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schaal et al\\.", "year": 2004}, {"title": "Reinforcement learning of hierarchical skills on the Sony Aibo robot", "author": ["V. Soni", "S. Singh"], "venue": "In Proceedings of the Fifth International Conference on Development and Learning,", "citeRegEx": "Soni and Singh,? \\Q2006\\E", "shortCiteRegEx": "Soni and Singh", "year": 2006}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Cross-domain transfer for reinforcement learning", "author": ["M. Taylor", "P. Stone"], "venue": "In Proceedings of the Twenty Fourth International Conference on Machine Learning,", "citeRegEx": "Taylor and Stone,? \\Q2007\\E", "shortCiteRegEx": "Taylor and Stone", "year": 2007}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. de Silva", "J. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}], "referenceMentions": [{"referenceID": 9, "context": "The most widely used hierarchical reinforcement learning formalism is the options framework (Sutton et al., 1999), where high-level options (also called skills) define temporally extended policies that can be used directly in learning and planning but abstract away the details of low-level control.", "startOffset": 92, "endOffset": 113}, {"referenceID": 7, "context": "Because the following experiments involve a multi-joint simulated robotic arm, we chose a policy representation that is particularly well-suited to robotics: Dynamic Movement Primitives (Schaal et al., 2004), or DMPs.", "startOffset": 186, "endOffset": 207}, {"referenceID": 11, "context": "To analyze the geometry and topology of the policy space and estimate the number D of lowerdimensional surfaces on which skill policies lie we used the ISOMAP algorithm (Tenenbaum et al., 2000).", "startOffset": 169, "endOffset": 193}, {"referenceID": 13, "context": "We use standard Support Vector Machines (SVM) (Vapnik, 1995) due to their good generalization capabilities and relatively low dependence on parameter tuning.", "startOffset": 46, "endOffset": 60}, {"referenceID": 12, "context": "As previously mentioned, if important correlations between policy and task parameters are known to exist, multivariate regression models might be preferable; one possibility in such cases are Structure Support Vector Machines (Tsochantaridis et al., 2005).", "startOffset": 226, "endOffset": 255}, {"referenceID": 5, "context": "Konidaris and Barto (2007) introduce a method for constructing reusable options by learning them in an agent-centered state space instead of in the original problem-space.", "startOffset": 0, "endOffset": 27}, {"referenceID": 5, "context": "Konidaris and Barto (2007) introduce a method for constructing reusable options by learning them in an agent-centered state space instead of in the original problem-space. This technique does not, however, construct generalized skills capable of solving a family of related tasks. Soni and Singh (2006) create adaptable options whose meta-parameters, e.", "startOffset": 0, "endOffset": 303}, {"referenceID": 5, "context": "Konidaris and Barto (2007) introduce a method for constructing reusable options by learning them in an agent-centered state space instead of in the original problem-space. This technique does not, however, construct generalized skills capable of solving a family of related tasks. Soni and Singh (2006) create adaptable options whose meta-parameters, e.g., their termination criteria, can be adapted on-the-fly in order to deal with unknown, changing aspects of a task. However, this technique does not directly predict a complete parameterization of the policy for new tasks. Liu and Stone (2006) propose a method for transferring a value function between a specific given pair of tasks but require prior knowledge of the task dynamics in the form of a Dynamic Bayes Network.", "startOffset": 0, "endOffset": 598}, {"referenceID": 8, "context": "It is also often assumed that a mapping between features and actions of the source and target tasks exists and is known a priori, as in Taylor and Stone (2007). Hausknecht and Stone (2011) propose a way of estimating a parameterized skill for kicking a soccer ball with varying amounts of energy.", "startOffset": 136, "endOffset": 160}, {"referenceID": 0, "context": "Hausknecht and Stone (2011) propose a way of estimating a parameterized skill for kicking a soccer ball with varying amounts of energy.", "startOffset": 0, "endOffset": 28}, {"referenceID": 0, "context": "Finally, Braun et al. (2010) discuss how Bayesian modeling can be used to explain experimental data from cognitive and motor neuroscience that supports the idea of structure learning in humans, a concept very similar in nature to the one of parameterized skills.", "startOffset": 9, "endOffset": 29}], "year": 2012, "abstractText": "We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location.", "creator": "LaTeX with hyperref package"}}}