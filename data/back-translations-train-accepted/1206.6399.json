{"id": "1206.6399", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Demand-Driven Clustering in Relational Domains for Predicting Adverse Drug Events", "abstract": "Learning from electronic medical records (EMR) is challenging due to their relational nature and the uncertain dependence between a patient's past and future health status. Statistical relational learning is a natural fit for analyzing EMRs but is less adept at handling their inherent latent structure, such as connections between related medications or diseases. One way to capture the latent structure is via a relational clustering of objects. We propose a novel approach that, instead of pre-clustering the objects, performs a demand-driven clustering during learning. We evaluate our algorithm on three real-world tasks where the goal is to use EMRs to predict whether a patient will have an adverse reaction to a medication. We find that our approach is more accurate than performing no clustering, pre-clustering, and using expert-constructed medical heterarchies.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (288kb)", "http://arxiv.org/abs/1206.6399v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["jesse davis", "v\u00edtor santos costa", "elizabeth berg", "david page", "peggy l peissig", "michael caldwell"], "accepted": true, "id": "1206.6399"}, "pdf": {"name": "1206.6399.pdf", "metadata": {"source": "META", "title": "Demand-Driven Clustering in Relational Domainsfor Predicting Adverse Drug Events", "authors": ["Jesse Davis", "Peggy Peissig", "Michael Caldwell"], "emails": ["jesse.davis@cs.kuleuven.be", "vsc@dcc.fc.up.pt", "peissig.peggy@marshfieldclinic.org", "caldwell.michael@marshfieldclinic.org", "berg@biostat.wisc.edu", "page@biostat.wisc.edu"], "sections": [{"heading": "1. Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2. Background", "text": "LUCID dynamically constructs clusters that capture latent relationships between different objects in a domain in the context of the VISTA SRL algorithm (Davis et al., 2007), which combines automated attribute construction and model learning in a single process. VISTA uses first-order defined clauses that capture relational information to define (binary) attributes that then become nodes in a Bayesian network."}, {"heading": "2.1. Datalog", "text": "VISTA defines features using the non-recursive database subset of first-order logic. The alphabet consists of three types of symbols: constants, variables, and predicates. Constants (e.g. the drug name propranolol) that begin with a capital letter denote specific objects in the domain. Variable symbols (e.g. disease) denoted by lowercase letters range over objects in the domain. Predict symbols P / n, where n refers to the uniformity of the predicate and n \u2265 0, represent relationships between objects. An atom is P (t1,.., tn), where each ti is a constant or variable. A letter is an atom or its negation. A sentence is a disjunction over a finite set of literals. A definitive sentence is a sentence that contains exactly one positive sentence."}, {"heading": "2.2. VISTA", "text": "VISTA uses certain clauses to define characteristics for the statistical model. Each defined clause becomes a binary feature in the underlying statistical model. The feature receives a value of one for an example if the data about the example meets (i.e. proves) the clause, and it receives a value of zero otherwise. VISTA starts by selecting a Model M over an empty FS range of functions, corresponding to a model that predicts the previous probability of the target predicate. It then repeatedly searches for new characteristics for a fixed number of iterations. In each iteration, VISTA first selects a random seed sample and then performs a generic to specific search through the space of the candidate clauses. To guide the search process, it constructs the lowest clause by finding all facts relevant to the seed example. (Muggleton, 1995) VISTA constructs a rule that contains only the target attribute."}, {"heading": "3. LUCID", "text": "At a high level, LUCID's key innovation is in the construction of property definitions, where the algorithm has the ability to invent hierarchical clusters that relate to a subset of objects (i.e. constants) in the domain. Intuitively, constants that appear in the same grouping may have some latent relationships, and the discovery and use of the latent structure in property definitions offers several advantages. First, it allows for more compact property definitions. Second, it helps to identify important features by aggregating across object groups that cannot be considered relevant by the learning algorithm. To illustrate the intuition behind LUCID, we use an ongoing example of WarfarinTM medication ADRs, which is a blood thinner frequently prescribed to patients at risk of stroke. However, warfarin is known to increase the risk of internal bleeding for some patients."}, {"heading": "3.1. Representing Latent Structure", "text": "The goal of LUCID is to capture hierarchical latent structures via specific objects in the domain. Conceptually, clustering represents the latent structure. LUCID introduces a predicate for each cluster it invents, such as Cluster1 / 1. The predicate applies to each object assigned to the cluster it represents. Once the definition is learned, it can appear in learned rules such as Rule (2). LUCID can be assigned to objects in two ways. First, LUCID can assign a cluster to individual objects, which means that certain constants are interchangeable in some cases. For example, terconazole, rifampicin, and ketoconazole are all enzyme inductors, and a doctor could reasonably prescribe them before inventing a new cluster (commonly known as Cluster1)."}, {"heading": "3.2. Learning Latent Structure", "text": "The key step in the algorithm definition is the discovery of the latent structure. (eg) The definition of a drug (eg) and a constant that is to be replaced by a cluster (eg), in a second step it is decided which objects are to be associated with the newly invented cluster. (eg) The first step rewrites the definition by replacing the specific constant with an invented latent structure. (eg) The second step decides which objects are to be associated with the newly invented cluster. (eg) The definition is rewritten by replacing the specific constant with an invented latent structure. (eg) The second step decides which objects are to be associated with the newly invented one. (eg) Rule (eg) would be converted into Rule. (2) The variable definition has replaced the constant terconazole, and the feature definition was extended with the founded cluster 1."}, {"heading": "3.3. Overall Algorithmic Structure", "text": "The algorithm 3.3 provides an overview of the efficiency of LUCID. It uses the same procedure as VISTA to construct a first set of candidate characteristics. Next, LUCID considers extending each feature definition by an additional, invented predicate by invoking the procedure outlined in subsection 3.2. This is the crucial difference to VISTA, as it results in a larger and much more meaningful set of candidate characteristics. However, due to the large number of candidate characteristics, it is prohibitive to consider the invention and inclusion of a learned cluster in any feature definition. Therefore, LUCID limits itself to inventing a latent concept only for characteristics that meet the following two conditions: Condition 1: The rule under consideration improves the score of the model. This provides initial evidence that the rule is useful, but the algorithm may be able to improve its quality by finding the following rule:"}, {"heading": "4. Empirical Evaluation", "text": "In this section, we evaluate our proposed approach based on three real data sets. In all tasks, we get patients taking a particular medication and the goal is to model those patients who have a related ADR. We compare the following algorithms. VISTA This is the basic VISTA algorithm (Davis et al., 2007). It does not have the ability to learn clusters that capture latent structures. Expert + VISTA In this context, we expand each data set to include handmade hierarchies for both diagnoses and medicines. For diagnoses, we have used all levels of the ICD9 hierarchy. For medicines, we use a hierarchy developed by our medical staff. Then, we run VISTA based on the extended data set. Instead of restricting ourselves to the specific disease diagnoses or medicines recorded for a patient, we can learn rules at EXPERT + VISTA that evaluate information about diseases or the medicines appearing in the UCESTA + hierarchy."}, {"heading": "4.1. Task Descriptions", "text": "Our data comes from a large multi-specialized clinic that has been using electronic medical records since 1985 and has electronic data dating back to the early 1960s. To conduct these studies, we use the approval of the review board. For all tasks, we have access to information about observations (e.g. vital signs, family history, etc.), laboratory results, disease diagnoses, and medications. We use patient data only up to a week before the first prescription of the drug in question. This ensures that we only build predictable models based on data generated prior to the patient's prescription of the drug. The characteristics of each task can be found in Table 1. We now briefly describe each task. Selective Cox2 inhibitors (e.g. VioxxTM) are a class of painkillers that increase a patient's risk of having a heart attack (i.e. a heart attack)."}, {"heading": "4.2. Methodology and Results", "text": "We are dealing here with a very complex structure."}, {"heading": "4.3. Learned Groupings", "text": "Another important yardstick is whether LUCID invents interesting and relevant concepts. We introduced several invented clusters to a physician with experience in circulatory diseases. We will focus our discussion on structures from the selective Cox-2 area. The expert identified a cluster that included the drugs diltiazem, a calcium channel blocker, and clopidogrel (PlavixTM), a platelet agent, which are commonly used in acute coronary syndrome, especially after angioplasty. In terms of diseases, the expert highlighted a cluster that describes heart catheters and coronary angioplasty, which are consistent with acute coronary syndrome, meaning that a patient is at high risk of having an MI. Another interest group concerned cholectomy, a procedure that removes the gallbladder, as women are often mistaken for having a diagnosis of MI with gallbladder pain."}, {"heading": "5. Related Work", "text": "SRL lies at the interface of relational learning and graphical model learning, so methods for discovering latent structures are based on predicate finding in relational learning (e.g. (Muggleton & Buntine, 1988) and latent variable discovery in propositional graphical models (e.g. (Elidan et al., 2000)). Our approach is closely related to Dietterich and Michalski's (1983) work in the field of relational learning via internal disjunction, which replaces a constant with a disjunction of several constants. We go beyond this work by allowing the reuse of an internal disjunction and, in particular, by explicitly discussing uncertainty in the data and the predicate invented. Our work is not the first to combine ideas from latent variable discoveries and dictate inventions to perform cluster-based concepts in uncertain relational domains (Kemp et al., 2006; Kok & Domingos, 2006, Step, Sutu, Xever; Uncuster)."}, {"heading": "6. Future Work and Conclusions", "text": "We introduced LUCID, a novel algorithm that detects latent structures through a dynamic, demand-driven process that, during learning, can invent clusters of objects in the domain and incorporate them into the learned model. We evaluated LUCID by evaluating models using electronic medical records (EMR) to predict which patients are most at risk of suffering from a particular adverse drug reaction (ADR). In all three tasks we studied, LUCID led to improved performance compared to a standard SRL baseline, a cluster-based latent structure-discovery algorithm, and medical heterarchies built using expert constructions. In addition, it generated significant latent structures. Important directions for further research include applications to other ADRs, other tasks in learning EMRs, and other types of relational databases."}, {"heading": "Acknowledgements", "text": "We thank Daniel Lowd, Maurice Bruynooghe and the reviewers for their helpful feedback. JD is partially supported by the K.U.Leuven Research Fund (CREA / 11 / 015 and OT / 11 / 051), the Marie Curie Career Integration Grant (# 294068) and FWO-Vlaanderen (G.0356.12). VSC is funded by the ERDF through the COMPETE programme and by the Portuguese Government through the FCT Foundation for Science and Technology projects LEAP (PTDC / EIA-CCO / 112158 / 2009) and ADE (PTDC / EIA-EIA / 121686 / 2010). MC, PP, EB and DP gratefully accept the support of NIGMS funding R01GM097618-01."}], "references": [{"title": "Change of representation for statistical relational learning", "author": ["J. Davis", "I. Ong", "J. Struyf", "E. Burnside", "D. Page", "Costa", "V. Santos"], "venue": "In Proc. of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "A comparative review of selected methods for learning from examples", "author": ["T.G. Dietterich", "R.S. Michalski"], "venue": "In Machine Learning: An Artificial Intelligence Approach,", "citeRegEx": "Dietterich and Michalski,? \\Q1983\\E", "shortCiteRegEx": "Dietterich and Michalski", "year": 1983}, {"title": "Discovering hidden variables: A structure-based approach", "author": ["G. Elidan", "N. Lotner", "N. Friedman", "D. Koller"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Elidan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Elidan et al\\.", "year": 2000}, {"title": "Bayesian networks classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning,", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "An Introduction to Statistical Relational Learning", "author": ["L. Getoor", "Taskar", "B. (eds"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J. Tenenbaum", "T. Griffiths", "T. Yamada", "N. Ueda"], "venue": "In Proc. of the 21st National Conference on Artificial Intelligence,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Statistical predicate invention", "author": ["S. Kok", "P. Domingos"], "venue": "In Proc. of the 24th International Conference on Machine Learning,", "citeRegEx": "Kok and Domingos,? \\Q2007\\E", "shortCiteRegEx": "Kok and Domingos", "year": 2007}, {"title": "Extracting semantic networks from text via relational clustering", "author": ["S. Kok", "P. Domingos"], "venue": "In Proc. of the European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Kok and Domingos,? \\Q2008\\E", "shortCiteRegEx": "Kok and Domingos", "year": 2008}, {"title": "Inverse entailment and Progol", "author": ["S. Muggleton"], "venue": "New Generation Computing,", "citeRegEx": "Muggleton,? \\Q1995\\E", "shortCiteRegEx": "Muggleton", "year": 1995}, {"title": "Machine invention of firstorder predicates by inverting resolution", "author": ["S. Muggleton", "W. Buntine"], "venue": "In Proc. of the 5th International Conference on Machine Learning,", "citeRegEx": "Muggleton and Buntine,? \\Q1988\\E", "shortCiteRegEx": "Muggleton and Buntine", "year": 1988}, {"title": "Cluster-based concept invention for statistical relational learning", "author": ["A. Popescul", "L. Ungar"], "venue": "In Proc. of the 10th ACM International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Popescul and Ungar,? \\Q2004\\E", "shortCiteRegEx": "Popescul and Ungar", "year": 2004}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J. Tenenbaum"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2010}, {"title": "Infinite hidden relational models", "author": ["Z. Xu", "V. Tresp", "K. Yu", "Kriegel", "H-P"], "venue": "In Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Xu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 5, "context": "While most state-of-the-art SRL systems are unable to effectively cope with the challenge of latent structure, a few approaches address this problem via a relational clustering of objects and/or relations in a domain (Kemp et al., 2006; Kok & Domingos, 2007; 2008; Sutskever et al., 2010).", "startOffset": 217, "endOffset": 288}, {"referenceID": 11, "context": "While most state-of-the-art SRL systems are unable to effectively cope with the challenge of latent structure, a few approaches address this problem via a relational clustering of objects and/or relations in a domain (Kemp et al., 2006; Kok & Domingos, 2007; 2008; Sutskever et al., 2010).", "startOffset": 217, "endOffset": 288}, {"referenceID": 5, "context": ", diseases, drugs) (Kemp et al., 2006; Kok & Domingos, 2007).", "startOffset": 19, "endOffset": 60}, {"referenceID": 0, "context": "It does so in the context of the SRL algorithm VISTA (Davis et al., 2007), which combines automated feature construction and model learning into a single process.", "startOffset": 53, "endOffset": 73}, {"referenceID": 8, "context": "To guide the search process, it constructs the bottom clause by finding all facts that are relevant to the seed example (Muggleton, 1995).", "startOffset": 120, "endOffset": 137}, {"referenceID": 3, "context": "In principle, any structure learner could be used, but VISTA typically uses a tree-augmented Naive Bayes model (Friedman et al., 1997).", "startOffset": 111, "endOffset": 134}, {"referenceID": 0, "context": "VISTA This is the basic VISTA algorithm (Davis et al., 2007).", "startOffset": 40, "endOffset": 60}, {"referenceID": 2, "context": ", (Elidan et al., 2000)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 1, "context": "Our approach is closely related to Dietterich and Michalski\u2019s (1983) relational learning work on internal disjunction.", "startOffset": 35, "endOffset": 69}, {"referenceID": 5, "context": "Our work is not the first to combine ideas from latent variable discovery and predicate invention to perform cluster-based concept discovery in uncertain, relational domains (Kemp et al., 2006; Kok & Domingos, 2007; 2008; Sutskever et al., 2010; Xu et al., 2006; Popescul & Ungar, 2004).", "startOffset": 174, "endOffset": 286}, {"referenceID": 11, "context": "Our work is not the first to combine ideas from latent variable discovery and predicate invention to perform cluster-based concept discovery in uncertain, relational domains (Kemp et al., 2006; Kok & Domingos, 2007; 2008; Sutskever et al., 2010; Xu et al., 2006; Popescul & Ungar, 2004).", "startOffset": 174, "endOffset": 286}, {"referenceID": 12, "context": "Our work is not the first to combine ideas from latent variable discovery and predicate invention to perform cluster-based concept discovery in uncertain, relational domains (Kemp et al., 2006; Kok & Domingos, 2007; 2008; Sutskever et al., 2010; Xu et al., 2006; Popescul & Ungar, 2004).", "startOffset": 174, "endOffset": 286}, {"referenceID": 5, "context": "Empirically, the SNE system (Kok & Domingos, 2008), which we compare to, outperformed the IRM (Kemp et al., 2006) and MRC (Kok & Domingos, 2007) on a domain of similar complexity and size to those we considered.", "startOffset": 94, "endOffset": 113}, {"referenceID": 5, "context": "Our work is not the first to combine ideas from latent variable discovery and predicate invention to perform cluster-based concept discovery in uncertain, relational domains (Kemp et al., 2006; Kok & Domingos, 2007; 2008; Sutskever et al., 2010; Xu et al., 2006; Popescul & Ungar, 2004). Popescul and Ungar (2004) use a pre-processing step that learns clusterings and then treats cluster membership as an invented feature during learning.", "startOffset": 175, "endOffset": 314}, {"referenceID": 5, "context": "Our work is not the first to combine ideas from latent variable discovery and predicate invention to perform cluster-based concept discovery in uncertain, relational domains (Kemp et al., 2006; Kok & Domingos, 2007; 2008; Sutskever et al., 2010; Xu et al., 2006; Popescul & Ungar, 2004). Popescul and Ungar (2004) use a pre-processing step that learns clusterings and then treats cluster membership as an invented feature during learning. In contrast, LUCID uses the learning process to guide cluster construction and it also allows reuse of clusters as part of new clusters. Sutskever et al. (2010) focus only on binary relations, whereas our domains have higher arity relations.", "startOffset": 175, "endOffset": 600}], "year": 2012, "abstractText": "Learning from electronic medical records (EMR) is challenging due to their relational nature and the uncertain dependence between a patient\u2019s past and future health status. Statistical relational learning is a natural fit for analyzing EMRs but is less adept at handling their inherent latent structure, such as connections between related medications or diseases. One way to capture the latent structure is via a relational clustering of objects. We propose a novel approach that, instead of pre-clustering the objects, performs a demand-driven clustering during learning. We evaluate our algorithm on three realworld tasks where the goal is to use EMRs to predict whether a patient will have an adverse reaction to a medication. We find that our approach is more accurate than performing no clustering, pre-clustering, and using expert-constructed medical heterarchies.", "creator": "LaTeX with hyperref package"}}}