{"id": "1206.6400", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret", "abstract": "Online learning algorithms are designed to learn even when their input is generated by an adversary. The widely-accepted formal definition of an online algorithm's ability to learn is the game-theoretic notion of regret. We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm's actions. We define the alternative notion of policy regret, which attempts to provide a more meaningful way to measure an online algorithm's performance against adaptive adversaries. Focusing on the online bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. On the other hand, if the adversary's memory is bounded, we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound. We extend this result to other variants of regret, such as switching regret, internal regret, and swap regret.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (296kb)", "http://arxiv.org/abs/1206.6400v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ofer dekel", "ambuj tewari", "raman arora"], "accepted": true, "id": "1206.6400"}, "pdf": {"name": "1206.6400.pdf", "metadata": {"source": "CRF", "title": "Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret", "authors": ["Raman Arora", "Ofer Dekel", "Ambuj Tewari"], "emails": ["arora@ttic.edu", "oferd@microsoft.com", "ambuj@cs.utexas.edu"], "sections": [{"heading": "1. Introduction", "text": "In each round of the game, the player chooses an action Xt from an action he chooses, and the player suffers a loss of ft (Xt). We often assume that ft (Xt) is limited in the world. The player observes the loss ft (s) and uses it to update his strategy for subsequent rounds. Unlike the full information player, the bandit player does not observe the total loss. The goal of the player is to accumulate the smallest possible loss over T rounds. While this presentation is intuitively appealing, it hides the information that the opponent can use in the selection."}, {"heading": "1.1. Related Work", "text": "In fact, it is so that most of us are able to keep to the rules that they have imposed on themselves. (...) It is not so that they keep to the rules. (...) It is not so that they keep to the rules. (...) It is as if they keep to the rules. (...) It is not as if they keep to the rules. \"(...) It is as if they keep to the rules.\" (...) It is as if they keep to the rules. \"(...) It is as if they keep to the rules.\" (...) It is as if they keep to the rules. \"(...) It is as if they keep to the rules.\" (...). \"(...).\" (...). \"(...).\" (. \"(.).\" (.). \"(...).\" (. \"(...).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (. \"(.).\" (.). (.). \"(.\" (.). \"(.). (.). (.). (.). (.\" (.). (.). (.). (.). (.). (.). (. (.). (.).). (. (.). (.). \"(. (.).). (. (.). (.). (. (.). (.).\" (.). (. (.). \"(. (.).). (.).\" (. \"(. (.). (.). (.). (.). (.).). (.). (.). (.). (. (.). (.). (.). (.).).\" (. (.). (.).). (. (.).)."}, {"heading": "2. Policy regret", "text": "It is not the first time that we have been able to obtain a non-trivial (sublinear) sequence of actions that is responsible for all adaptive action sequences, namely the sequences of form (y,.., y) for the way in which we can achieve a non-trivial (sublinear) mode of action. We prove that it is impossible to obtain a non-trivial (sublinear) form of action that holds for all adaptive action sequences. Theorem 1. For every player there is an adaptive adversary that the player's policy is consistent compared to the best constant action sequences (T).Proof. Let the policy (0, 1) be Pr (X1 = y). Define that the player's policy is durable compared to the best constant action sequences (T).Proof."}, {"heading": "3. Applying the Result", "text": "With Thm. 2 in hand, we prove that the political regret of the existing online bandit algorithms (7k log) 1 / 2 (T + 3) functions sublinear with T + 3. We start with the EXP3 algorithm (Auer et al., 2002) in the classic k-armed bandit environment, with its regret generated by an adaptive adversary. (D) The application of Thm. 2 with C = 7k log and q = 1 / 2 proves the following result. Leave X = {1,., k} and leave Ft out of all functions from X to [0, 1]. The political regret of the mini-batch version of the EXP3 algorithm is al., 2002), with the batch size of the (7k log.) \u2212 1 / 3T 1 / 3, bounded against an m memory."}, {"heading": "4. Extensions", "text": "Theorem 2 is presented in its simplest form, and we can expand it in various interesting ways."}, {"heading": "4.1. Relaxing the Adaptive Assumption", "text": "Let us remember that we assumed that A has a (standard) limit of repentance that applies to any loss sequence produced by an adaptive opponent. A closer look at the evidence of Thm. 2 shows that it is sufficient to assume that A's limit of repentance holds against any loss sequence produced by an adaptive opponent with 1 memory. To see why, note that the assumption that each foot is mmemory-bounded, the assumption that \u03c4 > m and the definition of f-j in Equation (2) together imply that each f-j is memory-bounded."}, {"heading": "4.2. When m is Known", "text": "We can strengthen Thm. 2 in two ways if the memory limit m is given to A\u03c4. Firstly, we define f-j (z1,.., zj) as1\u03c4 \u2212 m-b = m + 1 f (j-1) \u03c4 + k (z-1,..., z-1, z-k j). (8) Note that the first m rounds are omitted in the mini-batch. This makes the sequence (f-j) J = 1 a memory-limited sequence. In other words, we only need the regret of A, which necessarily applies to unsuspecting opponents. In addition to relaxing the assumption of the regret of the original algorithm A, we use m to further optimize the value. This reduces the linear dependence on m in our policy that exceeds the limits of regret, as shown in the following example."}, {"heading": "4.3. Switching Competitors", "text": "So far, we have defined CT as the simplest competitive class possible, the class of constant sequences of action. We are now redefining CT to include all piecemeal constant sequences with at most s switches (Auer et al., 2002). Namely, a sequence in CT is a concatenation of at most s shorter constant sequences, the total length of which is T. In this case, we assume that A's regret holds in comparison to sequences with s switches and we receive a political regret compared to sequences with s switches. Theorem 4. Let us repeat the assumptions of Thm. 2, except that CT's regret is the series of action sequences with at most s switches (where s is firm and independent of T) and A's regret holds in comparison to sequences with s switches. Theorem 4. Let us repeat the assumptions of Thm. 2, except that CT's regret returns in comparison to sequences with at most s switches (where we have fixed and independent of A's switches) with theorem."}, {"heading": "4.4. Internal Regret, Swap Regret, \u03a6-Regret", "text": "We have previously considered the standard term of external pseudo-regret, which compares the player's action sequence with the action sequence of internal regret (Blum & Mansour, 2007) and swap regret (Blum & Mansour, 2007). To define these terms, we have a series of action transformations, namely, each individual action. Each action is a function of the form of internal regret (X \u2192 X). The player's regret is then defined as: maximum action of the opponent against each action. In words, we compare the loss of the player with the loss that would have been achieved if the player had replaced his current action with an action."}, {"heading": "5. Discussion", "text": "We have defined the concept of political regret, arguing that it better grasps the intuitive semantics of the word \"regret\" than the standard definition, and then we have established non-trivial limits for the political regret of various bandit algorithms. In other words, we do not know how narrow our limits are. It is conceivable that bandit algorithms specifically designed to minimize political regret have superior limits, but we are not yet able to show this. On a related topic, we do not know whether our minibatching technique is really necessary: perhaps one could prove a non-trivial political regret tied to the original (unmodified) EXP3 algorithm. We are leaving these questions as an open problem for future research."}, {"heading": "Abernethy, J., Hazan, E., and Rakhlin, A. Competing in", "text": "the dark: An efficient algorithm for linear optimization of bandits. In COLT, pp. 263-274, 2008."}, {"heading": "Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.", "text": "The Nonstochastic, Multi-Armed Bandit Problem. SIAM Journal on Computing, 32 (1): 48-77, 2002.Awerbuch, B. and Kleinberg, R. D. Adaptive Routing with End-to-End Feedback: Distributed Learning and Geometric Approaches. In STOC, pp. 45-53, 2004."}, {"heading": "Bartlett, P. L., Dani, V., Hayes, T. P., Kakade, S., Rakhlin,", "text": "A., and Tewari, A. High-probability regret bounds for bandit online linear optimization. In COLT, pp. 335- 342, 2008.Blum, A. and Mansour, Y. From external to internal regrette. JMLR, 8: 1307-1324, 2007.Borodin, A. and El-Yaniv, R. Online computation and competitive analysis. Cambridge University Press, 1998.Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and games. Cambridge University Press, 2006.Dani, V. and Hayes, T. P. Robbing the bandit: Less regret in online geometric optimization against an adaptive opponent. In SODA, 2006.de Farias, D. P. and Megiddo, N. Combining expert advice in reactive environments. Journal of the ACM, 53 (5): 762-799, 2006."}, {"heading": "Dekel, O., Gilad-Bachrach, R., Shamir, Ohad, and Xiao,", "text": "Lin. Optimally distributed online prediction. In ICML, 2011."}, {"heading": "Even-Dar, E., Kakade, S. M., and Mansour, Y. Online", "text": "Markov Decision Processes. Math. of Operations Research, 34 (3): 726-736, 2009."}, {"heading": "Flaxman, A. D., Kalai, A. Tauman, and McMahan, B. H.", "text": "In SODA, pp. 385-394, 2005.Hazan, E. and Kale, S. Online submodular minimization. In Advances in Neural Information Processing Systems (NIPS), 2009.Hazan, E., Kalai, A., Kale, S., and Agarwal, A. Logarithmic regret algorithms for online convex optimization. In COLT, 2006.Kleinberg, R. Near-narrow limits for the continuously armed bandit problem. In NIPS, pp. 697-704, 2004."}, {"heading": "Maillard, O. and Munos, R. Adaptive bandits: Towards", "text": "The best story-based strategy. In AISTATS, 2010. McMahan, H. B. and Blum, A. Online geometric optimization in the bandit milieu against an adaptive adversary. In COLT, 2004."}, {"heading": "Merhav, N., Ordentlich, E., Seroussi, G., and Weinberger,", "text": "M.J. Sequential Strategies for Loss Functions with Memory. IEEE IT, 48 (7): 1947-1958, 2002.Nesterov, Y. E. and Nemirovsky, A. S. Interior point polynomial algorithms in convex programming. SIAM, 1994."}, {"heading": "Neu, G., Gyo\u0308rgy, A., Szepesva\u0301ri, C., and Antos, A. Online", "text": "Markov, H. Some Aspects of Sequential Design of Experiments. Bulletin of the AMS, 58: 527-535, 1952.Ryabko, D. and Hutter, M. On the Possibility of Learning in Reactive Environments with Arbitrary Dependence. Theoretical Comput. Sci., 405 (3): 274-284, 2008.Szepesva \u0301 ri, C. Algorithms for Enhanced Learning. Synth. Lectures in A.I. and Machine Learning. Morgan & Claypool Publishers, 2010."}, {"heading": "Yu, J. Y., Mannor, S., and Shimkin, N. Markov decision", "text": "Processes with arbitrary reward processes. Math. of Operations Research, 34 (3): 737-757, 2009.Zinkevich, M. Online convex programming and generalized increase in infinitesimal gradient. In ICML, 2003."}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["J. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "In COLT, pp", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: distributed learning and geometric approaches", "author": ["B. Awerbuch", "R.D. Kleinberg"], "venue": "In STOC, pp", "citeRegEx": "Awerbuch and Kleinberg,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg", "year": 2004}, {"title": "High-probability regret bounds for bandit online linear optimization", "author": ["P.L. Bartlett", "V. Dani", "T.P. Hayes", "S. Kakade", "A. Rakhlin", "A. Tewari"], "venue": "In COLT,", "citeRegEx": "Bartlett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2008}, {"title": "Online computation and competitive analysis", "author": ["A. Borodin", "R. El-Yaniv"], "venue": null, "citeRegEx": "Borodin and El.Yaniv,? \\Q1998\\E", "shortCiteRegEx": "Borodin and El.Yaniv", "year": 1998}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Robbing the bandit: Less regret in online geometric optimization against an adaptive adversary", "author": ["V. Dani", "T.P. Hayes"], "venue": "In SODA,", "citeRegEx": "Dani and Hayes,? \\Q2006\\E", "shortCiteRegEx": "Dani and Hayes", "year": 2006}, {"title": "Combining expert advice in reactive environments", "author": ["D.P. de Farias", "N. Megiddo"], "venue": "Journal of the ACM,", "citeRegEx": "Farias and Megiddo,? \\Q2006\\E", "shortCiteRegEx": "Farias and Megiddo", "year": 2006}, {"title": "Optimal distributed online prediction", "author": ["O. Dekel", "R. Gilad-Bachrach", "Shamir", "Ohad", "Xiao", "Lin"], "venue": "In ICML,", "citeRegEx": "Dekel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2011}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "Kalai", "A. Tauman", "B.H. McMahan"], "venue": "In SODA, pp", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Online submodular minimization", "author": ["E. Hazan", "S. Kale"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hazan and Kale,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal"], "venue": "In COLT,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "Nearly tight bounds for the continuumarmed bandit problem", "author": ["R. Kleinberg"], "venue": "In NIPS, pp", "citeRegEx": "Kleinberg,? \\Q2004\\E", "shortCiteRegEx": "Kleinberg", "year": 2004}, {"title": "Adaptive bandits: Towards the best history-dependent strategy", "author": ["O. Maillard", "R. Munos"], "venue": "In AISTATS,", "citeRegEx": "Maillard and Munos,? \\Q2010\\E", "shortCiteRegEx": "Maillard and Munos", "year": 2010}, {"title": "Online geometric optimization in the bandit setting against an adaptive adversary", "author": ["H.B. McMahan", "A. Blum"], "venue": "In COLT,", "citeRegEx": "McMahan and Blum,? \\Q2004\\E", "shortCiteRegEx": "McMahan and Blum", "year": 2004}, {"title": "Sequential strategies for loss functions with memory", "author": ["N. Merhav", "E. Ordentlich", "G. Seroussi", "M.J. Weinberger"], "venue": "IEEE IT,", "citeRegEx": "Merhav et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Merhav et al\\.", "year": 2002}, {"title": "Interior point polynomial algorithms in convex programming", "author": ["Y.E. Nesterov", "A.S. Nemirovsky"], "venue": null, "citeRegEx": "Nesterov and Nemirovsky,? \\Q1994\\E", "shortCiteRegEx": "Nesterov and Nemirovsky", "year": 1994}, {"title": "Online Markov decision processes under bandit feedback", "author": ["G. Neu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri", "A. Antos"], "venue": "In NIPS, pp", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "On the possibility of learning in reactive environments with arbitrary dependence", "author": ["D. Ryabko", "M. Hutter"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Ryabko and Hutter,? \\Q2008\\E", "shortCiteRegEx": "Ryabko and Hutter", "year": 2008}, {"title": "Algorithms for Reinforcement Learning. Synth", "author": ["C. Szepesv\u00e1ri"], "venue": "Lectures in A.I. and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "Szepesv\u00e1ri,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2010}, {"title": "Markov decision processes with arbitrary reward processes", "author": ["J.Y. Yu", "S. Mannor", "N. Shimkin"], "venue": "Math. of Operations Research,", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Interesting special cases of online learning with bandit feedback are the k-armed bandit (Robbins, 1952; Auer et al., 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al.", "startOffset": 89, "endOffset": 123}, {"referenceID": 1, "context": "Interesting special cases of online learning with bandit feedback are the k-armed bandit (Robbins, 1952; Auer et al., 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al.", "startOffset": 89, "endOffset": 123}, {"referenceID": 12, "context": ", 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan & Kale, 2009).", "startOffset": 36, "endOffset": 99}, {"referenceID": 9, "context": ", 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan & Kale, 2009).", "startOffset": 36, "endOffset": 99}, {"referenceID": 0, "context": ", 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan & Kale, 2009).", "startOffset": 36, "endOffset": 99}, {"referenceID": 3, "context": "A special case of bandit convex optimization is bandit linear optimization (Awerbuch & Kleinberg, 2004; Bartlett et al., 2008), where the functions in Ft are linear in their last argument.", "startOffset": 75, "endOffset": 126}, {"referenceID": 1, "context": "In practice, the most common way to evaluate the player\u2019s performance is to measure his external pseudo-regret compared to CT (Auer et al., 2002) (which we abbreviate as regret), defined as", "startOffset": 126, "endOffset": 145}, {"referenceID": 1, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 12, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 9, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 3, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 0, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 22, "context": ", (Zinkevich, 2003; CesaBianchi & Lugosi, 2006; Hazan et al., 2006; Blum & Mansour, 2007; Hazan & Kale, 2009)).", "startOffset": 2, "endOffset": 109}, {"referenceID": 11, "context": ", (Zinkevich, 2003; CesaBianchi & Lugosi, 2006; Hazan et al., 2006; Blum & Mansour, 2007; Hazan & Kale, 2009)).", "startOffset": 2, "endOffset": 109}, {"referenceID": 15, "context": "The problem described above seems to be largely overlooked in the online learning literature, with the exception of two important yet isolated papers (Merhav et al., 2002; de Farias & Megiddo, 2006).", "startOffset": 150, "endOffset": 198}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries.", "startOffset": 23, "endOffset": 44}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways.", "startOffset": 23, "endOffset": 345}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition.", "startOffset": 23, "endOffset": 390}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Second, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the fullinformation setting and can be compared to those of Merhav et al. (2002). While Merhav et al.", "startOffset": 23, "endOffset": 702}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Second, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the fullinformation setting and can be compared to those of Merhav et al. (2002). While Merhav et al. (2002) present one concrete algorithm with a policy regret bound, we show a general technique that endows any existing bandit algorithm with a policy regret bound.", "startOffset": 23, "endOffset": 730}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Second, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the fullinformation setting and can be compared to those of Merhav et al. (2002). While Merhav et al. (2002) present one concrete algorithm with a policy regret bound, we show a general technique that endows any existing bandit algorithm with a policy regret bound. Despite the wider scope of our result, our proofs are simpler and shorter than those in Merhav et al. (2002) and our bound is just as good.", "startOffset": 23, "endOffset": 996}, {"referenceID": 21, "context": "There are recent extensions (Yu et al., 2009; Neu et al., 2010) to the partial feedback or bandit setting but they either give asymptotic rates or make even more stringent assumptions on the underlying state transition dynamics.", "startOffset": 28, "endOffset": 63}, {"referenceID": 17, "context": "There are recent extensions (Yu et al., 2009; Neu et al., 2010) to the partial feedback or bandit setting but they either give asymptotic rates or make even more stringent assumptions on the underlying state transition dynamics.", "startOffset": 28, "endOffset": 63}, {"referenceID": 19, "context": "Specifically, the PAC-MDP framework (Szepesv\u00e1ri, 2010, section 2.4.2) models the player\u2019s state on each round; typically, there is a finite number S of states and the player\u2019s actions both incur a loss and cause him to transition from one state to another. The PAC-MDP bounds typically hold when the comparison is with all k policies (mappings from states to actions), not just the k constant-action policies. Our work is still substantially different from RL. The state transitions in RL are often assumed to be stochastic, whereas our setting is adversarial. An adversarial variant of the MDP setting was studied in Even-Dar et al. (2009), however, it assumes that all loss functions across all states are observed by the player.", "startOffset": 37, "endOffset": 641}, {"referenceID": 1, "context": "For example, the popular EXP3 algorithm (Auer et al., 2002) for the k-armed bandit problem maintains a distribution (p1,t, .", "startOffset": 40, "endOffset": 59}, {"referenceID": 8, "context": ", Dekel et al. (2011)).", "startOffset": 2, "endOffset": 22}, {"referenceID": 1, "context": "We begin with the EXP3 algorithm (Auer et al., 2002) in the classic k-armed bandit setting, with its regret bound of \u221a 7Jk log k against any sequence of J loss functions generated by an adaptive adversary.", "startOffset": 33, "endOffset": 52}, {"referenceID": 1, "context": "The policy regret of the mini-batched version of the EXP3 algorithm (Auer et al., 2002), with batch size \u03c4 = (7k log k)\u22121/3T , against an m-memory bounded adaptive adversary, is upper bounded by", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "The policy regret of the mini-batched version of Flaxman, Kalai, and McMahan\u2019s algorithm (Flaxman et al., 2005), with batch size \u03c4 = (18d( \u221a LD + 1))\u22124/5T , against an m-memory bounded adaptive adversary is upper bounded by", "startOffset": 89, "endOffset": 111}, {"referenceID": 9, "context": "The algorithm and analysis in Flaxman et al. (2005) guarantees a regret bound of 18d( \u221a LD+1)J against any sequence of J loss functions generated by an adaptive adversary, where d is the dimension, D is the diameter of X , and L is the Lipschitz coefficient of the loss functions.", "startOffset": 30, "endOffset": 52}, {"referenceID": 0, "context": "We use the algorithm of Abernethy, Hazan, and Rakhlin (Abernethy et al., 2008), whose regret bound is 16n \u221a \u03b8J log J when J > 8\u03b8 log J , for any sequence of J loss functions generated by an oblivious adversary.", "startOffset": 54, "endOffset": 78}, {"referenceID": 0, "context": "algorithm (Abernethy et al., 2008) where the first m loss values in each mini-batch are ignored, with batch size \u03c4 = m(16n \u221a \u03b8 log T )\u22122/3T , against an mmemory-bounded adaptive adversary, is upper bounded for all T > 8\u03b8 log T by", "startOffset": 10, "endOffset": 34}, {"referenceID": 1, "context": "We now redefine CT to include all piece-wise constant sequences with at most s switches (Auer et al., 2002).", "startOffset": 88, "endOffset": 107}, {"referenceID": 1, "context": "In the k-armed bandit setting, (Auer et al., 2002) defines an algorithm named EXP3.", "startOffset": 31, "endOffset": 50}], "year": 2012, "abstractText": "Online learning algorithms are designed to learn even when their input is generated by an adversary. The widely-accepted formal definition of an online algorithm\u2019s ability to learn is the game-theoretic notion of regret. We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm\u2019s actions. We define the alternative notion of policy regret, which attempts to provide a more meaningful way to measure an online algorithm\u2019s performance against adaptive adversaries. Focusing on the online bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. On the other hand, if the adversary\u2019s memory is bounded, we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound. We extend this result to other variants of regret, such as switching regret, internal regret, and swap regret.", "creator": "TeX"}}}