{"id": "1206.6401", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Consistent Multilabel Ranking through Univariate Losses", "abstract": "We consider the problem of rank loss minimization in the setting of multilabel classification, which is usually tackled by means of convex surrogate losses defined on pairs of labels. Very recently, this approach was put into question by a negative result showing that commonly used pairwise surrogate losses, such as exponential and logistic losses, are inconsistent. In this paper, we show a positive result which is arguably surprising in light of the previous one: the simpler univariate variants of exponential and logistic surrogates (i.e., defined on single labels) are consistent for rank loss minimization. Instead of directly proving convergence, we give a much stronger result by deriving regret bounds and convergence rates. The proposed losses suggest efficient and scalable algorithms, which are tested experimentally.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (416kb)", "http://arxiv.org/abs/1206.6401v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["krzysztof dembczynski", "wojciech kotlowski", "eyke h\u00fcllermeier"], "accepted": true, "id": "1206.6401"}, "pdf": {"name": "1206.6401.pdf", "metadata": {"source": "META", "title": "Consistent Multilabel Ranking through Univariate Loss Minimization", "authors": ["Krzysztof Dembczy\u0144ski", "Wojciech Kot lowski", "Eyke H\u00fcllermeier"], "emails": ["kdembczynski@cs.put.poznan.pl", "wkotlowski@cs.put.poznan.pl", "eyke@informatik.uni-marburg.de"], "sections": [{"heading": "1. Introduction", "text": "The problem of multi-label classification (MLC) has received increasing attention in machine learning research in recent years (Schapire & Singer, 2000; Elisseeff & Weston, 2001; Dekel et al., 2003; Dembczyn \u0301 ski et al., 2010). Unlike traditional (single-label) classifications, where each instance is associated with a unique class label, MLC allows for an instance that belongs to several classes at the same time. In other words, the ground truth is now a subset of positive labels instead of a single label. Accordingly, more complex models need to be trained for predictable purposes."}, {"heading": "2. Multilabel Classification", "text": "In this section, we explain the MLC problem more formally and introduce the notation used on the entire paper. Let X denote an instance space and let L = {\u03bb1, \u03bb2,..) be a finite set of class labels. We assume that an instance x-X (not deterministic) is associated with a subset of labels L-2L; this subset is often referred to as a set of relevant (positive) labels, while the L-L complement for x is considered irrelevant (negative). We identify a set of L relevant labels with a binary vector y = (y1, y2,..., ym) in which yi = 1 iff-II-L-L is denoted."}, {"heading": "2.1. Loss, Risk and Regret", "text": "The predictive accuracy of h is measured by its risk, that is, its expected (classification) lossL (h, P) = E ['(Y, h (X)]] = E' (y, h (x)) dP (x, y), (1) where \": Y \u00b7 Rm \u2192 R is a loss function. Furthermore, it will be convenient to use an expected loss conditioned on an instance of x (h, P | x) = E ['(Y, h (x) | x] =\" y-Y \"(y, h (x) P (y | x), so that L (h, P) = E [L (h, P | X)]. The risk of a classification is not always a good indicator of its true performance, as it does not take into account the severity of the problem."}, {"heading": "2.2. Rank Loss", "text": "In this work, we focus on rank loss, which is one of the most important loss functions in MLC and has attracted a lot of attention in recent years (Dembczyn \"ski et al., 2010; Gao & Zhou, 2011):\" rnk (y, h) = w (i, j): yi > yj (Jhi < hjK + 12 Jhi = hjK), (4) where J \u00b7 K is the standard {false, true} \u2192 {0, 1} mapping (for clarity's sake, we will suppress dependence on x in the notation whenever it is clear from the context). If the performance of the classification is treated as ranking, the rank loss compares the true label with this ranking, in which all relevant labels are ideally preceded by all irrelevant ones. Specifically, the rank loss counts the number of label pairs that violate this condition and multiplies it with a positive weight (y)."}, {"heading": "3. Main Result", "text": "We are preparing our main result to be presented in Section 3.3 with two auxiliary results. First, in Section 3.1, we show that rank repentance depends solely on the limit weights \u2206 ui and that we may replace the original distribution P with any other distribution P \u2032 as long as they both result in the same limit weights \u0445 ui. In particular, we can choose P \u2032 for which labels are (conditionally) independent. Second, in Section 3.2, we provide the basic argument for the use of universal loss functions, which shows that such losses, assuming independence, are sufficient for the uniform ranking of objects. This result will not be presented directly for MLC, but in connection with the related problem of two-sided ranking. Therefore, the final step in Section 3.3 is to transfer this result to the determination of the MLC using the trick of Section 3.1 and the fact that expected univariate losses depend only on the distribution by the limit weights."}, {"heading": "3.1. Label Dependence Does Not Influence Rank Regret", "text": "The main problem in the analysis of regret (3) is the conditional dependence of the labels indicated with x. However, as already mentioned, this dependence does not seem to play an important role in minimizing regret. In the following, we will clarify this observation by showing that the regret of the ranking depends exclusively on the limit weights."}, {"heading": "3.2. Univariate Loss Minimization is Sufficient under the Assumption of Independence", "text": "The split ranking problem (Cohen et al., 1999; Cle \"menc\" on et al., 2008; Kot \"lowski et al., 2011) is in a sense between MLC and standard binary classification. As in the latter case, there is only one binary classification problem, but as in the MLC, performance is measured in terms of rank loss rather than classification errors. However, the problem is to classify the instances themselves. More specifically, consider a simple binary classification problem with educational examples (x, y\"): \"X\" -1, + 1. \"A classifier h\" is a real rated function h. \""}, {"heading": "3.3. Minimizing Rank Loss in MLC", "text": "The exponential loss and logistical loss shown above are usually used in the standard classification. A simple extension of these losses to the MLC classification (1) results in the following (1): \"exp (y, h) = w (y) m.\" (11) \"log (y, h)\" (y, h) m. \"(12) Minimizing these losses leads to solvingm independent classification problems, one for each label. Any algorithm for classification with exponential or logistic surrogates, such as AdaBoost or logistic regression, can be used for this purpose. Despite its simplicity and efficiency, this approach offers a consistent way of minimizing losses, as shown by the following results."}, {"heading": "4. Relationship to Prior Work", "text": "This section is intended to look at the Gao & Zhou (2011) result in the light of our results to date, attempting to support a more intuitive understanding. (As previously mentioned, these authors consider pairs of convex replacement losses of the form '\u03c6 (y, h) = \u2211 (i, j): yi > yjw (y) \u03c6 (hi \u2212 hj), (18) where this result is a convex, differentiated, non-linear and non-increasing function and show that no such loss is consistent for multilabel ranking. Considering the existence of pairwise losses that are actually consistent for the split ranking, this result seems surprising at first glance, all the more so since we use in our evidence a reduction to the split ranking, too.The reason for the inconsistency becomes clearer when we consider the conditional expected loss: L\u03c6 (h, P | x) = j > j = 10ij."}, {"heading": "5. Empirical Results", "text": "We have measured the performance of the algorithms in terms of loss (4) with weights defined as: w (y) = (m \u2212 sy) \u2212 1, where they are between 0 (perfect order) and 1 (reverse order).The main objective of the experiment is to check whether the simple algorithms based on universal losses (11) and (12) compete with the state-of-the-art procedures that minimize the loss of rank."}, {"heading": "5.1. Synthetic Data", "text": "The model is based on Latin variables f = (f1, f2, fm): f = Ax +, where there is a two-dimensional vector taken from a Latin hard disk, is a matrix of linear coefficients, and it is a m-dimensional vector. The labels are pulled up from the Latin variables."}, {"heading": "5.2. Benchmark Data", "text": "We also conducted an experiment with commonly used benchmark datasets.4 We selected 4 medium-sized datasets with about 10 labels and a large dataset with 101 labels and more than 30K training examples. The datasets are described in Table 1. To facilitate comparison of the results presented in this paper, we used the original split for training and test sets. The results are shown in Table 2, again separately for 3AdaBoost.MR behaves rather strangely on these datasets, as it quickly goes too far for more than 20 stumps. Therefore, we also have the number of stumps in WBRAdaBoost.4 These datasets come from MULAN http: / / mulan.sourceforge.net / datasets.html and LibSVM http: / / www.csie.ntu.edu.tw / ~ cjmtools / libsvmtools / multipendel.html repositional exportation and logistic losses."}, {"heading": "6. Conclusions", "text": "In this paper, we have shown that univariate convex surrogates are consistent for a multi-level ranking. For several reasons, our results should be of interest to the machine learning community, especially because they are quite surprising in the light of (Gao & Zhou, 2011), which shows inconsistency for the most popular paired surrogates. On the more practical side, our results motivate simple and scalable algorithms for a multi-level ranking, which are simply variations of standard classification algorithms (such as logistic regression or AdaBoost).It is known that Krzysztof Dembczyn'ski is supported by the Foundation for Polish Science under the Homing Plus programme, which is co-funded by the European Regional Development Fund. Wojciech Kotlowski is supported by the German Foundation for Education (DFG 91) and the Polish Foundation for Higher Education."}], "references": [{"title": "Convexity, classification, and risk bounds", "author": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Ranking and empirical minimization of U-statistics", "author": ["S. Cl\u00e9men\u00e7on", "L. G\u00e1bor", "N. Vayatis"], "venue": "Annals of Statistics,", "citeRegEx": "Cl\u00e9men\u00e7on et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on et al\\.", "year": 2008}, {"title": "Learning to order things", "author": ["W.W. Cohen", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 1999}, {"title": "Log-linear models for label ranking", "author": ["O. Dekel", "Manning", "Ch", "Y. Singer"], "venue": "In NIPS,", "citeRegEx": "Dekel et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2003}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["K. Dembczy\u0144ski", "W. Cheng", "E. H\u00fcllermeier"], "venue": "In ICML, pp", "citeRegEx": "Dembczy\u0144ski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dembczy\u0144ski et al\\.", "year": 2010}, {"title": "On the consistency of ranking algorithms", "author": ["J. Duchi", "L. Mackey", "M. Jordan"], "venue": "In ICML, pp", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "A kernel method for multilabelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "In NIPS, pp", "citeRegEx": "Elisseeff and Weston,? \\Q2001\\E", "shortCiteRegEx": "Elisseeff and Weston", "year": 2001}, {"title": "On the consistency of multi-label learning", "author": ["W. Gao", "Z. Zhou"], "venue": "In COLT, pp", "citeRegEx": "Gao and Zhou,? \\Q2011\\E", "shortCiteRegEx": "Gao and Zhou", "year": 2011}, {"title": "Bipartite ranking through minimization of univariate loss", "author": ["W. Kot lowski", "K. Dembczy\u0144ski", "E. H\u00fcllermeier"], "venue": "In ICML, pp", "citeRegEx": "lowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "lowski et al\\.", "year": 2011}, {"title": "BoosTexter: A Boostingbased System for Text Categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q2000\\E", "shortCiteRegEx": "Schapire and Singer", "year": 2000}], "referenceMentions": [{"referenceID": 3, "context": "The problem of multilabel classification (MLC) has received increasing attention in machine learning research in recent years (Schapire & Singer, 2000; Elisseeff & Weston, 2001; Dekel et al., 2003; Dembczy\u0144ski et al., 2010).", "startOffset": 126, "endOffset": 223}, {"referenceID": 4, "context": "The problem of multilabel classification (MLC) has received increasing attention in machine learning research in recent years (Schapire & Singer, 2000; Elisseeff & Weston, 2001; Dekel et al., 2003; Dembczy\u0144ski et al., 2010).", "startOffset": 126, "endOffset": 223}, {"referenceID": 5, "context": "Interestingly, this approach has recently been called into question by Gao & Zhou (2011) (following the earlier results of Duchi et al. (2010)), who showed that the most commonly used convex surrogates of that kind are inconsistent.", "startOffset": 123, "endOffset": 143}, {"referenceID": 4, "context": "In this paper, we focus on the rank loss, which is among the most important loss functions in MLC and has attracted much attention in recent years (Dembczy\u0144ski et al., 2010; Gao & Zhou, 2011):", "startOffset": 147, "endOffset": 191}, {"referenceID": 4, "context": "In order to understand its meaning, it is convenient to consider the special case w(y) \u2261 1, in which the \u2206-values reduce to conditional probabilities (Dembczy\u0144ski et al., 2010).", "startOffset": 150, "endOffset": 176}, {"referenceID": 2, "context": "The bipartite ranking problem (Cohen et al., 1999; Cl\u00e9men\u00e7on et al., 2008; Kot lowski et al., 2011) is in a sense in-between MLC and standard binary classification.", "startOffset": 30, "endOffset": 99}, {"referenceID": 1, "context": "The bipartite ranking problem (Cohen et al., 1999; Cl\u00e9men\u00e7on et al., 2008; Kot lowski et al., 2011) is in a sense in-between MLC and standard binary classification.", "startOffset": 30, "endOffset": 99}, {"referenceID": 1, "context": "This is a non-normalized version of the rank loss, which is more useful for our purposes; in the literature, it is common to use a normalized version, which differs from the non-normalized one by a product of class priors (Cl\u00e9men\u00e7on et al., 2008).", "startOffset": 222, "endOffset": 246}, {"referenceID": 0, "context": "The answer is negative: Bartlett et al. (2006) already showed for binary classification (which can be casted as a special MLC ranking problem) that the square-root bound is unavoidable in the worst case.", "startOffset": 24, "endOffset": 47}, {"referenceID": 5, "context": "We note that Gao & Zhou (2011) (following Duchi et al. (2010)) showed consistency (but not regret bounds and convergence rates) of some specific pairwise surrogate losses, one of which can be rewritten as a univariate linear surrogate with regularization.", "startOffset": 42, "endOffset": 62}, {"referenceID": 3, "context": "MR (Schapire & Singer, 2000) and log-linear models for label ranking (LLLR) (Dekel et al., 2003).", "startOffset": 76, "endOffset": 96}], "year": 2012, "abstractText": "We consider the problem of rank loss minimization in the setting of multilabel classification, which is usually tackled by means of convex surrogate losses defined on pairs of labels. Very recently, this approach was put into question by a negative result showing that commonly used pairwise surrogate losses, such as exponential and logistic losses, are inconsistent. In this paper, we show a positive result which is arguably surprising in light of the previous one: the simpler univariate variants of exponential and logistic surrogates (i.e., defined on single labels) are consistent for rank loss minimization. Instead of directly proving convergence, we give a much stronger result by deriving regret bounds and convergence rates. The proposed losses suggest efficient and scalable algorithms, which are tested experimentally.", "creator": "LaTeX with hyperref package"}}}