{"id": "1206.6402", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization", "abstract": "Can one parallelize complex exploration exploitation tradeoffs? As an example, consider the problem of optimal high-throughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multi-armed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor independent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (462kb)", "http://arxiv.org/abs/1206.6402v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["thomas desautels", "andreas krause 0001", "joel w burdick"], "accepted": true, "id": "1206.6402"}, "pdf": {"name": "1206.6402.pdf", "metadata": {"source": "META", "title": "Parallelizing Exploration\u2013Exploitation Tradeoffs with Gaussian Process Bandit Optimization", "authors": ["Thomas Desautels", "Andreas Krause"], "emails": ["tadesaut@caltech.edu", "krausea@ethz.ch", "jwb@robotics.caltech.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is that most of us are able to surpass ourselves, and that they do so. (...) It is not as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they are able to surpass themselves. (...) It is as if they surpass themselves. (...) It is as if they surpass themselves. (...) It is as if they surpass themselves. (...) It is as if they surpass themselves. (...) It is as if they surpass themselves. (...) It is as if they surpass themselves. (...) It is as if they surpass themselves. (...) It is as if they surpass themselves. (...) It is so. (...) It is. (...) It is. (...) It is. (...) It is. (... It is. (...) It is. (... It is. (...) It is. () It is. (... It is. It is. (... It is.) It is. (... It is. It is. (... It is.) It is. (... It is. It is. It is. (... It is. It is. It is. (... It is.) It is. It is. (... It is. It is. \"It is."}, {"heading": "2. Problem Statement and Background", "text": "We want to make a sequence of decisions x1, x2,., xT, D, where D is referred to as a decision set, which is often (but not necessarily) a compact subset of Rd. For each decision we make, we can create a realistic scalar reward y1, y2,.., yT, where for each t, yt = f (xt) + \u03b5t and where f: D \u2192 R is in turn an unknown function that models the expected payout f (x) for each decision x. For now, we assume that the error variables \u03b5t i.d.d. Gaussian with known variance \u03c32n, i.e., \u03b5t \u00b2 N (0, \u03c32n). We will relax this assumption later, we allow xt on observations y1: t \u2212 1 associated with x1,., xt \u2212 1. Below, we will address the main problem in this paper: the challenging setting where xt can only depend on y1."}, {"heading": "4. Regret Bounds", "text": "Srinivas et al. (2010) prove that the cumulative value of the strictly sequential GP-UCB (up to logarithmic factors) is as RT = O-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A."}, {"heading": "5. Experiments", "text": "In fact, it is the case that one will be able to go to a place where one can go to a place, where one can go to another world, where one can go in search."}, {"heading": "6. Conclusions", "text": "We have developed the GP-BUCB algorithm for parallelizing exploration and exploitation of trade-offs in Gaussian process bandit optimization. We have demonstrated how GP-BUCB regret can be limited by an intuitive conditional mutual set of information. Based on this analysis, we demonstrate that GP-BUCB can be \"initialized\" to achieve regret limits that only additively depend on the batch size for many commonly used core functions. We also show how \"lazy\" variance evaluations can lead to improvements in order of magnitude in runtime. In our experiments, GP-BUCB compares favorably with the state of the art in parallel Bajean optimization that does not come with theoretical guarantees. We believe that our results represent an important step toward solving complex, large-scale exploration and exploitation actions."}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["J. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Batch bayesian optimization via simulation matching", "author": ["J. Azimi", "A. Fern", "X.Fern"], "venue": "In NIPS,", "citeRegEx": "Azimi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Azimi et al\\.", "year": 2010}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "M. Cora", "N. de Freitas"], "venue": "In TR-2009-23,", "citeRegEx": "Brochu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2009}, {"title": "Online optimization in X-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Bubeck et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2008}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In ALT,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Sdo: A statistical method for global optimization", "author": ["D.D. Cox", "S. John"], "venue": "Multidisciplinary Design Optimization: State of the Art,", "citeRegEx": "Cox and John,? \\Q1997\\E", "shortCiteRegEx": "Cox and John", "year": 1997}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "In COLT,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Efficient optimal learning for contextual bandits", "author": ["M. Dudik", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang"], "venue": "In UAI,", "citeRegEx": "Dudik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2011}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D.R. Jones", "M. Schonlau", "W.J. Welch"], "venue": "J Glob. Opti.,", "citeRegEx": "Jones et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1998}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "In STOC, pp", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "Bandit based montecarlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In ECML,", "citeRegEx": "Kocsis and Szepesv\u00e1ri,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri", "year": 2006}, {"title": "Automatic gait optimization with Gaussian process regression", "author": ["D. Lizotte", "T. Wang", "M. Bowling", "D. Schuurmans"], "venue": "In IJCAI, pp", "citeRegEx": "Lizotte et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lizotte et al\\.", "year": 2007}, {"title": "Accelerated greedy algorithms for maximizing submodular set functions", "author": ["M. Minoux"], "venue": "Optimization Techniques,", "citeRegEx": "Minoux,? \\Q1978\\E", "shortCiteRegEx": "Minoux", "year": 1978}, {"title": "Bayesian Approach to Global Optimization", "author": ["J. Mockus"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Mockus,? \\Q1989\\E", "shortCiteRegEx": "Mockus", "year": 1989}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bul. Am. Math. Soc.,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "In ICML,", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Inferring latent task structure for multitask learning by multiple kernel learning", "author": ["C. Widmer", "N. Toussaint", "Y. Altun", "G. R\u00e4tsch"], "venue": "BMC Bioinformatics,", "citeRegEx": "Widmer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Widmer et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "Our approach generalizes the GP-UCB approach (Srinivas et al., 2010) to the parallel setting.", "startOffset": 45, "endOffset": 68}, {"referenceID": 16, "context": "Related Work Classical work on multi-armed bandit problems has focused on the case of a finite number of decisions (Robbins, 1952).", "startOffset": 115, "endOffset": 130}, {"referenceID": 1, "context": "Optimistic allocation according to upper-confidence bounds (UCB) on the payoffs has proven to be particularly effective (Auer et al., 2002).", "startOffset": 120, "endOffset": 139}, {"referenceID": 7, "context": "Examples include bandits with linear (Dani et al., 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al.", "startOffset": 37, "endOffset": 80}, {"referenceID": 0, "context": "Examples include bandits with linear (Dani et al., 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al.", "startOffset": 37, "endOffset": 80}, {"referenceID": 10, "context": ", 2008) or Lipschitz-continous payoffs (Kleinberg et al., 2008), or bandits on trees (Kocsis & Szepesv\u00e1ri, 2006; Bubeck et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 4, "context": ", 2008), or bandits on trees (Kocsis & Szepesv\u00e1ri, 2006; Bubeck et al., 2008).", "startOffset": 29, "endOffset": 77}, {"referenceID": 3, "context": "The exploration-exploitation tradeoff has also been studied in Bayesian global optimization and response surface modeling, where Gaussian process models are often used due to their flexibility in incorporating prior assumptions about the payoff function (Brochu et al., 2009).", "startOffset": 254, "endOffset": 275}, {"referenceID": 9, "context": "Several heuristics, such as Maximum Expected Improvement (Jones et al., 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 14, "context": ", 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al.", "startOffset": 44, "endOffset": 58}, {"referenceID": 12, "context": ", 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al., 2007).", "startOffset": 221, "endOffset": 243}, {"referenceID": 0, "context": ", 2008; Abernethy et al., 2008) or Lipschitz-continous payoffs (Kleinberg et al., 2008), or bandits on trees (Kocsis & Szepesv\u00e1ri, 2006; Bubeck et al., 2008). The exploration-exploitation tradeoff has also been studied in Bayesian global optimization and response surface modeling, where Gaussian process models are often used due to their flexibility in incorporating prior assumptions about the payoff function (Brochu et al., 2009). Several heuristics, such as Maximum Expected Improvement (Jones et al., 1998), Maximum Probability of Improvement (Mockus, 1989), and upperconfidence based methods (Cox & John, 1997), have been developed to balance exploration with exploitation and successfully applied in learning problems (Lizotte et al., 2007). Recently, Srinivas et al. (2010) analyzed GP-UCB, an upper-confidence bound sampling based algorithm for this setting, and proved bounds on its cumulative regret, and thus convergence rates for Bayesian global optimization.", "startOffset": 8, "endOffset": 784}, {"referenceID": 7, "context": "Only recently, Dudik et al. (2011) demonstrated that it is possible to obtain regret bounds that only increase additively with the delay (i.", "startOffset": 15, "endOffset": 35}, {"referenceID": 7, "context": "Only recently, Dudik et al. (2011) demonstrated that it is possible to obtain regret bounds that only increase additively with the delay (i.e., the penalty becomes negligible for large numbers of decisions). However, the approach of Dudik et al. only applies to contextual bandit problems with finite decision sets, and thus not to settings with complex (even nonparametric) payoff functions. In contrast, there has been heuristic work in parallel Bayesian global optimization using GPs, e.g. by Ginsbourger et al. (2010). The state of the art is the simulation matching algorithm of Azimi et al.", "startOffset": 15, "endOffset": 522}, {"referenceID": 2, "context": "The state of the art is the simulation matching algorithm of Azimi et al. (2010). To our knowledge, no theoretical results regarding the regret of this algorithm exist.", "startOffset": 61, "endOffset": 81}, {"referenceID": 4, "context": ", RT /T \u2265 mint [f(x\u2217)\u2212 f(xt)] (often called the simple regret, Bubeck et al. (2009)).", "startOffset": 63, "endOffset": 84}, {"referenceID": 17, "context": "Recently, Srinivas et al. (2010) analyzed the Gaussian process Upper Confidence Bound (GPUCB) selection rule", "startOffset": 10, "endOffset": 33}, {"referenceID": 17, "context": "Srinivas et al. (2010) showed that, Algorithm 1 GP-BUCB", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "This idea generalizes to the bandit setting a technique proposed by Minoux (1978), which concerns calculating the greedy action for submodular maximization and leads to dramatically improved empirical computational speed, discussed in Section 5.", "startOffset": 68, "endOffset": 82}, {"referenceID": 17, "context": "Under this condition, a straightforward generalization of the arguments of Srinivas et al. (2010) leads to regret bounds of the form O\u2217( \u221a T\u03b2T \u03b3T ) (formal statement is given below).", "startOffset": 75, "endOffset": 98}, {"referenceID": 17, "context": "We can then leverage the machinery of Srinivas et al. (2010) to derive our regret bound below.", "startOffset": 38, "endOffset": 61}, {"referenceID": 17, "context": "Fortunately, Srinivas et al. (2010) prove bounds on how the information gain \u03b3T grows for some of the most commonly used kernels.", "startOffset": 13, "endOffset": 36}, {"referenceID": 17, "context": "For sake of notation, define R T to be the regret bound of Srinivas et al. (2010) associated with the sequential GP-UCB algorithm (i.", "startOffset": 59, "endOffset": 82}, {"referenceID": 2, "context": "We compare it with four alternatives: (1) The strictly sequential GP-UCB algorithm (B = 1); (2) NRB-UCB, an approach that simply picks the maximizer of the GP-UCB score B times; (3) NTB-UCB, an approach that picks the top B scores according to the GP-UCB criterion; (4) A state of the art algorithm for Batch Bayesian optimization proposed by Azimi et al. (2010), which can use either a UCB or Maximum Expected Improvement (MEI) decision rule, herein SM-UCB and SM-MEI respectively.", "startOffset": 343, "endOffset": 363}, {"referenceID": 18, "context": "Automated Vaccine Design We also tested GPBUCB on a database of Widmer et al. (2010), which describes the binding affinity of various peptides with a Major Histocompatibility Complex (MHC) Class I molecule, of importance when designing vaccines to exploit peptide binding properties.", "startOffset": 64, "endOffset": 85}], "year": 2012, "abstractText": "Can one parallelize complex exploration\u2013 exploitation tradeoffs? As an example, consider the problem of optimal highthroughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multiarmed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor independent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications.", "creator": "LaTeX with hyperref package"}}}