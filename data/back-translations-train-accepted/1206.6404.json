{"id": "1206.6404", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Policy Gradients with Variance Related Risk Criteria", "abstract": "Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-to-go in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (482kb)", "http://arxiv.org/abs/1206.6404v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CY math.OC stat.ML", "authors": ["dotan di castro", "aviv tamar", "shie mannor"], "accepted": true, "id": "1206.6404"}, "pdf": {"name": "1206.6404.pdf", "metadata": {"source": "META", "title": "Policy Gradients with Variance Related Risk Criteria", "authors": ["Aviv Tamar", "Dotan Di Castro", "Shie Mannor"], "emails": ["avivt@tx.technion.ac.il", "dot@tx.technion.ac.il", "shie@ee.technion.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Both Reinforcement Learning (RL; Bertsekas & Tsitsiklis, 1996) and Planning in Markov Decision Processes (MDPs; Puterman, 1994), the typical goal is to maximize the cumulative (possibly discounted) expected reward designated by J. When the parameters of the model are known, several well-established and efficient optimization algorithms are known. If the model parameters are not known, learning is necessary and there are several algorithmic frameworks that efficiently solve the learning problem, at least when the model is finite. However, in many applications, the decision maker is also interested in minimizing some form of the risk of politics."}, {"heading": "2. Framework and Background", "text": "In this section we present the framework considered in this paper and explain the difficulty in optimizing the mean variance."}, {"heading": "2.1. Definitions and Framework", "text": "We consider an agent interacting with an unknown environment defined by an MDP in discrete time with a finite set of states of 0 x. (1,.) Each selected action in a state x, which depends only on the current state x, X, determines a stochastic transition to the next state. (1,.) The agent maintains a parameterized political function, which is generally a probability function designated by a state x, x. (2,.) Mapping a state x, X into a probability distribution via the controls U. The parameter we use is a table parameter, and we assume it is a differentiable function."}, {"heading": "2.2. The Challenges of Trajectory-Variance Problems", "text": "As already recognized by Sobel (1982), the optimization of the mean-variance trade-off in MDPs cannot be solved with traditional dynamic programming methods such as policy iteration. Mannor & Tsitsiklis showed that the solution of problem (3) is generally a hard problem even in the case of a finite horizon. One reason for the severity of the problem is that, as suggested by Mannor & Tsitsiklis, the underlying optimization problem is not necessarily convex. In the following, we give an example where the set of all (J (x), V (x)) pairs spanning all possible strategies is not convex. Consider the following symmetric deterministic MDP with 8 states X = {x, x2a, x2b, x2c, x2d, 2c, 2) measures that we do not convex, but only convex."}, {"heading": "3. Formulae for the Trajectory Variance and its Gradient", "text": "In this section, we present formulas for the mean and variance of the accumulated reward problem between the visits of J and J. (The key point in our approach is the following observation: (1), a transition to x and a variance of the accumulated reward between the visits of J and J (2). (2), therefore, the following equation can be written for the value of J (x) (1), and in the following example, we show that these equations are solvable, the expressions for J and V giving 3.1. Let P be a stochastic matrix corresponding to a policy of satisfactory assumption 2.1, in which his (i, j) -th input is the transition from state i to state j. Define P is equal to a matrix."}, {"heading": "4. Gradient Based Algorithms", "text": "In this section, we derive gradient-based algorithms for solving problems (3) and (4). We present both exact algorithms that can be practicable for small problems, and simulation-based algorithms for larger problems. Our algorithms deal with constraints due to the penalty method described in the following section."}, {"heading": "4.1. Penalty methods", "text": "One approach to solving restricted optimization problems (COPs) such as (3) is to convert the COP into an equivalent, unrestricted problem that can be solved using standard, unrestricted optimization techniques, commonly known as 3http: / / tx.technion.ac.il / ~ avivt / icml12supp.pdfpenalty methods, which add a penalty term for unfeasibility, making unfeasible solutions suboptimal. Formally, for a COPmax f (x), s.t. c (x) \u2264 0, (9) we define an unfeasible problem max f (x) \u2212 \u03bbg (c (x)), (10) where g (x) is the tightening function, typically assumed to be g (x) = (max (0, x) 2, and f > 0 is the penalty coefficient."}, {"heading": "4.2. Exact Gradient Algorithm", "text": "If the MDP transitions are known, the expressions for the gradient in Lemma 3.2 can immediately be inserted into a gradient ascending algorithm for the following punished objective function of the problem (3) f\u03bb = J (x *) -\u03bbg (V (x *) -b). Let \u03b1k designate a sequence of positive step sizes. (11) Let us make the following assumption about the smoothness of the objective function and about setting your local Optima. 4assumption 4.1. For all promisculations, RK\u03b8 and \u03bb > 0, the objective function is x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "4.3. Simulation based optimization", "text": "If a simulator of MDP dynamics is available, it is possible to obtain unbiased estimates of the courses J (+ J) and J (+ J). (+ J) The following simulator gives the necessary course estimates for our Case.Lemma 4.2.) We have all policy course algorithms (Baxter & Bartlett, 2001; Marbach & Tsitsiklis, 1998). (The following simulator gives the necessary course estimates for our Case.Lemma 4.2. (x) = E [RV \u2212 10). (xD). (x0 = x). (X). (X). (X). (X). (X). (X). (X)."}, {"heading": "5. Experiments", "text": "In this section, we apply the simulation algorithms of Section 4 to a portfolio management problem where the available investment options comprise both liquid and non-liquid assets. In order to understand the performance of the various algorithms, we are looking at a rather simplistic model of the corresponding financial problem. We stress that dealing with richer models does not require algorithms to change. We are looking at a portfolio consisting of two types of assets (e.g. liquid bonds or options) that has a time-dependent interest rate (e.g. short-term T-bills), which can only be sold after a term of N-increments. In addition, there is a risk that the non-liquid assets will not be paid (i.e. a default) with a probability of prisk. A common investment strategy in this setup is the splitting of the investment into non-liquid assets that are regularly invested."}, {"heading": "6. Conclusion", "text": "This paper presented a novel algorithmic approach to RL with variance-related risk criteria, an issue that is important for many applications but known to pose significant algorithmic challenges. Since achieving an optimal solution appears difficult even when the model is known, we chose a gradient-based approach that achieves local optimalities, and some questions require further investigation. First, we note a possible extension to other risk measures such as the percentile criterion (Delage & Mannor, 2010), which requires a result reminiscent of Proposition 3.1 that would allow us to advance optimization. Second, we could consider variance in the optimization process to improve control-style convergence time. Political gradient algorithms are known to suffer from high variance when the recurrent state is not frequently visited. One technique to deal with this difficulty is the use of control variants (Greensmith et al, 2004)."}, {"heading": "Acknowledgements", "text": "The research that led to these results was funded by the Seventh Framework Programme of the European Union (FP7 / 2007-2013) under PASCAL2 (PUMP PRIMING) grant No. 216886 and the Marie Curie Reintegration Fellowship (IRG) grant No. 249254."}], "references": [{"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific, third edition,", "citeRegEx": "Bertsekas,? \\Q2006\\E", "shortCiteRegEx": "Bertsekas", "year": 2006}, {"title": "Stochastic approximation with two time scales", "author": ["V.S. Borkar"], "venue": "Systems & Control Letters,", "citeRegEx": "Borkar,? \\Q1997\\E", "shortCiteRegEx": "Borkar", "year": 1997}, {"title": "Risk-sensitive optimal control for markov decision processes with monotone cost", "author": ["V.S. Borkar", "S.P. Meyn"], "venue": "Math. Oper. Res.,", "citeRegEx": "Borkar and Meyn,? \\Q2002\\E", "shortCiteRegEx": "Borkar and Meyn", "year": 2002}, {"title": "Percentile optimization for Markov decision processes with parameter uncertainty", "author": ["E. Delage", "S. Mannor"], "venue": "Operations Research,", "citeRegEx": "Delage and Mannor,? \\Q2010\\E", "shortCiteRegEx": "Delage and Mannor", "year": 2010}, {"title": "Percentile performance criteria for limiting average markov decision processes", "author": ["J.A. Filar", "D. Krass", "K.W. Ross"], "venue": "IEEE Trans. Auto. Control,", "citeRegEx": "Filar et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Filar et al\\.", "year": 1995}, {"title": "Risk-sensitive reinforcement learning applied to control under", "author": ["P. Geibel", "F. Wysotzki"], "venue": "constraints. JAIR,", "citeRegEx": "Geibel and Wysotzki,? \\Q2005\\E", "shortCiteRegEx": "Geibel and Wysotzki", "year": 2005}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["E. Greensmith", "P.L. Bartlett", "J. Baxter"], "venue": "JMLR, 5:1471\u20131530,", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Risk-sensitive markov decision processes", "author": ["R.A. Howard", "J.E. Matheson"], "venue": "Management Science,", "citeRegEx": "Howard and Matheson,? \\Q1972\\E", "shortCiteRegEx": "Howard and Matheson", "year": 1972}, {"title": "Convergent multipletimescales reinforcement learning algorithms in normal form games", "author": ["D.S. Leslie", "E.J. Collins"], "venue": "Annals of App. Prob.,", "citeRegEx": "Leslie and Collins,? \\Q2002\\E", "shortCiteRegEx": "Leslie and Collins", "year": 2002}, {"title": "Investment Science", "author": ["D. Luenberger"], "venue": null, "citeRegEx": "Luenberger,? \\Q1998\\E", "shortCiteRegEx": "Luenberger", "year": 1998}, {"title": "Simulation-based optimization of markov reward processes", "author": ["P. Marbach", "J.N. Tsitsiklis"], "venue": "IEEE Trans. Auto. Control,", "citeRegEx": "Marbach and Tsitsiklis,? \\Q1998\\E", "shortCiteRegEx": "Marbach and Tsitsiklis", "year": 1998}, {"title": "Robust control of Markov decision processes with uncertain transition matrices", "author": ["A. Nilim", "L. El Ghaoui"], "venue": "Operations Research,", "citeRegEx": "Nilim and Ghaoui,? \\Q2005\\E", "shortCiteRegEx": "Nilim and Ghaoui", "year": 2005}, {"title": "An analytic solution to discrete bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "In ICML,", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Mutual fund performance", "author": ["W.F. Sharpe"], "venue": "The Journal of Business,", "citeRegEx": "Sharpe,? \\Q1966\\E", "shortCiteRegEx": "Sharpe", "year": 1966}, {"title": "The variance of discounted markov decision processes", "author": ["M.J. Sobel"], "venue": "J. Applied Probability,", "citeRegEx": "Sobel,? \\Q1982\\E", "shortCiteRegEx": "Sobel", "year": 1982}], "referenceMentions": [{"referenceID": 13, "context": "In both Reinforcement Learning (RL; Bertsekas & Tsitsiklis, 1996) and planning in Markov Decision Processes (MDPs; Puterman, 1994), the typical objective is to maximize the cumulative (possibly discounted) expected reward, denoted by J .", "startOffset": 108, "endOffset": 130}, {"referenceID": 9, "context": "(Luenberger, 1998).", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "In financial decision making, a popular performance criterion is the Sharpe Ratio (SR; Sharpe, 1966) \u2013 the ratio between the expected profit and its standard deviation.", "startOffset": 82, "endOffset": 100}, {"referenceID": 4, "context": "Another approach considers the percentile performance criterion (Filar et al., 1995), in which the average reward has to exceed some value with a given probability.", "startOffset": 64, "endOffset": 84}, {"referenceID": 1, "context": "Much less work has been done on risk sensitive criteria within the RL framework, with a notable exception of Borkar & Meyn (2002) who considered exponential utility functions and of Geibel & Wysotzki (2005) who considered models where some states are \u201cerror states,\u201d representing a bad or even catastrophic outcome.", "startOffset": 109, "endOffset": 130}, {"referenceID": 1, "context": "Much less work has been done on risk sensitive criteria within the RL framework, with a notable exception of Borkar & Meyn (2002) who considered exponential utility functions and of Geibel & Wysotzki (2005) who considered models where some states are \u201cerror states,\u201d representing a bad or even catastrophic outcome.", "startOffset": 109, "endOffset": 207}, {"referenceID": 15, "context": "It has long been recognized (Sobel, 1982) that optimization problems such as (a) are not amenable to standard dynamic programming techniques.", "startOffset": 28, "endOffset": 41}, {"referenceID": 15, "context": "As was already recognized by Sobel (1982), optimizing the mean-variance tradeoff in MDPs cannot be solved using traditional dynamic programming methods such as policy iteration.", "startOffset": 29, "endOffset": 42}, {"referenceID": 0, "context": "1 in (Bertsekas, 2006) we have that I \u2212 P \u2217 is invertible.", "startOffset": 5, "endOffset": 22}, {"referenceID": 15, "context": "The analysis in (Sobel, 1982) makes use of the fact that I \u2212 \u03b2P is invertible, therefore an extension of their results to the undiscounted case is not immediate.", "startOffset": 16, "endOffset": 29}, {"referenceID": 15, "context": "We remark that similar equations for the infinite horizon discounted return case were presented by Sobel (1982), in which I\u2212P \u2032 is replaced with I\u2212\u03b2P , where \u03b2 < 1 is the discount factor.", "startOffset": 99, "endOffset": 112}, {"referenceID": 1, "context": "(sketch) The proof relies on representing Equation (13) as a stochastic approximation with two timescales (Borkar, 1997), where J\u0303k and \u1e7ck are updated on a fast schedule while \u03b8k is updated on a slow schedule.", "startOffset": 106, "endOffset": 120}, {"referenceID": 6, "context": "One technique for dealing with this difficulty is by using control variates (Greensmith et al., 2004).", "startOffset": 76, "endOffset": 101}], "year": 2012, "abstractText": "Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-togo in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem.", "creator": "LaTeX with hyperref package"}}}