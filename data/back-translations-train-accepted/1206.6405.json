{"id": "1206.6405", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Bounded Planning in Passive POMDPs", "abstract": "In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (899kb)", "http://arxiv.org/abs/1206.6405v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["roy fox", "naftali tishby"], "accepted": true, "id": "1206.6405"}, "pdf": {"name": "1206.6405.pdf", "metadata": {"source": "META", "title": "Bounded Planning in Passive POMDPs", "authors": ["Roy Fox", "Naftali Tishby"], "emails": ["royf@cs.huji.ac.il", "tishby@cs.huji.ac.il"], "sections": [{"heading": "1. Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1. Passive POMDPs Planning", "text": "In each step, the previous storage state is updated according to the previous storage function, modelling the interaction of an agent with his environment as a discrete time stochastic process. Therefore, the environment goes through a sequence of world states W1,.., Wn in a finite domain W. These states are hidden from the agent except for an observation Ot in a finite domain Ot \u2212 O. These states are hidden from the agent Ot in a finite domain, where the action affects the cost, but not the world state. We assume that the world itself is a Markov chain, with states governed by a time-independent transition probability. (Wt | Wt \u2212 1) and an initial distribution P1 (W1).The agent maintains an internal storage state Mt in a finite domain."}, {"heading": "1.2. Information Constraints", "text": "The sufficiency of the exact faith allows the agent to minimize the external costs, but significant internal costs arise. The amount of information that the agent must keep in the memory can be large, and even any observation can be more than the agent can capture. In any case, not all of this information is equally useful to reduce external costs. Generally, the agent's information processing capacity can be limited in two ways: 1. The capacity of the agent's memory can limit its information rate between Mt \u2212 1 and Mt to RM 2.2. The capacity of the channel from the agent's sensors to its memory can limit the rate at which the agent is able to process the observation Ot as long as it is available, on RS. The requirement that the agent keep sufficient statistics and exact beliefs is unrealistic. Rather, the agent's memory must be a statistic of the O (t) sensors, in his sense, but that it is not sufficient in the algorithm's sense. \""}, {"heading": "1.3. Related Work", "text": "Unrestricted planning in passive POMDPs is easy to accomplish by maintaining the exact faith and selecting each action to minimize the subjective expected cost. Planning in general POMDPs is more difficult in one aspect due to the size of the space of faith. Many algorithms plan efficiently, but roughly, by focusing on a subset of that space. Multiple works do this by optimizing a finite state controller of a certain size (Poupart & Boutilier, 2003; Amato et al., 2010). Faith, which is represented by each state of the controller, is then the subordinate probability of the world state in light of this state of memory. Another approach is to explicitly select a subset of beliefs and use them to guide the iterations (Pineau et al., 2003). Another is to reduce the dimension of the space of faith to its main components (Roy & Gordon, 2002). In this paper, we present the POMs setting of information in passive planning by limiting the Maps."}, {"heading": "2. Preliminaries", "text": "Suppose that the model parameters P1, p, \u03c3 and d are given, and the agent strives to find a policy of consequence q (n) in such a way that the average expected costs are satisfactory1n \u2211 t = 1E Wt, Mt d (Wt, Mt) \u2264 D.for the minimum D. However, the agent operates under capacity constraints on the channels from Mt \u2212 1 and Ot to Mt. The external costs d correspond to the distortion theory, while the internal costs are information rates. Indeed, the agent must minimize a combination of these costs. Note that the agent will generally have some information about the next observation even before he sees it, i.e. Mt \u2212 1 and Ot will not be independent, so the agent has a certain freedom to choose which part of the information he remembers Mt \u2212 1 and Ot, and which part he neglects and observes."}, {"heading": "3. One-Step Optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Variational Principle", "text": "Before considering the long-term planning we need for problem 1, let us focus on the choice of qn (Mn-1, Wn-1, Wn-n) on the common distribution of Mn-1 and Wn-n (Mn-1, Wn) on the common distribution of Mn-1 and Wn, and havePr n (Mn-1, Wn-n) on the common distribution of Mn-1 and Wn, which includes all points (RM, RS, D) that are achievable, that is, for which there are some qn (Mn-1, Wn). (Mn-1, Wn) def = EWn, Mn (Wn) def = EWn, Mn (Wn, Mn) \u2264 DIC (Mn) def = I (Mn \u2212 d)."}, {"heading": "3.2. Properties of the One-Step Lagrangian", "text": "Theoretically, L1 in qn and q in qn is set to qn (1). L1 is strictly convex in parameters bound to mn \u2212 1 and to Pr\u03b8n (mn \u2212 1, on) > 0, and at the minimum are these satisfaction conditions (Mn \u2212 1, On), where Zn is a normalizing division function, and am (Mn) = exp (\u2212 1, onPr \u2212 n, on) qn (Mn \u2212 1, on), where Zn \u2212 1, on), where Zn is a normalizing division function, and am (Mn), mn \u2212 1, onPr \u2212 1, on) qn (mn \u2212 1, on), on) qn (Mn \u2212 1, on) q \"n\" (Mn \u2212 n \u2212 n. \""}, {"heading": "3.3. Optimization Algorithm", "text": "An algorithm that alternately minimizes L1 over each parameter and fixes the others in the style of Blahut-Arimoto (Cover & Thomas, 2006) will allow us to find the minimum.Theorem 4. Algorithm 1 converges monotonously to the global minimum of L1.Evidence. L1 is not augmenting in each iteration and is limited from below, which guarantees its monotonous convergence.This is 2For the sake of clarity, strict convergence, uniformity of the minimum and convergence in terms of events and transitions of positive probability should be taken into account here and in the rest of this paper, as by theorem 1. Algorithm 1. Last-Step OptimizationInput: P1, p, \u03c3, d, \u0421C, \u0421M, \u0421S, successn Output: optimal qn \u00b2 0 Initialize a proposal for qrn \u2212 n The marginal qr \u2212 n (eq \u00b2 r) are also a convergence with minimum 1 \u00b2 (qr)."}, {"heading": "4. Sequential Rate-Distortion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Variational Principle", "text": "To return to the entire process of problem 1, the order of common beliefs \u03b8 (2, n) = \u03b82,.., \u03b8n recursively depends on \u03b81 and the policy q (n). For each 1 \u2264 t < n\u03b8t + 1 (Mt, Wt + 1) (3) = \u2211 mt \u2212 1, wt\u03b8t (mt \u2212 1, wt) Pr qt (Mt, Wt + 1 | mt \u2212 1, wt), with the independent distribution of M0 and W1.Add the constraints of equation 3 with multipliers \u03bdt, mt, wt; t, p, p + 1, so the Lagrangic of problem 1 isLn (q (n), q (n), \u03b8 (2, n)) = 1n \u0445 t = 1 L1 (qt, qt, qt; t, qt, p, p, t, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "4.2. Local Optimization Algorithm", "text": "Nevertheless, problem 1 still has a certain structure that is instructive to explore. In particular, it shows some interesting similarities to the standard POMDP planning problem. Differentiation Ln to qt we have now getqt (Mt, Mt \u2212 1, Ot) (4) = exp (\u2212 \u03b3 \u2212 1G\u03bdq) + E Wt + 1, Ot \u2212 1, Ot, Mt \u2212 \u2212 \u2212 \u2212 p is now dependent on the future by the multiplier vector (Wt, Mt) = d (Wt, Mt) + E Wt + 1 (Wt, Mt, Wt \u2212 1, where ~ \u03bdn = 0. qt now depends on the future by the multiplier vector. Note how the expectation of the QT, Mt + 1 given Wt, plays a parallel role to that of d (Wt, Mt)."}, {"heading": "4.3. Joint-Belief MDP", "text": "Extending the recursion from ~ \u03bdt in Eq.5 to a closed form, and ignoring the resulting ~ \u03bbt, we find that for 1 < t \u2264 n and consistent parameters 3Ln \u2212 t (q (t, n); \u03b8t) (6) = 1 n \u2212 t + 1 \u2211 mt \u2212 1, wt\u03b8t (mt \u2212 1, wt) \u03bdt \u2212 1, mt \u2212 1, wt. If we extend the recursion by a further step to define ~ \u03bd0, we get that our minimization target isLn (q (n); \u03b81) = 1n E M0, W1 \u03bd0, M0, W1, W1. Minimization Vt (n) = min q (t) E Mt \u2212 1, Wt \u2212 1, Mt \u2212 1, Wt \u2212 1, Wt \u2212 1, Wtcan be considered as the cost-to-go, given the common belief prior to step."}, {"heading": "4.4. Bounded Planning Algorithm", "text": "Perhaps the determinism of the joint belief MDP is responsible for all the costs. Together with iterations of algorithm 2, which bring about local improvements, this will guarantee convergence to a local optimum. Our algorithm is a simple forward-backward algorithm, with a building block q (algorithm 2) that is itself forward-backward. In each iteration, we recursively forward calculate the common beliefs n (n) for the current policy q (n). Then we recursively backward calculate a new policy q (n) by finding in each step t a policy suffix that is locally optimal for the respective procedure. The criterion for optimism is that we either q (t + 1) from the previous step or q (t + 1) from the previous iteration or q (n) from the previous iteration, and whoever leads to lower costs is chosen.5. Theorem 5."}, {"heading": "5. Simulations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Symmetric Channel", "text": "Figure 2 shows the limit of the distortion region for the 30-step sequential symmetric channel problem. W, O and M domains are all binary. The agent correctly observes the state with probability 0.8. The state remains probable for the next step independently of each other with probability 0.8. The distortion is the delta function. The limit consists of three parts as in sequence 3. They have \u03b3M = 0 (left), \u03b3M = \u03b3S = 0 (middle) and \u03b3S = 0 (right). Empirically, the intake of \u03b3C = 0 is never feasible, because no optimal solution per IC has \u2264 IM + IS. To illustrate this further, Figure 3 shows a color contour map of the limit. The lower the distortion, the higher the required information rates. The compromise between memory and perception is illustrated by the negative slope of the contours."}, {"heading": "5.2. Kelly Gambling", "text": "Between races, the fitness of each horse can increase by 1 independently of each other, with the probability being 0.1 if it is not exhausted, or 1 if it falls, with the probability being 0.1 if it is not exhausted. Each horse maintains its fitness with the remaining probability. The only observations are side events conducted before each race: 2 random horses compete (with Softmax) and the identities of the winner and the loser are announced. Memory is a model of the world consisting of the supposed fitness of each horse. Protocol-timed proportional gambling strategy is applied (Kelly gambling, see Cover & Thomas, 2006), bets on horse i are a fraction of the wealth that is proportional to the expp (f-i). Each bet is double or nothing, and the distortion of the proportional gambling strategy is expected (Kelly gambling, see Cover & Thomas, 2006)."}, {"heading": "6. Conclusion", "text": "We have presented the problem of planning in passive POMDPs with information rate limitations. This problem takes the form of a sequential version of the rate editorship theory, and accordingly we have been able to provide algorithms that optimize each step individually. Unfortunately, the complete problem is not convex, and we expect it to have very hard instances. Nevertheless, it is expected that typical cases with a certain locality will be easier in their transitions and observations. We have introduced an efficient and simple algorithm for determining a local minimum and used it to illustrate the problem with two simulations, showing the origin of a conflict between memory and perception of the problem. Our work was motivated by the problem of planning in general POMDPs, which can benefit from an approach of faith guided by information theory. Applying our current results to this problem is left to future work."}, {"heading": "7. Acknowledgement", "text": "This project is supported in part by the MSEE DARPA Project and the Gatsby Charitable Foundation."}], "references": [{"title": "Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs", "author": ["Amato", "Christopher", "Bernstein", "Daniel S", "Zilberstein", "Shlomo"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Amato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2010}, {"title": "Multiterminal source coding", "author": ["Berger", "Toby"], "venue": "The Information Theory Approach to Communications,", "citeRegEx": "Berger and Toby.,? \\Q1977\\E", "shortCiteRegEx": "Berger and Toby.", "year": 1977}, {"title": "Reinforcement learning with perceptual aliasing: The perceptual distinctions approach", "author": ["Chrisman", "Lonnie"], "venue": "In Proceedings of the tenth national conference on Artificial intelligence,", "citeRegEx": "Chrisman and Lonnie.,? \\Q1992\\E", "shortCiteRegEx": "Chrisman and Lonnie.", "year": 1992}, {"title": "Optimal control as a graphical model inference problem", "author": ["Kappen", "Bert", "G\u00f3mez", "Vicen\u00e7", "Opper", "Manfred"], "venue": null, "citeRegEx": "Kappen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kappen et al\\.", "year": 2009}, {"title": "Bright illusions reduce the eye\u2019s pupil", "author": ["Laeng", "Bruno", "Endestad", "Tor"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Laeng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Laeng et al\\.", "year": 2012}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["Pineau", "Joelle", "Gordon", "Geoffrey J", "Thrun", "Sebastian"], "venue": "In IJCAI,", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Bounded finite state controllers", "author": ["Poupart", "Pascal", "Boutilier", "Craig"], "venue": "In NIPS,", "citeRegEx": "Poupart et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2003}, {"title": "Exponential family PCA for belief compression in POMDPs", "author": ["Roy", "Nicholas", "Gordon", "Geoffrey J"], "venue": "In NIPS,", "citeRegEx": "Roy et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2002}, {"title": "Information theory of decisions and actions", "author": ["Tishby", "Naftali", "Polani", "Daniel"], "venue": "PerceptionAction Cycle,", "citeRegEx": "Tishby et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2011}, {"title": "Linearly-solvable Markov decision problems", "author": ["Todorov", "Emanuel"], "venue": "In NIPS,", "citeRegEx": "Todorov and Emanuel.,? \\Q2006\\E", "shortCiteRegEx": "Todorov and Emanuel.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Several works do so by optimizing a finite-state controller of a given size (Poupart & Boutilier, 2003; Amato et al., 2010).", "startOffset": 76, "endOffset": 123}, {"referenceID": 5, "context": "A different approach is to explicitly select a subset of beliefs, and use them to guide the iterations (Pineau et al., 2003).", "startOffset": 103, "endOffset": 124}, {"referenceID": 3, "context": "Some research treats POMDPs where the cost is the DKL between the distributions of the next world state when it is controlled and uncontrolled (Todorov, 2006; Kappen et al., 2009).", "startOffset": 143, "endOffset": 179}, {"referenceID": 3, "context": "Some research treats POMDPs where the cost is the DKL between the distributions of the next world state when it is controlled and uncontrolled (Todorov, 2006; Kappen et al., 2009). This has interesting analogies to our setting. Our information-rate constraints define, in effect, components of the cost which are the DKL between the distribution of the next memory state and its marginals (see section 3.1). Tishby & Polani (2011) combine similar information-rate constraints of perception and action together.", "startOffset": 159, "endOffset": 431}], "year": 2012, "abstractText": "In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum.", "creator": "LaTeX with hyperref package"}}}