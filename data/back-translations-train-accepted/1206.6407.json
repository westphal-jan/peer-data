{"id": "1206.6407", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Large-Scale Feature Learning With Spike-and-Slab Sparse Coding", "abstract": "We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models? Transfer Learning Challenge.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (877kb)", "http://arxiv.org/abs/1206.6407v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1201.3382"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1201.3382", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ian j goodfellow", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1206.6407"}, "pdf": {"name": "1206.6407.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Feature Learning With Spike-and-Slab Sparse Coding", "authors": ["Ian J. Goodfellow", "Aaron Courville", "Yoshua Bengio"], "emails": ["goodfeli.@iro.umontreal.ca", "Aaron.Courville@umontreal.ca", "Yoshua.Bengio@umontreal.ca"], "sections": [{"heading": "1. Introduction", "text": "Here we look at the problem of unsupervised functional discovery for supervised learning. In supervised learning, a number of examples are given V = {v (1),.., v (m)} and associated labels {y (1),.., y (m)}. The aim is to learn a model p (y | v) so that new labels can be predicted from new undescribed examples v.Appearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner (s).The idea behind unsupervised functional models discovery is that the final learning problem can become much easier if the problem is presented in the right way. By learning the structure of V, we can discover a feature mapping (v) that can be used to pre-process the data before there is a standard for supervising learning algorithms, as SVtigM.There has been a great interest in investigating various unsupervised learning programs."}, {"heading": "2. The Spike-and-Slab Sparse Coding model", "text": "The spike-and-slab sparse coding (S3C) model consists of latent binary spike variables h-0, 1-N, latent real ceiling variables s-RN and real D-dimensional visible vectors v-RD, which are generated according to this process:. 1,.., N) p (vd | s, h) = N (vd | Wd: (h-s), p (hi = 1) = \u03c3 (bi) p (si | hi) = N (si | hi\u00b5i, \u03b1 \u2212 1ii) (1) p (vd | s, h) = N (vd | Wd: (h-s), \u03b2 \u2212 1dd), which is the logistic sigmoid function, b is a series of distortions in the spike variables, \u00b5 and W govern the linear dependence of s on h and v on s, \u03b1 and \u03b2 are diagonal precision matrices of their respective conditions, as hidden spike-dissent elements."}, {"heading": "2.1. Comparison to sparse coding", "text": "Lately, Coates and Ng (2011) have shown that this approach provides excellent performance based on the CIFAR-10 object recognition dataset.Sparse coding (Olshausen and Field, 1997) describes a class of generative models in which the observed data v is normally distributed as a set of continuous latent variables and a dictatorial matrix W: v \u0445 N (Ws, \u03c3I). Sparse coding places a factorial prior on s as a Cauchy or Laplace distribution, which was chosen to promote the mode of posterior p (s | v) in order to be economical. One can derive the S3C model from sparse coding by replacing the factorial Cauchy or Laplace classes with a spike-and-slab prior.One Drawing back of sparse coding is that the mode of the posterior p (s | v)."}, {"heading": "2.2. Comparison to Restricted Boltzmann Machines", "text": "An RBM (Smolensky, 1986) is a model defined by an energy function that describes the interactions between the observed data variables and a series of latent variables. It is possible to interpret the S3C as an energy-based model by cancelling the form exp {\u2212 E (v, s, h)} / Z, with the following energy function: E (v, h) = 12v \u2212 X i Wisihi! T \u03b2 v \u2212 X i Wisihi! + 12 NX i = 1 \u03b1i (si \u2212 \u00b5ihi) 2 \u2212 M = 1 bihi, (2) The ssRBM model family is a good starting point for S3C because it has demonstrated both reasonable performance and remarkable performance as a generative model (Courville et al, 2011a)."}, {"heading": "3. Other Related work", "text": "Outside the context of the unattended discovery of traits for supervised learning, the basic form of the S3C model (i.e. a latent factor model with spike-and-slab) has been repeated in various areas (Lucke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and La \ufffd zaro-Gredilla, 2011). In most work, the model differs slightly from S3C. Titsias and La \u0301 zaroGredilla (2011), for example, have a single probability parameter for the activation of spike across all spike variables. Lu \ufffd cke and Sheikh (2011) use exactly the S3C model, but use an insoluble exact inference. To this literature, we add a harmless inference scheme that scales to the type of objects we consider next."}, {"heading": "4. Variational EM for S3C", "text": "After explaining why S3C is a powerful model for unsupervised discoveries, we turn to the problem of how to perform learning and conclusions in this model. Since the calculation of the exact posterior distribution is not feasible, we derive from it an efficient and effective approach method and a variable EM learning algorithm. We turn to variable EM (Saul and Hinton, 1999) because this algorithm is well suited for models with latent variables whose posterior distribution mechanisms are intractable. It works by maximizing a variable lower limit on the energy functionality called loglikelihood (Neal and Hinton, 1999). Specifically, it is a variant of EM algorithms with the modification that make a variational approach to the posterior rather than to the posterior itself in the step. While our model allows a closed form of the M step, we found that online learning works better with small steps in practice."}, {"heading": "5. Performance results", "text": "The computational bottleneck in our classification pipeline is SVM training, not feature learning or feature extraction. Comparing the computational effort of our inference scheme with others is a difficult task because it could be confused by differences in implementation, and because it is not clear exactly which sparse encoding prob-tem corresponds to an equivalent spike-and-slab sparse encoding problem. However, we informally observed during our supervised learning experiments that S3C extraction took about the same time as feature extraction using sparse encoding. In Fig. 4, we show that our improvements in spikeand slab inference performance allow us to apply spikeand slab modeling to the problem needed for object detection."}, {"heading": "6. Classification results", "text": "We conducted experiments to evaluate the usefulness of the S3C features for supervised learning using the CIFAR 10 and CIFAR-100 datasets (Krizhevsky and Hinton, 2009), both of which consist of color images of objects such as animals and vehicles, each of which contains 50,000 train examples and 10,000 test examples. CIFAR-10 contains 10 classes, while CIFAR-100 contains 100 100 classes, so that in the case of CIFAR-100 there are fewer labeled examples per class. For all experiments, we used the same overall procedure as Coates and Ng (2011), except for feature learning. CIFAR-10 consists of 32 x 32 images. We train our feature extractor on 6 x 6 contrast-normalized and ZCA-white patches from the training set. During the test period, we extract features from all 6 x 6 patches on an image, in order to then bundle them averagely \u2212 the grid is arranged on an overlapping region."}, {"heading": "6.1. CIFAR-10", "text": "We are using CIFAR-10 to evaluate our hypothesis that S3C resembles a more regulated version of sparse encoding. In the complete data set, S3C achieves a test set accuracy of 78.3 \u00b1 0.9% with 95% certainty. Coates and Ng (2011) do not indicate the accuracy of the test sets for sparse \"natural encoding\" encoding (i.e., the extraction of features in a model whose parameters are all the same as in the model used for training), but the sparse encoding with parameters other than the training achieves an accuracy of 78.8 \u00b1 0.9% (Coates and Ng, 2011). Since we have not improved our performance by modifying the parameters in feature extraction, these results seem to indicate that S3C is roughly equivalent to the sparse encoding for this classification task."}, {"heading": "6.2. CIFAR-100", "text": "After verifying that S3C characteristics help regulate a classifier, we move on to using them to improve performance on the CIFAR 100 dataset, which has ten times as many classes and ten times fewer marked examples per class.We compare S3C with two other methods of extracting characteristics: OMP-1 with thresholds that Coates and Ng (2011) identified as the best feature extractor on CIFAR-10, and sparse encoding, which is known to work well when less marked data is available. We rated only a single set of hyperparameters for S3C. For the sparse encoding and OMP-1, we looked for the same set of hyperparameters as Coates and Ng (2011): {0.5, 0.75, 1.0, 1.25, 1.25} for the sparse encoding penalty and {0.1, 0.25, 0.5, 1.0} for the concentrated value."}, {"heading": "7. Transfer Learning Challenge", "text": "For the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models (Le et al., 2011), the organizers proposed a transfer learning contest using a data set of 32 x 32 color images, including 100,000 unlabeled examples, 50,000 labeled examples of 100 object classes not present in the test set, and 120 labeled examples of 10 object classes included in the test set. We used the same approach as for the CIFAR data sets and won the competition with a test set accuracy of 48.6%. This approach disregards the 50,000 labels and treats this transfer learning problem as a semi-supervised learning problem. We experimented with some transfer learning techniques, but the transfer-free approach showed the best performance in leave-one-out cross-validation on the 120 sample training sessions, so we decided to enter the transfer-free technique in the challenge."}, {"heading": "8. Conclusion", "text": "We have described a variable approximation scheme that makes it possible to perform learning and conclusions in large-scale S3C models. We have shown that S3C is an effective algorithm for detecting characteristics for both supervised and semi-supervised learning with small amounts of labeled data. This work addresses two scaling problems: the calculation problem of scaling the sparse coding of spikes to the problem sizes used in object detection, and the problem of scaling object detection techniques to work with more classes."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In Adv. Neural Inf. Proc. Sys", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In ICML\u20192011,", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "Unsupervised models of images by spike-and-slab RBMs", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the Twenty-eight International Conference on Machine Learning", "citeRegEx": "Courville et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Courville et al\\.", "year": 2011}, {"title": "A Spike and Slab Restricted Boltzmann Machine", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In AISTATS\u20192011,", "citeRegEx": "Courville et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Courville et al\\.", "year": 2011}, {"title": "Learning horizontal connections in a sparse coding model of natural images", "author": ["P. Garrigues", "B. Olshausen"], "venue": "In NIPS\u201920", "citeRegEx": "Garrigues and Olshausen.,? \\Q2008\\E", "shortCiteRegEx": "Garrigues and Olshausen.", "year": 2008}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Y. Jia", "C. Huang"], "venue": null, "citeRegEx": "Jia and Huang.,? \\Q2011\\E", "shortCiteRegEx": "Jia and Huang.", "year": 2011}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "In NIPS\u201910,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "A closed-form EM algorithm for sparse coding", "author": ["J. L\u00fccke", "A.-S. Sheikh"], "venue": null, "citeRegEx": "L\u00fccke and Sheikh.,? \\Q2011\\E", "shortCiteRegEx": "L\u00fccke and Sheikh.", "year": 2011}, {"title": "Bayesian variable selection in linear regression", "author": ["T.J. Mitchell", "J.J. Beauchamp"], "venue": "J. Amer. Statistical Assoc.,", "citeRegEx": "Mitchell and Beauchamp.,? \\Q1988\\E", "shortCiteRegEx": "Mitchell and Beauchamp.", "year": 1988}, {"title": "Bayesian and l1 approaches to sparse unsupervised learning", "author": ["S. Mohamed", "K. Heller", "Z. Ghahramani"], "venue": null, "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "A view of the em algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": null, "citeRegEx": "Neal and Hinton.,? \\Q1999\\E", "shortCiteRegEx": "Neal and Hinton.", "year": 1999}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Selftaught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "In Z. Ghahramani, editor,", "citeRegEx": "Raina et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2007}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proc. AISTATS\u20192009,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Exploiting tractable substructures in intractable networks. In NIPS\u201995", "author": ["L.K. Saul", "M.I. Jordan"], "venue": null, "citeRegEx": "Saul and Jordan.,? \\Q1996\\E", "shortCiteRegEx": "Saul and Jordan.", "year": 1996}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["N.N. Schraudolph"], "venue": "Neural Computation,", "citeRegEx": "Schraudolph.,? \\Q2002\\E", "shortCiteRegEx": "Schraudolph.", "year": 2002}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Parallel Distributed Processing,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Spike and slab variational inference for multi-task and multiple kernel learning", "author": ["M.K. Titsias", "M. L\u00e1zaro-Gredilla"], "venue": null, "citeRegEx": "Titsias and L\u00e1zaro.Gredilla.,? \\Q2011\\E", "shortCiteRegEx": "Titsias and L\u00e1zaro.Gredilla.", "year": 2011}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "In CVPR,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M. Zeiler", "G. Taylor", "R. Fergus"], "venue": "In ICML,", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}, {"title": "Non-parametric Bayesian dictionary learning for sparse image representations", "author": ["M. Zhou", "H. Chen", "J.W. Paisley", "L. Ren", "G. Sapiro", "L. Carin"], "venue": "In NIPS\u201909,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In particular, the goal of deep learning (Bengio, 2009) is to learn a function \u03c6 that consists of many layers of processing, each of which receives the previous layers as input and incrementally disentangles the factors of variation in the data.", "startOffset": 41, "endOffset": 55}, {"referenceID": 16, "context": "Examples of shallow models applied to feature discovery include sparse coding (Raina et al., 2007), restricted Boltzmann machines (RBMs) (Hinton et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 6, "context": ", 2007), restricted Boltzmann machines (RBMs) (Hinton et al., 2006; Courville et al., 2011b), various autoencoderbased models (Bengio et al.", "startOffset": 46, "endOffset": 92}, {"referenceID": 1, "context": ", 2011b), various autoencoderbased models (Bengio et al., 2007), and hybrids of autoencoders and sparse coding (Kavukcuoglu et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 8, "context": ", 2007), and hybrids of autoencoders and sparse coding (Kavukcuoglu et al., 2010).", "startOffset": 55, "endOffset": 81}, {"referenceID": 2, "context": "Single-layer convolutional models based on simple feature extractors currently achieve state-of-the-art performance on the CIFAR-10 object recognition dataset (Coates and Ng, 2011; Jia and Huang, 2011).", "startOffset": 159, "endOffset": 201}, {"referenceID": 7, "context": "Single-layer convolutional models based on simple feature extractors currently achieve state-of-the-art performance on the CIFAR-10 object recognition dataset (Coates and Ng, 2011; Jia and Huang, 2011).", "startOffset": 159, "endOffset": 201}, {"referenceID": 2, "context": "It is known that the best models for the detection layer of the convolutional model do not perform well when fewer labeled examples are available (Coates and Ng, 2011).", "startOffset": 146, "endOffset": 167}, {"referenceID": 16, "context": "Sparse coding has been widely used to discover features for classification (Raina et al., 2007).", "startOffset": 75, "endOffset": 95}, {"referenceID": 2, "context": "Recently Coates and Ng (2011) showed that this approach achieves excellent performance on the CIFAR-10 object recognition dataset.", "startOffset": 9, "endOffset": 30}, {"referenceID": 15, "context": "Sparse coding (Olshausen and Field, 1997) describes a class of generative models where the observed data v is normally distributed given a set of continuous latent variables s and a dictionary matrix W : v \u223c N (Ws, \u03c3I).", "startOffset": 14, "endOffset": 41}, {"referenceID": 22, "context": "While Yu et al. (2011) and Zeiler et al.", "startOffset": 6, "endOffset": 23}, {"referenceID": 22, "context": "While Yu et al. (2011) and Zeiler et al. (2011) have recently shown some success at learning hierarchical sparse coding, our goal for our future work is to integrate the feature extraction scheme into a proven", "startOffset": 6, "endOffset": 48}, {"referenceID": 17, "context": "generative model framework such as the deep Boltzmann machine (Salakhutdinov and Hinton, 2009).", "startOffset": 62, "endOffset": 94}, {"referenceID": 20, "context": "An RBM (Smolensky, 1986) is a model defined through an energy function that describes the interactions between the observed data variables and a set of latent variables.", "startOffset": 7, "endOffset": 24}, {"referenceID": 2, "context": "As shown by Coates and Ng (2011), the sparse Gaussian RBM is not a very good feature extractor \u2013 the set of basis functions W learned by the RBM actually work better for supervised learning when these parameters are plugged into a sparse coding model than when the RBM itself is used for feature extraction.", "startOffset": 12, "endOffset": 33}, {"referenceID": 12, "context": "The notion of a spike-and-slab prior was established in statistics by Mitchell and Beauchamp (1988). Outside the context of unsupervised feature discovery for supervised learning, the basic form of the S3C model (i.", "startOffset": 70, "endOffset": 100}, {"referenceID": 11, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 5, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 13, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 24, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 21, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011).", "startOffset": 46, "endOffset": 177}, {"referenceID": 5, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011). In most work, the model varies slightly from S3C. For example, Titsias and L\u00e1zaroGredilla (2011) share a single spike activation probability parameter across all spike variables.", "startOffset": 71, "endOffset": 276}, {"referenceID": 5, "context": "peared a number of times in different domains (L\u00fccke and Sheikh, 2011; Garrigues and Olshausen, 2008; Mohamed et al., 2011; Zhou et al., 2009; Titsias and L\u00e1zaro-Gredilla, 2011). In most work, the model varies slightly from S3C. For example, Titsias and L\u00e1zaroGredilla (2011) share a single spike activation probability parameter across all spike variables. L\u00fccke and Sheikh (2011) use exactly the S3C model, but use intractable exact inference.", "startOffset": 71, "endOffset": 382}, {"referenceID": 18, "context": "We turn to variational EM (Saul and Jordan, 1996) because this algorithm is well-suited for models with latent variables whose posterior is intractable.", "startOffset": 26, "endOffset": 49}, {"referenceID": 14, "context": "It works by maximizing a variational lower bound on the loglikelihood called the energy functional (Neal and Hinton, 1999).", "startOffset": 99, "endOffset": 122}, {"referenceID": 19, "context": "We implement conjugate gradient descent efficiently by using the R-operator to perform Hessianvector products rather than computing the entire Hessian explicitly (Schraudolph, 2002).", "startOffset": 162, "endOffset": 181}, {"referenceID": 9, "context": "This approach is not guaranteed to decrease the KL divergence on each iteration but it is a widely applied approach that works well in practice (Koller and Friedman, 2009).", "startOffset": 144, "endOffset": 171}, {"referenceID": 5, "context": "For comparison, to our knowledge the largest image patches used in previous spikeand-slab models with lateral interactions were 16\u00d7 16 (Garrigues and Olshausen, 2008).", "startOffset": 135, "endOffset": 166}, {"referenceID": 2, "context": "Previous object recognition work is from (Coates and Ng, 2011; Courville et al., 2011a).", "startOffset": 41, "endOffset": 87}, {"referenceID": 10, "context": "We conducted experiments to evaluate the usefulness of S3C features for supervised learning on the CIFAR10 and CIFAR-100 (Krizhevsky and Hinton, 2009) datasets.", "startOffset": 121, "endOffset": 150}, {"referenceID": 2, "context": "For all experiments, we used the same overall procedure as Coates and Ng (2011) except for feature learning.", "startOffset": 59, "endOffset": 80}, {"referenceID": 2, "context": "9% (Coates and Ng, 2011).", "startOffset": 3, "endOffset": 24}, {"referenceID": 2, "context": "Coates and Ng (2011) do not report test set accuracy for sparse coding with \u201cnatural encoding\u201d (i.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Coates and Ng (2011) do not report test set accuracy for sparse coding with \u201cnatural encoding\u201d (i.e., extracting features in a model whose parameters are all the same as in the model used for training) but sparse coding with different parameters for feature extraction than training achieves an accuracy of 78.8 \u00b1 0.9% (Coates and Ng, 2011). Since we have not enhanced our performance by modifying parameters at feature extraction time these results seem to indicate that S3C is roughly equivalent to sparse coding for this classification task. S3C also outperforms ssRBMs, which require 4,096 basis vectors per patch and a 3\u00d73 pooling grid to achieve 76.7 \u00b1 0.9% accuracy. All of these approaches are close to the best result using the pipeline from Coates and Ng (2011) of 81.", "startOffset": 0, "endOffset": 772}, {"referenceID": 2, "context": "We compare S3C to two other feature extraction methods: OMP-1 with thresholding, which Coates and Ng (2011) found to be the best feature extractor on CIFAR-10, and sparse coding, which is known to perform well when less labeled data is available.", "startOffset": 87, "endOffset": 108}, {"referenceID": 2, "context": "We compare S3C to two other feature extraction methods: OMP-1 with thresholding, which Coates and Ng (2011) found to be the best feature extractor on CIFAR-10, and sparse coding, which is known to perform well when less labeled data is available. We evaluated only a single set of hyperparameters for S3C. For sparse coding and OMP-1 we searched over the same set of hyperparameters as Coates and Ng (2011) did: {0.", "startOffset": 87, "endOffset": 407}, {"referenceID": 7, "context": "8 \u00b1 1% (Jia and Huang, 2011), achieved using a learned pooling structure on top of \u201ctriangle code\u201d features from a dictionary learned using k-means.", "startOffset": 7, "endOffset": 28}, {"referenceID": 7, "context": "8 \u00b1 1% (Jia and Huang, 2011), achieved using a learned pooling structure on top of \u201ctriangle code\u201d features from a dictionary learned using k-means. This feature extractor is very similar to thresholded OMP1 features and is known to perform slightly worse on CIFAR-10. Table 1 shows that S3C is the best known detector layer on CIFAR-100. If combined with the pooling strategy of Jia and Huang (2011) it has the potential to improve on the state of the art.", "startOffset": 8, "endOffset": 401}], "year": 2012, "abstractText": "We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models\u2019 Transfer Learning Challenge.", "creator": "TeX"}}}