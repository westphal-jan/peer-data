{"id": "1206.6408", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Sequential Nonparametric Regression", "abstract": "We present algorithms for nonparametric regression in settings where the data are obtained sequentially. While traditional estimators select bandwidths that depend upon the sample size, for sequential data the effective sample size is dynamically changing. We propose a linear time algorithm that adjusts the bandwidth for each new data point, and show that the estimator achieves the optimal minimax rate of convergence. We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function. We provide simulations that confirm the theoretical results, and demonstrate the effectiveness of the methods.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (219kb)", "http://arxiv.org/abs/1206.6408v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "stat.ME astro-ph.IM cs.LG", "authors": ["haijie gu", "john d lafferty"], "accepted": true, "id": "1206.6408"}, "pdf": {"name": "1206.6408.pdf", "metadata": {"source": "CRF", "title": "Sequential Nonparametric Regression", "authors": ["Haijie Gu", "John Lafferty"], "emails": ["haijieg@cs.cmu.edu", "lafferty@galton.uchicago.edu"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Sequential Local Polynomial Smoothing", "text": "Our efficient sequential estimator is based on local polynomial regression with a sequence of shrinking bandwidths. Among the various non-parametric reversal methods, local polynomial regression has strong minimax properties and gracefully addresses the problem of boundary distortion (Fan et al., 1993). Let {(X1, Y1), (X2, Y2),...} be an observation sequence according to the model given in Equation (1.1). Let us assume that the pairs (Xi, Yi) are independent and identically distributed random variables. Everywhere, for the sake of simplicity, let us assume that the domain is m (x) x [0, 1]. Expanding to higher dimensional x is straightforward. We discuss an extension to additive models for the high-dimensional case in Section 4.2."}, {"heading": "2.1. Sequential kernel regression", "text": "Let us first introduce the simplest version of the method. Remember, in the batch processing method ism 0n (x0) = \u2211 n i = 1 Kh (Xi, x0) Yi \u2211 n = 1 Kh (Xi, x0) (2,1), where h is the bandwidth, the kernel regression estimator is ism 0n (x0) = \u2211 n = 1 Kh (Xi, x0). Under the standard assumptions that the regression function is in a second order Sobolev space, the bandwidth is chosen as h = c \u00b7 n \u2212 1 / 5, and the estimator reaches the minimax convergence rate R (m, x0) = O (n \u2212 4 / 5). Let's assume that the data arrives sequentially. Our sequential estimator takes the formatting 0n (x0) = applied n t = 1 Kht (Xt, x0) Yt = 1 Kht convergence that the compression rate is n."}, {"heading": "2.2. Sequential local polynomial smoothing", "text": "In the case of the classic batch estimation, the sequence-d of local polynomial regression = 1 (1) x = 1 (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x) x (1) x) x (1) x (1) x) x (1) x (1) x (1) x x (1) x (1) x (1) x (1) x (1) x (1) x) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x) x x (1) x x x (1) x x x (1) x) x x x (1) x x x (1) x (1) x) x x x x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1) x (1 (1) x (1) x (1) x (1) x (1) x (1) x (1 (1) x (1) x (1) x (1) x (1) x (1) x (1 (1) x (1) x (1) x (1) x (1 (1) x (1) x (1) x (1) x (1 (1) x (1) x (1) x (1 (1) x (1) x (1) x (1 (1) x (1 (1) x (1) x (1) x (1) x (1) x (1 (1) x (1) x (1) x (1) x (1) x (1 x (1) x (1) x (1) x ("}, {"heading": "3. Risk Analysis", "text": "In this section we give a risk analysis of the sequential density estimation and regression. Assuming that the regression function m is in Cd, the class of functions with continuous derivatives, our goal is to show that the asymptotic risk of the online algorithm specified in the previous section reaches the statistical convergence rate of n \u2212 2d / (2d + 1), which is the minimum optimal rate for this function class. We start by analyzing the risk of the sequential estimation of kernel density and kernel regression (d = 2), because the analysis is simpler and more transparent for these particular cases than for the general case. We then generate the results in the sequence d \u2212 1 sequential polynomial regression in Cd. We assume that the true density function f and the true regression function m d = 2 have continuous derivatives."}, {"heading": "3.1. Sequential Kernel Density Estimation", "text": "& & # 252; K & # 252; K & # 252; K & # 252; K & # 252; K & # 252; K & # 252; K & # 252; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K # 160; K & # 160; K & # 160; K # 160; K & # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K & # 160; K # 160; K # 160; K # 160; K & # 160; K # 160; K # 160; K & # 160; K # 160; K # 160; K & # 160; K & # 160; K; K # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K & # 160; K # 160; K & # 160; K; K # 160; K & # 160; K # 160; K & # 160; K # 160; K # 160; K # 160; K & # 160; K # 160; K # 160; K & # 160; K # 160; K & # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K & # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K # 160; K & 160; K # 160; K # 160; K; K & 160; K # 160; K & 160; K & 160; K; K # 160; K & 160; K; K & 160; K & 160; K; K & 160; K; K;"}, {"heading": "3.2. Sequential Local Polynomial Regression", "text": "Similar results apply to the sequential local polynomial regression estimate. In particular, for d = 2 we use d = 1 n = 1 Kht (x, Xt) n = 1Kht (x, Xt) Yt (3.9), where f + n (x) = 1 n = 1 Kht (x, Xt). The following result is documented in section A.1. Lemma 3.2. The risk of the estimator m \u00b2 n (x) in (3.9) satisfactory R (m \u00b2 n, m) (3.10) = 14 \u03c32K 1nn \u00b2 t = 1h2t \u00b2 (m \u00b2 (x) + 2 \u2032 (x) f (x))) 2 dx + 2 K2 (x) dx (x) dxf (x) 1 n2 n \u00b2 n \u00b2 t = 11ht + o \u2212 t \u00b2 (x) f \u00b2 (x) 2 dx \u00b2 (x) dxf (x) dxf (x) 1 n2 n \u00b2 t 11 + o \u00b2 no \u00b2."}, {"heading": "4. Extensions", "text": "In this section, we expand the above analysis in two ways: First, we show how the expert mixing framework can be used to adapt to the smoothing exponent, and second, we show how the process can be extended to additive models."}, {"heading": "4.1. Adapting to unknown smoothness", "text": "The theoretical performance of the sequential estimators that we present in the series of sequential estimates is primarily limited to the correct sequence d'i (1), and in practice it depends on the constant c in the bandwidth, as well as on the different parameters (sequence d and constant c) through an exponential weighting strategy. It can be shown that the procedure adapts to the unknown smoothness of the optimal rate while maintaining a linear compression cost.Let C = {ci} 1, and D = Define M = CD have the exponential weighting of a series of sequential local polynomial regression estimates with different orders d and bandwidth constants c.Let C = {ci} 1, and D = D = Define M = CD, to be the size of the sequential estimates."}, {"heading": "4.2. Additive models", "text": "It is also possible to derive a sequential extension of the backfitting algorithm for additive models (Hastie & Tibshirani, 1990). Assuming that X p is dimensional, an additive regression model takes the formYi = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Array (X j) + i = 1,.., n-resigenz = 1,. \u2212 sequential m0 is an overall mean or intercept, the non-parametric regression functions mj are one-dimensional, and it is an average noise. The backfitting algorithm is a kind of coordinate derivation or Gauss-Seidel method that iteratively calculates the residuals for a variable j, and then smoothes those residuals to obtain a non-parametric estimate of the component function. In our sequential setting, a complication results from the fact that the residual residual functions cannot be sequentially updated for us - we do not need to expose the efficiency."}, {"heading": "5. Experiments", "text": "To illustrate the performance of a single online estimator (described in paragraph 2), we compare it with the batch algorithm, which has the same bandwidth h = cn \u2212 1 / 5. We look at the following 4 regression functions of different smoothness used by Yang (2001): f1 (x) = exp (\u2212 x), f2 (x) = 1 + 2x2 + exp (\u2212 5 (x \u2212 0.5) 2), f3 (x) = 1 + 2x2 + exp (\u2212 0.5)), f4 (x) = exp (\u2212 200 (x \u2212 0.2) 2), 2), 2 (x \u2212 0.8) 2), 2 (x) + exp. The sample size is 150 and \u03c32 = 0.5.We select the bandwidth constant c of C = {0.05, 0.07, 0.1}."}, {"heading": "6. Summary and Conclusions", "text": "The first contribution of this paper is the online algorithm, which shrinks the bandwidth for each incoming point. Second, the analysis, which shows that the order d d d d d d d d d d d achieves the optimal minimum convergence rate n \u2212 2d / 2d + 1. Finally, we show that the exponential weight mix of a family of such sequential estimators adapts to unknown smoothness at optimum speed and extends the algorithm to sequential backfitting for non-parametric additive models. Our experimental results confirm the theoretical analysis and show that the low loss of statistical efficiency is sacrificed by the compatible efficient online method. While we have shown the adaptation to global smoothness, an interesting direction for future work is to consider sequential estimators that adapt to spatially inhomogeneous function classes. A promising direction is the adjustment of the variable bandwidth of ski possibilities (to online resources)."}, {"heading": "Acknowledgements", "text": "Research supported in part by NSF funding IIS-1116730 and AFOSR contract FA9550-09-1-0373."}, {"heading": "Fan, Jianqing and Gijbels, Irene. Data-driven bandwidth", "text": "Selection in local polynomial fit: variable bandwidth and spatial fit. Journal of the Royal Statistical Society. Series B (Methodological), 57 (2): 371-394, 1995.Fan, Jianqing and Gijbels, Irene. Local Polynomial Modelling and Its Applications, pp. 57-105. Chapman and Hall / CRC, 1996."}, {"heading": "Fan, Jianqing, Gijbels, Irene, Gasser, Theo, Brockmann,", "text": "Michael, and Engel, Joachim. Local Polynomial Adjustment: A Standard for Nonparametric Regression, 1993."}, {"heading": "Gyo\u0308rfi, La\u0301slo\u0301, Kohler, Michael, Krzyz\u0307ak, Adam, and Walk,", "text": "Harro, A Distribution-Free Theory of Nonparametric Regression, Springer-Verlag, 2002.Hastie, T. J. and Tibshirani, R. J. Generalized additive models, London: Chapman & Hall, 1990, ISBN 0412343908."}, {"heading": "Kivinen, J., Smola, A. J., and Williamson, R. C. Online", "text": "Learning with cores. Signal Processing, IEEE Transactions on, 52: 2165-2176, 2004."}, {"heading": "Kristan, M., Skocaj, D., and Leonardis, A. Online kernel", "text": "Density estimation for interactive learning. Image and Vision Computing, 28: 1106-1116, 2010."}, {"heading": "Lepski, O.V., Mammen, E., and Spokoiny, V.G. Optimal", "text": "The Annals of Statistics, 25 (3): 929-947, 1997.Ruppert, D., Sheather, S. J, and Wand, M. P. An effective bandwidth selector for the regression of local minimum squares. Journal of the American Statistical Association, 90 (432), 1995.Steland, Ansgar. Sequential data-adaptive bandwidth selection by cross-validation for nonparametric predictions, 2010.Yang, Yuhong. Adaptive regression by mixing. Journal of the American Statistical Association, 96 (454): 574-588, 2001.Yang, Yuhong. Combining forecasting methods: Some theoretical results. Econometric theory, 20: 176-222, 2004."}], "references": [{"title": "Sequential procedures for aggregating arbitrary estimators of a conditional mean", "author": ["Bunea", "Florentina", "Nobel", "Andrew"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bunea et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bunea et al\\.", "year": 2008}, {"title": "Statistical learning theory and stochastic optimization", "author": ["Catoni", "Olivier"], "venue": "Ecole d\u2019Ete\u0301 des Probabilite\u0301s de Saint Flour,", "citeRegEx": "Catoni and Olivier.,? \\Q2001\\E", "shortCiteRegEx": "Catoni and Olivier.", "year": 2001}, {"title": "Prediction, Learning and Games", "author": ["Cesa-Bianchi", "Nicolo", "Lugosi", "Gabor"], "venue": null, "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Data-driven bandwidth selection in local polynomial fitting: variable bandwidth and spatial adaptation", "author": ["Fan", "Jianqing", "Gijbels", "Irene"], "venue": "Journal of the Royal Statistical Society. Series B(Methodological),", "citeRegEx": "Fan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1995}, {"title": "Local Polynomial Modelling and Its Applications, pp. 57\u2013105", "author": ["Fan", "Jianqing", "Gijbels", "Irene"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "Fan et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1996}, {"title": "Local polynomial fitting: A standard for nonparametric regression", "author": ["Fan", "Jianqing", "Gijbels", "Irene", "Gasser", "Theo", "Brockmann", "Michael", "Engel", "Joachim"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1993}, {"title": "A Distribution-Free Theory of Nonparametric Regression", "author": ["Gy\u00f6rfi", "L\u00e1sl\u00f3", "Kohler", "Michael", "Krzy\u017cak", "Adam", "Walk", "Harro"], "venue": null, "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2002}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A.J. Smola", "R.C. Williamson"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Kivinen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2004}, {"title": "Online kernel density estimation for interactive learning", "author": ["M. Kristan", "D. Skocaj", "A. Leonardis"], "venue": "Image and Vision Computing,", "citeRegEx": "Kristan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kristan et al\\.", "year": 2010}, {"title": "Optimal spatial adaptation to inhomogeneous smoothness: An approach based on kernel estimates with variable bandwidth selectors", "author": ["O.V. Lepski", "E. Mammen", "V.G. Spokoiny"], "venue": "The Annals of Statistics,", "citeRegEx": "Lepski et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Lepski et al\\.", "year": 1997}, {"title": "An effective bandwidth selector for local least squares regression", "author": ["D. Ruppert", "Sheather", "S. J", "M.P. Wand"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ruppert et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ruppert et al\\.", "year": 1995}, {"title": "Sequential data-adaptive bandwidth selection by cross validation for nonparametric prediction", "author": ["Steland", "Ansgar"], "venue": null, "citeRegEx": "Steland and Ansgar.,? \\Q2010\\E", "shortCiteRegEx": "Steland and Ansgar.", "year": 2010}, {"title": "Adaptive regression by mixing", "author": ["Yang", "Yuhong"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Yang and Yuhong.,? \\Q2001\\E", "shortCiteRegEx": "Yang and Yuhong.", "year": 2001}, {"title": "Combining forecasting procedures: Some theoretical results", "author": ["Yang", "Yuhong"], "venue": "Econometric Theory,", "citeRegEx": "Yang and Yuhong.,? \\Q2004\\E", "shortCiteRegEx": "Yang and Yuhong.", "year": 2004}], "referenceMentions": [{"referenceID": 6, "context": "2) thus has order h = O(n), which leads to the optimal minimax rate of convergence O(n); see Gy\u00f6rfi et al. (2002). More generally, if we assume further smoothness of m so that the d-th derivative of m exists and is bounded, then the minimax rate n is achieved using local polynomial regression of order d\u22121 with a bandwidth of order h = O(n) (Fan & Gijbels, 1996).", "startOffset": 93, "endOffset": 114}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space. The algorithms require linear computation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan & Gijbels (1995) consider using Residual Squares Criteria (RCS) for performing data-driven bandwidth selection; Ruppert et al.", "startOffset": 0, "endOffset": 472}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space. The algorithms require linear computation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan & Gijbels (1995) consider using Residual Squares Criteria (RCS) for performing data-driven bandwidth selection; Ruppert et al. (1995) propose a plug-in bandwidth selection scheme for local linear kernel estimators.", "startOffset": 0, "endOffset": 589}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space. The algorithms require linear computation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan & Gijbels (1995) consider using Residual Squares Criteria (RCS) for performing data-driven bandwidth selection; Ruppert et al. (1995) propose a plug-in bandwidth selection scheme for local linear kernel estimators. These are effective methods for adaptive estimation; however, they do not take into account the computational cost for online updating. The mixing expert framework has been a popular strategy for online prediction, and there is a rich literature on this topic (Cesa-Bianchi & Lugosi, 2006). Results by Bunea & Nobel (2008) give oracle inequalities for regression in terms of generalized simplex combinations of a set of fixed estimators in the online setting.", "startOffset": 0, "endOffset": 993}, {"referenceID": 5, "context": "Among the various nonparametric regression methods, local polynomial regression enjoys strong minimax properties, and gracefully deals with the problem of boundary bias (Fan et al., 1993).", "startOffset": 169, "endOffset": 187}, {"referenceID": 5, "context": "5) and m\u0302(x0) = \u03b2\u03020(x0) is an estimate of the regression function at x0 (Fan et al., 1993).", "startOffset": 72, "endOffset": 90}, {"referenceID": 9, "context": "We note in passing that a similar algorithm for sequential density estimation appears, without theoretical analysis, in Kristan et al. (2010). Theorem 3.", "startOffset": 120, "endOffset": 142}, {"referenceID": 10, "context": "One promising direction is to adapt the variable bandwidth estimator of Lepski et al. (1997) to online regression for Besov spaces.", "startOffset": 72, "endOffset": 93}], "year": 2012, "abstractText": "We present algorithms for nonparametric regression in settings where the data are obtained sequentially. While traditional estimators select bandwidths that depend upon the sample size, for sequential data the effective sample size is dynamically changing. We propose a linear time algorithm that adjusts the bandwidth for each new data point, and show that the estimator achieves the optimal minimax rate of convergence. We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function. We provide simulations that confirm the theoretical results, and demonstrate the effectiveness of the methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}