{"id": "1206.6410", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On the Partition Function and Random Maximum A-Posteriori Perturbations", "abstract": "In this paper we relate the partition function to the max-statistics of random variables. In particular, we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models. As a result, we can use efficient MAP solvers such as graph-cuts to evaluate the corresponding partition function. We show that our method excels in the typical \"high signal - high coupling\" regime that results in ragged energy landscapes difficult for alternative approaches.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (420kb)", "http://arxiv.org/abs/1206.6410v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tamir hazan", "tommi s jaakkola"], "accepted": true, "id": "1206.6410"}, "pdf": {"name": "1206.6410.pdf", "metadata": {"source": "META", "title": "On the Partition Function and Random Maximum A-Posteriori Perturbations", "authors": ["Tamir Hazan", "Tommi Jaakkola"], "emails": ["tamir@ttic.edu", "tommi@csail.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2. Background", "text": "Here we briefly define the counting and maximization problems of interest. Throughout the paper, we assume that real value potentials \u03c6 (y) = \u03c6 (y1,..., yn) < \u221e are defined via a separate product space Y = Y1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Yn. The domain is implicitly defined by \u03c6 (y) via exclusions \u03c6 (y) = \u2212 \u221e whenever y 6 \u0445 dom (\u03c6).The Gibbs distribution maps the real value potential functions to the probability scale (y1,..., yn) = 1Z exp (\u03c6 (y1,..., yn)). (2) Where the normalization constant Z is also known as a partition problem, the feasibility of using such a distribution for inference and learning is inherently linked to the ability to evaluate the partition configuration. In the special case where the implication (y), 0} the problem of the AP function in general is considered to be effective, the partition configuration II (very) can be very effective."}, {"heading": "3. Max-Statistics", "text": "In the following, we describe the basis of our framework. We show how to realize the partition function as the expected value of random MAP disturbances. Analytical expressions for the statistics of a random MAP disturbance can be derived for general discrete sets, if independent and identically distributed random disturbances are applied for each assignment y-Y. Let {\u03b3 (y) y-Y be a collection of random variables. Suppose these random variables are independent and identically distributed with F (t) as their cumulative distribution function, i.e. F (t) = P [g) \u2264 t] for each y-y-y disturbance y. The independence of p \u2212 y over y-Y implies that the cumulative distribution function of the random MAP disturbance maxy Y (y) + vice versa is the cumulative distribution function y-y the product of the cumulative distribution functions c-y-y-y."}, {"heading": "4. Low Dimensional Perturbations", "text": "In this section we will develop efficient approximations and limits for the partition function based on low-dimensional random MAP errors. We will begin by rewriting the previous result by exploiting the structure of the product space. Theorem 1. Let us leave {\u03b3i (yi)} yi-Yi, i = 1,..., n, a collection of independent and identically distributed (i.i.d.) random variables after the Gumbel distribution with F (t) = exp (\u2212 exp (\u2212 (t + c))), where c is the Euler constant. Let us define that the result from the application of Equation (4) is iterative."}, {"heading": "4.1. Upper Bounds on the Partition Function", "text": "Theorem 1 provides directly calculable boundaries for the log partition function. Intuitively, these boundaries correspond to the shifting of expectations outside of maximization operations. Each movement leads to an additional boundary, but also reduces the computational effort required for evaluation. In this case, the boundary is a simple average of MAP values that correspond to models with only single node perturbations."}, {"heading": "4.2. Approximating the Partition Function", "text": "We can use Theorem 1 to derive approximation schemes for the partition function, which also use efficient MAP solvers. As a simple step, we could simply replace the expectations in Theorem 1 with sampled estimates. The main subtlety lies in how these samples are reused as part of the outer loop maximization steps. Let's start by considering the n-th dimension alone. For each given setting of y1,..., yn \u2212 1, we need random values n (yn), for each yn ji, to estimate the result of Ynexp (y1,..., yn). Since the operation must be repeated for each other setting of y1, yn \u2212 1, we need random variables n (yn), j (y1, yn \u2212 1, yn), for each j-1, yn), for each j variable."}, {"heading": "4.3. Lower Bounds on the Partition Function", "text": "In contrast to the upper limit discussed above, however, the lower limit does not directly exploit theorem 1.Theorem 2. Let us consider any collection of subsets \u03b1 {1,..., n} and let them be independent random variables for each setting of \u03b1, y\u03b1. Let K\u03b1, y\u03b1 (\u03bb) = logE [exp (sustain1,..., n)] appear the proof in the supplementary material. The lower limit is slightly weaker than the upper limit in Korollar 1. In contrast to the upper limit, for example, in the case of a single \u03b1 = {1,..., n} we cannot specify the lower limit for each accumulated area."}, {"heading": "5. Conditional Random Fields", "text": "For a supervised learning problem, we assume that training data S of objects x x x and labels y Y such as images and their segmentations. If we obtain a feature vector p (x, y) < p for each object x (x, y) and labels y (Y), the learning task is to estimate parameters that maximize the probability of the protocol within the conditional random field model px (y) = exp (x, y) / Zx (\u03b8). This task can accordingly be specified as a loss minimization problem within the conditional random field model px (x, y). The formulation emphasizes the calculation costs of using conditional random fields due to the partition function. In the following, we will use a spare parts function (x, y) resulting from random MAP errors. Consider a family of subsets p (x)."}, {"heading": "In particular, if \u03b3\u03b1(y\u03b1) have the Gumbel distribution, J(\u03b8) upper bounds the conditional random field loss.", "text": "Proof: The loss function is convex in \u03b8, cf. (Rockafellar, 1974) Theorem 3. Conclusion 1 implies that the loss of the surrogate limits the loss of the partition above. To calculate the gradient, we use Theorem 23 in (Rockafellar, 1974) to differentiate below the integral. The subgradient of the maximum function is the indicator function above the maximum argument, cf. Proposition 4.5.1 in (Bertsekas et al., 2003). Since the expectation of the indicator function leads to a probability distribution, the loss of the surrogate is smooth and the gradient takes the form above. The structure of \u03b1 = 1,..., n} determines the statistical quality and algorithmic efficiency of the approximation. For example, if we use a single sentence \u03b1 = {1,..., n}, this formulation is an exact description of the conditional random fields, and its gradient describes the standard time to which the conditions fit."}, {"heading": "6. Empirical Evaluation", "text": "We evaluated our approach to Spin-Glass models, which are characterized by a relatively small group of dominant configurations. We evaluated our approach to Spin-Glass models, which are dominated by a relatively small group. We evaluated our approach to Spin-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Glass-Gl"}, {"heading": "7. Related Work", "text": "We refer the interested reader (Kotz & Nadarajah, 2000) to a more comprehensive introduction to extreme value statistics. To determine the expected value of random MAP interference in discrete product spaces, the approach of random MAP interference in discrete product spaces has not been extensively studied. Talagrand (Talagrand, 1994), Proposition 4.3) was the first to use random distribution variables with the Laplace distribution. Evidence technology is based on a compression argument and does not extend to the partition function. Limitation to the partition function (y)."}, {"heading": "8. Discussion", "text": "While it is well known that the ability to calculate the partition function also leads to a useful MAP algorithm, the opposite is true. We have shown here that a randomly malfunctioning MAP solver can approximate and limit the partition function, and the result allows us to use efficient MAP solvers. In addition, we have demonstrated the effectiveness of our approach in the \"High-Signal High-Coupling\" regime that dominates machine learning and is traditionally difficult for current methods. We have also applied our approach to conditional random fields and described the objective function to the moment matching algorithm of (Papandreou & Yuille, 2011). We hold the limits we presented with expectation. In practice, we calculate the empirical mean and standard techniques in the measure concentration, e.g. Chebyshev's inequality, describe how the sampled mean approximation relates to the expected values."}], "references": [{"title": "Random weighting, asymptotic counting, and inverse isoperimetry", "author": ["A. Barvinok", "A. Samorodnitsky"], "venue": "Israel Journal of Mathematics,", "citeRegEx": "Barvinok and Samorodnitsky,? \\Q2007\\E", "shortCiteRegEx": "Barvinok and Samorodnitsky", "year": 2007}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": null, "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2009}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Pacbayesian approach for minimization of phoneme error rate", "author": ["J. Keshet", "D. McAllester", "T. Hazan"], "venue": "In The International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "Probabilistic graphical models", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": null, "citeRegEx": "Kolmogorov,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Extreme value distributions: theory and applications", "author": ["S. Kotz", "S. Nadarajah"], "venue": "World Scientific Publishing Company,", "citeRegEx": "Kotz and Nadarajah,? \\Q2000\\E", "shortCiteRegEx": "Kotz and Nadarajah", "year": 2000}, {"title": "Structured determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "In Proc. Neural Information Processing Systems,", "citeRegEx": "Kulesza and Taskar,? \\Q2010\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2010}, {"title": "Statistical physics: Course of theoretical physics, vol", "author": ["L.D. Landau", "Lifshitz", "EM"], "venue": null, "citeRegEx": "Landau et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Landau et al\\.", "year": 1980}, {"title": "Conditional logit analysis of qualitative choice behavior, volume 1, pp. 105\u2013142", "author": ["D. McFadden"], "venue": null, "citeRegEx": "McFadden,? \\Q1974\\E", "shortCiteRegEx": "McFadden", "year": 1974}, {"title": "Gaussian sampling by local perturbations", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. Int. Conf. on Neural Information Processing Systems (NIPS),", "citeRegEx": "Papandreou and Yuille,? \\Q2010\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2010}, {"title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. IEEE Int. Conf. on Computer Vision (ICCV),", "citeRegEx": "Papandreou and Yuille,? \\Q2011\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2011}, {"title": "Conjugate duality and optimization", "author": ["R.T. Rockafellar"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "Rockafellar,? \\Q1974\\E", "shortCiteRegEx": "Rockafellar", "year": 1974}, {"title": "Finding MAPs for belief networks is NP-hard", "author": ["S.E. Shimony"], "venue": "Artificial Intelligence,", "citeRegEx": "Shimony,? \\Q1994\\E", "shortCiteRegEx": "Shimony", "year": 1994}, {"title": "Tightening LP relaxations for MAP using message passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss"], "venue": "In Conf. Uncertainty in Artificial Intelligence (UAI). Citeseer,", "citeRegEx": "Sontag et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2008}, {"title": "A comparative study of energy minimization methods for markov random fields with smoothness-based priors", "author": ["R. Szeliski", "R. Zabih", "D. Scharstein", "O. Veksler", "V. Kolmogorov", "A. Agarwala", "M. Tappen", "C. Rother"], "venue": null, "citeRegEx": "Szeliski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Szeliski et al\\.", "year": 2007}, {"title": "The supremum of some canonical processes", "author": ["M. Talagrand"], "venue": "American Journal of Mathematics,", "citeRegEx": "Talagrand,? \\Q1994\\E", "shortCiteRegEx": "Talagrand", "year": 1994}, {"title": "Concentration of measure and isoperimetric inequalities in product spaces", "author": ["M. Talagrand"], "venue": "Publications Mathe\u0301matiques de l\u2019Institut des Hautes E\u0301tudes Scientifiques,", "citeRegEx": "Talagrand,? \\Q1995\\E", "shortCiteRegEx": "Talagrand", "year": 1995}, {"title": "Randomized Optimum Models for Structured Prediction", "author": ["D. Tarlow", "R.P. Adams", "R.S. Zemel"], "venue": "In AISTATS,", "citeRegEx": "Tarlow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2006}, {"title": "The complexity of computing the permanent", "author": ["L.G. Valiant"], "venue": "Theoretical computer science,", "citeRegEx": "Valiant,? \\Q1979\\E", "shortCiteRegEx": "Valiant", "year": 1979}, {"title": "A new class of upper bounds on the log partition function", "author": ["M.J. Wainwright", "T.S. Jaakkola", "A.S. Willsky"], "venue": "Trans. on Information Theory,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf)", "author": ["T. Werner"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Werner,? \\Q2008\\E", "shortCiteRegEx": "Werner", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Examples include object detection (Felzenszwalb et al., 2009), stereo vision (Szeliski et al.", "startOffset": 34, "endOffset": 61}, {"referenceID": 17, "context": ", 2009), stereo vision (Szeliski et al., 2007), parsing (Koo et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 7, "context": ", 2007), parsing (Koo et al., 2010), or protein design (Sontag et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 16, "context": ", 2010), or protein design (Sontag et al., 2008).", "startOffset": 27, "endOffset": 48}, {"referenceID": 6, "context": "Indeed, substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity (Kolmogorov, 2006) or by devising approximate methods based on linear programming relaxations (Sontag et al.", "startOffset": 169, "endOffset": 187}, {"referenceID": 16, "context": "Indeed, substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity (Kolmogorov, 2006) or by devising approximate methods based on linear programming relaxations (Sontag et al., 2008; Werner, 2008).", "startOffset": 263, "endOffset": 298}, {"referenceID": 24, "context": "Indeed, substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity (Kolmogorov, 2006) or by devising approximate methods based on linear programming relaxations (Sontag et al., 2008; Werner, 2008).", "startOffset": 263, "endOffset": 298}, {"referenceID": 4, "context": "While models based on random perturbations have been considered recently (Papandreou & Yuille, 2011; Keshet et al., 2011; Tarlow et al., 2012), their relation to the partition function has not.", "startOffset": 73, "endOffset": 142}, {"referenceID": 20, "context": "While models based on random perturbations have been considered recently (Papandreou & Yuille, 2011; Keshet et al., 2011; Tarlow et al., 2012), their relation to the partition function has not.", "startOffset": 73, "endOffset": 142}, {"referenceID": 22, "context": "In general, counting problems are considered very hard, many belonging to the complexity class #P (Valiant, 1979).", "startOffset": 98, "endOffset": 113}, {"referenceID": 15, "context": "Although the MAP problem is NP-hard in general (Shimony, 1994), it is easier than computing the partition function.", "startOffset": 47, "endOffset": 62}, {"referenceID": 14, "context": "(Rockafellar, 1974) Theorem 3.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "To compute the gradient we use Theorem 23 in (Rockafellar, 1974) to differentiate under the integral.", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "When dealing with attractive potentials this was efficiently evaluated using graph-cuts (Boykov et al., 2001).", "startOffset": 88, "endOffset": 109}, {"referenceID": 16, "context": "The MAP was computed in the attractive case using graph-cuts and in the mixed case using MPLP (Sontag et al., 2008).", "startOffset": 94, "endOffset": 115}, {"referenceID": 23, "context": "\u2022 The sum-product form of tree re-weighted belief propagation with uniform distribution over the spanning trees (Wainwright et al., 2005).", "startOffset": 112, "endOffset": 137}, {"referenceID": 21, "context": "(Tsochantaridis et al., 2006)) and can be evaluated through MAP solvers.", "startOffset": 0, "endOffset": 29}, {"referenceID": 18, "context": "Talagrand ((Talagrand, 1994), Proposition 4.", "startOffset": 11, "endOffset": 28}, {"referenceID": 19, "context": "Their approach used the induction method of (Talagrand, 1995) to prove an upper bound using the logistic distribution.", "startOffset": 44, "endOffset": 61}, {"referenceID": 11, "context": "Specifically, the probability of choosing an alternative P [\u0177 \u2208 argmaxy{\u03c6(y) + \u03b3(y)}] follows the Gibbs distribution whenever \u03b3(y) are independent and distributed according to the Gumbel distribution (McFadden, 1974).", "startOffset": 200, "endOffset": 216}, {"referenceID": 20, "context": "For a more general class of probabilistic models that exploit efficient optimization we refer to (Tarlow et al., 2012).", "startOffset": 97, "endOffset": 118}, {"referenceID": 4, "context": "Whenever the perturbations occur in the feature space, random MAP perturbation models relate to PAC-Bayes generalization bounds (Keshet et al., 2011).", "startOffset": 128, "endOffset": 149}, {"referenceID": 3, "context": ", mean field) (Jordan et al., 1999).", "startOffset": 14, "endOffset": 35}, {"referenceID": 23, "context": ", (Wainwright et al., 2005)).", "startOffset": 2, "endOffset": 27}], "year": 2012, "abstractText": "In this paper we relate the partition function to the max-statistics of random variables. In particular, we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models. As a result, we can use efficient MAP solvers such as graph-cuts to evaluate the corresponding partition function. We show that our method excels in the typical \u201chigh signal high coupling\u201d regime that results in ragged energy landscapes difficult for alternative approaches.", "creator": "LaTeX with hyperref package"}}}