{"id": "1206.6411", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On the Difficulty of Nearest Neighbor Search", "abstract": "Fast approximate nearest neighbor (NN) search in large databases is becoming popular. Several powerful learning-based formulations have been proposed recently. However, not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Moreover, we present a theoretical analysis to prove how the difficulty measure (relative contrast) determines/affects the complexity of Local Sensitive Hashing, a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally, we show that most of the previous works in measuring NN search meaningfulness/difficulty can be derived as special asymptotic cases for dense vectors of the proposed measure.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (239kb)", "http://arxiv.org/abs/1206.6411v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.DB cs.IR stat.ML", "authors": ["junfeng he", "sanjiv kumar", "shih-fu chang"], "accepted": true, "id": "1206.6411"}, "pdf": {"name": "1206.6411.pdf", "metadata": {"source": "META", "title": "On the Difficulty of Nearest Neighbor Search", "authors": ["Junfeng He", "Sanjiv Kumar", "Shih-Fu Chang"], "emails": ["jh2700@columbia.edu", "sanjivk@google.com", "sfchang@ee.columbia.edu"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2.1. Definition", "text": "Suppose Dqmin = min i = 1,... n D (xi, q) is the distance to the nearest database sample 1, and Dqmean = Ex [D (x, q)] is the expected distance of a random database sample from the query q. We define the relative contrast for the record X for a query q as: Cqr = Dqmean Dqmin. It is a very intuitive measure of the separability of the nearest neighbor of q from the rest of the database points. If we now take the expectations for queries, the relative contrast for the record X is given: Cr = Eq [Dq mean] Eq [D q min] = Dmean Dmin (1) Intuitively, Cr grasps the difficulty of the NN search in X. Small Cr, more difficult the search. If Cr is close to 1, then a query q has on average almost the same distance to its nearest neighbor as the derivative to a random point in X. This contrast in the database is not very significant."}, {"heading": "2.2. Estimation", "text": "Suppose xj and qj are the jth dimensions of the vectors x and q. Let us define further, Rj = Eq [xj \u2212 qj | p], R = d \u00b2 j = 1Rj. (2) Both Rj and R are random variables (because x j is a random variable). Suppose each Rj has finite averages and variance, which is called \u00b5j = E [Rj]. (2) Without the loss of generality, we can scale the data so that the mean and variance of R are given as, \u00b5 = \u03b1 = 1\u00b5j's database. (Here, if dimensions are independent, then the dimensions are independent. (2) Without the loss of generality, we can scale the data so that the new mean of R is 1. The variance of the scaled data, which is called a normalized variance, is called a norm."}, {"heading": "2.3. Effect of normalized variance \u03c3\u2032 on Cr", "text": "From (4) it is clear that relative contrast is a function of database size n, normalized variance \u03c3 \u2032 2 and distance metric standard p. \u03c3 \u00b2 is a function of data features such as dimensionality and sparseness. Figure 1 shows how Cr changes according to (4) when n is varied from 100 to 100 M, and 0 < \u03c3 \u00b2 < 0.2 (Note that \u03c3 is usually very small for high-dimensional data, e.g. much smaller than 0.1). It is clear that smaller \u03c3 leads to a smaller relative contrast, i.e. a more difficult search for the nearest neighbor. In the diagrams above, p was set to 1, but other values yield similar results. Interestingly, the relative contrast increases with increasing database size n. In other words, the search for the nearest neighbor is more important for a larger database."}, {"heading": "2.4. Data Properties vs \u03c3\u2032", "text": "Since we already know the relationship between Cr and Vj2 by analyzing how the properties of the data affect \u03c3, we will find out how the properties of the data affect Cr, i.e. the difficulty of the NN search. Although many properties of the data can be studied, in this work we focus on Sparsity4It should not be confused with computational simplicity, since the computational costs are higher in larger databases. (A very important property in many areas, such as text, images, and videos), together with other properties such as data dimension and metrics. Suppose that the jth dimensions of the vectors x and q are distributed in the same way as a random variable Vj. But each dimension has only an sj \u2212 probability to have a non-zero value, at which 0 < sj \u2264 1. Denote mj, p as p-th moment of | Vj |, and m \u00b2 as p-th moment of |, and m \u00b2 dimensions: 2p = 2p \u00b2."}, {"heading": "2.5. Data Properties vs Relative Contrast Cr", "text": "We summarize how different database properties and distance metrics affect relative contrast.Data dimensionality (d): From (8) it is unfortunately easy to see that a larger d will lead to smaller \u03c3. Furthermore, from (4), smaller \u03c3 \"implies a lower relative contrast Cr, which makes the NN search less meaningful, indicating the well-known phenomenon of distance concentration in high-dimensional spaces. Fortunately, if dimensions are not independent, the rate at which distances begin to concentrate decreases. Data sparity (s): From (8) we can see that \u03c3 = 1 d1 / 2 \u221a (m \u00b2 2p \u2212 2m2p) + 2m2p s s [m \u00b2 p \u2212 2mp) s + 2mp] s = 2 \u2212 1. If m \u00b2 p \u2212 2mp = 0 becomes smaller (i.e. data vectors have less than 1), vectors become less than v2, the contrast becomes greater, and the relative contrast becomes \u2212 1."}, {"heading": "2.6. Validation of Relative Contrast", "text": "In order to verify the form of relative contrast derived in paragraph 2, we conducted experiments with both synthetic and real data sets, which are summarized below."}, {"heading": "2.6.1. Synthetic Data", "text": "We generated synthetic data by using each dimension as i.i.d of the universal distribution U = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1, Predictedr sC ontr ast = 500, p = 1 3 = 3 123456L pC ontr ast c rd = 60, s = 1 = 1 = 1 = 1, Predicted0 = 1 = 1 = 1, Predictedr sC ontr ast = 3 = 3, 3 = 3 = 3 = 3, 3 = 3 pC ontr ast c rd = 60, s = 0.5, s = 2 = 60, s = 0.5, s = 1000 10000 10000000.5Database size nC ontr rd = 30, s = 1 = 1 = 1 = 1, s = 1 = 1, s = 1 = 1 = 1 = 1, d = 1 = 1 = 1 = 2 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = = = 1 = 1 = 1 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "2.6.2. Real-world Data", "text": "Next, we conducted experiments with four real-world datasets commonly used in computer vision applications: sifting, grit, color, and image. The details of these datasets are in Table 1. The sighting and grit datasets contain 128-dimensional and 384-dimensional vectors, which are mostly dense. On the other hand, both color and image datasets are very high-dimensional and sparse. The color datasets contain color histograms of images, while the image datasets contain presumptive word representations of local features in images. While we derive the shape of relative contrast in Sec. 2, we assume that the dimensions were independent. However, this assumption might not apply to the real-world data. One way to address this problem would be to assume that the dimensions become independent after embedding the data in a reasonable low-dimensional space variation. In these experiments, we define the effective dimension of the number of the dimensions is necessary by 85%."}, {"heading": "3. Relative Contrast and Hashing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Relative Contrast and LSH", "text": "In each hash table, the number of those close to the hash function in LSH is higher than the number close to the hash function in LSH (x). We now provide the following theorems to show how the complexity of LSH.Theorem 3.1 LSH affects the exact complexity of LSH (Cr), as we can find the complexity of LSH.Theorem 3.1 LSH."}, {"heading": "3.2. Relative Contrast and PCA hashing", "text": "In this section, we show that PCA hashing actually looks for projections that maximize the relative contrast in any projection with L2 spacing under some assumptions. (Acommon used hash function in PCA-based hashing methods ish (x) = sgn (wTx + b) (9), where w is heuristically selected as the PCA direction, and b is a threshold normally chosen as E [wTx]. If the data is assumed to be null-centered, i.e., E [x] = 0, it results in b = 0. Since q and x are i.i.d samples from some unknown p (x), E [q] = 0 as well."}, {"heading": "4. Related Works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Previous Works", "text": "Some of the most influential papers analyzing the difficulty of finding NN are (Beyer et al., 1999) and (Francois et al., 2007), whose main results are shown in theorems 4.1 and 4.2.Theorem 4.1 (Beyer et al., 1999) Dqmax = maxi = 1,... n D (xi, q) and Dq min = mini = 1,... n D (xi, q). Iflim d \u2192 \u221e var (D (xi, q) pE [D (xi, q) p]))))) \u2192 0, then for each case \u2265 0, lim d \u2192 \u221e P [Dqmax \u2264 (1 + 0) Dqmin] = 1.Theorem 4.2 (Francois et al., 2007) If each dimension of the data is i.i.d.d., then for each case V ar (| xi \u2212 q | | p) E (| | xi \u2212 q | p) should go through."}, {"heading": "4.2. Relations Between Our Analysis and Previous Works", "text": "Regarding Beyer's work, it should be noted that if the distance function D (xi, q) in Beyer's work is Lp distance, then var (D (xi, q) pE [D (xi, q) p]) = \u03c32\u00b52 = (\u03c3 \u2032) 2. If the search for NN is not very \"meaningful\" because we cannot distinguish the closest neighbor from other points, and our theory works for the worst case (i.e., compare NN point with the worst point at maximum distance), while our work works for the average case.Relation to Francois \"work in Theorem 4.2, a measurement called\" relative variance, \"which is called\" relative variance \"(i.e., compare NN point with the worst point at maximum distance), while our work works only for the average case.Relation to Francois\" work in Theorem 4.2, a measurement called \"relative variance,\" which is defined as \"relative variance\" p. \""}, {"heading": "5. Conclusion and Future Work", "text": "In this paper, we have introduced a new measurement called relative contrast to describe the difficulty of finding the nearest neighbor in a dataset. The proposed measurement can be used to evaluate the influence of several critical data features such as dimensionality, thickness, and database size simultaneously in arbitrarily standardized metric spaces. In addition, we will show how relative contrast determines the difficulty of ANN searches with LSH and provide clues for better parameter settings. In the future, we want to loosen the assumption of independence used in the theory of relative contrast, and also investigate how relative contrast affects the complexity of other approximate NN search methods besides LSH. In addition, we will investigate a better but tougher definition of Cr = Eq [Dqmean Dqmin]."}], "references": [{"title": "On the surprising behavior of distance metrics in high dimensional space", "author": ["C. Aggarwal", "A. Hinneburg", "D. Keim"], "venue": null, "citeRegEx": "Aggarwal et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Aggarwal et al\\.", "year": 2001}, {"title": "Locality-sensitive hashing scheme based on pstable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "In SOGC,", "citeRegEx": "Datar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Datar et al\\.", "year": 2004}, {"title": "The concentration of fractional distances", "author": ["D. Francois", "V. Wertz", "M. Verleysen"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Francois et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Francois et al\\.", "year": 2007}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In VLDB,", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "In CVPR,", "citeRegEx": "Gong and Lazebnik,? \\Q2011\\E", "shortCiteRegEx": "Gong and Lazebnik", "year": 2011}, {"title": "Supplementary material for \u201don the difficulty of nearest neighbor search", "author": ["J. He", "et. al"], "venue": null, "citeRegEx": "He and al.,? \\Q2012\\E", "shortCiteRegEx": "He and al.", "year": 2012}, {"title": "An investigation of practical approximate nearest neighbor algorithms", "author": ["T. Liu", "A.W. Moore", "A. Gray", "K. Yang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "A large number of approximate Nearest Neighbor (NN) search techniques have been proposed in the last decade including hashing and tree-based methods, to name a few, (Datar et al., 2004; Liu et al., 2004; Weiss et al., 2008).", "startOffset": 165, "endOffset": 223}, {"referenceID": 6, "context": "A large number of approximate Nearest Neighbor (NN) search techniques have been proposed in the last decade including hashing and tree-based methods, to name a few, (Datar et al., 2004; Liu et al., 2004; Weiss et al., 2008).", "startOffset": 165, "endOffset": 223}, {"referenceID": 0, "context": "In terms of \u201dmeaningfulness\u201d of NN search problem in a given dataset, most of the existing works have focused on the effect of one data property: dimensionality, that too in an asymptotic sense, showing that NN search will be meaningless when the number of dimensions goes to infinity (Beyer et al., 1999; Aggarwal et al., 2001; Francois et al., 2007).", "startOffset": 285, "endOffset": 351}, {"referenceID": 2, "context": "In terms of \u201dmeaningfulness\u201d of NN search problem in a given dataset, most of the existing works have focused on the effect of one data property: dimensionality, that too in an asymptotic sense, showing that NN search will be meaningless when the number of dimensions goes to infinity (Beyer et al., 1999; Aggarwal et al., 2001; Francois et al., 2007).", "startOffset": 285, "endOffset": 351}, {"referenceID": 3, "context": "In terms of the complexity of approximate NN search methods like Locality Sensitive Hashing (LSH), some general bounds for (Gionis et al., 1999; Indyk & Motwani, 1998) have been presented.", "startOffset": 123, "endOffset": 167}, {"referenceID": 0, "context": "It is interesting to note that (4) also predicts the rate at which contrast changes with d, unlike the previous works (Beyer et al., 1999; Aggarwal et al., 2001) which only show that NN search becomes impossible when dimensionality goes to infinity.", "startOffset": 118, "endOffset": 161}, {"referenceID": 0, "context": "This observation matches the conclusion from (Aggarwal et al., 2001) for dense vectors.", "startOffset": 45, "endOffset": 68}, {"referenceID": 1, "context": "A commonly used hash function in LSH is h(x) = \u230awT x+b t \u230b, where w is a vector with entries sampled from a pstable distribution, and b is uniformly distributed as U [0, t] (Datar et al., 2004).", "startOffset": 173, "endOffset": 193}, {"referenceID": 3, "context": "Note that our theory shares some similarity to the results in (Gionis et al., 1999) about the complexity of LSH, however, it has several unique properties.", "startOffset": 62, "endOffset": 83}, {"referenceID": 2, "context": "2 (Francois et al., 2007) If every dimension of the data is i.", "startOffset": 2, "endOffset": 25}], "year": 2012, "abstractText": "Fast approximate nearest neighbor(NN) search in large databases is becoming popular. Several powerful learning-based formulations have been proposed recently. However, not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Moreover, we present a theoretical analysis to prove how the difficulty measure (relative contrast) determines/affects the complexity of Local Sensitive Hashing, a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally, we show that most of the previous works in measuring NN search meaningfulness/difficulty can be derived as special asymptotic cases for dense vectors of the proposed measure.", "creator": "LaTeX with hyperref package"}}}