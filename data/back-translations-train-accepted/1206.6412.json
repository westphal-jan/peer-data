{"id": "1206.6412", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound", "abstract": "In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (558kb)", "http://arxiv.org/abs/1206.6412v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ming ji", "tianbao yang", "binbin lin", "rong jin", "jiawei han 0001"], "accepted": true, "id": "1206.6412"}, "pdf": {"name": "1206.6412.pdf", "metadata": {"source": "CRF", "title": "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound", "authors": ["Ming Ji", "Tianbao Yang", "Binbin Lin", "Rong Jin", "Jiawei Han"], "emails": ["mingji1@illinois.edu", "yangtia1@msu.edu", "binbinlin@zju.edu.cn", "rongjin@cse.msu.edu", "hanj@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Algorithm and Empirical Validation", "text": "Let X be a compact domain or a manifold in Euclidean space Rd. Let D = {xi, i = 1,.., N | xi, X} be a collection of training examples. We randomly select n examples from D for the label. Without losing generality, we assume that the first n examples are labeled by yl = (y1,.., yn) Rn. Our goal is to learn an accurate prediction function by algorithm 1 A Simple Algorithm for Semisupervised Learning1: Input \u2022 D = {x1,., xN}."}, {"heading": "2.1. A Simple algorithm for Semi-Supervised Learning", "text": "We assume that we must apply the prediction function f (x) to the blank examples (or the marginal distribution PX). To this end, we assume that there is a precise prediction function g (x) in which the blank data is evaluated effectively. More specifically, we define the prediction function f (x) to the blank examples (or the marginal distribution PX). To this end, we assume that there is an accurate prediction function g (x)."}, {"heading": "2.2. Empirical study", "text": "Three real datasets, i.e. insurance, wine and temperature 1, are used in our empirical study; the statistics of these datasets are given in Table 1. the first two datasets come from UC Irvine Machine1http: / / www.remss.com / msuLearning Repository (Frank & Asuncion, 2010), while the task of the last two datasets is to predict temperature based on the coordinates (latitude, longitude) on the Earth's surface. All three datasets are designed for regression tasks with real-world assessments. We select these three datasets because they agree with our assumptions elaborated in Section 3.2.0. We randomly select 90% of the data for training, and use the rest 10% for testing. We randomly select 2%, 3%,."}, {"heading": "3. Generalization Error Bounds", "text": "To analyze the generalization power of the proposed algorithm, we first consider the simple scenario where we have access to an infinite number of unlabeled examples (i.e., the boundary distribution PX), and then present the generalization error, which is limited to a finite number of unlabeled examples."}, {"heading": "3.1. Generalization error for an infinite number of unlabeled examples", "text": "Given the marginal distribution of PX, we define the following assumptions about eigenvalues and eigenfunctions: \"We expect to achieve a better generalization error for the proposed semi-supervised learning algorithm.\" (Smale & Zhou, 2009) \"We expect to achieve a better generalization error for the proposed semi-supervised learning algorithms.\" (Sale) \"We expect to achieve the following assumptions about eigenfunctions and eigenfunctions of a linear operator (Smale & Zhou, 2009).We expect to achieve a better generalization error for the proposed semi-supervised learning algorithm.\" (Smale & Zhou, 2009).We expect to achieve the following assumptions about eigenfunctions and eigenfunctions of a linear operator (Smale & Zhou, 2009).We expect to achieve a better generalization error for the proposed semi-supervised learning algorithm."}, {"heading": "3.2. Generalization error for a finite number of unlabeled examples", "text": "We consider the scenario in which only a finite number (i.e. N) of undescribed examples is available. The key challenge arising from the finite sample analysis is that we do not have access to the eigenfunctions and eigenvalues of L. Instead, we must approximate the eigenfunctions and eigenvalues of L using their empirical counterpart L-N. These approximation errors complicate the analysis. To ensure that the approximation does not significantly increase the regression error, we make the following assumptions: \u2022 B1 Skewed eigenvalues distribution of L-N. We assume that eigenvalues of L-N, i = 1, 2,."}, {"heading": "4. Analysis", "text": "We present the complete analysis for the case of an infinite number of unlabeled examples and sketch only the analysis for a finite number of unlabeled examples due to lack of space. More detailed analysis can be found in the supplementary materials."}, {"heading": "4.1. Analysis for an infinite number of unlabeled examples", "text": "If we have an infinite number of undescribed examples, the learned prediction function is represented by g (x) = 2 x (x) = 2 x (x) = 1 x (x) = 1 x (x) = 2 x (x) = 2 x (x) = 2 x (x) = 2 x (x) = 2 x (x) = 2 x (x) = 2 x (x) = 2 x (x) = 2 x (x) = 2 x (x). Using the eigenfunctions of L, we write g (x), the optimal prediction function defined in (3), as g (x) = 2 x (x). We define gs (x), the projection of g (x) into subspace, asgs (x) = s) = 1 x (x). We use gs (x), we decompose the generalization error of g (x) into two parts, i.e."}, {"heading": "4.2. Analysis for a finite number of unlabeled examples", "text": "(2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2)) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) () (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) ("}, {"heading": "5. Conclusions", "text": "In this paper, we present a very simple algorithm for semi-supervised learning. Our analysis shows that the proposed algorithm, under appropriate assumptions about the integral operator, achieves a better generalization error than a supervised learning algorithm. In the future, we plan to further improve the scalability of the proposed algorithm by examining various approaches (e.g. the Nystro-m method) to efficiently estimate own functions from a large number of unlabeled examples."}, {"heading": "Acknowledgments", "text": "The work was partially supported by the U.S. Army Research Laboratory under Cooperation Agreement No. W911NF-09-2-0053 (NS-CTA), NSF IIS-0905215, NSF IIS-0643494, the U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265 and the Office of Navy Research (ONR Award N00014-09-1-0663 and N00014-12-1-0431)."}], "references": [{"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Belkin", "Mikhail", "Niyogi", "Partha", "Sindhwani", "Vikas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Near-optimal signal recovery from random projections: Universal encoding strategies", "author": ["Cand\u00e8s", "Emmanuel J", "Tao", "Terence"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "On the exponential value of labeled samples", "author": ["Castelli", "Vittorio", "Cover", "Thomas M"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Castelli et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Castelli et al\\.", "year": 1995}, {"title": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter", "author": ["Castelli", "Vittorio", "Cover", "Thomas M"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Castelli et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Castelli et al\\.", "year": 1996}, {"title": "Support vector regression machines", "author": ["Drucker", "Harris", "Burges", "Christopher J. C", "Kaufman", "Linda", "Smola", "Alex J", "Vapnik", "Vladimir"], "venue": "In NIPS, pp", "citeRegEx": "Drucker et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Drucker et al\\.", "year": 1996}, {"title": "Semisupervised learning in gigantic image collections", "author": ["Fergus", "Rob", "Weiss", "Yair", "Torralba", "Antonio"], "venue": "In NIPS, pp", "citeRegEx": "Fergus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2009}, {"title": "An empirical featurebased learning algorithm producing sparse approximations", "author": ["Guo", "Xin", "Zhou", "Ding-Xuan"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Guo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2011}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["Koltchinskii", "Vladimir"], "venue": null, "citeRegEx": "Koltchinskii and Vladimir.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii and Vladimir.", "year": 2011}, {"title": "Sparsity in multiple kernel learning", "author": ["Koltchinskii", "Vladimir", "Yuan", "Ming"], "venue": "Annuals of Statistics,", "citeRegEx": "Koltchinskii et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koltchinskii et al\\.", "year": 2010}, {"title": "Statistical analysis of semi-supervised regression", "author": ["Lafferty", "John D", "Wasserman", "Larry A"], "venue": "In NIPS,", "citeRegEx": "Lafferty et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2007}, {"title": "Some properties of gaussian reproducing kernel hilbert spaces and their implications for function approximation and learning theory", "author": ["Minh", "Ha Quang"], "venue": "Constructive Approximation,", "citeRegEx": "Minh and Quang.,? \\Q2010\\E", "shortCiteRegEx": "Minh and Quang.", "year": 2010}, {"title": "Statistical analysis of semi-supervised learning: The limit of infinite unlabelled data", "author": ["Nadler", "Boaz", "Srebro", "Nathan", "Zhou", "Xueyuan"], "venue": "In NIPS,", "citeRegEx": "Nadler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nadler et al\\.", "year": 2009}, {"title": "Manifold regularization and semi-supervised learning: Some theoretical analyses", "author": ["P. Niyogi"], "venue": "Technical report,", "citeRegEx": "Niyogi,? \\Q2008\\E", "shortCiteRegEx": "Niyogi", "year": 2008}, {"title": "Generalization error bounds in semisupervised classification under the cluster assumption", "author": ["Rigollet", "Philippe"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rigollet and Philippe.,? \\Q2007\\E", "shortCiteRegEx": "Rigollet and Philippe.", "year": 2007}, {"title": "Ridge regression learning algorithm in dual variables", "author": ["Saunders", "Craig", "Gammerman", "Alexander", "Vovk", "Volodya"], "venue": "In ICML, pp", "citeRegEx": "Saunders et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Saunders et al\\.", "year": 1998}, {"title": "Learning with labeled and unlabeled data", "author": ["Seeger", "Matthias"], "venue": "Technical report,", "citeRegEx": "Seeger and Matthias.,? \\Q2001\\E", "shortCiteRegEx": "Seeger and Matthias.", "year": 2001}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["Singh", "Aarti", "Nowak", "Robert D", "Zhu", "Xiaojin"], "venue": "In NIPS, pp", "citeRegEx": "Singh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2008}, {"title": "Semi-supervised learning using sparse eigenfunction bases", "author": ["Sinha", "Kaushik", "Belkin", "Mikhail"], "venue": "In NIPS, pp", "citeRegEx": "Sinha et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sinha et al\\.", "year": 2009}, {"title": "Geometry on probability spaces", "author": ["Smale", "Steve", "Zhou", "Ding-Xuan"], "venue": "Constructive Approximation,", "citeRegEx": "Smale et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Smale et al\\.", "year": 2009}, {"title": "An explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels", "author": ["Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2006}, {"title": "Introduction to Nonparametric Estimation", "author": ["Tsybakov", "Alexandre B"], "venue": "Springer, 1st edition,", "citeRegEx": "Tsybakov and B.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov and B.", "year": 2008}, {"title": "Analysis of spectral kernel design based semi-supervised learning", "author": ["Zhang", "Tong", "Ando", "Rie Kubota"], "venue": "In NIPS, pp", "citeRegEx": "Zhang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}, {"title": "Semi-supervised learning literature survey", "author": ["Zhu", "Xiaojin"], "venue": "Technical report, Computer Sciences, University of Wisconsin-Madison,", "citeRegEx": "Zhu and Xiaojin.,? \\Q2008\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "A number of theories have been proposed for semi-supervised learning, and most of them are based on one of the two assumptions: (1) the cluster assumption (Seeger, 2001; Rigollet, 2007; Lafferty & Wasserman, 2007; Singh et al., 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)", "startOffset": 155, "endOffset": 255}, {"referenceID": 13, "context": ", 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)", "startOffset": 215, "endOffset": 257}, {"referenceID": 12, "context": "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning.", "startOffset": 43, "endOffset": 92}, {"referenceID": 13, "context": "However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples.", "startOffset": 44, "endOffset": 58}, {"referenceID": 17, "context": "In addition, the learning algorithms suggested in (Rigollet, 2007; Singh et al., 2008; Zhang & Ando, 2005) are difficult to implement efficiently even if the cluster assumption holds, making them unpractical", "startOffset": 50, "endOffset": 106}, {"referenceID": 12, "context": "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples.", "startOffset": 72, "endOffset": 932}, {"referenceID": 12, "context": "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples. Furthermore, Singh et al. (2008) show that the mixture components can be identified if PX is a mixture of a finite number of smooth density functions and the separation/overlap between different mixture components is significantly large.", "startOffset": 72, "endOffset": 1134}, {"referenceID": 6, "context": "Unlike the previous studies of exploring eigenfunctions for semi-supervised learning (Fergus et al., 2009; Sinha & Belkin, 2009), we show that under appropriate assumptions, the proposed algorithm achieves a better generalization error bound than supervised learning algorithms.", "startOffset": 85, "endOffset": 128}, {"referenceID": 20, "context": "The assumption of skewed eigenvalue distributions has been verified and used in multiple studies of kernel learning (Koltchinskii, 2011; Steinwart et al., 2006; Minh, 2010; Zhang & Ando, 2005), while the assumption of bounded eigenvectors was mostly found in the study of compressive sensing (Cand\u00e8s & Tao, 2006).", "startOffset": 116, "endOffset": 192}, {"referenceID": 15, "context": ", Kernel Ridge Regression (KRR) (Saunders et al., 1998) and Support Vector Regression (SVR) (Drucker et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 5, "context": ", 1998) and Support Vector Regression (SVR) (Drucker et al., 1996), and a state-of-the-art algorithm for semi-supervised regression, i.", "startOffset": 44, "endOffset": 66}, {"referenceID": 0, "context": ", Laplacian Regularized Least Squares (LapRLS) (Belkin et al., 2006), are used as the baselines.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "Similar to many studies (Koltchinskii & Yuan, 2010; Steinwart et al., 2006; Minh, 2010), we assume the eigenvalues follow a power law distribution, i.", "startOffset": 24, "endOffset": 87}], "year": 2012, "abstractText": "In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study.", "creator": " TeX output 2012.05.20:2113"}}}