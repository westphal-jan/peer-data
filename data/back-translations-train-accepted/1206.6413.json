{"id": "1206.6413", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A convex relaxation for weakly supervised classifiers", "abstract": "This paper introduces a general multi-class approach to weakly supervised classification. Inferring the labels and learning the parameters of the model is usually done jointly through a block-coordinate descent algorithm such as expectation-maximization (EM), which may lead to local minima. To avoid this problem, we propose a cost function based on a convex relaxation of the soft-max loss. We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program (SDP). Empirically, our method compares favorably to standard ones on different datasets for multiple instance learning and semi-supervised learning as well as on clustering tasks.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (877kb)", "http://arxiv.org/abs/1206.6413v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["armand joulin", "francis r bach"], "accepted": true, "id": "1206.6413"}, "pdf": {"name": "1206.6413.pdf", "metadata": {"source": "META", "title": "A convex relaxation for weakly supervised classifiers", "authors": ["Armand Joulin", "Francis Bach"], "emails": ["armand.joulin@ens.fr", "francis.bach@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "In general, the labelling of training data is complete and precise. In the participating countries, this requirement is also not met due to the small number of points. The aim is to use the unspoken points to improve the performance of the player. In the participating countries, this is the case (Chapelle et al., 2006)."}, {"heading": "1.1. Related work", "text": "In fact, most of them are able to survive themselves, and that they are able to survive themselves. \"Most of them are able to survive themselves,\" he says. \"But most of them are able to survive themselves.\" Most of them are able to survive themselves. \"Most of them are able to survive themselves.\" Most of them are able to survive themselves, \"he says.\" But most of them are able to survive themselves. \"\" Most of them are able to survive themselves. \"\" Most of them are able to survive themselves. \"\" Most of them are able to survive themselves. \"\" Most of them are able to survive themselves, \"he says.\" Most of them have survived themselves. \"\" Most of them have survived themselves. \"\" Most of them have survived themselves. \"\" Most of them have survived themselves. \"\" Most of them have survived themselves. \"\""}, {"heading": "2. Proposed model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Notations", "text": "We assume that we observe I-bags of instances. For i in {1,.., I}, Ni is the set of instances in the i-bag, and Ni = | Ni | is its cardinality. We use N = \u2211 i Ni to denote the total number of instances. In each bag i, an instance n in Ni is associated with a attribute x and a label in L in a specific attribute and label space. In this paper, we assume that this label is common to all instances of the same bag and only partially explain the instances contained in the bag. We are therefore interested in finding a latent label zn-P that would give a better understanding of the data. We denounce through P and L the cardinalities of P and L. We also assume that the latent label zn of an instance n can only assume its values in a substance n."}, {"heading": "2.2. Problem formulation", "text": "In this paper we are interested in multi-class setting, where a natural choice for the soft loss function (Hastie et al., 2001)."}, {"heading": "3. Convex relaxation", "text": "(D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D) D (D) D (D) D (D) D (D) D (D) D (D) D) D (D) D (D) D) D (D) D (D) D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D (D) D) D (D) D (D) D) D (D) D) D (D) D) D (D) D) D (D) D (D) D) D (D) D (D) D) D (D) D (D) D) D (D) D (D) D) D (D) D (D) D) D (D) D) D (D) (D) D) D (D) D) D (D) (D) D) (D) D) D (D) (D) D) (D) D) (D) D) (D) (D) D) (D) D) (D) D) (D) (D) D) (D) D) (D) (D) D) (D) D) (D) (D) D) (D) (D) (D) D) (D) (D) (D) D) (D) (D) (D) (D) (D) (D) D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) (D) D) (D) (D) (D) (D) (D) (D) (D) (D) D) (D) (D) (D) D) (D) (D)"}, {"heading": "4. Optimization", "text": "Since our optimization involves a maximization in our inner loop, it cannot be solved directly by a universal toolbox. We propose an algorithm dedicated to our case. In the rest of this essay, we refer to maximization as an inner loop and the general minimization of our cost function as an outer loop."}, {"heading": "4.1. Inner loop", "text": "The evaluation of the cost function defined in Equation (5) involves maximizing the sum of entropy of 1 \u00b0 C and a function T defined as follows: T (1) = \u2212 12\u03bb tr (1) RTZR (1) TK). We use a proximal method with a reweighted Kullback-Leibler (KL) divergence which, of course, forces the point-by-point positivity contraction in W and leads to an efficient Bregman projection with a KL divergence (an I projection to be more precise) on the rest of the constraints that define W. More specifically, the proximal update is given by maximizing the following function: lD (2) = tr (1) T (1) T (1) -2) \u2212 2 (an optimal convergence of 0) \u2212 3 (2), whereby the convergence of 0), (6) where the L (2) of D (1) and D (1) can be contracted (1) -D (1) -D (1) -D (1)."}, {"heading": "4.2. Outer loop", "text": "Many approaches have been proposed to solve these kinds of problems (Goemans & Williamson, 1995; Burer & Monteiro, 2003; Journe \u0301 e et al., 2010), but to the best of their knowledge they all assume that the function and its gradient can be efficiently calculated and focus on projection; this is not the case in our problem, and we therefore propose a method adapted to our respective settings.First, to simplify projection on the ENR, we replace our cost function g (Z) with its diagonally rescaled version gR (Z) = g (diag (Z) \u2212 1 / 2Zdiag (Z) \u2212 1 / 2). Note that even if this function is independently based on the ENR, it coincides with g (Z) on the ENR and its limitation to this convex."}, {"heading": "4.3. Rounding", "text": "Following Bach & Harchaoui (2007) and Joulin et al. (2010), we use k-mean clusters on the eigenvectors associated with the k highest eigenvalues (Ng et al., 2001) to obtain a rounded solution. This z * is then used to initialize an EM method for solving the problem defined in Equation (1) and to obtain the parameters (w, b) of the classifier, resulting in finer details that are not captured by convex relativization. A peculiarity of the MIL framework is that strictly no point from a negative bag should be classified as positive, leading to the addition of equation (1), the following linear constraints on the parameters of the classifier:"}, {"heading": "5. Results", "text": "Implementation. Our algorithm is implemented in MATLAB and takes 1 to 5 minutes for 500 points. Note that we can efficiently calculate the solutions for different values of \u03bb by warm starts. Our total complexity is O (N3), but we can scale up to several thousand points. The complexity of the different steps in our algorithm is shown in Figure 1. For larger data sets we can use our relaxation on substances or on clustering the instances (with means) and initialize the EM on the complete data set."}, {"heading": "5.1. Discriminative clustering", "text": "In this section, we compare our method with two different discriminatory cluster methods in the case of multi-class formation: the SDP relaxation of the Soft Max problem without Intercept (Guo & Schuurmans, 2008) and the discriminatory cluster system introduced by Bach and Harchaoui (2007), the latter comparison is relevant because it proposes a convex cost function based on square loss with Intercept. In Figure 2, we consider as proof of the concept two toy examples where the goal is to find 3 and 5 clusters with linear cores and N = 500. Even if the clusters are linearly separable, the amount of value w and b leading to a perfect separation is very small (Figure 2, Panel (a)), which makes the problem challenging. For a fair comparison, we test different regulation parameters and show the one that leads to the best performance. We show the matrix that we have obtained for the matrix, which we see for our method x2, and that our method x3 has a clear matrix value."}, {"heading": "5.2. Multiple instance learning", "text": "In Figure 4, we show some comparisons with other MIL methods on standard data sets (Dietterich & Lathrop, 1997; Andrews et al., 2003) for a variety of tasks: predicting drug activity (Musk), image classification (Fox, Tiger and Elephant), and text classification (trec1). For comparison, we use the setting described by Andrews et al. (2003), where we create 10 random splits of data, train 90% of them, and test the remaining 10%. We test our algorithm with and without intercept and with uniform or bag-specific weights (i.e. 1 INi for instance in the bag i) and compare it with some classic MIL algorithms. Note that we have only tried one linear core, and choose the regulation parameter based on a double cross-validation for each split. Our algorithm achieves comparable performance with methods dedicated to MIL problems."}, {"heading": "5.3. Semi-supervised learning", "text": "For the SSL setting, we choose the standard SSL datasets and compare them with the methods proposed in Chapelle et al. (2006). The benchmarks (linear and non-linear) are based on an SVM formulation and the benchmark (Entropy-Reg.) uses entropy regularization. We use our method with either a linear or an RBF kernel. To determine our parameters, we follow the experimental setup of Chapelle et al. (2006). Each sentence contains 1500 points and either l = 10 or 100 of them are labeled. Results are shown in Figure 5. As the benchmarks and our formulation are very similar, the performances at l = 100 are largely similar. However, if l = 10 is more robust, our method will perform significantly higher, showing that convex relaxation is less sensitive to noise and poorly labeled data."}, {"heading": "6. Conclusion", "text": "In this paper, we propose a convex relaxation of a general cost function for poorly monitored problems. We show the importance of a narrow convex relaxation compared to relaxation where either the associated linear classifier is approximate (absence of intercept) or the loss function (square loss instead of softmax loss). Our comparison with non-convex standard methods for MIL and SSL shows the importance of initialization for the robustness of the approach. We believe that convex loosening is a powerful tool to obtain good initializations for non-convex problems. The compromise is that these methods are usually not scalable, which suggests applying them to subsets of dots or after a quantization step to initialize a more efficient algorithm, such as EM.Acknowledgement. This paper was partially supported by the European Research Council (SIERRA and VIDEOWORLORD projects)."}], "references": [{"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In NIPS,", "citeRegEx": "Andrews et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2003}, {"title": "A boosting approach to multiple instance learning", "author": ["P. Auer", "R. Ortner"], "venue": "In ECML,", "citeRegEx": "Auer and Ortner,? \\Q2004\\E", "shortCiteRegEx": "Auer and Ortner", "year": 2004}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "Yang", "M-H", "S. Belongie"], "venue": "In CVPR,", "citeRegEx": "Babenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Babenko et al\\.", "year": 2009}, {"title": "Diffrac : a discriminative and flexible framework for clustering", "author": ["F. Bach", "Z. Harchaoui"], "venue": "In NIPS,", "citeRegEx": "Bach and Harchaoui,? \\Q2007\\E", "shortCiteRegEx": "Bach and Harchaoui", "year": 2007}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Regularization and semi-supervised learning on large graphs", "author": ["M. Belkin", "I. Matveeva", "P. Niyogi"], "venue": "In COLT,", "citeRegEx": "Belkin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2004}, {"title": "Semi-supervised support vector machines", "author": ["K. Bennett", "A. Demiriz"], "venue": "In NIPS,", "citeRegEx": "Bennett and Demiriz,? \\Q1998\\E", "shortCiteRegEx": "Bennett and Demiriz", "year": 1998}, {"title": "Multi-instance tree learning", "author": ["H. Blockeel", "D. Page", "A. Srinivasan"], "venue": "In ICML,", "citeRegEx": "Blockeel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blockeel et al\\.", "year": 2005}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In COLT,", "citeRegEx": "Blum and Mitchell,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell", "year": 1998}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["S. Burer", "R.D.C. Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "Burer and Monteiro,? \\Q2003\\E", "shortCiteRegEx": "Burer and Monteiro", "year": 2003}, {"title": "Image categorization by learning and reasoning with regions", "author": ["Y. Chen", "Wang", "James Z"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2004}, {"title": "Learning from ambiguously labeled images", "author": ["T. Cour", "Sapp", "Ben", "Jordan", "Chris", "Taskar"], "venue": "In CVPR,", "citeRegEx": "Cour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2009}, {"title": "Discriminative cluster analysis", "author": ["F. De la Torre", "K. Takeo"], "venue": "In ICML,", "citeRegEx": "Torre and Takeo,? \\Q2006\\E", "shortCiteRegEx": "Torre and Takeo", "year": 2006}, {"title": "Solving the multipleinstance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop"], "venue": "Artificial Intelligence,", "citeRegEx": "Dietterich and Lathrop,? \\Q1997\\E", "shortCiteRegEx": "Dietterich and Lathrop", "year": 1997}, {"title": "An iterative procedure for estimation in contingency tables", "author": ["S.E. Fienberg"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Fienberg,? \\Q1970\\E", "shortCiteRegEx": "Fienberg", "year": 1970}, {"title": "Deterministic annealing for multiple-instance learning", "author": ["P.V. Gehler", "O. Chapelle"], "venue": "In AISTATS,", "citeRegEx": "Gehler and Chapelle,? \\Q2007\\E", "shortCiteRegEx": "Gehler and Chapelle", "year": 2007}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM,", "citeRegEx": "Goemans and Williamson,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson", "year": 1995}, {"title": "Convex relaxations of latent variable training", "author": ["Y. Guo", "D. Schuurmans"], "venue": "In NIPS,", "citeRegEx": "Guo and Schuurmans,? \\Q2008\\E", "shortCiteRegEx": "Guo and Schuurmans", "year": 2008}, {"title": "Learning from ambiguously labeled examples", "author": ["E. Hullermeier", "J. Beringer"], "venue": "In IDA,", "citeRegEx": "Hullermeier and Beringer,? \\Q2006\\E", "shortCiteRegEx": "Hullermeier and Beringer", "year": 2006}, {"title": "Learning with multiple labels", "author": ["R. Jin", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Jin and Ghahramani,? \\Q2003\\E", "shortCiteRegEx": "Jin and Ghahramani", "year": 2003}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": null, "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Efficient optimization for discriminative latent class models", "author": ["A. Joulin", "F. Bach", "J. Ponce"], "venue": "In NIPS,", "citeRegEx": "Joulin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2010}, {"title": "Lowrank optimization on the cone of positive semidefinite matrices", "author": ["M. Journ\u00e9e", "F. Bach", "Absil", "P.-A", "R. Sepulchre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Marginalized multi-instance kernels", "author": ["J.T. Kwok", "P. Cheung"], "venue": "In IJCAI,", "citeRegEx": "Kwok and Cheung,? \\Q2007\\E", "shortCiteRegEx": "Kwok and Cheung", "year": 2007}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": null, "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Multiple-instance learning for natural scene classification", "author": ["O. Maron", "A.L. Ratan"], "venue": "In ICML,", "citeRegEx": "Maron and Ratan,? \\Q1998\\E", "shortCiteRegEx": "Maron and Ratan", "year": 1998}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Supervised Versus Multiple Instance Learning: An Empirical Comparison", "author": ["S. Ray", "M. Craven"], "venue": "In ICML,", "citeRegEx": "Ray and Craven,? \\Q2005\\E", "shortCiteRegEx": "Ray and Craven", "year": 2005}, {"title": "Multiple instance boosting for object detection", "author": ["P. Viola", "Platt", "John C", "C. Zhang"], "venue": "In NIPS,", "citeRegEx": "Viola et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2006}, {"title": "Adaptive p-posterior mixture-model kernels for multiple instance learning", "author": ["Wang", "H.-Y", "Q. Yang", "H. Zha"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Solving multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J. Zucker"], "venue": "In ICML,", "citeRegEx": "Wang and Zucker,? \\Q2000\\E", "shortCiteRegEx": "Wang and Zucker", "year": 2000}, {"title": "Unsupervised and semisupervised multi-class support vector machines", "author": ["L. Xu", "D. Schuurmans"], "venue": "In AAAI,", "citeRegEx": "Xu and Schuurmans,? \\Q2005\\E", "shortCiteRegEx": "Xu and Schuurmans", "year": 2005}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}, {"title": "Logistic regression and boosting for labeled bags of instances", "author": ["X. Xu", "E. Frank"], "venue": "In KDDM,", "citeRegEx": "Xu and Frank,? \\Q2004\\E", "shortCiteRegEx": "Xu and Frank", "year": 2004}, {"title": "Image database retrieval with multiple-instance learning techniques", "author": ["C. Yang"], "venue": "In ICDE,", "citeRegEx": "Yang,? \\Q2000\\E", "shortCiteRegEx": "Yang", "year": 2000}, {"title": "Adapting rbf neural networks to multi-instance learning", "author": ["M. Zhang", "Z. Zhou"], "venue": "Neural Processing Letters,", "citeRegEx": "Zhang and Zhou,? \\Q2006\\E", "shortCiteRegEx": "Zhang and Zhou", "year": 2006}, {"title": "Em-dd: An improved multiple-instance learning technique", "author": ["Q. Zhang", "S.A. Goldman"], "venue": "In NIPS,", "citeRegEx": "Zhang and Goldman,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Goldman", "year": 2001}, {"title": "On the relation between multi-instance learning and semi-supervised learning", "author": ["Z. Zhou", "J. Xu"], "venue": "In ICML,", "citeRegEx": "Zhou and Xu,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Xu", "year": 2007}, {"title": "Semi-supervised learning literature", "author": ["X. Zhu"], "venue": null, "citeRegEx": "Zhu,? \\Q2006\\E", "shortCiteRegEx": "Zhu", "year": 2006}], "referenceMentions": [{"referenceID": 28, "context": "1998), object detection (Viola et al., 2006), object tracking in video (Babenko et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 2, "context": ", 2006), object tracking in video (Babenko et al., 2009), and image database retrieval (Yang, 2000).", "startOffset": 34, "endOffset": 56}, {"referenceID": 34, "context": ", 2009), and image database retrieval (Yang, 2000).", "startOffset": 38, "endOffset": 50}, {"referenceID": 7, "context": "For example, some are based on boosting (Auer & Ortner, 2004), others on nearest neighbors (Wang & Zucker, 2000), on neural networks (Zhang & Zhou, 2006), on decision trees (Blockeel et al., 2005), or the construction of an appropriate kernel (Wang et al.", "startOffset": 173, "endOffset": 196}, {"referenceID": 29, "context": ", 2005), or the construction of an appropriate kernel (Wang et al., 2008; G\u00e4rtner et al., 2002; Kwok & Cheung, 2007).", "startOffset": 54, "endOffset": 116}, {"referenceID": 0, "context": "Much of the work in the MIL community has focused on the use of discriminative classifiers, the most popular one being the support vector machine (SVM) (Andrews et al., 2003; Chen & Wang, 2004; Gehler & Chapelle, 2007).", "startOffset": 152, "endOffset": 218}, {"referenceID": 38, "context": "Many semi-supervised learning (SSL) methods have also been proposed in the past decade (see, e.g., Chapelle et al., 2006; Zhu, 2006).", "startOffset": 87, "endOffset": 132}, {"referenceID": 20, "context": "For example, some are based on maximizing the margin with an SVM framework (Joachims, 1999; Bennett & Demiriz, 1998; Xu & Schuurmans, 2005), and others use the unlabeled data for regularization (Belkin et al.", "startOffset": 75, "endOffset": 139}, {"referenceID": 5, "context": "For example, some are based on maximizing the margin with an SVM framework (Joachims, 1999; Bennett & Demiriz, 1998; Xu & Schuurmans, 2005), and others use the unlabeled data for regularization (Belkin et al., 2004) or co-training of weak classifiers (Blum & Mitchell, 1998).", "startOffset": 194, "endOffset": 215}, {"referenceID": 21, "context": "In this paper we use instead a natural cluster-size balancing term corresponding to an entropy penalization (Chapelle et al., 2006; Joulin et al., 2010).", "startOffset": 108, "endOffset": 152}, {"referenceID": 11, "context": "The idea of using a convex cost function in the weakly supervision context has been already studied in different contexts such as, for example, ambiguous labeling (Cour et al., 2009) or discriminative clustering (Xu et al.", "startOffset": 163, "endOffset": 182}, {"referenceID": 32, "context": ", 2009) or discriminative clustering (Xu et al., 2005; Bach & Harchaoui, 2007).", "startOffset": 37, "endOffset": 78}, {"referenceID": 30, "context": "For example, following the SVM approach of Xu et al. (2005), algorithms using linear discriminant analysis (De la Torre & Takeo, 2006) or ridge regression (Bach & Harchaoui, 2007) have been proposed.", "startOffset": 43, "endOffset": 60}, {"referenceID": 20, "context": ", 2006; Joulin et al., 2010). The link between SSL and MIL has been widely studied in the community. For example, in the context of image segmentation with text annotation, Barnard et al. (2003) propose a general weakly supervised model based on a multi-modal extension to a mixture of latent Dirichlet allocation.", "startOffset": 8, "endOffset": 195}, {"referenceID": 20, "context": ", 2006; Joulin et al., 2010). The link between SSL and MIL has been widely studied in the community. For example, in the context of image segmentation with text annotation, Barnard et al. (2003) propose a general weakly supervised model based on a multi-modal extension to a mixture of latent Dirichlet allocation. An important issue with this family of generative models is that learning the parameters is often untractable. Another example is Zhou & Xu (2007) who use the relation between MIL and SSL to develop a method for MIL.", "startOffset": 8, "endOffset": 462}, {"referenceID": 11, "context": "The idea of using a convex cost function in the weakly supervision context has been already studied in different contexts such as, for example, ambiguous labeling (Cour et al., 2009) or discriminative clustering (Xu et al., 2005; Bach & Harchaoui, 2007). In this paper, we are interested in the convex relaxation of a general multiclass loss function, i.e., the soft-max loss. Guo & Schuurmans (2008) propose a related relaxation but do not consider the intercept in the linear classifier.", "startOffset": 164, "endOffset": 401}, {"referenceID": 11, "context": "The idea of using a convex cost function in the weakly supervision context has been already studied in different contexts such as, for example, ambiguous labeling (Cour et al., 2009) or discriminative clustering (Xu et al., 2005; Bach & Harchaoui, 2007). In this paper, we are interested in the convex relaxation of a general multiclass loss function, i.e., the soft-max loss. Guo & Schuurmans (2008) propose a related relaxation but do not consider the intercept in the linear classifier. We extend their work to the case of linear classifiers with an intercept and show in the experiment section, why this difference is crucial when it comes to classification. Note that by using kernels, we can use non-linear classifiers as well. Also, our dedicated optimization scheme is more scalable than the one developed in Guo & Schuurmans (2008) and could be applied to their problem as well.", "startOffset": 164, "endOffset": 841}, {"referenceID": 32, "context": "Another solution used in the discriminative clustering community is to add constraints on the number of elements per class and per bag (Xu et al., 2005; Bach & Harchaoui, 2007).", "startOffset": 135, "endOffset": 176}, {"referenceID": 21, "context": "Penalizing by this entropy turns out to be equivalent to maximizing the log-likelihood of a graphical model where the features xn explain the labels yn through the latent labels zn (Joulin et al., 2010).", "startOffset": 181, "endOffset": 202}, {"referenceID": 14, "context": "The I-projection can be done efficiently with an iterative proportional fitting procedure (IPFP), which is guaranteed to converge to the global minimum with linear convergence rate (Fienberg, 1970).", "startOffset": 181, "endOffset": 197}, {"referenceID": 22, "context": "Many approaches have been proposed to solve this type of problems (Goemans & Williamson, 1995; Burer & Monteiro, 2003; Journ\u00e9e et al., 2010) but, to the best of our knowledge, they all assume that the function and its gradient can be computed efficiently and put the emphasis on the projection.", "startOffset": 66, "endOffset": 140}, {"referenceID": 26, "context": "(2010), we use k-means clustering on the eigenvectors associated with the k highest eigenvalues (Ng et al., 2001) to obtain a rounded solution z.", "startOffset": 96, "endOffset": 113}, {"referenceID": 21, "context": "and Joulin et al. (2010), we use k-means clustering on the eigenvectors associated with the k highest eigenvalues (Ng et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 24, "context": "The projection over this set of linear constraints is performed efficiently with an homotopy algorithm in the dual (Mairal et al., 2010).", "startOffset": 115, "endOffset": 136}, {"referenceID": 3, "context": "In this section, we compare our method to two different discriminative clustering methods for the multiclass case: the SDP relaxation of the soft-max problem with no intercept (Guo & Schuurmans, 2008) and the discriminative clustering framework introduced by Bach and Harchaoui (2007). The latter comparison is relevant since they propose a convex cost function based on the square loss with intercept.", "startOffset": 259, "endOffset": 285}, {"referenceID": 0, "context": "In Figure 4, we show some comparisons with other MIL methods on standard datasets (Dietterich & Lathrop, 1997; Andrews et al., 2003) for a variety of tasks:", "startOffset": 82, "endOffset": 132}, {"referenceID": 0, "context": "8 mi-SVM (Andrews et al., 2003) 87.", "startOffset": 9, "endOffset": 31}, {"referenceID": 0, "context": "6 MI-SVM (Andrews et al., 2003) 77.", "startOffset": 9, "endOffset": 31}, {"referenceID": 29, "context": "9 PPMM Kernel (Wang et al., 2008) 95.", "startOffset": 14, "endOffset": 33}, {"referenceID": 0, "context": "For comparison, we use the setting described by Andrews et al. (2003), where we create 10 random splits of the data, train on 90% of them and test on the remaining 10%.", "startOffset": 48, "endOffset": 70}], "year": 2012, "abstractText": "This paper introduces a general multi-class approach to weakly supervised classification. Inferring the labels and learning the parameters of the model is usually done jointly through a block-coordinate descent algorithm such as expectation-maximization (EM), which may lead to local minima. To avoid this problem, we propose a cost function based on a convex relaxation of the soft-max loss. We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program (SDP). Empirically, our method compares favorably to standard ones on different datasets for multiple instance learning and semi-supervised learning, as well as on clustering tasks.", "creator": "LaTeX with hyperref package"}}}