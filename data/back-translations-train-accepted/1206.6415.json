{"id": "1206.6415", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "The Big Data Bootstrap", "abstract": "The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB's behavior, including a study of its statistical correctness, its large-scale implementation and performance, selection of hyperparameters, and performance on real data.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (385kb)", "http://arxiv.org/abs/1206.6415v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: text overlap witharXiv:1112.5016"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: text overlap witharXiv:1112.5016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ariel kleiner", "ameet talwalkar", "purnamrita sarkar", "michael i jordan"], "accepted": true, "id": "1206.6415"}, "pdf": {"name": "1206.6415.pdf", "metadata": {"source": "META", "title": "The Big Data Bootstrap", "authors": ["Ariel Kleiner", "Ameet Talwalkar", "Purnamrita Sarkar"], "emails": ["akleiner@cs.berkeley.edu", "ameet@cs.berkeley.edu", "psarkar@cs.berkeley.edu", "jordan@cs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "2. Setting and Notation", "text": "We assume that we observe a sample X1,.., Xn-X, which we usually derive from a true (unknown) underlying distribution P-P. Only on the basis of these observed data do we obtain an estimate. The true (unknown) estimated population value is \u03b8 (P). For example, we could estimate a correlation measure, the parameters of a regressor, or the predictive accuracy of a trained classification model. Considering that this is a random quantity because it is based on n random observations, we define Qn (P) and Q as the true underlying distribution of the two correlations, which is determined both by P and by the shape of the mapping. Our end goal is to calculate a metric quantity because it is based on n random observations."}, {"heading": "3. Related Work", "text": "However, the number of data collected (Qn (Pn) cannot be calculated accurately in most cases, and it is generally affordable to have a simple and powerful estimate of Pn, which calculates the estimate for each individual sample, form the empirical distribution of Qn of the calculated estimates, and the approximate distribution of the collected values (Qn (P)) of the collected values is as follows: Although the conceptually simple and powerful approach requires repeated estimates comparable to the size of the original dataset, this calculation can be expensive."}, {"heading": "4. Bag of Little Bootstraps (BLB)", "text": "The terms (Qn) n, (P) n, b) n, b) n, b) n, b) n, b) n, b) n, b) n, b) n, b) n, b) n, b) n, b) n, b), b), b), b), b), b), b), b), b), b), b), b), b), b), b), b). (P), b) n), c), c), c). (P (P). (P). (P). (P) n, b). (P) n). (P) n). (P) n). (P) n), b). (P) n). (P) n). (P) n) n, b). (P) n. (P) n. (P) n) n, b)."}, {"heading": "5. Statistical Correctness", "text": "8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "6. Scalability", "text": "The experiments in the preceding section, which are primarily designed to examine statistical performance, also provide some insight into computing power (as shown in Figure 1, when the calculation is done in series, BLB requires less time overall, and therefore less overall computation, than the bootstrap to achieve comparable accuracy).These results only point to BLB's superior ability to scale large amounts of data, which we can now show in full length in the subsequent discussion and over large experiments. Modern massive datasets often exceed both the processing and storage capabilities of individual processors or computing nodes, requiring the use of parallel and distributed computing architectures. In fact, in large-scale data setting, they require a single fulldata estimate across multiple distributed computing nodes, among which the observed datasets are divided. As a result, the scalability of a qualitative assessment method is closely tied to its ability to make effective use of such resources."}, {"heading": "7. Hyperparameter Selection", "text": "How existing resampling-based methods for their selection illustrate the influence of r and LB on the relative errors achieved. (...) Like existing resampling-based methods such as the bootstrap s, BLB requires the specification of hyperparameters that control the number of partial samples and resamples (...). (...) If we consider such hyperparameters to be sufficiently large to ensure good statistical performance, however, we must set them as unnecessarily large results in wasteful calculation. (...) Do we not want to offer a general solution to this problem? (...) However, this approach reduces the level of automation of these methods and can be quite inefficient in large data settings. (...) So we are now investigating the dependence of BLB performance on the choice of r and s, with the aim of better understanding their influence and adaptation. (...) The left plot of Figure 3 illustrates the influence of r and LB errors."}, {"heading": "8. Real Data", "text": "In this context, it is not possible to objectively assess the statistical correctness of the methods used to evaluate the estimated quality. Consequently, we reduce ourselves to comparing the results of different methods; we now also report the average absolute confidence interval (across dimensions) generated by the byeach method rather than relative errors. Figure 4 shows the results for the BLB, the bootstrap and the b out of n bootstrap on the UCI Connect4 dataset (logistic regression, d = 42, n = 67, 557). We select the hyperparameters r and s using the adaptive method described in the previous section. It is noteworthy that the results of the BLB for all values considered and the output of the Bootstrap 4 dataset (logistic regression, d = 42, n = 67, 557) are tightly grouped around the same value; furthermore, as expected, the BLB converges faster than the bootstrap."}, {"heading": "9. Conclusion", "text": "We have introduced a new method, BLB, which offers a powerful new alternative to automated, accurate estimation quality assessments that are well suited for large amounts of data and modern parallel and distributed computing architectures."}, {"heading": "Acknowledgments", "text": "This research is supported in part by an NSF CISE Expeditions Award, gifts from Google, SAP, Amazon Web Services, Blue Goji, Cisco, Cloudera, Ericsson, General Electric, Hewlett Packard, Huawei, Intel, MarkLogic, Microsoft, NetApp, Oracle, Quanta, Splunk, VMware and DARPA (Contract No. FA8650-11-C-7136)."}], "references": [{"title": "Extrapolation and the bootstrap", "author": ["P.J. Bickel", "A. Sakov"], "venue": "Sankhya: The Indian Journal of Statistics,", "citeRegEx": "Bickel and Sakov,? \\Q2002\\E", "shortCiteRegEx": "Bickel and Sakov", "year": 2002}, {"title": "On the choice of m in the m out of n bootstrap and confidence bounds for extrema", "author": ["P.J. Bickel", "A. Sakov"], "venue": "Statistica Sinica,", "citeRegEx": "Bickel and Sakov,? \\Q2008\\E", "shortCiteRegEx": "Bickel and Sakov", "year": 2008}, {"title": "Richardson extrapolation and the bootstrap", "author": ["P.J. Bickel", "J.A. Yahav"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bickel and Yahav,? \\Q1988\\E", "shortCiteRegEx": "Bickel and Yahav", "year": 1988}, {"title": "Resampling fewer than n observations: Gains, losses, and remedies for losses", "author": ["P.J. Bickel", "F. Gotze", "W. van Zwet"], "venue": "Statistica Sinica,", "citeRegEx": "Bickel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 1997}, {"title": "Bootstrap methods: Another look at the jackknife", "author": ["B. Efron"], "venue": "Annals of Statistics,", "citeRegEx": "Efron,? \\Q1979\\E", "shortCiteRegEx": "Efron", "year": 1979}, {"title": "More efficient bootstrap computations", "author": ["B. Efron"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Efron,? \\Q1988\\E", "shortCiteRegEx": "Efron", "year": 1988}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1993}, {"title": "A scalable bootstrap for massive data", "author": ["A. Kleiner", "A. Talwalkar", "P. Sarkar", "M.I. Jordan"], "venue": "URL http://arxiv.org/abs/1112.5016", "citeRegEx": "Kleiner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kleiner et al\\.", "year": 2012}, {"title": "How many bootstraps", "author": ["R. Tibshirani"], "venue": "Technical report,", "citeRegEx": "Tibshirani,? \\Q1985\\E", "shortCiteRegEx": "Tibshirani", "year": 1985}], "referenceMentions": [{"referenceID": 4, "context": "The bootstrap (Efron, 1979; Efron & Tibshirani, 1993) is perhaps the best known and most widely used among these methods, due to its simplicity, generic applicability, and automatic nature.", "startOffset": 14, "endOffset": 53}, {"referenceID": 3, "context": "Efforts to ameliorate statistical shortcomings of the bootstrap in turn led to the development of related methods such as the m out of n bootstrap and subsampling (Bickel et al., 1997; Politis et al., 1999).", "startOffset": 163, "endOffset": 206}, {"referenceID": 4, "context": "The bootstrap (Efron, 1979; Efron & Tibshirani, 1993) provides an automatic and widely applicable means of quantifying estimator quality: it simply uses the datadriven plugin approximation \u03be(Qn(P )) \u2248 \u03be(Qn(Pn)).", "startOffset": 14, "endOffset": 53}, {"referenceID": 5, "context": "While the literature does contain some discussion of techniques for improving the computational efficiency of the bootstrap, that work is largely devoted to reducing the number of Monte Carlo resamples required (Efron, 1988; Efron & Tibshirani, 1993).", "startOffset": 211, "endOffset": 250}, {"referenceID": 3, "context": "Bootstrap variants such as the m out of n bootstrap (Bickel et al., 1997) and subsampling (Politis et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 7, "context": "Here we present a representative summary of our investigation of statistical correctness; see Kleiner et al. (2012) for more detail.", "startOffset": 94, "endOffset": 116}, {"referenceID": 7, "context": "In particular, BLB is asymptotically consistent for a broad class of estimators and quality measures (see Kleiner et al. (2012) for proof):", "startOffset": 106, "endOffset": 128}, {"referenceID": 7, "context": "See Kleiner et al. (2012) for our theoretical results on higher-order correctness.", "startOffset": 4, "endOffset": 26}, {"referenceID": 8, "context": "Prior work on the bootstrap and related procedures generally assumes that a procedure\u2019s user will simply select a priori a large, constant number of resamples to be processed (with the exception of Tibshirani (1985), who does not provide a general solution to this issue).", "startOffset": 198, "endOffset": 216}, {"referenceID": 7, "context": "We have obtained qualitatively similar results on six additional UCI datasets (ct-slice, magic, millionsong, parkinsons, poker, shuttle) with different estimators (linear regression and logistic regression) and a range of different values of n and d; see Kleiner et al. (2012) for additional plots.", "startOffset": 255, "endOffset": 277}], "year": 2012, "abstractText": "The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB\u2019s behavior, including a study of its statistical correctness, its largescale implementation and performance, selection of hyperparameters, and performance on real data.", "creator": "LaTeX with hyperref package"}}}