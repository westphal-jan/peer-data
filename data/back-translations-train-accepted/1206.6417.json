{"id": "1206.6417", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Learning Task Grouping and Overlap in Multi-task Learning", "abstract": "In the paradigm of multi-task learning, mul- tiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learn- ing that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combi- nation of a finite number of underlying basis tasks. The coefficients of the linear combina- tion are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on on the assumption that task pa- rameters within a group lie in a low dimen- sional subspace but allows the tasks in differ- ent groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (192kb)", "http://arxiv.org/abs/1206.6417v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["abhishek kumar 0001", "hal daum\u00e9 iii"], "accepted": true, "id": "1206.6417"}, "pdf": {"name": "1206.6417.pdf", "metadata": {"source": "META", "title": "Learning Task Grouping and Overlap in Multi-Task Learning", "authors": ["Abhishek Kumar", "Hal Daum\u00e9 III"], "emails": ["abhishek@cs.umd.edu", "hal@umiacs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related Work", "text": "Most of them work on the assumption that all tasks are interconnected (for example, in those areas where problem-solving is concerned); most of them are unable to perform their tasks (as in those areas where problem-solving is concerned); most of them are able to perform their tasks (as in those areas where problem-solving is concerned); and most of them are unable to perform their tasks (as in those areas where problem-solving is concerned)."}, {"heading": "3. Learning Task Grouping and Overlap", "text": "In this section, we describe our approach to modeling task groups and task overlaps. We call the proposed approach GO-MTL for group work and overlap in multi-task learning. Suppose we have T tasks and Zt = {xti, yti): i = 1, 2,., Nt} be the training set for each task t = 1, 2,. We assume that it is k (< T) latent basic tasks and each observed task can be represented as a linear combination of a subset of these basic tasks. This assumption allows us to write the weight matrix W as W = LS, where L is a matrix of size."}, {"heading": "3.1. Regression: Squared Loss", "text": "Here we give details of the optimization of the cost function of Equation 1 for quadratic loss L (a, b) = (a \u2212 b) 2, which is commonly used in regression problems. Let's call yt a column vector of length Nt, which contains all the names for task t. Similarly, let's call Xt the data matrix of size d \u00b7 Nt, which contains all samples for task t as columns. Equation 1 \"s cost function can be min L, ST, t = 11Nt | | yt \u2212 X \u2032 tLst | | | 2 + \u00b5 | S | 1 + \u03bb | | L | 2 F, (4) For a fixed L, we need the gradient and the Hessian property of the quadratic loss function f (st) = 1 Nt | yt \u2212 X \u2032 tLst | | 2 we optimize the Lvec system using the two-metric projection method."}, {"heading": "3.2. Classification: Logistic Loss", "text": "We use logistic regression for classification problems, even though the proposed method is not tied to any specific loss function. Here, we give details of the optimization of Eq. 1 for logistic loss function, which is specified as L (y, f (x)) = log (1 + exp (\u2212 yf (x))), where y (1, 1) is the true label. Let's call the logistic function L (x) = 1 / (1 + e \u2212 x). For a specified L, we need the gradient and Hessian of the loss function w.r.t. st To solve the solution by means of two-metric projection method, which can be applied by f (st) = 1NtNt - xti \u2212 xti \u2212 Rapixti (Rapixti))))) - stf (st) = \u2212 stf (st)."}, {"heading": "4. Experiments", "text": "We conduct extensive empirical evaluations of our approach to measure its effectiveness. We perform empirical comparisons with the following two competing multidimensional learning approaches: \u2022 No-group MTL (Argyriou et al., 2008a): All tasks are considered contiguous, and the task parameters are assumed to be located in a low-dimensional subspace, by punishing the nuclear norm of the weight matrix. \u2022 Disjoint-group MTL (DGMTL) (Kang et al., 2011): A recently proposed approach assuming multiple disjunctive groups of tasks. Task parameters within a group are located in a low-dimensional space. Furthermore, we compare with Baseline Single Task Learning (STL), in which tasks are learned independently. In the following, we report on results from two synthetic and four real data sets. The regulation parameters within a group lie in a low-dimensional space."}, {"heading": "4.1. Synthetic data", "text": "We use two synthetic datasets to study our approach. First, we use the synthetic data used in (Kang et al., 2011) 1. These data consist of 20-dimensional feature vectors, three sets of tasks, 15 training points and 50 test points per task. There are 10 tasks in each group whose parameter vectors are identical to each other up to a scaling factor. These parameters are used1This data along with the source code has been taken from the author's website: http: / / wwwscf.usc.edu / GoupMTLCode.zipin the model of linear regression for generating training data. The task groups in this data are disjoint.We generate a second synthetic dataset to simulate overlaps in groups. We retain the previous setting of 3 groups and 10 tasks in each group, but now we allow the groups to overlap into one. We generate parameter vectors for each of the four dimensions, we are... in each task."}, {"heading": "4.2. Real datasets", "text": "We evaluate the proposed approach based on the following four real data sets, two of which are regression problems and the other two classification problems. We treat reusable classification as a multipurpose learning problem, where each task is the classification of a class from all other classes. To be fair, we evaluate different learning approaches used in (Argyriou et al., 2008a; Kang et al., 2011). \u2022 Computer survey data: This regression has been used in the literature to evaluate different multipurpose learning approaches (Argyriou et al., 2008a; Agarwal et al., 2010)."}, {"heading": "5. Conclusion", "text": "Our approach does not assume a disjunctive grouping structure, and tasks belonging to different groups may overlap by sharing one or more latent basic tasks. This is a more realistic assumption, since we can thank the authors of (Kang et al., 2011) for providing us with the data used in their papers. Tasks in our pool that are not sufficiently related to be in the same group, but nevertheless share some information that can be used for better learning. We validated our model on two synthetic and four real data sets and made considerable gains compared to other competing approaches to regulated multi-task learning, which either does not take into account the grouping structure or assumes that tasks in different groups do not interact at all."}], "references": [{"title": "Learning Multiple Tasks using Manifold Regularization", "author": ["Agarwal", "Arvind", "Daum\u00e9 III", "Hal", "Gerber", "Samuel"], "venue": "In NIPS,", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "author": ["Ando", "Rie Kubota", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Sparse Bayesian Multi-Task Learning", "author": ["Archambeau", "Cedric", "Guo", "Shengbo", "Zoeter", "Onno"], "venue": "In NIPS,", "citeRegEx": "Archambeau et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Archambeau et al\\.", "year": 2011}, {"title": "Convex Multi-task Feature Learning", "author": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "An Algorithm for Transfer Learning in a Heterogeneous Environment", "author": ["Argyriou", "Andreas", "Maurer", "Pontil", "Massimiliano"], "venue": "In Proc. of ECML/PKDD,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Task Clustering and Gating for Bayesian Multitask Learning", "author": ["Bakker", "Bart", "Heskes", "Tom"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bakker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bakker et al\\.", "year": 2003}, {"title": "Multitask Learning", "author": ["Caruana", "Rich"], "venue": "Machine Learning,", "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Integrating Low-Rank and Group-Sparse Structures for Robust Multi-Task Learning", "author": ["Chen", "Jianhui", "Zhou", "Jiayu", "Ye", "Jieping"], "venue": "In KDD,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Bayesian Multitask Learning with Latent Hierarchies", "author": ["Daum\u00e9 III", "Hal"], "venue": "In UAI,", "citeRegEx": "III and Hal.,? \\Q2009\\E", "shortCiteRegEx": "III and Hal.", "year": 2009}, {"title": "Regularized multi-task learning", "author": ["Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "In SIGKDD International Conference on Knowledge Discovery and Data mining,", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "Two-metric projection methods for constrained optimization", "author": ["E. Gafni", "D. Bertsekas"], "venue": "SIAM J. Contr. Optim,", "citeRegEx": "Gafni and Bertsekas,? \\Q1984\\E", "shortCiteRegEx": "Gafni and Bertsekas", "year": 1984}, {"title": "Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and some Robust and Resistant Alternatives", "author": ["P.J. Green"], "venue": "Journal of Royal Statistical Society, Series B,", "citeRegEx": "Green,? \\Q1984\\E", "shortCiteRegEx": "Green", "year": 1984}, {"title": "A database for handwritten text recognition research", "author": ["J.J. Hull"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hull,? \\Q1994\\E", "shortCiteRegEx": "Hull", "year": 1994}, {"title": "Clustered Multi-task Learning: a Convex Formulation", "author": ["Jacob", "Laurent", "Bach", "Francis", "Vert", "Jean-Philippe"], "venue": "In NIPS,", "citeRegEx": "Jacob et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2008}, {"title": "A Dirty Model for Multi-task Learning", "author": ["Jalali", "Ali", "Ravikumar", "Pradeep", "Sanghavi", "Sujay", "Ruan", "Chao"], "venue": "In NIPS,", "citeRegEx": "Jalali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2010}, {"title": "Learning with Whom to Share in Multi-task Feature Learning", "author": ["Kang", "Zhuoliang", "Grauman", "Kristen", "Sha", "Fei"], "venue": "In ICML,", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning a meta-level prior for feature relevance from multiple related tasks", "author": ["S.I. Lee", "V. Chatalbashev", "D. Vickrey", "D. Koller"], "venue": "In ICML,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Multi-Task Feature Learning Via Efficient L2,1-Norm Minimization", "author": ["Liu", "Jun", "Ji", "Shuiwang", "Ye", "Jieping"], "venue": "In UAI,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Flexible Modeling of Latent Task Structures in Multitask Learning", "author": ["Passos", "Alexandre", "Rai", "Piyush", "Wainer", "Jacques", "Daum\u00e9 III", "Hal"], "venue": "In ICML,", "citeRegEx": "Passos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2012}, {"title": "Infinite Predictor Subspace Models for Multitask Learning", "author": ["Rai", "Piyush", "Daum\u00e9 III", "Hal"], "venue": "In AISTATS,", "citeRegEx": "Rai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rai et al\\.", "year": 2010}, {"title": "Fast Optimization Methods for L1 Regularization: A Comparative Study and Two New Approaches", "author": ["Schmidt", "Mark", "Fung", "Glenn", "Rosales", "Romer"], "venue": "In ECML,", "citeRegEx": "Schmidt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2007}, {"title": "Learning to Learn", "author": ["Thrun", "Sebastian", "Pratt", "Lorien"], "venue": null, "citeRegEx": "Thrun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1998}, {"title": "Multi-Task Learning for Classification with Dirichlet Process Priors", "author": ["Xue", "Ya", "Liao", "Xuejun", "Carin", "Lawrence", "Krishnapuram", "Balaji"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}, {"title": "Learning Gaussian Processes from Multiple Task", "author": ["Yu", "Kai", "Tresp", "Volker", "Schwaighofer", "Anton"], "venue": "In ICML,", "citeRegEx": "Yu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2005}, {"title": "Robust MultiTask Learning with t-Processes", "author": ["Yu", "Shipeng", "tresp", "Volker", "Kai"], "venue": "In ICML,", "citeRegEx": "Yu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2007}, {"title": "Learning Multiple Tasks with a Sparse Matrix-Normal Penalty", "author": ["Zhang", "Yi", "Schneider", "Jeff"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "A Convex Formulation for Learning Task Relationships in Multi-Task Learning", "author": ["Zhang", "Yu", "Yeung", "Dit-Yan"], "venue": "In UAI,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Multi-Task Learning using Generalized t-Process", "author": ["Zhang", "Yu", "Yeung", "Dit-Yan"], "venue": "In AISTATS,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Clustered Multi-task Learning Via Alternating Structure Optimization", "author": ["Zhou", "Jiayu", "Chen", "Jianhui", "Ye", "Jieping"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 24, "context": "Assumptions that task parameters lie close to each other in some geometric sense (Evgeniou & Pontil, 2004), or parameters share a common prior (Yu et al., 2005; Lee et al., 2007; Daum\u00e9 III, 2009), or they lie in a low dimensional subspace (Argyriou et al.", "startOffset": 143, "endOffset": 195}, {"referenceID": 17, "context": "Assumptions that task parameters lie close to each other in some geometric sense (Evgeniou & Pontil, 2004), or parameters share a common prior (Yu et al., 2005; Lee et al., 2007; Daum\u00e9 III, 2009), or they lie in a low dimensional subspace (Argyriou et al.", "startOffset": 143, "endOffset": 195}, {"referenceID": 0, "context": "fold (Agarwal et al., 2010) are some examples of introducing an inductive bias in the hope of achieving better generalization.", "startOffset": 5, "endOffset": 27}, {"referenceID": 13, "context": "Examples of such approaches are (Jacob et al., 2008; Xue et al., 2007) where tasks are assumed to be clustered and parameters of tasks within a cluster lie close to each other in l2 norm sense.", "startOffset": 32, "endOffset": 70}, {"referenceID": 23, "context": "Examples of such approaches are (Jacob et al., 2008; Xue et al., 2007) where tasks are assumed to be clustered and parameters of tasks within a cluster lie close to each other in l2 norm sense.", "startOffset": 32, "endOffset": 70}, {"referenceID": 15, "context": "Recently, task grouping in the subspace based regularization framework was proposed in (Kang et al., 2011).", "startOffset": 87, "endOffset": 106}, {"referenceID": 15, "context": "We validate our approach empirically with two synthetic and four real-world datasets and observe that our method either outperforms or performs as well as the relevant baseline methods of (Kang et al., 2011; Argyriou et al., 2008a).", "startOffset": 188, "endOffset": 231}, {"referenceID": 13, "context": "Some methods assume that tasks can be grouped in clusters and parameters of tasks within a cluster are either close to each other in some distance metric or share a common probabilistic prior (Bakker & Heskes, 2003; Jacob et al., 2008; Xue et al., 2007; Zhou et al., 2011).", "startOffset": 192, "endOffset": 272}, {"referenceID": 23, "context": "Some methods assume that tasks can be grouped in clusters and parameters of tasks within a cluster are either close to each other in some distance metric or share a common probabilistic prior (Bakker & Heskes, 2003; Jacob et al., 2008; Xue et al., 2007; Zhou et al., 2011).", "startOffset": 192, "endOffset": 272}, {"referenceID": 29, "context": "Some methods assume that tasks can be grouped in clusters and parameters of tasks within a cluster are either close to each other in some distance metric or share a common probabilistic prior (Bakker & Heskes, 2003; Jacob et al., 2008; Xue et al., 2007; Zhou et al., 2011).", "startOffset": 192, "endOffset": 272}, {"referenceID": 25, "context": "Other methods assume that there is one group of related tasks and a small number of outlier tasks that are not related to any task in the pool (Yu et al., 2007; Chen et al., 2011).", "startOffset": 143, "endOffset": 179}, {"referenceID": 7, "context": "Other methods assume that there is one group of related tasks and a small number of outlier tasks that are not related to any task in the pool (Yu et al., 2007; Chen et al., 2011).", "startOffset": 143, "endOffset": 179}, {"referenceID": 2, "context": "There also exist probabilistic models which attempt to learn full task covariance matrix and use it in learning of predictor functions (Zhang & Yeung, 2010b;a; Zhang & Schneider, 2010; Archambeau et al., 2011).", "startOffset": 135, "endOffset": 209}, {"referenceID": 18, "context": "Another common assumption is that task parameters lie in a low dimensional subspace that captures the predictive structure for all the tasks (Argyriou et al., 2008a; Liu et al., 2009).", "startOffset": 141, "endOffset": 183}, {"referenceID": 14, "context": "In (Jalali et al., 2010), this model was refined and features are assumed to be either active for all tasks, or inactive for most of the tasks.", "startOffset": 3, "endOffset": 24}, {"referenceID": 15, "context": "There exist a few methods that incorporate grouping structure in the subspace based regularization (Argyriou et al., 2008b; Kang et al., 2011).", "startOffset": 99, "endOffset": 142}, {"referenceID": 15, "context": "Very similar in spirit is the work of (Kang et al., 2011), where tasks within a group are assumed to lie in a low dimensional subspace", "startOffset": 38, "endOffset": 57}, {"referenceID": 19, "context": "Recently, (Passos et al., 2012) posited a mixture of sparse factor analyzers structure on the collection of the task weight vectors.", "startOffset": 10, "endOffset": 31}, {"referenceID": 21, "context": "2, which has superlinear convergence (Schmidt et al., 2007; Gafni & Bertsekas, 1984).", "startOffset": 37, "endOffset": 84}, {"referenceID": 11, "context": "For classification problems, we use logistic loss and optimize it using Newton-Raphson method, which is commonly used to estimate logistic regression parameters and is the basis of iterative reweighted least squares algorithm (IRLS) algorithm for logistic regression (Green, 1984).", "startOffset": 267, "endOffset": 280}, {"referenceID": 15, "context": "In this respect, it is similar to the \u201cnumber of groups\u201d parameter, G, in (Kang et al., 2011).", "startOffset": 74, "endOffset": 93}, {"referenceID": 15, "context": "Left: RMSE with DG-MTL (Kang et al., 2011) vs.", "startOffset": 23, "endOffset": 42}, {"referenceID": 15, "context": "\u2022 Disjoint-Group MTL (DGMTL) (Kang et al., 2011): A recently proposed approach that assumes multiple disjoint groups of tasks.", "startOffset": 29, "endOffset": 48}, {"referenceID": 15, "context": "First, we use the synthetic data used in (Kang et al., 2011).", "startOffset": 41, "endOffset": 60}, {"referenceID": 15, "context": "We also show the RMSE plot with changing value of parameter G (the number of groups) in the approach of (Kang et al., 2011).", "startOffset": 104, "endOffset": 123}, {"referenceID": 15, "context": "The performance of (Kang et al., 2011) is more sensitive to the number of groups parameter (G) and starts deteriorating when it is increased or decreased from the true value.", "startOffset": 19, "endOffset": 38}, {"referenceID": 15, "context": "To be fair in our comparisons, we evaluate on datasets that are used in (Argyriou et al., 2008a; Kang et al., 2011).", "startOffset": 72, "endOffset": 115}, {"referenceID": 15, "context": ", 2008a), DG-MTL (Kang et al., 2011), GO-MTL: the proposed method.", "startOffset": 17, "endOffset": 36}, {"referenceID": 0, "context": "\u2022 Computer Survey data: This regression dataset has been widely in the literature to evaluate various multi-task learning approaches (Argyriou et al., 2008a; Agarwal et al., 2010).", "startOffset": 133, "endOffset": 179}, {"referenceID": 0, "context": "\u2022 School data: This regression data has been used in previous works in multi-task learning (Argyriou et al., 2008a; Agarwal et al., 2010; Bakker & Heskes, 2003).", "startOffset": 91, "endOffset": 160}, {"referenceID": 12, "context": "\u2022 USPS Digits data: This is a handwritten digits dataset (Hull, 1994) with 10 classes.", "startOffset": 57, "endOffset": 69}, {"referenceID": 16, "context": "\u2022 MNIST Digits data: This is another handwritten digits dataset (LeCun et al., 1998) with 10 classes.", "startOffset": 64, "endOffset": 84}, {"referenceID": 15, "context": "For MNIST and USPS datasets, we use the same setup as in (Kang et al., 2011) where 1000, 500 and 500 samples are used for training, validation and test respectively.", "startOffset": 57, "endOffset": 76}, {"referenceID": 15, "context": "We thank the authors of (Kang et al., 2011) for providing us the data used in their paper.", "startOffset": 24, "endOffset": 43}], "year": 2012, "abstractText": "In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learning that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combination of a finite number of underlying basis tasks. The coefficients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods.", "creator": "LaTeX with hyperref package"}}}