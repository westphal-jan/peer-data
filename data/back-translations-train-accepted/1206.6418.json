{"id": "1206.6418", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Learning Invariant Representations with Local Transformations", "abstract": "Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (826kb)", "http://arxiv.org/abs/1206.6418v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["kihyuk sohn", "honglak lee"], "accepted": true, "id": "1206.6418"}, "pdf": {"name": "1206.6418.pdf", "metadata": {"source": "META", "title": "Learning Invariant Representations with Local Transformations", "authors": ["Kihyuk Sohn", "Honglak Lee"], "emails": ["kihyuks@umich.edu", "honglak@eecs.umich.edu"], "sections": [{"heading": "1. Introduction", "text": "In recent years, the number of those able to reform has multiplied."}, {"heading": "2. Preliminaries", "text": "In this paper, we present a general framework for learning locally invariable characteristics using transformations, using the Boltzmann Restricted Machine (RBM) as a main example1. We first describe the RBM below, followed by its novel extension (Section 3). However, the Boltzmann Restricted Machine is a two-part, undirected graphical model consisting of visible and hidden layers. Assuming binary weighted visible and hidden units, the energy function and common probability distribution are given as follows: 2E (v, h) = \u2212 vTWh \u2212 bTh \u2212 cTv, (1) P (v, h) = 1Z exp (\u2212 E (v, h)))) (2), where v {0, 1} D are binary visible units, h {0, 1} K are binary hidden units, and W (RD, b, RK, c, RD) are vulnerable units."}, {"heading": "3. Learning Transformation-Invariant Feature Representations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Transformation-invariant RBM", "text": "In this section we formulate a new type of Feature Learning Framework which can learn Invariance on one row of linear transformations = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j = j"}, {"heading": "3.2. Sparse TIRBM", "text": "Following Lee et al.'s (2008) approach, we can extend our model to sparse TIRBM by adding the following regularizer for a given dataset {v (1), \u00b7 \u00b7, v (N)} to the negative log probability: Lsp = K \u2211 j = 1 D (p, 1N N N \u2211 n = 1 E [z (n) j | v (n)]) (9), where D is a distance function; p is the target party.trix Ts also induces a linear transformation. In the work, we will occasionally misuse the term \"transformation\" to denote these two cases, as long as the context is clear. The expectation of pooled activation is written as E [zj | v] = s exp (w T j Tsv + bj, s)."}, {"heading": "3.3. Generating transformation matrices", "text": "In this section we will discuss how to design the transformation matrix T. Further, we will start from case D1 = D2 = D, but general cases will be discussed later. As already mentioned, T-RD \u00b7 D is a linear transformation matrix from x-RD to y-RD; i.e., each coordinate of y is constructed by linear combination of the coordinates in x with the weight matrix T as follows: yi = D-J = 1 Tijxj, \u03c6i = 1, \u00b7 \u00b7, D. (11) For example, the shift by s can be defined as Tij (s) = {1, if i = j + s, 0 otherwise.For 2-d image transformations such as rotation and scaling, the contribution of the input coordinates to each output coordinate can be calculated as Tij (s) = 0, if i = oj is calculated sparingly."}, {"heading": "3.4. Extensions to other methods", "text": "We emphasize that our transformation-invariant feature Learning Framework is not limited to energy-based probability models, but can also be extended to other unattended learning methods. First, it can be easily adapted to autoencoders by defining the following Softmax encoding and sigmoid decoding functions: fj, s (v) = exp (wTj Tsv + bj, s) 1 + exp (w T j Ts \"v + bj, s\") (12) v = sigmoid (p, s (TTs wj) ifj, s + ci) (13) Following the idea of TIRBM, we can formulate the transformation-invariant sparse encoding as follows: min W, H n K = 1 S s = sigmoid (p) ifj, s + ci) (13)."}, {"heading": "4. Related Work", "text": "In fact, it is such that the greater part of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to fight, to move, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "5. Experiments", "text": "We start with the description of the notation. In images, we assume a receptive field size of r \u00b7 r pixels (for input image fields) and a filter size of w \u00b7 w pixels. We define gs to specify the number of pixels corresponding to the transformation (e.g. translation or scaling). For example, we translate the w \u00b7 w filter via the receptive field r \u00b7 r with a step of g pixels (Figure 2 (a)) or scale from (r \u2212 l \u00b7 gs) \u00b7 (r \u2212 l \u00b7 gs) to w \u00b7 w (where 0 \u2264 l \u2264 b (r \u2212 w) / gsc) by using the same mean formatrix. We observed that learning with zero-padded square transformation matrices showed significant boundary effects in the visualization of filters, and this often resulted in a much worse classification performance than the receptive field (Figure 2 (b))."}, {"heading": "5.1. Handwritten digit recognition with prior transformation information", "text": "First and foremost, the question is to what extent the two candidates are a candidate who is able to choose another party and to what extent he has chosen another party that has chosen another candidate."}, {"heading": "5.2. Learning invariant features from natural images", "text": "For the handwritten number recognition in the previous section, we assumed that prior information about global image transformations (e.g. translation, rotation, and scale variations) could achieve significantly better classification performance for each set of data than the basic method, since the data-specific transformation information in TIRBM.However, it does not make sense for natural images to assume such global transformations due to the complex image structures. Indeed, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) implies that a certain level of inventory compared to local transformations (e.g. low pixel translation or coordinate-level noise) results in improved classification performance. From this point of view, it makes more sense to learn representations with local receptive fields that are invariant to generic image transformations (e.g. small amounts of transformations, rotation, and scaling)."}, {"heading": "5.3. Object recognition", "text": "First, we tested on the widely used CIFAR10 dataset = 4 different activation layers (Krizhevsky, 2009), which consisted of 50,000 training sessions and 10,000 test examples with 10 categories. Instead of learning the learning characteristics from the entire image (32 x 32 pixels), we trained TIRBMs on local image fields while maintaining the RGB channels. As suggested by Coates et al. (2011), we used the fixed filter size w = 6 and determined the receptive field size depending on the types of transformations. 7 Then, after unattended training with TIRBM, we used the Convolutionary Feature Extraction Scheme, also according to Coates et al. (2011), we calculated the TIRBM activations for each local r-r7For example, we used r = 6 for rotations. For both scale variations or translations, we used r = 8 and gs = 2, pixels with one pixel extracted."}, {"heading": "5.4. Phone classification", "text": "To demonstrate the broad applicability of our method to other types of data, we report 39 times the accuracy of the telephone classification on the TIMIT dataset. Following (Ngiam et al., 2011), we created 39-dimensional MFCC features and used 11 contiguous frames of them as input fields. For TIRBMs, we applied three time translations with the step of a frame.8 As reported in Table 4, the TIRBM showed an improvement over the sparse RBM as well as the sparse encoding and sparse filtering. In the next setting, we used the RBF kernel SVM (Chang & Lin, 2011) for the extracted features associated with the MFCC hierarchy."}, {"heading": "6. Conclusion and Future Work", "text": "In this paper, we have proposed novel feature-learning algorithms that can achieve invariance over predefined transformations; our method can handle general transformations (e.g. translation, rotation, and8We used K = 256 with a bilateral encoding function, using the positive and negative weight matrices [W, \u2212 W], as proposed by Ngiam et al. (2011).scaling); and we have experimentally shown that learning invariant features for such transformations leads to a strong classification performance; in future work, we plan to learn transformations from the data and combine them with our algorithm. By automatically learning transformation matrices from the data, we will be able to learn more robust features that will potentially lead to significantly better feature representations."}, {"heading": "Acknowledgments", "text": "This work was partially supported by a Google Faculty Research Award."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Hierarchical large-margin gaussian mixture models for phonetic classication", "author": ["H. Chang", "J.R. Glass"], "venue": "In ASRU,", "citeRegEx": "Chang and Glass,? \\Q2007\\E", "shortCiteRegEx": "Chang and Glass", "year": 2007}, {"title": "High-performance neural networks for visual object classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Technical report,", "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Coates and Ng,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Coates and Ng,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Reducing the dimensionality of data with neural", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "networks. Science,", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P.O. Hoyer", "M.O. Inki"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2001}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Learning convolutional feature hierachies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "In NIPS,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Transformation equivariant boltzmann machines", "author": ["J. Kivinen", "C. Williams"], "venue": "In ICANN,", "citeRegEx": "Kivinen and Williams,? \\Q2011\\E", "shortCiteRegEx": "Kivinen and Williams", "year": 2011}, {"title": "Learning multiple layers of features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Sparse deep belief network model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Unsupervised learning of hierarchical representations with convolutional deep belief networks", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Communications of the ACM,", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Object recognition from local scale-invariant features", "author": ["D. Lowe"], "venue": "In ICCV,", "citeRegEx": "Lowe,? \\Q1999\\E", "shortCiteRegEx": "Lowe", "year": 1999}, {"title": "Efficient learning of sparse representations with an energybased model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "In NIPS,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "Efficient learning of sparse, distributed, convolutional feature representations for object recognition", "author": ["K. Sohn", "D.Y. Jung", "H. Lee", "A. Hero III"], "venue": "In ICCV,", "citeRegEx": "Sohn et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2011}, {"title": "Independent component filters of natural images compared with simple cells in primary visual cortex", "author": ["J.H. van Hateren", "A. van der Schaaf"], "venue": "Proceedings of the Royal Society B,", "citeRegEx": "Hateren and Schaaf,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Schaaf", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In ICML,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "In CVPR,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "In recent years, unsupervised feature learning algorithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006).", "startOffset": 129, "endOffset": 193}, {"referenceID": 0, "context": "In recent years, unsupervised feature learning algorithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006).", "startOffset": 129, "endOffset": 193}, {"referenceID": 21, "context": "In recent years, unsupervised feature learning algorithms have emerged as promising tools for learning representations from data (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2006).", "startOffset": 129, "endOffset": 193}, {"referenceID": 19, "context": "For example, convolutional feature learning methods (Lee et al., 2011; Kavukcuoglu et al., 2010; Zeiler et al., 2010) can achieve shift-invariance by exploiting convolution operators.", "startOffset": 52, "endOffset": 117}, {"referenceID": 12, "context": "For example, convolutional feature learning methods (Lee et al., 2011; Kavukcuoglu et al., 2010; Zeiler et al., 2010) can achieve shift-invariance by exploiting convolution operators.", "startOffset": 52, "endOffset": 117}, {"referenceID": 24, "context": "As another example, the denoising autoencoder (Vincent et al., 2008) can learn features that are robust to the input noise by trying to reconstruct the original data from the hidden representation of the perturbed data.", "startOffset": 46, "endOffset": 68}, {"referenceID": 14, "context": "competitive performance in visual recognition tasks on CIFAR-10 (Krizhevsky, 2009) and STL-10 (Coates et al.", "startOffset": 64, "endOffset": 82}, {"referenceID": 6, "context": "competitive performance in visual recognition tasks on CIFAR-10 (Krizhevsky, 2009) and STL-10 (Coates et al., 2011) datasets.", "startOffset": 94, "endOffset": 115}, {"referenceID": 17, "context": "Due to space constraints, we present only the case of binary-valued input variables; however, the RBM with real-valued input variables can be formulated straightforwardly (Hinton & Salakhutdinov, 2006; Lee et al., 2008).", "startOffset": 171, "endOffset": 219}, {"referenceID": 7, "context": "Although computing the exact gradient is intractable, we can approximate it using contrastive divergence (Hinton, 2002).", "startOffset": 105, "endOffset": 119}, {"referenceID": 19, "context": "4 Therefore, v1 and v2 will both A similar technique is used in convolutional deep belief networks (Lee et al., 2011), in which spatial probabilistic max pooling is applied over a small spatial region.", "startOffset": 99, "endOffset": 117}, {"referenceID": 17, "context": "By following Lee et al.\u2019s (2008) approach, we can extend our model to sparse TIRBM by adding the following regularizer for a given set of data {v, \u00b7 \u00b7 \u00b7 ,v} to the negative log-likelihood:", "startOffset": 13, "endOffset": 33}, {"referenceID": 20, "context": "For example, the rotation- or scale-invariant descriptors, such as SIFT (Lowe, 1999), have shown a great success in many computer vision applications.", "startOffset": 72, "endOffset": 84}, {"referenceID": 24, "context": "As an example, the denoising autoencoder (Vincent et al., 2008) can learn robust features by trying to reconstruct the original data from the hidden representations of randomly perturbed data generated from the distortion processes, such as adding noise or multiplying zeros for randomly selected coordinates.", "startOffset": 41, "endOffset": 63}, {"referenceID": 16, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 12, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 19, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 22, "context": "Specifically, convolutional training (LeCun et al., 1989; Kavukcuoglu et al., 2010; Zeiler et al., 2010; Lee et al., 2011; Sohn et al., 2011) is one of the most popular methods that encourages shift-invariance during the feature learning.", "startOffset": 37, "endOffset": 141}, {"referenceID": 19, "context": "For example, the convolutional deep belief network (CDBN) (Lee et al., 2011), which is composed of multiple layers of convolutional restricted Boltzmann machines and probabilistic max pooling, can learn a representation invariant to local translation.", "startOffset": 58, "endOffset": 76}, {"referenceID": 13, "context": "In contemporary work of ours, Kivinen and Williams (2011) proposed the transformation equivariant Boltzmann machine, which shares a similar mathematical formulation to our models in that both try to infer the best matching filters by transforming them using linear transformation matrices.", "startOffset": 30, "endOffset": 58}, {"referenceID": 10, "context": "As another related work, feature learning methods with topographic maps can also learn invariant representations (Hyv\u00e4rinen et al., 2001; Kavukcuoglu et al., 2009).", "startOffset": 113, "endOffset": 163}, {"referenceID": 11, "context": "As another related work, feature learning methods with topographic maps can also learn invariant representations (Hyv\u00e4rinen et al., 2001; Kavukcuoglu et al., 2009).", "startOffset": 113, "endOffset": 163}, {"referenceID": 15, "context": "From the MNIST variation datasets (Larochelle et al., 2007), we tested on \u201cmnist-rot\u201d (rotated digits, referred to as rot) and \u201cmnist-rot-back-image\u201d (rotated digits with background images, referred to as rot-bgimg).", "startOffset": 34, "endOffset": 59}, {"referenceID": 25, "context": "It is worth noting that our error rates for the mnist-rot and mnistrot-back-image datasets are also significantly lower than the best published results obtained with stacked denoising autoencoders (Vincent et al., 2010) (9.", "startOffset": 197, "endOffset": 219}, {"referenceID": 26, "context": "In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.", "startOffset": 27, "endOffset": 84}, {"referenceID": 19, "context": "In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.", "startOffset": 27, "endOffset": 84}, {"referenceID": 24, "context": "In fact, recent literature (Yu et al., 2011; Lee et al., 2011; Vincent et al., 2008) suggests that some level of invariance to local transformations (e.", "startOffset": 27, "endOffset": 84}, {"referenceID": 6, "context": "The numbers with \u2020 and \u2021 are from (Coates et al., 2011) and (Coates & Ng, 2011a), respectively.", "startOffset": 34, "endOffset": 55}, {"referenceID": 3, "context": "9% deep NN (Ciresan et al., 2011) 80.", "startOffset": 11, "endOffset": 33}, {"referenceID": 12, "context": "These results suggest that TIRBMs can learn diverse sets of filters, which is reminiscent of the effects of convolutional training (Kavukcuoglu et al., 2010).", "startOffset": 131, "endOffset": 157}, {"referenceID": 14, "context": "First, we tested on the widely used CIFAR10 dataset (Krizhevsky, 2009), which is composed of 50,000 training and 10,000 testing examples with 10 categories.", "startOffset": 52, "endOffset": 70}, {"referenceID": 6, "context": "As suggested by Coates et al.(2011), we used the fixed filter size w = 6 and determined the receptive field size depending on the types of transformations.", "startOffset": 16, "endOffset": 36}, {"referenceID": 6, "context": "As suggested by Coates et al.(2011), we used the fixed filter size w = 6 and determined the receptive field size depending on the types of transformations. Then, after unsupervised training with TIRBM, we used the convolutional feature extraction scheme, also following the Coates et al.(2011). Specifically, we computed the TIRBM pooling-unit activations for each local r\u00d7r For example, we used r = 6 for rotations.", "startOffset": 16, "endOffset": 294}, {"referenceID": 6, "context": "We also performed the object classification task on STL-10 dataset (Coates et al., 2011), which is more challenging due to the smaller number of labeled training examples (100 per class for each training fold).", "startOffset": 67, "endOffset": 88}, {"referenceID": 18, "context": "5% MFCC + CDBN (Lee et al., 2009) 80.", "startOffset": 15, "endOffset": 33}], "year": 2012, "abstractText": "Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformationinvariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-theart performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.", "creator": "LaTeX with hyperref package"}}}