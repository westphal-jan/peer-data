{"id": "1206.6419", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Cross-Domain Multitask Learning with Latent Probit Models", "abstract": "Learning multiple tasks across heterogeneous domains is a challenging problem since the feature space may not be the same for different tasks. We assume the data in multiple tasks are generated from a latent common domain via sparse domain transforms and propose a latent probit model (LPM) to jointly learn the domain transforms, and the shared probit classifier in the common domain. To learn meaningful task relatedness and avoid over-fitting in classification, we introduce sparsity in the domain transforms matrices, as well as in the common classifier. We derive theoretical bounds for the estimation error of the classifier in terms of the sparsity of domain transforms. An expectation-maximization algorithm is derived for learning the LPM. The effectiveness of the approach is demonstrated on several real datasets.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (908kb)", "http://arxiv.org/abs/1206.6419v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shaobo han", "xuejun liao", "lawrence carin"], "accepted": true, "id": "1206.6419"}, "pdf": {"name": "1206.6419.pdf", "metadata": {"source": "META", "title": "Cross-Domain Multitask Learning with Latent Probit Models", "authors": ["Shaobo Han", "Xuejun Liao", "Lawrence Carin"], "emails": ["shaobo.han@duke.edu", "xjliao@duke.edu", "lcarin@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "There are two basic approaches to the analysis of two or more tasks, which one can assume are all capable of surpassing themselves, both in terms of the way in which they move, in terms of the way in which they move, in terms of the way in which they move, in terms of the way in which they move, in terms of the way in which they move and in which they move."}, {"heading": "2. The Latent probit Model", "text": "The latent probit model (LPM) is a generative probability model for M \u2265 2 that contains partially described sets of vectors (data points) with each dataset having a different representation; the parameters w specify the probit classification as shown in Figure 1; and Fm specifies the domain transform for the m-th dataset up to a translation (as specified by dm); the parameters w specify the probit classification by the tasks in the latent feature space; and Fm specifies the domain transform for the m-th dataset up to a translation (as specified by dm); the parameters w and {Fm} Mm = 1 are given hierarchical Laplacian priors (Figueiredo, 2003) to promote thrift with the priors specified by hyperparameters."}, {"heading": "3. Theoretical Analysis of the LPM", "text": "The goal of our analysis is to quantify the notion that the individual ranges in relation to the number of non-zeroes elements of actual, non-zeroes elements of non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeros, non-zeros, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeros, non-zeros, non-zeroes, non-zeroes, non-zeroes, non-zeroes, non-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroes, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no-zeroing, no, no-zeroing, no-zeroing, no-zeroing, no, no-zeroing, no-zeroing, no, no-zeroing, no-zeroing, no, no, no-zeroing, no, no-zeroing, no-zeroing, no, no, no-zeroing, no-zeroing, no, no, no-zeroing, no-not, no-zeroing, no-zeroing, no, no, no-zeroing, no, no-zeroing, no, no, no-zeroing, no-zeroing, no, no, no, no"}, {"heading": "4. Parameter Estimation", "text": "We are looking for a MAP estimate of the parameters contained in the data. (...) We are looking for a MAP estimate of the parameters. (...) We are looking for a MAP estimate of the parameters. (...) We are looking for a MAP estimate. (...) We are looking for a MAP estimate. (...) We are looking for a MAP estimate. (...) We are looking for a MAP estimate. (...) We are looking for a MAP estimate. (...) We are looking for a MAP estimate. (...) We are looking for a MAP estimate. (...) We are looking for an MAP estimate. (...) We are looking for an MAP estimate. (...) We are looking for an MAP estimate. (...) We are looking for an MAP estimate. (...) We are looking for an MAP estimate. (...) We are looking for an MAP estimate. (...) We are looking for an MAP estimate."}, {"heading": "5. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Cancer Diagnosis", "text": "First, we look at the two Wisconsin breast cancer datasets (original and diagnostic) from the UCI Machine Learning Repository2. The goal of both tasks is to identify benign or malignant cells. Characteristic dimensions are 9 for the original data and 30 for the diagnostic data. We place F0 on the smallest dimensionality of the tasks to favor error reduction (as suggested by (8)), and \u03b7 = 10 \u2212 3 to increase the role of the domain, transforms itself in linking the tasks to the regulation parameters (\u03b1, \u03d1) determined by cross-validation (the robustness of these parameters is shown below). We conduct both multitask learning and transfer learning experiments and compare 2UC Irvine Machine Learning Repository: http: / / archive.ics.uizs.edu / ml /.the LPM to STL and the methods in (Wang Mahadevan, 2011) as HDabbreviated."}, {"heading": "5.2. Mine Detection", "text": "The landmine detection problem (Xue et al., 2007) is based on aerial synthetic aperture radar (SAR) data and the underwater mine detection problem (Liu et al., 2009) is based on synthetic aperture sonar data (SAS) 3. Here we solve these two problems together using the proposed cross-sectoral multi-task learning approach. The dimensionality of landmine data is 9 and that of underwater mine data 13, and the labels do not have the same exact significance for the two problem areas. There are a total of 19 landmine tasks and 8 underwater mine tasks. The number of data points in the underwater mine tasks ranges from 756 to 3562, which is much greater than for the landmine tasks (from 445 to 454).This problem can be considered multitasking learning via heterogeneous input output output output domains."}, {"heading": "6. Conclusions", "text": "We have proposed the LPM model for multi-task learning across disciplines, based on heterogeneous representations of characteristics within the tasks. MTL's usefulness in LPM is based on the relation of the tasks in the latent attribute space, which is characterized by the sparse domain transformations. By promoting the sparse domain transformations and common classifiers, the exchange of information is promoted for the benefit of improving performance in each task. Theoretical analysis as well as experimental results demonstrate the importance of sparseness."}, {"heading": "Acknowledgement", "text": "II = Annex II = Annex II = Annex II = Annex II (7), Annex II (7), Annex II (7), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), Annex II (8), (8), (8), (8), (8), (8), (8), (8, 8, (8), (8), (8), (8),"}], "references": [{"title": "Bayesian analysis of binary and polychotomous response data", "author": ["J.H. Albert", "S. Chib"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Albert and Chib,? \\Q1993\\E", "shortCiteRegEx": "Albert and Chib", "year": 1993}, {"title": "Multi-task feature learning", "author": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Task clustering and gating for Bayesian multitask learning", "author": ["B. Bakker", "T. Heskes"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bakker and Heskes,? \\Q2003\\E", "shortCiteRegEx": "Bakker and Heskes", "year": 2003}, {"title": "A model of inductive bias learning", "author": ["J. Baxter"], "venue": "J. Artif. Intell. Res. (JAIR), pp", "citeRegEx": "Baxter,? \\Q2000\\E", "shortCiteRegEx": "Baxter", "year": 2000}, {"title": "A notion of task relatedness yielding provable multiple-task learning guarantees", "author": ["S. Ben-David", "R.S. Borbely"], "venue": "Machine Learning,", "citeRegEx": "Ben.David and Borbely,? \\Q2008\\E", "shortCiteRegEx": "Ben.David and Borbely", "year": 2008}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "B.T. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Bounds on the largest singular value of a matrix and the convergence of simultaneous and blockiterative algorithms for sparse linear systems", "author": ["C. Byrne"], "venue": "International Transactions in Operational Research,", "citeRegEx": "Byrne,? \\Q2009\\E", "shortCiteRegEx": "Byrne", "year": 2009}, {"title": "Adaptive sparseness for supervised learning", "author": ["M.A.T. Figueiredo"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Figueiredo,? \\Q2003\\E", "shortCiteRegEx": "Figueiredo", "year": 2003}, {"title": "A graph-based framework for multi-task multi-view learning", "author": ["J. He", "L. Rick"], "venue": "In Proceedings of the 28th International Conference on Machine Learning, ICML\u2019", "citeRegEx": "He and Rick,? \\Q2011\\E", "shortCiteRegEx": "He and Rick", "year": 2011}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Semisupervised multitask learning", "author": ["Q. Liu", "X. Liao", "H. Li", "J.R. Stack", "L. Carin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Taking advantage of sparsity in multi-task learning", "author": ["K. Lounici", "M. Pontil", "A.B. Tsybakov", "S. van de Geer"], "venue": "In Proceedings of the 22nd Conference on Information Theory,", "citeRegEx": "Lounici et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lounici et al\\.", "year": 2009}, {"title": "Learning from multiple outlooks", "author": ["H. Maayan", "S. Mannor"], "venue": "In ICML, pp", "citeRegEx": "Maayan and Mannor,? \\Q2011\\E", "shortCiteRegEx": "Maayan and Mannor", "year": 2011}, {"title": "Heterogeneous domain adaptation using manifold alignment", "author": ["C. Wang", "S. Mahadevan"], "venue": "IJCAI/AAAI,", "citeRegEx": "Wang and Mahadevan,? \\Q2011\\E", "shortCiteRegEx": "Wang and Mahadevan", "year": 2011}, {"title": "Multitask learning for classification with dirichlet process priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "Whereas STL solves each task in isolation, with possible relations between the tasks ignored, MTL solves the tasks jointly, exploiting between-task relations to reduce the hypothesis space and improve generalization (Baxter, 2000).", "startOffset": 216, "endOffset": 230}, {"referenceID": 3, "context": "For supervised learning, in particular, MTL can achieve the same level of generalization performance as STL, and yet uses significantly fewer labeled examples per task (Baxter, 2000).", "startOffset": 168, "endOffset": 182}, {"referenceID": 3, "context": "While the MTL literature has primarily assumed that the tasks have the same input and output domains and differ only in data distributions (Baxter, 2000; Bakker & Heskes, 2003; Argyriou et al., 2007; Ben-David & Borbely, 2008), a number of recent publications are beginning to break the limit of this assumption, in an attempt of extending MTL to a wider range of applications (He & Rick, 2011; Maayan & Mannor, 2011; Kulis et al.", "startOffset": 139, "endOffset": 226}, {"referenceID": 1, "context": "While the MTL literature has primarily assumed that the tasks have the same input and output domains and differ only in data distributions (Baxter, 2000; Bakker & Heskes, 2003; Argyriou et al., 2007; Ben-David & Borbely, 2008), a number of recent publications are beginning to break the limit of this assumption, in an attempt of extending MTL to a wider range of applications (He & Rick, 2011; Maayan & Mannor, 2011; Kulis et al.", "startOffset": 139, "endOffset": 226}, {"referenceID": 9, "context": ", 2007; Ben-David & Borbely, 2008), a number of recent publications are beginning to break the limit of this assumption, in an attempt of extending MTL to a wider range of applications (He & Rick, 2011; Maayan & Mannor, 2011; Kulis et al., 2011; Wang & Mahadevan, 2011).", "startOffset": 185, "endOffset": 269}, {"referenceID": 9, "context": "The work in (Kulis et al., 2011) considers a source task and a target task, assumed to have different feature dimensions, and learns a nonlinear transformation between the source feature domain and the target feature domain using kernel techniques.", "startOffset": 12, "endOffset": 32}, {"referenceID": 9, "context": "The approach we take differs from (Maayan & Mannor, 2011; Kulis et al., 2011; Wang & Mahadevan, 2011) in several important aspects.", "startOffset": 34, "endOffset": 101}, {"referenceID": 9, "context": "By contrast, the methods in (Maayan & Mannor, 2011; Kulis et al., 2011) are supervised, and the method in (Wang & Mahadevan, 2011) is semi-supervised in learning domain transforms, but supervised in learning classification.", "startOffset": 28, "endOffset": 71}, {"referenceID": 7, "context": "The proposed approach is based on a sparse hierarchical Bayesian model, referred to as the latent probit model (LPM), which jointly represents the sparse domain transforms and a common sparse probit classifier (Albert & Chib, 1993) in the latent feature space, with the sparsity imposed by a hierarchical Laplacian prior (Figueiredo, 2003).", "startOffset": 321, "endOffset": 339}, {"referenceID": 7, "context": "The parameters w and {Fm}m=1 are given hierarchical Laplacian priors (Figueiredo, 2003) to encourage sparsity, with the priors specified by hyperparameters {\u03b3, \u03bb}.", "startOffset": 69, "endOffset": 87}, {"referenceID": 5, "context": "We derive an upper bound to \u2016\u0175 \u2212 w\u20162, following similar arguments as in (Bickel et al., 2009; Lounici et al., 2009) and making use of a key result in (Byrne, 2009) on extreme singular values of Hermitian matrices.", "startOffset": 72, "endOffset": 115}, {"referenceID": 11, "context": "We derive an upper bound to \u2016\u0175 \u2212 w\u20162, following similar arguments as in (Bickel et al., 2009; Lounici et al., 2009) and making use of a key result in (Byrne, 2009) on extreme singular values of Hermitian matrices.", "startOffset": 72, "endOffset": 115}, {"referenceID": 6, "context": ", 2009) and making use of a key result in (Byrne, 2009) on extreme singular values of Hermitian matrices.", "startOffset": 42, "endOffset": 55}, {"referenceID": 9, "context": "the LPM to STL and the methods in (Wang & Mahadevan, 2011) (abbreviated as HDAMA), (Maayan & Mannor, 2011), and (Kulis et al., 2011), with all competing methods using standard probit classifiers.", "startOffset": 112, "endOffset": 132}, {"referenceID": 9, "context": "LPM (Proposed Method) STL HDAMA (Wang & Mahadevan,2011) Kulis et al.(2011) 50 100 150 200 0.", "startOffset": 56, "endOffset": 75}, {"referenceID": 9, "context": "LPM (Proposed Method) STL HDAMA (Wang & Mahadevan,2011) Maayan & Mannor (2011) Kulis et al.(2011) 50 100 150 200 0.", "startOffset": 79, "endOffset": 98}, {"referenceID": 9, "context": "LPM (Proposed Method) STL HDAMA (Wang & Mahadevan,2011) Maayan & Mannor (2011) Kulis et al.(2011)", "startOffset": 79, "endOffset": 98}, {"referenceID": 14, "context": "The land-mine detection problem (Xue et al., 2007) is based on airborne synthetic-aperture radar (SAR) data and the underwater mine detection problem (Liu et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 10, "context": ", 2007) is based on airborne synthetic-aperture radar (SAR) data and the underwater mine detection problem (Liu et al., 2009) is based on synthetic-aperture sonar (SAS) data.", "startOffset": 107, "endOffset": 125}, {"referenceID": 9, "context": "LPM (Proposed Method) STL HDAMA (Wang & Mahadevan,2011) Kulis et al.(2011) 50 100 150 200 250 300 0.", "startOffset": 56, "endOffset": 75}, {"referenceID": 9, "context": "LPM (Proposed Method) STL HDAMA (Wang & Mahadevan,2011) Maayan & Mannor (2011) Kulis et al.(2011) 50 100 150 200 250 300 0.", "startOffset": 79, "endOffset": 98}, {"referenceID": 9, "context": "LPM (Proposed Method) STL HDAMA (Wang & Mahadevan,2011) Maayan & Mannor (2011) Kulis et al.(2011)", "startOffset": 79, "endOffset": 98}, {"referenceID": 5, "context": "We follow (Bickel et al., 2009; Lounici et al., 2009) to similarly define \u03bas = min \u03b4 6=0 n \u22121/2 t \u2016\u03b4J\u2016 \u22121 2 \u2016\u03a8 T \u03b4\u20162, then", "startOffset": 10, "endOffset": 53}, {"referenceID": 11, "context": "We follow (Bickel et al., 2009; Lounici et al., 2009) to similarly define \u03bas = min \u03b4 6=0 n \u22121/2 t \u2016\u03b4J\u2016 \u22121 2 \u2016\u03a8 T \u03b4\u20162, then", "startOffset": 10, "endOffset": 53}, {"referenceID": 6, "context": "By the result in (Byrne, 2009),", "startOffset": 17, "endOffset": 30}], "year": 2012, "abstractText": "Learning multiple tasks across heterogeneous domains is a challenging problem since the feature space may not be the same for different tasks. We assume the data in multiple tasks are generated from a latent common domain via sparse domain transforms and propose a latent probit model (LPM) to jointly learn the domain transforms, and a probit classifier shared in the common domain. To learn meaningful task relatedness and avoid over-fitting in classification, we introduce sparsity in the domain transforms matrices, as well as in the common classifier parameters. We derive theoretical bounds for the estimation error of the classifier parameters in terms of the sparsity of domain transform matrices. An expectation-maximization algorithm is derived for learning the LPM. The effectiveness of the approach is demonstrated on several real datasets.", "creator": "LaTeX with hyperref package"}}}