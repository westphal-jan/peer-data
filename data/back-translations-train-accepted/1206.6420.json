{"id": "1206.6420", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Distributed Parameter Estimation via Pseudo-likelihood", "abstract": "Estimating statistical models within sensor networks requires distributed algorithms, in which both data and computation are distributed across the nodes of the network. We propose a general approach for distributed learning based on combining local estimators defined by pseudo-likelihood components, encompassing a number of combination methods, and provide both theoretical and experimental analysis. We show that simple linear combination or max-voting methods, when combined with second-order information, are statistically competitive with more advanced and costly joint optimization. Our algorithms have many attractive properties including low communication and computational cost and \"any-time\" behavior.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (1650kb)", "http://arxiv.org/abs/1206.6420v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.DC stat.ML", "authors": ["qiang liu", "alexander t ihler"], "accepted": true, "id": "1206.6420"}, "pdf": {"name": "1206.6420.pdf", "metadata": {"source": "META", "title": "Distributed Parameter Estimation via Pseudo-likelihood", "authors": ["Qiang Liu", "Alexander Ihler"], "emails": ["qliu1@uci.edu", "ihler@ics.uci.edu"], "sections": [{"heading": "1. Introduction", "text": "Wireless sensor networks are becoming ubiquitous, with applications such as environmental monitoring, health care and smart homes. Traditional centralized approaches to machine learning are not good for sensor networks because of the constraints on sensor resources. Sensors have limited local computing capacity, memory and power, and their wireless communication is expensive in terms of power consumption. These constraints make centralized data acquisition and processing difficult. Error tolerance and robustness are also critical features. Graphical models are a natural framework for distributed inferences in sensor networks (e.g. Cetin et al., 2007). However, most learning algorithms are not distributed and require centralized data processing and storage. In this work, we provide a general framework for distributed parameter assessments based on a combination of local and inexpensive estimates."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Graphical models for sensor networks", "text": "Consider a graphical model of a random vector x = [x1,.., xp] in exponential family form, p (x | \u03b8) = exp (\u03b8Tu (x) \u2212 logZ (\u03b8)), (1) where \u03b8 = {\u03b8\u03b1} \u03b1, I and u (x) = {u\u03b1 (x\u03b1) \u03b1, I are vectors of the same size and \u03b8Tu (x) are their internal product. I am a set of variable indices and u\u03b1 (x\u03b1) sufficient local statistics. Z (\u03b8) is the partition function that normalizes the distribution. The distribution is connected to a Markov diagram G = (V, E), with the node i-V representing a variable xi and edge (ij). E represents these xi and xj coordinates, which are identical in some \u03b1, i.e., the x coordinates are identical."}, {"heading": "2.2. M-estimators", "text": "M estimators are a broad class of parameter estimators; an M estimator with criterion function '(\u03b8; x) is constantly differentiable and has a unique maximum. If E [\u0432' (\u03b8; x) = 0, then asymptotic standard estimates (van der Vaart, 1998) show that this value is asymptotically consistent and normal, that is, it is n (\u03b8-\u03b8) N (0, V), with asymptotic variance (Godambe, 1960) V = H \u2212 TJH \u2212 1, where J = var (\u03b8, x) is the Fisher information matrix and H = \u2212 E (\u0432 \u2212 2) is the expected Hessian matrix, asymptotic variance, asymptotic variance, asymptotic variance, asymptotic variance. \""}, {"heading": "2.3. MLE and MPLE", "text": "In fact, most of them will be able to move to another world, in which they will be able to escape rather than to another world."}, {"heading": "3. A Distributed Paradigm", "text": "This defines an M estimator to be efficiently calculated locally by sensor i and which (3) has a unique maximum that guarantees that the results are asymptotically consistent and normal under normal technical conditions. Furthermore, we assume that each parameter component is covered by at least one local estimator and that (3) a valid global estimator can be constructed by combining them. Although our results are more general, in this work we mainly assume \"ilocal\" (both \u2212 LE \u2212 i) so that each parameter component can be covered by at least one local estimator and a valid global estimator can be constructed by combining them."}, {"heading": "3.1. One-Step Consensus.", "text": "The goal is to construct a combined estimator. Unfortunately, as we show below, this simple approach usually works poorly, partly because it weights all estimators equally and the worst estimator can significantly worsen the overall performance. Therefore, it would be helpful to weight the estimators according to their quality. Let's show in the following that this simple approach usually works poorly, partly because it weights all estimators equally and the worst estimator can significantly worsen the overall performance. Let's use the function of XA (i) and, depending on the other local estimators, an empirical measure of the quality of the i-th local estimator for estimating the parameters (e.g. w-i could be a function of V-ilocal.We present two methods to combine the estimators based on the weight (i)."}, {"heading": "3.2. Joint Optimization via ADMM", "text": "A more principled way to ensure consensus is to solve a common optimization problem, achieving maximum consensus (maximum consensus), maximum consensus (maximum consensus) and maximum consensus (maximum consensus).In this section, we derive a distributed algorithm for (6) that can be treated as an iterative version of the linear consensus introduced via (2).Our algorithm is based on the alternative direction method of multipliers (ADMM), which is well suited for distributed convex optimization (Boyd et al., 2011), especially distributed consensus (Bertsekas & Tsitsiklis, 1989)."}, {"heading": "4. Asymptotic Analysis", "text": "In this section, we give an asymptotic analysis of our methods, with which we provide methods to optimally determine the weights of the linear and maximum consensus. To ensure the convenience of evaluation, we embed the local payer at the highest level in a (possibly degenerative) payer of the entire parameter vector, by embedding the local payer at the highest level at the highest level at the highest level in the payer (at the highest level in the payer). Similarly, we have the payer at the highest level at Hilocal extended, and s i at the highest level at the highest level at the payer (at the highest level, at the highest level at zero). Similarly, we have the payer at the highest level at Hilocal extended, and s i at the highest level at the highest level at the asymptotic variance of the extended matrix, whereby the suymptotic V is at the highest level of the augmented payer."}, {"heading": "4.1. Optimal Choice of Weights", "text": "Note: It is very useful that the problem can be reformed in such a way that it can lead to the minimization of the consensus (V). In the following, we will discuss the optimal weights for the linear and maximum consensus. Weights for the maximum consensus make the optimal weights relatively simple: Proposition 4.4. For the maximum consensus scale, we can separate the optimal weights for the linear and maximum consensus. In practice, we can easily estimate the optimal weights by simply estimating the optimal weights using W-1 / V, which makes the maximum consensus feasible in practice."}, {"heading": "4.2. Illustration on One Parameter Case", "text": "In this section, we illustrate our asymptotic results in a toy example that offers an intuitive comparison of our algorithms. Suppose we have a scale parameter estimated by two unbiased estimators. (1) If the asymptotic variance (1), then the asymptotic variance is vi = (hi) \u2212 1 = var (si). Let us (s1, s2) be the correlation of the two estimators. (1) Linear consensus with uniform weights: The asymptotic variance is vi = (hi) \u2212 1 = var (si). Let us (s1, s2) be the correlation of the two estimators. (1) Linear consensus with uniform weights: The asymptotic variance is vi = 2 + var (s1, s2). The asymptotic variance is: var (s1 + s22) = 14 (v1 + v12).Linear consensus with hessian."}, {"heading": "5. Experiments", "text": "In this section, we test our algorithms both on small models (for which asymptotic variance can be calculated accurately) and on larger models of more practical size. We use a pair-wise Ising model p (x) \u0445 exp (\u2211 (ij) \u043a\u043d\u0438\u0441\u0442ijxixj + \u0445 V \u03b8ixi), xi \u0432 {\u2212 1, 1}, with random, true parameters generated by Phenomenij \u0445 N (0, \u03c3pair) and \u03b8i \u0445 N (0, \u03c3singleton). We test the Joint MPLE and Linear consensus with uniform weights (linear uniform uniform), with diagonal weights w \u0644i\u03b1 = 1 / V \u0441i \u03b1, \u03b1 (linear diagonal) and with the optimal vector weights specified in Proposition 4.6 (Linear-Opt). We also test the maximum consensus with diagonal weights (Max \u2212 Diagonal (Opt), which is calculated for the maximum sensitivity of either the 4.4 or the optimum sensitivity of the equilibrium."}, {"heading": "5.1. Small Models", "text": "Two small graphs are considered: star charts and grids that have opposing topological characteristics. For these small models, we estimate the paired parameters successij with known singleton parameters successi.Star graphs. A star chart has an unbalanced degree distribution that has reached its peak in the center of the node. There is theoretical and empirical work that shows that such grade-unbounded graphs are more difficult to learn than regular graphs (e.g. Liu & Ihler, 2011). The difficulty arises because the local estimators of high-grade nodes tend to deteriorate the overall performance. As we see in Section 4, the Max Consensus Method is suitable for such graphs as they can identify and discard the bad estimators. Simulation supports this expectation."}, {"heading": "5.2. Larger Models", "text": "We also test our algorithms using larger diagrams, including a 100-node free network generated by the Baraba-si-Albert model (Baraba-si & Albert, 1999) and a 100-node Euclidean diagram generated by uniformly connecting nearby sensors (distance \u2264.15) at the [0, 1] \u00d7 [0, 1] level; see Fig. 4. In these models, we estimate both the individual and the pair parameters. In Fig. 4 (a) - (b), we see trends similar to their smaller analogues, the star diagram and the 4 \u00d7 4 grid, which confirms that our analysis remains useful for larger-size models."}, {"heading": "6. Related Work", "text": "A very recent, independently developed paper (Wiesel & Hero, 2012) takes a similar but less general approach to Gaussian covariance estimation, proposing a similar linear consensus approach (with uniform weights only) and a similar parallel algorithm for the common MPLE, but does not discuss the maximum consensus or the Lin-ear consensus with general weights, and does not provide comprehensive theoretical analysis; another recent paper (Eidsvik et al., 2010) uses the composite probability for the parallel computation of spatial data. Bradley & Guestrin (2011) provided a sample complexity analysis for MPLE and disjoint MPLE, which can be extended to our algorithms; another work line approaches MLE by estimating the partition function with varying algorithms (e.g. Wainwright, 2006; Sutton & McCallum, 2009), which can even work well with a \"wrong\" model in prediction tasks and assume a measurement form that is potentially suitable for distribution in 2006 (e.g. McCallum)."}, {"heading": "7. Conclusion", "text": "In this paper, we present a general framework for distributed parameter learning. We show that the smart one-step consensus methods of local estimators, especially those that evaluate second-order local information, are both computationally efficient and statistically competitive with iterative methods for collective optimization. In particular, we show that the max combination method is well suited for scale-free networks, a well-identified problem for existing methods. Our theory of the combination of estimators is quite general and can be applied to other contexts to increase statistical performance."}], "references": [{"title": "Emergence of scaling in random networks", "author": ["Barab\u00e1si", "A.-L", "R. Albert"], "venue": "Science, 286(5439):509\u2013512,", "citeRegEx": "Barab\u00e1si et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barab\u00e1si et al\\.", "year": 1999}, {"title": "Parallel and distributed computation: numerical methods", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": null, "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1989}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "J. Royal Stat. Soc. D,", "citeRegEx": "Besag,? \\Q1975\\E", "shortCiteRegEx": "Besag", "year": 1975}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends in Machine Learn.,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["J.K. Bradley", "C. Guestrin"], "venue": "In AISTATS,", "citeRegEx": "Bradley and Guestrin,? \\Q2011\\E", "shortCiteRegEx": "Bradley and Guestrin", "year": 2011}, {"title": "Graphical models and fusion in sensor networks", "author": ["M. Cetin", "L. Chen", "J. Fisher III", "A. Ihler", "O. Kreidl", "R. Moses", "M. Wainwright", "J. Williams", "A. Willsky"], "venue": "Wireless Sensor Networks,", "citeRegEx": "Cetin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cetin et al\\.", "year": 2007}, {"title": "Estimation and prediction in spatial models with block composite likelihoods using parallel computing", "author": ["J. Eidsvik", "B.A. Shaby", "B.J. Reich", "M. Wheeler", "J. Niemi"], "venue": null, "citeRegEx": "Eidsvik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eidsvik et al\\.", "year": 2010}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M.I. Jordan"], "venue": "In ICML, pp", "citeRegEx": "Liang and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Liang and Jordan", "year": 2008}, {"title": "Composite likelihood methods", "author": ["B.G. Lindsay"], "venue": "Contemporary Mathematics,", "citeRegEx": "Lindsay,? \\Q1988\\E", "shortCiteRegEx": "Lindsay", "year": 1988}, {"title": "Learning scale free networks by reweighted L1 regularization", "author": ["Q. Liu", "A. Ihler"], "venue": "In AISTATS, pp", "citeRegEx": "Liu and Ihler,? \\Q2011\\E", "shortCiteRegEx": "Liu and Ihler", "year": 2011}, {"title": "Highdimensional Ising model selection using L1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J. Lafferty"], "venue": "Ann. Stat.,", "citeRegEx": "Ravikumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "Piecewise training for structured prediction", "author": ["C. Sutton", "A. McCallum"], "venue": "Mach. Learn.,", "citeRegEx": "Sutton and McCallum,? \\Q2009\\E", "shortCiteRegEx": "Sutton and McCallum", "year": 2009}, {"title": "Estimating the wrong graphical model: Benefits in the computation-limited setting", "author": ["M. Wainwright"], "venue": "J. Machine Learn. Res.,", "citeRegEx": "Wainwright,? \\Q2006\\E", "shortCiteRegEx": "Wainwright", "year": 2006}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Distributed covariance estimation in Gaussian graphical models", "author": ["A. Wiesel", "A.O. Hero"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "Wiesel and Hero,? \\Q2012\\E", "shortCiteRegEx": "Wiesel and Hero", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "` is said to be information-unbiased (Lindsay, 1988) if J = H.", "startOffset": 37, "endOffset": 52}, {"referenceID": 2, "context": "The maximum pseudo-likelihood estimator (MPLE) (Besag, 1975) provides a computationally efficient alternative to MLE.", "startOffset": 47, "endOffset": 60}, {"referenceID": 3, "context": "Our algorithm is based on the alternating direction method of multipliers (ADMM), which is well suited to distributed convex optimization (Boyd et al., 2011), particularly distributed consensus (Bertsekas & Tsitsiklis, 1989).", "startOffset": 138, "endOffset": 157}, {"referenceID": 10, "context": "This robustness makes max consensus useful for learning in difficult graphs, such as scale free graphs, for which standard methods often perform poorly (Ravikumar et al., 2010; Liu & Ihler, 2011).", "startOffset": 152, "endOffset": 195}, {"referenceID": 6, "context": "Another recent work (Eidsvik et al., 2010) uses composite likelihood for parallel computing on spatial data.", "startOffset": 20, "endOffset": 42}, {"referenceID": 6, "context": "Another recent work (Eidsvik et al., 2010) uses composite likelihood for parallel computing on spatial data. Bradley & Guestrin (2011) gave a sample complexity analysis for MPLE and disjoint MPLE, which may be extensible to our algorithms.", "startOffset": 21, "endOffset": 135}], "year": 2012, "abstractText": "Estimating statistical models within sensor networks requires distributed algorithms, in which both data and computation are distributed across the nodes of the network. We propose a general approach for distributed learning based on combining local estimators defined by pseudo-likelihood components, encompassing a number of combination methods, and provide both theoretical and experimental analysis. We show that simple linear combination or max-voting methods, when combined with second-order information, are statistically competitive with more advanced and costly joint optimization. Our algorithms have many attractive properties including low communication and computational cost and \u201cany-time\u201d behavior.", "creator": "LaTeX with hyperref package"}}}