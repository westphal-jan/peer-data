{"id": "1206.6422", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "An Online Boosting Algorithm with Theoretical Justifications", "abstract": "We study the task of online boosting--combining online weak learners into an online strong learner. While batch boosting has a sound theoretical foundation, online boosting deserves more study from the theoretical perspective. In this paper, we carefully compare the differences between online and batch boosting, and propose a novel and reasonable assumption for the online weak learner. Based on the assumption, we design an online boosting algorithm with a strong theoretical guarantee by adapting from the offline SmoothBoost algorithm that matches the assumption closely. We further tackle the task of deciding the number of weak learners using established theoretical results for online convex programming and predicting with expert advice. Experiments on real-world data sets demonstrate that the proposed algorithm compares favorably with existing online boosting algorithms.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (222kb)", "http://arxiv.org/abs/1206.6422v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shang-tse chen", "hsuan-tien lin", "chi-jen lu"], "accepted": true, "id": "1206.6422"}, "pdf": {"name": "1206.6422.pdf", "metadata": {"source": "META", "title": "An Online Boosting Algorithm with Theoretical Justifications", "authors": ["Shang-Tse Chen", "Hsuan-Tien Lin", "Chi-Jen Lu"], "emails": ["b95100@csie.ntu.edu.tw", "htlin@csie.ntu.edu.tw", "cjlu@iis.sinica.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to abide by the rules that they have imposed on themselves. (...) In fact, it is so that they are able to outdo themselves. (...) In fact, it is so that they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) \"It is as if they are able to outdo themselves. (...)\" (...) \"(...)\" (...) \"(...) (...)\" (...) () (() () (() () () () () () () () () () () () () () () () ()) () () () () () () ()) () () ()) () () ()) () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () ()) () () () () () () () () () ()) () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () (() () () () () () () () () () () (() (() () () () (() () () () ((() () () (() () () () ("}, {"heading": "2. Online versus Batch Boosting", "text": "We consider the online learning problem where an online learner must process a stream of examples (x1, y1),., (xD, yT), (x1), (x1), (x1), (x2), (x2), (x2), (x2), (x3), (x3), (x3), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x5), (x5), (x5), (x4), (x4), (4), (4), (4), (x4), (x4), (x4), (x4), (4), (x4), x4 (4), (x4), x4 (4), (x4), x4 (4), (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4 (x4), x4 (x4), x4), x4 (x4), x4 (x4 (x4), x4), x4 (x4), x4 (x4), x4"}, {"heading": "3. Online Weak Learners", "text": "In this section, we address the second difficulty discussed in the previous section and examine the condition for an online weak learner to have a positive advantage (= 2003). Consider the case that H, the weak hypotheses space, consists of linear functions so that each h, H can be considered a vector in Rd, with h (x) defined examples as < h, x >, the inner product of the vectors h and x. For simplicity, we assume that each h, 1 for each h, H can be considered a vector in Rd. We can reduce the problem of finding a good online weak learner to the well-known linear optimization problem as a consequence. With the T examples for S, which arrive sequentially, the weak learner in round is given the data xt as well as its weight wt, and it then produces a hypothesis that gets a reward rt (ht)."}, {"heading": "4. Our Online Boosting Algorithm", "text": "In this section we show how to select weights for examples and how to combine hypotheses of weak learners (to obtain an online boosting algorithm). (note) Note. (note) Note. (note) Note. (note) Note. (note) Note. (note) Note. (note) Note. (note) Note. (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note.). (note). (note.). (note.). (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.) (note. (note.) (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note.). (note. (note.). (note. (note.). (note. (note.). (note. (note.). (note.). (note.). (note. (note.). (note. (note.). (note.). (note.). (note. (note.). (note. (note.). (note.).). (note.). (note. (note.). (note.). (note. (note.). (note.). (note.).). (note. (note.). (note.).. (note.). (note.). (note.). (note.). (note.)... (note..).. (note.)..)..... (note..)...............)................"}, {"heading": "5. Experiments", "text": "In this section, we compare the empirical performance of the proposed algorithms with two other leading algorithms in online boosting. We do not compare them with some other leading algorithms, as they differ from our proposed algorithms. Thus, for example, Grbovic & Vucetic (2011) proposed the incremental boosting algorithm that may store previous examples, while our proposed algorithms rely only on the latest incoming example. Another case is the algorithm in Pelossof et al. (2008), which assumes that the weak learners are pre-trained statically, i.e. offline, and merely update the weights for combining the learners, while our proposed algorithms allow online, dynamic weak learners. Another family of online boosting algorithms is proposed in Babenko et al. (2009a), where the weak learners need to be updated using stochastic descent so that the weak learners can be linked with the optimization of a loss function."}, {"heading": "5.1. Compared Algorithms", "text": "1. Online AdaBoost (Oza & Russell, 2001) uses a Poisson sampling method to approximate the weighting method of AdaBoost. It guarantees that its algorithm will approach AdaBoost by using lossless weak online learners trained for the same set of lessons online and offline, as the number of models and training examples approaches infinity. However, the algorithm only asymptotically ensures a good hypothesis, although it is not proven to achieve low remorse during online learning. We will call the algorithm OzaBoost.2. Online GradientBoost (Leistner et al., 2009) is an online variant of GradientBoost (Friedman, 2000) that uses functional gradient pedigree to determine the optimal sample weights and greedily minimize the loss of interest. A special trick in the algorithm is to compare the use of some learning selectors based on \"one of the weakest possible selectors.\""}, {"heading": "5.2. Weak Learners", "text": "In our experiments, we select two distinct weak learners to take a closer look at the enhancement capability of our proposed algorithms: the first is Perceptron (Rosenblatt, 1962), a popular and famous online learning algorithm that, like our analysis in Section 3, takes into account a number of linear hypotheses; the second weak learner we select is Naive Bayes, a lossless online algorithm, a crucial prerequisite for the convergence of OzaBoost."}, {"heading": "5.3. Results", "text": "In fact, we will be able to go in search of a solution that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution. \""}, {"heading": "6. Conclusion", "text": "We propose a novel online boosting algorithm. The algorithm is simple in its formulation but nevertheless carefully designed from a theoretical point of view to avoid many intrinsic difficulties in adapting boosting algorithms to the online environment. In particular, we define the concept of weak learning for online boosting and take advantage of the idea of extending a promising offline boosting algorithm to its online version. We also address the problem of selecting an appropriate number of weak learners by carefully applying established theoretical results. Not only is the proposed algorithm sound in its theoretical justification, but it also yields promising experimental results. We show that the proposed algorithm can actually promote weak online learners on real data sets."}, {"heading": "Acknowledgments", "text": "We thank Ching-Hua Yu for the idea of using the framework of Predicting with Expert Advice. We also thank the anonymous reviewers for valuable suggestions. This work is supported by Taiwan's National Science Council (NSC 100-2628-E-002-010 and NSC 100-2221-E-001-008-MY3)."}], "references": [{"title": "A Family of Online Boosting Algorithms", "author": ["B. Babenko", "M. Yang", "S. Belongie"], "venue": "IEEE ICCV Workshop on On-line Learning for Computer Vision, pp", "citeRegEx": "Babenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Babenko et al\\.", "year": 2009}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "M. Yang", "S.J. Belongie"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Babenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Babenko et al\\.", "year": 2009}, {"title": "On boosting with polynomially bounded distributions", "author": ["N.H. Bshouty", "D. Gavinsky", "M. Long"], "venue": "JMLR, 3:483\u2013506,", "citeRegEx": "Bshouty et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bshouty et al\\.", "year": 2002}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Efficient projections onto the l1-ball for learning in high dimensions", "author": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Distribution-specific agnostic boosting", "author": ["Feldman", "Vitaly"], "venue": "In Proceedings of ICS, pp", "citeRegEx": "Feldman and Vitaly.,? \\Q2010\\E", "shortCiteRegEx": "Feldman and Vitaly.", "year": 2010}, {"title": "Game theory, on-line prediction and boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Proceedings of COLT, pp. 325\u2013332. ACM Press,", "citeRegEx": "Freund and Schapire,? \\Q1996\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1996}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman,? \\Q2000\\E", "shortCiteRegEx": "Friedman", "year": 2000}, {"title": "On-line boosting and vision", "author": ["H. Grabner", "H. Bischof"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Grabner and Bischof,? \\Q2006\\E", "shortCiteRegEx": "Grabner and Bischof", "year": 2006}, {"title": "Semisupervised on-line boosting for robust tracking", "author": ["H. Grabner", "C. Leistner", "H. Bischof"], "venue": "In Proceedings of ECCV, pp", "citeRegEx": "Grabner et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grabner et al\\.", "year": 2008}, {"title": "Tracking concept change with incremental boosting by minimization of the evolving exponential loss", "author": ["M. Grbovic", "S. Vucetic"], "venue": "In Proceedings of ECML/PKDD,", "citeRegEx": "Grbovic and Vucetic,? \\Q2011\\E", "shortCiteRegEx": "Grbovic and Vucetic", "year": 2011}, {"title": "Asymmetric gradient boosting with application to spam filtering", "author": ["J. He", "B. Thiesson"], "venue": "In Proceedings of CEAS,", "citeRegEx": "He and Thiesson,? \\Q2007\\E", "shortCiteRegEx": "He and Thiesson", "year": 2007}, {"title": "An application of boosting to graph classification", "author": ["T. Kudo", "E. Maeda", "Y. Matsumoto"], "venue": "In Proceedigns of NIPS,", "citeRegEx": "Kudo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kudo et al\\.", "year": 2004}, {"title": "On Robustness of On-line Boosting - A Competitive Study", "author": ["C. Leistner", "A. Saffari", "P.M. Roth", "H. Bischof"], "venue": "IEEE ICCV Workshop on On-line Learning for Computer Vision, pp", "citeRegEx": "Leistner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leistner et al\\.", "year": 2009}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1994}, {"title": "Gradient feature selection for online boosting", "author": ["X. Liu", "T. Yu"], "venue": "In Proceedings of ICCV, pp", "citeRegEx": "Liu and Yu,? \\Q2007\\E", "shortCiteRegEx": "Liu and Yu", "year": 2007}, {"title": "The rate of convergence of adaboost", "author": ["I. Mukherjee", "C. Rudin", "R.E. Schapire"], "venue": "JMLR W&CP,", "citeRegEx": "Mukherjee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2011}, {"title": "Online bagging and boosting", "author": ["N.C. Oza", "S. Russell"], "venue": "In Proceedings of AISTATS, pp", "citeRegEx": "Oza and Russell,? \\Q2001\\E", "shortCiteRegEx": "Oza and Russell", "year": 2001}, {"title": "Online Coordinate Boosting", "author": ["R. Pelossof", "M. Jones", "I. Vovsha", "C. Rudin"], "venue": "Oct 2008. URL http: //arxiv.org/abs/0810.4553", "citeRegEx": "Pelossof et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pelossof et al\\.", "year": 2008}, {"title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms", "author": ["F. Rosenblatt"], "venue": "Spartan,", "citeRegEx": "Rosenblatt,? \\Q1962\\E", "shortCiteRegEx": "Rosenblatt", "year": 1962}, {"title": "Boostexter: A boostingbased system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q2000\\E", "shortCiteRegEx": "Schapire and Singer", "year": 2000}, {"title": "Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee"], "venue": "The Annals of Statistics,", "citeRegEx": "Schapire et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1998}, {"title": "Smooth boosting and learning with malicious", "author": ["R.A. Servedio"], "venue": "noise. JMLR,", "citeRegEx": "Servedio,? \\Q2003\\E", "shortCiteRegEx": "Servedio", "year": 2003}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "On the theoretical side, boosting identifies the least (weakest) assumption on the learner to make learning possible (Freund & Schapire, 1996; Schapire et al., 1998; Mukherjee et al., 2011), and the assumption can be used to facilitate the analysis of existing algorithms and the design of new ones.", "startOffset": 117, "endOffset": 189}, {"referenceID": 17, "context": "On the theoretical side, boosting identifies the least (weakest) assumption on the learner to make learning possible (Freund & Schapire, 1996; Schapire et al., 1998; Mukherjee et al., 2011), and the assumption can be used to facilitate the analysis of existing algorithms and the design of new ones.", "startOffset": 117, "endOffset": 189}, {"referenceID": 13, "context": "algorithms in an efficient manner to improve performance, which matches the needs of many real-world applications (Schapire & Singer, 2000; Kudo et al., 2004; He & Thiesson, 2007).", "startOffset": 114, "endOffset": 179}, {"referenceID": 10, "context": "Many other online boosting algorithms have been proposed to tackle different application needs, such as semi-supervised learning (Grabner et al., 2008), multi-instance learning (Babenko et al.", "startOffset": 129, "endOffset": 151}, {"referenceID": 23, "context": "One particular boosting algorithm that not only fits our requirements with slight modifications but also comes with simple and elegant theoretical analysis is SmoothBoost (Servedio, 2003).", "startOffset": 171, "endOffset": 187}, {"referenceID": 24, "context": "We mitigate this problem by giving different voting weights to different weak learners and we determine these weights dynamically using Online Convex Programming (Zinkevich, 2003) and the framework of Predicting with Expert Advice (Cesa-Bianchi & Lugosi, 2006), both of which are well-established techniques in online learning.", "startOffset": 162, "endOffset": 179}, {"referenceID": 24, "context": "Therefore, we can apply the gradient descent algorithm of (Zinkevich, 2003) to produce ht in round t, and a standard regret analysis shows that for some constant c > 0,", "startOffset": 58, "endOffset": 75}, {"referenceID": 23, "context": "The concept of smoothness has also been applied to boosting in several frameworks, such as noise-tolerant learning (Servedio, 2003) and agnostic learning (Feldman, 2010), but to the best of our knowledge, this is the first work that incorporates this idea into the problem of online boosting.", "startOffset": 115, "endOffset": 131}, {"referenceID": 2, "context": "It is known that AdaBoost does not always produce such weights (Bshouty et al., 2002).", "startOffset": 63, "endOffset": 85}, {"referenceID": 23, "context": "Fortunately, we can adopt the weighting scheme similar to SmoothBoost (Servedio, 2003) by choosing", "startOffset": 70, "endOffset": 86}, {"referenceID": 23, "context": "We omit the proofs of these two lemmas because they are almost identical to those for Theorem 2 and Theorem 3 in Servedio (2003) respectively.", "startOffset": 113, "endOffset": 129}, {"referenceID": 24, "context": "Thus, if we use the gradient descent algorithm of (Zinkevich, 2003) to produce \u03b1t at step t, we can have", "startOffset": 50, "endOffset": 67}, {"referenceID": 17, "context": "Another case is the algorithm in Pelossof et al. (2008), which assumes the weak learners to be static, i.", "startOffset": 33, "endOffset": 56}, {"referenceID": 0, "context": "Yet another family of online boosting algorithms are proposed in Babenko et al. (2009a), which requires the weak learners to be updated using stochastic gradient descent so that the weak learners can be chained with optimizing a choice of loss function, while our proposed algorithms treat weak learners in a black-box manner with minimum assumptions.", "startOffset": 65, "endOffset": 88}, {"referenceID": 14, "context": "Online GradientBoost (Leistner et al., 2009) is an online variant of GradientBoost (Friedman, 2000), which uses functional gradient descent to", "startOffset": 21, "endOffset": 44}, {"referenceID": 8, "context": ", 2009) is an online variant of GradientBoost (Friedman, 2000), which uses functional gradient descent to", "startOffset": 46, "endOffset": 62}, {"referenceID": 14, "context": "We take such a K for a fair comparison, and run the algorithm with the logit loss function that has consistently been the best choice in existing experimental studies (Leistner et al., 2009).", "startOffset": 167, "endOffset": 190}, {"referenceID": 20, "context": "The first one is Perceptron (Rosenblatt, 1962), a standard and famous online learning algorithm which, like our analysis in Section 3, takes a hypothesis set of linear functions.", "startOffset": 28, "endOffset": 46}, {"referenceID": 4, "context": "OCP, an extra projection step is needed, for which we implement an O(N logN)-time algorithm by Duchi et al. (2008); in fact, a more sophisticated method in Duchi et al.", "startOffset": 95, "endOffset": 115}, {"referenceID": 4, "context": "OCP, an extra projection step is needed, for which we implement an O(N logN)-time algorithm by Duchi et al. (2008); in fact, a more sophisticated method in Duchi et al. (2008) can achieve this in expected O(N) time.", "startOffset": 95, "endOffset": 176}], "year": 2012, "abstractText": "We study the task of online boosting \u2014 combining online weak learners into an online strong learner. While batch boosting has a sound theoretical foundation, online boosting deserves more study from the theoretical perspective. In this paper, we carefully compare the differences between online and batch boosting, and propose a novel and reasonable assumption for the online weak learner. Based on the assumption, we design an online boosting algorithm with a strong theoretical guarantee by adapting from the offline SmoothBoost algorithm that matches the assumption closely. We further tackle the task of deciding the number of weak learners using established theoretical results for online convex programming and predicting with expert advice. Experiments on real-world data sets demonstrate that the proposed algorithm compares favorably with existing online boosting algorithms.", "creator": "LaTeX with hyperref package"}}}