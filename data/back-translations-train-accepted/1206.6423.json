{"id": "1206.6423", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Joint Model of Language and Perception for Grounded Attribute Learning", "abstract": "As robots become more ubiquitous and capable, it becomes ever more important to enable untrained users to easily interact with them. Recently, this has led to study of the language grounding problem, where the goal is to extract representations of the meanings of natural language tied to perception and actuation in the physical world. In this paper, we present an approach for joint learning of language and perception models for grounded attribute induction. Our perception model includes attribute classifiers, for example to detect object color and shape, and the language model is based on a probabilistic categorial grammar that enables the construction of rich, compositional meaning representations. The approach is evaluated on the task of interpreting sentences that describe sets of objects in a physical workspace. We demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (5766kb)", "http://arxiv.org/abs/1206.6423v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.RO", "authors": ["cynthia matuszek", "nicholas fitzgerald", "luke s zettlemoyer", "liefeng bo", "dieter fox"], "accepted": true, "id": "1206.6423"}, "pdf": {"name": "1206.6423.pdf", "metadata": {"source": "META", "title": "A Joint Model of Language and Perception  for Grounded Attribute Learning", "authors": ["Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer"], "emails": ["cynthia@cs.washington.edu", "nfitz@cs.washington.edu", "lsz@cs.washington.edu", "lfb@cs.washington.edu", "fox@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "To do this, however, a robot needs to think together about the various modalities encountered (for example, language and vision) and evoke rich associations with as little guidance as possible. Consider a simple phrase like \"These are the yellow blocks,\" pronounced in a setting where there is a Phys-Appearing in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, 2012. Copyright 2012 by the author (s) / owner (s).ical workspace, which contains a number of objects that vary in shape and color. We assume that a robot can understand sentences like this if it can solve the associated grounded object selection task. Specifically, it needs to recognize that words like \"yellow\" and \"blocks\" refer to object attributes, and the meaning of such words by mapping them to a perception system that explicates them."}, {"heading": "2. Overview of the Approach", "text": "We want to create a common language and perception model for the object selection task. The goal is to automatically set a natural language set x and a set of scene objects O to the subset G O of objects that are described by x. The left panel of Figure 1 shows a sample scene. The goal is to select the objects present in that scene. The individual objects o O are extracted from the scene by segmentation (the right panel of Figure 1 shows sample segments). Given the sentence x = \"Here are the yellow,\" the goal is to select the five yellow objects for the named group G.Model Components Given a set and segmented scene objects we learn a distribution P (G | x, O) over the selected scenes.Our approach combines current models of language and vision, including: (1) A semantic parsing model that defines P (z), which has a distribution over the logical meaning."}, {"heading": "3. Related Work", "text": "To the best of our knowledge, this paper presents the first approach to collaborative learning of visual classifiers and semantic parsers to produce rich, compositional models that extend directly from sensors to meaning. However, there are significant related works on the model components and grounded learning in general. Vision Current state-of-the-art object recognition systems (Felzenszwalb et al., 2009; Yang et al., 2009) are based on local image descriptors, such as SIFT over images over images over 3D point clouds (Johnson & Hebert, 1999). Visual attributes provide rich descriptions of objects and have become a popular topic in the vision community (Farhadi et al., 2009; Parikh & Grauman, 2011); although we still have a very successful understanding of the underlying design rules and how they measure similarity."}, {"heading": "4. Background on Semantic Parsing", "text": "Our grounded language acquisition includes a \"state-of-the-art\" model, FUBL, for semantic parsing, as described in this section. FUBL (Kwiatkowski et al., 2011) is an algorithm for learning factorial categorical grammar (CCG) lexicographs for semantic parsing. Faced with a data set {(xi, zi) | i = 1... n} of natural language sentences xi, paired with logical forms zi representing their meaning, UBL learns a factor lexicon consisting of a set of lexemes L and a set of lexical templates T. Lexemes combine with templates to form lexical items that can be used by a semantic parser to break down natural language sentences into logical forms. Thus, for example, the sentence x = \"this red block is in the shape of a half-pipe\" and the logical form (we combine the Ilogic form with an Ilozi-color)."}, {"heading": "5. Joint Language/Perception Model", "text": "As described in Sec. 2, the object selection task is to identify a subset of objects described in Sec. 2. (D) As described in Sec. 2, the object selection task is to identify a subset of objects. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "6. Model Learning", "text": "We look at the case where the learner already has a partial model, including a small vocabulary and a small set of attribute classifiers. The goal is to automatically expand the model to induce new classifiers bound to new words in the semantic parser. We first describe the learning algorithms, then show how we initialize the approach by working out models decoupled from small datasets with more extensive annotations.Aligning Words to Classifiers A key challenge is to learn new attribute classifiers associated with invisible words in the data."}, {"heading": "7. Experimental Setup", "text": "For each scene, we collected short RGB-D videos with a Kinect depth camera that showed a person gesticulating to a subset of the objects. Notes in natural language were collected using Mechanical Turk; workers were asked to describe the objects shown in the video (see fig. 3). The reference objects were then marked as G, the positive set of objects for that scene. A total of 142 scenes were shown that elicited descriptions of 12 attributes that were evenly divided into shapes and colors. In total, there were 1003 set / annotation pairs. Perceptual Features To automatically segment objects from each scene, we performed RANSAC attributes that match the Kinect depth values to find the table plane, and then extracted contiguous components (segments) of dots with more than a minimum distance above that level."}, {"heading": "8. Results", "text": "This section presents results and a discussion of our evaluation. We demonstrate effective learning in the full model for the object selection task. Afterwards, we briefly describe ablation studies and examples of learned models."}, {"heading": "8.1. Object Set Selection", "text": "To measure the performance of the set selection task, we divided the data by attributes. To initialize the model, we used the data for six of the attributes to train supervised classifiers, and provided logical forms for the corresponding sets to train the first semantic parsing model, as at the end of paragraph 6. Data for the remaining six attributes was used for evaluation, with 80% allocated for training and 20% allocated for testing. Here, all visual scenes are hitherto invisible, the words in the sentences describing the new attributes are unknown, and the only available labels are the output object set G. We report on precision, recall and F1 score for the set selection task. Results are averaged over 10 different runs, with training data presented in various randomized sequences. The system performs well, achieving an average precision of 82%, a memory of 71%, and a relative level of 76% of performance data is achieved within F1."}, {"heading": "8.2. Ablation Studies", "text": "In order to measure the performance of two models in which either the language or the visual component is severely restricted, we have manually created a thesaurus of words used in the dataset to refer to target attributes, which on average contain 5 different ways to refer to each color and shape. To learn the unattended concepts for this baseline, we have first extracted a list of all words that appear in the training corpus but not in the initialization data; words that appear in the thesaurus are grouped into synonym sentences. To train classifiers, we collect objects from scenes in which only terms from the given synonym set appear. Any synonym group that does not appear in at least two different scenes is discarded as a parser. The resulting positive and negative objects are used to form classifiers in which we combine the terms from the given synonym set."}, {"heading": "8.3. Discussion and Examples", "text": "This section discusses typical training runs and data requirements. We present examples of learned models that highlight what is learned and typical errors, and then describe simple experiments to examine the amount of monitored data required for initialization. However, the performance of the classifier after training affects the ability of the system to perform the selection task for the given task. During a sample experiment, the average accuracy of color and shape classifiers for newly learned concepts is sometimes difficult to differentiate. As mentioned in Sec. 4, the semantic parser contains lexemes that pair words with learned classifiers, and features that indicate the use of lexemes in parsing. Fig. 5 shows some selected word and classifier pairs, along with the weight for their associated characteristics (each study illustrates a large number of such lexemes)."}, {"heading": "9. Conclusion", "text": "This paper presents a common model of language and perception for grounded learning of attributes. Our approach learns representations of the meanings of natural language by using visual perception to ground those meanings in the physical world. Learning is done by optimizing data log probability using an online training algorithm reminiscent of EM. This system is able to learn accurate language and attribute models for the object selection task, as data only includes language, raw perceptions and targets. By learning language and perception models together, the approach can detect which new words are color attributes, format attributes or no attributes at all. We believe that our approach has considerable potential to scale to general language problems. Specifically, our modular framework has been designed so that future advances in visual classification and semantic parsing can easily be taken into account. We are also working to fully integrate the complexity of language and the goal of a more physical learning environment, with the ultimate scalability."}, {"heading": "Acknowledgments", "text": "This work was partially funded by the Intel Science and Technology Center for Pervasive Computing, the Robotics Consortium sponsored by the US Army Research Laboratory under the Collaborative Technology Alliance Program (W911NF-10-2-0016), and NSF funding IIS-1115966."}], "references": [{"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. De Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Barnard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barnard et al\\.", "year": 2003}, {"title": "Kernel descriptors for visual recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bo et al\\.", "year": 2010}, {"title": "Depth kernel descriptors for object recognition", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": "In IEEE/RSJ Int\u2019l Conf. on Intelligent Robots and Systems (IROS),", "citeRegEx": "Bo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bo et al\\.", "year": 2011}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D.L. Chen", "R.J. Mooney"], "venue": "In Proc. of the 25th AAAI Conf. on Artificial Intelligence", "citeRegEx": "Chen and Mooney,? \\Q2011\\E", "shortCiteRegEx": "Chen and Mooney", "year": 2011}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth"], "venue": "In Proc. of the Conf. on Computational Natural Language Learning,", "citeRegEx": "Clarke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Farhadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Object detection with discriminatively trained part based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2009}, {"title": "Confidence driven unsupervised semantic parsing", "author": ["D. Goldwasser", "R. Reichart", "J. Clarke", "D. Roth"], "venue": "In Proceedings. of the Association of Computational Linguistics,", "citeRegEx": "Goldwasser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2011}, {"title": "Understanding complex visually referring utterances", "author": ["P. Gorniak", "D. Roy"], "venue": "In Proc. of the HLT-NAACL 2003 Workshop on Learning Word Meaning from NonLinguistic Data,", "citeRegEx": "Gorniak and Roy,? \\Q2003\\E", "shortCiteRegEx": "Gorniak and Roy", "year": 2003}, {"title": "Spoken language understanding using the hidden vector state model", "author": ["Y. He", "S. Young"], "venue": "Speech Communication,", "citeRegEx": "He and Young,? \\Q2006\\E", "shortCiteRegEx": "He and Young", "year": 2006}, {"title": "Using spin images for efficient object recognition in cluttered 3D scenes", "author": ["A. Johnson", "M. Hebert"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Johnson and Hebert,? \\Q1999\\E", "shortCiteRegEx": "Johnson and Hebert", "year": 1999}, {"title": "Lexical generalization in CCG grammar induction for semantic parsing", "author": ["T. Kwiatkowski", "L.S. Zettlemoyer", "S. Goldwater", "M. Steedman"], "venue": "In Proc. of the Conf. on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2011}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A. Ng"], "venue": "In Proc. of the Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "In Proc. of the Association for Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "Int\u2019l Journal of Computer Vision (IJCV),", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["C. Matuszek", "E. Herbst", "L. Zettlemoyer", "D. Fox"], "venue": "In Proc. of the 13th Int\u2019l Symposium on Experimental Robotics (ISER),", "citeRegEx": "Matuszek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "Grounded situation models for robots: Where words and percepts meet", "author": ["N. Mavridis", "D. Roy"], "venue": "In IEEE/RSJ Int\u2019l Conf. on Intelligent Robots and Systems,", "citeRegEx": "Mavridis and Roy,? \\Q2006\\E", "shortCiteRegEx": "Mavridis and Roy", "year": 2006}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "In Int\u2019l Conf. on Computer Vision,", "citeRegEx": "Parikh and Grauman,? \\Q2011\\E", "shortCiteRegEx": "Parikh and Grauman", "year": 2011}, {"title": "Hidden-state conditional random fields", "author": ["A. Quattoni", "S. Wang", "L. p Morency", "M. Collins", "T. Darrell", "Csail", "Mit"], "venue": "In IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Quattoni et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2007}, {"title": "Learning meanings of words and constructions, grounded in a virtual game", "author": ["H. Reckman", "J. Orkin", "D. Roy"], "venue": "In Proc. of the 10th Conf. on Natural Language Processing (KONVENS),", "citeRegEx": "Reckman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reckman et al\\.", "year": 2010}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M.R. Walter", "A.G. Banerjee", "S. Teller", "N. Roy"], "venue": "In Proc. of the National Conf. on Artificial Intelligence (AAAI),", "citeRegEx": "Tellex et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Y.W. Wong", "R.J. Mooney"], "venue": "In Proc. of the Ass\u2019n for Computational Linguistics,", "citeRegEx": "Wong and Mooney,? \\Q2007\\E", "shortCiteRegEx": "Wong and Mooney", "year": 2007}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Improved local coordinate coding using local tangents", "author": ["K. Yu", "T. Zhang"], "venue": "In Proc. of the Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "Yu and Zhang,? \\Q2010\\E", "shortCiteRegEx": "Yu and Zhang", "year": 2010}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["J.M. Zelle", "R.J. Mooney"], "venue": "In Proc. of the National Conf. on Artificial Intelligence,", "citeRegEx": "Zelle and Mooney,? \\Q1996\\E", "shortCiteRegEx": "Zelle and Mooney", "year": 1996}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["L.S. Zettlemoyer", "M. Collins"], "venue": "In Proc. of the Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zettlemoyer and Collins,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Our approach builds on existing work on visual attribute classification (Bo et al., 2011) and probabilistic categorial grammar induction for semantic parsing (Zettlemoyer & Collins, 2005; Kwiatkowski et al.", "startOffset": 72, "endOffset": 89}, {"referenceID": 11, "context": ", 2011) and probabilistic categorial grammar induction for semantic parsing (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011).", "startOffset": 76, "endOffset": 131}, {"referenceID": 11, "context": "For this task, we build on an existing semantic parsing model (Kwiatkowski et al., 2011).", "startOffset": 62, "endOffset": 88}, {"referenceID": 6, "context": "Vision Current state-of-the-art object recognition systems (Felzenszwalb et al., 2009; Yang et al., 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson & Hebert, 1999).", "startOffset": 59, "endOffset": 105}, {"referenceID": 22, "context": "Vision Current state-of-the-art object recognition systems (Felzenszwalb et al., 2009; Yang et al., 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson & Hebert, 1999).", "startOffset": 59, "endOffset": 105}, {"referenceID": 14, "context": ", 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson & Hebert, 1999).", "startOffset": 75, "endOffset": 87}, {"referenceID": 5, "context": "attributes provide rich descriptions of objects, and have become a popular topic in the vision community (Farhadi et al., 2009; Parikh & Grauman, 2011); although very successful, we still lack a deep understanding of the design rules underlying them and how they measure similarity.", "startOffset": 105, "endOffset": 151}, {"referenceID": 1, "context": "Recent work on kernel descriptors (Bo et al., 2010) shows that these hand-designed features are equivalent to a type of match kernel that performs similarly to sparse coding (Yang et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 22, "context": ", 2010) shows that these hand-designed features are equivalent to a type of match kernel that performs similarly to sparse coding (Yang et al., 2009; Yu & Zhang, 2010) and deep networks (Lee et al.", "startOffset": 130, "endOffset": 167}, {"referenceID": 12, "context": ", 2009; Yu & Zhang, 2010) and deep networks (Lee et al., 2009) on many object recognition benchmarks (Bo et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 1, "context": ", 2009) on many object recognition benchmarks (Bo et al., 2010).", "startOffset": 46, "endOffset": 63}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision.", "startOffset": 66, "endOffset": 121}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Goldwasser (2011) presents work on unsupervised learning.", "startOffset": 96, "endOffset": 225}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Goldwasser (2011) presents work on unsupervised learning.", "startOffset": 96, "endOffset": 242}, {"referenceID": 11, "context": "Our research builds on work on supervised learning of CCG parsers (Zettlemoyer & Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Goldwasser (2011) presents work on unsupervised learning.", "startOffset": 96, "endOffset": 360}, {"referenceID": 19, "context": "Roy developed a series of techniques for grounding words in visual scenes (Mavridis & Roy, 2006; Reckman et al., 2010; Gorniak & Roy, 2003).", "startOffset": 74, "endOffset": 139}, {"referenceID": 0, "context": ", see (Barnard et al., 2003)); however, these approaches primarily focus on isolated word meaning, rather than compositional semantic analyses.", "startOffset": 6, "endOffset": 28}, {"referenceID": 15, "context": "Most closely related to our work are approaches that learn probabilistic language models from natural language input (Matuszek et al., 2012; Chen & Mooney, 2011), especially those that include a visual component (Tellex et al.", "startOffset": 117, "endOffset": 161}, {"referenceID": 20, "context": ", 2012; Chen & Mooney, 2011), especially those that include a visual component (Tellex et al., 2011).", "startOffset": 79, "endOffset": 100}, {"referenceID": 11, "context": "FUBL (Kwiatkowski et al., 2011) is an algorithm for learning factored Combinatory Categorial Grammar (CCG) lexicons for semantic parsing.", "startOffset": 5, "endOffset": 31}, {"referenceID": 18, "context": "where the inner difference of expectations is the familiar gradient of a log-linear model for conditional random fields with hidden variables (Quattoni et al., 2007; Kwiatkowski et al., 2010), and is weighted according to the expectation.", "startOffset": 142, "endOffset": 191}, {"referenceID": 11, "context": "As mentioned above, learning in this setting is completely decoupled and we can estimate the semantic parsing distribution P (zi|xi; \u0398) with the FUBL learning algorithm (Kwiatkowski et al., 2011) and the attribute classifiers P (wi|Oi; \u0398 ) with gradient ascent for logistic regression.", "startOffset": 169, "endOffset": 195}, {"referenceID": 2, "context": "After getting segmented objects, features for every object are extracted using kernel descriptors (Bo et al., 2011).", "startOffset": 98, "endOffset": 115}, {"referenceID": 11, "context": "Language Features We follow (Kwiatkowski et al., 2011) in including a standard set of binary indicator features to define the log-linear model P (z|x; \u0398) over logical forms, given sentences.", "startOffset": 28, "endOffset": 54}], "year": 2012, "abstractText": "As robots become more ubiquitous and capable, it becomes ever more important for untrained users to easily interact with them. Recently, this has led to study of the language grounding problem, where the goal is to extract representations of the meanings of natural language tied to the physical world. We present an approach for joint learning of language and perception models for grounded attribute induction. The perception model includes classifiers for physical characteristics and a language model based on a probabilistic categorial grammar that enables the construction of compositional meaning representations. We evaluate on the task of interpreting sentences that describe sets of objects in a physical workspace, and demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes.", "creator": "LaTeX with hyperref package"}}}