{"id": "1206.6425", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Sparse stochastic inference for latent Dirichlet allocation", "abstract": "We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (686kb)", "http://arxiv.org/abs/1206.6425v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["david m mimno", "matthew d hoffman", "david m blei"], "accepted": true, "id": "1206.6425"}, "pdf": {"name": "1206.6425.pdf", "metadata": {"source": "META", "title": "Sparse stochastic inference for latent Dirichlet allocation", "authors": ["David Mimno", "Matthew D. Hoffman", "David M. Blei"], "emails": ["mimno@cs.princeton.edu", "mdhoffma@cs.princeton.edu", "blei@cs.princeton.edu"], "sections": [{"heading": "1. Introduction", "text": "You can uncover the most important topics that permeate a corpus and then use these topics to organize, search and explore the documents. It follows that the conditional distribution of these variables is the central computerized problem. In this paper, we develop a method that links the topics and the proportions of the documents."}, {"heading": "2. Hybrid stochastic-MCMC inference", "text": "We model each of the D documents in a corpus as a mixture of K-themes. This theme model can be divided into global variables at the corpus level and local variables at the document level. The global variables are K-theme word distributions \u03b21,... \u03b2K via the V-dimensional vocabulary, each preceded by a dirichlet with parameters. For a document d of length Nd, the local variables (a) are a distribution over topics derived from a difunnel preceded by parameters \u03b1 and (b) Nd tokentopic indicator variables zd1,... zdNd drawn by length Nd. Our goal is to estimate the posterior distribution of the hidden variables in light of an observed corpus. We will use variational inference, which is variable in contrast to the standard center-field thought variations, but similar to Griffiths & Steyvers (2004) and Teh et al. (2006), we will exclude the topic proportions."}, {"heading": "2.1. Online stochastic inference", "text": "We optimize the variation parameters of the topic - word parameters - by using stochastic gradient ascension data. Stochastic gradient ascension data iteratively update parameters with noisy estimates of the gradient. We obtain these noisy estimates by subsampling the data (Sato, 2001; Hoffman et al., 2010). We can then generate a noisy approximation of this full gradient by first scanning a minibatch of documents B and then scaling the sum of document-specific gradients to match the overall size of the corpus."}, {"heading": "2.2. MCMC within stochastic inference", "text": "We cannot evaluate the expectation in Eq.8 because we would have to consider a combinatorial number of topic configurations. However, to use the stochastic increase in the gradient, we only need to approximate this expectation. We use the Markov Monte Carlo chain to sample topic configurations from q? (zd).We then use the empirical average of these samples to estimate the expectations for Equation 8.Gibbs sampling for a document that begins with a random initialization of the topic indicator variables zd. We then use iteratively the topic indicator at each position from the conditional distribution over the remaining topic indicator variables: q? (zdi = k | z\\ i)."}, {"heading": "2.3. Algorithm", "text": "For a sequence of learning rates \u03c1t = (t0 + t) \u2212 \u0445 the following update leads to a stationary point: \u03bbtkw \u2190 \u03bbt \u2212 1kw + \u03c1t D | B | \u2211 d \u00b2 B (N = dkw + 1 D (\u03b7 \u2212 \u03bbkw)) = (1 \u2212 \u03c1t) \u03bbt \u2212 1kw + \u03c1t (\u03b7 + D | B | \u2211 D \u00b2 B N \u00b2 dkw). (11) This update leads to algorithm 1. Two implementation details leading to sparse calculations can be found in Appendix A. Compared to the online LDA, this online algorithm has the important advantage that the sparseness of the topic-word parameters is maintained, so that it applies to most values of k and w. The sparseness increases the efficiency of updates to Squk and of Gibbs samples for zd. Previous variation methods lead to topic-word parameters being easily updated by means of KV-parameters."}, {"heading": "3. Related Work", "text": "The stochastic approximation EM (SAEM, Delyon et al., 1999) combines an EM algorithm with a stochastic online inference method. SAEM does not subsample data, but interpolates between the Monte Carlo estimates of the complete data. Kuhn & Lavielle (2004) extend SAEM to the use of MCMC estimates. Similarly, online EM (Cappe's & Moulines, 2009) subsamples are evaluated, but the standard inference methods for local variables are retained."}, {"heading": "4. Empirical Results", "text": "In this section, we compare the sampled online algorithm with two related online methods and measure the effect of model parameters. We use a selection of metrics to evaluate models."}, {"heading": "4.1. Evaluation", "text": "A model that characterizes the semantic structure of a corpus should place at least a greater part of its probability mass on reasonable documents than on random word sequences. We can use this assumption to compare different models by asking each model to estimate the probability of a previously unseen document. A better model, on average, should apply a higher probability to real documents than a less qualitative model. We evaluate the held probability using the left-to-right sequential scanning method (Wallach et al., 2009; Buntine, 2009). For each trained model, we generate point estimates of topic word probabilities p (w | k). We then process each document by iteration using the tokens w1,..., wNd. At each position i we calculate the borderword probability p (wi < i) = borderword probability p (zi = k < w < i, \u03b1) p wi (k)."}, {"heading": "4.2. Comparison to Online VB", "text": "Our first group consists of 350,000 research articles consisting of three main journals. \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4.3. Comparison to Sequential Monte Carlo", "text": "A Gibbs sampler starts with a random initialization for all hidden variables and returns repeatedly over the entire data set, updating each variable against the current value of all other variables. SMC sampling for hidden variables in sequential order, conditioning only to previously seen variables. It is common to maintain multiple sampling states or \"particles,\" but this process adds both computation and significant accounting complexity. Ahmed et al. (2012) use a single SMC status. To compare SMC with the sampled online algorithm, we have run 10 independent SMC samplers over the Science / Nature / PNAS data set, sorting these documents randomly. We also have 10 independent Sampled trainers that stop after sampling a number of documents corresponding to the size of the corpus."}, {"heading": "4.4. Effect of parameter settings", "text": "It is the first, we, in the first, third, fourth, fourth, fourth, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, sixth, sixth, sixth, sixth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth"}, {"heading": "4.5. Scalability", "text": "To demonstrate the scalability of the method, we modeled a collection of 1.2 million non-copyrighted books. Previous approaches of digital libraries have focused on keyword searches and word frequency histograms (Michel et al., 2011). Such methods do not take into account variability in meaning or context. There is no guarantee that the words to be counted will match the meaning assumed by the user. In contrast, an interface based on a theme model could distinguish, for example, the use of the word \"strain\" in immunology, mechanical engineering, and cookery. We divide each book into 10-page sections, resulting in 44 million \"documents\" with a word size of 216. We trained models with K-models {100, 500, 1000, 2000}."}, {"heading": "5. Conclusions", "text": "The algorithm presented in this paper combines the advantages of these two methods, and as a result, models can be trained on huge open corpora without requiring access to huge computer clusters. If parallel architectures are available, we can trivially parallelise the calculation within each minibatch. As this work is related to the online LDA algorithm by Hoffman et al. (2010), extensions of this model are also applicable, such as adaptive scheduling algorithms (Wahabzada & Kersting, 2011). Using MCMC within stochastic variation inferences reduces a source of bias in estimating local variables. Although we have focused on text analysis applications, this hybrid method generalizes to a broad class of Bayesian models."}, {"heading": "Acknowledgments", "text": "John Langford, Iain Murray, Charles Sutton made helpful comments. Yahoo! and PICSciE provided computing resources. DM is supported by a CRA CI Fellowship. MDH is supported by NSF ATM0934516, DOE DE-SC0002099 and IES R305D100017. DMB is supported by ONR N00014-11-1-0651, NSF CAREER 0745520, AFOSR FA9550-09-1-0668, the Alfred P. Sloan Foundation and a grant from Google."}, {"heading": "A. Sparse computation", "text": "The smoothing parameters can be taken into account \u2212 n \u2212 \u2212 \u2212 n from Equation 11, as long as we assume that all initial values are quantitatively different from O (k).The reordering of this equation to separate the Dirichlet hyperparameter is quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative"}], "references": [{"title": "Scalable inference in latent variable models", "author": ["Ahmed", "Amr", "Aly", "Mohamed", "Gonzalez", "Joseph", "Narayanamurthy", "Shravan", "Smola", "Alexander"], "venue": "In WSDM,", "citeRegEx": "Ahmed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2012}, {"title": "Asynchronous distributed learning of topic models", "author": ["Asuncion", "Arthur", "Smyth", "Padhraic", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Asuncion et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2008}, {"title": "Pattern Recognition and Machine Learning", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["Blei", "David", "Ng", "Andrew", "Jordan", "Michael"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Estimating likelihoods for topic models", "author": ["Buntine", "Wray L"], "venue": "In Asian Conference on Machine Learning,", "citeRegEx": "Buntine and L.,? \\Q2009\\E", "shortCiteRegEx": "Buntine and L.", "year": 2009}, {"title": "Online EM algorithm for latent data models", "author": ["Capp\u00e9", "Olivier", "Moulines", "Eric"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Capp\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Capp\u00e9 et al\\.", "year": 2009}, {"title": "Convergence of a stochastic approximation version of the EM algorithm", "author": ["Delyon", "Bernard", "Lavielle", "Marc", "Moulines", "Eric"], "venue": "Annals of Statistics,", "citeRegEx": "Delyon et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Delyon et al\\.", "year": 1999}, {"title": "Finding scientific topics", "author": ["Griffiths", "Thomas L", "Steyvers", "Mark"], "venue": "PNAS, 101(suppl", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Online learning for latent dirichlet allocation", "author": ["Hoffman", "Matthew", "Blei", "David", "Bach", "Francis"], "venue": "In NIPS,", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Introduction to variational methods for graphical models", "author": ["Jordan", "Michael", "Ghahramani", "Zoubin", "Jaakkola", "Tommi", "Saul", "Laurence"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Coupling a stochastic approximation version of EM with an MCMC procedure", "author": ["Kuhn", "Estelle", "Lavielle", "Marc"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "Kuhn et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 2004}, {"title": "Optimizing semantic coherence in topic models", "author": ["Mimno", "David", "Wallach", "Hanna", "Talley", "Edmund", "Leenders", "Miriam", "McCallum", "Andrew"], "venue": "In EMNLP,", "citeRegEx": "Mimno et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "Automatic evaluation of topic coherence", "author": ["Newman", "David", "Lau", "Jey Han", "Grieser", "Karl", "Baldwin", "Timothy"], "venue": "In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Newman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2010}, {"title": "Online model selection based on the variational Bayes", "author": ["M.A. Sato"], "venue": "Neural Computation,", "citeRegEx": "Sato,? \\Q2001\\E", "shortCiteRegEx": "Sato", "year": 2001}, {"title": "A collapsed variational bayesian inference algorithm for latent dirichlet allocation", "author": ["Teh", "Yee-Whye", "Newman", "David", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Larger residuals, less work: Active document scheduling for latent Dirichlet allocation", "author": ["Wahabzada", "Mirwaes", "Kersting", "Kristian"], "venue": "In ECML/PKDD,", "citeRegEx": "Wahabzada et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wahabzada et al\\.", "year": 2011}, {"title": "Evaluation methods for topic models", "author": ["Wallach", "Hanna", "Murray", "Iain", "Salakhutdinov", "Ruslan", "Mimno", "David"], "venue": "In ICML,", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Topic models are hierarchical Bayesian models of document collections (Blei et al., 2003).", "startOffset": 70, "endOffset": 89}, {"referenceID": 9, "context": "Our algorithm builds on variational inference (Jordan et al., 1999).", "startOffset": 46, "endOffset": 67}, {"referenceID": 8, "context": "Recently, Hoffman et al. (2010) introduced Online LDA, a stochastic gradient optimization algorithm for topic modeling.", "startOffset": 10, "endOffset": 32}, {"referenceID": 14, "context": "Unlike standard mean-field variational inference, but similar to Griffiths & Steyvers (2004) and Teh et al. (2006), we will marginalize out the topic proportions \u03b8d.", "startOffset": 97, "endOffset": 115}, {"referenceID": 13, "context": "We obtain these noisy estimates by subsampling the data (Sato, 2001; Hoffman et al., 2010).", "startOffset": 56, "endOffset": 90}, {"referenceID": 8, "context": "We obtain these noisy estimates by subsampling the data (Sato, 2001; Hoffman et al., 2010).", "startOffset": 56, "endOffset": 90}, {"referenceID": 13, "context": "Premultiplying the gradient of an objective function by the inverse Fisher information matrix of the distribution being optimized (in our case the variational distribution q) results in the natural gradient (Sato, 2001).", "startOffset": 207, "endOffset": 219}, {"referenceID": 8, "context": "8 (Hoffman et al., 2010).", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": "Stochastic approximation EM (SAEM, Delyon et al., 1999) combines an EM algorithm with a stochastic online inference procedure. SAEM does not subsample data, but rather interpolates between Monte Carlo estimates of the complete data. Kuhn & Lavielle (2004) extend SAEM to use MCMC estimates.", "startOffset": 35, "endOffset": 256}, {"referenceID": 1, "context": "As a result, topic assignment variables must in theory be sampled sequentially, although parallel approximations work well empirically (Asuncion et al., 2008).", "startOffset": 135, "endOffset": 158}, {"referenceID": 14, "context": "Collapsed variational inference (Teh et al., 2006) also analytically marginalizes over the topic proportions, but still maintains a fully factorized distribution over topic assignments at each position.", "startOffset": 32, "endOffset": 50}, {"referenceID": 16, "context": "We evaluate held-out probability using the left-to-right sequential sampling method (Wallach et al., 2009; Buntine, 2009).", "startOffset": 84, "endOffset": 121}, {"referenceID": 11, "context": "This metric measures the semantic quality of a topic by approximating the experience of a user viewing the W most probable words for the topic (Mimno et al., 2011).", "startOffset": 143, "endOffset": 163}, {"referenceID": 12, "context": "It is related to point-wise mutual information (Newman et al., 2010).", "startOffset": 47, "endOffset": 68}, {"referenceID": 8, "context": "Comparison of seconds per mini-batch between online variational Bayes (Hoffman et al., 2010) and sampled online inference (this paper).", "startOffset": 70, "endOffset": 92}, {"referenceID": 0, "context": "Sequential Monte Carlo is an online algorithm similar to Gibbs sampling in that it represents topics using sums over assignment variables (Ahmed et al., 2012).", "startOffset": 138, "endOffset": 158}, {"referenceID": 0, "context": "Sequential Monte Carlo is an online algorithm similar to Gibbs sampling in that it represents topics using sums over assignment variables (Ahmed et al., 2012). A Gibbs sampler starts with a random initialization for all hidden variables and sweeps repeatedly over the entire data set, updating each variable given the current value of all other variables. SMC samples values for hidden variables in sequential order, conditioning only on previously-seen variables. It is common to keep multiple sampling states or \u201cparticles\u201d, but this process adds both computation and significant bookkeeping complexity. Ahmed et al. (2012) use a single SMC state.", "startOffset": 139, "endOffset": 626}, {"referenceID": 8, "context": "As this work is related to the Online LDA algorithm of Hoffman et al. (2010), extensions to that model are also applicable, such as adaptive scheduling algorithms (Wahabzada & Kersting, 2011).", "startOffset": 55, "endOffset": 77}], "year": 2012, "abstractText": "We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.", "creator": "LaTeX with hyperref package"}}}