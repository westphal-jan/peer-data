{"id": "1206.6426", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A fast and simple algorithm for training neural probabilistic language models", "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (325kb)", "http://arxiv.org/abs/1206.6426v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["andriy mnih", "yee whye teh"], "accepted": true, "id": "1206.6426"}, "pdf": {"name": "1206.6426.pdf", "metadata": {"source": "META", "title": "A fast and simple algorithm for training neural probabilistic language models", "authors": ["Andriy Mnih", "Yee Whye Teh"], "emails": ["amnih@gatsby.ucl.ac.uk", "ywteh@gatsby.ucl.ac.uk"], "sections": [{"heading": null, "text": "Despite their superior performance, Neural Probabilistic Language Models (NPLMs) are far less commonly used than n-gram models due to their notoriously long training times, which are measured in weeks even for medium-sized datasets. NPLMs are mathematically expensive to train because they are explicitly normalized, which means that all words of the vocabulary must be taken into account when calculating the log likelihood history. We propose a fast and simple algorithm for training NPLMs based on noise-contrasting estimation, a newly introduced method for estimating unnormalized continuous distributions. We study the behavior of the algorithm on the Penn Treebank corpus and show that it reduces training times by more than one order of magnitude without compromising the quality of the resulting models. The algorithm is also more efficient and much more stable than importance samples because it requires far fewer vocabulary samples to demonstrate good performance of the word proposed in the 80K language set models."}, {"heading": "1. Introduction", "text": "By assigning probabilities to sentences, language models make it possible to distinguish between probable and unlikely sentences, making such models an important component of speech recognition, machine translation, and information gathering systems. Probabilistic language models are typically based on the Markov assumption, which means that they model the conditional distribution of the next word in a sentence based on a certain number of words that precede it. The group of words that rely on the context is known as h, while the word predicted as the target word is called w. n-gram models, which include effectively smoothed tables of standardized words / contexts, have dominated language modeling for decades because of their simplicity and excellence. In recent years, neural language models have become competitive with n-grams and routinely perform it better than them. (Mikolov et al, 2011) PLMs model the distribution of the next word as the next PLM."}, {"heading": "2. Neural probabilistic language models", "text": "A statistical language model is simply a collection of conditional distributions for the next word, indexed by its context. 1 In a neural language model, the conditional distribution corresponding to the context h, Ph (w), is defined as Ph\u03b8 (w) = exp (s\u03b8 (w, h)).The negated scoring function is sometimes referred to as an energy function (Bengio et al., 2000).Depending on the form of s\u03b8 (w, h), Equation 1 can describe both neuronal and maximum entropy language models (Berger et al., 1996).The main difference between these two model classes lies in the energy function (Bengio et al., 2000)."}, {"heading": "2.1. Log-bilinear model", "text": "The training method we propose is directly applicable to all neural probability and entropy models, which is the simplest neural language model. The LBL model performs linear predictions in the semantic word attribute space and does not exhibit non-linearities. Despite its simplicity, the LBL model has been shown to exceed n-grams, although the more complex neural language models (Mikolov et al., 2010; Mnih et al., 2009) may exceed it. In this paper, we use a slightly expanded version of the LBL model, which uses separate attribute tables for context words and target words. Thus, a context word w is represented with attribute vector rw, while a target word w is represented with attribute vector qw. Considering a context h, the model calculates the predicted representation of the word qriq for the targets and target words."}, {"heading": "2.2. Maximum likelihood learning", "text": "The contribution of a single context / word observation to the gradient of the log probability is derived from the calculation of the log probability gradient (w, h); the contribution of a single context / word observation to the log probability gradient (w, h) is derived from the calculation of the log probability gradient (w, h); the contribution of a single context / word observation to the log probability gradient is derived from the calculation of the log probability (w, h); the expectation of a doctorate (w, h) is expensive to evaluate because all words in the vocabulary require the calculation (w, h); since vocabularies typically contain tens of thousands of words, learning the maximum probability tends to be very slow."}, {"heading": "2.3. Importance sampling", "text": "Bengio and Sene \u0301 cal (2003) have proposed a method to accelerate the training of neural language models based on the approximation of expectation in Equation 4 using meaning samples. The idea is to generate k samples x1,..., xk from an easy-to-sample distribution Qh (w) and the gradients using v (x) = exp (s\u03b8 (x, h)) Qh (w = x) - 1V k = 1 v (xj). Normalization by V is necessary here because the weights of importance v are calculated using the unnormalized model distribution (x) = exp (s\u03b8 (x, h)) Qh (w = x). Typically, the proposal is an n-gram model that adapts to the training set, possibly with a different context size than the neural model."}, {"heading": "3. Noise-contrastive estimation", "text": "We propose to use a context-independent model (unigram) Pn (w) as a more stable alternative to sampling for the efficient training of neural language models and other models defined by Eq. 1. NCE was recently introduced by Gutmann and Hyva \ufffd rinen (2010) for non-normalized probability models. Although it was developed for estimating probability densities, we are interested in applying it to discrete distributions and will therefore use discrete distributions and usage probability functions instead of density functions. NCE's basic idea is to reduce the problem of density estimation to that of binary classification, which distinguishes between samples from the data distribution and samples from a known noise distribution. In the language modeling setting, the distribution of words occurring after a certain context will be reduced. Although it is possible to use context-dependent noise distributions, we will use a context-dependent noise distribution."}, {"heading": "3.1. Dealing with normalizing constants", "text": "In our first implementation of the NCE training, we learned a (log-) normalizing constant (c in Eq.8) for each context in the training set and stored it in a hash table indexed by the context. 3 Although this approach works well for small datasets, it requires estimating one parameter per context, making it difficult to scale to a large number of observed contexts encountered by models with large context sizes. Surprisingly, we found that setting the normalization constants to 1.4, rather than learning them, does not affect the performance of the resulting models."}, {"heading": "3.2. Potential speedup", "text": "Suppose c is the context size, d is the word attribute vector dimensionality, and v is the vocabulary size of the model. Then, calculating the predicted representation with Eq. 2 requires about cd2 operations for both NCE and ML. For ML, calculating the distribution of the next word from the predicted representation requires about V d operations. For NCE, evaluating the probability of k noise samples within the model requires about kd operations. Since the gradient calculation in each model has the same complexity as calculating the probabilities, the acceleration has for3We have not used the learned normalization constants in calculating the validation and test set perplexity. Instead, we explicitly normalize the probabilities in each model. 4This boils down to having the normalization parameters c on 0.each parameter update of the vocabulary defined by the use of the vocabulary."}, {"heading": "4. Penn Treebank results", "text": "We empirically examined the properties of the proposed algorithm on the Penn Treebank Corpus. As is common practice, we examined sections 0-20 (930Kwords) and used sections 21-22 (74k words) as a validation set and sections 23-24 (82k words) as a test set. The standard vocabulary of the most common words was replaced with the remaining words by a special token. We decided to use this dataset to keep the training time reasonable for an exact maximum probability of learning. Learning rates were adjusted at the end of each epoch, based on the change in validation that perplexity has set since the end of the previous epoch. Rates were halved as if the perplexicity increased and was left unchanged. Parameters were updated based on mini batches of 1000 context / word pairs that were each trained."}, {"heading": "5. Sentence Completion Challenge", "text": "To demonstrate the scalability and effectiveness of our approach, we used several large neural language models for the Microsoft Research Sentence Completion Challenge (Zweig & Burges, 2011), which was designed as a benchmark for semantic models and consists of SAT-style sentence complexity problems. Given 1040 sentences, each missing a word, the task is to select the right word for each sentence, so that all complexities are grammatically correct, but the correct answer was unequivocal. Although people can achieve over 90% accuracy on the task, statistical models are much worse produced with the best result of 49% than a full set of LSA models, and n-gram models only achieve 39% accuracy (Zweig & Burges, 2011)."}, {"heading": "6. Discussion", "text": "Our results show that the learning algorithm is very stable and can produce models as powerful as those that have been trained with maximum probability in less than a tenth of the time. In a large-scale test of the approach, we trained several neural speech models based on a collection of Gutenberg texts, achieving state-of-the-art performance on the Microsoft Research Sentence Completion Challenge datasets. Although we have shown that the unigrammatic sound distribution is sufficient to train neural speech models efficiently, context-dependent sound distributions are worth investigating because they could lead to even faster training by reducing the number of sound samples required. Recently, Pihlaja et al. (2010) introduced a family of estimates for non-normalized models that include NCE and meaning distributions as special cases. Other members of this family may be of interest for the training of language models, although our preliminary results suggest that none of them may perform better than neural models we believe. Finally, many of these models may be applied to CE models."}, {"heading": "Acknowledgments", "text": "We thank Vinayak Rao and Lloyd Elliot for their helpful comments and the Gatsby Charitable Foundation for their generous funding."}], "references": [{"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Bengio", "Yoshua", "Sen\u00e9cal", "Jean-S\u00e9bastien"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2000}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "Rejean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "V.J.D. Pietra", "S.A.D. Pietra"], "venue": "Computational linguistics,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "In Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS2010),", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? \\Q2010\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2010}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? \\Q2012\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2012}, {"title": "A probabilistic model for semantic word vectors", "author": ["A.L. Maas", "A.Y. Ng"], "venue": "In NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Maas and Ng,? \\Q2010\\E", "shortCiteRegEx": "Maas and Ng", "year": 2010}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "In Eleventh Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Mnih and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Improving a statistical language model through non-linear prediction", "author": ["A. Mnih", "Z. Yuecheng", "G. Hinton"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In AISTATS\u201905,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "A family of computationally efficient and simple estimators for unnormalized statistical models", "author": ["M. Pihlaja", "M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence (UAI2010).,", "citeRegEx": "Pihlaja et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pihlaja et al\\.", "year": 2010}, {"title": "Training neural network language models on very large corpora", "author": ["Schwenk", "Holger", "Gauvain", "Jean-Luc"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schwenk et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2005}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "SRILM \u2013 an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "The Microsoft Research Sentence Completion Challenge", "author": ["G. Zweig", "C.J.C. Burges"], "venue": "Technical Report MSR-TR-2011-129, Microsoft Research,", "citeRegEx": "Zweig and Burges,? \\Q2011\\E", "shortCiteRegEx": "Zweig and Burges", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "In the last few years neural language models have become competitive with n-grams and now routinely outperform them (Mikolov et al., 2011).", "startOffset": 116, "endOffset": 138}, {"referenceID": 18, "context": "Word representations learned by language models are also used for natural language processing applications such as semantic role labelling (Collobert & Weston, 2008), sentiment analysis (Maas & Ng, 2010), named entity recognition (Turian et al., 2010), and parsing (Socher et al.", "startOffset": 230, "endOffset": 251}, {"referenceID": 16, "context": ", 2010), and parsing (Socher et al., 2011).", "startOffset": 21, "endOffset": 42}, {"referenceID": 1, "context": "The negated scoring function is sometimes referred to as the energy function (Bengio et al., 2000).", "startOffset": 77, "endOffset": 98}, {"referenceID": 3, "context": "1 can describe both neural and maximum entropy language models (Berger et al., 1996).", "startOffset": 63, "endOffset": 84}, {"referenceID": 1, "context": "In some models, different feature vector tables are used for the context and the next word vocabularies (Bengio et al., 2000), while in others the table is shared (Bengio et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 2, "context": ", 2000), while in others the table is shared (Bengio et al., 2003; Mnih & Hinton, 2007).", "startOffset": 45, "endOffset": 87}, {"referenceID": 8, "context": "In spite of its simplicity, the LBL model has been shown to outperform n-grams, though the more complex neural language models (Mikolov et al., 2010; Mnih et al., 2009) can outperform it.", "startOffset": 127, "endOffset": 168}, {"referenceID": 11, "context": "In spite of its simplicity, the LBL model has been shown to outperform n-grams, though the more complex neural language models (Mikolov et al., 2010; Mnih et al., 2009) can outperform it.", "startOffset": 127, "endOffset": 168}, {"referenceID": 5, "context": "NCE has recently been introduced by Gutmann and Hyv\u00e4rinen (2010) for training unnormalized probabilistic models.", "startOffset": 36, "endOffset": 65}, {"referenceID": 5, "context": "Following Gutmann and Hyv\u00e4rinen (2012), we assume that noise samples are k times more frequent than data samples, so that datapoints come from the mixture 1 k+1P h d (w)+ k k+1Pn(w).", "startOffset": 10, "endOffset": 39}, {"referenceID": 17, "context": "As a baseline for comparison, we also trained several ngram models (with modified Kneser-Ney smoothing) using the SRILM toolkit (Stolcke, 2002), obtaining results similar to those reported by Zweig & Burges (2011).", "startOffset": 128, "endOffset": 143}, {"referenceID": 17, "context": "As a baseline for comparison, we also trained several ngram models (with modified Kneser-Ney smoothing) using the SRILM toolkit (Stolcke, 2002), obtaining results similar to those reported by Zweig & Burges (2011).", "startOffset": 129, "endOffset": 214}, {"referenceID": 14, "context": "Recently, Pihlaja et al. (2010) introduced a family of estimation methods for unnormalized models that includes NCE and importance sampling as special cases.", "startOffset": 10, "endOffset": 32}], "year": 2012, "abstractText": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-ofthe-art results on the Microsoft Research Sentence Completion Challenge dataset. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "LaTeX with hyperref package"}}}