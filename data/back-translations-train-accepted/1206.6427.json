{"id": "1206.6427", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients", "abstract": "The speed of convergence of the Expectation Maximization (EM) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components. In this paper, we study the impact of mixing coefficients on the convergence of EM. We show that when the mixture components exhibit some overlap, the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases. We propose a deterministic anti-annealing algorithm, that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients. The proposed algorithm is compared against other standard optimization techniques like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we propose a similar deterministic anti-annealing based algorithm for the Dirichlet process mixture model and demonstrate its advantages over the conventional variational Bayesian approach.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (847kb)", "http://arxiv.org/abs/1206.6427v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["iftekhar naim", "daniel gildea"], "accepted": true, "id": "1206.6427"}, "pdf": {"name": "1206.6427.pdf", "metadata": {"source": "META", "title": "Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients", "authors": ["Iftekhar Naim", "Daniel Gildea"], "emails": ["inaim@cs.rochester.edu", "gildea@cs.rochester.edu"], "sections": [{"heading": "1. Introduction", "text": "In many of these applications (for example, biological data analysis, anomaly detection, image segmentation, etc.) the goal is to identify rare groups or small clusters (in terms of number of members) in the presence of others appearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. Copyright 2012 by author (s) / owner (s). Larger clusters (in terms of number of members) focus on the particular problem of clustering large datasets with high dynamic range in cluster sises.The Gaussian mixing model is a powerful model for data clusters.We model the data as a mixture of multiple Gaussian distributions in which each Gaussian component corresponds to a cluster. Leave X = 2, components."}, {"heading": "2. Convergence of EM Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Related Work", "text": "The EM algorithm is guaranteed to converge monotonously to the local optimum under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Speakers and Walker (1984) demonstrate that EM has a linear convergence rate and suggest that Newton or quasi-Newton methods should be preferred to the EM algorithm. They also demonstrate experimentally that EM convergence converges slowly at the presence of overlapping clusters. Xu and Jordan (1996) analyze the convergence rate of the EM algorithm and show that EM has a superlinear convergence rate because the overlap between the mixture components is zero (Xu & Jordan, 1996). They form a link between the EM algorithm and the gradient ascent and demonstrate that the convergence rate of the EM algorithm is dependent on the conditional number of a projected Hessional matrix component, the Hessional component of the component H."}, {"heading": "2.2. Convergence of EM for Small Clusters", "text": "We start with 3 simulation examples. First, we generate synthetic data with a 2-component univariate Gaussian mixture with the parameters \u03b1 = (0.025, 0.975) T, (0.025, 0.92) T, (0.059, 0.9410) T, (6.25, 6.25) (as shown in Figure 1 (a). We perform a 2-component EM fit and after 100 iterations we get a parameter estimate of \u03b1 = (0.059, 0.9410) T, (4.1, \u00b5 2) = (\u2212 0.497, 5.08), (V1, V2) = (23.15, 5.92). In this simple setting, the inaccurate result is due to the slow rate of convergence. If we allow EM to run for many more iterations, it usually converges with the true parameters."}, {"heading": "2.3. Why Slow Convergence?", "text": "Next, we investigate the reason why EM shows slow convergence in the presence of small clusters. We explain it using the framework proposed by Xu and Jordan (1996). Xu and Jordan (and later Ma et al. (2000) have shown that the convergence rate of EM is at the upper limit of the normal range, with the log probability interface being spherical and allowing for rapid convergence. On the other hand, a higher value of the conditional number of the effective Hessian probability function implies slow convergence for each linear convergence method. Our simulation results show that in the presence of some overlaps, the conditional number of the Hessian matrix as a slow log probability function can cause slow convergence for each linear convergence method."}, {"heading": "3. Proposed Solution", "text": "In this section, we propose a variant of Deterministic Annealing EM (Ueda & Nakano, 1998) to address the above problem of the rate of convergence and to compare the result with two other known optimization techniques: Expectation Conjugate Gradient (ECG) (Salakhutdinov et al., 2003) and QuasiNewton Method (BFGS) (Liu & Nocedal, 1989)."}, {"heading": "3.1. Deterministic Anti-Annealing Expectation-Maximization", "text": "We have used the following equations: hj (t) = \u03b1jP (xt). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x. (x.). (x. (x.). (x. (x.). (x.). (x. (x.). (x.). (x. (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.). (x.).). (x.).). (x.). (x.). (x.).). (x.). (x.).). (x.). (x.).). (x.).).).). (x. (x.). (x.).). (x.). (x.).). (x.).). (x.). (x.).). (x.). (x.).). (x.).). (x.).). (x. (x. (x.). (x.).).). (x. (x.).). (x.). (x.). (x.).). (x. (x.).).).). (x.). (x.).). (x. (x.)."}, {"heading": "3.2. Expectation Conjugate Gradient (ECG)", "text": "Salakhutdinov et al. (2003) proposed a method for optimising the log probability functions in the case of highly overlapping clusters. To ensure that the ECG algorithm meets the same constraints, Salakhutdinov et al. (2003) propose to re-parameterise the model parameters using parameters: \u03b1i \u2265 0, \u2211 i \u03b1i = 1, and \u03a3 0. To ensure that the ECG algorithm meets the same constraints, Salakhutdinov et al. (2003) propose to repair the model parameters: \u03b1j = e \u03bbj Pl e\u03b2i = 1, and \u04450."}, {"heading": "3.3. Quasi-Newton Method: BFGS", "text": "Quasi-Newton methods (e.g. BFGS, LBFGS, etc.) approach the Hessian on the basis of gradient values and generally converge faster than first order gradient methods. However, these methods (without line search) lack the convergence guarantee of EM and require a line search that introduces additional calculations. We implement BFGS with the same gradient functions as for the conjugate gradient and the Matlab Optimization Toolbox and compare with our algorithm."}, {"heading": "4. Extension to Dirichlet Process Mixture Model", "text": "Unlike traditional mixing models, DPMM does not assume a fixed number of density components and allows model complexity to grow as more data points become visible. We have expanded the variational Dirichlet process mixing model (Lead & Jordan, 2006) using Deterministic Anti-Annealing. Bayesian variable DPMM is based on the abbreviated stifling representation. Let X represent the random variable following the Dirichlet process mixing model and let W be the latent variables W = {V, \u03b7, Z} representing the floor lengths, individual mixing component parameters and cluster memberships. Let q (v, \u03b7, z) be the factored variation distribution (z) and T be the truncation parameter. Subordinate responsibility of a data point for a mixing component j = 1 (PMj) is expressed by a protocol (Sq = 1)."}, {"heading": "5. Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "We are experimenting with three different datasets.1 The first dataset consists of samples taken from a two-component mixture of Gaussians, with cluster sizes of 200k and 200 data points respectively (Figure 7 (a)). We deliberately set the averages and covariances so that there is overlap between the mixture components; the second dataset also consists of a synthetic mixture of 4 Gaussians (Figure 7 (b)), each with 150k, 100k, 50k and 150 data points. Finally, we are experimenting with the handwritten MNIST dataset, which consists of high-dimensional images of handwritten digits (0-9). We randomly selected 5000 images of the handwritten digits \"4\" and 250 images of the handwritten digit \"8.\" We then combine these samples and reduce the dimensionality of the combined dataset to two dimensions using PCA. We select these two particular digits based on their beautiful elliptical form of the D2 (Figure 2D) component."}, {"heading": "5.2. Experimental Results", "text": "We observe the convergence behavior of all four different algorithms: EM, Deterministic Anti-Annealing = 1,2 and the second stage. We observe the convergence behavior of all four different algorithms: EM, Deterministic Anti-Annealing, BFGS and ECG, on all three data sets described above. Each algorithm terminates when it meets the stop criterion, and is the tolerance parameter. For all algorithms except Determinisitc Anti-Annealing, we set the tolerance variables to 10 \u2212 10. Deterministic Anti-Annealing, we set the tolerance parameters to 10 \u2212 6. Since Anti-Annealing is able to accelerate convergence at the later stages, we do not need conservative tolerance."}, {"heading": "5.3. Discussion", "text": "The experimental results show that both the deterministic anti-annealing method and the BFGS significantly improve the rate of convergence compared to conventional EM. For the minimal error scenario, the BFGS exceeds the deterministic anti-annealing method by a small margin. However, the deterministic anti-annealing method is on average more stable and exhibits better convergence behavior. Both the BFGS and the ECG exhibit significant variability, especially for the second (4 Gaussian) dataset. They often end in a poorly degenerated local optima, where one or more of the mixing coefficients are jammed to zero. Therefore, the deterministic anti-annealing method exceeds the BFGS on average. Salakhutdinov et al. (2003) suggested a hybrid EMEKG algorithm that entropy of cluster memberships as a measure of missing information (in other words: overlapping the cluster, although it is sufficient to apply the entropy) is not sufficient."}, {"heading": "6. Conclusion", "text": "The proposed Deterministic Anti-Annealing Scheme has much potential for faster convergence of data sets with smaller clusters. It offers several key advantages such as: 1) more robust global convergence, 2) faster convergence for small clusters through anti-annealing, 3) simple implementation, no line search required, 4) parameter constraints are met without requiring repair measurement. Our experimental results show that deterministic Anti-Annealing EM outperforms all other three algorithms on average, both in terms of speed and accuracy of convergence. However, the temperature planning of the Deterministic Annealing often requires some coordination (Rangarajan, 2000). A thorough examination of the temperature schedule can be an interesting future direction. As a general guideline, the timetable should be based on the complexity of the data. The more complex the data, the slower we should vary the temperature parameters."}, {"heading": "7. Acknowledgement", "text": "We would like to thank Daniel S'tefankovic, Gaurav Sharma and Suprakash Datta for many useful comments and feedback, partly funded by the NSF Prize IIS-0910611."}], "references": [{"title": "Variational inference for dirichlet process mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan,? \\Q2006\\E", "shortCiteRegEx": "Blei and Jordan", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "Jain et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1999}, {"title": "Deterministic annealing variant of variational bayes method", "author": ["K. Katahira", "K. Watanabe", "M. Okada"], "venue": "In Journal of Physics: Conference Series,", "citeRegEx": "Katahira et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katahira et al\\.", "year": 2008}, {"title": "An informationtheoretic analysis of hard and soft assignment methods for clustering", "author": ["M.J. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "Nato ASI Series, Series D: Behavioural and Social Sciences,", "citeRegEx": "Kearns et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1998}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming,", "citeRegEx": "Liu and Nocedal,? \\Q1989\\E", "shortCiteRegEx": "Liu and Nocedal", "year": 1989}, {"title": "Asymptotic convergence rate of the EM algorithm for Gaussian mixtures", "author": ["J. Ma", "L. Xu", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Ma et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2000}, {"title": "Finite Mixture Models", "author": ["G. McLachlan", "D. Peel"], "venue": "New York: Wiley Interscience,", "citeRegEx": "McLachlan and Peel,? \\Q2000\\E", "shortCiteRegEx": "McLachlan and Peel", "year": 2000}, {"title": "Self-annealing and self-annihilation: unifying deterministic annealing and relaxation labeling", "author": ["A. Rangarajan"], "venue": "Pattern Recognition,", "citeRegEx": "Rangarajan,? \\Q2000\\E", "shortCiteRegEx": "Rangarajan", "year": 2000}, {"title": "The infinite Gaussian mixture model", "author": ["C.E. Rasmussen"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Rasmussen,? \\Q2000\\E", "shortCiteRegEx": "Rasmussen", "year": 2000}, {"title": "Mixture densities, maximum likelihood and the EM algorithm", "author": ["R A Redner", "Walker", "H F"], "venue": "SIAM Review,", "citeRegEx": "Redner et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Redner et al\\.", "year": 1984}, {"title": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems", "author": ["K. Rose"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rose,? \\Q1998\\E", "shortCiteRegEx": "Rose", "year": 1998}, {"title": "Optimization with EM and Expectation-ConjugateGradient", "author": ["R. Salakhutdinov", "S. Roweis", "Z. Ghahramani"], "venue": "Proceedings of ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Deterministic annealing EM algorithm", "author": ["N. Ueda", "R. Nakano"], "venue": "Neural Networks,", "citeRegEx": "Ueda and Nakano,? \\Q1998\\E", "shortCiteRegEx": "Ueda and Nakano", "year": 1998}, {"title": "On the convergence properties of the EM algorithm", "author": ["Wu", "CF"], "venue": "The Annals of Statistics,", "citeRegEx": "Wu and CF.,? \\Q1983\\E", "shortCiteRegEx": "Wu and CF.", "year": 1983}, {"title": "On convergence properties of the EM algorithm for Gaussian mixtures", "author": ["L. Xu", "M.I. Jordan"], "venue": "Neural computation,", "citeRegEx": "Xu and Jordan,? \\Q1996\\E", "shortCiteRegEx": "Xu and Jordan", "year": 1996}], "referenceMentions": [{"referenceID": 2, "context": "Clustering is a widely used exploratory data analysis tool that has been successfully applied to biology, social science, information retrieval, signal processing, and many other fields (Jain et al., 1999).", "startOffset": 186, "endOffset": 205}, {"referenceID": 1, "context": "The parameter estimation is typically accomplished by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977).", "startOffset": 98, "endOffset": 121}, {"referenceID": 11, "context": "We present a solution to this problem based on Deterministic Annealing (Rose, 1998; Ueda & Nakano, 1998).", "startOffset": 71, "endOffset": 104}, {"referenceID": 1, "context": "The parameter estimation is typically accomplished by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). In this paper, we empirically show that EM exhibits slow convergence if one of the Gaussian mixture components has a very small mixing coefficient compared to others, and there exists some overlap among the mixture components. We explain this slow convergence of EM for a mixture with unbalanced mixing coefficients using the convergence analysis framework presented by Xu and Jordan (1996), and Ma et al.", "startOffset": 99, "endOffset": 514}, {"referenceID": 1, "context": "The parameter estimation is typically accomplished by the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). In this paper, we empirically show that EM exhibits slow convergence if one of the Gaussian mixture components has a very small mixing coefficient compared to others, and there exists some overlap among the mixture components. We explain this slow convergence of EM for a mixture with unbalanced mixing coefficients using the convergence analysis framework presented by Xu and Jordan (1996), and Ma et al. (2000). We present a solution to this problem based on Deterministic Annealing (Rose, 1998; Ueda & Nakano, 1998).", "startOffset": 99, "endOffset": 536}, {"referenceID": 1, "context": "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983).", "startOffset": 119, "endOffset": 152}, {"referenceID": 1, "context": "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton\u2019s or Quasi-Newton methods should be preferred over the EM algorithm.", "startOffset": 120, "endOffset": 179}, {"referenceID": 1, "context": "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton\u2019s or Quasi-Newton methods should be preferred over the EM algorithm. They also experimentally show that EM converges slowly in the presence of overlapping clusters. Xu and Jordan (1996) analyze the rate of convergence of the EM algorithm and show that EM exhibits a super-linear rate of convergence as the overlap among the mixture components goes to zero (Xu & Jordan, 1996).", "startOffset": 120, "endOffset": 433}, {"referenceID": 1, "context": "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton\u2019s or Quasi-Newton methods should be preferred over the EM algorithm. They also experimentally show that EM converges slowly in the presence of overlapping clusters. Xu and Jordan (1996) analyze the rate of convergence of the EM algorithm and show that EM exhibits a super-linear rate of convergence as the overlap among the mixture components goes to zero (Xu & Jordan, 1996). They forge a connection between the EM algorithm and gradient ascent and prove that rate of convergence of the EM algorithm depends on the condition number of a projected Hessian matrix EP (\u0398\u2217)H(\u0398\u2217)E, where \u0398\u2217 is the optimum parameter value, E = [e1, . . . , em] is a set of unit basis vectors spanning the constrained parameter space (satisfying the constraint \u2211K j=1 \u03b1j = 1), P (\u0398 \u2217) is a projection matrix, and H(\u0398\u2217) is the Hessian of the log-likelihood function. Later, Ma et al. (2000) extend this result and show that the rate of convergence of EM is a higher order infinitesimal of maximum pairwise overlap among the mixture components.", "startOffset": 120, "endOffset": 1115}, {"referenceID": 1, "context": "Related Work The EM algorithm is guaranteed to monotonically converge to local optima under mild continuity conditions (Dempster et al., 1977; Wu, 1983). Redner and Walker (1984) show that EM has linear rate of convergence and suggest that Newton\u2019s or Quasi-Newton methods should be preferred over the EM algorithm. They also experimentally show that EM converges slowly in the presence of overlapping clusters. Xu and Jordan (1996) analyze the rate of convergence of the EM algorithm and show that EM exhibits a super-linear rate of convergence as the overlap among the mixture components goes to zero (Xu & Jordan, 1996). They forge a connection between the EM algorithm and gradient ascent and prove that rate of convergence of the EM algorithm depends on the condition number of a projected Hessian matrix EP (\u0398\u2217)H(\u0398\u2217)E, where \u0398\u2217 is the optimum parameter value, E = [e1, . . . , em] is a set of unit basis vectors spanning the constrained parameter space (satisfying the constraint \u2211K j=1 \u03b1j = 1), P (\u0398 \u2217) is a projection matrix, and H(\u0398\u2217) is the Hessian of the log-likelihood function. Later, Ma et al. (2000) extend this result and show that the rate of convergence of EM is a higher order infinitesimal of maximum pairwise overlap among the mixture components. Salakhutdinov et al. (2003) propose the Expectation Conjugate Gradient (ECG) algorithm for highly overlapping Gaussian mixtures.", "startOffset": 120, "endOffset": 1296}, {"referenceID": 14, "context": "We explain it using the framework proposed by Xu and Jordan (1996). Xu and Jordan (and later Ma et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 6, "context": "Xu and Jordan (and later Ma et al. (2000)) have shown that the rate of convergence of EM is upper bounded by the norm \u2225", "startOffset": 25, "endOffset": 42}, {"referenceID": 12, "context": "We compare the result with two other well-known optimization techniques: Expectation Conjugate Gradient (ECG) (Salakhutdinov et al., 2003) and QuasiNewton Method (BFGS) (Liu & Nocedal, 1989).", "startOffset": 110, "endOffset": 138}, {"referenceID": 13, "context": "Ueda and Nakano (1998) proposed a Deterministic Annealing Expectation Maximization (DAEM) algorithm that varies the temperature from high to low temperature, and deterministically optimizes the objective function at each temperature.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "On the other hand, as \u03b2 becomes larger, DAEM allows less and less overlap and for \u03b2 \u2192 \u221e it becomes analogous to the winner-takeall (Kearns et al., 1998) approach, i.", "startOffset": 131, "endOffset": 152}, {"referenceID": 6, "context": "Using the results shown by Ma et al. (2000), we explain the faster convergence of the proposed Anti-annealing method.", "startOffset": 27, "endOffset": 44}, {"referenceID": 6, "context": "Ma et al. (2000) show that the rate of convergence r is a higher order infinitesimal of e(\u0398\u2217):", "startOffset": 0, "endOffset": 17}, {"referenceID": 12, "context": "Salakhutdinov et al. (2003) proposed an expectation conjugate gradient (ECG) method for optimizing the log-likelihood functions in the case of highly overlapping clusters.", "startOffset": 0, "endOffset": 28}, {"referenceID": 12, "context": "To ensure that the ECG algorithm satisfies the same set of constraints, Salakhutdinov et al. (2003) propose to re-parameterize the model parameters: \u03b1j = e \u03bbj P", "startOffset": 72, "endOffset": 100}, {"referenceID": 9, "context": "The Dirichlet process mixture model (DPMM) (Rasmussen, 2000) has gained significant attention for flexible density estimation.", "startOffset": 43, "endOffset": 60}, {"referenceID": 3, "context": "extended to DPMM by a straight-forward modification of equation (3) (Katahira et al., 2008):", "startOffset": 68, "endOffset": 91}, {"referenceID": 12, "context": "Salakhutdinov et al. (2003) proposed a hybrid EMECG algorithm, that estimates the entropy of cluster memberships as a measure of missing information (in other words, cluster overlap), and chooses to apply ECG if the entropy value is larger than certain threshold.", "startOffset": 0, "endOffset": 28}, {"referenceID": 8, "context": "However, the temperature scheduling of Deterministic Annealing often requires some tuning (Rangarajan, 2000).", "startOffset": 90, "endOffset": 108}], "year": 2012, "abstractText": "The speed of convergence of the Expectation Maximization (EM) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components. In this paper, we study the impact of mixing coefficients on the convergence of EM. We show that when the mixture components exhibit some overlap, the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases. We propose a deterministic anti-annealing algorithm, that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients. The proposed algorithm is compared against other standard optimization techniques like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we propose a similar deterministic antiannealing based algorithm for the Dirichlet process mixture model and demonstrate its advantages over the conventional variational Bayesian approach.", "creator": "LaTeX with hyperref package"}}}