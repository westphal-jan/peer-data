{"id": "1206.6428", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Binary Classification Framework for Two-Stage Multiple Kernel Learning", "abstract": "With the advent of kernel methods, automating the task of specifying a suitable kernel has become increasingly important. In this context, the Multiple Kernel Learning (MKL) problem of finding a combination of pre-specified base kernels that is suitable for the task at hand has received significant attention from researchers. In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel. Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler, and, arguably, more accessible to practitioners. Experiments on nine data sets from different domains show that, despite its simplicity, the proposed technique compares favorably with current leading MKL approaches.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (392kb)", "http://arxiv.org/abs/1206.6428v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["abhishek kumar 0001", "alexandru niculescu-mizil", "koray kavukcuoglu", "hal daum\u00e9 iii"], "accepted": true, "id": "1206.6428"}, "pdf": {"name": "1206.6428.pdf", "metadata": {"source": "META", "title": "A Binary Classification Framework for Two-Stage Multiple Kernel Learning", "authors": ["Abhishek Kumar", "Koray Kavukcoglu"], "emails": ["abhishek@cs.umd.edu", "alex@nec-labs.com", "koray@nec-labs.com", "hal@umiacs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "The core methods, such as support for vector machines (SVM) (Cortes & Vapnik, 1995), which we only asked at user setup, are a problem we know about; the kernel backward regression, or kernel PCA (Smola & Muller, 1999), which uses a positive semidefinite (PSD) method to implicitly learn the results of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012. The main flaw of the kernel methods is that they require the user to specify a single suitable kernel, which is often crucial to the success of the method, but is usually a tough task even if the user has good familiarity with the problem domain. To ease this burden, significant attention has been paid to the problem of automatic learning of the kernel."}, {"heading": "2. Method", "text": "We consider a classification problem where instances (x, y) are drawn by a distribution P over X \u00b7 Y, with a finite discrete series of labels. We assume that we have access to p positive semi-definite (PSD) basic functions K1, \u00b7 \u00b7 \u00b7, Kp with Ki: X \u00b7 X \u2192 R. Our goal is to learn a combination of these kernel functions, which is itself a positive semi-definite function and is a \"good\" kernel for the classification task. To achieve this, however, we define a new binary classification problem via a new instance space {(zxx)."}, {"heading": "2.1. Connection to Target Alignment", "text": "Previous two-stage kernel learning approaches (Christianini et al., 2001; Cortes et al., 2010b) learn a non-negative linear combination of base cores that maximizes alignment with the target core K (t) (xi, xj) = yiyj on the training set. This is achieved by solving the optimization problem max \u00b5 0 < \u2211 p = 1 \u00b5lKl, K (t) > | | \u2211 p = 1 \u00b5lKl | | F, s.t. | \u00b5 | | 2 = 1, (3), where A is the gram matrix of kernel A on the training set < A > = tr (ABT) > | | A | 2F = tr (AAT). The above optimization problem can be rewritten in our terminology of K examples as follows: max \u00b5 \u2265 0\u00b5T (proved tij = 1 zij et tij = \u2212 zij.) From the perspective, the difference between K is very small."}, {"heading": "2.2. Connection to Learning with Hyperkernels", "text": "The approach proposed in this publication can also be applied in the context of learning with hyperkernels (Ong et al., 2005), which provides a general recipe for kernel learning and which includes several kernel learning effects as a specific case. It presents the terms of kernel quality that can be learned. Once the desired measure of \"quality\" of a kernel is defined, a semi-definite program (SDP) must be solved to optimize the quality of functional regulation by the standard used by the Hyper-RKHS. When using an SVM as a K classifier, TS-MKL can be put into learning with hyperkernel framework by defining the hyperkernel-RKHS to be the non-negative linear combinations of the base."}, {"heading": "4. Empirical Evaluation", "text": "We evaluate the proposed method on the basis of two data sets for object recognition (Caltech-101 and Caltech-256), three data sets for bioinformatics (Psort +, Psort and Plant) and four UCI data sets (Sonar, Pima, Vertebral and Ionosphere) and compare our method with several baselines: the best kernel, a uniform combination of Average, Target Alignment and the MKL algorithms SILP (Sonnenburg et al., 2006), SimpleMKL (Rakotomamonjy et al., 2007), L2 standard MLK (Kloft et al., 2011) and UFO-MKL (Orabona & Jie, 2011). For two-stage methods, we use LIBSVM (Chang & Lin et al., 2011) to train the data classifier and the regulation parameter C via 4-fold cross-Urabus validation for Caleta, all data sets via Caleta-101, a common function class via CalSVL, and MIBL."}, {"heading": "4.1. Methodology for TS-MKL", "text": "To learn the combination weights \u00b5 with TSMKL, we optimize the target in Equation 2 using Pegasos (Shalev-Shwartz et al., 2007) with an additional projection on the non-negative constraints set after each sub-gradient step. We use a batch size of 100 for each sub-gradient calculation and perform 103 sub-gradient steps for UCI data and 105 for all others. Figure 2 records the accuracy of the test data against the number of gradient iterations on Caltech-101 and shows that after 105 iterations the change in accuracy is minimal. For the larger Caltech-256, there is also essentially no change after 105 iterations. We use subsampling to balance the positive and negative K examples. To select the parameters \u03bb, we use a single 80% -20% breakdown of the pegasos training group and the search for the forest with the lowest validation losses."}, {"heading": "4.2. Caltech-101 and Caltech-256", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "4.3. Bioinformatics datasets", "text": "We evaluate our method on the basis of a problem relevant to cell biology: the subcellular localization of proteins, which is critical to inferring protein function and protein interactions; following the experimental setup of (Zien & Ong, 2007) and using the same 69 nuclei; the first two datasets relate to the problem of bacterial protein sites (Gardy et al., 2004); the Psort + dataset has 541 data points with 4 classes and Port datasets with 5 classes; the first two datasets relate to the problem of bacterial protein sites (Gardy et al., 2004); the Psort + dataset has 541 data points with 4 classes; and the Port dataset has 1444 data points with 5 classes."}, {"heading": "4.4. UCI datasets", "text": "We use four UCI data sets: Sonar, Ionosphere, Pima and Vertebral (the three-class version). For each of these data sets, we perform two types of MKL experiments. In the first setting, we construct a total of 13 nuclei on the full feature vectors: 9 Gaussian nuclei (e \u2212 \u03b3 | | xi \u2212 xj | | 2) with \u03b3 = {2 \u2212 10, 2 \u2212 9,.., 2 \u2212 2}, 3 polynomial nuclei of degrees 2.3 and 4 and a linear kernel. In the second setting, we add another set of Gaussian, polynomial and linear nuclei based on individual characteristics of the data. The range of parameters for Gaussian nuclei and degree parameters for polynomial nuclei is maintained as before. If the data have d characteristics, we have a total of 13d + 13 nuclei constructed on individual characteristics of the data."}, {"heading": "4.5. Computational Efficiency", "text": "Since the number of K samples is square in the number of training instances, one might worry about the scalability of the TS-MKL method. In this section, we compare the runtime of the TS-MKL technique with target alignment and UFO-MKL (Ultra-Fast Optimization MKL), which to our knowledge is the fastest single-stage MKL technique to date. Table 3 shows runtimes for the sonar, pima and Caltech 101 datasets. Runtime is for a single run using the best parameter settings (i.e. it does not include the time for selecting parameters). For TS-MKL and Target Alingment, we also show the runtime taken by the core learning stage alone, without the final data SVM, on Caltech-101. For sonar, which has only 166 training samples, the runtime of UFO-MKL and Target Alingment is comparable in paranthesis."}, {"heading": "5. Conclusions and Future Work", "text": "Our approach is underpinned by formal theoretical guarantees and empirical evaluations showing that it is always better or equivalent to leading single-level and two-level learning methods for kernels, which is a remarkable achievement for a method that is quite simple and intuitive. This new perspective on multi-core learning opens the door to a number of interesting questions to be answered in subsequent research, such as: exploring the use of non-linear K classifiers in conjunction with the framework for learning with similarity functions; improving performance in scarce data relationships through semi-supervised and multi-functional learning of multiple kernels through the use of such techniques to learn K classification; or applying TS-MKL to semi-supervised clustering and dimensionality problems where the monitored signal is normally given in the form of linkages rather than in the form of linkages."}], "references": [{"title": "Consistency of the Group Lasso and Multiple Kernel Learning", "author": ["F. Bach"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "On a Theory of Learning with Similarity Functions", "author": ["Balcan", "M.-F", "A. Blum"], "venue": "In ICML,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson", "year": 2002}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Learning non-linear combinations of kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Generalization bounds for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Two-Stage Learning Kernel Algorithms", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "On Kernel-Target Alignment", "author": ["N. Cristianini", "J. Shawe-Taylor", "A. Elisseeff", "J.S. Kandola"], "venue": "In NIPS,", "citeRegEx": "Cristianini et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2001}, {"title": "Predicting subcellular localization of proteins based on their N-terminal amino acid sequence", "author": ["O. Emanuelsson", "H. Nielsen", "S. Brunak", "G. von Heijne"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Emanuelsson et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Emanuelsson et al\\.", "year": 2000}, {"title": "PSORTb v.2.0: expanded prediction of bacterial protein subcellular localization and insights gained from comparative proteome analysis", "author": ["J.L. Gardy", "M.R. Laird", "F. Chen", "S. Rey", "C.J. Walsh", "M. Ester", "F.S.L. Brinkman"], "venue": "Bioinfomatics, 21:617\u2013623,", "citeRegEx": "Gardy et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gardy et al\\.", "year": 2004}, {"title": "On Feature Combination for Multiclass Object Detection", "author": ["P. Gehler", "S. Nowozin"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Gehler and Nowozin,? \\Q2009\\E", "shortCiteRegEx": "Gehler and Nowozin", "year": 2009}, {"title": "Optimizing Kernel Alignment over Combination of Kernels", "author": ["J.S. Kandola", "J. Shawe-Taylor", "N. Cristianini"], "venue": "In Tech. Report 121, Dept. of CS, Univ. of London, UK,", "citeRegEx": "Kandola et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kandola et al\\.", "year": 2002}, {"title": "`pNorm Multiple Kernel Learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kloft et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2011}, {"title": "Learning the Kernel Matrix with Semidefinite Programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "Ghaoui", "L. El", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Learning the kernel with hyperkernels", "author": ["C.S. Ong", "A. Smola", "R. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2005}, {"title": "Ultra-fast optimization algorithm for sparse multi kernel learning", "author": ["F. Orabona", "L. Jie"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Orabona and Jie,? \\Q2011\\E", "shortCiteRegEx": "Orabona and Jie", "year": 2011}, {"title": "More efficiency in multiple kernel learning", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Rakotomamonjy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rakotomamonjy et al\\.", "year": 2007}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Non-parametric group orthogonal matching pursuit for sparse learning with multiple kernels", "author": ["V. Sindhwani", "A.C. Lozano"], "venue": "In NIPS,", "citeRegEx": "Sindhwani and Lozano,? \\Q2011\\E", "shortCiteRegEx": "Sindhwani and Lozano", "year": 2011}, {"title": "Kernel Principal Component Analysis", "author": ["Smola", "B. Scholkopf A", "Muller", "K.-R"], "venue": "Advances in Kernel Methods - Support Vector Learning,", "citeRegEx": "Smola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Smola et al\\.", "year": 1999}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. Ratsch", "C. Schafer", "B. Scholkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "How Good is a Kernel When Used as a Similarity Measure", "author": ["N. Srebro"], "venue": "In COLT,", "citeRegEx": "Srebro,? \\Q2007\\E", "shortCiteRegEx": "Srebro", "year": 2007}, {"title": "Multiclass Multiple Kernel Learning", "author": ["A. Zien", "C.S. Ong"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Zien and Ong,? \\Q2007\\E", "shortCiteRegEx": "Zien and Ong", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "This one-stage approach was first proposed by (Lanckriet et al., 2004) and has since received significant attention (Rakotomamonjy et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 16, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 20, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 12, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 0, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 4, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 7, "context": "This approach has been initially proposed in (Cristianini et al., 2001) and (Kandola et al.", "startOffset": 45, "endOffset": 71}, {"referenceID": 11, "context": ", 2001) and (Kandola et al., 2002), and recently revisited by (Cortes et al.", "startOffset": 12, "endOffset": 34}, {"referenceID": 17, "context": "For the results presented in this paper we learn K-classifiers (and hence kernels) by training L2 regularized linear SVMs with positive weights using the stochastic projected sub-gradient descent method from Pegasos (Shalev-Shwartz et al., 2007).", "startOffset": 216, "endOffset": 245}, {"referenceID": 17, "context": "sos (Shalev-Shwartz et al., 2007), with an additional projection to the non-negative constraint set after every gradient step.", "startOffset": 4, "endOffset": 33}, {"referenceID": 7, "context": "Previous two-stage kernel learning approaches (Cristianini et al., 2001; Cortes et al., 2010b) learn a non-negative linear combination of base kernels that maximizes the alignment with the target kernel K(xi,xj) = yiyj on the training set.", "startOffset": 46, "endOffset": 94}, {"referenceID": 14, "context": "The approach proposed in this paper can also be cast in the framework of learning with hyperkernels (Ong et al., 2005) which provides a general recipe for kernel learning and includes Multiple Kernel Learning as a", "startOffset": 100, "endOffset": 118}, {"referenceID": 21, "context": "The following definition states formally what we mean by a good kernel (Srebro, 2007).", "startOffset": 71, "endOffset": 85}, {"referenceID": 7, "context": "This is the analog of the concentration bounds for target alignment in (Cortes et al., 2010b; Cristianini et al., 2001).", "startOffset": 71, "endOffset": 119}, {"referenceID": 20, "context": "We compare our method with several baselines: best kernel, uniform combination of base kernels (Average), target alignment, and the onestage MKL algorithms SILP (Sonnenburg et al., 2006), SimpleMKL (Rakotomamonjy et al.", "startOffset": 161, "endOffset": 186}, {"referenceID": 16, "context": ", 2006), SimpleMKL (Rakotomamonjy et al., 2007), L2-Norm MLK (Kloft et al.", "startOffset": 19, "endOffset": 47}, {"referenceID": 12, "context": ", 2007), L2-Norm MLK (Kloft et al., 2011), and UFO-MKL (Orabona & Jie, 2011).", "startOffset": 21, "endOffset": 41}, {"referenceID": 17, "context": "2 using Pegasos (Shalev-Shwartz et al., 2007) with an additional projection to the non-negative constraint set after each sub-gradient step.", "startOffset": 16, "endOffset": 45}, {"referenceID": 9, "context": "The first two datasets are for the problem of bacterial protein locations (Gardy et al., 2004).", "startOffset": 74, "endOffset": 94}, {"referenceID": 8, "context": "The third dataset used is the original plant dataset of TargetP (Emanuelsson et al., 2000), and has 940 examples with 4 classes.", "startOffset": 64, "endOffset": 90}, {"referenceID": 9, "context": "The papers that have used the Psort datasets in the past (Gardy et al., 2004; Zien & Ong, 2007), reported results after filtering out the most unsure predictions in the test set.", "startOffset": 57, "endOffset": 95}], "year": 2012, "abstractText": "With the advent of kernel methods, automating the task of specifying a suitable kernel has become increasingly important. In this context, the Multiple Kernel Learning (MKL) problem of finding a combination of prespecified base kernels that is suitable for the task at hand has received significant attention from researchers. In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel. Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler, and, arguably, more accessible to practitioners. Experiments on nine data sets from different domains show that, despite its simplicity, the proposed technique compares favorably with current leading MKL approaches.", "creator": "LaTeX with hyperref package"}}}