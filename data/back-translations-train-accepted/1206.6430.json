{"id": "1206.6430", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Variational Bayesian Inference with Stochastic Search", "abstract": "Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (575kb)", "http://arxiv.org/abs/1206.6430v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.CO stat.ML", "authors": ["john william paisley", "david m blei", "michael i jordan"], "accepted": true, "id": "1206.6430"}, "pdf": {"name": "1206.6430.pdf", "metadata": {"source": "META", "title": "Variational Bayesian Inference with Stochastic Search", "authors": ["John Paisley", "David M. Blei", "Michael I. Jordan"], "emails": ["jpaisley@berkeley.edu", "blei@cs.princeton.edu", "jordan@eecs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of us are able to seek a solution that has its origin in the history of humanity. (...) Most of them are able to understand themselves. (...) Most of them are not able to understand themselves. (...) Most of them are not. (...) Most of them are. (...) Most of them are. (...) Most of them are. (...) Most of them are. (...) \"(...)\" (...) \"(...)\" (... \")\" (... \")\" (... \")\" (... \"(...)\" (... \")\" (... \"()\" (... \")\" (... \")\" (... \")\" (... \")\" (... \")\" () \"(...\" () \"()\" (... \"()\" () \"() (...\" () \"()\" () (... \"()\" () \"() ()\" () (... \"()\" () \"()\" () () \"() () ()\" () \"()\" () () () \"()\" () \"()\" () \"()\" () \"() () ()\" () \"() () ()\" () () () \"() () () ()\" () () () \"() () () () ()\" () () () () () () () () () \"() () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () (() () () (() () () (() () () (() () () (((() () () (() () () (() () () (() (() (() () () (((() (() (() () () (() () (() (((() () ((() () () () ("}, {"heading": "2. Mean-field variational inference", "text": "In the introduction, we will define the factorized distribution on the basis of the Q distribution, which will prove to be a problem. In the introduction, these variables, in which they represent the parameters of the Qi distributions, are defined as an objective function. The variational objective function is created by using the marginal probability, lnP (X) = objective results of the Qi distributions. In the introduction, the results of the Qi distributions are used, in which the results of the Qi distributions, lnP (X) = objective results of the Qi distributions are discussed. In the introduction, the expectations of the Qi distributions are discussed."}, {"heading": "3. Stochastic search variational Bayes", "text": "Next, we present a method based on the stochastic search for a direct optimization of the variable objective function L in cases where some q values cannot be calculated in the log coefficient probability. This method uses a stochastic approximation of the gradient q with respect to the variable parameters of the associated q distribution. To further simplify the notation, we drop all q indices (f), except for Ef. Remarkably, h contains all other functions derived from the expectations calculated with respect to q."}, {"heading": "4. Searching with control variates", "text": "A practical problem with the stochastic approximation proposed in Section 3 is that the variance of the gradient approximation can be very large. Given the S samples of a random vector X, the covariance of its unbiased sample is known as Cov (X) = Cov (X) / S. If the diagonal values of Cov (X) are large, many samples will be required to bring that variance below a desired level to approximate expectation. As our experiments in Section 6 will show, the value of S can be very large in practice and lead to a slow algorithm. We are therefore looking for a variance reduction method to reduce the number of samples required to construct the stochastic search guideline. We are introducing a control variant (Ross, 2006) to reduce the variance of the stochastic gradient constructed in equation variable. (6) A control variable is a random variable that is correlated to a class that is tractable in a high degree with a tractable."}, {"heading": "4.1. A control variate for f(\u03b8)", "text": "In general, variance reduction works by modifying a function of a random variable in such a way that its expectation remains the same but its variance decreases. Towards this end, we introduce a control variant that approaches in the highly probable regions as defined by q, but also has a closed expectation under q. (8) This function has the same expectation as f and can therefore replace it in L. (3) The next step is to minimize the value of a variance of f. A simple calculation shows that thatVar (f) \u2212 2aCov (f, g) + a2Var (g)."}, {"heading": "4.2. The stochastic search case", "text": "We have introduced a control variant for f (\u03b8), but in reality we want to minimize the variance of the vector f (\u03b8), ln q. in equation (6). In this case, the control variant becomes g (\u03b8), ln q and we have the following modification. Let's be the kth dimension of equation. Then, the discussion continues in paragraph 4.1 for each dimension, but for f x x x x x x x x k and g x x x x x x x x k instead of f and g. The variance of each dimension follows equation (9) again, and we look for one to minimize the sum of these equations. This results in the optimal value ea = x k Kov (f x x, g x ln q), g x k Var (g x x x x x), which we approximate by means of samples. We summarize the stochastic search variance in Baym 1."}, {"heading": "5. Stochastic search VB for two models", "text": "Next, we will illustrate stochastic search variations through logistic regression and a finite approximation to the hierarchical Dirichlet process (Teh et al., 2007). For logistic regression, we will consider two control variants, one of which is a lower limit and the other no limit. For the finite HDP, we will consider a piecemeal control variant, with one part representing an upper limit of the original function."}, {"heading": "5.1. Logistic regression", "text": "The parameter is that the law of prediction sets a previous distribution on the coefficient vector in the ranges Normal (0, cI) and Normal (0, cI). For the inference, we define a Gaussian variation method (b) = Normal (p) = Normal (p). The variable lower limit for this model is L = 1 Eq (n) + Eq (n) + Eq (n) + Eq (n) + Eq (n) + Eq (n) + Eq) + Eq (n) + Eq (n) + Eq (n) + Eq (n) + Eq (l) p (p) \u2212 ln (p) \u2212 ln) The expectations of the fn (n)."}, {"heading": "5.2. Hierarchical Dirichlet processes", "text": "We investigate a stochastic search for a generic hierarchical structure in Eq (17), a stochastic search for a hierarchical structure in Eq (17), a stochastic search for a hierarchical structure in Eq (17), and a stochastic search for a hierarchical structure in Eq (17)."}, {"heading": "6. Experiments", "text": "We conduct experiments with stochastic search for a binary classification with logistic regression and compare topic modeling with approximate HDP 12. Next, we will give the details of the experiments we are conducting, and the data sets and algorithms used for comparison. Data and setup. For logistic regression, however, we will use five sets of data from the UCI repository: Iris, Pima, SPECTF, Voting and WDBC. These data sets range from 150 to 768 described examples that live in 5 to 45 dimensions, including one dimension of all that must be considered for offset. We will conduct experiments with stochastic search variations that use the two control variants discussed in Sec. 5.1. We will compare with two additional methods for posterior approximation: variative inference bound with the Jaakkola & Jordan (2000) and laplaces method for evaluating performance on the true target function in Eq."}, {"heading": "7. Conclusion", "text": "The algorithm is based on a stochastic approximation of the gradient; we demonstrated how control variants can significantly reduce the variance of this Monte Carlo integral. Since existing lower limits can be recast as control variants, our approach is relevant to many existing MFVB algorithms. However, a lack of constraints on control variants allows other types of functional approximations when a good limit is not readily available. We introduced the control variant Delta Method in this sense. Acknowledgements J.P. and M.J. are supported by the MURI program through ONR grant number N00014-11-1-0651, NSF CAREER 0745520, AFOSR FA9550-09-1-0668, the Alfred P. Sloan Foundation and Google."}], "references": [{"title": "Variational Algorithms for Approximate Bayesian Inference", "author": ["M.J. Beal"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["D. Blei", "M. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan,? \\Q2006\\E", "shortCiteRegEx": "Blei and Jordan", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Graves,? \\Q2011\\E", "shortCiteRegEx": "Graves", "year": 2011}, {"title": "Online learning for latent Dirichlet allocation", "author": ["M. Hoffman", "D. Blei", "F. Bach"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Bayesian parameter estimation via variational methods", "author": ["T. Jaakkola", "M.I. Jordan"], "venue": "Statistics and Computing,", "citeRegEx": "Jaakkola and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 2000}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Non-conjugate variational message passing for multinomial and binary regression", "author": ["D.A. Knowles", "T.P. Minka"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Knowles and Minka,? \\Q2011\\E", "shortCiteRegEx": "Knowles and Minka", "year": 2011}, {"title": "A tighter bound for graphical models", "author": ["M.A.R. Leisink", "H.J. Kappen"], "venue": "Neural Computation,", "citeRegEx": "Leisink and Kappen,? \\Q2001\\E", "shortCiteRegEx": "Leisink and Kappen", "year": 2001}, {"title": "Piecewise bounds for estimating Bernoulli-logistic latent Gaussian models", "author": ["B. Marlin", "E. Khan", "K. Murphy"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Marlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2011}, {"title": "Nonparametric factor analysis with beta process priors", "author": ["J. Paisley", "L. Carin"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Paisley and Carin,? \\Q2009\\E", "shortCiteRegEx": "Paisley and Carin", "year": 2009}, {"title": "Hierarchical Dirichlet processes", "author": ["Y. Teh", "M. Jordan", "M. Beal", "D. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Online variational inference for the hierarchical Dirichlet process", "author": ["C. Wang", "J. Paisley", "D. Blei"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Stochastic search using the natural gradient", "author": ["S. Yi", "D. Wierstra", "T. Schaul", "J. Schmidhuber"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Yi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Mean-field variational Bayesian (MFVB) inference is an optimization-based approach to approximating the full posterior of the latent variables of a Bayesian model (Jordan et al., 1999).", "startOffset": 163, "endOffset": 184}, {"referenceID": 0, "context": "It has been applied to many problem domains, for example mixture modeling (Blei & Jordan, 2006), sequential modeling (Beal, 2003) and factor analysis (Paisley & Carin, 2009).", "startOffset": 117, "endOffset": 129}, {"referenceID": 4, "context": "In addition, recent development of the theory has extended the method to online inference and stochastic optimization settings, making variational Bayes a viable approach for Bayesian learning with massive data sets (Hoffman et al., 2010; Wang et al., 2011).", "startOffset": 216, "endOffset": 257}, {"referenceID": 12, "context": "In addition, recent development of the theory has extended the method to online inference and stochastic optimization settings, making variational Bayes a viable approach for Bayesian learning with massive data sets (Hoffman et al., 2010; Wang et al., 2011).", "startOffset": 216, "endOffset": 257}, {"referenceID": 9, "context": ", Jaakkola & Jordan (2000), Marlin et al. (2011), Leisink & Kappen (2001)).", "startOffset": 28, "endOffset": 49}, {"referenceID": 9, "context": ", Jaakkola & Jordan (2000), Marlin et al. (2011), Leisink & Kappen (2001)).", "startOffset": 28, "endOffset": 74}, {"referenceID": 3, "context": "Graves (2011) considers a similar problem for neural networks, but a lack of control variates limits the algorithm to significantly simpler variational approximations.", "startOffset": 0, "endOffset": 14}, {"referenceID": 3, "context": "Graves (2011) considers a similar problem for neural networks, but a lack of control variates limits the algorithm to significantly simpler variational approximations. Stochastic search algorithms have also been developed for models of Evolution Strategies (see, e.g., Yi et al. (2009)).", "startOffset": 0, "endOffset": 286}, {"referenceID": 9, "context": ", Marlin et al. (2011), Knowles & Minka (2011)).", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": ", Marlin et al. (2011), Knowles & Minka (2011)).", "startOffset": 2, "endOffset": 47}, {"referenceID": 11, "context": "We next illustrate stochastic search variational inference on logistic regression and a finite approximation to the hierarchical Dirichlet process (Teh et al., 2007).", "startOffset": 147, "endOffset": 165}, {"referenceID": 11, "context": "We also investigate a stochastic search VB algorithm for an approximation to the hierarchical Dirichlet process (Teh et al., 2007).", "startOffset": 112, "endOffset": 130}, {"referenceID": 2, "context": "We compare with (i) a point estimate of the top-level probability vector using a delta q distribution, and (ii) fixing the top-level distribution to the uniform vector, which is equivalent to LDA (Blei et al., 2003).", "startOffset": 196, "endOffset": 215}], "year": 2012, "abstractText": "Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.", "creator": "LaTeX with hyperref package"}}}