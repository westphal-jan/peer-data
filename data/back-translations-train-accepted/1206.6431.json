{"id": "1206.6431", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Exact Maximum Margin Structure Learning of Bayesian Networks", "abstract": "Recently, there has been much interest in finding globally optimal Bayesian network structures. These techniques were developed for generative scores and can not be directly extended to discriminative scores, as desired for classification. In this paper, we propose an exact method for finding network structures maximizing the probabilistic soft margin, a successfully applied discriminative score. Our method is based on branch-and-bound techniques within a linear programming framework and maintains an any-time solution, together with worst-case sub-optimality bounds. We apply a set of order constraints for enforcing the network structure to be acyclic, which allows a compact problem representation and the use of general-purpose optimization techniques. In classification experiments, our methods clearly outperform generatively trained network structures and compete with support vector machines.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (401kb)", "http://arxiv.org/abs/1206.6431v1", "ICML"]], "COMMENTS": "ICML", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["robert peharz", "franz pernkopf"], "accepted": true, "id": "1206.6431"}, "pdf": {"name": "1206.6431.pdf", "metadata": {"source": "META", "title": "Exact Maximum Margin Structure Learning of Bayesian Networks", "authors": ["Robert Peharz", "Franz Pernkopf"], "emails": ["robert.peharz@tugraz.at", "pernkopf@tugraz.at"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to integrate themselves, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in fact, in fact, are able to integrate themselves, are able to integrate themselves, are able to integrate themselves, in which they are able to integrate themselves"}, {"heading": "2. Background and Notation", "text": "Throughout the work, we assume that discrete RVs, where single uppercase letters replace single RVs = > uppercase letters, represent sets of RVs. The set of states that can be assumed by RV X is called val (X), and we define sp (X) = | val (X) |. For simplicity of notation, we use Val (X) to indicate the number of possible common states of a set of RVs X, and leave sp (X) =. Lowercase single letters represent values or states of RVs, e.g. x is a value of RV X. Similarly, lowercase letters represent common states of variable sets, e.g. x is a state of RV X.If y is a state of Y, we assume that a corresponding state of Y."}, {"heading": "3. Related Work", "text": "The goal was to maximize a generative score such as MDL / BIC or BDe. For each variable XII, we identify a number of possible parent sets Si = {Si, 1,. Si, Qi, where Qi, 1, and each Si, 1, and each Si, j and Xi, while being reasonably small, so that the algorithms remain tractable. In (de Campos et al., 2009) a principal strategy was presented that excludes all parent sets that cannot occur in an optimal combination of parent sets."}, {"heading": "4. Max-Margin Structure Learning", "text": "First, we limit the maximum number of parents for each variable to obtain a tractable number of parent sets. Second, because we only have to consider the empty parent sets that contain C, since all other parent sets have no effect on marginality. Second, by using generative parameters, the resulting BN can still be interpreted as a generative model, although its structure is determined in a discriminatory manner. First, because we would make the notation of the max margin structure and parameters introduced in Section 3 intractable to our approach. Second, by interpreting the BN as a generative model, we can extend the BN distribution (1) to toPB (x)."}, {"heading": "5. Binary Margin Formulation", "text": "The main limitations of the problem (19) are that we have a problem that we can only solve if we reduce the number of samples by using a representative subset of D (1). (2) The limitations of the model are very simple and can be dealt with in future work. (2) The basic idea is to propose a formula that only needs M (C) -1) that takes into account all the limitations of the model. (2) The limitations of the model are very simple and can be dealt with with with many class values. (1) If we want to apply a one-against-all classification scheme, we want a one-against-class c against all other classes, we replace the parameters used in (10)."}, {"heading": "6. Experiments", "text": "We used the 25 datasets we have already used in (Friedman et al., 1997), plus six additional datasets containing between 80 and 45222 samples; for more detailed information, refer to (Frank & Asuncion, 2010); and to estimate the accuracy of the classifiers, a test set was used for the datasets. \"mofn-3-10,\" satimage, \"segment\" we shuttle-small, \"\" waveform-21, \"and the six additional datasets."}, {"heading": "7. Conclusion", "text": "We proposed an exact method for the combinatorial problem of finding a BN structure that maximizes the likely soft margin, and extended previous methods for exact generative learning of the BN structure. We demonstrated the applicability of our methods to small and medium-sized datasets and achieved promising results. Having an exact algorithm is valuable - although the problem is difficult. First, it is important to address these datasets when the problem proves comprehensible. Second, a key feature of the methods presented in this paper is that they provide solutions at all times, i.e. if the problem proves unfeasible, they still provide an approximation along with a worst-case estimate of suboptimality. Therefore, these methods can provide satisfactory results even for more difficult problems. Furthermore, exact methods provide interesting theoretical insights into the nature of the problem and possibly motivate new heuristics and approximation attempts for inaccurate structural learning."}, {"heading": "Acknowledgments", "text": "This work was supported by the Austrian Science Fund (project number P22488-N23)."}], "references": [{"title": "Theory refinement on Bayesian networks", "author": ["W. Buntine"], "venue": "In UAI, pp", "citeRegEx": "Buntine,? \\Q1991\\E", "shortCiteRegEx": "Buntine", "year": 1991}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Learning Bayesian networks is NPcomplete", "author": ["D.M. Chickering"], "venue": null, "citeRegEx": "Chickering,? \\Q1996\\E", "shortCiteRegEx": "Chickering", "year": 1996}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine Learning,", "citeRegEx": "Cooper and Herskovits,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits", "year": 1992}, {"title": "Maximum likelihood pedigree reconstruction using integer programming", "author": ["J. Cussens"], "venue": "In Proceedings of WCB-10,", "citeRegEx": "Cussens,? \\Q2010\\E", "shortCiteRegEx": "Cussens", "year": 2010}, {"title": "Bayesian network learning with cutting planes", "author": ["J. Cussens"], "venue": "In UAI, pp", "citeRegEx": "Cussens,? \\Q2011\\E", "shortCiteRegEx": "Cussens", "year": 2011}, {"title": "Efficient structure learning of Bayesian networks using constraints", "author": ["C.P. de Campos", "Q. Ji"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Campos and Ji,? \\Q2011\\E", "shortCiteRegEx": "Campos and Ji", "year": 2011}, {"title": "Structure learning of Bayesian networks using constraints", "author": ["C.P. de Campos", "Z. Zeng", "Q. Ji"], "venue": "In ICML, pp", "citeRegEx": "Campos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Campos et al\\.", "year": 2009}, {"title": "Multi-interval discretization of continuous-valued attributes for classification learning", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": "In UAI, pp", "citeRegEx": "Fayyad and Irani,? \\Q1993\\E", "shortCiteRegEx": "Fayyad and Irani", "year": 1993}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning,", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Maximum margin Bayesian networks", "author": ["Y. Guo", "D. Wilkinson", "D. Schuurmans"], "venue": "In UAI,", "citeRegEx": "Guo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2005}, {"title": "Learning Bayesian networks: the combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine Learning,", "citeRegEx": "Heckerman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Learning Bayesian network structure using LP relaxations", "author": ["T. Jaakkola", "D. Sontag", "A. Globerson", "M. Meila"], "venue": "In AI Statistics,", "citeRegEx": "Jaakkola et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 2010}, {"title": "Exact Bayesian structure discovery in Bayesian networks", "author": ["M. Koivisto", "K. Sood"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Koivisto and Sood,? \\Q2004\\E", "shortCiteRegEx": "Koivisto and Sood", "year": 2004}, {"title": "Probabilistic Graphical Models - Principles and Techniques", "author": ["D. Koller", "N. Friedmann"], "venue": null, "citeRegEx": "Koller and Friedmann,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedmann", "year": 2009}, {"title": "Exact structure discovery in Bayesian networks with less space", "author": ["P. Parviainen", "M. Koivisto"], "venue": "In UAI, pp", "citeRegEx": "Parviainen and Koivisto,? \\Q2009\\E", "shortCiteRegEx": "Parviainen and Koivisto", "year": 2009}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Stochastic marginbased structure learning of Bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr"], "venue": "Pattern Recognition,", "citeRegEx": "Pernkopf and Wohlmayr,? \\Q2012\\E", "shortCiteRegEx": "Pernkopf and Wohlmayr", "year": 2012}, {"title": "Maximum margin structure learning of Bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr", "M. M\u00fccke"], "venue": "In ICASSP, pp. 2076\u20132079,", "citeRegEx": "Pernkopf et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pernkopf et al\\.", "year": 2011}, {"title": "Maximum margin Bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr", "S. Tschiatschek"], "venue": "IEEE TPAMI,", "citeRegEx": "Pernkopf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pernkopf et al\\.", "year": 2012}, {"title": "Modeling by shortest data", "author": ["J. Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen,? \\Q1978\\E", "shortCiteRegEx": "Rissanen", "year": 1978}, {"title": "A simple approach for finding the globally optimal Bayesian network structure", "author": ["T. Silander", "P. Myllym\u00e4ki"], "venue": "In UAI, pp", "citeRegEx": "Silander and Myllym\u00e4ki,? \\Q2006\\E", "shortCiteRegEx": "Silander and Myllym\u00e4ki", "year": 2006}, {"title": "A construction of Bayesian networks from databases based on an MDL principle", "author": ["J. Suzuki"], "venue": "In UAI, pp", "citeRegEx": "Suzuki,? \\Q1993\\E", "shortCiteRegEx": "Suzuki", "year": 1993}], "referenceMentions": [{"referenceID": 16, "context": "Bayesian networks (BNs) are an important type of probabilistic graphical models (Pearl, 1988; Koller & Friedmann, 2009) and specify a probability distribution over a set of random variables (RVs).", "startOffset": 80, "endOffset": 119}, {"referenceID": 2, "context": "This is a combinatorial problem and known to be NP-hard in general (Chickering, 1996).", "startOffset": 67, "endOffset": 85}, {"referenceID": 12, "context": "Alternatively to dynamic programming, branch-and-bound (B&B) techniques have been exploited for exact structure learning (de Campos et al., 2009; de Campos & Ji, 2011; Jaakkola et al., 2010).", "startOffset": 121, "endOffset": 190}, {"referenceID": 20, "context": "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al.", "startOffset": 55, "endOffset": 85}, {"referenceID": 22, "context": "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al.", "startOffset": 55, "endOffset": 85}, {"referenceID": 0, "context": "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al., 1995).", "startOffset": 93, "endOffset": 159}, {"referenceID": 11, "context": "they aim to maximize generative scores such as MDL/BIC (Rissanen, 1978; Suzuki, 1993) or BDe (Buntine, 1991; Cooper & Herskovits, 1992; Heckerman et al., 1995).", "startOffset": 93, "endOffset": 159}, {"referenceID": 10, "context": "On the other hand, when the learned BN shall be used as classifier, we aim to maximize a discriminative score, such as (parameter penalized) conditional likelihood, classification rate, or a recently proposed probabilistic margin formulation (Guo et al., 2005; Pernkopf et al., 2012).", "startOffset": 242, "endOffset": 283}, {"referenceID": 19, "context": "On the other hand, when the learned BN shall be used as classifier, we aim to maximize a discriminative score, such as (parameter penalized) conditional likelihood, classification rate, or a recently proposed probabilistic margin formulation (Guo et al., 2005; Pernkopf et al., 2012).", "startOffset": 242, "endOffset": 283}, {"referenceID": 12, "context": "For this purpose, we use concepts developed in (de Campos et al., 2009; Jaakkola et al., 2010; Cussens, 2011), leading to a formulation as mixed integer linear program (MILP).", "startOffset": 47, "endOffset": 109}, {"referenceID": 5, "context": "For this purpose, we use concepts developed in (de Campos et al., 2009; Jaakkola et al., 2010; Cussens, 2011), leading to a formulation as mixed integer linear program (MILP).", "startOffset": 47, "endOffset": 109}, {"referenceID": 4, "context": "Similar as in (Cussens, 2010), we use a set of order constraints to enforce acyclicity in the directed graph, rather than the cluster constraints used in (Jaakkola et al.", "startOffset": 14, "endOffset": 29}, {"referenceID": 12, "context": "Similar as in (Cussens, 2010), we use a set of order constraints to enforce acyclicity in the directed graph, rather than the cluster constraints used in (Jaakkola et al., 2010).", "startOffset": 154, "endOffset": 177}, {"referenceID": 10, "context": "Similarly as in (Guo et al., 2005), the margin formulation needs one linear constraint per training sample and per competing class.", "startOffset": 16, "endOffset": 34}, {"referenceID": 12, "context": "We adopt the framework developed in (Jaakkola et al., 2010), where the aim was to maximize a generative score such as MDL/BIC or BDe.", "startOffset": 36, "endOffset": 59}, {"referenceID": 2, "context": "Note that P has super-exponentially many facets in the number of variables, which, in agreement with theory (Chickering, 1996), makes the problem hard.", "startOffset": 108, "endOffset": 126}, {"referenceID": 12, "context": "Therefore, in (Jaakkola et al., 2010; Cussens, 2011) the constraint in problem (3) was replaced with", "startOffset": 14, "endOffset": 52}, {"referenceID": 5, "context": "Therefore, in (Jaakkola et al., 2010; Cussens, 2011) the constraint in problem (3) was replaced with", "startOffset": 14, "endOffset": 52}, {"referenceID": 10, "context": "Jaakkola et al. (2010) solve the relaxed problem in the dual, where each in-active cluster constraint corresponds to a zero dual variable, and Cussens (2011) uses a cutting plane approach, iteratively adding violated cluster constraints.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "(2010) solve the relaxed problem in the dual, where each in-active cluster constraint corresponds to a zero dual variable, and Cussens (2011) uses a cutting plane approach, iteratively adding violated cluster constraints.", "startOffset": 127, "endOffset": 142}, {"referenceID": 10, "context": "The margin \u03b4 of the m sample is defined as (Guo et al., 2005):", "startOffset": 43, "endOffset": 61}, {"referenceID": 18, "context": "Motivated by SVMs, Pernkopf et al. (2012) defined a soft margin (SM) using the hinge loss:", "startOffset": 19, "endOffset": 42}, {"referenceID": 19, "context": "In (Pernkopf et al., 2012) this score was used for parameter learning using a conjugate gradient method, and in (Pernkopf et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 18, "context": ", 2012) this score was used for parameter learning using a conjugate gradient method, and in (Pernkopf et al., 2011; Pernkopf & Wohlmayr, 2012) the same score was used for inexact BN structure learning, based on greedy hillclimbing and simulated annealing.", "startOffset": 93, "endOffset": 143}, {"referenceID": 4, "context": "Similar constraints, using the same mechanism as depicted here, have been proposed for maximum-likelihood pedigree learning (Cussens, 2010).", "startOffset": 124, "endOffset": 139}, {"referenceID": 9, "context": "We used the 25 datasets already used in (Friedman et al., 1997), plus six additional datasets: \u201cabalone\u201d, \u201cadult\u201d, \u201ccar\u201d, \u201cmushroom\u201d, \u201cnursery\u201d, and \u201cspambase\u201d.", "startOffset": 40, "endOffset": 63}, {"referenceID": 9, "context": "We compared our methods with naive Bayes (NB), the tree-augmented naive Bayes (TAN) (Friedman et al., 1997) and with a BN with generatively trained structure, using MDL as score function (Suzuki, 1993).", "startOffset": 84, "endOffset": 107}, {"referenceID": 22, "context": ", 1997) and with a BN with generatively trained structure, using MDL as score function (Suzuki, 1993).", "startOffset": 87, "endOffset": 101}, {"referenceID": 10, "context": "Additionally, we could train the parameters of the resulting BNs in a discriminative way, to further improve classification results (Guo et al., 2005; Pernkopf et al., 2012).", "startOffset": 132, "endOffset": 173}, {"referenceID": 19, "context": "Additionally, we could train the parameters of the resulting BNs in a discriminative way, to further improve classification results (Guo et al., 2005; Pernkopf et al., 2012).", "startOffset": 132, "endOffset": 173}], "year": 2012, "abstractText": "Recently, there has been much interest in finding globally optimal Bayesian network structures. These techniques were developed for generative scores and can not be directly extended to discriminative scores, as desired for classification. In this paper, we propose an exact method for finding network structures maximizing the probabilistic soft margin, a successfully applied discriminative score. Our method is based on branch-and-bound techniques within a linear programming framework and maintains an any-time solution, together with worstcase sub-optimality bounds. We apply a set of order constraints for enforcing the network structure to be acyclic, which allows a compact problem representation and the use of general-purpose optimization techniques. In classification experiments, our methods clearly outperform generatively trained network structures and compete with support vector machines.", "creator": "LaTeX with hyperref package"}}}