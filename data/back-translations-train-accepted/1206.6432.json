{"id": "1206.6432", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Sparse Support Vector Infinite Push", "abstract": "In this paper, we address the problem of embedded feature selection for ranking on top of the list problems. We pose this problem as a regularized empirical risk minimization with $p$-norm push loss function ($p=\\infty$) and sparsity inducing regularizers. We leverage the issues related to this challenging optimization problem by considering an alternating direction method of multipliers algorithm which is built upon proximal operators of the loss function and the regularizer. Our main technical contribution is thus to provide a numerical scheme for computing the infinite push loss function proximal operator. Experimental results on toy, DNA microarray and BCI problems show how our novel algorithm compares favorably to competitors for ranking on top while using fewer variables in the scoring function.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (195kb)", "http://arxiv.org/abs/1206.6432v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CE stat.ML", "authors": ["alain rakotomamonjy"], "accepted": true, "id": "1206.6432"}, "pdf": {"name": "1206.6432.pdf", "metadata": {"source": "META", "title": "Sparse Support Vector Infinite Push", "authors": ["Alain Rakotomamonjy"], "emails": ["alain.rakoto@insa-rouen.fr"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "2. Infinite Push framework", "text": "In this section we present the Infinite Push loss function and the support vector Infinite Push optimization problem that interest us. Existence and uniqueness of solutions to the problem are also discussed."}, {"heading": "2.1. Infinite Push loss function", "text": "We limited ourselves to the case of the two-sided ranking problem, the aim of which is to learn a function which, in the light of a training set {xi} i = 1, xi Rd with m positive and n negative examples, provides higher values for positive examples than for negative ones. Learning such a function can be placed in an empirical, regulated risk mitigation framework, in which the loss function associated with risk is designed to favour higher values for positive examples. Typically, in such a context, the loss function focuses on the average paired point losses and it can be described as follows: L (f (\u00b7), S) = 1mn \u2211 i, jIf (x + i) \u2264 f (x \u2212 j) where I \u00b7 is the indicator function, S is a series of examples with known terms and f (\u00b7) is the rating function we want to evaluate. Several extensions of this loss function have recently been considered to account for errors that we have made at the top of the lists, adding more to the loss."}, {"heading": "2.2. Support Vector Infinite Push", "text": "We can define the empirical risk minimization problem (ERM) for learning the scoring function f (\u00b7), which we have chosen to be linear, so that f (x) = w \u2212 x. The loss function given in Equation (1) is non-convex and different convexifications suggested in the literature have led to different ERM frameworks and algorithms. We can mention, for example, relaxation by exponential loss leading to a boosting-like algorithm (Rudin, 2009). If the loss is used as convex relaxation, then we get the following support mechanisms such as the optimization problem problem: min w \u2212 x. \u2212 x. \u2212 x."}, {"heading": "3. Algorithm for sparse Support Vector Infinite Push", "text": "In this section we show how to effectively use the problems posed by the non-smooth lens function in Problem (2) and describe in detail the ADMM algo rithm that we propose to solve the sparse support vector infinite push problem."}, {"heading": "3.1. Deriving ADMM formulation", "text": "Before we turn to derivatives, we would like to mention that Douglas-Rachford splitting algorithm is tailored to minimize the sum of two non-smooth objective functions. To this end, we rewrite the optimization problem (2) as the following linear problem: min w, a \u00b2 (w) + max 1 \u2264 j \u2264 n (1m \u00b2 i (ai, j) +) ai, j = 1 \u2212 w T (x + i \u2212 x \u2212 j) (4), which can be any parity-inducing standard, any mixed standard (Bach et al., 2011) or the classic b \u00b2 regularization concept. Then, by adapting the matrix X (the lines of the form (x + i \u2212 j \u2212 j) T), the vector and the function (a) ak (a) = a mathematical term (m), k = 1 \u00b5m (w = 1), w (w)."}, {"heading": "3.2. Solving problem (6)", "text": "Depending on the shape of the tow, the tow optimization problem can be repeated as a crest return problem if another (probably known) regression term is taken into account. In the case of parity-inducing regulators (e.g. 1 standard), the problem must be solved numerically, and therefore each iteration of the ADMM approach involves the resolution of a lasso. Depending on the lasso algorithm used, one can benefit from a warm-up of the solution, since one lot is not expected to vary between two consecutive ADMM iterations. For the regulator of the 2 standard, the solution has a closed form of the solution method wk + 1 = (X + X + 1 standard), since one lot is not expected to vary between two successive ADMM iterations."}, {"heading": "3.3. Solving problem (7)", "text": "If we now assume that w and the Lagrangian multiplicators in the Lagrangian list have been fixed since 2001, the optimization problem with respect to (7) is ultimately as follows: ak + + + + + + (a) + + + + + + + + + + + + (a) + + + + + + + + + (a) + + + + + + + + + (a) + + + + + + + + + + + (a) + + + + + + + + + + + + (a) + + + + + + + + + + + + + + + (a) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + (a) + + + + + + + + + + + + (a) + + + + + + + + + + (a) + + + + + + + + + + + +) + + + + + + + + + + + + + + (a) + + + + + + + + + + + + + + + +) + + + + + + + + + + + + + + +) + + + + + + + + + + + + + + + + + + +) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +"}, {"heading": "3.4. Convergence analysis", "text": "The convergence of algorithm 1 to solve the original infinite push problem builds on the classic convergence results of the ADMM or Douglas-Rachford splitting algorithm (Eckstein & Bertsekas, 1992). Indeed, a direct application of theorem 8 in this paper tells us that our algorithm converges for each \u00b5 > 0 as long as the matrix X has the full column rank (condition met by most undegenerated problems for which d < m \u00b7 n) and that the arithmetic errors of problem (6) and problem (7) can be summed up. Practically, this latter condition means that the convergence criterion for these two problems should become increasingly narrow over the iterations."}, {"heading": "3.5. Computational complexity", "text": "The two most mathematically demanding parts of our algorithm for sparse infinite push is the lasso problem, which must be solved with each iteration and projection on the ball. For the lasso there are efficient algorithms, which scale linearly with the number of training examples. However, we can for example use the SpaRSa algorithm of Figueiredo et al. (Figueiredo et al., 2007) as well as the projection on100 150 200 300 300 350 350 40.450.50.650.7 # Training examplesra teo f itive sat # useful variables: 10, # noisy variables: 200.030.060.000.00 0.00 0,00.000.13L1 InfPush RFE InfPush Rank100 150 200 250 350 0.250.350.450.50.50.50.600.600.600.600.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.10.10.10.10.10.10.10.10.10.00.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.1000.100000.1000.1000.100000.1000.1000.100000.1000.1000.1000.1000.100000.1000.1000.1000.100000.1000.1000.100000.1000.10000000.1000.100000.1000.1000.1000.1000"}, {"heading": "4. Experiments", "text": "Our goal here is to provide empirical evidence that our method may be advantageous for problems with noisy or redundant characteristics compared to an infinite push approach that takes all characteristics into account. We also show that our embedded approach based on a sparsity-inducing standard provides higher accuracy at the top of the list compared to other methods for feature selection such as eliminating recursive characteristics (RFE) in an infinite push context. Note that we have not compared our methods with other ranking algorithms except SVMRank (Chapelle & Keerthi, 2010), since Agarwal (2011) has already demonstrated the superiority of the infinite push model over other methods for ranking positive instances at the top of the list, and because these methods such as SVMMAP do not have their sparse counterpart in the literature."}, {"heading": "4.1. Toy problem", "text": "rE \"s tis rf\u00fc ide f\u00fc for one,\" tS os os rf\u00fc ide rf\u00fc for one, for one, for one, for the other, for one, for one, for one, for the other, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one, for one"}, {"heading": "4.2. Real-world problems", "text": "These data sets are mainly related to DNA microarray analysis (colon, yeast) or derived from the UCI data store (ionosphere, sonar, spectaculum, wpbc) and the P300-based BCI speller. The same preprocessing was applied to these real data as to the toy data set. We compared a simple infinite push method that does not perform embedded variable selection, and an infinite push value of 1 SVM on our sparse push value. Comparison criteria are the rate of the positive examples placed above and the number of variables used by the scoring functions. Average results over 10 iterations were reported in Table 1. Results clearly show that our infinite push model is the model that achieves the best compromise between accuracy at the top of the list and variable selection."}, {"heading": "5. Conclusions", "text": "We have shown in this paper that the selection of embedded characteristics based on parity-inducing standards can be extended to loss functions that are themselves non-differentiable and intrinsically complex. For the sparse SVM infinite push, we have proposed an algorithm based on the alternative direction of multipliers, which alternatively solves a lasso (or related) problem and uses the proximal operator of the infinite push loss. To calculate this proximal operator, we have developed a novel algorithm based on the projection on the sphere 1-3. Our experimental results show that our sparse SVM infinite push can be compared favourably with other approaches in terms of the number of variables used in the model and the accuracy of the rank at the top of the list."}, {"heading": "Acknowledgments", "text": "This work is partially supported by the PASCAL2 Network of Excellence, ICT-216886, ANR Project ASAP ANR-09-EMER-001."}], "references": [{"title": "The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list", "author": ["S. Agarwal"], "venue": "In Proceedings of the SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Agarwal,? \\Q2011\\E", "shortCiteRegEx": "Agarwal", "year": 2011}, {"title": "Ranking chemical structures for drug discovery: A new machine learning approach", "author": ["S. Agarwal", "D. Dugar", "S. Sengupta"], "venue": "Journal of Chemical Information and Modeling,", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "Convex optimization with sparsity-inducing norms. In Optimization for Machine Learning", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": null, "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "Efficient algorithms for ranking with svms", "author": ["O. Chapelle", "S.S. Keerthi"], "venue": "Information Retrieval Journal,", "citeRegEx": "Chapelle and Keerthi,? \\Q2010\\E", "shortCiteRegEx": "Chapelle and Keerthi", "year": 2010}, {"title": "Tree-based ranking methods", "author": ["S. Cl\u00e9mencon", "N. Vayatis"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cl\u00e9mencon and Vayatis,? \\Q2009\\E", "shortCiteRegEx": "Cl\u00e9mencon and Vayatis", "year": 2009}, {"title": "A douglas- rachford splitting approach to nonsmooth convex variational signal recovery", "author": ["P. Combettes", "Pesquet", "J.-C"], "venue": "IEEE Journal Selected Topics Signal Processing,", "citeRegEx": "Combettes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Combettes et al\\.", "year": 2007}, {"title": "Signal recovery by proximal forward-backward splitting", "author": ["P. Combettes", "V. Wajs"], "venue": "Multiscale Modeling and Simulation,", "citeRegEx": "Combettes and Wajs,? \\Q2005\\E", "shortCiteRegEx": "Combettes and Wajs", "year": 2005}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Feature selection for ranking", "author": ["X. Geng", "Liu", "T.-Y", "T. Qin", "H. Lin"], "venue": "In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Geng et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Geng et al\\.", "year": 2007}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "Guyon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2002}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Joachims,? \\Q2002\\E", "shortCiteRegEx": "Joachims", "year": 2002}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Quattoni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "The p-norm push: A simple convex ranking algorithm that concentrates at the top of the list", "author": ["C. Rudin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin,? \\Q2009\\E", "shortCiteRegEx": "Rudin", "year": 2009}, {"title": "Fast projections onto l1,q-norm balls for grouped feature selection", "author": ["S. Sra"], "venue": "In Proceedings of the European Conference on Machine Learning,", "citeRegEx": "Sra,? \\Q2011\\E", "shortCiteRegEx": "Sra", "year": 2011}, {"title": "Convergence of block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "Journal of Optimization Theory and Application,", "citeRegEx": "Tseng,? \\Q2001\\E", "shortCiteRegEx": "Tseng", "year": 2001}, {"title": "Ranking with ordered weigthed pairwise classification", "author": ["N. Usunier", "D. Buffoni", "P. Gallinari"], "venue": "In Proceeding of the 26 International Conference on Machine Learning,", "citeRegEx": "Usunier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2009}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "Examples of applications in which ranking is central are information retrieval (Chapelle & Keerthi, 2010), drug discovery (Agarwal et al., 2010).", "startOffset": 122, "endOffset": 144}, {"referenceID": 10, "context": "(2003) or using Hinge loss (Joachims, 2002).", "startOffset": 27, "endOffset": 43}, {"referenceID": 7, "context": "Some of them aim at optimizing a pairwise ranking criterion using exponential loss, like the RankBoost of Freund et al. (2003) or using Hinge loss (Joachims, 2002).", "startOffset": 106, "endOffset": 127}, {"referenceID": 12, "context": "For this reason, several recently proposed algorithms focus on correctly ranking the best instances (Rudin, 2009; Agarwal, 2011).", "startOffset": 100, "endOffset": 128}, {"referenceID": 0, "context": "For this reason, several recently proposed algorithms focus on correctly ranking the best instances (Rudin, 2009; Agarwal, 2011).", "startOffset": 100, "endOffset": 128}, {"referenceID": 8, "context": "To the best of our knowledge, very few works have addressed the problem of feature selection in ranking (Geng et al., 2007) and in ranking on top of the list problems.", "startOffset": 104, "endOffset": 123}, {"referenceID": 11, "context": "We focus on the recent p-norm push loss function introduced by Rudin (2009) and more specifically, on the case where p = \u221e, denoted as an infinite push framework by Agarwal (2011).", "startOffset": 63, "endOffset": 76}, {"referenceID": 0, "context": "We focus on the recent p-norm push loss function introduced by Rudin (2009) and more specifically, on the case where p = \u221e, denoted as an infinite push framework by Agarwal (2011). In this latter work, Agarwal (2011) has also proposed a support vector like algorithm, named as support vector Infinite Push, which has been proved to perform better than competitors when the goal is to maximize the number of", "startOffset": 165, "endOffset": 180}, {"referenceID": 0, "context": "We focus on the recent p-norm push loss function introduced by Rudin (2009) and more specifically, on the case where p = \u221e, denoted as an infinite push framework by Agarwal (2011). In this latter work, Agarwal (2011) has also proposed a support vector like algorithm, named as support vector Infinite Push, which has been proved to perform better than competitors when the goal is to maximize the number of", "startOffset": 165, "endOffset": 217}, {"referenceID": 2, "context": "While typical methods dealing with sparsity suppose that the loss function is smooth (Bach et al., 2011), our optimization problem is challenging since both the loss function and the regularizer can be non-differentiable.", "startOffset": 85, "endOffset": 104}, {"referenceID": 15, "context": "Several extensions of this loss function have been recently considered in order to provide more importance to errors made on top of the lists, for instance by weighting the pairwise loss (Usunier et al., 2009) or by replacing the mean with some more appropriate functions.", "startOffset": 187, "endOffset": 209}, {"referenceID": 12, "context": "For this purpose, Rudin (2009) has introduced the Infinite Push loss function", "startOffset": 18, "endOffset": 31}, {"referenceID": 12, "context": "We can mention for instance the relaxation by means of exponential loss that yield to boosting-like algorithm (Rudin, 2009).", "startOffset": 110, "endOffset": 123}, {"referenceID": 0, "context": "The algorithm is based on a nice and clever gradient projection algorithm (Agarwal, 2011).", "startOffset": 74, "endOffset": 89}, {"referenceID": 0, "context": "Conversely, when considering the dual problem (3) as in Agarwal (2011), this property may be lost.", "startOffset": 56, "endOffset": 71}, {"referenceID": 2, "context": "where \u03a9(w) can be any sparsity inducing norm like the l1 norm, any mixed-norm (Bach et al., 2011) or the classical l2 regularization term.", "startOffset": 78, "endOffset": 97}, {"referenceID": 14, "context": "Before providing algorithmic details, we state here a proposition based on the work of Tseng (2001) that guarantees the soundness of the BCD algorithm.", "startOffset": 87, "endOffset": 100}, {"referenceID": 14, "context": "1 of Tseng (2001) concludes the proof.", "startOffset": 5, "endOffset": 18}, {"referenceID": 13, "context": "For solving this problem, we use classical result from convex analysis and proximal operator (Combettes & Wajs, 2005; Sra, 2011), which states that the proximal operator of a norm \u2016 \u00b7 \u2016 is", "startOffset": 93, "endOffset": 128}, {"referenceID": 11, "context": "This problem is easily tractable since projection of vector on a l1 \u2212 \u221e ball has been recently studied and several efficient algorithms proposed (Quattoni et al., 2009; Sra, 2011).", "startOffset": 145, "endOffset": 179}, {"referenceID": 13, "context": "This problem is easily tractable since projection of vector on a l1 \u2212 \u221e ball has been recently studied and several efficient algorithms proposed (Quattoni et al., 2009; Sra, 2011).", "startOffset": 145, "endOffset": 179}, {"referenceID": 11, "context": "the l1 \u2212 l\u221e ball of Quattoni et al. (2009) has a complexity of O(n log n) in the size of the vector to project.", "startOffset": 20, "endOffset": 43}, {"referenceID": 0, "context": "Note that we have not compared our methods to other ranking algorithms, except SVMRank (Chapelle & Keerthi, 2010), since Agarwal (2011) has already shown the superiority of the infinite push model on other methods for ranking positive instances on top of the list and because these methods such as SVMMAP does not have their sparse counterpart in the literature.", "startOffset": 121, "endOffset": 136}, {"referenceID": 9, "context": "Our RFE implementation follows exactly the same procedure as the one used for SVM RFE (Guyon et al., 2002), but replacing the SVM with the infinite push algorithm as proposed by Agarwal (2011).", "startOffset": 86, "endOffset": 106}, {"referenceID": 0, "context": ", 2002), but replacing the SVM with the infinite push algorithm as proposed by Agarwal (2011). This infinite push RFE bears strong resemblance with the backward elimination of Geng et al.", "startOffset": 79, "endOffset": 94}, {"referenceID": 0, "context": ", 2002), but replacing the SVM with the infinite push algorithm as proposed by Agarwal (2011). This infinite push RFE bears strong resemblance with the backward elimination of Geng et al. (2007). For a baseline comparison, we have also included an l1 SVM Rank.", "startOffset": 79, "endOffset": 195}, {"referenceID": 16, "context": "This is a well known issue of the l1 norm that can be overcome using an adaptive approach (Zou, 2006).", "startOffset": 90, "endOffset": 101}], "year": 2012, "abstractText": "In this paper, we address the problem of embedded feature selection for ranking on top of the list problems. We pose this problem as a regularized empirical risk minimization with p-norm push loss function (p = \u221e) and sparsity inducing regularizers. We leverage the issues related to this challenging optimization problem by considering an alternating direction method of multipliers algorithm which is built upon proximal operators of the loss function and the regularizer. Our main technical contribution is thus to provide a numerical scheme for computing the infinite push loss function proximal operator. Experimental results on toy, DNA microarray and BCI problems show how our novel algorithm compares favorably to competitors for ranking on top while using fewer variables in the scoring function.", "creator": "LaTeX with hyperref package"}}}