{"id": "1206.6435", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Rethinking Collapsed Variational Bayes Inference for LDA", "abstract": "We propose a novel interpretation of the collapsed variational Bayes inference with a zero-order Taylor expansion approximation, called CVB0 inference, for latent Dirichlet allocation (LDA). We clarify the properties of the CVB0 inference by using the alpha-divergence. We show that the CVB0 inference is composed of two different divergence projections: alpha=1 and -1. This interpretation will help shed light on CVB0 works.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (175kb)", "http://arxiv.org/abs/1206.6435v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["issei sato", "hiroshi nakagawa"], "accepted": true, "id": "1206.6435"}, "pdf": {"name": "1206.6435.pdf", "metadata": {"source": "CRF", "title": "Rethinking Collapsed Variational Bayes Inference for LDA", "authors": ["Issei Sato", "Hiroshi Nakagawa"], "emails": ["sato@r.dl.itc.u-tokyo.ac.jp", "n3@dl.itc.u-tokyo.ac.jp"], "sections": [{"heading": "1. Introduction", "text": "It is used to model the simultaneous occurrence of words by using latent variables referred to as topics in which a document is presented as a \"bag of words.\" It has a variety of applications in many areas. Originally, the variational inference of Bayes (VB) was used for learning VDA. The collapsed variation table Bayes (CVB) was developed as an alternative deterministic inference for LDA (Teh et al., 2007). The CVB inference is a variable inference that is improved by the marginalization of parameters as in a collapsed Gibbs sampler (Griffiths & Steyvers, 2004). (Sung et al., 2008) generalized the CVB inference for conjugate exponential family models called latent-space variational Bayes (LSVB)."}, {"heading": "2. Preliminaries", "text": "Suppose we have N documents, V vocabulary, and T topics. W = {wd} Nd = 1 stands for a set of documents, and z = {zd} Nd = 1 stands for a set of assigned topics. \u03b8d, t stands for the probability that the topic t will appear in document D. t, v stands for the probability that the word v will appear in the topic t.nd, t (z) = number of observations of the topic t in the document d. nd denotes the total number of words in the document d. nt, v (w, z) denotes the number of observations of the word v associated with the topic t and nt, \u00b7 (z) = number of observations of the topic t, v (w, z). For simplicity purposes, we denote them by nd, t, nt, v, and nt, \u00b7. The heading \"\\ d, i\" denotes the corresponding variables or counts with wd, i, and zd, i excluding, e.g. w\\ d, i, d, assignment\\ d, and nt, the order of the two (and z)."}, {"heading": "3. Overview of LDA", "text": "The following generation process is assumed at LDA: First, document-theme distribution \u03b8d and topic-word distribution k are generated using a T-dimensional vector and \u03b2 = (\u03b21, \u00b7 \u00b7 \u00b7, \u03b2V) is a V-dimensional vector. (2) Wallach et al. (Wallach et al., 2009) investigated the effects of the choice of the i-topic zd, i and the i-word wd, i: zd, i-multi (\u03b8d), wd, i-multi (zd, i). They found simulations in the Markov chain Monte Carlo (MCMC) where asymmetric and symmetric \u03b2-results lead to better predictive results for documents held."}, {"heading": "4. CVB/CVB0 inference for LDA", "text": "(Teh et al., 2007) proposed the CVB conclusion on LDA, inspired by the collapsed q = q = q = q = q (sampler) and showed that the CVB-LDA outperformed the VB-LDA in terms of perplexity; they merely introduced a variation of trailing q (z) by marginalizing the free energy of the CVB-LDA (s). (4) Thus, the updates for q (z) -LDA are obtained by using FCV B [q (z) = 1 q (zd) in terms of divergence (zd) distribution by taking the derivatives of FCV B [z). (zd) in terms of equation to zero: q (zd), i = t) and equation to zero: q (zd), i = t)."}, {"heading": "6. Local \u03b1-divergence projection", "text": "Suppose that the approximate distribution q (x) is fully factored. We derive the actualization q (xi) to minimize \u03b1 divergence as follows. If we take derivatives of \u03b1 divergence (12) with respect to q (xi) and set it to zero, we obtain the following fixed point titeration equations: q (xi) q [(p (x) q (x\\ i)) \u03b1] 1\u03b1 q (x\\ i) (18). In many cases this actualization is impracticable and therefore we introduce an approximation for Equation (18). Since Equation (18) isq (xi) q (p (x\\ i) q (x\\ i) p (x\\ i) p (x\\ i) q (x\\ i) q (x\\ i) q (x\\ i) q (x\\ i) q (x\\ i) q (x\\ i)."}, {"heading": "7. CVB0 as \u03b1-divergence projection", "text": "In this section, we derive the CVB0 conclusion by using the local alpha divergence = q = q = q = q q (V). First, we describe how the case \u03b1 = 1, i.e. EP, cannot be applied to the collapsed LDA. Second, we derive a divergence projection applicable to the collapsed LDA and explain the relationship between this projection and the CVB0 conclusion.We apply Eq (21) with \u03b1 = 1 (EP) to the collapsed LDA. (22) The update for q (zd) = argmin q (zd) = argmin q (zd, i) KL [p (zd, i | w, z\\ d, i) q (z\\ d, i)."}, {"heading": "8. Discussion", "text": "In this section we explain why the CVB0 conclusion exceeds the CVB0 conclusion (43). To summarize this discussion: In the CVB0 conclusion (44), the \"zero coercion effect\" is effective only with the nt, \u00b7\\\\ estimate, while the CVB conclusion works with the q (z) estimate. The previous section showed that the CVB0 conclusion (z) is composed of the three projections with a mixture of \u03b1 = 1 and \u03b1 (z)\\ d."}, {"heading": "9. Subspecies inspired by CVB0", "text": "In this section, we will consider other projection-based algorithms that help clarify the property q = q of the samples (\u03b2 q = VB = 0,9.1. CVB with (\u03b2 = 1) -divergenceFrom our point of view, the CVB0 conclusion consists of two divergence projections of different types: \u03b1 = 1, \u2212 1. We will consider the use of only \u03b1 = 1. To do this, we must calculate the analytical solution for this expectation (\u03b1 = 1) (zd, i = t) = E [1n\\ d, i t, \u00b7 V \u03b2] q (z\\ d, i). (48) Since we cannot derive the analytical solution for this expectation, we propose two approximation methods, the first being a stochastic approximation called a sample averaging determined by the specified number (\u03b1 = 1) (zd, i = t \u03b2 = t) = Accuracy (z = 11n\\ d, i t)."}, {"heading": "9.2. Type-base CVB0 Inference", "text": "We derive a type-based conclusion as an application of our framework. In a type-based conclusion, we calculate only the probability distribution for each type in a document, not for each symbol; this is advantageous for calculation costs and memory usage. We exclude all numbers of the word v from document d, which are denoted by the uppercase \"\\ d, v.\" The probability of the word v is w\\ d, v d and z\\ d, v + \u03b2 n (wd, v \u00b7 w\\ d, v, z\\ d, v, v) = T \u2211 t = 1n\\ d, v d, t + \u03b3tn\\ d, v + \u03b2 n (wd, v + \u03b2\\ d, v \u00b7 V \u03b2 (51). In addition, we have divergence (zd, v = t | w\\ d, v\\ d), v (v\\ d), v\\ v\\ d, v\\ v\\ v), v (v\\ v v, v, v, v), v (v)."}, {"heading": "10. Experiments", "text": "We compared CVB0 with its subspecies in terms of perplexity to investigate the effect of \u03b1 = -1. All results are averaged from five random initialization experimental runs. We set the number of iterations to 100 for each inferencing. We use a fixed point equation that is also able to move."}, {"heading": "11. Conclusion", "text": "We have shown that the CVB0 conclusion consists of (\u03b1 = 1, \u2212 1) divergence projections and that \u03b1 = \u2212 1 is similar to \u03b1 = 1 in LDA, which means that CVB0 is not affected by the zero constraint effect in LDA. Combining the marginalization of parameters and heterogeneous divergence projection is useful because it is easily applicable to other topic models learned from the collapsed Gibbs sampler. Future work will be to develop an extension of the online update, such as that of (Hoffman et al., 2010; Sato et al., 2011; Wang et al., 2011). From the relationship between EP and assumed density filtering, we can extend the local q divergence projection into an online algorithm d leading to the online CVB0 convergence analysis."}], "references": [{"title": "Approximate mean field for dirichlet-based", "author": ["Springer", "New York", "A. 1985. Asuncion"], "venue": null, "citeRegEx": "Springer et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Springer et al\\.", "year": 1985}, {"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Y.W. Teh"], "venue": "In Proceedings of the International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Asuncion et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proc Natl Acad Sci U S A,", "citeRegEx": "Griffiths and Steyvers,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers", "year": 2004}, {"title": "Online learning for latent dirichlet allocation", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Bach", "Francis R"], "venue": "In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Expectation-Propagation for the generative aspect model", "author": ["T. Minka", "J. Lafferty"], "venue": "In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Minka and Lafferty,? \\Q2002\\E", "shortCiteRegEx": "Minka and Lafferty", "year": 2002}, {"title": "Divergence measures and message passing", "author": ["Minka", "Thomas"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Minka and Thomas.,? \\Q2005\\E", "shortCiteRegEx": "Minka and Thomas.", "year": 2005}, {"title": "Estimating a dirichlet distribution", "author": ["Minka", "Thomas P"], "venue": "Technical report, Microsoft,", "citeRegEx": "Minka and P.,? \\Q2000\\E", "shortCiteRegEx": "Minka and P.", "year": 2000}, {"title": "The dlr hierarchy of approximate inference", "author": ["Rosen-Zvi", "Michal", "Jordan", "Michael I", "Yuille", "Alan L"], "venue": "In Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2005}, {"title": "Deterministic single-pass algorithm for lda", "author": ["Sato", "Issei", "Kurihara", "Kenichi", "Nakagawa", "Hiroshi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sato et al\\.", "year": 2010}, {"title": "Latent-space variational bayes", "author": ["Sung", "Jaemo", "Ghahramani", "Zoubin", "Bang", "Sung-Yang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sung et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2008}, {"title": "A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation", "author": ["Teh", "Yee Whye", "Newman", "David", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Collapsed variational inference for hdp", "author": ["Teh", "Yee Whye", "Kurihara", "Kenichi", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Teh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2008}, {"title": "A generalized predictive criterion for model selection", "author": ["M. Trottini", "F. Spezzaferri"], "venue": "In Canadian Journal of Statisticse,", "citeRegEx": "Trottini and Spezzaferri,? \\Q2002\\E", "shortCiteRegEx": "Trottini and Spezzaferri", "year": 2002}, {"title": "Rethinking lda: Why priors matter", "author": ["Wallach", "Hanna", "Mimno", "David", "McCallum", "Andrew"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["Wang", "Chong", "Paisley", "John William", "Blei", "David M"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Information geometric measurements of generalisation", "author": ["Zhu", "Huaiyu", "Rohwer", "Richard"], "venue": "Technical report, Aston University,", "citeRegEx": "Zhu et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1995}], "referenceMentions": [{"referenceID": 10, "context": "The collapsed variational Bayes (CVB) inference was developed as an alternative deterministic inference for LDA (Teh et al., 2007).", "startOffset": 112, "endOffset": 130}, {"referenceID": 9, "context": "(Sung et al., 2008) generalized the CVB inference for conjugate-exponential family models, called latent-space variational Bayes (LSVB) inference.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "(Teh et al., 2007) used a second-order Taylor expansion to perform the integrals.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "(Asuncion et al., 2009; Asuncion, 2010) proposed another approximation that uses only the zeroorder information, called the CVB0 inference.", "startOffset": 0, "endOffset": 39}, {"referenceID": 13, "context": "(Wallach et al., 2009) explored the effects of choosing \u03b3 and \u03b2 in LDA.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "(Teh et al., 2007) proposed the CVB inference to LDA inspired by the collapsed Gibbs sampler and showed that the CVB-LDA outperformed the VB-LDA in terms of perplexity.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "(Asuncion et al., 2009) showed the usefulness of an approximation using only zero-order information, called the CVB0 inference.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "In the case \u03b1 = 1, the update (20) is similar to belief propagation, and the factorized neighbors algorithm (Rosen-Zvi et al., 2005).", "startOffset": 108, "endOffset": 132}, {"referenceID": 1, "context": "(Asuncion et al., 2009) showed that CVB0-LDA with \u03b2 = 0.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "The comparison metric we used for document modeling was the perplexity used by (Teh et al., 2007; 2008) that indicates the prediction performance for held-out words.", "startOffset": 79, "endOffset": 103}, {"referenceID": 3, "context": "Future work is to develop an online-update extension, such as that by (Hoffman et al., 2010; Sato et al., 2010; Wang et al., 2011).", "startOffset": 70, "endOffset": 130}, {"referenceID": 8, "context": "Future work is to develop an online-update extension, such as that by (Hoffman et al., 2010; Sato et al., 2010; Wang et al., 2011).", "startOffset": 70, "endOffset": 130}, {"referenceID": 14, "context": "Future work is to develop an online-update extension, such as that by (Hoffman et al., 2010; Sato et al., 2010; Wang et al., 2011).", "startOffset": 70, "endOffset": 130}], "year": 2012, "abstractText": "We propose a novel interpretation of the collapsed variational Bayes inference with a zero-order Taylor expansion approximation, called CVB0 inference, for latent Dirichlet allocation (LDA). We clarify the properties of the CVB0 inference by using the \u03b1divergence. We show that the CVB0 inference is composed of two different divergence projections: \u03b1 = 1 and \u22121. This interpretation will help shed light on CVB0 works.", "creator": " TeX output 2012.05.21:0841"}}}