{"id": "1206.6436", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Efficient Structured Prediction with Latent Variables for General Graphical Models", "abstract": "In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (552kb)", "http://arxiv.org/abs/1206.6436v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexander g schwing", "tamir hazan", "marc pollefeys", "raquel urtasun"], "accepted": true, "id": "1206.6436"}, "pdf": {"name": "1206.6436.pdf", "metadata": {"source": "META", "title": "Efficient Structured Prediction with Latent Variables for General Graphical Models", "authors": ["Alexander G. Schwing", "Tamir Hazan"], "emails": ["aschwing@inf.ethz.ch", "tamir@ttic.edu", "pomarc@inf.ethz.ch", "rurtasun@ttic.edu"], "sections": [{"heading": "1. Introduction", "text": "In recent years, the number of newcomers to Germany has multiplied and the number of newcomers has multiplied."}, {"heading": "2. Loss minimization with latent variables", "text": "In this section, we propose a general model for reducing losses with latent variables. (...) Note that S (...) can depend on the example of x-dimensional features x-dimensional features x-dimensional features. (...) We are interested in finding the parameters x-dimensional features x-dimensional features x-dimensional features. (...) In this paper, we will discuss the weakly inflated setting in which we obtain a training platform. (...) We are interested in finding the parameters x-dimensional features x-dimensional features x-dimensional features x-dimensional features, the possible selection results x-dimensional features x-dimensional features x-dimensional features x-dimensional features, x-dimensional features x-dimensional features, x-dimensional features, x-dimensional features, x-dimensional features, x-dimensional features, x-dimensional features, x-dimensional features, x-dimensional features, x-dimensional features, x-dimensional features."}, {"heading": "3. Approximate latent structured loss minimization", "text": "The unlimited minimization problem in Eq (5) is difficult because it contains a sum of convex and concave terms with exponentially large sums. To make the minimization more tractable, one usually follows a convex double and a non-convex bi-linear term as described in the following claims. Program 1 Approximate structured prediction with latent variables, which are then referred to as q (x, y)."}, {"heading": "4. Message Passing Algorithm", "text": "Before deriving an algorithm to solve the problem that we are discussing, we start with the discussion about the characteristics of the approach. (...) For the counting of numbers and annexation factors that are greater than zero, it is necessary that we (...) have all the data (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which we (...), which (...), which (...), which we (...), which (...), which (...), which we (..., which we (...), which (...), which we (...), which (..., which we (...), which we (...), which we (...), which we (...), which we (..., which we (...), which we (...), which we (...), which we (..., which we (...), which we (...), which we (..., which we (...), which we (...), which we (..., which we (...), which we (...), which we (..., which we (...), which we (..., which we (...), which we (..., which we (...), which we (...), which we (... (...), which we (..., which we (...), which we (...), which we (..., which we (...), which we (... (...), which we (... (...), which we (...), which we (... (..., which we (...), which we (...), which we (..., which we (...), which we (..., which we (...), which we (... (...), which we (..., which we (...), which we (..., which we (...), which"}, {"heading": "5. Experiments", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "6. Related Work and Discussion", "text": "In fact, it is a matter of a way in which it is a matter of the realization of goals and objectives which are based on the limits of the realization of goals and objectives. (...) It is about the realization of goals and objectives. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals. (... It is about the realization of goals. (...) It is about the realization of goals. (...) It is about the realization of goals."}, {"heading": "7. Conclusion", "text": "We have proposed a framework that combines HCRF and latently structured SVMs. Subsequently, we have constructed an approximation of the resulting insoluble optimization problem using local entropies and derived an algorithm for general graphs that effectively utilizes the graphical model structure imposed by the characteristics. We have demonstrated the effectiveness of our approach to a segmentation task as well as the prediction of the 3D layout from individual images. We plan to expand this work in two directions in the sense of v.d.Maaten et al. (2011) by addressing non-linear structured predictions with latent variables and by examining relationships to deep faith networks."}], "references": [{"title": "Training Structural SVMs when Exact Inference is Intractable", "author": ["T. Finley", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Finley and Joachims,? \\Q2008\\E", "shortCiteRegEx": "Finley and Joachims", "year": 2008}, {"title": "Object Detection with Grammer Models", "author": ["R. Girshick", "P. Felzenszwalb", "D. McAllester"], "venue": "In NIPS,", "citeRegEx": "Girshick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2011}, {"title": "Norm-Product Belief Propagtion: Primal-Dual Message-Passing for LPRelaxation and Approximate-Inference", "author": ["T. Hazan", "A. Shashua"], "venue": "Trans. on Information Theory,", "citeRegEx": "Hazan and Shashua,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Shashua", "year": 2010}, {"title": "A Primal-Dual MessagePassing Algorithm for Approximated Large Scale Structured Prediction", "author": ["T. Hazan", "R. Urtasun"], "venue": "In NIPS,", "citeRegEx": "Hazan and Urtasun,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Urtasun", "year": 2010}, {"title": "Recovering the Spatial Layout of Cluttered Rooms", "author": ["V. Hedau", "D. Hoiem", "D. Forsyth"], "venue": "In ICCV,", "citeRegEx": "Hedau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hedau et al\\.", "year": 2009}, {"title": "Recovering surface layout from an image", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": null, "citeRegEx": "Hoiem et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hoiem et al\\.", "year": 2007}, {"title": "An Introduction to Variational Methods for Graphical Models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Learning to Cluster Using High Order Graphical Models with Latent Variables", "author": ["N. Komodakis"], "venue": "In ICCV,", "citeRegEx": "Komodakis,? \\Q2011\\E", "shortCiteRegEx": "Komodakis", "year": 2011}, {"title": "Structured Learning with Approximate Inference", "author": ["A. Kulesza", "F. Pereira"], "venue": "In NIPS,", "citeRegEx": "Kulesza and Pereira,? \\Q2008\\E", "shortCiteRegEx": "Kulesza and Pereira", "year": 2008}, {"title": "Self-Paced Learning for Latent Variable Models", "author": ["P. Kumar", "B. Packer", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Geometric Reasoning for Single Image Structure Recovery", "author": ["D.C. Lee", "M. Hebert", "T. Kanade"], "venue": "In CVPR,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces", "author": ["D.C. Lee", "A. Gupta", "M. Hebert", "T. Kanade"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Entropy and Margin Maximization for Structured Output Learning", "author": ["P. Pletscher", "C.S. Ong", "J.M. Buhmann"], "venue": "In ECML PKDD,", "citeRegEx": "Pletscher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pletscher et al\\.", "year": 2010}, {"title": "Distributed Message Passing for Large Scale Graphical Models", "author": ["A.G. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun"], "venue": "In CVPR,", "citeRegEx": "Schwing et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schwing et al\\.", "year": 2011}, {"title": "Efficient Structured Prediction for 3D Indoor Scene Understanding", "author": ["A.G. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun"], "venue": "In CVPR,", "citeRegEx": "Schwing et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwing et al\\.", "year": 2012}, {"title": "On the Convergence of the Concave-Convex Procedure", "author": ["B. Sriperumbudur", "G. Lanckriet"], "venue": "In NIPS,", "citeRegEx": "Sriperumbudur and Lanckriet,? \\Q2009\\E", "shortCiteRegEx": "Sriperumbudur and Lanckriet", "year": 2009}, {"title": "Structured Output Learning with Higher Order Loss Functions", "author": ["D. Tarlow", "R.S. Zemel"], "venue": "In AISTATS,", "citeRegEx": "Tarlow and Zemel,? \\Q2012\\E", "shortCiteRegEx": "Tarlow and Zemel", "year": 2012}, {"title": "Learning Structured Prediction Models: A Large Margin Approach", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "In ICML,", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Support Vector Learning for Interdependent and Structured Output Spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Discriminative Learning with Latent Variables for Cluttered Indoor Scene Understanding", "author": ["H. Wang", "S. Gould", "D. Koller"], "venue": "In ECCV,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Learning Structural SVMs with Latent Variables", "author": ["Yu", "C.-N. J", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "The Concave-Convex Procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation,", "citeRegEx": "Yuille and Rangarajan,? \\Q2003\\E", "shortCiteRegEx": "Yuille and Rangarajan", "year": 2003}], "referenceMentions": [{"referenceID": 12, "context": "To control the variance of the log-linear probability model we follow (Hazan & Urtasun, 2010; Pletscher et al., 2010) and introduce a temperature parameter , i.", "startOffset": 70, "endOffset": 117}, {"referenceID": 6, "context": ", in (Jordan et al., 1999) like the concave-convex procedure (CCCP) (Yuille & Rangarajan, 2003; Sriperumbudur & Lanckriet, 2009) by separating the cost function into two functions f1(w, \u03bb) and f3(d), convex in their parameters and a bilinear term f2(w, d) connecting the two.", "startOffset": 5, "endOffset": 26}, {"referenceID": 4, "context": "Following existing approaches (Hedau et al., 2009; Lee et al., 2010), we employ F = 55 features based on geometric context (GC) and orientation maps (OM) and refer the interested reader to (Hoiem et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 11, "context": "Following existing approaches (Hedau et al., 2009; Lee et al., 2010), we employ F = 55 features based on geometric context (GC) and orientation maps (OM) and refer the interested reader to (Hoiem et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 5, "context": ", 2010), we employ F = 55 features based on geometric context (GC) and orientation maps (OM) and refer the interested reader to (Hoiem et al., 2007) and (Lee et al.", "startOffset": 128, "endOffset": 148}, {"referenceID": 10, "context": ", 2007) and (Lee et al., 2009) for respective details.", "startOffset": 12, "endOffset": 30}, {"referenceID": 5, "context": "fully weakly Error (Hoiem et al., 2007) 209 0 28.", "startOffset": 19, "endOffset": 39}, {"referenceID": 4, "context": "9% (Hedau et al., 2009) 209 0 21.", "startOffset": 3, "endOffset": 23}, {"referenceID": 19, "context": "2% (Wang et al., 2010) 209 0 22.", "startOffset": 3, "endOffset": 22}, {"referenceID": 11, "context": "2% (Lee et al., 2010) 209 0 18.", "startOffset": 3, "endOffset": 21}, {"referenceID": 4, "context": "Comparison to state-of-the-art on the layout data set of (Hedau et al., 2009).", "startOffset": 57, "endOffset": 77}, {"referenceID": 14, "context": "6% (Schwing et al., 2012).", "startOffset": 3, "endOffset": 25}, {"referenceID": 1, "context": "The weak-label structured SVM presented in (Girshick et al., 2011) is obtained when = 0 and p = 2.", "startOffset": 43, "endOffset": 66}, {"referenceID": 1, "context": "The weak-label structured SVM presented in (Girshick et al., 2011) is obtained when = 0 and p = 2. For = 1, p = 2, ` \u2261 0 \u2200x, y, \u0125 and ` \u2261 0 \u2200x, y, \u0177, \u0125 we recover the likelihood formulation presented by Quattoni et al. (2007). For general , but without latent variables, i.", "startOffset": 44, "endOffset": 226}, {"referenceID": 18, "context": "The influence of approximate inference algorithms on structured SVMs (Taskar et al., 2004; 2005; Tsochantaridis et al., 2004) without latent variables has been investigated by (Finley & Joachims, 2008; Kulesza & Pereira, 2008), where they reported a \u201cgenerally poor performance\u201d when combining belief propagation and structured SVMs.", "startOffset": 69, "endOffset": 125}, {"referenceID": 7, "context": "To address efficiency Komodakis (2011) suggested to use a small number of CRF iterations.", "startOffset": 22, "endOffset": 39}, {"referenceID": 6, "context": "For LSSVMs, the standard structured SVM loss was applied and adapted by Komodakis (2011). Girshick et al.", "startOffset": 72, "endOffset": 89}, {"referenceID": 1, "context": "Girshick et al. (2011) proposed to introduce a second loss function into the \u2018latent variable prediction problem\u2019 while Tarlow & Zemel (2012) investigate the impact of higher order loss functions.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Girshick et al. (2011) proposed to introduce a second loss function into the \u2018latent variable prediction problem\u2019 while Tarlow & Zemel (2012) investigate the impact of higher order loss functions.", "startOffset": 0, "endOffset": 142}, {"referenceID": 1, "context": "Girshick et al. (2011) proposed to introduce a second loss function into the \u2018latent variable prediction problem\u2019 while Tarlow & Zemel (2012) investigate the impact of higher order loss functions. Kumar et al. (2010) proposed the self paced learning algorithm, which starts with \u201ceasy\u201d examples before gradually adding more difficult ones.", "startOffset": 0, "endOffset": 217}, {"referenceID": 13, "context": "To parallelize message passing one could employ (Schwing et al., 2011).", "startOffset": 48, "endOffset": 70}], "year": 2012, "abstractText": "In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.", "creator": "LaTeX with hyperref package"}}}