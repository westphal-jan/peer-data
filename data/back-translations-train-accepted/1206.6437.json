{"id": "1206.6437", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Large Scale Variational Bayesian Inference for Structured Scale Mixture Models", "abstract": "Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in non-factorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial \"sparse\" methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with non-factorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (750kb)", "http://arxiv.org/abs/1206.6437v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["young-jun ko", "matthias w seeger"], "accepted": true, "id": "1206.6437"}, "pdf": {"name": "1206.6437.pdf", "metadata": {"source": "META", "title": "Large Scale Variational Bayesian Inference for  Structured Scale Mixture Models", "authors": ["Young Jun Ko", "Matthias Seeger"], "emails": ["youngjun.ko@epfl.ch", "matthias.seeger@epfl.ch"], "sections": [{"heading": "1. Introduction", "text": "Most such methods incorporate simple factor principles based on individual coefficients or groups. Other key benefits derive from modelling higher wavelength dependencies through structured, non-factorial distributions."}, {"heading": "1.1. Related Work", "text": "Crouse et al. (1998) use Gaussian mixing potentials as well as a hidden Markov tree based on discrete mixing indicators and estimate latent signal and mixing parameters based on expectation maximization. They estimate parameters through nonlinear optimization. Papandreou et al. (2008) use a hidden Markov tree and Gaussian mixing potentials based on an overcomplete wave representation and estimate signals and parameters through Viterbi training. None of these indicators uses Bayesian inference beyond the image or non-Gaussian potentials. He et al. (2010) uses play-and-slab potentials beyond the indicators."}, {"heading": "2. Structured Image Model", "text": "In this case, we are able to reconstruct the image (which we do not do); in this case, we are able to reconstruct the image (which we do not do); in this case, we are able to reconstruct the image (which we do not do); in this case, we are able to reconstruct the image (which we do not do); in this case, we are able to reconstruct the image (which we do not do); in this case, we are able to reconstruct the image (which we do not do); in this case, we are not able to reconstruct the image (which we do); in this case, we are not able to reconstruct the image (which we do); and in this case, we are not able to reconstruct the image (which we do)."}, {"heading": "3. Large Scale Variational Inference", "text": "In this section, we derive a scalable algorithm for calculating variable approximations based on everything we do. < Q (1) for a non-factorial scale mix before (4). There are evidently strong interdependencies between the components of u or s and, unlike previous work (He et al., 2010), we do not need factoring assumptions between them. The high idea behind our approach is iterative decoupling. We combine a standard variational reduction that allows us to address the latter by propagation of belief) with the double loop framework of (Seeger & Nickisch, 2011), which means decoupling and covariance calculations via u. The latter provides a computer-based reduction to the least square optimizations and the Gaussian sampling method, which is critical for scalability."}, {"heading": "3.1. MAP Estimation. Factorial Priors", "text": "Importantly, we can obtain scalable algorithms for estimating the MAP or for inferences with factorial or non-factorial priorities by making minor simplistic changes, otherwise using exactly the same underlying code. First, if we use a factorial prior of the form P (u) = \u0430j tj (sj), we simply eliminate t1j, set < \u03b4j > = 0, and eliminate Q (\u03b4 | y) altogether. The inner loop now consists of a single punished problem with the fewest squares (6). Second, the MAP estimate is achieved by simply setting z = 0, skipping variance calculations, and performing a single outer loop iteration. MAP estimation for a non-factorial prior runs in an expectation-maximizing manner, alternating between punished smallest squares (PLS) and faith propagation (Crouse et al, 1998)."}, {"heading": "3.2. Learning Prior Hyperparameters", "text": "In this section, we will show how to learn hyperparameters in an automatic process that leads us to optimistic hyperparameters for the respective learning processes. To get the best performance, it is necessary to equip an image before P (u) (whether factual or not) with a significant number of hyperparameters that need to be adapted to the respective problem. < p (0), or (3), or (1), or (1). In our experiments with non-factorial priors, we use L = 8 scale levels, resulting in 16 hyperparameters (one for each level l and a low / high state r): too many to be reasonably set by non-Bayesian methods such as cross-validation. In this section, we will show how to learn hyperparameters in an automatic Bayesian method."}, {"heading": "3.3. Student\u2019s T Potentials", "text": "When applied to models with student t-potentials (3), the algorithm only requires detailed, non-convex PLS problems (6) that must be solved in the inner loop. Frequently used first-order solvers can fail dramatically on non-convex problems. As we use a double-loop strategy anyway, it is easier and much more robust to use additional constraints to obtain a convex inner loop problem, an idea that has already been described in Seeger & Nickisch, 2011, and applied to students t-potentials, but our applications here and our learning method for hyperparameters are novel. Imagine the representation (5) of t (s)."}, {"heading": "4. Experiments", "text": "Our results are averaged over 77 commonly used images (gray scale, 256 x 256), a dataset4 from (Seeger & Nickisch, 2008).Our implementation is based on the glm-ie toolbox (www.mloss.org / software / view / 269 /).We compare 8 methods: MAP estimation (MAP) vs. variable inference (VB), factual previous (fact) vs. latent tree scale mixture before (tree) and Laplacian (Lap) vs. student t-potentials.The Lap-tree model uses two laplace potentials (sj), t1j (sj) with different hyperparameters (fact) vs. latent tree scale mixture before (tree).The Lap-tree model uses two laplace potentials (sj), t0j (sj), t1j (sj)) with different hyperparameters (L), one hyperparameter."}, {"heading": "4.1. Denoising", "text": "We add Gaussian noise of variance \u03c32 = 0.01 to each image (with pixel values ui [0, 1]). All methods use the correct value \u03c32 in their probability. Note that in this case Gaussian variances z can be calculated exactly without cost. Namely, 4 thanks to H. Nickisch for providing the data. We have added 2 more images. 5 In inpainting, the missing pixels are set to mean (yi).A (\u03c0) = \u03c3 \u2212 2I + BT\u041aB, where BTB = I, i.e. thatz = diag \u2212 1 (BA (\u03c0) \u2212 1BT) = (\u03c3 \u2212 21 + \u03c0) \u2212 1.Therefore, Perturb & MAP, the prevailing costs for VB in general, is not required. The results are in Table 1. For this application, differences between MAP and VB reconstruction are not significant. On the other hand, the non-factorial previous PSNR improves somewhat. Hyperparameter learning significantly improves the performance of VB, especially when not used."}, {"heading": "4.2. Inpainting", "text": "The design matrix is X = IJ, \u00b7, the noise deviation is set at \u03c32 = 10 \u2212 5. Results are shown in Table 2. Since the PSNR does not always correlate well with visual quality, we show a series of images in Figure 2, Figure 3, Figure 4.VB posterior mean predictions are clear superior to MAP reconstruction, and VB with non-factorial latent tree before performance best. While the VB with a factorial laplace prior (lap-fact) has PSNR values similar to the VB-Lap-tree, the visual appearance of the results is clearly superior to the latter (additional results included in the supplementary material support these results).The additional runtime compared to the MAP estimate, mainly due to the estimation of the deviations z, does not pay off for these problems.6 There is no justification for maximizing the posterior."}, {"heading": "5. Conclusion", "text": "We introduced a dual loop algorithm for Bayesian variational conclusions in linear models with priors of the non-factorial scale mix, based on a latent discrete tree distribution. Our method can work on the same scales as the MAP estimate, but its (approximate) posterior mean prediction significantly and consistently exceeds the posterior mode in a number of inpainting problems. Both selective smoothing by predictive variances and coupling of the wavelength coefficient across the latent tree help eliminate artifacts that plague the results of the MAP estimate and inference by factorial priors. Free hyperparameters are automatically learned by marginal probability maximization, which is folded into the variation optimization.In future work, we will try to adapt our large-scale inference methodology to more complex hierarchical models that have latent (continuous) portilla."}, {"heading": "Acknowledgments", "text": "The support from a DFG grant SE 2008 / 1-1 (AOBJ 578593) and an ERC Starting Grant (277815 - SCALABIM) is gratefully accepted."}], "references": [{"title": "Sparse signal acquisition and recovery with graphical models", "author": ["V. Cevher", "P. Indyk", "L. Carin", "R. Baraniuk"], "venue": "IEEE Sig. Proc. Mag.,", "citeRegEx": "Cevher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cevher et al\\.", "year": 2010}, {"title": "Waveletbased statistical signal processing using hidden Markov models", "author": ["M. Crouse", "R. Nowak", "R. Baraniuk"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "Crouse et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Crouse et al\\.", "year": 1998}, {"title": "Tree-structured compressive sensing with variational Bayesian analysis", "author": ["L. He", "H. Chen", "L. Carin"], "venue": "IEEE Sig. Proc. Letters,", "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "Variational EM algorithms for non-Gaussian latent variable models", "author": ["J. Palmer", "D. Wipf", "K. Kreutz-Delgado", "B. Rao"], "venue": "In NIPS", "citeRegEx": "Palmer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2006}, {"title": "Gaussian sampling by local perturbations", "author": ["G. Papandreou", "A. Yuille"], "venue": "In NIPS", "citeRegEx": "Papandreou and Yuille,? \\Q2010\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2010}, {"title": "Image inpainting with a wavelet domain hidden markov tree model", "author": ["G. Papandreou", "P. Maragos", "A. Kokaram"], "venue": "In ICASSP,", "citeRegEx": "Papandreou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2008}, {"title": "Image denoising using Gaussian scale mixtures in the wavelet domain", "author": ["J. Portilla", "V. Strela", "M. Wainwright", "E. Simoncelli"], "venue": "IEEE Trans. Image Proc.,", "citeRegEx": "Portilla et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Portilla et al\\.", "year": 2003}, {"title": "Compressed sensing and Bayesian experimental design", "author": ["M. Seeger", "H. Nickisch"], "venue": "In ICML", "citeRegEx": "Seeger and Nickisch,? \\Q2008\\E", "shortCiteRegEx": "Seeger and Nickisch", "year": 2008}, {"title": "Large scale Bayesian inference and experimental design for sparse linear models", "author": ["M. Seeger", "H. Nickisch"], "venue": "SIAM J. Imag. Sciences,", "citeRegEx": "Seeger and Nickisch,? \\Q2011\\E", "shortCiteRegEx": "Seeger and Nickisch", "year": 2011}, {"title": "A new view of automatic relevance determination", "author": ["D. Wipf", "S. Nagarajan"], "venue": "In NIPS", "citeRegEx": "Wipf and Nagarajan,? \\Q2008\\E", "shortCiteRegEx": "Wipf and Nagarajan", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010).", "startOffset": 219, "endOffset": 287}, {"referenceID": 0, "context": "While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010).", "startOffset": 219, "endOffset": 287}, {"referenceID": 1, "context": "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).", "startOffset": 171, "endOffset": 234}, {"referenceID": 5, "context": "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).", "startOffset": 171, "endOffset": 234}, {"referenceID": 2, "context": "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).", "startOffset": 171, "endOffset": 234}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities.", "startOffset": 0, "endOffset": 221}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training.", "startOffset": 0, "endOffset": 411}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training. None of these employ Bayesian inference over the image or non-Gaussian potentials. He et al. (2010) use spike-and-slab prior potentials with a hidden Markov tree over the indicators.", "startOffset": 0, "endOffset": 668}, {"referenceID": 6, "context": "Beyond marginals, image statistics exhibit complex dependencies, and capturing these in non-factorial priors can lead to further leaps in performance (Portilla et al., 2003).", "startOffset": 150, "endOffset": 173}, {"referenceID": 6, "context": "Even though the sj are approximately uncorrelated for natural images, it is well known that there are substantial causal dependencies between coefficients in neighbouring levels (Portilla et al., 2003).", "startOffset": 178, "endOffset": 201}, {"referenceID": 6, "context": "1 Our model is somewhat simpler than that of (Portilla et al., 2003).", "startOffset": 45, "endOffset": 68}, {"referenceID": 2, "context": "obviously strong dependencies between components of u or s, and in contrast to previous work (He et al., 2010), we do not require any factorization assumptions between them.", "startOffset": 93, "endOffset": 110}, {"referenceID": 3, "context": "For the purpose of image priors, we restrict ourselves to even, super-Gaussian potentials trj(sj) (Palmer et al., 2006), which can be represented as", "startOffset": 98, "endOffset": 119}, {"referenceID": 1, "context": "MAP estimation for a non-factorial prior proceeds in an expectationmaximization fashion, alternating between penalized least squares (PLS) and belief propagation (Crouse et al., 1998).", "startOffset": 162, "endOffset": 183}, {"referenceID": 1, "context": "For each run, we initialize hyperparameters \u03b8 as in (Crouse et al., 1998), by maximizing the prior probability of the raw data y (for the tree cases, this involves a few steps of expectation maximization), then optimize them by minimizing \u03c6.", "startOffset": 52, "endOffset": 73}, {"referenceID": 6, "context": "In future work, we will try to adapt our large scale inference methodology to more complex hierarchical models, featuring non-Gaussian continuous latent variables (Portilla et al., 2003).", "startOffset": 163, "endOffset": 186}], "year": 2012, "abstractText": "Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in non-factorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial \u201csparse\u201d methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with nonfactorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors.", "creator": "LaTeX with hyperref package"}}}