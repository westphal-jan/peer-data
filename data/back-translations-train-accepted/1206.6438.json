{"id": "1206.6438", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation", "abstract": "We study the problem of unsupervised domain adaptation, which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain. Many existing approaches first learn domain-invariant features and then construct classifiers with them. We propose a novel approach that jointly learn the both. Specifically, while the method identifies a feature space where data in the source and the target domains are similarly distributed, it also learns the feature space discriminatively, optimizing an information-theoretic metric as an proxy to the expected misclassification error on the target domain. We show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain. Empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (422kb)", "http://arxiv.org/abs/1206.6438v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yuan shi", "fei sha"], "accepted": true, "id": "1206.6438"}, "pdf": {"name": "1206.6438.pdf", "metadata": {"source": "META", "title": "Information-Theoretical Learning of  Discriminative Clusters for Unsupervised Domain Adaptation", "authors": ["Yuan Shi", "Fei Sha"], "emails": ["yuanshi@usc.edu", "feisha@usc.edu"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Discriminative Clustering for Domain Adaptation", "text": "At the core of our approach is the assumption of discriminatory clustering. Specifically, we assume that in a suitable attribute space 1) separation. Data in the source and target domains are clustered in a discriminatory manner, with the cluster Ids corresponding to class labels; 2) alignment. The clusters from the two domains corresponding to the same label are geometrically close. Figure 1 illustrates these two assumptions and how they can be used for adaptation. It can be assumed that the assumptions are more \"loose\" than those in existing adaptation work. Specifically, they do not imply that boundary distributions are equal across domains and certainly do not imply the same posterior distributions. In fact, these assumptions are slightly satisfactory in application."}, {"heading": "3. Proposed Approach", "text": "In the following, we are given N-marked instances from the source domain: {(xs, ys)}, where xs, X, RD, and ys take a value from the C class name: ys, Y = {1, 2,.., C}. We also have M-unmarked instances from the target domain: {xt}, where xt, X, and xs have the same domain X, i.e. the same dimensionality. Extensions to more general cases are possible, analogous to (Kulis et al., 2011). Our goal is to construct a classifier for f: x, X, and Y. To overcome this difficulty, we want the classifier to perform well on the target domain DT, which xt is sampled. This is a problem by nature, as we do not have labels from the target domain."}, {"heading": "3.1. Conditional models in the feature space", "text": "We look at the latent attribute space induced by a linear transformation L-Rd-D. In the new attribute space, we use k-nearest neighbors (kNN) to classify how we have assumed that the data form well-separated clusters. In addition, we choose k = 1 to avoid cross-validation of this parameter.The square distance between two points xi and xj in this attribute space is thus defined by d2ij = xi-Lxj-22 = (xi \u2212 xj) TM (xi \u2212 xj) (1), with M = LTL defining a (low-level) Mahalanobis distance metric in the original space.In view of a point xi and a series of data points {xj}, we use the following model pij = e \u2212 d 2 ij-j-j 6 = i e \u2212 d2ij (2) to define the conditional probability that xj-labels will be Xi's next-class neighbor to gold-based learning model, including a number of conditioners used in many contextual studies."}, {"heading": "3.2. Discriminative clustering in the source", "text": "In order to derive a classifier that can perform well in the target domain, we would certainly need to perform well in the source domain classifier, since we have assumed that the two domains have similar cluster structures. Therefore, our first desideratum is to minimize the expected classification error in the source domain if we classify it by 1-NN. This error is estimated based on the empirical average of leave-one-out accuracy for a certain point xs in the source domain DS: \u03b5s = 1 \u2212 1N \u2211 c p-sc\u03b4sc (4) Note that if we only minimize this error and ignore the target domain, we end up with the metric learning technique (Goldberger et al., 2004)."}, {"heading": "3.3. Discriminative clustering in the target", "text": "Since we do not have labels on the target domain, we cannot define the expected classification error, as we did in eq. (4) for the source domain. How discriminatory can we be without using labels? Consider an instance xt from the target domain and all instances {xs} from the source domain, the conditional model pts of eq. (2) leads to the probability of having a particular xs as the closest neighbor of xt. If we use this conditional model as well as the source designations to calculate the rear Y as in eq. (3) The conditional model pts of eq. (2) would not be the correct rear label for the target domain. However, if our assumptions about two sets of clusters that are geometrically close together actually hold in the dataset, then the estimate p."}, {"heading": "3.4. Discriminability: source versus target", "text": "The previous discussion of discriminatory clustering in the target domain depends on the assumption that q q should not be too far apart for the source and the target domains.We quantify this term more precisely below. Conceptually, this concept is similar to the idea in existing work to make marginal distributions across domains similar. Why is such a term desirable? To use the source domain designations as proxies to estimate the rear probabilities for the target data (as in eq. (3)), we would share the source and the target domain some common probabilities in the attribute space. In particular, we would consider the case where we consider two instances that classify xt and xt \u00b2 from the target domain to be equivalent if there is the same label c when there is a lot of labeled source data in class c in their neighborhoods. Then, we would expect with a high probability that xt and xt \u00b2 are the next domain in each other domain as well."}, {"heading": "3.5. Learning and model selection", "text": "We have described three information theory variables: classification accuracy on the source domain \u03b5S of eq. (4), discriminatory clustering on the target domain It (X; Y) of eq. (5), and discriminability between the source domain and the target domain Ist (X; Q) of eq. (6). These variables have been derived from our assumptions about the source and target domains, especially the discriminatory cluster structures. They are all in the linear transformation L. We learn the optimal L by balancing these variables with the following optimization problems \u2212 It (X; Y) + \u03bbIs (X; Q), which are subject to the track (LTL), \u2264 d (7), where the limitation is to control the scale of distances calculated using L. The regulation coefficient must be minimized by the following optimization forms. We select the optimal validity, the minimum reaches the source domain."}, {"heading": "3.6. Numerical Optimization", "text": "Equation (7) is non-convex optimization. We use gradient-based methods to optimize the objective function. Theoretically, the methods are susceptible to a local optimum, but we use heuristics to initialize: either the PCA of the target domain data or the factorization of a discriminatively trained metric on the source data, such as the most distant one (LMNN) (Weinberger & Saul, 2009). In most cases, these heuristics work well and result in vastly improved results via initialization points. Details are described in the Supplementary Material."}, {"heading": "3.7. Extensions", "text": "If the target domain has a few marked instances, this is called the semi-monitored fit problem. Our approach can be easily expanded to include the marked target domain instances. Details, including experimental results, are described in the supplementary material."}, {"heading": "4. Experimental Results", "text": "We evaluate the proposed method using two benchmark tasks: object recognition and sentiment analysis of product ratings. We compare the method with baselines and other recently proposed methods for unattended domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011). In the Supplementary Material we report on results of semi-monitored adaptation where the target domain has some marked instances."}, {"heading": "4.1. Setup", "text": "Object recognition. We use four databases of object images: Caltech-256 (Griffin et al., 2007), Amazon (images from the catalogues of online retailers), webcam (low-resolution images from web cameras), and DSLR (high-resolution images from digital SLR cameras). The last three sets of data have been examined in (Gopalan et al., 2011; Saenko et al., 2010). Caltech-256 is added to increase the diversity of domains. We treat each dataset as a domain. There are 10 common object categories that deal with the subject, calculator, computer keyboard, computer monitor, laptop 101, touring bike, and video projector. There are 2533 images in total, with 8 to 151 images per domain."}, {"heading": "4.2. Results on unsupervised adaptation", "text": "In fact, most of us are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) Most of us have no idea what they are doing. (...) Most of us have no idea what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing. (...) Most of us do not know what they are doing."}, {"heading": "5. Related Work", "text": "The information theory approach has been applied to semi-supervised learning (Grandvalet & Bengio, 2005), where the basic idea is to reduce the confusion (among possible labels) of unmarked data by classifiers trained on the marked data. However, they assumed that the data came from the same distribution, so there is no need to learn a domain invariant attribute space. Rastrow et al. described an information theory criterion for model selection in domain fitting (Rastrow et al., 2010). Model selection is a challenging problem when cross-validation is not possible due to the lack of marked data in the target domain. However, their approach is two-step: they refine model parameters on unmarked data by minimizing the conditional entropy of the label function from the original model, which is matched to the marked source data."}, {"heading": "6. Conclusion", "text": "We propose a one-step approach that collectively learns a domain invariant trait space and optimizes information theoretical metrics directly related to the non-discriminatory classification of the target domain. Our empirical results support the validity of our modelling assumptions that data is clustered in a discriminatory manner in both source and target domains. We show that existing approaches in which learning traits are disconnected from learning discriminatory classifiers can be significantly improved by taking into account cluster structures. For future work, we plan to investigate discriminatory learning of non-linear trait transformations for domain adaptation. It is known that this work was partially supported by DARPA D11AP00278, NSF IIS-1065243 and a USC Annenberg Fellowship (Y. Shi)."}], "references": [{"title": "SURF: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": null, "citeRegEx": "Bay et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bay et al\\.", "year": 2006}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": null, "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "Discriminative learning for differing training and test distributions", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "In Prof. of ICML, pp", "citeRegEx": "Bickel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In Proc. of ACL,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Semi-supervised learning, volume 2. MIT press", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A Zien"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Domain adaptation for statistical classifiers", "author": ["H. Daum\u00e9 III", "D. Marcu"], "venue": "JAIR, 26:101\u2013126,", "citeRegEx": "III and Marcu,? \\Q2006\\E", "shortCiteRegEx": "III and Marcu", "year": 2006}, {"title": "A methodology for information theoretic feature extraction", "author": ["J.W. Fisher III", "J.C. Principe"], "venue": "In Proc. IEEE World Congress on Comp. Intell.,", "citeRegEx": "III and Principe,? \\Q1998\\E", "shortCiteRegEx": "III and Principe", "year": 1998}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of ICML,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Discriminative clustering by regularized information maximization", "author": ["R. Gomes", "A. Krause", "P. Perona"], "venue": "In NIP,", "citeRegEx": "Gomes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2010}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "In Proc. of ICCV,", "citeRegEx": "Gopalan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2011}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "NIPS, 17:529\u2013236,", "citeRegEx": "Grandvalet and Bengio,? \\Q2005\\E", "shortCiteRegEx": "Grandvalet and Bengio", "year": 2005}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "Technical report, California Institute of Technology,", "citeRegEx": "Griffin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Griffin et al\\.", "year": 2007}, {"title": "Stochastic neighbor embedding", "author": ["G. Hinton", "S.T. Roweis"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Hinton and Roweis,? \\Q2002\\E", "shortCiteRegEx": "Hinton and Roweis", "year": 2002}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["J. Huang", "A.J. Smola", "A. Gretton", "K.M. Borgwardt", "B. Scholkopf"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "In Proc. of CVPR,", "citeRegEx": "Kulis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "Proc. of COLT,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowl. Engi.,", "citeRegEx": "Pan and Yang,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Trans. Neur. Nets.,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Dataset Shift in Machine Learning", "author": ["J. Qui\u00f1onero-Candela", "M. Sugiyama", "A. Schwaighofer"], "venue": null, "citeRegEx": "Qui\u00f1onero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qui\u00f1onero.Candela et al\\.", "year": 2009}, {"title": "Unsupervised model adaptation using informationtheoretic criterion", "author": ["A. Rastrow", "F. Jelinek", "A. Sethy", "B. Ramabhadran"], "venue": "In Proc. HLT-NAACL,", "citeRegEx": "Rastrow et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rastrow et al\\.", "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In Proc. of ECCV,", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "J. of Stat. Plan. and Infer.,", "citeRegEx": "Shimodaira,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira", "year": 2000}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "JMLR, 10:207\u2013244,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "Techniques for addressing learning problems with mismatched distributions are often referred as domain adaptation, or sometimes transfer learning (Daum\u00e9 III & Marcu, 2006; Pan & Yang, 2010; Qui\u00f1onero-Candela et al., 2009).", "startOffset": 146, "endOffset": 221}, {"referenceID": 5, "context": "This goal sets domain adaptation apart from semi-supervised learning, whose primary goal is to improve the performance on the labeled data with unlabeled data (Chapelle et al., 2006).", "startOffset": 159, "endOffset": 182}, {"referenceID": 23, "context": "For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same.", "startOffset": 33, "endOffset": 92}, {"referenceID": 2, "context": "For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same.", "startOffset": 33, "endOffset": 92}, {"referenceID": 15, "context": "For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same.", "startOffset": 33, "endOffset": 92}, {"referenceID": 19, "context": "Other works have also followed similar paradigms (Pan et al., 2011; Gopalan et al., 2011).", "startOffset": 49, "endOffset": 89}, {"referenceID": 11, "context": "Other works have also followed similar paradigms (Pan et al., 2011; Gopalan et al., 2011).", "startOffset": 49, "endOffset": 89}, {"referenceID": 3, "context": "In the structural correspondence learning, the original features are first augmented with features that are more likely to be domain invariant and then a classifier is trained (Blitzer et al., 2006).", "startOffset": 176, "endOffset": 198}, {"referenceID": 8, "context": "Alternatively, in deep learning architecture for domain adaptation, the augmenting features are highly nonlinear transformation of the original ones (Glorot et al., 2011).", "startOffset": 149, "endOffset": 170}, {"referenceID": 1, "context": "Theoretical analysis have showed that the loss on the target domain for any labeling functions depends on the difference between the marginal distributions, thus justifying the need to identify a feature space such that the two domains look alike to each other (Ben-David et al., 2007; Mansour et al., 2009).", "startOffset": 261, "endOffset": 307}, {"referenceID": 17, "context": "Theoretical analysis have showed that the loss on the target domain for any labeling functions depends on the difference between the marginal distributions, thus justifying the need to identify a feature space such that the two domains look alike to each other (Ben-David et al., 2007; Mansour et al., 2009).", "startOffset": 261, "endOffset": 307}, {"referenceID": 16, "context": "Extensions to more general cases are possible, analogous to (Kulis et al., 2011).", "startOffset": 60, "endOffset": 80}, {"referenceID": 9, "context": "The above conditional model has been used in many contexts, including metric learning (Goldberger et al., 2004), dimensionality reduction (Hinton & Roweis, 2002), etc.", "startOffset": 86, "endOffset": 111}, {"referenceID": 9, "context": "Note that, if we minimize this error only and ignore the target domain, we will arrive at the metric learning technique in (Goldberger et al., 2004).", "startOffset": 123, "endOffset": 148}, {"referenceID": 10, "context": "(Gomes et al., 2010; Dhillon et al., 2003).", "startOffset": 0, "endOffset": 42}, {"referenceID": 4, "context": "We mention in passing that it is found that the accuracy of a binary domain classifier reflects similarities between domains (Blitzer et al., 2007), thus approximating the original intractable combinatorial measure of similarities (Ben-David et al.", "startOffset": 125, "endOffset": 147}, {"referenceID": 1, "context": ", 2007), thus approximating the original intractable combinatorial measure of similarities (Ben-David et al., 2007).", "startOffset": 91, "endOffset": 115}, {"referenceID": 11, "context": "We compare the method to baselines and other recently proposed ones for unsupervised domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011).", "startOffset": 103, "endOffset": 165}, {"referenceID": 3, "context": "We compare the method to baselines and other recently proposed ones for unsupervised domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011).", "startOffset": 103, "endOffset": 165}, {"referenceID": 19, "context": "We compare the method to baselines and other recently proposed ones for unsupervised domain adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011).", "startOffset": 103, "endOffset": 165}, {"referenceID": 13, "context": "We use four databases of object images: Caltech-256 (Griffin et al., 2007), Amazon (images from online merchants\u2019s catalogues), Webcam (low-resolution images by web cameras), and DSLR (high-resolution images by digital SLR cameras).", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "The last three datasets were studied in (Gopalan et al., 2011; Saenko et al., 2010).", "startOffset": 40, "endOffset": 83}, {"referenceID": 22, "context": "The last three datasets were studied in (Gopalan et al., 2011; Saenko et al., 2010).", "startOffset": 40, "endOffset": 83}, {"referenceID": 22, "context": "Following the experimental protocols in previous work (Saenko et al., 2010), we extract SURF features (Bay et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 0, "context": ", 2010), we extract SURF features (Bay et al., 2006) and encode each image with a 800bin histogram (the codebook is trained from a subset of Amazon images).", "startOffset": 34, "endOffset": 52}, {"referenceID": 4, "context": "We use the dataset that consists of Amazon product reviews on four product types: kitchen appliances, DVDs, books and electronics (Blitzer et al., 2007).", "startOffset": 130, "endOffset": 152}, {"referenceID": 19, "context": "\u2022 Transfer Component Analysis (TCA) (Pan et al., 2011).", "startOffset": 36, "endOffset": 54}, {"referenceID": 3, "context": "\u2022 Structural Correspondence Learning (SCL) (Blitzer et al., 2006).", "startOffset": 43, "endOffset": 65}, {"referenceID": 11, "context": "\u2022 Geodesic Flow Subspaces (GFS) (Gopalan et al., 2011).", "startOffset": 32, "endOffset": 54}, {"referenceID": 22, "context": "\u2022 Metric Learning (Metric) (Saenko et al., 2010).", "startOffset": 27, "endOffset": 48}, {"referenceID": 21, "context": "described an information-theoretical based criterion for model selection in domain adaptation (Rastrow et al., 2010).", "startOffset": 94, "endOffset": 116}, {"referenceID": 10, "context": "Our work is also related to the recent study of regularized information maximization for discriminative clustering (Gomes et al., 2010).", "startOffset": 115, "endOffset": 135}], "year": 2012, "abstractText": "We study the problem of unsupervised domain adaptation, which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain. Many existing approaches first learn domain-invariant features and then construct classifiers with them. We propose a novel approach that jointly learn the both. Specifically, while the method identifies a feature space where data in the source and the target domains are similarly distributed, it also learns the feature space discriminatively, optimizing an informationtheoretic metric as an proxy to the expected misclassification error on the target domain. We show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain. Empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies.", "creator": "LaTeX with hyperref package"}}}