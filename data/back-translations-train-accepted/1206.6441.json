{"id": "1206.6441", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Topic Model for Melodic Sequences", "abstract": "We examine the problem of learning a probabilistic model for melody directly from musical sequences belonging to the same genre. This is a challenging task as one needs to capture not only the rich temporal structure evident in music, but also the complex statistical dependencies among different music components. To address this problem we introduce the Variable-gram Topic Model, which couples the latent topic formalism with a systematic model for contextual information. We evaluate the model on next-step prediction. Additionally, we present a novel way of model evaluation, where we directly compare model samples with data sequences using the Maximum Mean Discrepancy of string kernels, to assess how close is the model distribution to the data distribution. We show that the model has the highest performance under both evaluation measures when compared to LDA, the Topic Bigram and related non-topic models.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (485kb)", "http://arxiv.org/abs/1206.6441v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.IR stat.ML", "authors": ["athina spiliopoulou", "amos j storkey"], "accepted": true, "id": "1206.6441"}, "pdf": {"name": "1206.6441.pdf", "metadata": {"source": "META", "title": "A Topic Model for Melodic Sequences", "authors": ["Athina Spiliopoulou", "Amos Storkey"], "emails": ["a.spiliopoulou@ed.ac.uk", "a.storkey@ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Modelling the complexity of music is an interesting problem for machine learning. In Western music, pieces are typically composed according to a system of musical organization that reflects the musical structure as one of the foundations of music. Nevertheless, the characterization of this structure is particularly difficult, since it depends not only on the realization of several musical elements, but also on the relation of these elements within individual time frames and time frames. This leads to an infinite number of possible variations, which are typically built according to a single musical form."}, {"heading": "2. Background", "text": "Lavrenko & Pickens (2003) propose Markov Random Fields (MRFs) for modeling polyphonic music; the model is very general, but in order to remain detectable, much information is discarded, making it less suitable for realistic music. Weiland et al. (2005) propose a Hierarchical Hidden Markov Model (HHMM) for pitch; the model has three internal states predefined according to the structure of the music genre studied; Eck & Lapalme propose an LSTM Recurrent Neural Network for modeling melodies; the network is conditioned on the chord and certain previous time steps predefined according to the structure of the music genre."}, {"heading": "3. The Variable-gram Topic Model", "text": "In this section we will introduce the Variable-gram topic model, which we will later apply to melodic sequences. In the context of music modeling, documents correspond to pieces of music and words correspond to notes. The Variable-gram topic model extends the latent dirichlet allocation (LDA) by the Dirichlet model with variable length (Dirichlet-VMM) (Spiliopoulou & Storkey, 2011) for parameterizing the topical distributions over words. We will start with a description of the Dirichlet VMM."}, {"heading": "3.1. The Dirichlet-VMM", "text": "The conditional probability distribution of the next symbol is modeled in a context in which the length of the context varies according to what we actually observe. Long contexts, which frequently occur in the data, are used during the prediction, while for rare species their shorter counterparts are used. Similar to a VMM, the model is represented by a suffix tree that stores contexts as paths starting at the root node; the deeper a node in the tree the longer the corresponding context is considered. L is the depth of the tree, the maximum allowable length for a context. The tree is not complete; only contexts that occur frequently enough in the data and provide useful information for predicting the next symbol are stored."}, {"heading": "3.2. Introducing Latent Topics", "text": "The question we have to ask ourselves is whether we will be able to do what we do. (...) The question is whether we will do it when we do it. (...) The question we ask ourselves is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it. (...) The question is whether we will do it."}, {"heading": "3.3. Inference & Learning", "text": "The overall probability of the model underlying a set of hyperparameters is P (\u03c9, z, \u03b2, \u03b2, \u03b2, p, p, p, p, p, p, p, p, p, p, p, p). (4) Using these independence relations and the conjugation of the Dirichlet theme to the multinomial procedure, we can use all parameters independently of each other and equal for the entire procedure P (wt, d, p, k, k, d, p). (4) Using these independence relations and the conjugation of the dirichlet to the multinomial procedure, we can use the model parameters N\u03b1, p, p and p, each of which yield a closed form solution for the common predictive probability of a corpus, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, k, p, p, p, p, p, p, p, p"}, {"heading": "4. Experiments", "text": "In the following section, we evaluate the variable gram topic model by comparing its performance with the LatentDirichlet allocation, the Bigram topic model, and the Dirichlet VMM. First, we consider a next-step predictive task commonly used for music context evaluation (Lavrenko & Pickens, 2003; Paiement et al., 2009; Begleiter et al., 2004) Although the predictive probability is an indication of the model's performance, only certain aspects of what a model has learned are examined. More specifically, the probability of logic decreases sharply when a model matches, but it does not punish a model as strong that assigns many of its probability mass to improbable configurations. This is problematic because, in the uncontrolled learning of complex data, it is common for a model to have subordinate properties."}, {"heading": "4.1. Experimental Setup", "text": "For our experiments we use a dataset of MIDI files comprising 264 Scottish and Irish reels from the Nottingham Folk Music Database. Approximately half of the pieces are on the G major scale and the rest on the D major scale and all pieces have 4 / 4 meters. We use the representation of Spiliopoulou & Storkey (2011), where time is discredited in eight tones and the MIDI values are assigned to a 26 multinomic variable, with 24 values representing pitch (C4-B5) and 2 special values representing \"silence\" and \"continuation.\" This representation is shown in Figure 3.to determine the hyperparameters, \u03b1 and \u03b2, of the theme models, we use a 10x cross validation method and perform grid searches over the product space of the values {0.01, 1, 5, 10, 50, 100}. We present the results for theme models with 5, 10 and 50 topics."}, {"heading": "4.2. Next-Step Prediction Task", "text": "This year it has come to the point where it will be able to reconsider the aforementioned lcihsrVo, where it will be able to reconsider itself, \"he says.\" It's the way it is, \"he says.\" But it's the way it is, \"he says.\" It's the way it is, \"he says,\" it's the way it is, \"he says."}, {"heading": "4.3. Maximum Mean Discrepancy of String Kernels", "text": "In this section, we present a new approach to evaluating the model generation. We use string kernels and the maximum mean discrepancy (Gretton et al., 2006) to estimate the distance between the model distribution, Q, and the actual \"theoretical\" data distribution. Then, we quantify the distance between the two populations by comparing the intrapulation similarity values to the interpopulation results. A small distance indicates that a model has many of the different substructures that occur in the data. It cannot evaluate the generalization properties of a model, but it identifies the fit by measuring how close model generations are to the data sequences."}, {"heading": "4.4. Qualitative evaluation", "text": "An attractive feature of topic models when applied to NLP tasks is that they discover meaningful themes. In this section, we examine aspects of latent theme allocation and the parameters derived from it in the variable gram theme model and analyze them in terms of musical characteristics. Figure 6 shows a scatter chart showing how often each topic has been assigned in pieces from the G key (x-axis) and in pieces from the D key (y-axis) after the Gibbs sampler has converged. The model has learned to differentiate the key because the theme allocations in this plot are negatively correlated, meaning that each topic tends to be assigned in pieces from a single key. Figure 7 shows the conditional distributions across two themes used in pieces from the D key for four different contexts. The first context is the empty string, i.e. the distribution of the root node of the dirichlet corresponds to the respective time component."}, {"heading": "5. Discussion", "text": "Using two evaluation objectives, we showed that the model exceeds a number of related methods for modeling melodic sequences. Both evaluations showed that in this context, although latent issues improve performance, the need for a systematic time model is not overcome. In addition, we presented a novel method for evaluating model performance, in which we use the MMD of string cores calculated between data sequences and model samples to directly evaluate model distribution.This evaluation yielded the same comparative results as a prediction in a next step, although it deals with various aspects of model behavior.Looking at the erroneous cores in Figure 5, we can visualize the progress made by a model and \"how much\" of the structure has not yet been captured. Finally, it is interesting to examine the aspect of novelty in music. Pearce & Wiggins (2004) moves in this direction by using the cross-product of two previously constructed models, where the data is only used for a different task and cannot be constructed for the sequence."}, {"heading": "Acknowledgments", "text": "The authors thank Iain Murray and the anonymous reviewers for their useful comments."}], "references": [{"title": "On prediction using variable order Markov models", "author": ["R. Begleiter", "R. El-Yaniv", "G. Yona"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Begleiter et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Begleiter et al\\.", "year": 2004}, {"title": "Using machine-learning methods for musical style modeling", "author": ["S. Dubnov", "G. Assayag", "O. Lartillot", "G. Bejerano"], "venue": null, "citeRegEx": "Dubnov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dubnov et al\\.", "year": 2003}, {"title": "Learning musical structure directly from sequences of music", "author": ["D. Eck", "J. Lapalme"], "venue": "Technical report, Universite\u0301 de Montreal,", "citeRegEx": "Eck and Lapalme,? \\Q2008\\E", "shortCiteRegEx": "Eck and Lapalme", "year": 2008}, {"title": "A kernel method for the two-sample problem", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2006}, {"title": "Polyphonic music modeling with random fields", "author": ["V. Lavrenko", "J. Pickens"], "venue": "In ACM Multimedia,", "citeRegEx": "Lavrenko and Pickens,? \\Q2003\\E", "shortCiteRegEx": "Lavrenko and Pickens", "year": 2003}, {"title": "Mismatch string kernels for discriminative protein", "author": ["C.S. Leslie", "E. Eskin", "A. Cohen", "J. Weston", "W.S. Noble"], "venue": "classification. Bioinformatics,", "citeRegEx": "Leslie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2004}, {"title": "A hierarchical Dirichlet language model", "author": ["D.J.C. Mackay", "L.C.B. Peto"], "venue": "Natural Language Engineering,", "citeRegEx": "Mackay and Peto,? \\Q1995\\E", "shortCiteRegEx": "Mackay and Peto", "year": 1995}, {"title": "Predictive models for music", "author": ["J.F. Paiement", "Y. Grandvalet", "S. Bengio"], "venue": "Connection Science,", "citeRegEx": "Paiement et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Paiement et al\\.", "year": 2009}, {"title": "Improved methods for statistical modelling of monophonic music", "author": ["M. Pearce", "G. Wiggins"], "venue": "Journal of New Music Research,", "citeRegEx": "Pearce and Wiggins,? \\Q2004\\E", "shortCiteRegEx": "Pearce and Wiggins", "year": 2004}, {"title": "The power of amnesia", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "Ron et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ron et al\\.", "year": 1994}, {"title": "Comparing probabilistic models for melodic sequences", "author": ["A. Spiliopoulou", "A.J. Storkey"], "venue": "In ECML/PKDD,", "citeRegEx": "Spiliopoulou and Storkey,? \\Q2011\\E", "shortCiteRegEx": "Spiliopoulou and Storkey", "year": 2011}, {"title": "Fast and space efficient string kernels using suffix arrays", "author": ["C.H. Teo", "S.V.N. Vishwanathan"], "venue": "In ICML,", "citeRegEx": "Teo and Vishwanathan,? \\Q2006\\E", "shortCiteRegEx": "Teo and Vishwanathan", "year": 2006}, {"title": "Topic modeling: Beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "In ICML, pp. 977\u2013984", "citeRegEx": "Wallach,? \\Q2006\\E", "shortCiteRegEx": "Wallach", "year": 2006}, {"title": "Learning musical pitch structures with hierarchical hidden Markov models", "author": ["M. Weiland", "A. Smaill", "P. Nelson"], "venue": "Technical report, University of Edinburgh,", "citeRegEx": "Weiland et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weiland et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "The second is the Maximum Mean Discrepancy (MMD) (Gretton et al., 2006) of string kernels computed between model samples and test-data sequences.", "startOffset": 49, "endOffset": 71}, {"referenceID": 12, "context": "Weiland et al. (2005) propose a Hierarchical Hidden Markov Model (HHMM) for pitch.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Weiland et al. (2005) propose a Hierarchical Hidden Markov Model (HHMM) for pitch. The model has three internal states that are predefined according to the structure of the music genre examined. Eck & Lapalme (2008) propose an LSTM Recurrent Neural Network for modelling melody.", "startOffset": 0, "endOffset": 216}, {"referenceID": 7, "context": "Paiement et al. (2009) provide an interesting approach that incorporates musical knowldege in the melody modelling task.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Dubnov et al. (2003) propose two dictionary-based prediction methods, Incremental Parsing (IP) and Prediction Suffix Trees (PSTs), for modelling melodies with a Variable-Length Markov model (VMM).", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Begleiter et al. (2004) study six different alogrithms for training a VMM.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Begleiter et al. (2004) study six different alogrithms for training a VMM. These differ in the way they handle the counting of occurences, the smoothing of unobserved events and the variable-length modelling. Spiliopoulou & Storkey (2011) propose a Bayesian formulation of the VMM, the Dirichlet-VMM, for the problem of melody modelling.", "startOffset": 0, "endOffset": 239}, {"referenceID": 0, "context": "Begleiter et al. (2004) study six different alogrithms for training a VMM. These differ in the way they handle the counting of occurences, the smoothing of unobserved events and the variable-length modelling. Spiliopoulou & Storkey (2011) propose a Bayesian formulation of the VMM, the Dirichlet-VMM, for the problem of melody modelling. The model is shown to significantly outperform a VMM trained using the PST algorithm. Finally, an interesting application of dictionary-based predictors in the music context is presented in Pearce & Wiggins (2004). They describe a multiple viewpoint system comprising a cross-product of Prediction by Partial Match (PPM) models.", "startOffset": 0, "endOffset": 552}, {"referenceID": 9, "context": "The Probabilistic Suffix Tree algorithm for constructing a VMM tree is detailed in Ron et al. (1994).", "startOffset": 83, "endOffset": 101}, {"referenceID": 12, "context": "If we consider only first order (` = L = 1) dependencies, instead of variable order ones, then we retrieve the Bigram Topic model of Wallach (2006). Algorithm 1 Generative Process for the Variablegram Topic model.", "startOffset": 133, "endOffset": 148}, {"referenceID": 7, "context": "First, we consider a next-step prediction task, which is commonly used for evaluation in the music context (Lavrenko & Pickens, 2003; Paiement et al., 2009; Begleiter et al., 2004).", "startOffset": 107, "endOffset": 180}, {"referenceID": 0, "context": "First, we consider a next-step prediction task, which is commonly used for evaluation in the music context (Lavrenko & Pickens, 2003; Paiement et al., 2009; Begleiter et al., 2004).", "startOffset": 107, "endOffset": 180}, {"referenceID": 3, "context": "To address this issue, we introduce a novel framework for model evaluation which employs string kernels and the Maximum Mean Discrepancy (Gretton et al., 2006) to compare samples from the model with test sequences.", "startOffset": 137, "endOffset": 159}, {"referenceID": 3, "context": "We employ string kernels and the Maximum Mean Discrepancy (Gretton et al., 2006) to estimate the distance between the model distribution, Q, and the true \u201ctheoretical\u201d data distribution, P , based on finite samples drawn i.", "startOffset": 58, "endOffset": 80}, {"referenceID": 5, "context": "We use the mismatch kernel (Leslie et al., 2004), K(k,m)(x,x \u2032), which for a pair of sequences x and x\u2032 computes the shared occurences of k-length subsequences that have at most m mismatches.", "startOffset": 27, "endOffset": 48}, {"referenceID": 5, "context": "This kernel has been successfully used for biological sequence classification (Leslie et al., 2004) and NLP tasks (Teo & Vishwanathan, 2006).", "startOffset": 78, "endOffset": 99}], "year": 2012, "abstractText": "We examine the problem of learning a probabilistic model for melody directly from musical sequences belonging to the same genre. This is a challenging task as one needs to capture not only the rich temporal structure evident in music, but also the complex statistical dependencies among different music components. To address this problem we introduce the Variable-gram Topic Model, which couples the latent topic formalism with a systematic model for contextual information. We evaluate the model on next-step prediction. Additionally, we present a novel way of model evaluation, where we directly compare model samples with data sequences using the Maximum Mean Discrepancy of string kernels, to assess how close is the model distribution to the data distribution. We show that the model has the highest performance under both evaluation measures when compared to LDA, the Topic Bigram and related non-topic models.", "creator": "LaTeX with hyperref package"}}}