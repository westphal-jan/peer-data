{"id": "1206.6442", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Minimizing The Misclassification Error Rate Using a Surrogate Convex Loss", "abstract": "We carefully study how well minimizing convex surrogate loss functions, corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors. In particular, we show that amongst all convex surrogate losses, the hinge loss gives essentially the best possible bound, of all convex loss functions, for the misclassification error rate of the resulting linear predictor in terms of the best possible margin error rate. We also provide lower bounds for specific convex surrogates that show how different commonly used losses qualitatively differ from each other.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (633kb)", "http://arxiv.org/abs/1206.6442v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["shai ben-david", "david loker", "nathan srebro", "karthik sridharan"], "accepted": true, "id": "1206.6442"}, "pdf": {"name": "1206.6442.pdf", "metadata": {"source": "META", "title": "Minimizing The Misclassification Error Rate  Using a Surrogate Convex Loss", "authors": ["Shai Ben-David", "David Loker", "Nathan Srebro", "Karthik Sridharan"], "emails": ["shai@cs.uwaterloo.ca", "dloker@cs.uwaterloo.ca", "nati@ttic.edu", "skarthik@wharton.upenn.edu"], "sections": [{"heading": "1. Introduction", "text": "Perhaps the most fundamental question explored in machine learning theory is that of binary half-space classification, but the problem of agnostic half-space learning is commonly known as NP-hard (Kearns et al., 1994). Even if you want to learn only one half-space relative to the best possible M-margin error, Ben-David & Simon (2000) show that (subject to P 6 = NP) there is no correct learning algorithm (i.e. the return of a linear predictor) that runs in time in both 1 / M and the desired accuracy. Under a cryptographic hardness assumption, this result is expanded to improper learning (i.e., the algorithm can output any predictor as long as it generalizes well)."}, {"heading": "1.1. More Related Work", "text": "This year it is more than ever before."}, {"heading": "2. Setting", "text": "Let D (x, y) be a distribution over U \u0443 {+ 1, 1}, where for some d U = {x 2 round: kxk 1} is the ddimensional sphere of unity. We will often actually consider finite samples, where D is meant to be a uniform distribution over points in the sample. A linear predictor is described by a vector and a distorted term: (w, w0), w 2 Rd, w0 2 R. For a loss function: R! R, the -risk is given by: RD (w, w0) = E (x, y) D [(y (hw, xi + w0)]. If the distribution D is understood from the context, we will just use R (w, w0). We will be particularly interested in the 0-1 loss 01 (z, w0) = 1 {z 0} and the margin loss (z, xi + w0) = 1 {z < 1} as a prediction."}, {"heading": "3. Misclassification Error Guarantee", "text": "For the first time in a long time, I have been able to find the right place for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person for the right person"}, {"heading": "3.1. Surrogate loss minimization when margins exist", "text": "rE \"s tis rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\""}, {"heading": "3.2. Bounds for Specific Losses", "text": "Before proceeding, we would like to mention an alternative to equation (3), which is often easier to get to grips with. (For this purpose, it should be noted that by (3,1): EG (,,, B) min (1 4, sup 2 [0,1] inf. (0, 4p 5] max. (2) 4 (2), BA ((((((B 5) 2)) (2)) (2) (2) (2) inf. (0, 4p 5) max. (x2) the ratio (x1) (x1 x2) monotonously not decreasing in x1 and so, EG (1), sup 2 [0,1] inf. (0, 4p 5] max. (EG 8), EG 5] max. ((((((B 5)."}, {"heading": "3.3. General Hypothesis Classes", "text": "The Misclassification Errors Guarantee (EG) was specific to linear predictors (with standard limited by B), and this definition of the Misclassification Errors Guarantee can easily be generalized."}, {"heading": "4. Classification Calibration and the", "text": "In this context, it should be noted that the loss of a class rich enough to minimize the expected loss actually also minimizes the misclassification error. Calibration can be considered an extreme point of the misclassification error guarantee. (EC) Firstly, that the optimal predictor then minimizes the expected loss. (D) The calibration of the class can be considered an extreme point of misclassification. (G) First, because the convex losses are so small that the expected loss is in fact minimized. (D) The calibration of the class can be considered an extreme point of the misclassification error guarantee."}, {"heading": "5. Including Estimation Error Rates", "text": "In practice, we get a finite training sample, and when selecting the hypothesis that minimizes some empirical losses, the error in estimation involved in minimizing empirical losses is more likely to come into view. In selecting the loss, one should take into account both the error guarantee of misclassification (EC) and the associated error in estimation of the problem. For example, one might think that a square error is better than one might expect a square error rate where n is the sample size. However, the square error rate is large, as we see in Section 3.2.For high-dimensional cases, it can be argued that a loss of hinges is the loss of choice even when we take into account the error in estimation. For the conservative update of the algorithm, the loss of hinges with its corresponding analysis of (Shalev-Shwartz, 2007) or for the exact results of the SVR emphases that take into account the error in estimation."}, {"heading": "6. Proofs of Theorems 3 and 4", "text": "The distribution for this theorem is as follows: \"There are fewer points at x = 1.\" (\"There are points at x = 1.\") \"There are points at x = 1.\" (\"There are points at x = 1.\") \"There are points at x = 1.\" (\"There are points at x = 1.\") \"There are points at x = 1.\" (\"There are points at x = 1.\") \"There is points at R (w? 0) = 2.\" \"The margin loss of (w? 0) is unambiguous.\" \"There are no other points that need to be microssed.\" \"We need the x-axis.Case 1Assume that we have a classifier that has less than points, and that this classifier intersectsthe x axis between M, 0.\""}, {"heading": "Remaining cases", "text": "This year, the number of job-related redundancies has tripled in the last five years, resulting in a 20% drop in the number of job-related redundancies compared to the previous five years."}, {"heading": "7. Discussion", "text": "In this paper, we show that the error rate in the misclassification of the linear predictor, which minimizes the expected loss of the hinge, is limited by B + 1, with B + 1 tied to the M margin error and B = 1 / M. In addition, we show that when using linear predictors, any algorithm that minimizes the loss of the hinge has a misclassification error of at least B + 1 2. We argue that the analysis can be used to optimally compare the losses of the hinge up to factor 2. We also show lower limits for specific losses of the convex and that any strongly convex loss has a qualitatively inferior warranty when compared with the loss of the hinge. We argue that the analysis can be used to compare the losses of the hinge used for binary classification."}, {"heading": "Christmann, Andreas and Steinwart, Ingo. On robustness", "text": "Properties of Convex Risk Minimization Methods for Pattern Recognition. J. Mach. Learn. Res., 5: 1007-1034, December 2004. ISSN 1532-4435."}, {"heading": "Kalai, Adam Tauman, Klivans, Adam R., Mansour,", "text": "Yishay, and Servedio, Rocco A. Agnostically Learning Half-Spaces. SIAM J. Comput., 37: 1777-1805, March 2008. ISSN 0097-5397.Kalai, A.T. and Sastry, R. The Isotron Algorithm: High-Dimensional Isotonic Regression. In Proceedings of the 22th Annual Conference on Learning Theory. Citeseer, 2009."}, {"heading": "Kearns, Michael, Schapire, Robert E., Sellie, Linda M., and", "text": "Hellerstein, Lisa. Toward e cient agnostic learning. In Machine Learning, pp. 341-352. ACM Press, 1994."}, {"heading": "Long, Phil and Servedio, Rocco A. Learning large-margin", "text": "In Advances in Neural Information Processing Systems 24, pp. 91-99. 2011. Long, Philip M. and Servedio, Rocco A. Random classification noise conquers all convex potential boosters. In Proceedings of the 25th International Conference on Machine Learning, ICML '08, pp. 608-615, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4."}, {"heading": "Masnadi-Shirazi, Hamed and Vasconcelos, Nuno. On the", "text": "Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost. In Advances in Neural Information Processing Systems 21, pp. 1049-1056. 2009.Nock, Richard and Nielsen, Frank. For e cient minimization of classification-calibrated surrogates. In NIPS '08, pp. 1201-1208, 2008."}, {"heading": "Rosasco, L., De, E., Caponnetto, Vito A., Piana, M., and", "text": "Neural Computation, 16 (5), 2004.Shalev-Shwartz, S. Online Learning: Theory, Algorithms, and Applications. Doctoral Thesis, The Hebrew University, 2007.Shalev-Shwartz, S., Shamir, O., and Sridharan, K. Learning kernel-based half spaces with the zero-one loss. Arxiv preprint arXiv: 1005.3681, 2010."}, {"heading": "Srebro, N., Sridharan, K., and Tewari, A. Smoothness, low", "text": "Noise and Fast Rates. In NIPS, 2010.Zhang, T. Solving Large Linear Prediction Problems Using Stochastic Gradient Descendancy Algorithms. In Proceedings of the Twenty-First International Conference on Machine Learning, 2004."}], "references": [{"title": "Convexity, classification, and risk bounds", "author": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuli\u21b5e"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "E cient learning of linear perceptrons", "author": ["Ben-david", "Shai", "Simon", "Hans Ulrich"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ben.david et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ben.david et al\\.", "year": 2000}, {"title": "On robustness properties of convex risk minimization methods for pattern recognition", "author": ["Christmann", "Andreas", "Steinwart", "Ingo"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Christmann et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Christmann et al\\.", "year": 2004}, {"title": "Agnostically learning halfspaces", "author": ["Kalai", "Adam Tauman", "Klivans", "Adam R", "Mansour", "Yishay", "Servedio", "Rocco A"], "venue": "SIAM J. Comput.,", "citeRegEx": "Kalai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2008}, {"title": "The isotron algorithm: Highdimensional isotonic regression", "author": ["A.T. Kalai", "R. Sastry"], "venue": "In Proceedings of the 22th Annual Conference on Learning Theory. Citeseer,", "citeRegEx": "Kalai and Sastry,? \\Q2009\\E", "shortCiteRegEx": "Kalai and Sastry", "year": 2009}, {"title": "Toward e cient agnostic learning", "author": ["Kearns", "Michael", "Schapire", "Robert E", "Sellie", "Linda M", "Hellerstein", "Lisa"], "venue": "In Machine Learning,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "Learning large-margin halfspaces with more malicious noise", "author": ["Long", "Phil", "Servedio", "Rocco A"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Long et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Long et al\\.", "year": 2011}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["Long", "Philip M", "Servedio", "Rocco A"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Long et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Long et al\\.", "year": 2008}, {"title": "On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost", "author": ["Masnadi-Shirazi", "Hamed", "Vasconcelos", "Nuno"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Masnadi.Shirazi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Masnadi.Shirazi et al\\.", "year": 2009}, {"title": "On the e cient minimization of classification calibrated surrogates", "author": ["Nock", "Richard", "Nielsen", "Frank"], "venue": "In NIPS\u201908,", "citeRegEx": "Nock et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nock et al\\.", "year": 2008}, {"title": "Are loss functions all the same", "author": ["L. Rosasco", "E. De", "Caponnetto", "Vito A", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "Rosasco et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosasco et al\\.", "year": 2004}, {"title": "Online Learning: Theory, Algorithms, and Applications", "author": ["S. Shalev-Shwartz"], "venue": "PhD thesis, The Hebrew University,", "citeRegEx": "Shalev.Shwartz,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz", "year": 2007}, {"title": "Learning kernel-based halfspaces with the zero-one loss", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan"], "venue": "Arxiv preprint arXiv:1005.3681,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Smoothness, low noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "However the problem of agnostically learning half-spaces is known to be NP-hard in general (Kearns et al., 1994).", "startOffset": 91, "endOffset": 112}, {"referenceID": 12, "context": "Under a cryptographic hardness assumption, (Shalev-Shwartz et al., 2010) extend this result to improper learning (i.", "startOffset": 43, "endOffset": 72}, {"referenceID": 5, "context": "However the problem of agnostically learning half-spaces is known to be NP-hard in general (Kearns et al., 1994). Even when one only wants to learn a half-space relative to the best possible M -margin error, Ben-david & Simon (2000) show that (subject to P 6= NP ) there exists no proper learning algorithm (i.", "startOffset": 92, "endOffset": 233}, {"referenceID": 14, "context": "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of \u201cclassification calibrated\u201d loss functions (Zhang, 2004; Bartlett et al., 2006).", "startOffset": 174, "endOffset": 210}, {"referenceID": 0, "context": "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of \u201cclassification calibrated\u201d loss functions (Zhang, 2004; Bartlett et al., 2006).", "startOffset": 174, "endOffset": 210}, {"referenceID": 3, "context": "In terms of directly minimizing the misclassification error rate, without using a convex surrogate loss, Kalai et al. (2008) provide non-asymptotic finite sample bounds for e cient binary prediction with half spaces.", "startOffset": 105, "endOffset": 125}, {"referenceID": 3, "context": "In terms of directly minimizing the misclassification error rate, without using a convex surrogate loss, Kalai et al. (2008) provide non-asymptotic finite sample bounds for e cient binary prediction with half spaces. However, to do so they assume the inputs are uniformly distributed on the surface of a sphere, which is an extremely strong an unrealistic assumption. Similarly, Kalai & Sastry (2009) also provide an e cient algorithm for minimizing the misclassification error, but only under the assumption that the conditional distribution of the label given the input x is some monotonic function of w \u00b7 x for some w\u2014again significantly departing from the agnostic setting.", "startOffset": 105, "endOffset": 401}, {"referenceID": 0, "context": "in the separable case, this is essentially a question about \u201cclassification calibration\u201d, and we have that for convex , EG( , 0) = 0 if and only if is di\u21b5erentiable at zero and 0(0) < 0 (Bartlett et al., 2006)\u2014see Section 4.", "startOffset": 186, "endOffset": 209}, {"referenceID": 14, "context": "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of \u201cclassification calibrated\u201d loss functions (Zhang, 2004; Bartlett et al., 2006).", "startOffset": 174, "endOffset": 210}, {"referenceID": 0, "context": "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of \u201cclassification calibrated\u201d loss functions (Zhang, 2004; Bartlett et al., 2006).", "startOffset": 174, "endOffset": 210}, {"referenceID": 13, "context": "Discussing \u232b = 0 corresponds to considering only the separable case, which is in a sense the point of intersection of our study and that of Zhang (2004); Bartlett et al.", "startOffset": 140, "endOffset": 153}, {"referenceID": 0, "context": "Discussing \u232b = 0 corresponds to considering only the separable case, which is in a sense the point of intersection of our study and that of Zhang (2004); Bartlett et al. (2006).", "startOffset": 154, "endOffset": 177}, {"referenceID": 13, "context": "For example, based on Zhang (2004), Rosasco et al.", "startOffset": 22, "endOffset": 35}, {"referenceID": 10, "context": "For example, based on Zhang (2004), Rosasco et al. (2004) argue that the hinge loss (and also logistic loss) enjoy better rates than other losses like squared loss.", "startOffset": 36, "endOffset": 58}, {"referenceID": 11, "context": "the hinge loss with its corresponding analysis by (Shalev-Shwartz, 2007), or for the exact minimizer (which corresponds to the SVM) of empirical hinge loss using results in (Srebro et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 13, "context": "the hinge loss with its corresponding analysis by (Shalev-Shwartz, 2007), or for the exact minimizer (which corresponds to the SVM) of empirical hinge loss using results in (Srebro et al., 2010) (and noticing that hinge loss upper bounds a smooth version of margin loss which in turn upper bounds the zero-one loss) one can show that if b w", "startOffset": 173, "endOffset": 194}, {"referenceID": 13, "context": "Further, when the dimensionality is large (compared to the sample size) then the estimation error rate for the squared loss (with linear predictors) can be lower bounded by B2/n (see for instance (Srebro et al., 2010)) and so the best guarantee that can be provided on the classification risk of estimator obtained by minimizing squared loss scales is \u232b(B 1)2 + B2/n.", "startOffset": 196, "endOffset": 217}], "year": 2012, "abstractText": "We carefully study how well minimizing convex surrogate loss functions corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors. We consider the agnostic setting, and investigate guarantees on the misclassification error of the loss-minimizer in terms of the margin error rate of the best predictor. We show that, aiming for such a guarantee, the hinge loss is essentially optimal among all convex losses.", "creator": "TeXShop"}}}