{"id": "1206.6445", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Deep Lambertian Networks", "abstract": "Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reflectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (5632kb)", "http://arxiv.org/abs/1206.6445v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["yichuan tang", "ruslan salakhutdinov", "geoffrey e hinton"], "accepted": true, "id": "1206.6445"}, "pdf": {"name": "1206.6445.pdf", "metadata": {"source": "META", "title": "Deep Lambertian Networks", "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"], "emails": ["tang@cs.toronto.edu", "rsalakhu@cs.toronto.edu", "hinton@cs.toronto.edu"], "sections": [{"heading": "1. Introduction", "text": "These models share the same underlying principle of first learning generatively from data before using the learned latent variables (characteristics) for discriminatory tasks. The advantage of using this indirect approach to discrimination is that it is possible to learn meaningful latent variables that achieve a strong generalization. In the vision, illumination is a major cause of variation. If the light sources in the proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner. Changes in intensity in a scene occur dramatic changes in image intensity. This is more detrimental to recognition performance than most algorithms as inputs. A natural way to address this problem is to learn a model in which the albedo, surface characteristics and lighting are explicitly presented as variable."}, {"heading": "2. Gaussian Restricted Boltzmann Machines", "text": "We briefly describe the Gaussian Restricted Boltzmann Machines (GRBMs), which are used to model albedo and surface norms. GRBMs (Hinton & Salakhutdinov, 2006) have been successfully used to extend binary RBMs to real-valued visible units for tasks such as image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011). GRBMs can be considered as a mixture of diagonal Gaussians with common parameters where the number of mixing components is exponential in the number of hidden nodes. With visible nodes v, RNv, and hidden nodes h, the energy of the common configuration is represented by: EGRBM (v, h) = 12 (vi \u2212 bi) 2 (vajj \u2212 ev."}, {"heading": "3. Deep Lambertian Networks", "text": "GRBMs and DBNs use Equation 2 to generate the intensity of a specific pixel vi. This generative model is inefficient in dealing with illumination variations in v. Specifically, the hidden activations required to produce a bright image of an object are very different from the activations required to produce a dark image of the same object. The Lambertian reflection model is widely used to model illumination variations and is a good approximation for diffuse object surfaces (those without special lights).Under the Lambertian model shown in Fig. 1, the i-th pixel intensity is modeled as vi = ai \u00d7 max (~ nTi ~, 0).The Albedo ai, also known as a reflection coefficient, is the diffuse reflectivity of a surface at pixels i that is materially dependent, but illumination invariant. In contrast to the reflective process of an object, the GRAM can change the normal surface under different illumination conditions."}, {"heading": "3.1. The Model", "text": "The DLN is a hybrid, undirected generative model that maps the distribution over the surface. \"We use the GRBMs to model the albedo and the surface standards.\" It is important that we expect the distribution over the surface, and we expect them to be distributed over the surface. \"We use the GRBMs to model the albedo and the surface standards.\" It is important to use the GRBMs because we expect the distribution to be across the surface. \"We use the GRBMs to model the albedo and the surface standards, and a Gaussian in front of the model."}, {"heading": "3.2. Inference", "text": "Given that the two models are two different types of distribution, it is possible to use four types of distribution: p (A, L, V), p (A, V), p (A, V), p (A, V), p (A, L, V), p (A, V), p (A, L, V), p (A, V), p (A, L), p (A, V), p (A), p (A), p (A), p (A, V), p (A), p (A), h), p (G, h), h (A), p (A), h), p (A)."}, {"heading": "3.3. Learning", "text": "In the E step, the MCMC samples are taken from the approximate posterior distribution (Neal & Hinton, 1998), we first sample the conditional distributions in Sec. 3.2 to approximate the posterior p (a, N, L, h, g | V; \u03b8old), and then we optimize the function of the common log probability w.r.t. of the model parameters. Specifically, we approach the integral using: 1N N N N, I, I, E (a, i, h, g | V; successold), rather, we measure the integral using: 1N N, I, E (V, a (i), N (i), L (i), h (g), g, g, g, g, g, g (c)."}, {"heading": "4. Experiments", "text": "We experiment with the Yale B and the Extended Yale B face databases. Combined, the two databases contain 64 frontal images from 38 different subjects. 45 images for each subject are further divided into 4 subsets of increasing illumination variations. Fig. 3 shows samples from the Yale B and Extended Yale B databases. For each subject, we used about 45 frontal images for our experiments. 2 We separated 28 subjects from the Extended Yale B database for training and kept-out all 10 subjects from the original Yale B database for testing3. The processing step involved2A some of the images are corrupted. 3 We used the cropped images obtained from the Yale B Algorithm 1 Learning Deep Lambertian Networks 1: Pretrain the {a, h} albedo GRBM with faces and initialize {W, c} of the DLN with the parameters."}, {"heading": "4.1. Inference", "text": "Although the DLN can use multiple images of the same object during inference, it is important to study how well it performs with a single test image. We are also interested in the number of iterations that subjects would need to find the rear image mode.In our first experiment, we presented the model with a single Yale-B facial image of a held-up subject, as shown in Fig. 5. The light source illuminates the subject from the bottom right and causes a significant shadow over the upper left half of the subject's face. As the albedo captures a light-invariant representation of the face, the correct rear image distribution should automatically perform an illumination standardization. Using the algorithm described in Sec. 3.2, we clamp the visible nodes onto the image of the test surface and the sample from the 4 conditions that HMC takes in alternate ways."}, {"heading": "4.2. Relighting", "text": "Realistic images can only be generated if the albedo and surface norms of that particular person have been correctly taken. We first sample the light variable \"from their Gaussian history, defined by {\u00b5, \u0441}. Subject to the inferred a and N (see Fig. 6 (b)), we use Eq. 5 to draw samples of Fig. 6 (c) showing reviewed facial images of outstretched subjects."}, {"heading": "4.3. Recognition", "text": "For the 10 subjects of Yale B, only images from subset 1 (with 7 images) are used for the training. Images from subset 2-4 are used for the test. To use DLN for the detection, we first conclude that albedo (ai) and surface norms (ni) are conditioned on the provided training image (s) of the subjects. For each subject, a three-dimensional linear subspace is spanned by the inferred albedo and surface standards. In particular, we look at the matrix M of dimensions Nv x x 3, with the i-th row set to mi = aini. The columns of M comprise the three-dimensional linear subspace. Test images of the subjects are compared with all 10 subspaces and are labeled according to the designation of their next subranges."}, {"heading": "4.4. Generic Objects", "text": "We used 50 objects from the database of the Amsterdam Library of Images (ALOI) (Geusebroek et al., 2005). For each object, 15 images with different illumination were divided into 10 for the training and 5 for the test. Using the masks provided for each object, the images were cropped and scaled back to a resolution of 48 x 48. We used a DLN with Nh = 1000 and Ng = 1500. Additionally, a 500 h2 layer and 500 g2 layer was added. After the training, we performed a posterior inference with one of the held-up images. Fig. 8 shows results. The upper row contains test images, the middle row shows the derived albedo images after 50 alternating Gibbs iterations, and the lower row shows the derived surface standards."}, {"heading": "5. Discussions", "text": "We have introduced a generative model with meaningful latent variables and multiplicative interactions that simulates the Lambertian reflection model. We have shown that by learning the priors of these illumination invariant variables directly from data, we can improve our recognition tasks at a glance and generate images under novel illumination."}, {"heading": "Acknowledgements", "text": "We thank Maksims Volkovs, James Martens and Abdelrahman Mohamed for the discussions supported by NSERC and CIFAR."}], "references": [{"title": "Recovering intrinsic scene characteristics from images", "author": ["H.G. Barrow", "J.M. Tenenbaum"], "venue": "In CVS78,", "citeRegEx": "Barrow and Tenenbaum,? \\Q1978\\E", "shortCiteRegEx": "Barrow and Tenenbaum", "year": 1978}, {"title": "What is the set of images of an object under all possible lighting conditions", "author": ["P.N. Belhumeur", "D.J. Kriegman"], "venue": "In CVPR, pp", "citeRegEx": "Belhumeur and Kriegman,? \\Q1996\\E", "shortCiteRegEx": "Belhumeur and Kriegman", "year": 1996}, {"title": "A morphable model for the synthesis of 3-D faces", "author": ["V. Blanz", "T. Vetter"], "venue": "In SIGGraph-99,", "citeRegEx": "Blanz and Vetter,? \\Q1999\\E", "shortCiteRegEx": "Blanz and Vetter", "year": 1999}, {"title": "Recovering intrinsic images with a global sparsity prior on reflectance", "author": ["P. Gehler", "C. Rother", "M. Kiefel", "L. Zhang", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "Gehler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gehler et al\\.", "year": 2011}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["Georghiades", "Athinodoros S", "Belhumeur", "Peter N", "Kriegman", "David J"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "The amsterdam library of object images", "author": ["J.M. Geusebroek", "G.J. Burghouts", "A.W.M. Smeulders"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Geusebroek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Geusebroek et al\\.", "year": 2005}, {"title": "Photometric stereo under a light-source with arbitrary motion", "author": ["H. Hayakawa"], "venue": "Journal of the Optical Society of America,", "citeRegEx": "Hayakawa,? \\Q1994\\E", "shortCiteRegEx": "Hayakawa", "year": 1994}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "On the perception of shape from shading", "author": ["Kleffner", "Dorothy A", "V.S. Ramachandran"], "venue": "Perception and Psychophysics,", "citeRegEx": "Kleffner et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kleffner et al\\.", "year": 1992}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.C. Lee", "Ho", "Jeffrey", "Kriegman", "David"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2005}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "MCMC using Hamiltonian dynamics. in Handbook of Markov Chain Monte Carlo", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "Neal,? \\Q2010\\E", "shortCiteRegEx": "Neal", "year": 2010}, {"title": "A new view of the EM algorithm that justifies incremental, sparse and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Learning in Graphical Models,", "citeRegEx": "Neal and Hinton,? \\Q1998\\E", "shortCiteRegEx": "Neal and Hinton", "year": 1998}, {"title": "Modeling pixel means and covariances using factorized third-order boltzmann machines", "author": ["M. Ranzato", "G. Hinton"], "venue": null, "citeRegEx": "Ranzato and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Ranzato and Hinton", "year": 2010}, {"title": "The Toronto Face Database", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton"], "venue": "Technical report,", "citeRegEx": "Susskind et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2011}, {"title": "Springer, 2010", "author": ["Taylor", "Graham W.", "Fergus", "Rob", "LeCun", "Yann", "Bregler", "Christoph. Convolutional learning of spatiotemporal features. In ECCV"], "venue": "ISBN 978-3-642-15566-6. URL http://dx.doi.org/10.1007/ 978-3-642-15567-3.", "citeRegEx": "Taylor et al\\.,? 2010", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Face relighting from a single image under arbitrary unknown lighting conditions", "author": ["Y. Wang", "L. Zhang", "Z.C. Liu", "G. Hua", "Z. Wen", "Z.Y. Zhang", "D. Samaras"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Photometric method for determining surface orientation from multiple images", "author": ["R.J. Woodham"], "venue": "Optical Engineering,", "citeRegEx": "Woodham,? \\Q1980\\E", "shortCiteRegEx": "Woodham", "year": 1980}, {"title": "Determining generative models of objects under varying illumination: Shape and albedo from multiple images using SVD and integrability", "author": ["A.L. Yuille", "D. Snow", "R. Epstein", "P.N. Belhumeur"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Yuille et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 1999}, {"title": "Face recognition from a single training image under arbitrary unknown lighting using spherical harmonics", "author": ["L. Zhang", "D. Samaras"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhang and Samaras,? \\Q2006\\E", "shortCiteRegEx": "Zhang and Samaras", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "Multilayer generative models have recently achieved excellent recognition results on many challenging datasets (Ranzato & Hinton, 2010; Quoc et al., 2010; Mohamed et al., 2011).", "startOffset": 111, "endOffset": 176}, {"referenceID": 20, "context": "Separating the surface normals and the albedo of objects using multiple images obtained under different lighting conditions is known as photometric stereo (Woodham, 1980).", "startOffset": 155, "endOffset": 170}, {"referenceID": 3, "context": "A related problem is the estimation of intrinsic images (Barrow & Tenenbaum, 1978; Gehler et al., 2011).", "startOffset": 56, "endOffset": 103}, {"referenceID": 5, "context": "Hayakawa (1994) described a method for photometric stereo using SVD, which estimated the shape and albedo up to a linear transformation.", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "Hayakawa (1994) described a method for photometric stereo using SVD, which estimated the shape and albedo up to a linear transformation. Using integrability constraints, Yuille et al. (1999) proposed a similar method to reduce the ambiguities to a generalized bas relief ambiguity.", "startOffset": 0, "endOffset": 191}, {"referenceID": 4, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005).", "startOffset": 87, "endOffset": 131}, {"referenceID": 12, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005).", "startOffset": 87, "endOffset": 131}, {"referenceID": 4, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005). The main drawback of these models is that they require multiple images of an object under varying lighting conditions for estimation. While Zhang & Samaras (2006); Wang et al.", "startOffset": 88, "endOffset": 296}, {"referenceID": 4, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005). The main drawback of these models is that they require multiple images of an object under varying lighting conditions for estimation. While Zhang & Samaras (2006); Wang et al. (2009) present algorithms that only use a single training image, their algorithms require bootstrapping with a 3D morphable face model.", "startOffset": 88, "endOffset": 316}, {"referenceID": 11, "context": "As the extension of binary RBMs to real-valued visible units, GRBMs (Hinton & Salakhutdinov, 2006) have been successfully applied to tasks including image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011).", "startOffset": 220, "endOffset": 299}, {"referenceID": 18, "context": "As the extension of binary RBMs to real-valued visible units, GRBMs (Hinton & Salakhutdinov, 2006) have been successfully applied to tasks including image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011).", "startOffset": 220, "endOffset": 299}, {"referenceID": 13, "context": "As the extension of binary RBMs to real-valued visible units, GRBMs (Hinton & Salakhutdinov, 2006) have been successfully applied to tasks including image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011).", "startOffset": 220, "endOffset": 299}, {"referenceID": 9, "context": "Additional layers of binary RBMs are often stacked on top of a GRBM to form a Deep Belief Net (DBN) (Hinton et al., 2006).", "startOffset": 100, "endOffset": 121}, {"referenceID": 14, "context": "HMC (Duane et al., 1987; Neal, 2010) is an auxiliary variable MCMC method which combines Hamiltonian dynamics with the Metropolis algorithm to sample continuous random variables.", "startOffset": 4, "endOffset": 36}, {"referenceID": 7, "context": "We therefore turn to Contrastive Divergence (CD) (Hinton, 2002) to compute an approximate gradient during learning.", "startOffset": 49, "endOffset": 63}, {"referenceID": 17, "context": "Therefore, we leverage a large set of the face images from the Toronto Face Database (TFD) (Susskind et al., 2011).", "startOffset": 91, "endOffset": 114}, {"referenceID": 19, "context": "In the computer vision literature, Zhang & Samaras (2006); Wang et al. (2009) report lower error rates on the Yale B dataset.", "startOffset": 59, "endOffset": 78}, {"referenceID": 5, "context": "We used 50 objects from the Amsterdam Library of Images (ALOI) database (Geusebroek et al., 2005).", "startOffset": 72, "endOffset": 97}], "year": 2012, "abstractText": "Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reflectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition.", "creator": "LaTeX with hyperref package"}}}