{"id": "1206.6446", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Agglomerative Bregman Clustering", "abstract": "This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondifferentiable convex functions.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (481kb)", "http://arxiv.org/abs/1206.6446v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matus telgarsky", "sanjoy dasgupta"], "accepted": true, "id": "1206.6446"}, "pdf": {"name": "1206.6446.pdf", "metadata": {"source": "CRF", "title": "Agglomerative Bregman Clustering", "authors": ["Matus Telgarsky", "Sanjoy Dasgupta"], "emails": ["mtelgars@cs.ucsd.edu", "dasgupta@cs.ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "Starting from the points {xi} mi = 1 and a pair merging of costs \u0445 (\u00b7, \u00b7), classical agglomerative clustering produces a single hierarchical tree as follows (Duda et al., 2001) 1. Start with m clusters: Ci: = {xi} for each i.2. While at least two clusters remain: (a) Select {Ci, Cj} with minimal (Ci, Cj). (b) Remove {Ci, Cj}, add in Ci, Cj. To build a hierarchy with low k mean costs, you can use the merger costs owed to Ward (1963)."}, {"heading": "1.1. Bregman clustering", "text": "There is already a rich theory of clustering with Bregman divergences and, in particular, the relationship of these divergences to exponential family distributions (Banerjee et al., 2005).The standard development has two deficiencies, the first of which is amplified in the agglomerative environment.Degenerated divergences. Many divergences lead to costs that are not defined on certain input factors.This scenario is exacerbated by small clusters; for example, in Gaussian clusters, the corresponding divergence rule is the KL divergence, which requires full cluster covariances.This is impossible at \u2264 d points, but the agglomerative procedure above starts with singlets.Minimal representations. The standard theory of exponential families and their links to Bregman divergences depend on minimal representations: there is only one way to write down a specific distribution. On the other hand, natural coding is effective for many problems - e.g. the output of models and many other right-hand examples in text4."}, {"heading": "1.2. Contribution", "text": "The approach of this manuscript is to carefully construct a theory of Bregman divergences constructed from convex but non-differentiable functions. Section 2 will present the basic definition and confirm this generalization, which fulfills the usual Bregman divergence characteristics. Section 3 will revisit the standard Bregman hard clustering model (Banerjee et al., 2005) and show how it naturally leads to fusion costs. Section 4 will then construct exponential families and show that non-differentiable Bregman divergences are not minimal, but nevertheless meet all the usual characteristics. To overcome the above cases of small samples where deviations may not be well defined, Section 5 presents smooth procedures that directly follow the previous technical development.To conclude, Section 6 presents the final algorithm, and Section 7 provides experimental validation both by measuring cluster suitability and providing cluster features."}, {"heading": "1.3. Related work", "text": "A number of papers present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions or the analogous goal of maximizing model probability, or most recently in conjunction with the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999). Furthermore, Merugu (2006) examined the same algorithm as the present paper, formulated in terms of Bregman divergences. These preceding methods either do not explicitly mention divergence degenerations or circumvent them using Bajesian techniques, a link between divergences for non-differentiable functions discussed in Section 5.Bregman has been examined in a number of places."}, {"heading": "1.4. Notation", "text": "The following concepts from the convex analysis are used throughout the text; the interested reader is referred to the underlying text of Rockafellar (1970). A set is convex if the line segment between any two elements is again within the set; the epigraph of a function f: Rn \u2192 R, where R = R \u00b2, is a series of dots below f; i.e., the set {(x, r): x Rn, r \u00b2 f (x)} Rn \u00d7 R, where a function is convex if its epigraph is convex, and closed if its epigraph is closed; the domain dom (f) of a function f: Rn \u2192 R is the subset of inputs that are not mapped to + \u00b2: dom (f) = {x \u00b2 Rn: f < [f] < a function is correct if dom (f) is not empty and f is never the value."}, {"heading": "2. Bregman divergences", "text": "Considering a convex function f (y \u2212 x), the Bregman divergence Bf (\u00b7, y) is the gap between f and its linearization in y. (\u00b7) Considering a convex function f (x, y), the convex function f (x, y) is the convex function f (x, y) the convex function f (x, y) the convex function f (x, y) the convex function f (f) isBf (x, y): f (x) -f (y) -f (y) -f (y, y \u2212 x)."}, {"heading": "3. Cluster model", "text": "Let a finite capture C of the points {xi} mi = 1 be given in any abstract space X - say, documents or vectors. (To cluster these with Bregman divergences, the first step is to map them in Rn.Definition 3.1. (Then it seems sufficient that the identity map (with X = Rn), with an additional convenience of the means of calculation, in Section 4, will, however, rely on the development of exponential families. (Given a statistical map: X \u2192 Rn and convex function f, the cost of a single cluster C i.f), will be x x. (C)."}, {"heading": "4. Exponential families", "text": "So far, this manuscript has developed a mathematical basis for clustering divergences with Bregman q. But what matters when examples of significant Bregman divergences are few and far apart? The primary mechanism for constructing significant merge costs is to model the clusters as exponential family distributions. In this section, it does not matter whether the corresponding exponential family distribution is a measurement value above X and further determines the statistical map. \u2212 Definition 4.1. In the face of a measurable statistical map and a vector Rn of canonical parameters, the corresponding exponential family distribution has densitypical values (x): = exp (< x), \u03b8 (x). \u2212 Definition it is possible that the normalization term, which is usually referred to as a cumulative or protocol-based partition."}, {"heading": "5. Smoothing", "text": "The final piece of the technical puzzle is the smoothing process: Most of the above properties - for example, that Bf (\u03c4 (C1), \u03c4 (C2))) < \u221e - depend on \u03c4 (C2) ri (dom (f)). Relative boundaries lead to degeneration; this characterizes, for example, the Gaussian degeneration identified in the introduction. Definition 5.1. For a (non-empty) convex set S, a statistical map \u03c4: X \u2192 Rn is a smoothing statistical map for S if there is any finite set C X, earth (C) ri (S).It is very easy to construct smoothing statistical maps.Theorem 5.2. Let us give a nonempty convex set S. Let us specify a nonempty convex set S. Let it happen: 0: X \u2192 Rn a statistical map, the one for each quantity C (cl) \u2212 S."}, {"heading": "In general, \u03c41 is a smoothing statistic map for S. If additionally S is a convex cone, then \u03c42 is also a smoothing statistic map for S.", "text": "The following two examples smooth Gaussians and multinomials over theorem 5.2. The parameters \u03b1 and z are selected from the data, and furthermore, as the total amount of available data grows, the mean parameters of a multinomial option are within the probability magnitude simplex, a compact convex set. As discussed in Example 4.7, only the relative interior of the simplex will provide canonical parameters. According to theorem 5.2, all that remains in solving this problem is the determination of the predetermined multinomic magnitude 0 (C) = a quantitative element based on a finite sample of magnitude m, and thus the true parameters are within a confidence interval in x; crucially, this confidence interval reduces the relative interior of the probability variance."}, {"heading": "6. Clustering algorithm", "text": "The algorithm appears in algorithm 1. If you let T \u2206 f, \u03c4 detect an upper limit for the time needed to calculate aAlgorithm 1 agglomerate, this results in costs for merging f, \u03c4, points {xi} mi = 1X. Initialize the forest as F: = {xi}: i [m]. While | F | > 1 doLet {Ci, Cj} F can be any pair to minimize f, \u03c4 (Ci, Cj) as calculated by Proposition 3.8. Remove {Ci, Cj} from F, add Ci-Cj.end while you return the single tree within F.single merge cost, a brute force implementation (over m points) takes up space O (m) and time O (m3T, Cj) as it is calculated."}, {"heading": "7. Empirical results", "text": "There are two types of data: Euclidean (dots in some Rn) and text data. There are three Euclidean datasets: UCI's Glass (214 dots in R9); 3s and 5s from the mnist digit recognition problem (1984 training digits and 1984 check digits in R49, reduced from the original 28x28); UCI's Spambase data (2301 train and 2300 test points in R57). Text data is drawn from the 20 newsgroups data, which have a vocabulary of 61,188 words and 1984 check digits in R49; a difficult sealotomy (20n-h), the pair of test documents (8556 / 7556 train), and the different types of cryptogram (20n-h)."}, {"heading": "7.1. Cluster compatibility", "text": "Table 1 contains cluster purity values, a standard measure of dendrogram quality, which is defined as follows: for two points with the same denomination l, you will find the smallest cluster C in the tree, which contains both; the purity in relation to these two points is a fraction of C with the denomination l. The purity of the dendrogram is the weighted sum of paired purity across all pairs of points. The data from glass, spam and 20 newsgroups appear in Heller & Ghahramani (2005); although a direct comparison is difficult as these experiments used sub-sample and randomized purity, the Euclidean experiments produce similar results, and the text experiments perform slightly better here. For another experiment, which now evaluates the viability of agglomerate as the initialization of EM on mixtures of molecules, see Appendix F."}, {"heading": "7.2. Feature generation", "text": "The final experiment consists in the use of dendrograms constructed from training data to generate characteristics for classification tasks. In view of a budget of characteristics k, the uppermost k clusters {Ci} ki = 1 of a specified dendrogram are selected, and for any example x is the ith characteristic \u2206 (Ci, {x}).Statistically, this characteristic measures the amount by which the model probability deteriorates when Ci is adapted to the fit to x. Selection of the tree was based on the purity of the training set from Table 1. In all tests, the original characteristics are discarded (i.e., only the k-generated characteristics are used).Figure 2 shows the performance of logistic regression classifiers based on tree characteristics as well as SVD characteristics."}, {"heading": "Acknowledgements", "text": "This work was thankfully supported by NSF under the grant IIS-0713540."}], "references": [{"title": "Relative loss bounds for on-line density estimation with the exponential family of distributions", "author": ["Azoury", "Katy S", "Warmuth", "Manfred K"], "venue": "Machine Learning,", "citeRegEx": "Azoury et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Azoury et al\\.", "year": 2001}, {"title": "Clustering with Bregman divergences", "author": ["Banerjee", "Arindam", "Merugu", "Srujana", "Dhillon", "Inderjit S", "Ghosh", "Joydeep"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Convex Analysis and Nonlinear Optimization", "author": ["Borwein", "Jonathan", "Lewis", "Adrian"], "venue": null, "citeRegEx": "Borwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Borwein et al\\.", "year": 2000}, {"title": "Fundamentals of Statistical Exponential Families", "author": ["Brown", "Lawrence D"], "venue": "Insitute of Mathematical Statistics,", "citeRegEx": "Brown and D.,? \\Q1986\\E", "shortCiteRegEx": "Brown and D.", "year": 1986}, {"title": "Generalized minimizers of convex integral functionals, Bregman distance, Pythagorean identities", "author": ["Csisz\u00e1r", "Imre", "Mat\u00fa\u0161", "Frant\u01d0sek"], "venue": null, "citeRegEx": "Csisz\u00e1r et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Csisz\u00e1r et al\\.", "year": 2012}, {"title": "Real analysis: modern techniques and their applicatins", "author": ["Folland", "Gerald B"], "venue": "Wiley Interscience,", "citeRegEx": "Folland and B.,? \\Q1999\\E", "shortCiteRegEx": "Folland and B.", "year": 1999}, {"title": "Algorithms for model-based gaussian hierarchical clustering", "author": ["C. Fraley"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Fraley,? \\Q1998\\E", "shortCiteRegEx": "Fraley", "year": 1998}, {"title": "Hierarchical gaussian mixture model", "author": ["Garcia", "Vincent", "Nielsen", "Frank", "Nock", "Richard"], "venue": "In ICASSP,", "citeRegEx": "Garcia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2010}, {"title": "Approximate Solutions to Markov Decision Processes", "author": ["Gordon", "Geoff J"], "venue": "PhD thesis,", "citeRegEx": "Gordon and J.,? \\Q1999\\E", "shortCiteRegEx": "Gordon and J.", "year": 1999}, {"title": "Bayesian hierarchical clustering", "author": ["Heller", "Katherine A", "Ghahramani", "Zoubin"], "venue": "In ICML,", "citeRegEx": "Heller et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Heller et al\\.", "year": 2005}, {"title": "Fundamentals of Convex Analysis", "author": ["Hiriart-Urruty", "Jean-Baptiste", "Lemar\u00e9chal", "Claude"], "venue": null, "citeRegEx": "Hiriart.Urruty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hiriart.Urruty et al\\.", "year": 2001}, {"title": "Hierarchical bayesian clustering for automatic text classification", "author": ["Iwayama", "Makoto", "Tokunaga", "Takenobu"], "venue": "In IJCAI,", "citeRegEx": "Iwayama et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Iwayama et al\\.", "year": 1995}, {"title": "Proximal minimization methods with generalized Bregman functions", "author": ["Kiwiel", "Krzysztof C"], "venue": "SIAM journal on control and optimization,", "citeRegEx": "Kiwiel and C.,? \\Q1995\\E", "shortCiteRegEx": "Kiwiel and C.", "year": 1995}, {"title": "Distributed Learning using Generative Models", "author": ["Merugu", "Srujana"], "venue": "PhD thesis, University of Texas, Austin,", "citeRegEx": "Merugu and Srujana.,? \\Q2006\\E", "shortCiteRegEx": "Merugu and Srujana.", "year": 2006}, {"title": "A Survey of Recent Advances in Hierarchical Clustering Algorithms", "author": ["Murtagh", "Fionn"], "venue": "The Computer Journal,", "citeRegEx": "Murtagh and Fionn.,? \\Q1983\\E", "shortCiteRegEx": "Murtagh and Fionn.", "year": 1983}, {"title": "Convex Analysis", "author": ["Rockafellar", "R. Tyrrell"], "venue": null, "citeRegEx": "Rockafellar and Tyrrell.,? \\Q1970\\E", "shortCiteRegEx": "Rockafellar and Tyrrell.", "year": 1970}, {"title": "Agglomerative information bottleneck", "author": ["Slonim", "Noam", "Tishby", "Naftali"], "venue": "pp. 617\u2013623,", "citeRegEx": "Slonim et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Slonim et al\\.", "year": 1999}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["Wainwright", "Martin J", "Jordan", "Michael I"], "venue": "Now Publishers Inc.,", "citeRegEx": "Wainwright et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2008}, {"title": "Hierarchical grouping to optimize an objective function", "author": ["Ward", "Joe H"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ward and H.,? \\Q1963\\E", "shortCiteRegEx": "Ward and H.", "year": 1963}], "referenceMentions": [{"referenceID": 1, "context": "There is already a rich theory of clustering with Bregman divergences, and in particular the relationship of these divergences with exponential family distributions (Banerjee et al., 2005).", "startOffset": 165, "endOffset": 188}, {"referenceID": 1, "context": "Section 3 will revisit the standard Bregman hard clustering model (Banerjee et al., 2005), and show how it naturally leads to a merge cost \u2206.", "startOffset": 66, "endOffset": 89}, {"referenceID": 6, "context": "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999).", "startOffset": 267, "endOffset": 401}, {"referenceID": 7, "context": "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999).", "startOffset": 267, "endOffset": 401}, {"referenceID": 6, "context": "A number of works present agglomerative schemes for clustering with exponential families, from the perspective of KL divergences between distributions, or the analogous goal of maximizing model likelihood, or lastly in connection to the information bottleneck method (Iwayama & Tokunaga, 1995; Fraley, 1998; Heller & Ghahramani, 2005; Garcia et al., 2010; Blundell et al., 2010; Slonim & Tishby, 1999). Furthermore, Merugu (2006) studied the same algorithm as the present work, phrased in terms of Bregman divergences.", "startOffset": 294, "endOffset": 430}, {"referenceID": 1, "context": "The development here of exponential families and related Bregman properties generalizes results found in a variety of sources (Brown, 1986; Azoury & Warmuth, 2001; Banerjee et al., 2005; Wainwright & Jordan, 2008); further bibliographic remarks will appear throughout, and in Appendix G.", "startOffset": 126, "endOffset": 213}, {"referenceID": 1, "context": "(This cost was the basis for Bregman hard clustering (Banerjee et al., 2005).", "startOffset": 53, "endOffset": 76}], "year": 2012, "abstractText": "This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondifferentiable convex functions.", "creator": "pdfsam-console (Ver. 2.0.6e)"}}}