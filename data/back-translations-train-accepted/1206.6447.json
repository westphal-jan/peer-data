{"id": "1206.6447", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering", "abstract": "Functional neuroimaging can measure the brain?s response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (1437kb)", "http://arxiv.org/abs/1206.6447v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.AP stat.ML", "authors": ["ga\u00ebl varoquaux", "alexandre gramfort", "bertrand thirion"], "accepted": true, "id": "1206.6447"}, "pdf": {"name": "1206.6447.pdf", "metadata": {"source": "META", "title": "Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering", "authors": ["Ga\u00ebl Varoquaux", "Alexandre Gramfort"], "emails": ["gael.varoquaux@inria.fr", "alexandre.gramfort@inria.fr", "bertrand.thirion@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "It is classically a classic problem driven by a linear task. There are statistical links between the observed brain images and the corresponding behavior of the subjects, which can be formulated as a machine learning problem (Mitchell et al., 2004). In fact, predicting neuroimaging data has led to impressive results, such as guessing which image a subject is looking at. Usually, the main objective of a functional neuroimaging study is not prediction per se, but brain mapping, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner (s). of his brain activity (Haxby et al., 2001). The main objective of a functional neuroimaging study is prediction per se, but brain mapping, which is the identification of the brain regions involved in cognitive processing."}, {"heading": "2. Preliminaries: small-sample estimation of sparse linear models", "text": "The problem at hand is the restoration of sparse signals, for which only a limited number of effects is required. (The problem at hand is the restoration of economical models in the presence of noise.) The problem at hand is the restoration of efficiency. (The problem at hand is the restoration of efficiency.) The problem at hand is the restoration of efficiency. (The problem at hand is the restoration of efficiency.) The problem at hand is the restoration of efficiency. (The problem at hand is that the applicability of a few hundred images will typically be while thousands of images of voxels. This is the number of samples that are bound to the estimate of a linear model with n observations and p variables.) (The problem at hand is O (p)."}, {"heading": "3. Randomization and clustering for sparse recovery in brain imaging", "text": "Our work is based on the random result that we scale only a fraction of the training samples and random scales of each variable in our case each vocabulary. By repeating the later procedure and then counting each variable, each variable is selected through the repetitions, each variable being mappable. Higher scores denote variables that are likely to be among the true vocabulary. Let's show the repetition and the corresponding estimated coefficients."}, {"heading": "4. Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Synthetic data", "text": "In fact, we are in a position to take the lead, \"he told the German Press Agency.\" It's not easy, \"he said,\" but we have to do it. \"\" We have done it, \"he said,\" we have done it. \""}, {"heading": "4.2. FMRI data", "text": "In order to assess the performance of our approach on real fMRI data, we examined two sets of data. The first, by Tom et al. (2007), is a game task in which the subject is asked to accept or reject games of chance that offer a 50 / 50 chance of winning or losing money. Each game has an amount that can be used as a target in a regression setting; the second, by Haxby et al. (2001), is a visual object recognition task. Each object category can then be used as a target in a classification setting. In this setting, we use a frugal logistic regression model instead of a lasso. We refer to Tom et al. (2007) and Haxby et al. (2001) for a detailed description of the experimental protocol. Data are publicly available at http: / / openfmri.org.Regression After standard pre-processing (slice timing, motion analysis with a general pro-linear model, subjective regression, the regression data have been subjected)."}, {"heading": "5. Conclusion", "text": "Motivated by the problem of brain imaging using functional MRI data, we address the problem of identifying support patterns when working with strongly spa-correlated designs and a small number of samples. We propose to use clustering of variables and randomization techniques to position the problem in situations where good recovery is achievable, i.e. a low correlation between variables in support and a limited number of variables compared to the number of samples. Furthermore, informed by the strong spatial correlations between voxels in fMRI data, we spatially restrict the clusters. This constraint injects additional predictions into the estimate to facilitate the recovery of spatially coherent support patterns. Our formulation enables the use of computationally efficient linear regression models and provides an overall algorithmic complexity that is linear in the number of features."}], "references": [{"title": "Bolasso: model consistent lasso estimation through the bootstrap", "author": ["F. Bach"], "venue": "In ICML, pp", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Self-concordant analysis for logistic regression", "author": ["F. Bach"], "venue": "Elec J Stat,", "citeRegEx": "Bach,? \\Q2010\\E", "shortCiteRegEx": "Bach", "year": 2010}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E.J. Candes", "J.K. Romberg", "T. Tao"], "venue": "Comm. Pure Appl. Math.,", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Prediction and interpretation of distributed neural activity with sparse models", "author": ["M.K. Carroll", "G.A. Cecchi", "I. Rish", "R. Garg", "A.R. Rao"], "venue": null, "citeRegEx": "Carroll et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Carroll et al\\.", "year": 2009}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C-C", "Lin", "C-J"], "venue": "ACM Trans Intell Syst Tech,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Trace lasso: a trace norm regularization for correlated designs", "author": ["E. Grave", "G.R. Obozinski", "F. Bach"], "venue": "In Adv NIPS,", "citeRegEx": "Grave et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grave et al\\.", "year": 2011}, {"title": "The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures", "author": ["Haury", "A-C", "P Gestraud", "Vert", "J-P"], "venue": "PLoS ONE, pp", "citeRegEx": "Haury et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Haury et al\\.", "year": 2011}, {"title": "Distributed and overlapping representations of faces and objects in ventral temporal cortex", "author": ["J.V. Haxby", "I.M. Gobbini", "Furey", "M.L"], "venue": "Science, 293:2425,", "citeRegEx": "Haxby et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Haxby et al\\.", "year": 2001}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J.P. Vert"], "venue": "In ICML, pp", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "On model consistency of the elastic net when p n", "author": ["J. Jia", "B. Yu"], "venue": "Statistica Sinica,", "citeRegEx": "Jia and Yu,? \\Q2010\\E", "shortCiteRegEx": "Jia and Yu", "year": 2010}, {"title": "Analyses of regional-average activation and multivoxel pattern information tell complementary", "author": ["K. Jimura", "R. Poldrack"], "venue": "stories. Neuropsychologia,", "citeRegEx": "Jimura and Poldrack,? \\Q2012\\E", "shortCiteRegEx": "Jimura and Poldrack", "year": 2012}, {"title": "Matching pursuits with timefrequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "Trans Signal Process,", "citeRegEx": "Mallat and Zhang,? \\Q1993\\E", "shortCiteRegEx": "Mallat and Zhang", "year": 1993}, {"title": "A supervised clustering approach for fMRI-based inference of brain states", "author": ["V. Michel", "A. Gramfort", "G Varoquaux"], "venue": "Pattern Recognition,", "citeRegEx": "Michel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Michel et al\\.", "year": 2012}, {"title": "Learning to decode cognitive states from brain images", "author": ["T.M. Mitchell", "R. Hutchinson", "R.S. Niculescu", "F Pereira"], "venue": "Machine Learning,", "citeRegEx": "Mitchell et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2004}, {"title": "Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data", "author": ["S. Monti", "P. Tamayo", "J. Mesirov", "T. Golub"], "venue": "Machine learning,", "citeRegEx": "Monti et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Monti et al\\.", "year": 2003}, {"title": "Modern hierarchical, agglomerative clustering algorithms", "author": ["D. M\u00fcllner"], "venue": "Arxiv preprint arXiv:1109.2378,", "citeRegEx": "M\u00fcllner,? \\Q2011\\E", "shortCiteRegEx": "M\u00fcllner", "year": 2011}, {"title": "Feature selection, l1 vs. l2 regularization, and rotational invariance", "author": ["A. Ng"], "venue": "In ICML, pp", "citeRegEx": "Ng,? \\Q2004\\E", "shortCiteRegEx": "Ng", "year": 2004}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A Gramfort"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Statistical Society B,", "citeRegEx": "Tibshirani,? \\Q1994\\E", "shortCiteRegEx": "Tibshirani", "year": 1994}, {"title": "The neural basis of loss aversion in decision-making under risk", "author": ["S.M. Tom", "C.R. Fox", "C. Trepel", "R. Poldrack"], "venue": "Science, 315:515,", "citeRegEx": "Tom et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tom et al\\.", "year": 2007}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "Trans Inf Theory,", "citeRegEx": "Tropp,? \\Q2004\\E", "shortCiteRegEx": "Tropp", "year": 2004}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming", "author": ["M. Wainwright"], "venue": "Trans Inf Theory,", "citeRegEx": "Wainwright,? \\Q2009\\E", "shortCiteRegEx": "Wainwright", "year": 2009}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Roy. Statistical Society B,", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "J. Amer. Statistical Assoc.,", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "J. Roy. Statistical Society B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 14, "context": "Learning statistical links between the observed brain images and the corresponding subject\u2019s behavior can be formulated as a machine learning problem (Mitchell et al., 2004).", "startOffset": 150, "endOffset": 173}, {"referenceID": 8, "context": "from his brain activity (Haxby et al., 2001).", "startOffset": 24, "endOffset": 44}, {"referenceID": 3, "context": "There is great potential interest to use sparse recovery techniques (Carroll et al., 2009), that can recover active voxels suffering only a loss in detection power sub-linear in the number of voxels.", "startOffset": 68, "endOffset": 90}, {"referenceID": 7, "context": "However, with brain mapping as with many other experimental fields, the design matrix is imposed by the problem, and due to the strong correlations across regressors, univariate approaches are often more effective than multivariate approaches (Haury et al., 2011).", "startOffset": 243, "endOffset": 263}, {"referenceID": 3, "context": "Unlike previous work in fMRI (Carroll et al., 2009), we focus on recovery and not predictive power.", "startOffset": 29, "endOffset": 51}, {"referenceID": 22, "context": "linear models for which k p coefficients are non-zeros, only O ( k log(p\u2212k) ) observations are required to identify these coefficients (Wainwright, 2009; Candes et al., 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 135, "endOffset": 174}, {"referenceID": 2, "context": "linear models for which k p coefficients are non-zeros, only O ( k log(p\u2212k) ) observations are required to identify these coefficients (Wainwright, 2009; Candes et al., 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 135, "endOffset": 174}, {"referenceID": 19, "context": ", 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 146, "endOffset": 164}, {"referenceID": 21, "context": ", 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 187, "endOffset": 200}, {"referenceID": 2, "context": "i) subsets of the columns of design matrix X larger than k should be well conditioned, as for instance implied by the Restricted Isometry Property (RIP) (Candes et al., 2006).", "startOffset": 153, "endOffset": 174}, {"referenceID": 21, "context": "ii) the regressors on the signal subspace XS should not be too correlated to regressors on the noise subspace XS , as formalized by Tropp\u2019s Exact Recovery Condition (ERC) (Tropp, 2004) or for the `1-penalized regression the irrepresentable condition (Zhao & Yu, 2006), or mutual incoherence (Wainwright, 2009).", "startOffset": 171, "endOffset": 184}, {"referenceID": 22, "context": "ii) the regressors on the signal subspace XS should not be too correlated to regressors on the noise subspace XS , as formalized by Tropp\u2019s Exact Recovery Condition (ERC) (Tropp, 2004) or for the `1-penalized regression the irrepresentable condition (Zhao & Yu, 2006), or mutual incoherence (Wainwright, 2009).", "startOffset": 291, "endOffset": 309}, {"referenceID": 22, "context": "The number of observations necessary and sufficient to almost surely recover the sparsity grows as nmin = 2 \u03b8 k log(p \u2212 k) where \u03b8 depends on the various factors listed previously (Wainwright, 2009).", "startOffset": 180, "endOffset": 198}, {"referenceID": 9, "context": "For image data, correlation across neighboring voxels can be modeled using overlapping groups (Jacob et al., 2009).", "startOffset": 94, "endOffset": 114}, {"referenceID": 6, "context": "When the group structure is unknown, Grave et al. (2011) proposed the trace-lasso, adapting the penalization to the design matrix.", "startOffset": 37, "endOffset": 57}, {"referenceID": 0, "context": "For these reasons, Bach (2008) and Meinshausen & B\u00fchlmann (2010) introduce resampled estimators based on the Lasso.", "startOffset": 19, "endOffset": 31}, {"referenceID": 0, "context": "For these reasons, Bach (2008) and Meinshausen & B\u00fchlmann (2010) introduce resampled estimators based on the Lasso.", "startOffset": 19, "endOffset": 65}, {"referenceID": 0, "context": "Bach (2010) extends non-asymptotic least-square results to logistic regression by approximating it as a weighted least square and show that similar conditions on the design apply for sparse recovery with `1 penalization.", "startOffset": 0, "endOffset": 12}, {"referenceID": 0, "context": "Bach (2010) extends non-asymptotic least-square results to logistic regression by approximating it as a weighted least square and show that similar conditions on the design apply for sparse recovery with `1 penalization. Small sample learning rates for `1-penalized logistic regression were established earlier by Ng (2004) based on the non rotational invariance of the `1 ball.", "startOffset": 0, "endOffset": 324}, {"referenceID": 13, "context": "Following Michel et al. (2012), we use spatially-constrained Ward hierarchical clustering to cluster the voxels in q spatially connected regions in which the signal is averaged.", "startOffset": 10, "endOffset": 31}, {"referenceID": 15, "context": "Similarly, in consensus clustering (Monti et al., 2003) resampling is applied to the construction of clusters to account for the sensibility of the algorithms, that are most often based on greedy approaches to non-convex problems.", "startOffset": 35, "endOffset": 55}, {"referenceID": 16, "context": "Indeed, agglomerative clustering can be implemented in linear time with regards to the number of possible agglomerations tested (M\u00fcllner, 2011).", "startOffset": 128, "endOffset": 143}, {"referenceID": 6, "context": "Grave et al. (2011).", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "Results We investigated two non-convex approaches: a greedy algorithm, orthogonal matching pursuit (Mallat & Zhang, 1993), and an iterative approach, the adaptive Lasso (Zou, 2006).", "startOffset": 169, "endOffset": 180}, {"referenceID": 19, "context": "The first one from Tom et al. (2007) is a gambling task where the subject is asked to accept or reject gambles that offered a 50/50 chance of gaining or losing money.", "startOffset": 19, "endOffset": 37}, {"referenceID": 8, "context": "The second dataset from Haxby et al. (2001) is a visual object recognition task.", "startOffset": 24, "endOffset": 44}, {"referenceID": 8, "context": "The second dataset from Haxby et al. (2001) is a visual object recognition task. Each object category can then be used as target in a classification setting. In this setting, we use a sparse logistic regression model instead of a Lasso. We refer to Tom et al. (2007) and Haxby et al.", "startOffset": 24, "endOffset": 267}, {"referenceID": 8, "context": "The second dataset from Haxby et al. (2001) is a visual object recognition task. Each object category can then be used as target in a classification setting. In this setting, we use a sparse logistic regression model instead of a Lasso. We refer to Tom et al. (2007) and Haxby et al. (2001) for a detailed description of the experimental protocol.", "startOffset": 24, "endOffset": 291}, {"referenceID": 7, "context": "As for such real data the ground truth is not known, we test these qualitative observations by training a predictive model on the voxels with the highest scores as in (Haury et al., 2011).", "startOffset": 167, "endOffset": 187}, {"referenceID": 18, "context": "Experiments were performed with the scikit-learn (Pedregosa et al., 2011), using LIBLINEAR (Fan et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 5, "context": ", 2011), using LIBLINEAR (Fan et al., 2008) for logistic regression and LIBSVM (Chang & Lin, 2011) for SVM.", "startOffset": 25, "endOffset": 43}], "year": 2012, "abstractText": "Functional neuroimaging can measure the brain\u2019s response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets.", "creator": "LaTeX with hyperref package"}}}