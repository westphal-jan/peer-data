{"id": "1206.6448", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Online Alternating Direction Method", "abstract": "Online optimization has emerged as powerful tool in large scale optimization. In this paper, we introduce efficient online algorithms based on the alternating directions method (ADM). We introduce a new proof technique for ADM in the batch setting, which yields the O(1/T) convergence rate of ADM and forms the basis of regret analysis in the online setting. We consider two scenarios in the online setting, based on whether the solution needs to lie in the feasible set or not. In both settings, we establish regret bounds for both the objective function as well as constraint violation for general and strongly convex functions. Preliminary results are presented to illustrate the performance of the proposed algorithms.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (589kb)", "http://arxiv.org/abs/1206.6448v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["huahua wang", "arindam banerjee"], "accepted": true, "id": "1206.6448"}, "pdf": {"name": "1206.6448.pdf", "metadata": {"source": "META", "title": "Online Alternating Direction Method", "authors": ["Huahua Wang", "Arindam Banerjee"], "emails": ["HUWANG@CS.UMN.EDU", "BANERJEE@CS.UMN.EDU"], "sections": [{"heading": "1. Introduction", "text": "In recent years, the number of those able to reform, to soar, has soared. (...) In recent years, the number of those able to reform has soared. (...) In the second half of the year, the number of those able to reform has increased. (...) In the third half of the year, the number of those able to reform in the first half of the year in the second half of the year in the second half of the year in the third half of the year in the second half of the year in the second half of the year in the second half of the year in the third half of the year in the second half of the year in the third half of the third half of the year in the second half of the year in the second half of the year in the third half of the third half of the year in the third half of the second half of the year in the second half of the year in the second half of the year in the third half of the year in the third half of the third half of the year in the third half of the third half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the second half of the year in the second half of the third of the year in the third of the third half of the year in the third half of the third of the third half of the year in the third half of the third of the year in the third of the third half of the third half of the third of the third half of the third of the third of the year in the third of the third of the third of the third of the year in the third half of the third half of the third of the third half of the third of the third half of the third half of the year in the third of the third of the third half of the third of the third half of the third half of the year in the third of the third half of the third of the third half of the third half of the year in the third of the third of the third of the third half of the third of the third of the third of the third half of the third of the third of the third of the year in the third of the third of the third of the third half of the third half of the third half of the third of the third of the third of the third of the third of the third of the third of the third of the third of the"}, {"heading": "2. Analysis for Batch ADM", "text": "The advanced Lagrange problem for (1) isL\u03c1 (x, y, z) = f (x) + g (z) + < y, Ax + Bz \u2212 c > + 0, to convergence (Boyd et al., 2010) performs the following three steps iteratively: xt + 1 = argmin xf (x) + < yt, Ax + Bzt \u2212 c > + Bzt \u2212 c, 2, (3) zt + 1 = argmin zg (z) + < yt, axe + 1 + Bz \u2212 c > + Bz \u2212 c, (4) zzt \u2212 c \u00b2, to convergence (z) + < yt, axe + 1 + Bz \u2212 z, to convergence analysis."}, {"heading": "2.1. Bounds for Objective and Constraints", "text": "The following theorem shows that both the cumulative objective difference from the optimal and the cumulative norms of constraints known as primary and dual residuals (Boyd et al., 2010) are limited by constants independent of the number of iterations T. Theorem 1 Let us generate the sequences {xt, zt, yt} from ADM. For each x *, z * satisfactory Ax * + Bz * = c, for each T we have T * t = 0 [f (xt + 1) + g (zt + 1) \u2212 (f (x *) + g (z *)] \u2264 \u03bbBmaxD 2 z\u03c1 2, (6) T * t = 0 \u0445 ax + 1 \u2212 c * 22 + GTBzt + 1 \u2212 Bzt \u00b2 22 \u2264 BmaxD2z + g (z *)]."}, {"heading": "2.2. Rate of Convergence of ADM", "text": "We now show the convergence rate O (1 / T) for ADM using a variable inequality (VI) based on the Lagrange value given in (2). Let us determine the convergence rate O (1 / T) based on a variable inequality (VI) that optimally solves the original problem in (1) if it meets the following variable inequality (Facchinei & Pang, 2003; Nemirovski, 2004): Any w (w), h (w) \u2212 h (w) + (w) TF (w) \u2265 0, (8) where F (w) T = [yTA yTB \u2212 (Ax + Bz \u2212 c) T] is the gradient of the last term of the Lagrange, and h (w) = f (x) + g (z) + g (z). Then we show w (T = (x), z (z), y) approximate the solution of the problem with precision if it meets the last term of the value + w (f) and (x) (z)."}, {"heading": "3. Online ADM", "text": "In this section, we extend the ADM iteratively to the online learning environment, so that the existing methods are fulfilled. Specifically, we focus on the use of online ADM (OADM) to solve the problem in (1). For our analysis, A and B are assumed to be fixed solutions. (1) We consider the solution of the following regulated optimization problems: xt + 1 = argmin Ax + Bz = c ft (x) + g (z) + g (z) + n (x, xt), where we apply a learning rate and Bregman divergence base (x, xt). (2) If the above problem is solved in each step, standard analysis techniques (Hazan et al., 2007) can be applied to achieve sublinear regrets. (10) While we can be solved by batch ADM, we essentially get a double loop in which the functional changes in the outer loop and the inner loop run."}, {"heading": "4. Regret Analysis of OADM", "text": "As described in section 3, we consider two types of regret in OADM: The first type is regret of the target, which is based on variable splitting, i.e. R1 (T) = T \u2211 t = 0 ft (xt) + g (zt) \u2212 min Ax + Bz = c T \u2211 t = 0 ft (x) + g (z). (18) Apart from the use of splitting variables, R1 is the default regret in the online learning environment. The second is regret of the restriction violation defined in (17) Rc."}, {"heading": "4.1. General Convex Functions", "text": "In the following, the limits of regret are set for OADM. Theorem 3 Let the sequences {xt, zt, yt} be generated by OADM and assumptions (1) - (4) apply. For each x *, z * satisfactory Ax * + Bz * = c setting \u03b7 = Gf * TDx \u221a 2\u03b1 and \u03c1 = \u221a T, we have R1 (T) \u2264 BmaxD2z \u221a T / 2 + \u221a 2GfDx \u221a T / \u221a \u03b1, Rc (T) \u2264 BmaxD2z + \u221a 2DxGf / \u221a \u03b1 + 2F \u221a T. Note that the limits are reached without explicit assumptions for A, B, c.1 The subgradient of regret must be limited, but the subgradient of g is not necessarily limited. Thus, the limits apply in the case that g is an indicator function of a convex road. In addition to O (T) the regret is limited."}, {"heading": "4.2. Strongly Convex Functions", "text": "We assume that ft (x) and g are strongly convex. Specifically, we assume that ft (x) is strongly convex in terms of a differentiable function \u03c6 \u03b21-strongly convex, i.e. ft (x) \u2265 ft (x) + < f \u2032 t (x), x * \u2212 x > + \u03b21B\u03c6 (x *, xt + 1), (19) where \u03b21 > 0 and g is a \u03b22-strongly convex function, i.e. g (z *) \u2265 g (z) + < g \u2032 (z), z \u0445 \u2212 z > + \u03b22 \u0445 z, (20) where \u03b22 > 0 can be specified. Theorem 4 Let us apply assumptions (1) - (4). Assumptions ft (x) and g are strongly convex in (19) and (20) \u03b2.For all explicitly satisfactory A\u03bbx + 2x BDA, they must be (1) and (2) DDA (2.0) and (2.0) DA + 2 and (2) respectively."}, {"heading": "5. Regret Analysis of OADM with \u03b7 = 0", "text": "In this case, OADM has the same updates as ADM. For analysis, we consider zt as the central primordial variable and calculate x-t using zt, so that Ax-t + Bzt = c. Since (x-t, zt) naturally fulfils the constraints, we consider the following regrets: R2 (T) = T-t = 0 ft (x-t) + g (zt) \u2212 min Ax + Bz = c T-t = 0 ft (x-t) + g (z). (21) Where Ax-t + Bzt = c. A common case we often find is when A = I, B = \u2212 I, c = 0, i.e. x-t = zt. While {x-t, zt} the equality constraint is fulfilled, (xt, zt) axe + Bzt -c = 0 does not have to be fulfilled."}, {"heading": "5.1. General Convex Functions", "text": "The following theorem shows the limits of regret. Theorem 5 Leave \u03b7 = 0 in OADM and assumptions (1) - (4) and A is reversible hold. For each x *, z * satisfactory Ax * + Bz * = c, setting \u03c1 = Gf * TDz * * Amin\u03bb B max, we have R2 (T) \u2264 GfDz * \u03bbBmaxT / \u03bb A min, Rc (T) \u2264 \u03bbBmaxD2z + 2FDz. [Amin\u03bb B maxT / Gf]. Without requiring an additional Bregman divergence, R2 reaches the \u221a T limit as R1. While R1 depends on xt, which may not remain in the realizable amount, R2 is defined by x, which always meets the equality constraint. The corresponding algorithm requires finding in each iteration x-t = c \u2212 Bzt, which is the solution of a linear system, for example, the algorithm will be efficient in some settings."}, {"heading": "5.2. Strongly Convex Functions", "text": "The following theorem specifies the logarithmic regret, which assumes g is \u03b2-strongly convex as indicated in (20). Theorem 6 Let \u03b7 = 0 in OADM. Suppose g (z) \u03b22is strongly convex, A is invertable and assumptions (1) - (4) apply. If we set \u03c1t = \u03b22t / \u03bbBmax, we have R2 (T) \u2264 G2f\u03bb B max2\u03bbAmin\u03b22 (log (T + 1)) + \u03b22D2 z, (22) Rc (T) \u2264 BmaxD2z + 2F\u03bbBmax log (T + 1) / \u03b22. (23) In contrast to Theorem 4, theorem 6 shows that OADM can achieve the logarithmic regret without ft having to be strongly convex, which is consistent with other online learning algorithms for compound goals."}, {"heading": "6. Connections to Related Work", "text": "In this section, we assume that the three steps of OADM lead to a reduction of toxt + 1 = argmin xft (x) + < yt, x \u2212 zt > + 0, x \u2212 zt > + 0, z + 1 = yt + 1 (xt + 1 \u2212 zt) 2, (24) zt + 1 = argmin zg (z) + < yt, xt + 1 \u2212 z (z) 2, (25) yt + 1 = yt + 1 \u2212 zt (xt + 1 \u2212 zt + 1). (26) Let f \u2032 t (xt + 1) \u0445 ft (x), g \u2032 t (zt + 1), x \u2212 z (z)."}, {"heading": "7. Experimental Results", "text": "In this section, we use OADM to solve generalized lasso problems (Boyd et al., 2010), including lasso (Tibshirani, 1996) and total variation (TV) (Rudin et al., 1992). We present simulation results to show the convergence of objective and constraints in OADM. We also compare it with Batch ADM and other two online learning algorithms: FOBOS and Regulated Dual Mean (RDA) in selecting sparse dimensions in the lasso and recovering data in total variation."}, {"heading": "7.1. Generalized Lasso", "text": "The generalized lasso problem is formulated as follows: min x1N N \u2211 t = 1 \u0445 atx \u2212 bt = 22 + \u03bb | Dx | 1, (30) where D = I is an upper bidiagonal matrix with diagonal 1 and off-diagonal \u2212 1, (30) is the total variation. ADM form of (30) is: min Dx = z1N N \u2211 t = 1 \u2211 atx \u2212 bt \u0445 22 + \u03bb | z | 1, (31) where z-Rm \u00d7 1. The three updates of OADM are: xt + 1 = (a T at + \u03c1D TD + thays) \u2212 1v, (32) zt + 1 = Scipe / \u03c1 (x + u), (33) ut + xt \u2212 x, (34) where the problem is T = QT + thaises and we at T = 1b-v- (T)."}, {"heading": "7.2. Simulation", "text": "Our experiments follow the lasso and the total variation examples in Boyd's website, 2 although we have changed the codes to accommodate our setup. We first randomly generate A with N examples of dimensionality n. A is then normalized along the column. Then, a real x0 pattern is randomly calculated with certain online learning patterns for lasso and TV. b is calculated by adding the number of non-admissions (NNZs) k = 100 in x and various combinations of parameters from n = [1000, 5000] which facilitate matrix inversion in ADM and are cyclically traversed in the three online learning algorithms. We consider lasso the number of non-admissions (NNZs) k = 100 and try different parameters from n = [1000, 5000], \u03c1 = [0,1, 10] and q = [0,1, 0,5] for the learning algorithms."}, {"heading": "8. Conclusions", "text": "In this paper, we propose an efficient online learning algorithm called Online-ADM (OADM). New evidence techniques have been developed to analyze the convergence of ADM, which shows that ADM has a convergence rate of O (1 / T). Using the evidence techniques, we establish the limits of regret for the objective and limitation of the violation of general and strongly convex functions in OADM. Finally, we demonstrate the effectiveness of OADM in solving lasso and total variation."}, {"heading": "Acknowledgment", "text": "The research was supported by the NSF-CAREER Prize IIS0953274, and the NSF Prize for IIS-0916750, IIS-0812183 and IIS-1029711."}, {"heading": "A. Proof of Rate of Convergence of ADM", "text": "The proof: We begin by stating that the VI corresponds to the update of xt + 1 in (3) = < w (5), p (x) \u2212 f (xt + 1) + < x \u2212 xt + 1, AT \u2212 yt + 0 (axe + 1 + Bzt \u2212 c) > 0 (5), p (x + 1) \u2212 f (x) + < x \u2212 x (z) \u2212 g (zt + 1 > \u2264) < Ax \u2212 axe + 1, Bzt \u2212 c) > 0 (5), w (36) The VI corresponds to the update of zt + 1 in (4) \u2212 t (z) \u2212 g (z) \u2212 g (zt + 1 > \u2264) < z \u2212 z \u2212 zt + 1, BT {yt (axe + 1 + Bzt \u2212 b), w (36)."}], "references": [{"title": "Decomposition methods for large scale LP decoding", "author": ["S. Barman", "X. Liu", "S. Draper", "B. Recht"], "venue": "In Arxiv,", "citeRegEx": "Barman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Barman et al\\.", "year": 2012}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "author": ["A. Beck", "M. Teboulle"], "venue": "Operations Research Letters,", "citeRegEx": "Beck and Teboulle,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2003}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1989}, {"title": "Parallel Optimization: Theory, Algorithms, and Applications", "author": ["Y. Censor", "S. Zenios"], "venue": null, "citeRegEx": "Censor and Zenios,? \\Q1998\\E", "shortCiteRegEx": "Censor and Zenios", "year": 1998}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "JMLR, 10:2873\u20132898,", "citeRegEx": "Duchi and Singer,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer", "year": 2009}, {"title": "Composite objective mirror descent", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "In COLT,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Dual averaging for distributed optimization: Convergence analysis and network", "author": ["J. Duchi", "A. Agarwal", "M. Wainwright"], "venue": "scaling. arXiv,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators", "author": ["J. Eckstein", "D.P. Bertsekas"], "venue": "Mathematical Programming,", "citeRegEx": "Eckstein and Bertsekas,? \\Q1992\\E", "shortCiteRegEx": "Eckstein and Bertsekas", "year": 1992}, {"title": "Finite-Dimensional Variational Inequalities and Complementarity Problems, volume I", "author": ["F. Facchinei", "Pang", "J.-S"], "venue": null, "citeRegEx": "Facchinei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Facchinei et al\\.", "year": 2003}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite-element approximations", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers and Mathematics with Applications,", "citeRegEx": "Gabay and Mercier,? \\Q1976\\E", "shortCiteRegEx": "Gabay and Mercier", "year": 1976}, {"title": "Matrix Computations,3rd ed", "author": ["G.H. Golub", "C.V. Loan"], "venue": null, "citeRegEx": "Golub and Loan,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan", "year": 1996}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Stochastic approximation approach to stochastic programming", "author": ["A. Juditsky", "G. Lan", "A. Nemirovski", "A. Shapiro"], "venue": "SIAM J. Optim.,", "citeRegEx": "Juditsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Juditsky et al\\.", "year": 2009}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG09-2215,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "The TV patterns found by OADM, ADM, FOBOS and RDA. OADM is the best in recovering", "author": ["M. Mahdavi", "R. Jin", "T. Yang"], "venue": "Arxiv,", "citeRegEx": "Mahdavi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2011}, {"title": "Online learning with constraints", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "In COLT, pp", "citeRegEx": "Mannor and Tsitsiklis,? \\Q2006\\E", "shortCiteRegEx": "Mannor and Tsitsiklis", "year": 2006}, {"title": "An alternating direction method for dual MAP LP relaxation", "author": ["O. Meshi", "A. Globerson"], "venue": "In ECML11,", "citeRegEx": "Meshi and Globerson,? \\Q2011\\E", "shortCiteRegEx": "Meshi and Globerson", "year": 2011}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM J. Optim.,", "citeRegEx": "Nemirovski,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski", "year": 2004}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov,? \\Q2009\\E", "shortCiteRegEx": "Nesterov", "year": 2009}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L. Rudin", "S.J. Osher", "E. Fatemi"], "venue": "Physica D,", "citeRegEx": "Rudin et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 1992}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "JMLR, 11:2543\u20132596,", "citeRegEx": "Xiao,? \\Q2010\\E", "shortCiteRegEx": "Xiao", "year": 2010}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 22, "context": "In recent years, online learning (Zinkevich, 2003; Hazan et al., 2007) and its batch counterpart stochastic gradient descent (Juditsky et al.", "startOffset": 33, "endOffset": 70}, {"referenceID": 11, "context": "In recent years, online learning (Zinkevich, 2003; Hazan et al., 2007) and its batch counterpart stochastic gradient descent (Juditsky et al.", "startOffset": 33, "endOffset": 70}, {"referenceID": 12, "context": ", 2007) and its batch counterpart stochastic gradient descent (Juditsky et al., 2009) has contributed substantially to advances in large scale optimization techniques for machine learning.", "startOffset": 62, "endOffset": 85}, {"referenceID": 5, "context": "Online convex optimization has been generalized to handle time-varying and non-smooth convex functions (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010).", "startOffset": 103, "endOffset": 157}, {"referenceID": 21, "context": "Online convex optimization has been generalized to handle time-varying and non-smooth convex functions (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010).", "startOffset": 103, "endOffset": 157}, {"referenceID": 5, "context": "Composite objective mirror descent (COMID) (Duchi et al., 2010) generalizes mirror descent (Beck & Teboulle, 2003) to the online setting.", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "Regularized dual averaging (RDA) (Xiao, 2010) general-", "startOffset": 33, "endOffset": 45}, {"referenceID": 18, "context": "izes dual averaging (Nesterov, 2009) to online and composite optimization, and can be used for distributed optimization (Duchi et al.", "startOffset": 20, "endOffset": 36}, {"referenceID": 6, "context": "izes dual averaging (Nesterov, 2009) to online and composite optimization, and can be used for distributed optimization (Duchi et al., 2011).", "startOffset": 120, "endOffset": 140}, {"referenceID": 13, "context": "First introduced in (Gabay & Mercier, 1976), the alternating direction method (ADM) has become popular in recent years due to its ease of applicability and empirical performance in a wide variety of problems, including composite objectives (Boyd et al., 2010; Eckstein & Bertsekas, 1992; Lin et al., 2009).", "startOffset": 240, "endOffset": 305}, {"referenceID": 21, "context": "We also present preliminary experimental results illustrating the performance of the proposed OADM algorithms in comparison with FOBOS and RDA (Duchi & Singer, 2009; Xiao, 2010).", "startOffset": 143, "endOffset": 177}, {"referenceID": 14, "context": "Further, the notion of regret in both the objective as well as constraints may contribute towards development of suitable analysis tools for online constrained optimization problems (Mannor & Tsitsiklis, 2006; Mahdavi et al., 2011).", "startOffset": 182, "endOffset": 231}, {"referenceID": 17, "context": "Any w\u2217 = (x\u2217, z\u2217,y\u2217) \u2208 \u03a9 solves the original problem in (1) optimally if it satisfies the following variational inequality (Facchinei & Pang, 2003; Nemirovski, 2004):", "startOffset": 123, "endOffset": 165}, {"referenceID": 11, "context": "If the above problem is solved in every step, standard analysis techniques (Hazan et al., 2007) can be suitably adopted to obtain sublinear regret bounds.", "startOffset": 75, "endOffset": 95}, {"referenceID": 11, "context": "Note that existing online methods, such as projected gradient descent and variants (Hazan et al., 2007; Duchi et al., 2010) do assume a black-box approach for projecting onto the feasible set, which for linear constraints may require iterative cyclic projections (Censor & Zenios, 1998).", "startOffset": 83, "endOffset": 123}, {"referenceID": 5, "context": "Note that existing online methods, such as projected gradient descent and variants (Hazan et al., 2007; Duchi et al., 2010) do assume a black-box approach for projecting onto the feasible set, which for linear constraints may require iterative cyclic projections (Censor & Zenios, 1998).", "startOffset": 83, "endOffset": 123}, {"referenceID": 20, "context": "Examples include generalized lasso and group lasso (Boyd et al., 2010; Tibshirani, 1996; Xiao, 2010).", "startOffset": 51, "endOffset": 100}, {"referenceID": 21, "context": "Examples include generalized lasso and group lasso (Boyd et al., 2010; Tibshirani, 1996; Xiao, 2010).", "startOffset": 51, "endOffset": 100}, {"referenceID": 0, "context": "MAP LP relaxation (Meshi & Globerson, 2011) and LP decoding (Barman et al., 2012), and non-smooth optimization,", "startOffset": 60, "endOffset": 81}, {"referenceID": 13, "context": "robust PCA (Lin et al., 2009) where ft is nuclear norm and g is `1 norm.", "startOffset": 11, "endOffset": 29}, {"referenceID": 14, "context": "(4) For any t, ft(xt+1)+g(zt+1)\u2212(ft(z)+g(z)) \u2265 \u2212F , which is true if the functions are lower bounded or Lipschitz continuous in the convex set (Mahdavi et al., 2011).", "startOffset": 143, "endOffset": 165}, {"referenceID": 5, "context": "In addition to the O( \u221a T ) regret bound, OADM achieves the O( \u221a T ) bound for the constraint violation, which is not existent in the start-ofthe-art online learning algorithms (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010), since they do not explicitly handle linear constraints of the form Axt+Bz = c.", "startOffset": 177, "endOffset": 231}, {"referenceID": 21, "context": "In addition to the O( \u221a T ) regret bound, OADM achieves the O( \u221a T ) bound for the constraint violation, which is not existent in the start-ofthe-art online learning algorithms (Duchi et al., 2010; Duchi & Singer, 2009; Xiao, 2010), since they do not explicitly handle linear constraints of the form Axt+Bz = c.", "startOffset": 177, "endOffset": 231}, {"referenceID": 11, "context": "We recover the projected gradient descent (Hazan et al., 2007).", "startOffset": 42, "endOffset": 62}, {"referenceID": 20, "context": ", 2010), including lasso (Tibshirani, 1996) and total variation (TV)(Rudin et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 19, "context": ", 2010), including lasso (Tibshirani, 1996) and total variation (TV)(Rudin et al., 1992).", "startOffset": 68, "endOffset": 88}, {"referenceID": 21, "context": "The NNZs in FOBOS is large and oscillates in a big range, which has also been observed in (Xiao, 2010).", "startOffset": 90, "endOffset": 102}], "year": 2012, "abstractText": "Online optimization has emerged as powerful tool in large scale optimization. In this paper, we introduce efficient online algorithms based on the alternating directions method (ADM). We introduce a new proof technique for ADM in the batch setting, which yields the O(1/T ) convergence rate of ADM and forms the basis of regret analysis in the online setting. We consider two scenarios in the online setting, based on whether the solution needs to lie in the feasible set or not. In both settings, we establish regret bounds for both the objective function as well as constraint violation for general and strongly convex functions. Preliminary results are presented to illustrate the performance of the proposed algorithms.", "creator": "LaTeX with hyperref package"}}}