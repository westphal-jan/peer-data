{"id": "1206.6449", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Monte Carlo Bayesian Reinforcement Learning", "abstract": "Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space. The POMDP does not require conjugate distributions for belief representation, as earlier works do, and can be solved relatively easily with point-based approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (972kb)", "http://arxiv.org/abs/1206.6449v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yi wang 0006", "kok sung won", "david hsu", "wee sun lee"], "accepted": true, "id": "1206.6449"}, "pdf": {"name": "1206.6449.pdf", "metadata": {"source": "META", "title": "Monte Carlo Bayesian Reinforcement Learning", "authors": ["Yi Wang", "Kok Sung Won", "David Hsu", "Wee Sun Lee"], "emails": ["WANGY@COMP.NUS.EDU.SG", "KOKSUNG@COMP.NUS.EDU.SG", "DYHSU@COMP.NUS.EDU.SG", "LEEWS@COMP.NUS.EDU.SG"], "sections": [{"heading": "1. Introduction", "text": "A major obstacle to greater convergence is the slow convergence that requires many studies to learn effective policy. It explicitly represents uncertainty in the model parameters by maintaining a probability distribution over them, and selects measures that maximize the expected long-term reward in relation to this distribution. An approach to BRL is to occupy it as a partially observable Markov decision-making process (POMDP) P (Duff, 2002). The State of PAppearing in the Procedures of the 29th International Conference on Machine Learning, Edinburgh, UK, 2012."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. MDP and POMDP", "text": "An MDP is a tuple < S, A, T, R, GP >, where S is a set of world states, A is a set of actions, T (s, a, s) indicates the transition probability to reach the state s if it takes action in the states s, R (s, a, s). The value of a policy \u03c0 is defined as the expected cumulative discounted reward E (s, s), and \u03b3 is a discount factor.A policy \u03c0: S \u2192 A for an MDP is a function that indicates which action to take in each state s. The value of a policy \u03c0 is defined as the expected cumulative discounted reward E (s, t = 0 \u03b3tR (st), st + 1)), with the expectation regarding the random variable st being the state in step t, the state in step t, the state in step t, the state in step t. The goal of a policy \u03c0 is defined as the optimal political capacity to act, st + 1)), where the expectation with respect to the random variable st is the state, the state in step t, the state in step t, the state in step t."}, {"heading": "2.2. Related Works", "text": "A Common Approach for BRL adopts the proposal in (Duff, 2002) and presents BRL as a POMDP-P with a hybrid state space (Wang et al., 2005; Poupart et al., 2006; Ross et al., 2007; Castro & Precup, 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008). Maintaining the posterior belief in continuous model parameters requires either a closed presentation or effective approximate follow-up techniques. Instead of solving P directly, MC-BRL approaches it with a discrete POMDP-P, evaluating the previous distribution and taking advantage of recent advances in point-based discrete POMDP algorithms. In this way, we avoid the restrictive assumption of a near-formal faith representation and get a simpler and more general approach. Sampling was used in BRL (Castro & Precup, 2007; Ross et al.)."}, {"heading": "3. Monte Carlo BRL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. BRL as POMDP", "text": "To simplify the presentation, let us first consider BRL of an MDP. Faced with an MDP < S, A, T, R, \u03b3 >, the task of the BRL is to find an optimal policy if the transition function T is unknown. It has been shown that the BRL problem is called POMDP P = < SP, AP, OP, TP, ZP, \u03b3, b0P > (Duff, 2002). It has been shown that the BRL problem is called POMDP P = < SP, OP, TP, POP, POP > (Duff, 2002). The state space SP = S \u00b2 is the cross-product of the MDP states and the parameter space. A state (s, 2001) consists of a world state of the MDP and a hypothesized value of the MDP. The measures AP are identical to measures A in the MDP. Adopting the parameters does not change the transition function."}, {"heading": "3.2. Algorithm", "text": "MC-BRL consists of two phases, offline and online. Given a prior distribution b0 (\u03b8) and a sample size K, the offline phase of the algorithm can work in three steps.1. Example K hypotheses that are used before using the hypotheses... Example K hypotheses that are used before using the hypotheses... Example K hypotheses that are used before using the hypotheses... Example K hypotheses that are used before using the hypotheses.... Example K hypotheses that are used before using the hypotheses.. Example K hypotheses that are used before using the hypotheses... Example P hypotheses that are used before using the hypotheses."}, {"heading": "3.3. Generalization to Partially Observable Environments", "text": "Suppose we get a POMDP < S, A, O, T, Z, R, \u03b3 >, and we aim for an optimal policy when both the transition function T and the observation function Z. The unknown parameters can be called a pair, \u03b8 being defined as it was previously defined, whereas vice versa. MC-BRL can of course be adapted to solve this problem with two modifications of the offline phase. First, the hypotheses are taken from a common previous distribution b0 (zA, o) instead of b0 (zA, o). Second, the POMDP P is modified by making OP = O and ZP (s, k, o) = compatible."}, {"heading": "4. Theoretical Analysis", "text": "To analyze the quality of this approach, we deduce a probably roughly correct (PAC) policy, tied to the regret of the MC-BRL solution, compared to the optimal solution for P. We assume that a POMDP policy \u03c0 is presented as a political graph G. This is a trend-setting graph with designated nodes and edges. Each node of the P-BRL policy is provided with an action that has a node in G and is each labeled with a unique observation o-Op. The size of the political arrangement, which is called \"\u03c0,\" is the number of nodes in G. To execute the policy, the agent first selects a node in G according to the original belief. He then takes the action associated with the node, receives an observation, and transmits the next node by labeling it with that observation."}, {"heading": "5. Experiments", "text": "We are now experimenting with MC-BRL on both fully observable and partially observable amplification learning tasks. First, we are evaluating MC-BRL on two small synthetic domains that are widely used in the existing work on BRL (Sections 5.1 and 5.2). In this standard setup, we need to measure the performance of an algorithm against certain model parameter values, rather than the average performance in relation to a prior distribution of model parameters. Therefore, the limit in Theorem 1 is not applicable here. Next, we test MC-BRL on two more realistic domains (Sections 5.3 and 5.4), where we measure the average performance of MC-BRL and show that it performs well in this sense, as our theoretical result is guaranteed. All experiments are conducted on a 16-core Intel-Xeon server at 2.4GHz."}, {"heading": "5.1. Chain", "text": "Starting with the chain problem in (Dearden et al., 1998; Poupart et al., 2006), this problem consists of a chain of 5 states and 2 actions {a, b}. The actions cause the transitions between states and receive corresponding rewards as shown in Figure 1. They slip with the probability of 0.2 and cause the opposite effect. The optimal policy of this problem is to take measures again and again."}, {"heading": "5.2. Tiger", "text": "Next, we test MC-BRL for the tiger problem (Kaelbling et al., 1998) with partial observability. In this problem, the agent must decide whether he opens one of two doors or listens to the position of the tiger in each time step. Opening the wrong door leads to the fact that the agent is eaten by a tiger, with a penalty of \u2212 100, while opening the right door gives a reward of 10. Listening costs \u2212 1 and indicates the true position of the tiger with 15% error. We assume that the transition and the reward functions are given, but the observation error rates are unknown. We evaluate MC-BRL on the basis of 1000 simulations. Each simulation consists of 100 episodes. In each episode, the agent takes action and receives the observation sequentially. The episode ends when the agent opens a door and the position of the tiger is reset. We test MC-BRL with K = 10 and 100. Subsequently (Ross et al., 2007) we use Dirichlet (3) as the previous unknown distribution parameter."}, {"heading": "5.3. Iterated Prisoner\u2019s Dilemma", "text": "In this section we have the repeated version of \"The Iterated Prisoner's Dilemma\" (Axelrod, 1984), and show that MC-BRL can achieve excellent performance in this area. In this section the game is repeated and each player knows the history of his opponents. A key factor for a high reward is the ability to model the behavior of the opponent based on the story. It has been shown that any memoryless and single-level memory opponent can be modeled over and over again. < PS, PT, PR >, these are the probabilities that the opponent is able to model the behavior of the opponent based on the story."}, {"heading": "5.4. Intersection Navigation", "text": "This problem is motivated by an accident in 2007 DARPA Urban Challenge (Leonard et al., 2008). In this case, two autonomous vehicles, R and A, approached an uncontrolled intersection as shown in Figure 3. MC MC had the right of way and proceeded. However, possibly due to sensor failures or imperfect driving strategy, A did not lead to R and caused a near-accident. This situation is quite common and often occurs even with human drivers. Crossing the intersection safely and efficiently without knowing the driving strategy of A. We formulate the problem as an RL problem and cause a problem that the underlying model is a POMDP. The state consists of the positions and speeds of R and A. For simplicity, we discredit the environment into a unified grid. In each step, we can take three actions: acceleration, maintaining the speed and deceleration. It then receives an observation based on its own observations and both the state's observations."}, {"heading": "6. Conclusion", "text": "We have introduced MC-BRL, a simple and general approach to Bayesian amplification learning. We demonstrate that by evaluating a limited number of hypotheses from the model parameter space on a random basis, MC-BRL produces a discrete POMDP that approaches the underlying BRL problem well and with guaranteed performance. We provide experimental results that demonstrate a strong performance of the approach in practice. Furthermore, MC-BRL naturally treats both fully and partially observable worlds. An important problem for MC-BRL is the effective capture of the model parameter space. A naive method is to uniformly discredit the parameter space and treat the fixed grid points as samples. However, this method suffers from the \"curse of dimensionality\" and is difficult to scale as the number of parameters increases (Poupart et al., 2006). MC-BRL goes a step further and bases a series of hypotheses independently of a given distribution."}, {"heading": "Acknowledgments", "text": "Y. Wang and D. Hsu are supported in part by the 2010-T2-2-071 MoE AcRF Scholarship and the MDA GAMBIT Scholarship R-252000-398-490. K.S. Won is supported by an NUS President's Fellowship. W.S. Lee is partially supported by the Air Force Research Laboratory under contract number FA2386-12-1-4031. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official guidelines or recommendations of the Air Force Research Laboratory or the U.S. Government, neither express nor implied."}], "references": [{"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "In UAI,", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "The Evolution of Cooperation", "author": ["R. Axelrod"], "venue": "Basic Books,", "citeRegEx": "Axelrod,? \\Q1984\\E", "shortCiteRegEx": "Axelrod", "year": 1984}, {"title": "Using linear programming for Bayesian exploration in Markov Decision Processes", "author": ["P.S. Castro", "D. Precup"], "venue": "In IJCAI,", "citeRegEx": "Castro and Precup,? \\Q2007\\E", "shortCiteRegEx": "Castro and Precup", "year": 2007}, {"title": "Model-based Bayesian exploration", "author": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In UAI,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes", "author": ["M.O. Duff"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Duff,? \\Q2002\\E", "shortCiteRegEx": "Duff", "year": 2002}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Evolution of learning among pavlov strategies in a competitive environment with noise", "author": ["D. Kraines", "V. Kraines"], "venue": "Journal of Conflict Resolution,", "citeRegEx": "Kraines and Kraines,? \\Q1995\\E", "shortCiteRegEx": "Kraines and Kraines", "year": 1995}, {"title": "SARSOP: Efficient pointbased POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "In RSS,", "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "A perception driven autonomous urban vehicle", "author": ["J. Leonard", "J. How", "S. Teller"], "venue": "Journal of Field Robotics,", "citeRegEx": "Leonard et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Leonard et al\\.", "year": 2008}, {"title": "How to design a strategy to win an IPD tournament", "author": ["J. Li"], "venue": "In The Iterated Prisoners\u2019 Dilemma: 20 Years On,", "citeRegEx": "Li,? \\Q2007\\E", "shortCiteRegEx": "Li", "year": 2007}, {"title": "Leading best-response strategies in repeated games", "author": ["M.L. Littman", "P. Stone"], "venue": "In IJCAI Workshop on Economic Agents, Models, and Mechanisms,", "citeRegEx": "Littman and Stone,? \\Q2001\\E", "shortCiteRegEx": "Littman and Stone", "year": 2001}, {"title": "Human driver model and driver decision making for intersection driving", "author": ["Y. Liu", "U. Ozguner"], "venue": "IEEE Intelligent Vehicles Symposium,", "citeRegEx": "Liu and Ozguner,? \\Q2007\\E", "shortCiteRegEx": "Liu and Ozguner", "year": 2007}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["A. Ng", "M. Jordan"], "venue": "In UAI, pp", "citeRegEx": "Ng and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan", "year": 2000}, {"title": "A strategy of win-stay, lose-shift that outperforms tit-for-tat in the prisoner\u2019s dilemma", "author": ["M. Nowak", "K. Sigmund"], "venue": "game. Nature,", "citeRegEx": "Nowak and Sigmund,? \\Q1993\\E", "shortCiteRegEx": "Nowak and Sigmund", "year": 1993}, {"title": "Planning under uncertainty for robotic tasks with mixed observability", "author": ["S.C.W. Ong", "S.W. Png", "D. Hsu", "W.S. Lee"], "venue": null, "citeRegEx": "Ong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2010}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In IJCAI, pp", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Model-based Bayesian reinforcement learning in partially observable domains", "author": ["P. Poupart", "N. Vlassis"], "venue": "In ISAIM,", "citeRegEx": "Poupart and Vlassis,? \\Q2008\\E", "shortCiteRegEx": "Poupart and Vlassis", "year": 2008}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "In ICML, pp", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Model-based Bayesian reinforcement learning in large structured domains", "author": ["S. Ross", "J. Pineau"], "venue": "In UAI,", "citeRegEx": "Ross and Pineau,? \\Q2008\\E", "shortCiteRegEx": "Ross and Pineau", "year": 2008}, {"title": "On some winning strategies for the Iterated Prisoner\u2019s Dilemma or Mr", "author": ["W. Slany", "W. Kienreich"], "venue": "Nice Guy and the Cosa Nostra. In The Iterated Prisoners\u2019 Dilemma:", "citeRegEx": "Slany and Kienreich,? \\Q2007\\E", "shortCiteRegEx": "Slany and Kienreich", "year": 2007}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation", "author": ["T. Smith", "R.G. Simmons"], "venue": "In UAI, pp", "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "One approach to BRL is to cast it as a partially observable Markov decision process (POMDP) P (Duff, 2002).", "startOffset": 94, "endOffset": 106}, {"referenceID": 4, "context": "Since model parameters are continuous in general, P has a hybrid state space and requires the restrictive assumption of conjugate distributions to represent beliefs during the policy computation (Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 195, "endOffset": 273}, {"referenceID": 17, "context": "Since model parameters are continuous in general, P has a hybrid state space and requires the restrictive assumption of conjugate distributions to represent beliefs during the policy computation (Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 195, "endOffset": 273}, {"referenceID": 7, "context": ", (Kurniawati et al., 2008).", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": "One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al.", "startOffset": 50, "endOffset": 62}, {"referenceID": 21, "context": "One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al., 2005; Poupart et al., 2006; Ross et al., 2007; Castro & Precup, 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008).", "startOffset": 116, "endOffset": 245}, {"referenceID": 17, "context": "One common approach to BRL adopts the proposal in (Duff, 2002) and casts BRL as a POMDP P with a hybrid state space (Wang et al., 2005; Poupart et al., 2006; Ross et al., 2007; Castro & Precup, 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008).", "startOffset": 116, "endOffset": 245}, {"referenceID": 0, "context": "Sampling has been used extensively in BRL (Castro & Precup, 2007; Ross et al., 2007; Poupart & Vlassis, 2008; Ross & Pineau, 2008; Asmuth et al., 2009).", "startOffset": 42, "endOffset": 151}, {"referenceID": 4, "context": "It has been shown that the BRL problem can be formulated as a POMDP P = \u3008SP , AP , OP , TP , ZP , RP , \u03b3, bP\u3009 (Duff, 2002).", "startOffset": 110, "endOffset": 122}, {"referenceID": 3, "context": "In order to attain a closed-form representation, most existing work assumes a conjugate prior bP over the parameter \u03b8, such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 154, "endOffset": 254}, {"referenceID": 4, "context": "In order to attain a closed-form representation, most existing work assumes a conjugate prior bP over the parameter \u03b8, such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 154, "endOffset": 254}, {"referenceID": 17, "context": "In order to attain a closed-form representation, most existing work assumes a conjugate prior bP over the parameter \u03b8, such as the Dirichlet distribution (Dearden et al., 1999; Duff, 2002; Poupart et al., 2006; Ross et al., 2007; Poupart & Vlassis, 2008).", "startOffset": 154, "endOffset": 254}, {"referenceID": 4, "context": "Although several approximate algorithms based on function approximation and online planning have been proposed (Duff, 2002; Poupart et al., 2006; Ross et al., 2007), there is no satisfactory answer in general.", "startOffset": 111, "endOffset": 164}, {"referenceID": 17, "context": "Although several approximate algorithms based on function approximation and online planning have been proposed (Duff, 2002; Poupart et al., 2006; Ross et al., 2007), there is no satisfactory answer in general.", "startOffset": 111, "endOffset": 164}, {"referenceID": 15, "context": "The discrete POMDP P\u0302 can be readily solved with point-based approximation algorithms (Pineau et al., 2003; Smith & Simmons, 2005; Kurniawati et al., 2008).", "startOffset": 86, "endOffset": 155}, {"referenceID": 7, "context": "The discrete POMDP P\u0302 can be readily solved with point-based approximation algorithms (Pineau et al., 2003; Smith & Simmons, 2005; Kurniawati et al., 2008).", "startOffset": 86, "endOffset": 155}, {"referenceID": 14, "context": "It has been shown that MOMDPs admit a compact factored representation of the state space, which can be exploited to speed up POMDP planning (Ong et al., 2010).", "startOffset": 140, "endOffset": 158}, {"referenceID": 14, "context": "In this paper, we use SARSOP (Ong et al., 2010) to solve P\u0302 which readily takes advantage of the MOMDP representation.", "startOffset": 29, "endOffset": 47}, {"referenceID": 7, "context": "Algorithms such as HSVI (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008) output such bounds as a by-product of POMDP policy computation.", "startOffset": 59, "endOffset": 84}, {"referenceID": 17, "context": "We start with the Chain problem used in (Dearden et al., 1998; Poupart et al., 2006).", "startOffset": 40, "endOffset": 84}, {"referenceID": 17, "context": "(Poupart et al., 2006).", "startOffset": 0, "endOffset": 22}, {"referenceID": 17, "context": "For comparison, we also report an upper bound on the reward that could be achieved only if we had known the true model parameters, as well as the rewards of three alternatives: the Beetle algorithm (Poupart et al., 2006), the Exploit heuristic, which never explores but takes the optimal action with respect to the expected MDP under the current belief, and Q-learning with -greedy exploration and linear learning rate.", "startOffset": 198, "endOffset": 220}, {"referenceID": 5, "context": "We next test MC-BRL on the Tiger problem (Kaelbling et al., 1998) with partial observability.", "startOffset": 41, "endOffset": 65}, {"referenceID": 1, "context": "In this section, we studied its repeated version, the Iterated Prisoner\u2019s Dilemma (IPD) (Axelrod, 1984), and show that MC-BRL can achieve excellent performance on this problem.", "startOffset": 88, "endOffset": 103}, {"referenceID": 1, "context": "For reference, we also compare MC-BRL with two classic hand-crafted strategies, Tit-for-Tat (TFT) (Axelrod, 1984) and Pavlov (Nowak & Sigmund, 1993), and the two winning entries of the 2005 IPD tournament, Adaptive Pavlov (AP) (Li, 2007) and Omega Tit-for-Tat (OTFT) (Slany & Kienreich, 2007).", "startOffset": 98, "endOffset": 113}, {"referenceID": 9, "context": "For reference, we also compare MC-BRL with two classic hand-crafted strategies, Tit-for-Tat (TFT) (Axelrod, 1984) and Pavlov (Nowak & Sigmund, 1993), and the two winning entries of the 2005 IPD tournament, Adaptive Pavlov (AP) (Li, 2007) and Omega Tit-for-Tat (OTFT) (Slany & Kienreich, 2007).", "startOffset": 227, "endOffset": 237}, {"referenceID": 8, "context": "This problem is motivated by an accident in the 2007 DARPA Urban Challenge (Leonard et al., 2008).", "startOffset": 75, "endOffset": 97}, {"referenceID": 17, "context": "This method, however, suffers from the \u201ccurse of dimensionality\u201d and is difficult to scale up as the number of parameters increases (Poupart et al., 2006).", "startOffset": 132, "endOffset": 154}], "year": 2012, "abstractText": "Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space. The POMDP does not require conjugate distributions for belief representation, as earlier works do, and can be solved relatively easily with pointbased approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance.", "creator": "LaTeX with hyperref package"}}}