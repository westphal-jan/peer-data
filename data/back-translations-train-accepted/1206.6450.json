{"id": "1206.6450", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Conditional Sparse Coding and Grouped Multivariate Regression", "abstract": "We study the problem of multivariate regression where the data are naturally grouped, and a regression matrix is to be estimated for each group. We propose an approach in which a dictionary of low rank parameter matrices is estimated across groups, and a sparse linear combination of the dictionary elements is estimated to form a model within each group. We refer to the method as conditional sparse coding since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X. This approach captures the shared information across the groups while adapting to the structure within each group. It exploits the same intuition behind sparse coding that has been successfully developed in computer vision and computational neuroscience. We propose an algorithm for conditional sparse coding, analyze its theoretical properties in terms of predictive accuracy, and present the results of simulation and brain imaging experiments that compare the new technique to reduced rank regression.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (184kb)", "http://arxiv.org/abs/1206.6450v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["min xu", "john d lafferty"], "accepted": true, "id": "1206.6450"}, "pdf": {"name": "1206.6450.pdf", "metadata": {"source": "CRF", "title": "Conditional Sparse Coding and Grouped Multivariate Regression", "authors": ["Min Xu", "John Lafferty"], "emails": ["minx@cs.cmu.edu", "lafferty@galton.uchicago.edu"], "sections": [{"heading": "1. Introduction", "text": "The method was proposed by Olshausen & Field (1996) for the creation of a simple mathematical model of neural coding in the visual cortex. By using savings and a large amount of knowledge, it is possible to efficiently capture a large collection of characteristics common to a population of signals. Variants of economical coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Yang et al., 2009; Zhou et al., 2010; Bengio et al.) We have considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2009; Yang et al.)."}, {"heading": "2. Related Work", "text": "Mairal et al. (2010) have explored a different way of using dictionary learning for supervised tasks; in this approach, we first encode data X and then use encoding to perform classification or regression; our work is more closely related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and, in particular, the generalization of a model by Argyriou et al. (2006), which requires that all \u03b1 (g) have the same thriftiness pattern so that all groups use the same small subset of dictionary elements; by allowing different groups to use different subsets of the dictionary, our model is much more flexible, but at the expense of non-convex optimization. Kang et al. (2011) used mixed holistic programming to generalize the model of Argyriou et al. (2006) Although our formulation is more flexible and our optimization is easier."}, {"heading": "3. Problem Formulation", "text": "In this thesis, we focus on problems where the data are naturally grouped. Suppose we have G groups, indexed by g = 1,..., G. Let us leave the explanatory and response variables for the ith sample in group g. For each group, let B * (g) = arg minB (g) R (B (g))) be the oracle regression matrix in which we define R (B (g) = EX (g), Y (g), Y (g) - B (g) - B (g) - 2F. For simplicity, let us assume that the sample size n is the same for all groups, noting that it generally varies with g. Let us leave X (g) = (g) 1,., X (g) n) - B (g) - B (g) - and Y (g) - Z (g)."}, {"heading": "4. Conditional Sparse Coding", "text": "The basic idea behind this is that we are a collection of low-ranked matrices (D1,..., DK) too big to be too big. (...) The basic idea that underlies us is that we are a collection of low-ranked matrices (D1,..., DK) too big. (...) The basic idea that underlies us is that we are a collection of low-ranked matrices (D1,..., DK) too big. (..., DK) The basic idea that underlies us is that we are the entire objective function f (..., D) too high-ranked (..., D) too high-ranked (..., D) too high-ranked (...,...)"}, {"heading": "5. Theoretical Analysis", "text": "In order to get a more complete understanding of the CSC, we perform both a pessimistic analysis and an optimistic analysis. (In the pessimistic analysis, we do not assume that our model is correct, and we do not assume an underlying common structure between the groups. It is obvious that under the general pessimistic attitude, we cannot achieve greater statistical accuracy with CSC than with the alternative of estimating separate low matrices for each group. (Our pessimistic analysis provides a simple rule for determining how much worse CSC is than the alternative.In the optimistic analysis, we focus on a very specific setting in which we only have to adjust the coefficients of an already existing set of learned dictionary entries. We assume that the learned dictionary has captured a common structure that exists among the groups. We show that in this setting we can produce an accurate estimate with fewer samples than the alternative of estimating separate matrices."}, {"heading": "5.1. Pessimistic Analysis", "text": "The results of this section justify the limits of the excess risk R (Dlearn, \u03b1learn (g) \u03bb) - R (B). We stress that we do not assume that we have a lower target than the random initial dictionary. Before setting out our main theories, it is instructive to first consider the excess risk we would get if we used only the random initial dictionary coefficients without achieving additional dictionary coefficients. The assumption that the assumptions hold A1, A2, A3."}, {"heading": "5.2. Optimistic Analysis", "text": "For our optimistic analysis, we consider the specific environment in which the dictionary has already been learned and analyze the excessive risk that arises when we adjust the coefficients from data not used in the dictionary learning process.A4. The dictionary learned {Dlearn1,..., DlearnK} is independent of the data X (g) i for all groups g anditems i = 1,..., n.With the dictionary we learn the excessive risk R (Dlearn1,..., DlearnK} argmin {\u03b1 (g): The excessive risks are interpreted as the sparse coefficients that minimize the true risk. We can then learn the excessive risk R (Dlearn, \u03b1) (g) orakel \u2261 (g) argmin {\u03b1 (g)))) as a measure of the extent to which the oracle regression matrices B (g) divide structure, and the learned dictionary grasp this structure.5,3"}, {"heading": "5.3. Proof Sketches", "text": "The crux of our reasoning is the following uniform generalization error boundary. Lemma 5.1. With a probability of at least 1 \u2212 exp (\u2212 cp), for all matrices B (g), so all matrices B (g), B (g), A (g), B (g), B (g), B (g), CL2 (p + q), Log (Gn) n + Ru, where c, C) constants are based only on A1, and Ru is a term that does not depend on B (g). We prove that Lemma 5.1 by combining the technique of Greenshtein & Ritov (2004) with a concentration result from the random matrix theory, which states that we have independent subgaussian random vectors Z1, Zn, 1n (n), n), i = 1 ZiZ (n), Z (n), with probability of 5.1 \u2212 p."}, {"heading": "6. Experiments", "text": "The main purpose of our experiments is to compare conditional sparse coding against regression with reduced rank. Furthermore, the experiments show that the coefficients estimated by CSC are actually sparse and that the dictionary entries have a low rank."}, {"heading": "6.1. Simulation Data", "text": "We generate data using a linear model Y (g) = B (g) X (g) + B (g), using each sample X (g) i (0, Ip).We consider three different settings: 1. In the structured case, we construct each B (g) as a random, three-dimensional linear combination of a setWe measure the performance of the algorithms with respect to both estimation errors 1G (g) = 1. B (g) and prediction error R (B), which is calculated from a large test set of (X (g), Y (g)) pairs. We compare CSC with separate, reduced ranking regressions for each group using nuclear normalization."}, {"heading": "6.2. fMRI Data", "text": "The data set collected by Mitchell et al. (2008) includes the patterns of brain activity of 9 subjects, each presented with a single concrete English noun. We sample the original neuronal signal by taking only one measurement in each 4 x 3 x 4 voxel region of the brain. Specifically, we have X as a design matrix of neuronal signals with dimension (p = 434) \u00d7 (n = 60) and Y as a response matrix with dimension (q = 192) \u00d7 (n = 60) semantic characteristics of the 60 nouns shown to the subjects. We let each subject be a group, and therefore our goal is to show the semantic characteristics of the subject with dimension (q = 192) \u00d7 (n = 60) \u00d7 (n = 60) semantic characteristics shown to the subjects being shown to the subjects."}, {"heading": "Acknowledgements", "text": "Research supported in part by NSF funding IIS-1116730 and AFOSR contract FA9550-09-1-0373."}], "references": [{"title": "Multi-task feature learning", "author": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "NIPS 19,", "citeRegEx": "Argyriou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2006}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["Beck", "Amir", "Teboulle", "Marc"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "Beck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2009}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["Ben-David", "Shai", "Schuller", "Reba"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Ben.David et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2003}, {"title": "Group sparse coding", "author": ["Bengio", "Samy", "Pereira", "Fernando", "Singer", "Yoram", "Strelow", "Dennis"], "venue": "NIPS 22. MIT Press,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["Caruana", "Rich"], "venue": null, "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Elad and Aharon,? \\Q2006\\E", "shortCiteRegEx": "Elad and Aharon", "year": 2006}, {"title": "Regularized multi\u2013task learning", "author": ["Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "Proceedings of the Tenth ACM SIGKDD,", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "Pathwise coordinate optimization", "author": ["Friedman", "Jerome", "Hastie", "Trevor", "H\u00f6fling", "Holger", "Tibshirani", "Robert"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "On the local correctness of l1-minimization for dictionary", "author": ["Geng", "Quan", "Wang", "Huan", "Wright", "John"], "venue": null, "citeRegEx": "Geng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Geng et al\\.", "year": 2011}, {"title": "Persistency in high dimensional linear predictor-selection and the virtue of overparametrization", "author": ["E. Greenshtein", "Y. Ritov"], "venue": null, "citeRegEx": "Greenshtein and Ritov,? \\Q2004\\E", "shortCiteRegEx": "Greenshtein and Ritov", "year": 2004}, {"title": "Learning with whom to share in multitask feature learning", "author": ["Kang", "Zhuoliang", "Grauman", "Kristen", "Sha", "Fei"], "venue": null, "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Learning temporal causal graphs for relational time-series analysis", "author": ["Liu", "Yan", "Niculescu-Mizil", "Alexandru", "Lozano", "Aurelie", "Lu", "Yong"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "arXiv: 1009.5358,", "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Bounds for linear multi-task learning", "author": ["Maurer", "Andreas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maurer and Andreas.,? \\Q2006\\E", "shortCiteRegEx": "Maurer and Andreas.", "year": 2006}, {"title": "K-dimensional coding scheme in hilbert spaces", "author": ["Maurer", "Andreas", "Pontil", "Massimiliano"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Maurer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maurer et al\\.", "year": 2010}, {"title": "Predicting human brain activity associated with the meanings of nouns", "author": ["Mitchell", "Tom M", "Shinkareva", "Svetlana V", "Carlson", "Andrew", "Chang", "Kai-Min", "Malave", "Vicente L", "Mason", "Robert A", "Just", "Marcel Adam"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "author": ["S. Negahban", "M.J. Wainwright"], "venue": "Annals of Statistics,", "citeRegEx": "Negahban and Wainwright,? \\Q2011\\E", "shortCiteRegEx": "Negahban and Wainwright", "year": 2011}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Nature,", "citeRegEx": "Olshausen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1996}, {"title": "The sample complexity of dictionary learning", "author": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": null, "citeRegEx": "Vainsencher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vainsencher et al\\.", "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In CVPR,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Dimension reduction and coefficient estimation in multivariate linear regression", "author": ["Yuan", "Ming", "A. Ekici", "Lu", "Zhaosong", "R. Monteiro"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Yuan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2007}, {"title": "Image classification using super-vector coding of local image descriptors", "author": ["Zhou", "Xi", "Yu", "Kai", "Zhang", "Tong", "Huang", "Thomas S"], "venue": "In ECCV\u201910,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 20, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 22, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 3, "context": "Variants of sparse coding have enjoyed considerable success in computer vision (Elad & Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).", "startOffset": 79, "endOffset": 198}, {"referenceID": 21, "context": "constraints, and has be recently studied in the context of multivariate regression (Yuan et al., 2007; Negahban & Wainwright, 2011).", "startOffset": 83, "endOffset": 131}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements.", "startOffset": 141, "endOffset": 164}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al.", "startOffset": 141, "endOffset": 474}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al. (2006) although our formulation is still more flexible and our optimization simpler.", "startOffset": 141, "endOffset": 555}, {"referenceID": 0, "context": "Our work is more related to multi-task learning (Caruana, 1997; Evgeniou & Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all \u03b1 have the same sparsity pattern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al. (2006) although our formulation is still more flexible and our optimization simpler. The approach of Liu et al. (2010) could be adapted to our setting, although their notion of task-relatedness is very different from ours.", "startOffset": 141, "endOffset": 667}, {"referenceID": 19, "context": "Some work studies the generalization error of dictionary learning (Vainsencher et al., 2010; Maurer & Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al.", "startOffset": 66, "endOffset": 115}, {"referenceID": 8, "context": ", 2010; Maurer & Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al., 2011).", "startOffset": 109, "endOffset": 128}, {"referenceID": 8, "context": ", 2010; Maurer & Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al., 2011). Jeong & Kim (2009) consider sparse approximability and prove an information theoretic lower bound on sparse approximability of general p-dimensional vectors.", "startOffset": 110, "endOffset": 149}, {"referenceID": 7, "context": "1) A variety of algorithms are available to solve the lasso efficiently, notably iterative soft thresholding, a form of coordinate descent (Friedman et al., 2007).", "startOffset": 139, "endOffset": 162}, {"referenceID": 16, "context": "The dataset, gathered by Mitchell et al. (2008), comprises the brain activity patterns of 9 human subjects when presented with a single concrete English noun.", "startOffset": 25, "endOffset": 48}, {"referenceID": 16, "context": "Following Mitchell et al. (2008), we use hold-two-out cross-validation for evaluation.", "startOffset": 10, "endOffset": 33}], "year": 2012, "abstractText": "We study the problem of multivariate regression where the data are naturally grouped, and a regression matrix is to be estimated for each group. We propose an approach in which a dictionary of low rank parameter matrices is estimated across groups, and a sparse linear combination of the dictionary elements is estimated to form a model within each group. We refer to the method as conditional sparse coding since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X. This approach captures the shared information across the groups while adapting to the structure within each group. It exploits the same intuition behind sparse coding that has been successfully developed in computer vision and computational neuroscience. We propose an algorithm for conditional sparse coding, analyze its theoretical properties in terms of predictive accuracy, and present the results of simulation and brain imaging experiments that compare the new technique to reduced rank regression.", "creator": "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"}}}