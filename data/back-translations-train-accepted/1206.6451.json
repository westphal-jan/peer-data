{"id": "1206.6451", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "The Greedy Miser: Learning under Test-time Budgets", "abstract": "As machine learning algorithms enter applications in industrial settings, there is increased interest in controlling their cpu-time during testing. The cpu-time consists of the running time of the algorithm and the extraction time of the features. The latter can vary drastically when the feature set is diverse. In this paper, we propose an algorithm, the Greedy Miser, that incorporates the feature extraction cost during training to explicitly minimize the cpu-time during testing. The algorithm is a straightforward extension of stage-wise regression and is equally suitable for regression or multi-class classification. Compared to prior work, it is significantly more cost-effective and scales to larger data sets.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (956kb)", "http://arxiv.org/abs/1206.6451v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["zhixiang eddie xu", "kilian q weinberger", "olivier chapelle"], "accepted": true, "id": "1206.6451"}, "pdf": {"name": "1206.6451.pdf", "metadata": {"source": "META", "title": "The Greedy Miser: Learning under Test-time Budgets", "authors": ["Zhixiang (Eddie) Xu", "Kilian Q. Weinberger", "Olivier Chapelle"], "emails": ["XUZX@CSE.WUSTL.EDU", "KILIAN@WUSTL.EDU", "OLIVIER@CHAPELLE.CC"], "sections": [{"heading": "1. Introduction", "text": "The last decade has shown how the field of machine learning has established itself as a necessary component in multi-billion dollar industries ranging from web search engines (Zheng et al., 2008) to product recommendations (Fleck et al., 1996) to e-mail spam filters (Weinberger et al., 2009). Real industry is an interesting new problem in machine research, but runtime also needs to be budgeted for and costs taken into account during the testing phase. Imagine an algorithm running 10 million times a day."}, {"heading": "2. Related Work", "text": "Most of them have a cascade of weak classifications in front of them, learning a set of weak classifiers that amplifies the gradation and removing the data points during the test phase using proximity points. Although their algorithm requires almost no additional training costs, the improvement is usually limited. Lefakis & Fleuret (2010) and Dundar & Bi learn a softcascade that relies on the likelihood of exceeding all levels."}, {"heading": "3. Notation and Setup", "text": "Our training data consists of n input vectors {x1,..., xn} \u03b2 Rd with corresponding labels {y1,..., yn} Y, drawn from an unknown distributionD. Labels can be continuous (regression) or categorical (binary or multi-class classification). We assume that each feature has a recording cost c\u03b1 > 0 during its original call. Once a feature has been obtained, its subsequent retrieval is free (or set to a minor constant). In addition, we receive an arbitrary continuous loss function \"and aim to learn a linear prediction H\u03b2 (x) = \u03b2 > h (x) to minimize the loss function, min \u03b2 '(\u03b2), (1) within a test cost budget defined in the following paragraph. An example of\" is the square loss limit \"sq'sq (\u03b2) = 12n n, n, n, n\" n = \"1.\""}, {"heading": "4. Method", "text": "In this section, we formalize the optimization problem of the test time calculation = cost = 1 cost = 1 cost = 1 cost (1 cost). We follow the settings (1 cost) introduced in (Chen et al., 2012). We follow the settings (1 cost) introduced in (Chen et al., 2012). There are two factors contributing to this cost: the cost of the functional evaluation of all trees used in these trees, and the cost of the trait extraction of all traits used in these trees. Let e > 0 be the cost of evaluating a tree if all traits have been extracted previously. With this indication, both costs can be expressed in a single function asc. (\u03b2) The cost of all traits used in these trees. Let e > 0 is the cost of evaluating a tree that has been extracted previously. (3) Where the costs are expressed in a single function asc (\u03b2)."}, {"heading": "5. Algorithm Derivation", "text": "In this section, we link (4) to our miser's algorithm by showing that miser's algorithm almost solves a relaxed version of the optimization problem."}, {"heading": "5.1. Relaxation", "text": "The optimization as performed in eq. (4) is not continuous due to the l0-q standard in terms of cost - and difficult to optimize. We start with the introduction of minor facilitations for both terms in (3) to make it better. Due to the extremely high dimensionality (dictated by the number of all possible regression trees that can be represented within the accuracy of the computer) and the comparatively minuscule number of iterations (\u2264 5000), it is reasonable to assume that a dimension will never be doubled. In other words, the weight vector \u03b2 is extremely sparse and (up to a re-scaling of 1\u043c) binary: 1 throu\u00df \u00b2 T.Tree valuation cost."}, {"heading": "5.2. Optimization", "text": "In this section we describe miser, our adaptation of the step-by-step regression (Friedman, \u03b2) (\u03b2), a (local) solution to the optimization problem in (9).Solution. we follow the approach of Rosset et al. (2004) and find a solution for (9) even tree evaluation budgets ranging from B \u00b2 t = 0 to B \u00b2 t = Bt. along the path we iteratively increment, we can solve the intermediate optimization problem repeatedly by warm start (9) with the previous solution and allow the weight vector around which the weight vector of B \u00b2 t can change. (\u03b2) We cannot solve the weight vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector solver (9)."}, {"heading": "6. Results", "text": "We conduct experiments on two benchmark tasks from very different domains, each replicating relevant data (the cost for each query is approximately 0 points), each costing approximately $10 (Chapelle & Chang, 2011) and the Scene Recovery Data Set by Lazebnik et al. (2006).Yahoo Data Set contains documents / query pairs with label values of {0, 1, 2, 3, 4}, where 0 means the document is irrelevant to the query, and 4 means highly relevant. Overall, it has 473134, 71083, 165660, training, validation, and test pairs. As this is a regression task, we use the squared loss function as our loss function. \"Although the data is representative of a Web search ranking training dataset, there are many irrelevant data points in a real test environment. Usually, only a few documents are relevant to each query, and the other hundreds of thousands are completely irrelevant."}, {"heading": "7. Conclusion", "text": "We believe that understanding and controlling this trade-off will become a fundamental part of machine learning research in the near future. This paper is a natural extension of gradual regression (Friedman, 2001), which also includes the cost of features during training; the resulting algorithm, the Greedy Miser, is easy to implement, naturally scales to large data sets, and outperforms most cost-effective classifiers to date; and future work will include combining our approach with early exits (Cambazoglu et al., 2010) or cascade-based learning methods such as (Chen et al., 2012)."}, {"heading": "8. Acknowledgements", "text": "KQW and ZX would like to thank NIH for their support with the grant U01 1U01NS073457-01."}], "references": [{"title": "Classification and regression trees", "author": ["L. Breiman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Breiman,? \\Q1984\\E", "shortCiteRegEx": "Breiman", "year": 1984}, {"title": "Exploiting site-level information to improve web search", "author": ["A. Broder", "E. Gabrilovich", "V. Josifovski", "G. Mavromatis", "D. Metzler", "J. Wang"], "venue": "In CIKM", "citeRegEx": "Broder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Broder et al\\.", "year": 2010}, {"title": "Early exit optimizations for additive machine learned ranking systems", "author": ["B.B. Cambazoglu", "H. Zaragoza", "O. Chapelle", "J. Chen", "C. Liao", "Z. Zheng", "J. Degenhardt"], "venue": "In ICDM,", "citeRegEx": "Cambazoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cambazoglu et al\\.", "year": 2010}, {"title": "Yahoo! learning to rank challenge overview", "author": ["O. Chapelle", "Y. Chang"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Chapelle and Chang,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Chang", "year": 2011}, {"title": "Boosted multi-task learning", "author": ["O. Chapelle", "P. Shivaswamy", "S. Vadrevu", "K. Weinberger", "Y. Zhang", "B. Tseng"], "venue": "Machine Learning, pp", "citeRegEx": "Chapelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["Chen", "Minmin", "Xu", "Zhixiang", "Weinberger", "Kilian Q", "Chapelle", "Olivier"], "venue": "In AISTATS", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Joint optimization of cascaded classifiers for computer aided detection", "author": ["M.M. Dundar", "J. Bi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dundar and Bi,? \\Q2007\\E", "shortCiteRegEx": "Dundar and Bi", "year": 2007}, {"title": "Finding naked people", "author": ["M. Fleck", "D. Forsyth", "C. Bregler"], "venue": "Computer Vision\u2014ECCV\u201996, pp", "citeRegEx": "Fleck et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Fleck et al\\.", "year": 1996}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, pp. 1189\u20131232,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Active classification based on value of classifier", "author": ["Gao", "Tianshi", "Koller", "Daphne"], "venue": "K.Q. (eds.),", "citeRegEx": "Gao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["Grubb", "Alex", "Bagnell", "Drew"], "venue": "In AISTATS", "citeRegEx": "Grubb et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grubb et al\\.", "year": 2012}, {"title": "The elements of statistical learning", "author": ["Hastie", "Trevor", "Tibshirani", "Robert", "Friedman", "JH (Jerome H"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen", "year": 2002}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Lazebnik et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lazebnik et al\\.", "year": 2006}, {"title": "Object bank: A highlevel image representation for scene classification and semantic feature sparsification", "author": ["L.J. Li", "H. Su", "E.P. Xing", "L. Fei-Fei"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Using classifier cascades for scalable e-mail classification", "author": ["J. Pujara", "H. Daum\u00e9 III", "L. Getoor"], "venue": "CEAS", "citeRegEx": "Pujara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pujara et al\\.", "year": 2011}, {"title": "Designing efficient cascaded classifiers: tradeoff between accuracy and cost", "author": ["V.C. Raykar", "B. Krishnapuram", "S. Yu"], "venue": "In SIGKDD", "citeRegEx": "Raykar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raykar et al\\.", "year": 2010}, {"title": "Boosting as a regularized path to a maximum margin classifier", "author": ["S. Rosset", "J. Zhu", "T. Hastie"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Rosset et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosset et al\\.", "year": 2004}, {"title": "Boosting Classifier Cascades", "author": ["Saberian", "Mohammad", "Vasconcelos", "Nuno"], "venue": "NIPS 23,", "citeRegEx": "Saberian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saberian et al\\.", "year": 2010}, {"title": "A brief introduction to boosting", "author": ["R.E. Schapire"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Schapire,? \\Q1999\\E", "shortCiteRegEx": "Schapire", "year": 1999}, {"title": "The kernel trick for distances", "author": ["B. Sch\u00f6lkopf"], "venue": "NIPS, pp", "citeRegEx": "Sch\u00f6lkopf,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf", "year": 2001}, {"title": "Robust real-time object detection", "author": ["P. Viola", "M. Jones"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola and Jones,? \\Q2002\\E", "shortCiteRegEx": "Viola and Jones", "year": 2002}, {"title": "Feature hashing for large scale multitask learning", "author": ["K.Q. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "ICMl", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": ", 2008), over product recommendation (Fleck et al., 1996), to email and web spam filtering (Weinberger et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 22, "context": ", 1996), to email and web spam filtering (Weinberger et al., 2009).", "startOffset": 41, "endOffset": 66}, {"referenceID": 8, "context": "Subsequently, we derive an update rule that shows the resulting loss lends itself naturally to greedy optimization with stage-wise regression (Friedman, 2001).", "startOffset": 142, "endOffset": 158}, {"referenceID": 15, "context": "Different from previous approaches (Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), our algorithm does not build cascades of classifiers.", "startOffset": 35, "endOffset": 130}, {"referenceID": 5, "context": "Different from previous approaches (Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), our algorithm does not build cascades of classifiers.", "startOffset": 35, "endOffset": 130}, {"referenceID": 19, "context": "Most prominently, Viola & Jones (2002) greedily train a cascade of weak classifiers with Adaboost (Schapire, 1999) for visual object recognition.", "startOffset": 98, "endOffset": 114}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis & Fleuret (2010) and Dundar & Bi (2007) learn a softcascade, which re-weights inputs based on their probability of passing all stages.", "startOffset": 0, "endOffset": 351}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis & Fleuret (2010) and Dundar & Bi (2007) learn a softcascade, which re-weights inputs based on their probability of passing all stages.", "startOffset": 0, "endOffset": 374}, {"referenceID": 2, "context": "Cambazoglu et al. (2010) propose a cascade framework explicitly for web-search ranking. They learn a set of additive weak classifiers using gradient boosting, and remove data points during test-time using proximity scores. Although their algorithm requires almost no extra training cost, the improvement is typically limited. Lefakis & Fleuret (2010) and Dundar & Bi (2007) learn a softcascade, which re-weights inputs based on their probability of passing all stages. Different from our method, they employ a global probabilistic model, do not explicitly incorporate feature extraction costs and are restricted to binary classification problems. Saberian & Vasconcelos (2010) also learn classifier cascades.", "startOffset": 0, "endOffset": 677}, {"referenceID": 14, "context": "Raykar et al. (2010) learn classifier cascades, but they group features by their costs and restrict classifiers at each stage to only use a small subset.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Pujara et al. (2011) suggest the use of sampling to derive a cascade of classifiers with increasing cost for email spam filtering.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "Most recently, Chen et al. (2012) introduce Cronus, which explicitly considers the feature extraction cost during training and constructs a cascade to encourage removal of unpromising data points early-on.", "startOffset": 15, "endOffset": 34}, {"referenceID": 5, "context": "Most recently, Chen et al. (2012) introduce Cronus, which explicitly considers the feature extraction cost during training and constructs a cascade to encourage removal of unpromising data points early-on. At each stage, they optimize the coefficients of the weak classifiers to minimize the classification error and trees/features extraction costs. We pursue a very different (orthogonal) approach and do not optimize the cascade stages globally. Instead, we strictly incorporate the feature cost into the weak learners. Moreover, as our algorithm is a variant of stage-wise regression, it can operate naturally in both regression and multi-class classification scenarios. (Simultaneous with this publication, Grubb & Bagnell (2012) also proposed a complementary approach to incorporate feature cost into gradient boosting.", "startOffset": 15, "endOffset": 734}, {"referenceID": 11, "context": "but other losses, for example the multi-class logloss (Hastie et al., 2009), are equally suitable.", "startOffset": 54, "endOffset": 75}, {"referenceID": 20, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al.", "startOffset": 78, "endOffset": 95}, {"referenceID": 8, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010).", "startOffset": 151, "endOffset": 211}, {"referenceID": 17, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010).", "startOffset": 151, "endOffset": 211}, {"referenceID": 4, "context": "Typically, the mapping h can be performed implicitly through the kernel-trick (Sch\u00f6lkopf, 2001) or explicitly through, for example, the boosting-trick (Friedman, 2001; Rosset et al., 2004; Chapelle et al., 2010).", "startOffset": 151, "endOffset": 211}, {"referenceID": 0, "context": "In this paper we use the latter approach with limited-depth regression trees (Breiman, 1984).", "startOffset": 77, "endOffset": 92}, {"referenceID": 5, "context": "We follow the setup introduced in (Chen et al., 2012), formalizing the test-time computational cost of evaluating the classifier H for a given weight-vector \u03b2.", "startOffset": 34, "endOffset": 53}, {"referenceID": 0, "context": "1 During iteration t, the greedy Classification and Regression Tree (CART) algorithm (Breiman, 1984) is used to generate a new tree ht, which is added to the classifier H\u03b2.", "startOffset": 85, "endOffset": 100}, {"referenceID": 11, "context": "Typical choices for g are the squared loss (2) or the label entropy (Hastie et al., 2009).", "startOffset": 68, "endOffset": 89}, {"referenceID": 8, "context": "In this section we describe how miser, our adaptation of stage-wise regression (Friedman, 2001), finds a (local) solution to the optimization problem in (9).", "startOffset": 79, "endOffset": 95}, {"referenceID": 17, "context": "We follow the approach from Rosset et al. (2004) and find a solution path for (9) for evenly spaced tree-evaluation budgets, ranging from B\u2032 t = 0 to B \u2032 t =Bt.", "startOffset": 28, "endOffset": 49}, {"referenceID": 13, "context": "We conduct experiments on two benchmark tasks from very different domains: the Yahoo Learning to Rank Challenge data set (Chapelle & Chang, 2011) and the scene recognition data set from Lazebnik et al. (2006).", "startOffset": 186, "endOffset": 209}, {"referenceID": 5, "context": "Therefore, we follow the convention of Chen et al. (2012) and replicate each irrelevant data point (label value is 0) 10 times.", "startOffset": 39, "endOffset": 58}, {"referenceID": 1, "context": "The cheapest features (cost value is 1) are those that can be acquired by looking up a table (such as the statistics of a given document), whereas the most expensive ones (such as BM25FSD described in Broder et al. (2010)), typically involve term proximity scoring.", "startOffset": 201, "endOffset": 222}, {"referenceID": 8, "context": "The baseline, stage-wise regression (Friedman, 2001), is equivalent to miser with \u03bb = 0 and is essentially building trees without any cost consideration.", "startOffset": 36, "endOffset": 52}, {"referenceID": 2, "context": "In addition to stage-wise regression, we also compare against Stage-wise regression feature subsets, Early Exit (Cambazoglu et al., 2010) and Cronus (Chen et al.", "startOffset": 112, "endOffset": 137}, {"referenceID": 5, "context": ", 2010) and Cronus (Chen et al., 2012).", "startOffset": 19, "endOffset": 38}, {"referenceID": 2, "context": "In addition to stage-wise regression, we also compare against Stage-wise regression feature subsets, Early Exit (Cambazoglu et al., 2010) and Cronus (Chen et al., 2012). Stage-wise regression feature subsets is a natural extension to stage-wise regression. We group all features according to the feature cost, and gradually use more expensive feature groups. The curve is generated by only using features whose cost\u2264 1, 20, 100, 200. Early Exit, proposed by Cambazoglu et al. (2010), trains trees identical to stage-wise regression\u2014however, it reduces the average test-time cost by removing unpromising documents early-on during test-time.", "startOffset": 113, "endOffset": 483}, {"referenceID": 8, "context": "Stage\u2212wise regression (Friedman, 2001) Stage\u2212wise regression feature subsets Early exit (Cambazoglu et.", "startOffset": 22, "endOffset": 38}, {"referenceID": 5, "context": "Since Cronus does not scale to the full data set, we use the subset of the Yahoo data from Chen et al. (2012) of 141397, 146769, 184968, training, validation and testing points respectively.", "startOffset": 91, "endOffset": 110}, {"referenceID": 16, "context": "This highlights a great advantage of miser over some other cascade algorithms (Raykar et al., 2010), which learn cascades with pre-assigned feature costs and cannot extract good but expensive features until the very end.", "startOffset": 78, "endOffset": 99}, {"referenceID": 13, "context": "The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain.", "startOffset": 22, "endOffset": 45}, {"referenceID": 13, "context": "The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain. It contains 4485 images from 15 scene classes and the task is to classify images according to scene. Figure 4 shows one example image for each scene category. We follow the procedure used by Lazebnik et al. (2006); Li et al.", "startOffset": 23, "endOffset": 298}, {"referenceID": 13, "context": "The Scene-15 data set (Lazebnik et al., 2006) is from a very different data domain. It contains 4485 images from 15 scene classes and the task is to classify images according to scene. Figure 4 shows one example image for each scene category. We follow the procedure used by Lazebnik et al. (2006); Li et al. (2010), randomly sampling 100 images from each class, resulting in 1500 training images.", "startOffset": 23, "endOffset": 316}, {"referenceID": 14, "context": "We use a diverse set of visual descriptors varying in computation time and accuracy: GIST, spatial HOG, Local Binary Pattern, self-similarity, texton histogram, geometric texton, geometric texton, geometric color, and Object Bank (Li et al., 2010).", "startOffset": 230, "endOffset": 247}, {"referenceID": 11, "context": ") As loss function `, we use the multi-class log-loss (Hastie et al., 2009) and maintain 15 tree-ensemble classifiers H, .", "startOffset": 54, "endOffset": 75}, {"referenceID": 8, "context": "As baseline we use stage-wise regression (Friedman, 2001) and an SVM with the averaged kernel of all descriptors.", "startOffset": 41, "endOffset": 57}, {"referenceID": 8, "context": "This paper introduces a natural extension to stage-wise regression (Friedman, 2001), which incorporates feature cost during training.", "startOffset": 67, "endOffset": 83}, {"referenceID": 2, "context": "Future work includes combining our approach with Early Exits (Cambazoglu et al., 2010) or cascade based learning methods such as (Chen et al.", "startOffset": 61, "endOffset": 86}, {"referenceID": 5, "context": ", 2010) or cascade based learning methods such as (Chen et al., 2012).", "startOffset": 50, "endOffset": 69}], "year": 2012, "abstractText": "As machine learning algorithms enter applications in industrial settings, there is increased interest in controlling their cpu-time during testing. The cpu-time consists of the running time of the algorithm and the extraction time of the features. The latter can vary drastically when the feature set is diverse. In this paper, we propose an algorithm, the Greedy Miser, that incorporates the feature extraction cost during training to explicitly minimize the cpu-time during testing. The algorithm is a straightforward extension of stagewise regression and is equally suitable for regression or multi-class classification. Compared to prior work, it is significantly more cost-effective and scales to larger data sets.", "creator": "LaTeX with hyperref package"}}}