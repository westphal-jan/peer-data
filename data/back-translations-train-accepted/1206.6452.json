{"id": "1206.6452", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Smoothness and Structure Learning by Proxy", "abstract": "As data sets grow in size, the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account. For instance, Bayesian networks, the model chosen in this paper, have a super-exponentially large search space for a fixed number of variables. One possible method to alleviate this problem is to use a proxy, such as a Gaussian Process regressor, in place of the true scoring function, training it on a selection of sampled networks. We prove here that the use of such a proxy is well-founded, as we can bound the smoothness of a commonly-used scoring function for Bayesian network structure learning. We show here that, compared to an identical search strategy using the network?s exact scores, our proxy-based search is able to get equivalent or better scores on a number of data sets in a fraction of the time.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (351kb)", "http://arxiv.org/abs/1206.6452v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["benjamin yackley", "terran lane"], "accepted": true, "id": "1206.6452"}, "pdf": {"name": "1206.6452.pdf", "metadata": {"source": "META", "title": "Smoothness and Structure Learning by Proxy", "authors": ["Benjamin Yackley"], "emails": ["benj@cs.unm.edu", "terran@cs.unm.edu"], "sections": [{"heading": "1. Introduction", "text": "Probabilistic graphical models such as the Bayesian Networks (Koller & Friedman, 2009), which explain patterns in data by interdependence relationships between variables, are a useful tool because of the visibility and simplicity of the interpretation of the models, and because of the ability to estimate distributions based on known values. However, generating these models from observed data encounters problems in many ways. If the dataset has too many variables, the number of possible models grows exponentially, and if there is too much data. Although this growth is only linear in the number of data points, modern datasets run into gigabytes or larger. What is needed is a way to separate the size of the dataset from the search process, and that is the purpose of the proxy. By training a function to approximate exact values of the search function, we can use the exact results of the search engine results over and over and over again."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Bayesian networks", "text": "A Bayesian network (Heckerman et al., 1995) is a statistical model used to represent probable relationships between a set of variables as a directed acyclic graph, with the distribution of a single variable defined by the values of the parents of that variable in the graph. Bayesian networks are often used to derive distributions of unobserved or querying variables to established known values for others (for example, spam classification (Sebastiani & Ramoni, 2001) or disease diagnosis (Burge et al., 2009); the process of learning a Bayesian network in the face of a set of observed data is difficult and, indeed, NP-complete when it comes to finding an exact optimum (Chickering, 1996; Chickering et al., 1995); most techniques are still limited in practice in terms of the number of variables they can handle at once. A key component of many of these algorithms is a data scan, which is a data score."}, {"heading": "2.2. The BDe score", "text": "There are many Bayesian network scoring functions that could be used as a basis for a search, but the BDe score (Heckerman et al., 1995) has several desired properties. First, it is decomposable, which means that it can be expressed as a function of independent components, one for each node in the graph. Second, it is a Bayesian formulation that allows us to enforce an earlier belief in graph structures independently of the data itself. Finally, the structure of the BDe score is simple and only requires data queries (which can be facilitated with the help of an ADTree (Anderson & Moore, 1998), as described below) and the Log Gamma function, which can easily approximate itself numerically. Finally, the form of the BDe score is: sc (G | D) = n score is a previous effect of the data (which can be simplified with an ADTree (Anderson & Moore, 1998)) and the Log Gamma function, which can approximate itself."}, {"heading": "2.3. Gaussian process regression", "text": "In previous work (Yackley et al., 2008) we have shown that a spline-based regression model can be used to estimate the BDe score of a network. However, this particular model proved to be unsuitable for searching; while the values it returned are both mathematically simpler than the gradients. We have used the shape of the Gau scesor (Rasmussen, 2004) as a simple criterion."}, {"heading": "3. Motivation", "text": "In order to quickly learn structures, we want to create a proxy for the exact score function, and this proxy must have two key characteristics - it must be quick to evaluate, and it must be a good approximation of the true function. However, using a Gaussian process regressor brings us the first; once trained, its calculation is a simple matrix1 See (Rasmussen, 2004), Equation 5.9products. to obtain the second, we need to know that the true function we are approximating is smooth enough to model a Gaussian process. This, in turn, requires us to define some topology about the set of directed graphs over which we can say that the function is smooth. The topology we call the metagraph here (Yackley et al., 2008) is defined as a graph of a relationship across a series of combination objects that delineate edges. In this case, the objects themselves are directed graphs, and the relationship between them is exact."}, {"heading": "4. Analysis of smoothness of BDe score", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Notation", "text": "Let the dataset D-Nm \u00b7 n denote a data matrix of discrete values consisting of m i.i.d. observations of n variables. Name a Bayesian network using these variables as having the diagram G and the parameters GS, where G = < X, E > and X, the set of variables {x1, x2, xn}. A score function sc (G | D) generates diagrams of real numbers, which are provided with a fixed dataset, the convention being that a higher value represents a diagram that provides a better explanation of the data. Each variable xi has a corresponding finite set of possible values Vi and a possibly empty set of parents in the diagram Pa (xi). The set of superior configurations Ci is represented by the cartesian product Ci = Coordinate Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility"}, {"heading": "4.2. Basic Definitions", "text": "Consider the standard definition of the BDe score as given in Equation 1. In practice, we are more concerned with its logarithm: log sc (G | D) = \u2211 i \u2211 j-Ci (log-p) \u2212 log-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p) (4) We assume here that the form of the previous one is such that \u03bbijk is the same for all k with a fixed i and j and that this value is inversely proportional to the cardinality of Ci. In other words, we assume that the form of the previous one is such that all nodes are binary, then we simply have a sum ij = 2-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "4.3. Form of the bound on sc\u2206 \u2212 sc\u2032\u2206", "text": "From what has been said above: sc (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=), c (=, c (=), c (=), c (=, c (=), c (=), c (=), c (=, c (=), c (=), c (=, c (=), c (=, c (=), c (=, c (=, c (=), c (=), c (=, c (=), c (=), c (=, c (=, c (=), c (=), c (=, c (=, c (=), c (=, c (=, c (=, c (=), c (=), c (=, c (=, c, c (=, c, c, c, c, c, c (=, c, c, c, c, c, c (=, c, c, c (=, c, c, c, c, c, c, c, c, c, c"}, {"heading": "4.4. The function \u03b3(a, b)", "text": "Let us define the function \u03b3 (a, b) as a follow-up function: \u03b3 (a, b) = logbook B (a, b) \u2212 logbook (a) \u2212 logbook (b) (6) 2This function is related to the standard beta function; \u03b3 (a, b) = logbook B (a, b) Using Stirling's approximation for the logbook gamma function (Abramowitz & Stegun, 1964) (lnx! = x lnx + x \u2212 logbook (x))) we get a result that will be important later: \u03b3 (a, a) = 2a logbook (2a) \u2212 2a (logbook 2a) \u2212 2 (logbook a \u2212 a (logbook a)) = (2 logbook 2) + (logbook a) (7) Now we can use the equation for sc (2a) \u2212 2a (logbook 2a) \u2212 2 (logbook) \u2212 Nja = 00 logbook (1), Nja (1), Nja (1), Nja (1), Nja (1), Nja (1), Nja (1), Nja (1), Nja (1)."}, {"heading": "4.5. Getting to the extrema", "text": "We search upper and lower boundaries on sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-sc-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c"}, {"heading": "4.6. Implications", "text": "Predictably, the worst-case scenario is to add an edge that does not provide any information at all. Meanwhile, if the common distribution between xi and its new parent is uniform, the model achieves nothing by putting the edge there, while the score (as it should) punishes the addition. Meanwhile, the best case is that the new edge xi associates with a parent that in all cases perfectly matches its values (or at least a permutation thereof), while the marginals of the common distribution are completely uniform and uninformative, which fit our intuitions about how edges should be interpreted in a Bayesian network. Also, because the worst changes in the score are merely logarithmic in the size of the dataset, the search landscape is sufficiently smooth that a Gaussian process regressor is an appropriate choice to represent them. Gaussian process regressor is a good choice for another reason - the fact that it is based on a core function, meaning that we need its complexity to fit in the size of the niche, rather than on the size of the original niche."}, {"heading": "4.7. Other score functions", "text": "It is an open question to which we want to address in the future whether the same kind of smoothing limit can be demonstrated for other Bayesian network score functions. For example, the BIC score (Schwarz, 1978) is defined as follows, in the form of a log probability value and a penalty termination. scBIC (G | D) = n \u2211 i = 1 robbery i = 1 robbery i = 1 robbery i = Ci robbery i nijk log (Nijk Nij) \u2212 12 log (m) | B | (15) | = robbery i = 1 (# Vi \u2212 1) # Ci is the number of degrees of freedom beyond the specified parameter set. In this form, adding an edge to a network as before splits the group of parent configurations by adding another term to the product that defines Ci. However, it will also change the value of the punitive word."}, {"heading": "5. Proxy-Accelerated Search Results", "text": "The results of these three datasets show that even with ADTree-based acceleration, we are able to find comparable values in a much shorter time by storing the structure needed to achieve a much faster speed in the type of Nijk counting needed to calculate a BDe score. Results of these three datasets show that even with ADTree-based acceleration, we are able to find comparable values in a much shorter time by using the proxy. Proxy-based search was performed five times with randomly selected training values; the results of these three datasets are the averages and standard deviations. The algorithm was a standard search."}, {"heading": "5.1. Discussion", "text": "The effects of the proxy search are clear; in all but one case, the networks found by the proxy-based search were either comparable or significantly better than those found by the exact scoring version, and always in shorter time. At present, we do not know what property of the Census Income dataset led to it performing so poorly. However, in any other case, the proxy was able to find a network with a significantly improved score, and this is most dramatic in the case of the Musk dataset. With a relatively small number of samples across the immense space of networks at 168 nodes, the proxy was nevertheless able to find a network with a significantly improved score. The reason for this - and the reason why smoothness is so important - can be seen in Figure 5.1. These lines are the search paths, with search steps on the x-axis and scores on the y-axis, with the thick line of the exact search being taken by the blue line during the exact five scans."}, {"heading": "6. Future Work", "text": "We are currently working on extending the proxy to other score-based search strategies, such as simulated annealing (Kirkpatrick et al., 1983), as well as other combinatorial objects, such as general 0-1 matrices and permutations, and the success of these strategies seems to be based on finding a suitable form for a core function on these objects, thereby defining the topology of the space traversed by both the search method and the approximator. Another direction we would like to expand on is the implementation of the training phase on a massively parallel system, which would significantly reduce the time required to build the proxy, and would also require the implementation of a way to combine the training results; here, a block matrix inversion technique will be useful, as well as the possibility of adding further training data in the midst of an ongoing search, allowing the space to be studied and refined by an apparent local maximum."}, {"heading": "7. Conclusion", "text": "The larger the dataset, the longer it takes, and the larger the search space, the greater the likelihood that a search will hit a local maximum and not the desired global one. A proxy function will alleviate both of these problems; in particular, we have shown that the BDe score, which is viewed via a search space of monochrome additions and deletions, is smooth enough to make a proxy-based search feasible, and the results bear this out. This process, in which a proxy function is constructed from a series of random samples and then used to perform a search, can easily be applied to any search algorithm that depends on the computation of a set of results, from a simple greedy search to more complex ones such as Markov Chain Monte Carlo. These new accelerated forms of algorithms will allow researchers in fields as diverse as astronomy (Kent, 1994), biology (Roy et al., 2007) and linguistics (Markov Chain Monte Carlo) to generate larger amounts of data than were available in 2009, when we were generating large volumes of data in 2009 and 2008."}, {"heading": "8. Acknowledgements", "text": "The authors would like to thank Blake Anderson and Eduardo Corona for their ideas and support, as well as the Machine Learning Reading Group at the University of New Mexico, which was supported by the National Science Foundation with a grant IIS-0705681 and the Office of Naval Research with a grant N000141110139."}], "references": [{"title": "Handbook of mathematical functions with formulas, graphs, and mathematical tables", "author": ["M. Abramowitz", "I.A. Stegun"], "venue": "Dover publications,", "citeRegEx": "Abramowitz and Stegun,? \\Q1964\\E", "shortCiteRegEx": "Abramowitz and Stegun", "year": 1964}, {"title": "ADtrees for fast counting and for fast learning of association rules", "author": ["B. Anderson", "A. Moore"], "venue": "In Knowledge Discovery from Databases Conference,", "citeRegEx": "Anderson and Moore,? \\Q1998\\E", "shortCiteRegEx": "Anderson and Moore", "year": 1998}, {"title": "Discrete dynamic bayesian network analysis of fmri data", "author": ["J. Burge", "T. Lane", "H. Link", "S. Qiu", "V.P. Clark"], "venue": "Human Brain Mapping,", "citeRegEx": "Burge et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Burge et al\\.", "year": 2009}, {"title": "Learning bayesian networks is npcomplete", "author": ["D.M. Chickering"], "venue": "Learning from data: Artificial intelligence and statistics,", "citeRegEx": "Chickering,? \\Q1996\\E", "shortCiteRegEx": "Chickering", "year": 1996}, {"title": "Learning bayesian networks: Search methods and experimental results", "author": ["D.M. Chickering", "D. Geiger", "D. Heckerman"], "venue": "In Proceedings of Fifth Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Chickering et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Chickering et al\\.", "year": 1995}, {"title": "The 385+ million word corpus of contemporary american english (19902008+): Design, architecture, and linguistic insights", "author": ["M. Davies"], "venue": "International Journal of Corpus Linguistics,", "citeRegEx": "Davies,? \\Q2009\\E", "shortCiteRegEx": "Davies", "year": 2009}, {"title": "Learning bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine Learning,", "citeRegEx": "Heckerman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "D.G. Jr.", "M.P. Vecchi"], "venue": null, "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "BNT structure learning package: Documentation and experiments", "author": ["P. Leray", "O. Francois"], "venue": "Laboratoire PSI, Tech. Rep,", "citeRegEx": "Leray and Francois,? \\Q2004\\E", "shortCiteRegEx": "Leray and Francois", "year": 2004}, {"title": "The bayes net toolbox for matlab", "author": ["K. Murphy"], "venue": "Computing science and statistics,", "citeRegEx": "Murphy,? \\Q2001\\E", "shortCiteRegEx": "Murphy", "year": 2001}, {"title": "Gaussian processes in machine learning", "author": ["C.E. Rasmussen"], "venue": "Advanced Lectures on Machine Learning,", "citeRegEx": "Rasmussen,? \\Q2004\\E", "shortCiteRegEx": "Rasmussen", "year": 2004}, {"title": "Integrative construction and analysis of condition-specific biological networks", "author": ["S. Roy", "T. Lane", "M. Warner-Washburne"], "venue": "Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Roy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2007}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2001}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics,", "citeRegEx": "Schwarz,? \\Q1978\\E", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "On the use of Bayesian networks to analyze survey data", "author": ["P. Sebastiani", "M. Ramoni"], "venue": "Research in Official Statistics,", "citeRegEx": "Sebastiani and Ramoni,? \\Q2001\\E", "shortCiteRegEx": "Sebastiani and Ramoni", "year": 2001}, {"title": "Bayesian network score approximation using a metagraph kernel", "author": ["B. Yackley", "E. Corona", "T. Lane"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Yackley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yackley et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "A Bayesian network (Heckerman et al., 1995) is a statistical model used to represent probabilistic relationships among a set of variables as a directed acyclic graph, where the distribution of a single variable is defined in terms of the values of that variable\u2019s parents in the graph.", "startOffset": 19, "endOffset": 43}, {"referenceID": 2, "context": "Bayesian networks are commonly used to infer distributions over unobserved or query variables given known values for others (for instance, spam classification (Sebastiani & Ramoni, 2001) or disease diagnosis (Burge et al., 2009)).", "startOffset": 208, "endOffset": 228}, {"referenceID": 3, "context": "The process of learning a Bayesian network given a set of observed data is difficult, and is in fact NP-complete in the case of finding an exact optimum (Chickering, 1996; Chickering et al., 1995); most techniques are still limited in", "startOffset": 153, "endOffset": 196}, {"referenceID": 4, "context": "The process of learning a Bayesian network given a set of observed data is difficult, and is in fact NP-complete in the case of finding an exact optimum (Chickering, 1996; Chickering et al., 1995); most techniques are still limited in", "startOffset": 153, "endOffset": 196}, {"referenceID": 6, "context": "There are many Bayesian network scoring functions one could use as a basis for a search, but the BDe score (Heckerman et al., 1995), has several desired properties.", "startOffset": 107, "endOffset": 131}, {"referenceID": 16, "context": "In previous work (Yackley et al., 2008), we showed that a spline-based regression model could be used to estimate the BDe score of a network.", "startOffset": 17, "endOffset": 39}, {"referenceID": 11, "context": "The form of Gaussian process regressor (Rasmussen, 2004) we use is known as simple kriging, and takes the form:", "startOffset": 39, "endOffset": 56}, {"referenceID": 11, "context": "See (Rasmussen, 2004), Equation 5.", "startOffset": 4, "endOffset": 21}, {"referenceID": 16, "context": "The topology we use here, which we call the metagraph (Yackley et al., 2008), is defined as the graph of some relation over a set of combinatorial objects.", "startOffset": 54, "endOffset": 76}, {"referenceID": 14, "context": "For example, the BIC score (Schwarz, 1978) is defined as follows, in terms of a log-likelihood score and a penalty term.", "startOffset": 27, "endOffset": 42}, {"referenceID": 10, "context": "All of these are too large for an ADTree to fit in memory, and so the scores were calculated using the Bayes Net Toolkit (Murphy, 2001) and its accompanying Structure Learning Package (Leray & Francois, 2004).", "startOffset": 121, "endOffset": 135}, {"referenceID": 7, "context": "annealing (Kirkpatrick et al., 1983), as well as to other combinatorial objects such as general 0-1 matrices and permutations.", "startOffset": 10, "endOffset": 36}, {"referenceID": 12, "context": "These new accelerated forms of algorithms will allow researchers in fields as diverse as astronomy (Kent, 1994), biology (Roy et al., 2007), and linguistics (Davies, 2009) to better analyze data and create hypotheses given their often staggeringly large data sets.", "startOffset": 121, "endOffset": 139}, {"referenceID": 5, "context": ", 2007), and linguistics (Davies, 2009) to better analyze data and create hypotheses given their often staggeringly large data sets.", "startOffset": 25, "endOffset": 39}], "year": 2012, "abstractText": "As data sets grow in size, the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account. For instance, Bayesian networks, the model chosen in this paper, have a super-exponentially large search space for a fixed number of variables. One possible method to alleviate this problem is to use a proxy, such as a Gaussian Process regressor, in place of the true scoring function, training it on a selection of sampled networks. We prove here that the use of such a proxy is well-founded, as we can bound the smoothness of a commonly-used scoring function for Bayesian network structure learning. We show here that, compared to an identical search strategy using the network\u2019s exact scores, our proxy-based search is able to get equivalent or better scores on a number of data sets in a fraction of the time.", "creator": "LaTeX with hyperref package"}}}