{"id": "1206.6454", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Hierarchical Exploration for Accelerating Contextual Bandits", "abstract": "Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (1601kb)", "http://arxiv.org/abs/1206.6454v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yisong yue", "sue ann hong", "carlos guestrin"], "accepted": true, "id": "1206.6454"}, "pdf": {"name": "1206.6454.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Exploration for Accelerating Contextual Bandits", "authors": ["Yisong Yue", "Sue Ann Hong", "Carlos Guestrin"], "emails": ["yisongyue@cmu.edu", "sahong@cs.cmu.edu", "guestrin@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2. The Learning Problem", "text": "We study the linear stochastic bandit problem (Abbasi-Yadkori et al., 2011), which formalizes a referral system as a bandit algorithm that performs iterative actions and learns from the rewards received per action. At each iteration t = 1,..., T, our algorithm interacts with the user as follows: \u2022 The system recommends an item (i.e., performs an action) that is associated with the feature vector xt-Xt-D that encodes content and user functions. \u2022 The user gives feedback (i.e. reward) y-t.Rewards are not modelled as a linear function of actions x-D in such a way that E [y-t | x] = w-x, with the weight vector w-x indicating the (unknown) preferences of the user. We assume that feedback is independently sampled and within [0, 1], 2, and that all-x applies to T-1."}, {"heading": "3. Feature Hierarchies", "text": "In order to learn a reliable user model (i.e. a reliable estimate of the number of users) from user feedback, bandit algorithms must make recommendations that examine the entire D-dimensional feature space. Conventional bandit algorithms such as LinUCB attach a uniform a priori meaning to each dimension that can be inefficient. 2Our results also apply if each y is independent of sub-Gaussian noise and mean w-shaped text (see Appendix A). 3Since the rewards are scanned independently of each other, each guarantee results in (1) a high probability of regretting the feedback observed T t = 1 w-shaped x-shaped x-shaped x-shaped x-shaped, especially if additional structure can be adopted. We are now motivating and formalizing such a structure: the functional hierarchy. For example, we assume that two of the D features correspond to interest in articles about baseball and cricket."}, {"heading": "3.1. Extension to Deeper Hierarchies", "text": "For the -th level, we define the projected w * asw * \u2212 1 = Uw * + w *, *.Then, w * = U1 (U2 (... (ULw * L + w * L \u2212 1,)... w * 1, *) + w *.Algorithm 1 CoFineUCB1: Input: \u03bb,., U, ct (\u00b7), c * t (\u00b7) 2: for t = 1,.., T do 3: Define Xt [x1, x2,., xt \u2212 1] 4: Define X * t, UXt 5: Define Yt (y * 1, y \u00b2 2,..., y \u2212 1] 6: M + IK + X tX tX t \u00b2 t 7: w \u00b2 t \u00b2 t \u00b2 (maxi \u00b2), maxi \u00b2 t \u00b2 t \u00b2 t / / / least squares on rough level 8: Mt \u00b2 ID + XtX \u00b2 t \u00b2 t: evt \u00b2 t \u00b2 t \u00b2 t \u00b2 t (c \u00b2 t \u00b2) (maxi \u00b2) \u00b2 t \u00b2 t \u00b2 (maxi \u00b2) \u00b2 t \u00b2 t \u00b2 (t \u00b2)."}, {"heading": "4. Algorithm & Main Results", "text": "We present a bandit algorithm which exploits the hierarchies. Our algorithm, CoFineUCB, is an algorithm with upper trust binding, which generalizes the well-examined LinUCB algorithm and automatically slides between the exploration of the rough and complete trust space. (CoFineUCB is described in algorithm 1. (CoFineUCB estimates the preferences of the user in the subspace, w-log, as well as the complete trust space, w.) Both estimates are solved by regulated minimum square regression. (First, w-log, w-log, w-log, w-log, w-log, the preferences of the user in the subspace, w-log, as well as the complete confidence space, w. (Both estimates are regressed by regulated minimum square regressions.) First, w-log, w-log, w-log, w-log, w-log, w-log, the preferences of the subspace, w-log, and the complete trust space. (Both estimates are regressed by regulated minimum square regressions.) First, w-trust Lowg, w-log, w-trust, w-log, w, w-log, w-log, the trust space, and the trust space."}, {"heading": "5. Constructing Feature Hierarchies", "text": "We now show how to find a subspace U with the already existing user profiles W = {w} i Ni = 1, where each profile is independent of a common distribution w \u00b2 i \u00b2 W. In this context, a reasonable goal is to find a U that minimizes an empirical estimate of the boundary to RT (W), which includes w \u00b2 and w \u00b2 W. Our approach is outlined in Algorithm 2. We assume that the search for a K-dimensional subspace with low residual standards w \u00b2,...., D} 2: (A, A) SV D (W) 3: U0 UUUUU2: UU3 U3: U3 U4: 0 U3 U4: 0 U3 U4: 0 U3 U4: 0 U3 U4: 0 U3 U4: 0 U4: 0 U4 U4 U4: 0 U4 U4: 0 U4 U4 U4: 0 U4 U4: 0 U4 U4: 0 U4 U4: 0 U4 U4: 0 U4 U5: 0 U5: 0 U5 U5 U5: 6 0: 6 0: 0."}, {"heading": "6. Experiments", "text": "We evaluate CoFineUCB both through simulations and a live user study in the area of personalized message recommendations. First, we describe alternative methods or baselines to use prior knowledge (pre-existing profiles W-D-N) that do not use a functional hierarchy. Conceptually, these baselines can be formulated as special cases of CoFineUCB. The basic idea is that alter4One can also be regulated by inserting an axis-aligned \"ridge\" in W (i.e. W-ID). The feature space is such that w * is small in the new space. Therefore, the operation of LinUCB in the modified feature space leads to improved limitation of the regret (12) that is linear in w *."}, {"heading": "6.1. Baseline Approaches", "text": "The estimation problem can be written aswt = argmin wt \u2212 1\u03c4 = 1 (wx\u03c4 \u2212 y) 2 + \u03bbw \u2212 w \u00b2 2, which implies a lesser regret. Reshape Another approach is to use LinUCB with a feature space that is \"reshaped\" via a transformation UD \u00b2 D: wt = argmin wt \u2212 1\u0432 = 1 (wUDx\u03c4 \u2212 y \u00b2) 2 + \u03bbw2. (18) As in the above approach of middle regulation, we would like to have the representation of w \u00b2 in the redesigned space to have a small standard. In our experiments, we use UD = LearnU (W, D).We can integrate such a transformation into the CoUCeFineB \u00b2 space to have a small standard that we compile with UxU (W, D)."}, {"heading": "6.2. Experimental Setting", "text": "We use the submodular bandit extension of linear stochastic bandits (Yue & Guestrin, 2011) to model the message recommendation setting, where the 5W-UDUD-1 UDW algorithm has to select a series of L actions and receives rewards based on both the quality and variety of actions chosen (L = 1 is the conventional bandit setting).Using this structured action space results in a more realistic content recommendation setting, as recommendation systems often have to recommend multiple items at once. It is easy to extend CoFineUCB to the submodular bandit setting (see Appendix C)."}, {"heading": "6.3. Simulations", "text": "We performed simulation evaluations using data collected from a previous user study in Personalized Messaging Recommendations (\u03b2 & Guestrin, 2011), which included featured articles (D = 100) and N = 77 user profiles. We used leave-one-out validation: for each user, however, the transformations UD and U (K = 5) were trained on the remaining user profiles. For each user, we performed 25 simulations (T = 10000). All algorithms used the same U and UD projections where applicable. We also compared with a variant of CoFineUCB, CoFineUCB Focus, which reduces exploration in full space by a factor of 0.25. Figure 3 (a) shows the cumulative regret of each algorithm averaged over all users when we recommend one article per iteration (L = 1). All algorithms dramatically exceed exploration into full space."}, {"heading": "6.4. User Study", "text": "In fact, most of them are able to survive by themselves if they do not see themselves in a position to do so."}, {"heading": "7. Related Work", "text": "Most of them are unable to address the problem of exploration and are often preoccupied with pre-conceived feedback, which can lead to a distorted model."}, {"heading": "8. Conclusion", "text": "We have presented a general approach to encoding prior knowledge to accelerate contextual bandit learning. Specifically, our approach uses a roughly too fine hierarchy of traits that drastically reduces the amount of exploration required. We evaluated our approach in defining personalized message recommendations, where we showed significant improvements over existing approaches to encoding prior knowledge. Authors thank the anonymous reviewers for their helpful comments. Authors also thank Khalid El-Arini for his help in data collection and processing, which was partially supported by ONR (PECASE) N0001410672, ONR Young Investigator Pro-gram N00014-08-1-0752 and the Intel Science and Tech-nology Center for Embedded Computing."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Abbasi-Yadkori", "Yasin", "P\u00e1l", "David", "Szepesv\u00e1ri", "Csaba"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Online-to-confidence-set conversions and application to sparse stochastic bandits", "author": ["Abbasi-Yadkori", "Yasin", "Pal", "David", "Szepesvari", "Csaba"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2012}, {"title": "Fair and balanced: Learning to present news stories", "author": ["Ahmed", "Amr", "Teo", "Choon Hui", "S.V.N. Vishwanathan", "Smola", "Alexander"], "venue": "In ACM Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Ahmed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2012}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["Argyriou", "Andreas", "Micchelli", "Charles A", "Pontil", "Massimiliano", "Ying", "Yiming"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["Blei", "David", "Ng", "Andrew", "Jordan", "Michael"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Bandit theory meets compressed sensing for high dimensional stochastic linear bandit", "author": ["Carpentier", "Alexandra", "Munos", "Remi"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Carpentier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Carpentier et al\\.", "year": 2012}, {"title": "Combinatorial bandits", "author": ["Cesa-Bianchi", "Nicol", "Lugosi", "Gabor"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2009}, {"title": "An empirical evaluation of thompson sampling", "author": ["Chapelle", "Olivier", "Li", "Lihong"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas", "Kakade", "Sham"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Turning down the noise in the blogosphere", "author": ["El-Arini", "Khalid", "Veda", "Gaurav", "Shahaf", "Dafna", "Guestrin", "Carlos"], "venue": "In ACM Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "El.Arini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "El.Arini et al\\.", "year": 2009}, {"title": "Contextual gaussian process bandit optimization", "author": ["Krause", "Andreas", "Ong", "Cheng Soon"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Krause et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2011}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["Langford", "John", "Zhang", "Tong"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Langford et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2007}, {"title": "Scene: A scalable two-stage personalized news recommendation system", "author": ["Li", "Lei", "Wang", "Dingding", "Tao", "Knox", "Daniel", "Padmanabhan", "Balaji"], "venue": "In ACM Conference on Information Retrieval (SIGIR),", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Schapire", "Robert"], "venue": "In World Wide Web Conference (WWW),", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Bandits for taxonomies: A model-based approach", "author": ["Pandey", "Sandeep", "Agarwal", "Deepak", "Chakrabarti", "Deepayan", "Josifovski", "Vanja"], "venue": "In SIAM Conference on Data Mining (SDM),", "citeRegEx": "Pandey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pandey et al\\.", "year": 2007}, {"title": "Multi-armed bandit problems with dependent arms", "author": ["Pandey", "Sandeep", "Chakrabarti", "Deepayan", "Agarwal", "Deepak"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Pandey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pandey et al\\.", "year": 2007}, {"title": "Linearly parameterized bandits", "author": ["Rusmevichientong", "Paat", "Tsitsiklis", "John"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Rusmevichientong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rusmevichientong et al\\.", "year": 2010}, {"title": "Contextual bandits with similarity information", "author": ["Slivkins", "Aleksandrs"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Slivkins and Aleksandrs.,? \\Q2011\\E", "shortCiteRegEx": "Slivkins and Aleksandrs.", "year": 2011}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Streeter", "Matthew", "Golovin", "Daniel"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Streeter et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2008}, {"title": "Linear submodular bandits and their application to diversified retrieval", "author": ["Yue", "Yisong", "Guestrin", "Carlos"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Zhang", "Yu", "Yeung", "Dit-Yan"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "A common formalization of such a problem is the linear stochastic bandit problem (Li et al., 2010), which models user utility as a linear function of user and content features.", "startOffset": 81, "endOffset": 98}, {"referenceID": 8, "context": "For instance, the well-studied LinUCB algorithm (Dani et al., 2008; Abbasi-Yadkori et al., 2011) achieves a regret bound that is linear in the dimensionality of the feature space, which cannot be improved without further assumptions.", "startOffset": 48, "endOffset": 96}, {"referenceID": 0, "context": "For instance, the well-studied LinUCB algorithm (Dani et al., 2008; Abbasi-Yadkori et al., 2011) achieves a regret bound that is linear in the dimensionality of the feature space, which cannot be improved without further assumptions.", "startOffset": 48, "endOffset": 96}, {"referenceID": 13, "context": "Therefore, a common approach to dealing with slow convergence is dimensionality reduction based on prior knowledge, such as previously learned user profiles, by representing new users as linear combinations of \u201cstereotypical users\u201d (Li et al., 2010; Yue & Guestrin, 2011).", "startOffset": 232, "endOffset": 271}, {"referenceID": 8, "context": "We perform empirical validaThe regret bound is information-theoretically optimal up to log factors (Dani et al., 2008).", "startOffset": 99, "endOffset": 118}, {"referenceID": 0, "context": "We study the linear stochastic bandit problem (Abbasi-Yadkori et al., 2011), which formalizes a recommendation system as a bandit algorithm that iteratively performs actions and learns from rewards received per action.", "startOffset": 46, "endOffset": 75}, {"referenceID": 3, "context": "This formulation is akin to multi-task structure learning, where W0 would denote the various tasks and \u03a9 denotes feature relationships common across tasks (Argyriou et al., 2007; Zhang & Yeung, 2010).", "startOffset": 155, "endOffset": 199}, {"referenceID": 13, "context": "While the method seems to perform well given a good subspace (as seen in (Li et al., 2010; Chapelle & Li, 2011; Yue & Guestrin, 2011), among others), it can yield linear regret if the residual of the user\u2019s preference is strong, as we will see in the experiments.", "startOffset": 73, "endOffset": 133}, {"referenceID": 4, "context": "We represented articles usingD = 100 features corresponding to topics learned via latent Dirichlet Allocation (Blei et al., 2003).", "startOffset": 110, "endOffset": 129}, {"referenceID": 13, "context": "Optimizing recommender systems via user feedback has become increasingly popular in recent years (ElArini et al., 2009; Li et al., 2010; 2011; Yue & Guestrin, 2011; Ahmed et al., 2012).", "startOffset": 97, "endOffset": 184}, {"referenceID": 2, "context": "Optimizing recommender systems via user feedback has become increasingly popular in recent years (ElArini et al., 2009; Li et al., 2010; 2011; Yue & Guestrin, 2011; Ahmed et al., 2012).", "startOffset": 97, "endOffset": 184}, {"referenceID": 13, "context": "The exploration-exploitation tradeoff inherent in learning from user feedback is naturally modeled as a contextual bandit problem (Langford & Zhang, 2007; Li et al., 2010; Slivkins, 2011; Chapelle & Li, 2011; Krause & Ong, 2011).", "startOffset": 130, "endOffset": 228}, {"referenceID": 8, "context": "Our work builds upon a long line of research on linear stochastic bandits (Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Abbasi-Yadkori et al., 2011).", "startOffset": 74, "endOffset": 159}, {"referenceID": 0, "context": "Our work builds upon a long line of research on linear stochastic bandits (Dani et al., 2008; Rusmevichientong & Tsitsiklis, 2010; Abbasi-Yadkori et al., 2011).", "startOffset": 74, "endOffset": 159}, {"referenceID": 1, "context": "Another related line of work is that of sparse linear bandits (Abbasi-Yadkori et al., 2012; Carpentier & Munos, 2012).", "startOffset": 62, "endOffset": 117}, {"referenceID": 3, "context": "The problem of learning a good subspace U is related to finding a good regularization structure for multitask learning (Argyriou et al., 2007; Zhang & Yeung, 2010).", "startOffset": 119, "endOffset": 163}], "year": 2012, "abstractText": "Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user\u2019s preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation.", "creator": "Preview"}}}