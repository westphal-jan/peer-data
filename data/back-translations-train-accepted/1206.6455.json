{"id": "1206.6455", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Regularizers versus Losses for Nonlinear Dimensionality Reduction: A Factored View with New Convex Relaxations", "abstract": "We demonstrate that almost all non-parametric dimensionality reduction methods can be expressed by a simple procedure: regularized loss minimization plus singular value truncation. By distinguishing the role of the loss and regularizer in such a process, we recover a factored perspective that reveals some gaps in the current literature. Beyond identifying a useful new loss for manifold unfolding, a key contribution is to derive new convex regularizers that combine distance maximization with rank reduction. These regularizers can be applied to any loss.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (451kb)", "http://arxiv.org/abs/1206.6455v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james neufeld", "yaoliang yu", "xinhua zhang", "ryan kiros", "dale schuurmans"], "accepted": true, "id": "1206.6455"}, "pdf": {"name": "1206.6455.pdf", "metadata": {"source": "META", "title": "Regularizers versus Losses for Nonlinear Dimensionality Reduction", "authors": ["Yaoliang Yu", "James Neufeld", "Ryan Kiros", "Xinhua Zhang", "Dale Schuurmans"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, it is not only possible for them to reveal an important structure in the data, but also to provide an automated form of noise suppression and data normalization that allows subsequent data analysis. Although linear dimensionality reductions are a well-studied subject, recent advances continue with the study of convex regulators, such as the trace standard, which allows the application of general losses beyond the square sources of error."}, {"heading": "2. Preliminaries", "text": "In the following, we have to manipulate data, kernels and Euclidean distance matrices and put them in relation to each other. Suppose we get t observations, either expressed as t \u00b7 n data matrix X; a t \u00b7 t core matrix K, where K = K \u2032 and K < 0; or a t \u00b7 t squared Euclidean distance matrix D, where D = D \u2032, D \u2265 0, \u03b4 (D) = 0 and HDH 4 0 are addressed, so that we denote diagonally andH = I \u2212 1t11 \u2032 the centering matrix. Then we can map between these different matrices viaK (X) = XX \u2032 (X) = ciphering (X) = ciphering rivalation (XX \u2032) 1 ciphering rivalation (XX \u2032) and ciphering (XX \u2032) ciphering rivalation (X \u2032) = eusionality."}, {"heading": "3. Background: Linear Case", "text": "It turns out that a simple, generic strategy covers almost all the methods that have been proposed so far: Firstly, the regularized loss minimization (X-X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X-X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) X) (X) (X) (X) (X) X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X)) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X) (X (X) (X) (X) (X) (X) (X) (X) (X (X) (X) (X) (X) (X) (X) (X (X) X (X) (X) (X (X) (X) (X (X) X (X (X) X) X (X (X) (X) X (X (X) (X (X) X (X"}, {"heading": "4. Nonlinear Case", "text": "A general non-parametric approach to dimensionality reduction can be achieved by expressing the problem with kernel matrices. (Remember that data or Euclidean removal matrices can always be converted into kernel matrices.) Again, a simple, generic strategy covers almost all proposed methods: First, solve the regularized loss minimization min K = K, K = K < 0, K = 1L (D (K)) + R (K) for a given loss L and regulator R by considering the reconstructed loss minimization min K = K (K)."}, {"heading": "4.1. Regularizers", "text": "The natural role for the regulation is the most commonly used regulation, often connected with a reduction (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K).). (K). (K). (K). (K). (K). (K).). (K).). (K). (K). (K). (K). (K). (K). (K). (K).)."}, {"heading": "4.2. Loss Functions", "text": "The role of loss (D \u2212 D) H 2F (22) D (22) D (23) D (23) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D) D (29) D (29) D (29) D) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) D (29) (29) D (29) D (29) D (29) D (29) (29) D (29) D (29) D (29) (29) D (29) D (29) D ("}, {"heading": "5. New Convex Regularizers", "text": "Our main contribution is to propose two new convex regulators for non-parametric dimensionality reduction. In particular, we formulate convex relaxations of the partitioned regularizer (19), which simultaneously tries to distribute the distances to the uppermost d-dimensions and to reduce the distances in the other directions. Consequences for both the rank reduction and the multiple unfolding are clear. We first introduce a slight modification of (19) by defining a small square smoother R5 (K) = min P-P\u03b3 2 tr (K-2) + \u03b2tr (K-2) \u2212 (\u03b1 + \u03b2) tr (PK-2), (41), where \u03b3-0 and P are the same as in (20). Although this modified regulator is not yet convex in K-2, it allows two useful relaxations."}, {"heading": "5.1. Completed Square", "text": "The first loosening we propose is extremely simple: (41) can be made convex in K and P together simply by completing the square, giving R6 (K) = min P-P\u03b2tr (K) + GP-P-2-F. (42) This is a guaranteed upper limit for (41), since we are merely adding a non-negative term g2-P-2-F. A lower limit (41) can be recovered by subtracting d (\u03b1 + \u03b2) 22\u03b3 from (42), since P-2 F-d is for all P-P. The main advantage of this loosening is that it is extremely simple and mathematically attractive: a simple modification of the alternating minimization strategy of (Shaw & Jebara, 2007) now yields a global solution. However, this does not result in the closest convex approximation of (41), as we now show."}, {"heading": "5.2. Bi-conjugation", "text": "Remember that the conjugation of a function f = 1 chel max. (f) < x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (x (f) x (f) x (f) x (f) x (f) x (f) x (f) x (f (f) x (f) x (f) x (f (f) x) x (f) x (f) x (f (f) x (f (f) x) x (f (f) x) x (f (f) x) x (f (f (f) x) x x (f (f) x (f (f) x) x) x (f (f (f (f) x) x x"}, {"heading": "6. Experimental Evaluation", "text": "Most assessments of dimensionality reduction methods rely on subjective assessments of specific case studies, but there are many proposals for quantitative evaluation of the quality of different methods in a relatively objective way (Sun et al., 2011; Lee & Verleysen, 2010b; van der Maaten, 2009).The evaluation is clearer if the regulator and the loss components are considered separately.In order to compare the regulators, an objective evaluation can be based on the loss values achieved by the low-ranking reconstruction. For a given loss function L, we measure the reconstruction loss L (D (X-X); D) achieved by the restored low representation X-X. Another objective evaluation can be based on the runtime of the corresponding methods.5 Finally, we can measure the quality of convex relaxations by measuring the gap between the final targets achieved by the relaxed regulator R5 and the quellary R5."}, {"heading": "7. Conclusion", "text": "The result is a concise overview of much of the literature that shows a natural loss function that has not yet been thoroughly researched. More importantly, we have developed two new convex loosenings of a useful but non-convex reg-6 dataset that are included in the Supplement.ularizer. We examined the behavior of the new regulators in a representative sample of loss functions. Important directions for future research include expanding the convex regulatory framework to take into account other target topologies, sparseness, and mixtures."}], "references": [{"title": "Clustering with Bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I. Dhillon", "J. Ghosh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "GTM: the generative topographic mapping", "author": ["C. Bishop", "M. Svensen", "C. Williams"], "venue": "Neural Computation,", "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "On the impossibility of dimension reduction in l1", "author": ["B. Brinkman", "M. Charikar"], "venue": "In Proc. STOC,", "citeRegEx": "Brinkman and Charikar,? \\Q2003\\E", "shortCiteRegEx": "Brinkman and Charikar", "year": 2003}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J. Cai", "E. Candes", "Z. Shen"], "venue": "SIAM Journal on Optimzation,", "citeRegEx": "Cai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2008}, {"title": "Robust principal component analysis", "author": ["E. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "JACM, 58:1\u201337,", "citeRegEx": "Candes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2011}, {"title": "Robust Euclidean embedding", "author": ["L. Cayton", "S. Dasgupta"], "venue": "In ICML,", "citeRegEx": "Cayton and Dasgupta,? \\Q2006\\E", "shortCiteRegEx": "Cayton and Dasgupta", "year": 2006}, {"title": "A generalization of principal component analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R. Schapire"], "venue": "In NIPS", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "Convex Optimization and Euclidean Distance Geometry", "author": ["J. Dattorro"], "venue": "Meboo Publishing,", "citeRegEx": "Dattorro,? \\Q2012\\E", "shortCiteRegEx": "Dattorro", "year": 2012}, {"title": "A kernel view of dimensionality reduction of manifolds", "author": ["J. Ham", "D. Lee", "S. Mika", "B. Schoelkopf"], "venue": "In ICML,", "citeRegEx": "Ham et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2004}, {"title": "Convex envelopes of complexity controlling penalties", "author": ["V. Jojic", "S. Saria", "D. Koller"], "venue": "In AISTATS,", "citeRegEx": "Jojic et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jojic et al\\.", "year": 2011}, {"title": "Classification using discriminative restricted Boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Larochelle and Bengio,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio", "year": 2008}, {"title": "Spectral dimensionality reduction via maximum entropy", "author": ["N. Lawrence"], "venue": "In AISTATS,", "citeRegEx": "Lawrence,? \\Q2011\\E", "shortCiteRegEx": "Lawrence", "year": 2011}, {"title": "Nonlinear Dimensionality Reduction", "author": ["J. Lee", "M. Verleysen"], "venue": null, "citeRegEx": "Lee and Verleysen,? \\Q2010\\E", "shortCiteRegEx": "Lee and Verleysen", "year": 2010}, {"title": "Scale-independent quality criteria for dimensionality reduction", "author": ["J. Lee", "M. Verleysen"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Lee and Verleysen,? \\Q2010\\E", "shortCiteRegEx": "Lee and Verleysen", "year": 2010}, {"title": "Kernelized sorting", "author": ["N. Quadrianto", "A. Smola", "L. Song", "T. Tuytelaars"], "venue": "IEEE PAMI,", "citeRegEx": "Quadrianto et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Quadrianto et al\\.", "year": 2010}, {"title": "Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "Contractive auto-encoders: explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": null, "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Think globally, fit locally: unsupervised learning of low dimensional manifolds", "author": ["L. Saul", "S. Roweis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Saul and Roweis,? \\Q2003\\E", "shortCiteRegEx": "Saul and Roweis", "year": 2003}, {"title": "Kernel principal component analysis", "author": ["B. Schoelkopf", "A. Smola", "K. Mueller"], "venue": "In NIPS", "citeRegEx": "Schoelkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schoelkopf et al\\.", "year": 1999}, {"title": "Minimum volume embedding", "author": ["B. Shaw", "T. Jebara"], "venue": "In AISTATS,", "citeRegEx": "Shaw and Jebara,? \\Q2007\\E", "shortCiteRegEx": "Shaw and Jebara", "year": 2007}, {"title": "Structure preserving embedding", "author": ["B. Shaw", "T. Jebara"], "venue": "In ICML,", "citeRegEx": "Shaw and Jebara,? \\Q2009\\E", "shortCiteRegEx": "Shaw and Jebara", "year": 2009}, {"title": "Curvilinear components analysis and Bregman divergences", "author": ["J. Sun", "M. Crowe", "C. Fyfe"], "venue": "In ESANN,", "citeRegEx": "Sun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2010}, {"title": "Extending metric multidimensional scaling with Bregman divergences", "author": ["J. Sun", "M. Crowe", "C. Fyfe"], "venue": "Pattern Recognition,", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. de Silva", "J. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": "Submitted to SIAM Journal on Optimization,", "citeRegEx": "Tseng,? \\Q2008\\E", "shortCiteRegEx": "Tseng", "year": 2008}, {"title": "Learning a parametric embedding by preserving local structure", "author": ["L. van der Maaten"], "venue": "In AISTATS,", "citeRegEx": "Maaten,? \\Q2009\\E", "shortCiteRegEx": "Maaten", "year": 2009}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "J. Mach. Learn. Research,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Unsupervised kernel dimension reduction", "author": ["M. Wang", "F. Sha", "M. Jordan"], "venue": "In NIPS", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["K. Weinberger", "F. Sha", "L. Saul"], "venue": "In ICML,", "citeRegEx": "Weinberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2004}, {"title": "Graph Laplacian regularization for large-scale semidefinite programming", "author": ["K. Weinberger", "F. Sha", "Q. Zhu", "L. Saul"], "venue": "In NIPS", "citeRegEx": "Weinberger et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2007}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "In NIPS", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Although linear dimensionality reduction is a well studied topic, recent progress continues to be made with the investigation of convex regularizers, such as the trace norm or 2,1-norm, that enable application to general losses beyond squared error (Candes et al., 2011; Xu et al., 2010; Srebro & Shraibman, 2005).", "startOffset": 249, "endOffset": 313}, {"referenceID": 32, "context": "Although linear dimensionality reduction is a well studied topic, recent progress continues to be made with the investigation of convex regularizers, such as the trace norm or 2,1-norm, that enable application to general losses beyond squared error (Candes et al., 2011; Xu et al., 2010; Srebro & Shraibman, 2005).", "startOffset": 249, "endOffset": 313}, {"referenceID": 16, "context": "as in restricted Boltzmann machines (Larochelle & Bengio, 2008), auto-encoders (Rifai et al., 2011), or parameterized kernel reductions (Wang et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 28, "context": ", 2011), or parameterized kernel reductions (Wang et al., 2010).", "startOffset": 44, "endOffset": 63}, {"referenceID": 6, "context": "For other losses, such as absolute error (11) or Bregman divergence (12), rank is normally enforced by means of alternating descent in a factored representation: minAB L(AB;X) where A and B are t \u00d7 d and d\u00d7n respectively (Collins et al., 2001; Gordon, 2002).", "startOffset": 221, "endOffset": 257}, {"referenceID": 4, "context": "The difficulty of working with rank explains the emergence of convex, rank-reducing regularizers such as the trace norm (8) (Candes et al., 2011; Srebro & Shraibman, 2005) and block norm (9) (Xu et al.", "startOffset": 124, "endOffset": 171}, {"referenceID": 32, "context": ", 2011; Srebro & Shraibman, 2005) and block norm (9) (Xu et al., 2010).", "startOffset": 53, "endOffset": 70}, {"referenceID": 3, "context": "These regularizers allow a tractable formulation for general convex losses, and also allow a desired rank to be enforced by appropriately choosing \u03b1 (Cai et al., 2008).", "startOffset": 149, "endOffset": 167}, {"referenceID": 15, "context": "2 Specifically, it is the bi-conjugate of the rank function over the unit sphere in spectral-norm (Recht et al., 2010).", "startOffset": 98, "endOffset": 118}, {"referenceID": 4, "context": "absolute loss (11) has been recently proposed for robust PCA (Candes et al., 2011; Xu et al., 2010), and Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al.", "startOffset": 61, "endOffset": 99}, {"referenceID": 32, "context": "absolute loss (11) has been recently proposed for robust PCA (Candes et al., 2011; Xu et al., 2010), and Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al.", "startOffset": 61, "endOffset": 99}, {"referenceID": 6, "context": ", 2010), and Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al., 2001; Gordon, 2002).", "startOffset": 94, "endOffset": 130}, {"referenceID": 0, "context": "3 Noting the equivalence between regular exponential families and Bregman divergences (Banerjee et al., 2005).", "startOffset": 86, "endOffset": 109}, {"referenceID": 19, "context": "For example, the rank indicator (15) is the most commonly used regularizer, often associated with classical spectral dimensionality reduction (kernel PCA) using squared error (22) (Schoelkopf et al., 1999; Ham et al., 2004).", "startOffset": 180, "endOffset": 223}, {"referenceID": 8, "context": "For example, the rank indicator (15) is the most commonly used regularizer, often associated with classical spectral dimensionality reduction (kernel PCA) using squared error (22) (Schoelkopf et al., 1999; Ham et al., 2004).", "startOffset": 180, "endOffset": 223}, {"referenceID": 30, "context": "Consequently, the negated regularizer (17) has proved more effective, forming one of the key components of maximum variance unfolding (MVU) (28) (Weinberger et al., 2004; 2007).", "startOffset": 145, "endOffset": 176}, {"referenceID": 14, "context": "which provides a generalized approach to topographic embedding (Quadrianto et al., 2010; Bishop et al., 1998).", "startOffset": 63, "endOffset": 109}, {"referenceID": 1, "context": "which provides a generalized approach to topographic embedding (Quadrianto et al., 2010; Bishop et al., 1998).", "startOffset": 63, "endOffset": 109}, {"referenceID": 19, "context": "The doubly centered squared loss (22) is used in kernel PCA (Schoelkopf et al., 1999) (recall HDH = \u22122K(D)).", "startOffset": 60, "endOffset": 85}, {"referenceID": 7, "context": "Although (23) is frequently mentioned in the multidimensional scaling (MDS) literature (Cox & Cox, 2001), its use is rare since it cannot be tractably combined with rank (15) (Dattorro, 2012).", "startOffset": 175, "endOffset": 191}, {"referenceID": 23, "context": "These are the Sammon loss (26) (Sun et al., 2011; Lee & Verleysen, 2010a); the curvilinear components loss (27) (Sun et al.", "startOffset": 31, "endOffset": 73}, {"referenceID": 22, "context": ", 2011; Lee & Verleysen, 2010a); the curvilinear components loss (27) (Sun et al., 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 30, "context": ", 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al., 2004; Tenenbaum et al., 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al.", "startOffset": 64, "endOffset": 113}, {"referenceID": 24, "context": ", 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al., 2004; Tenenbaum et al., 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al.", "startOffset": 64, "endOffset": 113}, {"referenceID": 31, "context": ", 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al., 2007), respectively.", "startOffset": 65, "endOffset": 90}, {"referenceID": 23, "context": "The unnormalized entropy (30) was proposed in (Sun et al., 2011) to approximate the Sammon loss (26), whereas the reciprocal exponential Bregman divergence (31) was proposed in (Sun et al.", "startOffset": 46, "endOffset": 64}, {"referenceID": 22, "context": ", 2011) to approximate the Sammon loss (26), whereas the reciprocal exponential Bregman divergence (31) was proposed in (Sun et al., 2010) to approximate the curvilinear components loss (27) under w(d\u0302) = exp(d\u0302/\u03c3).", "startOffset": 120, "endOffset": 138}, {"referenceID": 8, "context": "The second loss (40) can be shown to be equivalent to local linear embedding (LLE) (Roweis & Saul, 2000; Saul & Roweis, 2003; Ham et al., 2004) if one tracks the solution in the limit as \u03c1\u2198 0.", "startOffset": 83, "endOffset": 143}, {"referenceID": 11, "context": "Interestingly, this loss-based framework generalizes probabilistic formulations (Lawrence, 2011), since e.", "startOffset": 80, "endOffset": 96}, {"referenceID": 9, "context": "Therefore, a general strategy for deriving maximal convex lower bounds on objective functions can be based on Fenchel bi-conjugation (Jojic et al., 2011).", "startOffset": 133, "endOffset": 153}, {"referenceID": 30, "context": "Putting \u03b3 = 0, Theorem 1 implies that the Fenchel biconjugate of (19) is exactly (17) (for Z = \u2212\u03b1I) if d > 0 and is (16) (for Z = \u03b2I) if d = 0, which might explain the success of MVU (Weinberger et al., 2004).", "startOffset": 183, "endOffset": 208}, {"referenceID": 23, "context": "However, many proposals exist for quantitatively evaluating the quality of different methods in a somewhat objective manner (Sun et al., 2011; Lee & Verleysen, 2010b; van der Maaten, 2009).", "startOffset": 124, "endOffset": 188}, {"referenceID": 25, "context": "5 Here we measure run time using a common implementation based on accelerated projected subgradient descent (Tseng, 2008).", "startOffset": 108, "endOffset": 121}], "year": 2012, "abstractText": "We demonstrate that almost all nonparametric dimensionality reduction methods can be expressed by a simple procedure: regularized loss minimization plus singular value truncation. By distinguishing the role of the loss and regularizer in such a process, we recover a factored perspective that reveals some gaps in the current literature. Beyond identifying a useful new loss for manifold unfolding, a key contribution is to derive new convex regularizers that combine distance maximization with rank reduction. These regularizers can be applied to any loss.", "creator": "LaTeX with hyperref package"}}}