{"id": "1206.6458", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Batch Active Learning via Coordinated Matching", "abstract": "Most prior work on active learning of classifiers has focused on sequentially selecting one unlabeled example at a time to be labeled in order to reduce the overall labeling effort. In many scenarios, however, it is desirable to label an entire batch of examples at once, for example, when labels can be acquired in parallel. This motivates us to study batch active learning, which iteratively selects batches of $k&gt;1$ examples to be labeled. We propose a novel batch active learning method that leverages the availability of high-quality and efficient sequential active-learning policies by attempting to approximate their behavior when applied for $k$ steps. Specifically, our algorithm first uses Monte-Carlo simulation to estimate the distribution of unlabeled examples selected by a sequential policy over $k$ step executions. The algorithm then attempts to select a set of $k$ examples that best matches this distribution, leading to a combinatorial optimization problem that we term \"bounded coordinated matching\". While we show this problem is NP-hard in general, we give an efficient greedy solution, which inherits approximation bounds from supermodular minimization theory. Our experimental results on eight benchmark datasets show that the proposed approach is highly effective", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (337kb)", "http://arxiv.org/abs/1206.6458v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["javad azimi", "alan fern", "xiaoli zhang fern", "glencora borradaile", "brent heeringa"], "accepted": true, "id": "1206.6458"}, "pdf": {"name": "1206.6458.pdf", "metadata": {"source": "META", "title": "Batch Active Learning via Coordinated Matching", "authors": ["Javad Azimi", "Alan Fern", "Xiaoli Z. Fern", "Glencora Borradaile"], "emails": ["AZIMI@EECS.OREGONSTATE.EDU", "AFERN@EECS.OREGONSTATE.EDU", "XFERN@EECS.OREGONSTATE.EDU", "GLENCORA@EECS.OREGONSTATE.EDU", "HEERINGA@CS.WILLIAMS.EDU"], "sections": [{"heading": "1. Introduction", "text": "In this book, we consider active learning of classification functions. We present a number of examples of the availability of more than one million people in the world, of which there are only a few. However, there are also examples that we have a pool of undescribed input factors. There are a number of examples that follow undescribed input factors. There are a number of examples that follow undescribed input factors. There are a number of examples that follow undescribed input factors that are not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, input factors that are not described, not described, not described, not described, not described, not described, not described, not described, not, not described, not described, not, not described, not, not described, not, not described, not, not described, not, not described, not, not described, not, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not, not described, not described, not described, not described, not described, not described, not, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described, not described,"}, {"heading": "2. Simulation Matching for Batch Selection", "text": "Faced with a dataset of Dl with marked examples, we are now considering how to select the next batch of k examples to label. A key question in this choice is how to handle the trade-off between selecting the examples that look most informative individually for learning and selecting a diverse set of examples. A common measure of informativeness, for example, is the margin or class uncertainty of an example with respect to the currently learned classifier. However, when selecting the top k example among such measures, clusters of nearby examples are often selected that are quite redundant. Previous work on active learning has considered various objective functions to capture this conflict, and then look for batches that roughly optimize these objectives. In this work, we are pursuing a different approach motivated by the fact that sequential active learning has been extensively studied and a variety of computationally efficient and empirically effective strategies."}, {"heading": "2.1. Sequential Policy Simulation", "text": "Given a number of labeled examples Dl and unlabeled examples Dl, \u03c0 returns the next example x-Du to be labeled. We would like to \"closely coordinate\" the behavior of \u03c0 when applied to k steps. However, without the labels of the selected instance, we do not know how \u03c0 would behave after selecting the first example. In particular, different labeling results are likely to result in a selection of other k examples. In this thesis, we assume the availability of a subordinate distribution of the labels of any x example, which can be estimated with the help of a probable classifier. A k-step execution of \u03c0 probably results in a set of k-selected examples from Du. Let sc\u03c0 be the random variable that denotes the set of k examples resulting from such a k-step execution of \u03c0, which have a well-defined distribution P-kp (\u00b7) over the subsets of Du.It is important that it is generally easy to use a series of MonteK to such a simulation."}, {"heading": "2.2. Coordinated Matching Objective", "text": "Our goal is to select such a model based on examples that best reflect \"the behavior\" of a basic sequential policy based on the examples currently described. Specifically, we consider a series B to be a good match if it has a high probability (relative to other sets) below the dataset distribution P k\u03c0. Unfortunately, for all but trivial sequential measures, there is no closed form for P k.M that makes it difficult to directly optimize the probability of B. Therefore, our approach is to first make an approximate distribution via Qk, the essential aspects of P kp and then the return of batch B optimized under Q k.Matching Model. A naive choice for Qk would be to present it as a latent mixing model, such as a Gaussian mixing model from which i.e. examples are drawn to produce a batch."}, {"heading": "3. Optimization Approach", "text": "In this section we present a greedy approximation algorithm for BCM, which is motivated by theoretical results to minimize non-increasing, supermodular set functions."}, {"heading": "3.1. Greedy Approximation Algorithm", "text": "Definition 1: Given a finite set A, a function on subsets of A, g: 2A \u2192 R + is supermodular only if for all A1 A2 A and x A\\ A2, it applies that g (A1) \u2212 g (A1 x) \u2265 g (A2) \u2212 g (A2 x). In other words, a supermodular function shows \"decreasing yields,\" because adding an element to set A reduces the value of g (\u00b7) by at most as much as adding the element to a subset of A. Moreover, a set function is not increasing if we have a g (A) for each set A and element x. It turns out that the problem of finding a subset of size k from A minimizes a non-increasing supermodular function g (\u00b7), approximately solves."}, {"heading": "3.2. Accelerated Greedy Algorithm", "text": "Each iteration of the greedy algorithm requires the evaluation of the cost function (Eq.5) for the removal of each element x from the current set \u00b5, which is at most equal to the size of Du. Each cost function evaluation involves the determination of N minimum cost comparisons, between each of the Si and \u00b5\\ x (if \u00b5\\ x is larger than Si, some elements of \u00b5 are not coordinated), which can be done via N calls to the Hungarian algorithm. While polynomial, for a naive implementation, each iteration can be mathematically expensive if you are large, fortunately there are at least three ways to speed up the calculation, which leads to drastic time reductions in our experience and the calculation is independent of the size of Du.First, let's leave the current set of elements in \u00b5 that match Si in the minimum fit. It's easy to verify this g (\u00b5)."}, {"heading": "4. Scalability", "text": "The calculation of our batch selection approach can be divided into two stages: 1) simulation of the sequential policy and 2) solution of the resulting BCM problem. As the number of unlabeled data points n increases, the simulation time will also increase, since each simulation step includes a sequential policy that typically takes into account each unlabeled point. Typical sequential strategies, including those used in our experiments, will increase the time complexity linearly in n. Fortunately, the N simulations created during the first stage are independent, allowing easy parallelization, which may result in a time reduction of a factor of N. In other words, parallelization does not require any time expenditure compared to the execution of a typical sequential algorithm for k-steps. Moreover, as described above, the time complexity of the second stage does not depend on n, but rather on N \u00b7 k. Overall, the scalability of the combined two stages is similar to the underlying sequential policy."}, {"heading": "5. Experimental Results", "text": "In this section, we evaluate our proposed method of active learning using eight binary classification problems from the UCI repository of machine learning (Asuncion & Newman, 2010), including (the number of seeds and attributes shown in brackets for each data set): Breast (569, 32), Ionosphere (351, 34), Pima (768, 8), German (1000, 24), Haberman (208, 60), EF (1543, 16) and MN (1575, 16). EF and MN datasets are subsets of the original multi-class character dataset, which are respected only by letters E and F, M and N. Baseline algorithms that we evaluate. To evaluate the effectiveness of the proposed algorithms, we compare our algorithms against four baseline methods."}, {"heading": "6. Conclusions", "text": "In this paper, we present a novel method of active learning in batch, which follows a recently proposed general approach called \"Simulation Matching.\" While this general approach has been successfully applied to the problem of batch Bayesian optimization, the term \"matching,\" which has been used in previous work, is not suitable for active learning by mimicking the behavior of a high-quality sequential strategy using simulation. We present a principled adaptation of the simulation matching approach to active learning in batch. Essentially, we consider sc\u03c0, the set of k points selected by sequential policy \u03c0, as a random variable. Since the distribution of sc\u03c0 is too complex to estimate directly, we prefer samples from this distribution using simulation and approach the distribution using a kMatching Mixture model, which is then used to select the batch algorithm we call \"Comparative M,\" which is used to coordinate the batch approach."}, {"heading": "A. Proof of Theorem 1", "text": "Proof. Given S1,.., SN, where each problem contains a set of k-points, the objective function of BCM is replaced by the following representation of the problem: \"weight reduction.\" We define a weighted two-part solution G = (U, V, E), where V = {S1, \u00b7 SN}, and U = Du, which represents the set of unmarked examples. \"U \u00b7 V\" is a weighted two-part solution G = (u, V, E), where V = {S1, \u00b7 SN}, and U = Du, which represents the set of unmarked examples. \"Weight reduction\" is a weighted two-part version of M (u, V, V, E). A coordinated assignment to G is a subset of edges E. \"\" E, \"so that for 1 \u2264 i \u2264 t, (U \u00b7 Si),\" weight reduction \"E\" is an adjustment to \"weight reduction.\""}, {"heading": "Acknowledgments", "text": "The authors acknowledge NSF's support through the funding programmes IIS-0812514, IIS-1055113 and IIS-0905678."}], "references": [{"title": "Batch bayesian optimization via simulation matching", "author": ["J. Azimi", "A. Fern", "X.Z. Fern"], "venue": null, "citeRegEx": "Azimi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Azimi et al\\.", "year": 2011}, {"title": "Incorporating diversity in active learning with support vector machines", "author": ["Brinker", "Klaus"], "venue": "In ICML,", "citeRegEx": "Brinker and Klaus.,? \\Q2003\\E", "shortCiteRegEx": "Brinker and Klaus.", "year": 2003}, {"title": "An analysis of the approximations for maximizing submodular set functions", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programmingn,", "citeRegEx": "Nemhauser et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Nemhauser et al\\.", "year": 1978}, {"title": "Discriminative batch mode active", "author": ["Y. Guo", "D. Schuurmans"], "venue": "learning. NIPS,", "citeRegEx": "Guo and Schuurmans,? \\Q2007\\E", "shortCiteRegEx": "Guo and Schuurmans", "year": 2007}, {"title": "Active instance sampling via matrix partition", "author": ["Guo", "Yuhong"], "venue": "In NIPS, pp", "citeRegEx": "Guo and Yuhong.,? \\Q2010\\E", "shortCiteRegEx": "Guo and Yuhong.", "year": 2010}, {"title": "Largescale text categorization by batch mode active learning", "author": ["Hoi", "Steven C. H", "Jin", "Rong", "Lyu", "Michael R"], "venue": "In WWW,", "citeRegEx": "Hoi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2006}, {"title": "Batch mode active learning and its application to medical image classification", "author": ["Hoi", "Steven C. H", "Jin", "Rong", "Zhu", "Jianke", "Lyu", "Michael R"], "venue": "In ICML,", "citeRegEx": "Hoi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2006}, {"title": "An approximation guarantee of the greedy descent algorithm for minimizing a supermodular set function", "author": ["Il\u2019ev", "Victor P"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Il.ev and P.,? \\Q2001\\E", "shortCiteRegEx": "Il.ev and P.", "year": 2001}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Krause et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Integer and combinatorial optimization", "author": ["G.L. Nemhauser", "L.A. Wolsey"], "venue": null, "citeRegEx": "Nemhauser and Wolsey,? \\Q1999\\E", "shortCiteRegEx": "Nemhauser and Wolsey", "year": 1999}, {"title": "Active learning literature survey", "author": ["Settles", "Burr"], "venue": "Computer Sciences Technical Report 1648,", "citeRegEx": "Settles and Burr.,? \\Q2009\\E", "shortCiteRegEx": "Settles and Burr.", "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Guo and Schuurmans (2007) posed batch active learning as a complex optimization problem that maximizes the discrimina-", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "This idea has been explored recently for the very different problem of batch Bayesian optimization (Azimi et al., 2011).", "startOffset": 99, "endOffset": 119}, {"referenceID": 8, "context": "This idea is analogous to a similar speedup approach used for submodular maximization (Krause et al., 2008).", "startOffset": 86, "endOffset": 107}], "year": 2012, "abstractText": "We propose a novel batch active learning method that leverages the availability of high-quality and efficient sequential active-learning policies by approximating their behavior when applied for k steps. Specifically, our algorithm uses MonteCarlo simulation to estimate the distribution of unlabeled examples selected by a sequential policy over k steps. The algorithm then selects k examples that best matches this distribution, leading to a combinatorial optimization problem that we term \u201cbounded coordinated matching\u201d. While we show this problem is NP-hard, we give an efficient greedy solution, which inherits approximation bounds from supermodular minimization theory. Experiments on eight benchmark datasets show that the proposed approach is highly effective.", "creator": "LaTeX with hyperref package"}}}