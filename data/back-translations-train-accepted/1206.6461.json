{"id": "1206.6461", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model", "abstract": "We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \\gamma\\in[0,1) only O(N\\log(N/\\delta)/((1-\\gamma)^3\\epsilon^2)) samples are required to find an \\epsilon-optimal estimation of the action-value function with the probability 1-\\delta. We also prove a matching lower bound of \\Theta (N\\log(N/\\delta)/((1-\\gamma)^3\\epsilon^2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, \\epsilon, \\delta and 1/(1-\\gamma). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1-\\gamma).", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (468kb)", "http://arxiv.org/abs/1206.6461v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mohammad gheshlaghi azar", "r\u00e9mi munos", "bert kappen"], "accepted": true, "id": "1206.6461"}, "pdf": {"name": "1206.6461.pdf", "metadata": {"source": "META", "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model ", "authors": ["Mohammad Gheshlaghi Azar", "R\u00e9mi Munos"], "emails": ["m.azar@science.ru.nl", "remi.munos@inria.fr", "b.kappen@science.ru.nl"], "sections": [{"heading": null, "text": "(N log (N / \u03b4) / (((1 \u2212 \u03b3) 3\u03b52)) Samples are required to find an \u03b5-optimal estimate of the action value function with a probability of 1 \u2212 \u03b4. We also prove a suitable lower limit of \u044b (N log (N / \u03b4) / (((1 \u2212 \u03b3) 3\u03b52))) with respect to sample complexity in estimating the optimal action value function by any RL algorithm. To our knowledge, this is the first matching result with respect to sample complexity in estimating the optimal (action) value function where the upper limit corresponds to the lower limit of RL with respect to N, \u03b5, \u03b4 and 1 / (1 \u2212 \u03b3). Furthermore, both our lower and our upper limits improve significantly compared to the state of the art with respect to 1 / (1 \u2212 \u03b3)."}, {"heading": "1. Introduction", "text": "We have shown that an action value variant of VI, model-based iteration (QVI), provides an optimal estimate of the action value function with high probability only using T = O (1 \u2212 2)."}, {"heading": "2. Background", "text": "In this section, we review some standard concepts and definitions from the theory of Markov decision-making processes (MDPs), then present the model-based Q-value iteration algorithm of Kearns & Singh (1999), and consider the Standard Reinforcement Learning (RL) framework (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998), in which a learning agent interacts with a stochastic environment and this interaction is modeled as a discretely time-discounted MDP. A-counted MDP is a quintuple (X, A, P, R, \u03b3), where X and A are the set of states and actions, P is the state transition distribution, R is the reward function (0, 1) is a discount factor, and r (x, a) is the probability distribution over the next state and the immediate reward distribution."}, {"heading": "2.1. Model-based Q-value Iteration (QVI)", "text": "The algorithm performs n transition samples from each state effect pair z-Z, for which it makes n calls to the generative model. 4 Subsequently, it builds an empirical model of transition chances such as: P (y | z), m (y, z) / n, 3To simplify the exposure, we subsequently remove the dependence on z and x, e.g. by writing Q for Q (z) and V for V (x), if there is no possible confusion. 4The total number of calls to the generative model results from T = nN. where m (y, z) indicates the number of times in which the state y-X has been reached from z-Z. The algorithm then makes an empirical estimate of the optimal action-value function Q-Z by iterating some action value functions Qk with the initial value of Q0 by the empirical Bellman optimality operator T-Z. 5"}, {"heading": "3. Main Results", "text": "Our main results are in the form of PAC (probably approximately correct) limits based on the \"Q\" standard of difference of the optimal action value function \"Q\" and its sample estimate: Theorem 1 (PAC-bound for model-based Q-value iteration). Let assumption 1 apply and T is a positive integer. Then, some constants \"c\" and \"c0\" exist so that for all samples \"A\" (0, 1) and \"B\" (0, 1) a total sample budget of \"T\" = \"c\u03b2 3N\u03b52 logc0N\u03b4,\" sufficient for the uniform approximation error \"Q\" \u2212 QK. \"(with the probability) at least 1 \u2212 \u043c\" log \"(6\u03b2 / \u03b5) / T\" (1 / \u03b3) an iteration of the QVI algorithm. \"6 In particular, c = 68 and c0 = 12.The following overall result provides a narrow limit for the number of transitions.\""}, {"heading": "4. Analysis", "text": "In this section, we first present the complete proof of the end-time PAC boundary of QVI, which is described in Theorem 1, in Subsection 4.1, and then we prove Theorem 2, the lower boundary of the Directive, in Subsection 4.2. We must also stress that we reject some of the evidence of the technical problems due to the lack of space, which we provide in a long version of this paper."}, {"heading": "4.1. Poof of Theorem 1", "text": "I'm not sure what I'm going to do with it, I'm not going to do it, I'm not going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, I'm going to do it, '."}, {"heading": "4.2. Proof of the Lower-bound", "text": "In our analysis, we rely on the probability ratio method (Q = 1) previously used to demonstrate a lower limit for multi-armed bandits (Mannor & Tsitsiklis, 2004) and extend this approach to RL and MDPs. We begin by defining a class of MDPs for which the proposed lower limit is to be achieved (see Figure 1). We assume that state space X consists of three smaller sets of MDPs M, Y1 and Y2. The defined S states of each of these states correspond to the set of measures A = {a1, a2,., aL}, whereas states in Y1 and Y2 are single action states."}, {"heading": "5. Conclusion and Future Works", "text": "In this paper, we have presented the first Minimax results tied to the sample complexity of estimating the optimal action value function in discounted reward MDPs. We have demonstrated that the model-based Q-value iteration algorithm (QVI) is an optimal learning algorithm because it minimizes dependencies to 1 / \u03b5, N, \u03b4 and \u03b2. Furthermore, our results have significantly improved in terms of dependence on \u03b2 compared to the state of the art. Overall, we conclude that QVI is an efficient RL algorithm that reduces the gap between the lower and upper limit of the sample complexity of RL in the presence of a generative model of MDP. In this work, we are only interested in estimating the optimal action value function and not in the problem of exploration. Therefore, we have not compared our results with the state-of-the-art method of PAC-MDP, but with the effectiveness of the upper scatterial method (2010, 2009, 2009 and 2009)."}, {"heading": "Acknowledgments", "text": "The authors are pleased to receive support from the Seventh Framework Programme of the European Community (FP7 / 2007-2013) under Funding Agreement No. 231495. We would also like to thank Bruno Scherrer for useful discussions and anonymous reviewers for their valuable comments."}], "references": [{"title": "Reinforcement learning with a near optimal rate of convergence", "author": ["Azar", "M. Gheshlaghi", "R. Munos", "M. Ghavamzadeh", "H.J. Kappen"], "venue": "Technical Report inria00636615,", "citeRegEx": "Azar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2011}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["P.L. Bartlett", "A. Tewari"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Bartlett and Tewari,? \\Q2009\\E", "shortCiteRegEx": "Bartlett and Tewari", "year": 2009}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximators", "author": ["Bu\u015f", "L. oniu", "Babu\u0161", "R. ka", "B. De Schutter", "D. Ernst"], "venue": null, "citeRegEx": "Bu\u015f et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bu\u015f et al\\.", "year": 2010}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["S.M. Kakade"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "citeRegEx": "Kakade,? \\Q2004\\E", "shortCiteRegEx": "Kakade", "year": 2004}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["M. Kearns", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kearns and Singh,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Singh", "year": 1999}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis", "year": 2004}, {"title": "Influence and variance of a Markov chain : Application to adaptive discretizations in optimal control", "author": ["R. Munos", "A. Moore"], "venue": "In Proceedings of the 38th IEEE Conference on Decision and Control,", "citeRegEx": "Munos and Moore,? \\Q1999\\E", "shortCiteRegEx": "Munos and Moore", "year": 1999}, {"title": "The variance of discounted markov decision processes", "author": ["M.J. Sobel"], "venue": "Journal of Applied Probability,", "citeRegEx": "Sobel,? \\Q1982\\E", "shortCiteRegEx": "Sobel", "year": 1982}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A.L. Strehl", "L. Li", "M.L. Littman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["Szepesv\u00e1ri", "Cs"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "Szepesv\u00e1ri and Cs.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Cs.", "year": 2010}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "Szepesv\u00e1ri", "Cs"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Szita et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Szita et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "We also rely on the fact that the variance of the sum of discounted rewards, like the expected value of the sum (value function), satisfies a Bellman-like equation, in which the variance of the value function plays the role of the instant reward (Munos & Moore, 1999; Sobel, 1982), to derive a sharp bound on the variance of the value function.", "startOffset": 246, "endOffset": 280}, {"referenceID": 0, "context": "In the case of lower bound, we improve on the result of Azar et al. (2011b) by adding some structure to the class of MDPs for which we prove the lower bound: In the new model, there is a high probability for transition from every intermediate state to itself.", "startOffset": 56, "endOffset": 76}, {"referenceID": 10, "context": "Therefore, we did not compare our results with the-state-of-the-art of PAC-MDP (Strehl et al., 2009; Szita & Szepesv\u00e1ri, 2010) and upper-confidence bound based algorithms (Bartlett & Tewari, 2009; Jaksch et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 4, "context": ", 2009; Szita & Szepesv\u00e1ri, 2010) and upper-confidence bound based algorithms (Bartlett & Tewari, 2009; Jaksch et al., 2010), in which the choice of the exploration policy has an influence on the behavior of the learning algorithm.", "startOffset": 78, "endOffset": 124}, {"referenceID": 10, "context": "Also, we believe that the existing lower bound on the sample complexity of exploration of any reinforcement learning algorithm (Strehl et al., 2009) can be significantly improved in terms of dependency on \u03b2 using the new \u201chard\u201d class of MDPs presented in this paper.", "startOffset": 127, "endOffset": 148}], "year": 2012, "abstractText": "We consider the problem of learning the optimal action-value function in the discountedreward Markov decision processes (MDPs). We prove a new PAC bound on the samplecomplexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \u03b3 \u2208 [0, 1) only O ( N log(N/\u03b4)/ ( (1 \u2212 \u03b3)\u03b5 )) samples are required to find an \u03b5-optimal estimation of the action-value function with the probability 1 \u2212 \u03b4. We also prove a matching lower bound of \u0398 ( N log(N/\u03b4)/ ( (1 \u2212 \u03b3)\u03b5 )) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action) value function in which the upper bound matches the lower bound of RL in terms ofN , \u03b5, \u03b4 and 1/(1\u2212\u03b3). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1\u2212 \u03b3).", "creator": "LaTeX with hyperref package"}}}