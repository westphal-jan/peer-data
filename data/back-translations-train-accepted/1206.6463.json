{"id": "1206.6463", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "An Iterative Locally Linear Embedding Algorithm", "abstract": "Local Linear embedding (LLE) is a popular dimension reduction method. In this paper, we first show LLE with nonnegative constraint is equivalent to the widely used Laplacian embedding. We further propose to iterate the two steps in LLE repeatedly to improve the results. Thirdly, we relax the kNN constraint of LLE and present a sparse similarity learning algorithm. The final Iterative LLE combines these three improvements. Extensive experiment results show that iterative LLE algorithm significantly improve both classification and clustering results.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (223kb)", "http://arxiv.org/abs/1206.6463v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["deguang kong", "chris h q ding"], "accepted": true, "id": "1206.6463"}, "pdf": {"name": "1206.6463.pdf", "metadata": {"source": "META", "title": "An Iterative Locally Linear Embedding Algorithm", "authors": ["Deguang Kong", "Chris Ding", "Heng Huang", "Feiping Nie"], "emails": ["doogkong@gmail.com", "chqding@uta.edu", "heng@uta.edu", "feipingnie@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "Recently there have been many algorithms proposed for nonlinear dimension reduction, the Isomap (Tenenbaum et al., 2000), local linear embedding (LLE) (Roweis & Saul, 2000), kernel LLE (Ham et al., 2004), Hessian LLE (Donoho & Grimes, 2003), local tangent alignment (Zhang & Zha, 2004), Laplacian embedding (Hall, 1971; Belkin & Niyogi, 2001), and many variations. Above dimension reduction algorithms usually cover two main steps: (A) for each data point, learn the local geometry information W. This W can be seen as similarity between data points or the edge weights of a data points are the data points, we call this W-learning, or learning the graph. \"B) using the learned W to embed the high-dimensional data points in a lower-dimensional space Y."}, {"heading": "2. LLE and New Formations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Brief overview of LLE", "text": "LLE (Roweis & Saul, 2000) is a non-linear approach to reducing dimensions. Suppose the data X = [x1, x2, \u00b7 \u00b7 \u00b7, xn] and < p \u00b7 n consist of n data points xi, each of which is equipped with dimensionality p. LLE expects each data point and its neighbors to lie on or near a locally linear multiplicity that determines how the weight coefficient W from Equation (1) is constructed. It then reconstructs each data point (low k-dimensional embedding vectors {yi}) from its neighbors via the same neighborhood relations by minimizing a quadratic cost function equation (2) that summarizes the percentage of each data point in the construction."}, {"heading": "2.2. LLE Improvements in two directions", "text": "In this paper, we propose improved formulations in both main steps in LLE. (A) In the W learning step of Eq. (1) we propose new, improved formulations in Eq. (2) we first propose a slightly modified formulation and prove that the solution of Eq. (2) is identical to the embedding of Normalized Section or Laplace. (B) Our iterative LLE algorithm is based on these improved formations in both LLE steps. (2) To establish a link to the embedding of diagrams, we restrict W not to be negative, i.e., we add constraints W \u2265 0 to Equation (1). (as in (Wang & Zhang, 2006). (2) We symmetrize W = 12 (W + WT)."}, {"heading": "2.3. LLE Y-learning is identical to Normalized Cut Spectral Clustering", "text": "In fact, this is a general result which is not limited to LNES.( 4) More precisely, we have theorem (1), theorem (1). Consequently, we have the denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated denominated dendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendendenden"}, {"heading": "2.4. Proof of Theorem 1", "text": "To prove theorem 1, we need Lemma 1. Lemma 1. The optimal solution to Eq q. (4) is, Y * = FTD \u2212 1 2, (9) where F = [f1, f2,... \u2212 fk] 2fk = \u03bblt; n \u00b7 k is the smallest k eigenvectors of (I \u2212 Z) 2, Z \u2212 1 2ZD \u2212 1, i.e., (I \u2212 Z \u2212 2) 2fk = \u03bbkfk. (10) Proof of Lemma 1. Proof. Note Y = [y1, y2, \u00b7, yn] T < k \u00b7 n. Lety i \u2212 z \u2212 j (D \u2212 1Z) ijyj, (11) and then Y = [y \u00b2 2, \u00b7 y \u00b2 Yy \u00b2 n]. Note Y = 1 < k \u00b7 vlt; k \u00b7 n."}, {"heading": "3. An Iterative LLE Learning Algorithm (ILLE)", "text": "We now use the above results, coupled with two new schemes (A, B), to derive a new learning algorithm."}, {"heading": "3.1. Motivation of iterative LLE", "text": "(A) Iterative process of LLEIn LLE, starting with X, we learn W and then learn Y as the low-dimensional embedding of data X. In this paper, we propose to use Y as the new data and iterate this process to further improve the embedding. The key observation is that the class structure of the data in Y is clearer than in X (this is the original embedding purpose of LLE). Therefore, we use Y as the new data and repeat this process to learn improved Y. (B) Kernel generalizationFrom experiments on multiple data sets, the results of using linear formulation on X for learning W in equivalent (1) are generally not as good as other state-of-the-art methods. Here, we use the kernel trick to generalize this to an arbitrary nonlinear similarity function."}, {"heading": "3.2. Proposed algorithm", "text": "By incorporating the above schemes of (A, B), we outline our iterative LLE learning algorithm as follows. (1) Solve for Wt with Eq. (16) or Eq. (18) 1. (2) Solve for Yt with Lemma 1. (3) Calculate for Wt with Eq. (16) or Eq. (18) 1. In pairs of similarities Wt, we solve for Yt with Lemma 1. (3) Calculate a new kernel Kt + 1 either as the end result of our algorithm (both embedding Yt and Kernel Kt + 1) or as an input to Step (1). Details of the Kt + 1 construction can be found in \u00a7 3.3. First, we get K1 from data X, we repeat above 3 steps for serval iterations to get a better kernel. See Algorithm 1 for more details. Note in Step (1): We have two alternatives to calculate Wt."}, {"heading": "3.3. Construction of the new kernel", "text": "In step (3) of our algorithm, once the low-dimensional embedding Yt = is obtained in general, we have the following possibilities. (a) Construct a new kernel from Yt. There are many ways to construct the kernel KY simply by using the Gaussian kernel, i.e., KY = e \u2212 \u03b3 | | yi \u2212 yj | | 2, where \u03b3 is the scale parameter. Another way is to construct a new kernel KY as the linear kernel in low-dimensional space, i.e., KY = YYYYT. (b) Construct the kernel Kt + 1 either as the end result of our algorithm or as an input to step (1). There are many ways to set (b1) Kt + 1 = KtY element."}, {"heading": "4. Improved W-Learning Formulation", "text": "Here we propose an improvement of the W-learning step of LLE. So far, we are sticking to the original LLE convention that W maintains the kNN structure, i.e. Wij 6 = 0 for only j-Ni (kNN of object i). In other words, we are completely bypassing kNN. We are now presenting a new approach to learn the paired similarity matrix S. \u2212 n-n, where Sij represents the contribution of the i-th data to the reconstruction of the data point xj. We hope that the newly learned S has a much clearer structure. We are using the symbol S to emphasize that W is learned with the new approach."}, {"heading": "4.1. Computational algorithm for Eq.(18)", "text": "The algorithm starts with a first guess of S = E (E is a matrix of all ones), iteratively updates S to Sij \u2190 Sij Kij (KS + \u03b1S) ij + \u03b22. (21) This algorithm converges very fast. The calculation algorithm for eq. (18) is very simple and can be implemented efficiently."}, {"heading": "4.2. Convergence of Updating rule of Eq.(21)", "text": "We have theorem (2) to prove the convergence of the algorithm if K. is not negative. Theorem 2. Updating S using the rule of equation (21) monotonously decreases the objective function of equation (18). The proof of this theorem is tedious and similar to that in Ding et al., 2010; Kong et al., 2011."}, {"heading": "4.3. Correctness of Updating Rule of Eq.(21)", "text": "We prove that the convergent solution fulfills the Karush-Kuhn-Tucker condition of the limited optimization theory.Theorem 3: In the case of convergence, the convergent solution S of the update rule of Eq. (21) fulfills the KKT condition of the optimization theory.Proof: The KKT condition for S with constraints Sij \u2265 0 is XI J (S) \u2202 Sij = 0, 2 (i), j. The derivative of J (S) (Eq.18) is XI J (S) \u2202 Sij = (\u2212 2K + 2KS + 2\u03b1S + \u03b2E) ij. Thus, the KKT condition for S (\u2212 2K + 2KS + \u03b2E) ij Sij = 0 (i, j) j. (22) On the other hand, the convergence solution S \u2212 KT corresponds to the update rule of Eq. (21) of the convergence solution S (S) \u03b1j (S)."}, {"heading": "5. Experiments", "text": "We evaluate the proposed iterative LLE learning algorithm (\u00a7 3) and the sparse similarity learning algorithm (\u00a7 4) and then show the embedding results from our approach. These datasets come from a wide range of domains, including three face datasets AT & T, umist and yale (Georghiades et al., 2001), two-digit datasets mnist (Lecun et al., 1998) and binalpha 1, two image datasets Caltec101 (Caltec) (Dueck & Frey, 2007) and MSRC (Lee & Grauman et al., 2001), two-digit datasets mnist (Lecun et al., 1998) and two image datasets Lecun et al. (Lecun et al., 1998), two image datasets Caltec101 (Caltec) (Dueck & Frey, 2007) and MSRC (Lee & Grauman et al., 2009), two text datasets and two text datasets."}, {"heading": "5.1. Clustering Results", "text": "We compare three standard cluster algorithms: (1) normalized section, which in the context of our iterative LLE simply means K clusters on learned embedding Y; (2) spectral clustering (Ng et al., 2001), which means K clusters on embedding Y on unit sphere. (3) symmetrical NMF, which performs the learned W in iterative LLE. All results are the averages of 10 K averages clusters with random starts. We use accuracy, normalized mutual information (NMI) and purity as a measurement of cluster qualities and the results are shown in Table 2. We show the cluster results obtained by using (1) the original / input core (K0), (2) LLE1: results on learned Y iteration."}, {"heading": "5.2. Semi-supervised learning results", "text": "We use the results of K0, LLE1 and LLE4 (learned W) as input to perform three semi-monitored methods: harmonic function (Zhu et al., 2003), local and global consistency (Zhou et al., 2004), green function (Ding et al., 2007). We compare the classification accuracy of the above three methods using the original kernel (K0) and the results of LLE1 and LLE4 on 9 data sets. For all methods and data sets, we randomly select 10%, 20% of the marked data for each class and use the rest as unmarked data. We perform 10-fold and 5-fold cross-validation respectively. Finally, we report the average of semi-monitored classification accuracy in Table 3. In all cases, we achieve higher classification accuracy by applying iterative LLE learning algorithms (represented as LLE4 and LLE1)."}, {"heading": "5.3. Demonstration of embedding results", "text": "We demonstrate the advantages of the iterative LLE learning algorithm (\u00a7 3) and the sparse similarity learning algorithm (\u00a7 4) by means of two-dimensional visualization. We randomly select four digits from the MNIST dataset (\"0,\" \"3,\" \"6,\" \"9\"), and the other parameters are specified as mentioned above. Embedding results from the original Gaussian Kernel K0, the 4-pass iterative LLE learning algorithm (LLE4) and the 1-pass of the W learning algorithm (LLE1) are shown in the figures. (1 (a), 1 (c), 1 (b). In the original LLE4 comparison, all images from different groups increase together."}, {"heading": "6. Conclusion", "text": "In summary, the main contribution of our work is threefold. (1) We show that an improved ylearning formulation of LLE is identical to normalized cross-sectional clustering. (2) We present an improved W-learning algorithm that learns a non-negative, sparse pair-by-pair similarity from an input kernel function. (3) To gradually refine / improve the solution, an iterative process of the above two steps is proposed. Experiments show that the iterative inclusion of LLE (1,2,3) leads to better clusters and semi-monitored learning outcomes. Recognition. this work is partially supported by NSF-CCF-0939187, NSF-CCF-0917274, NSF-DMS-0915228."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": null, "citeRegEx": "Belkin and Niyogi,? \\Q2001\\E", "shortCiteRegEx": "Belkin and Niyogi", "year": 2001}, {"title": "Spectral k-way ratio-cut partitioning and clustering", "author": ["P.K. Chan", "M.Schlag", "J.Y. Zien"], "venue": "IEEE Trans. CADIntegrated Circuits and Systems,", "citeRegEx": "Chan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Chan et al\\.", "year": 1994}, {"title": "K-means clustering and principal component analysis", "author": ["C. Ding", "X. He"], "venue": "Int\u2019l Conf. Machine Learning (ICML),", "citeRegEx": "Ding and He,? \\Q2004\\E", "shortCiteRegEx": "Ding and He", "year": 2004}, {"title": "A learning framework using green\u2019s function and kernel regularization with application to recommender system", "author": ["C. Ding", "R. Jin", "T. Li", "H.D. Simon"], "venue": "In KDD, pp", "citeRegEx": "Ding et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2007}, {"title": "Convex and seminonnegative matrix factorizations", "author": ["C. Ding", "T. Li", "M.I. Jordan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ding et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2010}, {"title": "Hessian eigenmaps: New locally linear embedding techniques for high-dimensional data", "author": ["D.L. Donoho", "C. Grimes"], "venue": "Proceedings of National Academy of Science,", "citeRegEx": "Donoho and Grimes,? \\Q2003\\E", "shortCiteRegEx": "Donoho and Grimes", "year": 2003}, {"title": "Non-metric affinity propagation for unsupervised image categorization", "author": ["D. Dueck", "B.J. Frey"], "venue": "In ICCV, pp. 1\u2013", "citeRegEx": "Dueck and Frey,? \\Q2007\\E", "shortCiteRegEx": "Dueck and Frey", "year": 2007}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "R-dimensional quadratic placement algorithm", "author": ["K.M. Hall"], "venue": "Management Science,", "citeRegEx": "Hall,? \\Q1971\\E", "shortCiteRegEx": "Hall", "year": 1971}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["J. Ham", "D.D. Lee", "S. Mika", "B. Scholkopf"], "venue": null, "citeRegEx": "Ham et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2004}, {"title": "Robust nonnegative matrix factorization using l21-norm", "author": ["D. Kong", "C.H.Q. Ding", "H. Huang"], "venue": "In CIKM, pp", "citeRegEx": "Kong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Foreground focus: Unsupervised learning from partially matching images", "author": ["Y.J. Lee", "K. Grauman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lee and Grauman,? \\Q2009\\E", "shortCiteRegEx": "Lee and Grauman", "year": 2009}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Ng", "Andrew Y", "Jordan", "Michael I", "Weiss", "Yair"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": null, "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik,? \\Q1997\\E", "shortCiteRegEx": "Shi and Malik", "year": 1997}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Label propagation through linear neighborhoods", "author": ["Wang", "Fei", "Zhang", "Changshui"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Spectral relaxation for K-means clustering", "author": ["H. Zha", "C. Ding", "M. Gu", "X. He", "H.D. Simon"], "venue": null, "citeRegEx": "Zha et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zha et al\\.", "year": 2001}, {"title": "Principal manifolds and nonlinear dimensionality reduction via tangent space alignment", "author": ["Z. Zhang", "Z. Zha"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "Zhang and Zha,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Zha", "year": 2004}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Schlkopf"], "venue": "In NIPS,", "citeRegEx": "Zhou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "Proc. Int\u2019l Conf. Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 16, "context": "Recently, there have been many algorithms proposed for nonlinear dimension reduction, which include Isomap (Tenenbaum et al., 2000), locally linear embedding (LLE) (Roweis & Saul, 2000), kernel-LLE (Ham et al.", "startOffset": 107, "endOffset": 131}, {"referenceID": 9, "context": ", 2000), locally linear embedding (LLE) (Roweis & Saul, 2000), kernel-LLE (Ham et al., 2004), Hessian LLE (Donoho & Grimes, 2003), local tangent alignment (Zhang & Zha, 2004), Laplacian embedding (Hall, 1971; Belkin & Niyogi, 2001), and many variations.", "startOffset": 74, "endOffset": 92}, {"referenceID": 8, "context": ", 2004), Hessian LLE (Donoho & Grimes, 2003), local tangent alignment (Zhang & Zha, 2004), Laplacian embedding (Hall, 1971; Belkin & Niyogi, 2001), and many variations.", "startOffset": 111, "endOffset": 146}, {"referenceID": 13, "context": "In past decades, many clustering algorithms have been proposed such as K-means, spectral clustering and its variants (Ng et al., 2001), normalized cut (Shi & Malik, 1997), ratio cut (Chan et al.", "startOffset": 117, "endOffset": 134}, {"referenceID": 1, "context": ", 2001), normalized cut (Shi & Malik, 1997), ratio cut (Chan et al., 1994), etc.", "startOffset": 55, "endOffset": 74}, {"referenceID": 1, "context": "Similarly, Laplacian embedding using coordinates with standard normalization VV = Ik is precisely the continuous relaxation of the cluster indicators of multi-way ratio cut spectral clustering (Chan et al., 1994); The widely used linear embedding, Principal component analysis (PCA) is precisely the continuous relaxation of the cluster indicators of the multi-way K-means clustering (Zha et al.", "startOffset": 193, "endOffset": 212}, {"referenceID": 18, "context": ", 1994); The widely used linear embedding, Principal component analysis (PCA) is precisely the continuous relaxation of the cluster indicators of the multi-way K-means clustering (Zha et al., 2001; Ding & He, 2004).", "startOffset": 179, "endOffset": 214}, {"referenceID": 4, "context": "The proof of this theorem is lengthy and is similar to that in (Ding et al., 2010; Kong et al., 2011).", "startOffset": 63, "endOffset": 101}, {"referenceID": 10, "context": "The proof of this theorem is lengthy and is similar to that in (Ding et al., 2010; Kong et al., 2011).", "startOffset": 63, "endOffset": 101}, {"referenceID": 7, "context": "Dataset These data sets come from a wide range of domains, including three face datasets AT&T, umist and yale (Georghiades et al., 2001), two digit datasets mnist (Lecun et al.", "startOffset": 110, "endOffset": 136}, {"referenceID": 11, "context": ", 2001), two digit datasets mnist (Lecun et al., 1998) and binalpha , two image scene datasets Caltec101(Caltec) (Dueck & Frey, 2007) and MSRC (Lee & Grauman, 2009), and two text datasets Newsgroup, Reuters.", "startOffset": 34, "endOffset": 54}, {"referenceID": 13, "context": "We compare three standard clustering algorithms: (1) normalized cut, which in the context of our iterative LLE, is simply K-means clustering on learned embeddingY; (2) spectral clustering (Ng et al., 2001), which is K-means clustering on embedding Y normalized onto unit sphere.", "startOffset": 188, "endOffset": 205}, {"referenceID": 21, "context": "Semi-supervised learning results We use K, LLE1 and LLE4 results (learned W) as the input to run three semi-supervised methods: harmonic function(Zhu et al., 2003), local and global consistency(Zhou et al.", "startOffset": 145, "endOffset": 163}, {"referenceID": 20, "context": ", 2003), local and global consistency(Zhou et al., 2004), green\u2019s function(Ding et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 3, "context": ", 2004), green\u2019s function(Ding et al., 2007).", "startOffset": 25, "endOffset": 44}], "year": 2012, "abstractText": "Locally Linear embedding (LLE) is a popular dimension reduction method. In this paper, we systematically improve the two main steps of LLE: (A) learning the graph weights W, and (B) learning the embedding Y. We propose a sparse nonnegative W learning algorithm. We propose a weighted formulation for learning Y and show the results are identical to normalized cuts spectral clustering. We further propose to iterate the two steps in LLE repeatedly to improve the results. Extensive experiment results show that iterative LLE algorithm significantly improves both classification and clustering results.", "creator": "LaTeX with hyperref package"}}}