{"id": "1206.6464", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Estimating the Hessian by Back-propagating Curvature", "abstract": "In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show in experiments that CP turns out to work well in practice, giving very accurate estimates of the Hessian of neural networks, for example, with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (748kb)", "http://arxiv.org/abs/1206.6464v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Tue, 4 Sep 2012 18:32:03 GMT  (703kb)", "http://arxiv.org/abs/1206.6464v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james martens", "ilya sutskever", "kevin swersky"], "accepted": true, "id": "1206.6464"}, "pdf": {"name": "1206.6464.pdf", "metadata": {"source": "META", "title": "Estimating the Hessian by Back-propagating Curvature", "authors": ["James Martens", "Ilya Sutskever", "Kevin Swersky"], "emails": ["JMARTENS@CS.TORONTO.EDU", "ILYA@CS.UTORONTO.CA", "KSWERSKY@CS.TORONTO.EDU"], "sections": [{"heading": "1. Introduction", "text": "There are many models and learning algorithms where it becomes necessary, or at least is very useful, to calculate entries of Hessian from some complicated functions. For functions that can be calculated with a arithmetic diagram, there are automatic methods for calculating Hessian vector products accurately (e.g. Pearlmutter, 1994) which can be used to recover certain columns of Hessian, but they are inefficient in restoring other parts of the matrix, such as large blocks, or the diagonals. For the diagonals of the Hessian network, there are deterministic approaches such as those of Becker and Le Cun, but these are not guaranteed to be exact."}, {"heading": "2. Derivation of CP", "text": "In the following section, we will develop the Curvature Propagation (CP) method for functions defined on the basis of general calculation diagrams. We will present a version of the approach based on the use of complex arithmetic, and later also give a version that uses only real arithmetic. At a high level, we will define complex vector-weighted linear functions on the basis of the calculation diagram of our target function f, and then, using a series of lemmats, show that the expectation of the self-expressed product of this function is actually the Hessian matrix. This function can be calculated by modifying automatic differentiation in reverse mode, in which noise is injected at each node."}, {"heading": "2.1. Setting and notation", "text": "We will assume that f can be calculated using a mathematical graph that consists of a series of nodes N = {i: 1 \u2264 i \u2264 L} and directed edges E = (i, j): i, j \u00b2 N, where on each node i there is a vector calculated via yi = fi (xi) for a double differentiable function fi. We will identify node 1 as input to node i, and it will be concatenated by concatenating vectors yk for k-Pi and Pi = {k: k is a parent of i} = (k, i). We will identify node 1 as input or \"source\" node i, and by concatenating vectors yk for k-Pi and Pi-Pi as output or \"sink\" node x, with yL = f (y1)."}, {"heading": "2.2. Computing gradients and Hessians", "text": "JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > k, i (2) JfxkR > i, i (2) JfxkR > i, i (2) JfxkR > i, i (2) JfxkR > i, i (2) Jfxi > i, i (2) Jfi, ki (xi) > i, i (2) > ki, xi, ki (xi) > i, i (2) >, ki (xi) >, i (3) JfxkJ, 2 (2) > ki, ki (xi) > i, xi, ki > i > i, i (2)"}, {"heading": "2.3. The S function", "text": "We now define an efficiently computable function S, which enables us to obtain a Ranking 1 estimate of the Hessian. Its argument consists of an ordered list of vectors V \u2261 {vi} Li = 1, where vi-R'i, and its output is a n-dimensional vector (which may be complex). It is defined as S (V) \u2261 Sy1 (V), where Syi (V) \u0109Cni and Sxi (V) \u30fb Cmi are vector-weighted functions of V, which are defined recursively using the equations SyL (V) = 0 (12) Syi (V) = \u2211 k \u0445 Ci R > k, iSxk (V) (13) Sxi (V) = F > i vi + J yi xi > Syi (V) (14), where each Fi is a (not necessarily square) complex weighted matrix in C'i \u00d7 mi satisfactory compilation level of F > Mi."}, {"heading": "2.4. Properties of the S function with stochastic inputs", "text": "Suppose the random variable V is satisfactory: \u0395i E [viv > i] = I and \u0394j 6 = i, E [viv > j] = 0 (15) For example, each vi could be taken from a multivariate normality with mean 0 and covariance matrix I. We will now provide a result that supports the usefulness of S (V) as a tool for approximating H. The proof for this and other theorems will be found in the Appendix / Addendum. Theorem 2.1. S (V) S (V) > is an unbiased estimator of H (\u2261 Hfy1, y1) In addition to its impartiality, the estimator S (V) S (V) > will be symmetrical and possibly complex. To achieve a real estimate, we can instead use only the real component of S (V) S (V) >, which is itself an impartial estimator of Hf1, since the imaginary part of S (V) is in zero."}, {"heading": "2.5. Avoiding complex numbers", "text": "The factorization of the Mi's and the resulting complex arithmetic associated with the use of these factors can be avoided if we redefine V so that each vi is of dimension mi (instead of \"i), and we define the real vector-weighted functions T (V) \u2261 Ty1 (V) and U (V) \u2261 Uy1 (V) according to the following recursions: TyL (V) = 0 UyL (V) = 0 Tyi (V) = 0 k (V).Ci Jxkyi > Txk (V) Uyi (V).K (Ci Jxkyi > Uxk (V) Txi (V) = Mivi + J yi xi > Tyi (V) Uxi (V) = vi + J yi > Uyi (V) > Uyi (V).Both recursions for T and U are trivial modifications of those given for S (V), the only difference being bmatized is the matrix."}, {"heading": "2.6. Matrix interpretation of S, T and U", "text": "Suppose we represent V as a large vector v \u2261 [v > 1.. v > L] > with the dimension m Looking imi. Then the functions S, T and U in the vi's are linear (a fact resulting from the recursive definitions of these functions) and therefore v. Thus, S, T and U have a corresponding representation as matrices S-V-Cn-m, T-V-Rn-m and U-Rn-m w.r.t. The coordinate bases given by v-V. Then we find that S (V) S (V) S (V) S > S-S-V and this condition (15) E [vv >] = I corresponds, we get Hfy1, y1 = E [S-vv > S] = S-E [vv >] S-vv > fy1, fHy1 and thus we can see that S-S is an interpretation as a \"factor\" of Hf1, 1, 1, 1, 1, and 1."}, {"heading": "3. A simpler method?", "text": "At the cost of approximately two runs of the arithmetic chart, it is possible to calculate the Hessian vector Hw for any vector w-Rn (e.g. Pearlmutter, 1994), which suggests the following simple approach to calculating an unbiased ranking of H: Draw w w w from a distribution that satisfies E [ww >] = I and then take the outer product of Hw with you. It is easy to see that this is unbiased, sincere E [HwwT] = H E [wwT] = H (16) Mathematically, this estimator is as expensive as CP, but since there are several existing methods for calculating Hessian vector products, it may be easier to implement. However, in the next section we will prove that the CP estimator will have a much smaller variance in most situations, and confirm these results experimentally later."}, {"heading": "4. Covariance analysis", "text": "Let us AB > an arbitrary matrix factoring of H, with A, B, Cn, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B,"}, {"heading": "5. Practical aspects", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Computing and factoring the Mi\u2019s", "text": "The calculation of the matrices Mi for each node i is necessary to calculate the S, T and U functions, and for S we must also be able to factor them in. Fortunately, each Mi can be calculated directly after eqn.7 As long as the operations performed on nodes i are simple enough. And each Hyi, qxi, xi is fully determined by the local function fi, which is calculated at nodes i. The Jacobic term Jfyi, q = [Jfyi] q, which appears in the formula for Mi, is only a scalar, and is the derivative of f w.r.t. yi This can be made cheap and readily available by performing the calculation of S (V), the standard automatic differentiation for the calculation of f w.r.t."}, {"heading": "5.2. Increasing the rank", "text": "Fortunately, scanning and calculating S (V) for multiple V's is trivially parallelizable, and it can be easily implemented in vectorized code for k samples by taking the defining recursions for S (equivalents 12, 13, and 14), and redefining Syi (V) and Sxi (V) as matrix functions (with k columns), and vi as a mi x k matrix of column vectors derived from independent drawings from the usual distribution for vi.In the case where f is a sum of B similarly structured terms that frequently occurs in machine learning, such as when f is the sum of regression errors or protocol probability concepts over a collection of training cases, one can apply CP individually to each term in the sum, at almost no additional cost as a mere application to f, obtaining a ranking estimate from f instead of a ranking estimate."}, {"heading": "5.3. Curvature Propagation for Diagonal Hessian Estimation in Feedforward Neural Networks", "text": "In this section we will apply CP to the specific example of calculating an unbiased estimate diag (H) b) of the diagonal of H (diag (H)) of a forward-facing neural network with \"layers.\" The pseudo code below calculates the target f of our neural network for a series of B cases1: Input: z1, a matrix of inputs (with B columns, one per case) 2: for all i from 1 to \"\u2212 1 Szi + 1 \u2190 Wizi 4: zi + 1 \u2190 g (ui + 1) 5: end for 6: f \u2190 B b = 1 Lb (z,\" b) / B 7: Output: fHere g (x) is a coordinatively smart nonlinearity, zi are matrices containing the outputs of neural units at layer i, and similarly the matrices ui for all cases."}, {"heading": "6. Hardness of exact computation", "text": "rE \"s rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\" rE \"s rf\u00fc ide for the green for the green for the green for the green"}, {"heading": "7. Related work", "text": "The simplest method for calculating the inputs of the Hessian, including the diagonals, is to use an algorithm for Hessian vector multiplication and to traverse the vectors ei for i = 1... n, restoring each column of H in turn. Unfortunately, this method is too expensive in most situations and in the example function f used in Section 6 would require O (n3) time. Chapelle's and Erhan's method (2011) can be regarded as a special case of the CP, in which all Mi except the Mi, which is associated with final nonlinearity, are set to zero. Therefore, all the results proven in this paper apply to this approach as well, but with the Hessian one replaced by the Gauss-Newton matrix. Becker and Le Cun (1988) gave an approach for approximating the diagonals of the Hessian of a neural network formation goal using a deterministic algorithm, i.e., all the neessian passes made by the neessial tree are adjusted to the 1992 (these are the large ones)."}, {"heading": "8. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1. Accuracy Evaluation", "text": "In this section, we test the accuracy of CP using a small neural network, as we vary the number of samples. The network consists of 3 hidden layers of 20 units each; the input and output layers are of the size 256 and 10 respectively, yielding a total of 6190 parameters; we tested both a network with random weights determined by Gaussian noise with a variance of 0.01; and a network trained to classify handwritten digits from the USPS dataset 2. For the random vectors, we tested both Gaussian and {\u2212 1, 1} Bernoulli noise with the CP estimators, which are based on the use of S and T / U, and the simpler estimator discussed in Section 3 on the use of H / I. To make a comparison, we also tested the deterministic method of (Becker and Le Cun, 1988) Bernoulli noise set using the CS-1000 dataset and tested the S-1000 dataset."}, {"heading": "8.2. Score-Matching Experiments", "text": "To achieve the effectiveness of belts, we have to stick to the limits of possibilities, \"he said.\" We have to stick to the rules, \"he said.\" We have to stick to the rules, \"he said.\" We have to stick to the rules, \"he said.\" We have to stick to the rules, \"he said.\" We have to stick to the rules, \"he said.\" We have to stick to the rules, \"he said.\" We have to stick to the rules, \"he said.\" We have to stick to the rules. \""}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Olivier Chapelle for his helpful conversations."}], "references": [{"title": "Improving the convergence of backpropagation learning with second order methods", "author": ["S. Becker", "Y. Le Cun"], "venue": "In Proceedings of the 1988 connectionist models summer school,", "citeRegEx": "Becker and Cun.,? \\Q1988\\E", "shortCiteRegEx": "Becker and Cun.", "year": 1988}, {"title": "Exact calculation of the hessian matrix for the multilayer perceptron", "author": ["C. Bishop"], "venue": "Neural Computation,", "citeRegEx": "Bishop.,? \\Q1992\\E", "shortCiteRegEx": "Bishop.", "year": 1992}, {"title": "Improved preconditioner for hessian free optimization", "author": ["O. Chapelle", "D. Erhan"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Chapelle and Erhan.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Erhan.", "year": 2011}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["A. Hyvarinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyvarinen.,? \\Q2006\\E", "shortCiteRegEx": "Hyvarinen.", "year": 2006}, {"title": "A two-layer ica-like model estimated by score matching", "author": ["U. K\u00f6ster", "A. Hyv\u00e4rinen"], "venue": "Artificial Neural Networks\u2013ICANN", "citeRegEx": "K\u00f6ster and Hyv\u00e4rinen.,? \\Q2007\\E", "shortCiteRegEx": "K\u00f6ster and Hyv\u00e4rinen.", "year": 2007}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation,", "citeRegEx": "Pearlmutter.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter.", "year": 1994}, {"title": "Factored 3-way restricted boltzmann machines for modeling natural images", "author": ["M. Ranzato", "A. Krizhevsky", "G.E. Hinton"], "venue": "In Proc. Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ranzato et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2010}, {"title": "Lower bounds for matrix product, in bounded depth circuits with arbitrary gates", "author": ["R. Raz", "A. Shpilka"], "venue": "Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Raz and Shpilka.,? \\Q2001\\E", "shortCiteRegEx": "Raz and Shpilka.", "year": 2001}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "In Proceedings of the ECML/PKDD", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Learning representations by back-propagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "B.M. Marlin", "N. de Freitas"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "The complexity of partial derivatives", "author": ["B. Walter", "V. Strassen"], "venue": "Theoretical Computer Science,", "citeRegEx": "Walter and Strassen.,? \\Q1983\\E", "shortCiteRegEx": "Walter and Strassen.", "year": 1983}], "referenceMentions": [{"referenceID": 6, "context": "Pearlmutter, 1994). These can be used to recover specific columns of the Hessian, but are inefficient at recovering other parts of the matrix such as large blocks, or the diagonal. For the diagonal of the Hessian of a neural network training objective, there are deterministic approximations available such as that of Becker and Le Cun (1988), but these are not guaranteed to be accurate.", "startOffset": 0, "endOffset": 343}, {"referenceID": 2, "context": "Recently Chapelle and Erhan (2011) showed how to compute an unbiased estimate of the diagonal of the Gauss-", "startOffset": 9, "endOffset": 35}, {"referenceID": 5, "context": "Newton matrix, and used this to perform preconditioning within a Hessian-free Newton optimization algorithm (Martens, 2010).", "startOffset": 108, "endOffset": 123}, {"referenceID": 2, "context": "As with the algorithm of Chapelle and Erhan (2011), CP involves reverse sweeps of the computational graph of the function, which can be repeated to obtain higher-rank estimates of arbitrary accuracy.", "startOffset": 25, "endOffset": 51}, {"referenceID": 3, "context": "Score Matching (Hyvarinen, 2006), a method for parameter estimation in Markov Random Fields, uses the diagonal of the Hessian within its objective, making it expensive to apply to all but the simplest models.", "startOffset": 15, "endOffset": 32}, {"referenceID": 2, "context": "The diagonal of the Hessian can be used as a preconditioner for first and second order nonlinear optimizers, which is the motivating application of Becker and Le Cun (1988) and Chapelle and Erhan (2011). Score Matching (Hyvarinen, 2006), a method for parameter estimation in Markov Random Fields, uses the diagonal of the Hessian within its objective, making it expensive to apply to all but the simplest models.", "startOffset": 177, "endOffset": 203}, {"referenceID": 10, "context": "also known as back-propagation (Rumelhart et al., 1986) in the context of neural networks children.", "startOffset": 31, "endOffset": 55}, {"referenceID": 9, "context": "It is also worth noting that this estimator underlies the Hessian norm estimation technique used in Rifai et al. (2011). That this is true is due to the equivalence between the stochastic finite-difference formulation used in that work and matrix-vector products with randomly drawn vectors.", "startOffset": 100, "endOffset": 120}, {"referenceID": 8, "context": "To do this we will reduce the problem of multiplying two matrices to that of computing (exactly) the diagonal of the Hessian of a certain function f , and then appeal to a hardness due to Raz and Shpilka (2001) which shows that matrix multiplication will require asymptotically more computation than CP does when it is applied to f .", "startOffset": 188, "endOffset": 211}, {"referenceID": 8, "context": "The basic idea of the proof is to use the existence of such a circuit family to construct a family of circuits with bounded depth and edge count O(n), that can multiply arbitrary n \u00d7 n matrices (which will turn out to be the matrices P andQ that parameterize f ), contradicting a theorem of Raz and Shpilka (2001) which shows that any such circuit fam-", "startOffset": 291, "endOffset": 314}, {"referenceID": 2, "context": "The method of Chapelle and Erhan (2011) can be viewed as a special case of CP, where all the Mi\u2019s except for the Mi associated with the final nonlinearity are set to zero.", "startOffset": 14, "endOffset": 40}, {"referenceID": 1, "context": "In Bishop (1992), a method for computing entries of the Hessian of a feedforward neural network was derived.", "startOffset": 3, "endOffset": 17}, {"referenceID": 4, "context": "Score matching is a simple alternative to maximum likelihood that has been widely used to train energy-based models (K\u00f6ster and Hyv\u00e4rinen, 2007; Swersky et al., 2011).", "startOffset": 116, "endOffset": 166}, {"referenceID": 11, "context": "Score matching is a simple alternative to maximum likelihood that has been widely used to train energy-based models (K\u00f6ster and Hyv\u00e4rinen, 2007; Swersky et al., 2011).", "startOffset": 116, "endOffset": 166}, {"referenceID": 7, "context": "Our specific test involves learning the parameters of a covariance-restricted Boltzmann machine (cRBM; Ranzato et al., 2010).", "startOffset": 96, "endOffset": 124}, {"referenceID": 7, "context": "Our specific test involves learning the parameters of a covariance-restricted Boltzmann machine (cRBM; Ranzato et al., 2010). This can be seen as a two-layer network where the first layer uses the squared activation function followed by a second layer that uses the softplus activation function: log(1+exp(x)). The details of applying score matching to this model can be found in Swersky et al. (2011).", "startOffset": 103, "endOffset": 402}, {"referenceID": 7, "context": "Our setup is identical to Ranzato et al. (2010). In particular, our cRBM contained 256 factors and hidden units.", "startOffset": 26, "endOffset": 48}], "year": 2012, "abstractText": "In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show in experiments that CP turns out to work well in practice, giving very accurate estimates of the Hessian of neural networks, for example, with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models.", "creator": "LaTeX with hyperref package"}}}