{"id": "1206.6467", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Semi-Supervised Collective Classification via Hybrid Label Regularization", "abstract": "Many classification problems involve data instances that are interlinked with each other, such as webpages connected by hyperlinks. Techniques for \"collective classification\" (CC) often increase accuracy for such data graphs, but usually require a fully-labeled training graph. In contrast, we examine how to improve the semi-supervised learning of CC models when given only a sparsely-labeled graph, a common situation. We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non-relational features. We also extend the ideas of \"label regularization\" to such hybrid classifiers, enabling them to leverage the unlabeled data to bias the learning process. We find that these techniques, which are efficient and easy to implement, significantly increase accuracy on three real datasets. In addition, our results explain conflicting findings from prior related studies.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (376kb)", "http://arxiv.org/abs/1206.6467v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["luke k mcdowell", "david w aha"], "accepted": true, "id": "1206.6467"}, "pdf": {"name": "1206.6467.pdf", "metadata": {"source": "META", "title": "Semi-Supervised Collective Classification via Hybrid Label Regularization", "authors": ["Luke K. McDowell", "David W. Aha"], "emails": ["lmcdowel@usna.edu", "david.aha@nrl.navy.mil"], "sections": [{"heading": "1. Introduction", "text": "Most work with CCs results in very poor performance (Shi et al., 2011). In response, a few researchers recently examined a fully labeled education curve. However, acquiring such labels can be very difficult, and learning a classifier with only a few such labels can result in very poor performance (Shi et al., 2011). In response, some researchers recently published in the Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author (s) / owner (s). The CC task, where a classifier must be learned from a partially labeled education graph, using some forms of semi-supervised learning (SSL) to use the unlabeled part of the graph. However, they have contradictory or weak results, even when using the same data sets and similar algorithms."}, {"heading": "2. Background and Related Work", "text": "Suppose we get a graph G = (V, E, XA, Y, C) in which V is a set of nodes, E is a set of edges, each xA-XA is an attribute vector for a node vi-V, each Yi-Y is a label variable for vi and C is the set of possible labels. We also get a set of \"known\" values Y-K for the nodes V K-V, so that Y K = {yi | vi-V K}. Conventional approaches then ignore the link relations and classify each page using the attributes xA derived from its content (e.g. words in the page). In contrast, methods for collective classification (Jensen et al., 2004) can be explicitly based on page Y."}, {"heading": "2.1. Semi-supervised Collective Classification", "text": "This year it is more than ever before."}, {"heading": "2.2. Other Approaches to Semi-supervised CC", "text": "Two other relevant studies are Taskar et al. (2001) and Chu et al. (2006), which, however, do not handle cyclic graphs or are at least square in the number of nodes (N) and therefore do not scale to large, realistic graphs. In N, the methods we use are only linear (assuming realistic link densities), even with our improvements in Sections 3 and 4. Others have explored how to perform CC within the network without having to learn an explicit model for link-based characteristics (the challenge for ICA discussed in Section 2.1). Thus, Tang & Liu (2009) have used the links to create latent characteristics that allow classification of nodes without a collective conclusion. Shi et al. (2011) propose mark propagation based on derived latent links. Some authors have suggested \"only relational\" methods that do not perform learning, but take some mark propagation or precipitate Macassy steps to classify nodes (e.g.)."}, {"heading": "3. Hybrid Classifiers for CC", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "4. Adding Label Regularization", "text": "Labeling regulation (Mann & McCallum, 2007) is designed to make SSL more robust by encouraging a learned LR classifier (or another exponential family model) to make probability estimates based on the unlabeled data so that the resulting class distribution resembles an expected distribution. More specifically, let p (y) be the expected label distribution that can be calculated from the training data. Let p (y) be the empirical distribution calculated over the unlabeled portion of the training data, taking into account current parameter settings, such as p (y) the expected label distribution (y) = 1 (XU)."}, {"heading": "5. Experimental Study", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets and Features", "text": "Previous studies used a maximum of two non-synthetic datasets with semi-monitored ICA; we used the unification of their datasets (see Tables 1 & 2. We removed all nodes without linkages, but we did not (like some others) use only the largest connected component of the graphs. Cora (see Sen et al. 2008) is a collection of machine learning papers. Citeseer (see Sen et al. (2010) is a collection of research originating from the Citeseer collection. For both datasets, the attributes represent the presence or absence of certain words, and citations represent linkages between the documents. We ignored the linking direction, as with Bilgic et al. (2010). They also report substantially higher accuracy using the main component analysis (PCA) to reduce the dimensionality of the attributes, so that we provide a stronger baseline that we use as binding characteristics, and the 100-by-by-by-application of the top attributes."}, {"heading": "5.2. Node Classifier and Regularization", "text": "For the node classifier MAR, we evaluate five options: LR is a single (non-hybrid) classifier that uses logistic regression; LR + LR is a hybrid classifier that uses two LR classifiers, while LR + LR + Reg adds a label regularization; LR + NB is a hybrid classifier that uses LR for the attributes and NB for the relational features, and LR + Reg adds the label regularization; and for the standard regulation (separate from the label regularization), we used a Gaussian precursor with LR parameters, with LR parameters labeled with variance-2 as described below; for NB, we used a discharge before each feature that was selected as selective (separate from the labeling)."}, {"heading": "5.3. Learning Algorithms", "text": "We evaluate four variants of the semi-monitored ICA (see section 2): All-EM, All-OnePass, Known-EM and Known-OnePass. Step 6 of the algorithm performs ICA with 10 iterations each, and the EMvariants use n = 10 iterations of the main loop. We compare against three baselines: The first, NoSSL, is like Known-OnePass in the one-time use of ICA with attributes and relational characteristics, but No-SSL learns the node classifier without using any blank data (i.e. without SSL); the second, AttrOnly, predicts the unknown labels only once, using an attribute-only classifier that ignores blank data during learning; the third, Relat-Only, is a standard relative-only baseline (the wvRN + RL Classifier from Macskassy & Prov2007) that repeatedly evaluates labels based on the three connected labels."}, {"heading": "5.4. Evaluation Procedure", "text": "For each study, we randomly selected a fraction of the nodes (the \"marking density\") as \"known\" nodes V K. The remaining nodes V U have unknown labels and form the test set part of Chart G. We focus on the sparsely labeled case where the density is less than 10%. To assess the significance, we use paired t-tests with a significance level of 5%. However, the test sets in the 15 studies are not fragmented, and therefore a traditional paired t-test can lead to incorrect conclusions. To compensate for this effect, we use the recently described method by Wang et al. (2011), which has been shown to reduce false positives to the expected level, making our results more conservative compared to uncorrected tests."}, {"heading": "5.5. Results", "text": "In fact, most people are able to survive themselves, and they are able to survive themselves, \"he said in an interview with The New York Times,\" I don't think they are able to survive themselves. \"In other words,\" I don't think they will feel able to change the world. \""}, {"heading": "6. Conclusion", "text": "We have generalized the algorithms of several previous studies of semi-monitored ICA, explained their performance trends, and demonstrated that a hybrid classifier with label regulation can significantly increase accuracy compared to alternative approaches. In our data, the LR + NB combination performs best, although not for every data set. However, our hybrid approach allows us to select any combination of probabilistic classifiers based on data characteristics. In addition, our expansion of label regulation can also be used to increase accuracy as long as one of the classifiers belongs to the exponential family (such as LR). Such hybrid classifiers with label regulation can be useful for many other tasks, including cross-network CC or non-relational classification. Our results need to be confirmed by additional data sets, and we will examine alternatives to ICA such as Gibbs sampling and soft labeling methods and the use of hybrid combination rules such as stacking."}, {"heading": "Acknowledgments", "text": "Thanks to Alexandra Olteanu, Li Pu, Majid Yazdani and the anonymous referees for comments that have helped to improve this work. Parts of this analysis used Proximity, an open source software environment from the University of Massachusetts, Amherst. This work was partially supported by NSF prize number 1116439."}], "references": [{"title": "Cost-Sensitive Information Acquisition in Structured Domains", "author": ["M. Bilgic"], "venue": "PhD thesis, Department of Computer Science, University of Maryland at College Park,", "citeRegEx": "Bilgic,? \\Q2010\\E", "shortCiteRegEx": "Bilgic", "year": 2010}, {"title": "Active learning for networked data", "author": ["M. Bilgic", "L. Mihalkova", "L. Getoor"], "venue": "In Proc. of ICML,", "citeRegEx": "Bilgic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bilgic et al\\.", "year": 2010}, {"title": "Relational learning with gaussian processes", "author": ["W. Chu", "V. Sindhwani", "Z. Ghahramani", "S.S. Keerthi"], "venue": "In NIPS, pp", "citeRegEx": "Chu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2006}, {"title": "Across-model collective ensemble classification", "author": ["H. Eldardiry", "J. Neville"], "venue": "In Proc. of AAAI,", "citeRegEx": "Eldardiry and Neville,? \\Q2011\\E", "shortCiteRegEx": "Eldardiry and Neville", "year": 2011}, {"title": "Why collective inference improves relational classification", "author": ["D. Jensen", "J. Neville", "B. Gallagher"], "venue": "In Proc. of KDD, pp", "citeRegEx": "Jensen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2004}, {"title": "Link-based classification using labeled and unlabeled data", "author": ["Q. Lu", "L. Getoor"], "venue": "In ICML Workshop on the Continuum from Labeled to Unlabeled data,", "citeRegEx": "Lu and Getoor,? \\Q2003\\E", "shortCiteRegEx": "Lu and Getoor", "year": 2003}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["S. Macskassy", "F. Provost"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Macskassy and Provost,? \\Q2007\\E", "shortCiteRegEx": "Macskassy and Provost", "year": 2007}, {"title": "Generalized expectation criteria for semi-supervised learning with weakly labeled data", "author": ["G. Mann", "A. McCallum"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mann and McCallum,? \\Q2010\\E", "shortCiteRegEx": "Mann and McCallum", "year": 2010}, {"title": "Simple, robust, scalable semi-supervised learning via expectation regularization", "author": ["G.S. Mann", "A. McCallum"], "venue": "In Proc. of ICML, pp", "citeRegEx": "Mann and McCallum,? \\Q2007\\E", "shortCiteRegEx": "Mann and McCallum", "year": 2007}, {"title": "Cautious collective classification", "author": ["L.K. McDowell", "K.M. Gupta", "D.W. Aha"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "McDowell et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McDowell et al\\.", "year": 2009}, {"title": "Ensembles of relational classifiers", "author": ["C. Preisach", "L. Schmidt-Thieme"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Preisach and Schmidt.Thieme,? \\Q2008\\E", "shortCiteRegEx": "Preisach and Schmidt.Thieme", "year": 2008}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Collective prediction with latent graphs", "author": ["X. Shi", "Y. Li", "P.S. Yu"], "venue": "In Proc. of CIKM, pp", "citeRegEx": "Shi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2011}, {"title": "Relational learning via latent social dimensions", "author": ["L. Tang", "H. Liu"], "venue": "In Proc. of KDD, pp", "citeRegEx": "Tang and Liu,? \\Q2009\\E", "shortCiteRegEx": "Tang and Liu", "year": 2009}, {"title": "Probabilistic classification and clustering in relational data", "author": ["B. Taskar", "E. Segal", "D. Koller"], "venue": "In Proc. of IJCAI, pp", "citeRegEx": "Taskar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2001}, {"title": "Correcting bias in statistical tests for network classifier evaluation", "author": ["T. Wang", "J. Neville", "B. Gallagher", "T. Eliassi-Rad"], "venue": "In Proc. of ECML,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Pseudolikelihood EM for withinnetwork relational learning", "author": ["R. Xiang", "J. Neville"], "venue": "In Proc. of ICDM,", "citeRegEx": "Xiang and Neville,? \\Q2008\\E", "shortCiteRegEx": "Xiang and Neville", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Collective classification (CC) often substantially increases classification accuracy when the class labels of inter-related objects are correlated (Jensen et al., 2004; Sen et al., 2008).", "startOffset": 147, "endOffset": 186}, {"referenceID": 11, "context": "Collective classification (CC) often substantially increases classification accuracy when the class labels of inter-related objects are correlated (Jensen et al., 2004; Sen et al., 2008).", "startOffset": 147, "endOffset": 186}, {"referenceID": 12, "context": "However, acquiring such labels can be very difficult, and learning a classifier with only a few such labels can lead to very poor performance (Shi et al., 2011).", "startOffset": 142, "endOffset": 160}, {"referenceID": 0, "context": "This includes Bilgic et al. (2010), who found moderate gains from SSL, whereas Shi et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 0, "context": "This includes Bilgic et al. (2010), who found moderate gains from SSL, whereas Shi et al. (2011) reported otherwise.", "startOffset": 14, "endOffset": 97}, {"referenceID": 11, "context": "We focus on traditional CC algorithms that learn a relational model of the data and then apply a collective inference algorithm such as the Iterative Classification Algorithm (ICA) or Gibbs sampling (Sen et al., 2008).", "startOffset": 199, "endOffset": 217}, {"referenceID": 12, "context": "2011), then these approaches may need to be entirely replaced with alternatives such as latent feature models (Tang & Liu, 2009) or label propagation (Shi et al., 2011).", "startOffset": 150, "endOffset": 168}, {"referenceID": 10, "context": "per Figure 1 Data used with SSL-ICA Shi et al. (2011) Log.", "startOffset": 36, "endOffset": 54}, {"referenceID": 0, "context": "regression (LR) Hard All-OnePass Citeseer Bilgic et al. (2010) Log.", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "regression (LR) Hard All-OnePass Citeseer Bilgic et al. (2010) Log. regression (LR) Hard Known-OnePass Cora, Citeseer Lu & Getoor (2003) Log.", "startOffset": 42, "endOffset": 137}, {"referenceID": 0, "context": "regression (LR) Hard All-OnePass Citeseer Bilgic et al. (2010) Log. regression (LR) Hard Known-OnePass Cora, Citeseer Lu & Getoor (2003) Log. regression (LR) Hard All-EM Cora, Citeseer Xiang & Neville (2008) RPT Soft Known-EM Gene, synthetic", "startOffset": 42, "endOffset": 208}, {"referenceID": 4, "context": "In contrast, methods for collective classification (Jensen et al., 2004) explicitly use the link structure by constructing additional relational features xR based on the labels of neighboring pages.", "startOffset": 51, "endOffset": 72}, {"referenceID": 11, "context": "This can be done using algorithms such as belief propagation, Gibbs sampling, relaxation labeling, or ICA (Sen et al., 2008).", "startOffset": 106, "endOffset": 124}, {"referenceID": 12, "context": "The simplest approach, taken by Shi et al. (2011), is to learn the classifier MAR using all of the labels, attributes, and relational feature values (step 5).", "startOffset": 32, "endOffset": 50}, {"referenceID": 0, "context": "Bilgic et al. (2010) use the same approach, except that during step 5, they perform learning using only the known nodes V K .", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "First, there are conflicting results for whether semi-supervised ICA improves accuracy, with reports of no improvement (Shi et al., 2011), moderate improvement (Bilgic et al.", "startOffset": 119, "endOffset": 137}, {"referenceID": 1, "context": ", 2011), moderate improvement (Bilgic et al., 2010), or mixed results (Xiang & Neville, 2008).", "startOffset": 30, "endOffset": 51}, {"referenceID": 0, "context": ", 2011), moderate improvement (Bilgic et al., 2010), or mixed results (Xiang & Neville, 2008). Lu & Getoor (2003) report substantial gains, but only consider graphs with at least 20% of the nodes labeled.", "startOffset": 31, "endOffset": 114}, {"referenceID": 0, "context": ", 2011), moderate improvement (Bilgic et al., 2010), or mixed results (Xiang & Neville, 2008). Lu & Getoor (2003) report substantial gains, but only consider graphs with at least 20% of the nodes labeled. Second, there is almost no comparison between the four SSL variants (or even discussion of the choices involved). One exception is Bilgic (2010), which finds Known-OnePass to be superior to All-OnePass on two datasets, but does not investigate why.", "startOffset": 31, "endOffset": 350}, {"referenceID": 13, "context": "Two additional relevant studies are Taskar et al. (2001) and Chu et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "(2001) and Chu et al. (2006). However, these methods cannot handle cyclic graphs or have time complexity at least quadratic in the number of nodes (N), and thus do not scale to large, realistic graphs.", "startOffset": 11, "endOffset": 29}, {"referenceID": 12, "context": "Shi et al. (2011) propose label propagation based on derived latent links.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "However, McDowell et al. (2009) found that \u201cmultiset\u201d relational features usually performed best, but are incompatible with the vectorbased representation of LR.", "startOffset": 9, "endOffset": 32}, {"referenceID": 0, "context": "direction, as with Bilgic et al. (2010). They also report substantially higher accuracies using principal component analysis (PCA) to reduce the dimensionality of the attributes, so to provide a stronger baseline we mimic their setup and use the 100 top attribute features after applying PCA to the entire graph.", "startOffset": 19, "endOffset": 40}, {"referenceID": 4, "context": "Gene (see Jensen et al. 2004) describes the yeast genome at the protein level; links represent protein interactions. We mimic Xiang & Neville (2008) and predict protein localization using four attributes: Phenotype, Class, Essential, and Chromosome.", "startOffset": 10, "endOffset": 149}, {"referenceID": 9, "context": "Previous work found such features to perform best for NB (McDowell et al., 2009).", "startOffset": 57, "endOffset": 80}, {"referenceID": 0, "context": "For relational features, LR classifiers used \u201cproportion\u201d features, which compute the fraction of a node\u2019s neighbors that have label y, as done by Bilgic et al. (2010). NB classifiers instead used \u201cmultiset\u201d features which record the distribution of labels in each node\u2019s neighborhood, then use conditional independence assumptions to update the estimated probabilities based on each such label.", "startOffset": 147, "endOffset": 168}, {"referenceID": 11, "context": "Prior work has usually found LR to be superior to NB for CC (Sen et al., 2008; Bilgic et al., 2010) and therefore we always use LR for (at least) the attribute-based classification.", "startOffset": 60, "endOffset": 99}, {"referenceID": 1, "context": "Prior work has usually found LR to be superior to NB for CC (Sen et al., 2008; Bilgic et al., 2010) and therefore we always use LR for (at least) the attribute-based classification.", "startOffset": 60, "endOffset": 99}, {"referenceID": 9, "context": "For NB, we used a Dirichlet prior on each feature with \u03b1 chosen as described below (McDowell et al., 2009).", "startOffset": 83, "endOffset": 106}, {"referenceID": 15, "context": "To compensate for this effect, we use the recently described methodology of Wang et al. (2011), which was shown to reduce false positives to the expected level.", "startOffset": 76, "endOffset": 95}, {"referenceID": 0, "context": "This setting is closest to that of Bilgic et al. (2010) and Shi et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "This setting is closest to that of Bilgic et al. (2010) and Shi et al. (2011). To save space, we show only Cora; trends are similar with Citeseer and Gene.", "startOffset": 35, "endOffset": 78}, {"referenceID": 0, "context": "First, with a simple LR classifier, Known-OnePass outperforms All-OnePass (and provides reasonable gains over No-SSL) \u2013 consistent with Bilgic et al. (2010). Second, All-OnePass behaves similarly to No-SSL \u2013 the same poor behavior that led Shi et al.", "startOffset": 136, "endOffset": 157}, {"referenceID": 0, "context": "First, with a simple LR classifier, Known-OnePass outperforms All-OnePass (and provides reasonable gains over No-SSL) \u2013 consistent with Bilgic et al. (2010). Second, All-OnePass behaves similarly to No-SSL \u2013 the same poor behavior that led Shi et al. (2011) to reject semi-supervised ICA.", "startOffset": 136, "endOffset": 258}, {"referenceID": 0, "context": "This SSL algorithm worked best with LR and mimics the setup of Bilgic et al. (2010), who reported spending considerable effort to improve their performance.", "startOffset": 63, "endOffset": 84}], "year": 2012, "abstractText": "Many classification problems involve data instances that are interlinked with each other, such as webpages connected by hyperlinks. Techniques for collective classification (CC) often increase accuracy for such data graphs, but usually require a fully-labeled training graph. In contrast, we examine how to improve the semi-supervised learning of CC models when given only a sparsely-labeled graph, a common situation. We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non-relational features. We also extend the ideas of label regularization to such hybrid classifiers, enabling them to leverage the unlabeled data to bias the learning process. We find that these techniques, which are efficient and easy to implement, significantly increase accuracy on three real datasets. In addition, our results explain conflicting findings from prior related studies.", "creator": "LaTeX with hyperref package"}}}