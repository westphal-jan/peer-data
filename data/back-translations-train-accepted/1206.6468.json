{"id": "1206.6468", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Variational Inference in Non-negative Factorial Hidden Markov Models for Efficient Audio Source Separation", "abstract": "The past decade has seen substantial work on the use of non-negative matrix factorization and its probabilistic counterparts for audio source separation. Although able to capture audio spectral structure well, these models neglect the non-stationarity and temporal dynamics that are important properties of audio. The recently proposed non-negative factorial hidden Markov model (N-FHMM) introduces a temporal dimension and improves source separation performance. However, the factorial nature of this model makes the complexity of inference exponential in the number of sound sources. Here, we present a Bayesian variant of the N-FHMM suited to an efficient variational inference algorithm, whose complexity is linear in the number of sound sources. Our algorithm performs comparably to exact inference in the original N-FHMM but is significantly faster. In typical configurations of the N-FHMM, our method achieves around a 30x increase in speed.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (1242kb)", "http://arxiv.org/abs/1206.6468v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.SD stat.ML", "authors": ["gautham j mysore", "maneesh sahani"], "accepted": true, "id": "1206.6468"}, "pdf": {"name": "1206.6468.pdf", "metadata": {"source": "META", "title": "Variational Inference in Non-negative Factorial Hidden Markov Models for Efficient Audio Source Separation", "authors": ["Gautham J. Mysore", "Maneesh Sahani"], "emails": ["gmysore@adobe.com", "maneesh@gatsby.ucl.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "2. Probabilistic Models", "text": "In this section, we first describe the probabilistic model of N-HMM (Mysore et al., 2010) for individual sources, as it forms the basis for N-FHMM. Then, we describe the probabilistic model of the proposed Bayesian variant of N-FHMM. In these models, each time frame of the spectrogram is considered a histogram of \"sound quanta,\" just as a document is considered a histogram of words in theme models (Hofmann, 1999; Lead et al., 2003)."}, {"heading": "2.1. Non-negative Hidden Markov Model", "text": "The random variables D1... T form a Markov chain, and the spectra in each timeframe are independent of these variables. Every possible value of Dt identifies a spectral dictionary. Each dictionary contains a series of dictionary elements (analogous to topics), one of which is selected for each tone quantum by the random variable Zt. Each dictionary element is a normalized vector over frequencies (analogous to a word distribution). The frequency associated with a certain quantum is determined by Ft.The generative process in the timeframe t is as follows: 1. Select the state Dt | Dt \u2212 1 \u0445 Discr [\u03c1 (Dt \u2212 1)] 2. Repeat for each quanta: - Select the dictorial element Zt \u00b2 Discr (Dt) - select the frequency Ft \u00b2 Discr (Dt, Zt] 2."}, {"heading": "2.2. Non-negative Factorial Hidden Markov Model", "text": "The original N-FHMM introduced an independent Markov chain D (s) 1... T for each source s (\u03b2) (and time-dependent mixing weights that would have selected elements from a combined state-dependent dictionary \u03b8t (d (1), d (2))). However, at this point we expand this model in two ways. First, we treat this model as a dirichlet-distributed latent variable rather than a parameter. Separation of the mixture requires an estimate of the state D (s) t is likely to occur, and elements from more than one dictionary may appear in principle. Second, we treat it as a dirichlet-distributed latent variable rather than a parameter. The older N-FHMM formulation uses ML estimates; here we use a variable posterior. Thus, the generative process (Figure 7) in the t timeframe is: 1. Selection states for each source: D (s)."}, {"heading": "3. Variational Inference", "text": "The parameters describing each source N-HMM are learned from isolated training data of this source. Therefore, the aim of the conclusion in N-FHMM is only to resolve the mixture; specifically, to estimate the marginalized posterior distribution of the mix weights P (\u03b8t | f) in each time frame. Once this distribution is found, we can reconstruct the individual sources and therefore perform a source separation.The complete posterior distribution is given by P (Z, \u03b8, D (1), D (2) | f), where \u03b8, D (1) and D (2) represent the source separation.The complete posterior distribution is given by P (Z) in all time frames and Z represents all drawings of Zt in all timeframes. f represents the observed values of Ft in all timeframes. The computational costs for determining the posterior distribution are exponentially in the number of sources MX."}, {"heading": "3.1. Difficulties with Decoupling", "text": "In fact, the latent variables, which are able to identify the individual sources, indeed lead to a new latent variable St in order to indicate the proportions of the quanta drawn at any time, and then generate Z (1) and Z (2) separately. In this parameter, the rear spectrum is P (1), D (2), D (2), D (3), D (3), D (3), D (4), D (4), D (4), D (4), D (4), D (4), D (4), D (5), D (5), D (5), D (5), \"D (5),\" D (5), \"D (5),\" D (5), \"D (5), D (5), D (5),\" D (5), \"D (5), D (5,\" D (5), \"D (5), D (5,\" D (5), D (5), \"D (5,\" D, \"D (5), D (5), D (5,\" D, \"D (5), D (5,\" 5, \"D (5), D (5), D (5,\" D, \"D (5), D (5), D, D (5,\" D (5), \"D (5), D (5,\" 5, \"), D (5, D (5), D, D (5,\"), D (5, D (5, \"), D (5, D, D (5), D, D (5), D (5,\"), D (5, D (5), D (5, \"), D (5), D (5, D (5), D (5,\"), D (5), D (5), D (5), D (5, 5), D (5, D (5), D (5), D (5, D (5), 5, D (5, 5, 5), D (5), D (5), D (5, 5), D (5), D (5), D (5"}, {"heading": "3.2. Proposed Variational Approximation", "text": "In the proposed variant of the N-FHMM, therefore, the link between Markov state and dictionary element is less absolute. Also, we estimate a complete posterior level of mixing ratios, which would have created a similar barrier to the exploration of all possible dictionary elements, rather than engaging in the correct evaluation of the mixing spectrum, while strongly favoring the interaction between Markov state variables and previous explanations, which focus on a single dictionary per source. To develop the variable algorithms for this model, we approach the posterior distribution P (Z), D (1), D (2), F) with the following factored form: (2)."}, {"heading": "4. Source Separation", "text": "We reconstruct the spectrograms of each source by taking linear combinations of the dictionary elements of all the dictionaries of each source based on the estimated mixture weights \u03b1-t, k in each timeframe. This gives us estimates of the separate spectrograms of each source V-t-t. We can simply go back in time with these estimates using the phase of the original mixture. However, there is a common practice of source separation to obtain more sophisticated spectrogram estimates first by applying the following masking strategy: V-lt = Vlt-V-t-t-t-t-t-t-t, where Vlt is the original mixture spectrogram. Therefore, the final estimated spectrogram for each source is V-t-t. We apply this strategy in our experiments."}, {"heading": "5. Experimental Results", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6. Conclusions", "text": "We have proposed a Bayesian variant of the N-FHMM and an efficient variation inference algorithm for the model. The computational complexity of the algorithm is linear in the number of sources and about 30 times faster than the exact conclusion of an empirically optimal configuration of the N-FHMM with comparable accuracy of source separation. Although the variation inferences in the N-FHMM have been demonstrated in the task of source separation, it is a general model for sound mixing and can be used for various other audio tasks such as simultaneous speech recognition of multiple speakers and automatic music transcription."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Factorial hidden Markov models", "author": ["Z. Ghahramani", "M. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Ghahramani and Jordan,? \\Q1997\\E", "shortCiteRegEx": "Ghahramani and Jordan", "year": 1997}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In Proceedings of the 22nd International Conference on Research and Development in Information Retrieval, Berkeley,", "citeRegEx": "Hofmann,? \\Q1999\\E", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Algorithms for nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lee and Seung,? \\Q2001\\E", "shortCiteRegEx": "Lee and Seung", "year": 2001}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Non-negative matrix factorization for polyphonic music transcription", "author": ["P. Smaragdis", "J.C. Brown"], "venue": "In Proceedings of the IEEE Workshop of Applications of Signal Processing to Audio and Acoustics,", "citeRegEx": "Smaragdis and Brown,? \\Q2003\\E", "shortCiteRegEx": "Smaragdis and Brown", "year": 2003}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "C. Fevotte", "R. Gribonval"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Vincent et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2006}, {"title": "Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria", "author": ["T. Virtanen"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Virtanen,? \\Q2007\\E", "shortCiteRegEx": "Virtanen", "year": 2007}], "referenceMentions": [{"referenceID": 8, "context": "NMF and its probabilistic counterparts have been used extensively for audio source separation (Virtanen, 2007; Smaragdis et al., 2007).", "startOffset": 94, "endOffset": 134}, {"referenceID": 2, "context": "In these models, each time frame of the spectrogram is viewed as a histogram of \u201csound quanta\u201d in the same way that a document is viewed as a histogram of words in topic models (Hofmann, 1999; Blei et al., 2003).", "startOffset": 177, "endOffset": 211}, {"referenceID": 0, "context": "In these models, each time frame of the spectrogram is viewed as a histogram of \u201csound quanta\u201d in the same way that a document is viewed as a histogram of words in topic models (Hofmann, 1999; Blei et al., 2003).", "startOffset": 177, "endOffset": 211}, {"referenceID": 0, "context": "Without the temporal dynamics, our formulation is similar to that of latent Dirichlet allocation LDA (Blei et al., 2003).", "startOffset": 101, "endOffset": 120}, {"referenceID": 3, "context": "Variational inference (Jordan et al., 1999) refers to a class of techniques that are used to approximate an intractable posterior distribution with a simpler (typically factorized) distribution.", "startOffset": 22, "endOffset": 43}, {"referenceID": 5, "context": "The variational iterations then update each individual NHMM posterior using the forward\u2013backward algorithm (Rabiner, 1989) while keeping the contribution of the other N-HMM fixed; and then revise the mixing proportions of the sources.", "startOffset": 107, "endOffset": 122}, {"referenceID": 3, "context": "By minimizing the KL divergence between the true posterior distribution and the factorized distribution, we obtain the following variational inference solution (Jordan et al., 1999) for each of the factors:", "startOffset": 160, "endOffset": 181}, {"referenceID": 0, "context": "The digamma terms arise from the normalizing \u0393-functions of the Dirichlet distribution (Blei et al., 2003).", "startOffset": 87, "endOffset": 106}, {"referenceID": 7, "context": "We used the standard BSS-EVAL suite of metrics (Vincent et al., 2006) to evaluate the source separation performance.", "startOffset": 47, "endOffset": 69}], "year": 2012, "abstractText": "The past decade has seen substantial work on the use of non-negative matrix factorization and its probabilistic counterparts for audio source separation. Although able to capture audio spectral structure well, these models neglect the non-stationarity and temporal dynamics that are important properties of audio. The recently proposed non-negative factorial hidden Markov model (N-FHMM) introduces a temporal dimension and improves source separation performance. However, the factorial nature of this model makes the complexity of inference exponential in the number of sound sources. Here, we present a Bayesian variant of the N-FHMM suited to an efficient variational inference algorithm, whose complexity is linear in the number of sound sources. Our algorithm performs comparably to exact inference in the original NFHMM but is significantly faster. In typical configurations of the N-FHMM, our method achieves around a 30x increase in speed.", "creator": "LaTeX with hyperref package"}}}