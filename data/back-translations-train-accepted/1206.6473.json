{"id": "1206.6473", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Compositional Planning Using Optimal Option Models", "abstract": "In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macro-operators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intra- and inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (366kb)", "http://arxiv.org/abs/1206.6473v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["david silver", "kamil ciosek"], "accepted": true, "id": "1206.6473"}, "pdf": {"name": "1206.6473.pdf", "metadata": {"source": "META", "title": "Compositional Planning Using Optimal Option Models", "authors": ["David Silver", "Kamil Ciosek"], "emails": ["d.silver@cs.ucl.ac.uk", "k.ciosek@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is not the case that the individual countries are countries where most of them are unable to assert their interests. In fact, it is the case that most of them are unable to assert their interests. In fact, it is the case that most of them are unable to assert their interests. In fact, it is the case that most of them are unable to assert their interests. In fact, it is the case that most of them are unable to assert their interests. In fact, it is the case that they are unable to assert their interests. In fact, it is the case that they are unable to assert their interests."}, {"heading": "2 Background", "text": "An MDP is defined by a set of n-states, a set of measures A, action transition matrices P a = \u03b2 function and action reward vectors Ra for each action A and a discount factor 0 \u2264 \u03b3 < 1. Each component of the action transition matrix P ass \u2032 is the discounted probability of the next state s \u2032 that the action a was selected in state s, P ass \u2032 = \u03b3 Pr (st + 1 = s \u2032 s, at = a). Each component of the action reward vector Ras is the expected reward for the action a being selected in state s, Ras = E [rt = s, at = s, at = a]. The discount factor can be regarded as the chance to end an absorbing end state with probability. The discounted probability P ass \u2032 can be interpreted as the probability that the state s s is reached without exiting.An optimal state policy S (s, a) is the probability that the action b."}, {"heading": "3 Models", "text": "Informal, a model is a stochastic representation from state to state, combined with the reward accumulated along the way. Applying a model to a state leads to a distribution of outcome states and an expected reward. To jointly build models, we apply a second model to this outcome distribution and expected reward, and arrive at a new state distribution and reward. We now formalize these ideas along the lines of Sutton (1995). We define a rasp (reward and state probabilities), [r | p] as a 1 \u00d7 (1 + n) line vector, where n = | S | r is a scalar reward, and p is a 1 \u00d7 n line vector that provides a discounted probability distribution p over states in S. We use ss to designate the deterministic rasp, which is probability 1 in state s, and has a reward component of zero; we shorten it to s if there is no ambiguity."}, {"heading": "3.1 Model Sets", "text": "Models can represent the results of actions, options and policies. An option model Oo-O represents the result at the end of a corresponding option o-o. It combines an option transition matrix with an option reward vector, Oo = [1 0 Ro P o]. An action model Aa-A represents the result of a primary action a-A, where Aa = [1 0 Ra P a]. Action models are option models, A-O, corresponding to options ending with probability 1. A policy model in which the reverse and the identity transition matrix, I = [1 0 V \u03c0 0], represent the result of the execution of a policy forever. Policy models are also option models, E-O, corresponding to options ending with probability 0. Finally, we define an identity model I corresponding to zero backwards and the identity transition matrix, I = [1 0 0 0 I]; this can be considered a zero model without discounting."}, {"heading": "3.2 Value Models", "text": "A value model V-V in which V = [1 0 V 0] as a reward vector (i.e. the total reward before leaving) has a range matrix of zero (i.e., it always leaves) and a reward vector given by the components of V (s) as a reward vector (i.e. the total reward before leaving). Value models can be used to express several familiar value functions. A state value function V-V can be represented by composition with the corresponding value model sV; an action value function can be represented by sAV; and an interoption value function (Sutton et al., 1999) can be represented by sOV.The true value model G- = [1 0 V-0] represents the overall objective of maximizing the total reward. It is defined to have a value function V \u2212 < < (s) which represents a lower limit on the value of the total value function."}, {"heading": "3.3 Expectation Models", "text": "An expectation model E\u03c1 (M) is the expected model under a certain distribution \u03c1 (s, \u00b7) over models. For example, an action expectation model E\u03c0 (A) contains the average of all action models Aa-A according to the guidelines \u03c0 (s, a). Specifically, each line of E\u03c0 (A) contains the expected rasp of states after an action has been performed by \u03c0, Ea \u0445 (s, \u00b7) [sAa | s], E\u03c0 (A) = 1 0 Ea (s, \u00b7) [sAa | s = s1]... Ea \u0445 (s, \u00b7) [sAa | s = sn] (3) The composition of a model with a deterministic rasp selects the line corresponding to the state s, sE\u03c0 (s, \u00b7) [sAa \u00b2 (s, a) sAa \u2012 S (4)"}, {"heading": "3.4 Maximising Models", "text": "Each reward component is the maximum sV value from the state s.max V-W = 1.0max sV-W, s = s1 0... max sV-W, s = sn (5) An argmax model argmax MV M-M maximizes the models in the set M, relative to the value model V. Each line of the Argmax MV M-M is the rasp sM, which maximizes the sMV value from the state s.argmax MV M-M = 1 0 argmax sMV-M, s = s1... argmax sMV sM-M, s = sn (6) The composition of an argmax model with a deterministic rasp determines the maximizing line, s argmax MV-M-M = argmax sMV-M, s-S sM-M-M (7)"}, {"heading": "4 Model Equations", "text": "For didactic reasons, we start with the composition of primitive actions into political models and develop a model equation similar to the Bellman equation. We then expand this approach to include the composition of option models into political models, the composition of action models into option models, and finally the composition of option models into other option models. We provide evidence of unique fixed points in the complementary material."}, {"heading": "4.1 Action-Policy Model Composition", "text": "We start by rewriting the Bellman expectation equation into a model composition, V = E\u03c0 (A) V (8) We call this equation the expectation equation of the action model. It rewrites the Bellman expectation equation into homogeneous coordinates. This equation has a fixed point V = \u03c0, i.e. the composition of the action expectation model E\u03c0 (A) with the political model \u03c0 leads to the same political model. We also consider this equation as the model max AV A-A, which maximizes the state action value sAV from each state. We can then rewrite the Bellman optimality equation into homogeneous coordinates, V = max AV A-A (9) We call this equation the optimality equation of the action model. The optimal political model is the maximum model max."}, {"heading": "4.2 Option-Policy Model Composition", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "4.3 Action-Option Model Composition", "text": "Primitive actions can be grouped together into option models to define intra-option model \u03b2 = \u03b2 (A). 1Hierarchical optimality is a global optimality state. In contrast, recursive optimality (Dietterich, 2000) is a weaker, local optimality state that provides all suboptions. Many hierarchical reinforcement learning algorithms achieve recursive optimality, but not hierarchical optionality. This requires a mechanism to integrate option termination into model compositions. We represent the termination condition \u03b2 (s) by a termination model E\u03b2 (I, M). This is an expectation model of {I, M} that represents each line from the identity model I with probability \u03b2 (s) or from the model M with probability \u03b2 (s) or from the model M with probability AG."}, {"heading": "4.4 Option-Option Model Composition", "text": "We now present the most general case where option models are composed of other option models, which combines intra-option model learning with interoption model learning, a key step toward our goal of compositional planning. As with the composition of option models, we assume that we obtain a base set of options and a corresponding set of option models that we must compose together. As with the composition of action option models, we consider both the termination conditions and the policies. Combining these ideas together, we obtain the option model expectation equation, M = E\u03c0 (O) E\u03b2 (I, M) (16) with fixed point M = O < \u03c0, \u03b2 > and the option option option option model optimality equation, M = argmax OBG OB | O, B, M} (17) It is not generally possible to construct all option models due to the limitations of the option model. Instead, we consider the option model {> > hierarchical, < Option model <"}, {"heading": "4.5 Optimal \u03b2- and \u03c0-Option Models", "text": "There are, in fact, two dimensions of optionality for option models: Optimality of policies \u03c0 = > Optimality of termination condition \u03b2 = \u03b2 \u03b2 \u03b2 | \u03b2 = \u03b2 \u03b2 \u03b2-option condition \u03b2. The preceding sections jointly discussed optimal option models that maximize both policies and termination conditions. We are now considering option models that optimize only one of these two dimensions. An optimal \u03b2 option model argmax OG O < \u03c0, \u03b2 = \u03b2 is the Argmax model over the set of options with termination condition \u03b2, i.e. it maximizes policies for a given termination condition \u03b2. Likewise, an optimal \u03c0 option model argmax OGO < \u03c0, \u03b2 > | B is the Argmax model over the set of options with termination condition \u03b2, i.e. it maximizes policies for a given termination condition \u03b2. Likewise, an optimal \u03c0 option model argmax < gmax < Optima model < Optima < Optima < Optima < Optima; Optima < Optima; Optima &ltb Optima; Optima &ltb Optima; Optima; Optima &ltb Optima; Optima &ltb Optima; Optima; Optima; Optima; Optimum model &ltb Optima; Optima; Optima; Optima; Optima; Optima; Optima; Optima; Optima; Optima; Optima; Optimum; Optima; Optima; Optima; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optimum; Optim"}, {"heading": "5 Option-Option Model Iteration", "text": "The Bellman optimality equation forms the basis for a variety of OMDP planning algorithms (Sutton & Barto, 1998). Similarly, the model optimality equations can be used to derive a variety of MDP planning algorithms. Specifically, the option option model equations can be used to derive algorithms for compositional planning in MDPs. Here, we focus on a dynamic programming algorithm that uses the option model option model option equation (Equation 17) as an iterative update. This algorithm, which we call option modelliteration (OOMI), can be seen as a generalization of the value variation to option models for multiple subtargets. We assume that we obtain a base set of option models, and also become in the subtarget value models {G1, Gm} for different subtargets. Each iteration, the algorithm updates the option models."}, {"heading": "6 Empirical Results", "text": "This year it is more than ever before."}, {"heading": "7 Conclusion", "text": "The Bellman optimality equation motivated the development of a variety of MDP planning algorithms. We have generalized the Bellman equation 3Note, but have shown that each backup comes at a higher cost with the iteration of the model, since a complete set needs to be updated. In several important dimensions, we allow an analog variety of compositional planning algorithms. We have illustrated such an approach by the iteration of the option model: This is the first MDP planning algorithm to dynamically create its own planning operators, which are composed to allow ever deeper and targeted jumps through the state space. As the iteration of the option model, the iteration of the option model applies full-area backup copies over complete swings of the state space. In principle, the model equations could also be solved by example backups over sample trajectories, resulting in compositional algorithms for hierarchical reinforcement."}], "references": [{"title": "On representations of problems of reasoning about actions", "author": ["S. Amarel"], "venue": "Machine Intelligence,", "citeRegEx": "Amarel,? \\Q1968\\E", "shortCiteRegEx": "Amarel", "year": 1968}, {"title": "State abstraction for programmable reinforcement learning agents", "author": ["D. Andre", "S. Russell"], "venue": "In 18th National Conference on Artificial Intelligence,", "citeRegEx": "Andre and Russell,? \\Q2002\\E", "shortCiteRegEx": "Andre and Russell", "year": 2002}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["T. Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich,? \\Q2000\\E", "shortCiteRegEx": "Dietterich", "year": 2000}, {"title": "The role of macros in tractable planning", "author": ["A. Jonsson"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Jonsson,? \\Q2009\\E", "shortCiteRegEx": "Jonsson", "year": 2009}, {"title": "Efficient skill learning using abstraction selection", "author": ["G. Konidaris", "A. Barto"], "venue": "In 21st International Joint Conference on Artificial Intelligence,", "citeRegEx": "Konidaris and Barto,? \\Q2009\\E", "shortCiteRegEx": "Konidaris and Barto", "year": 2009}, {"title": "Learning to solve problems by searching for macrooperators", "author": ["R. Korf"], "venue": "Pitman Publishing,", "citeRegEx": "Korf,? \\Q1985\\E", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Chunking in SOAR: The anatomy of a general learning mechanism", "author": ["J. Laird", "P. Rosenbloom", "A. Newell"], "venue": "Machine Learning,", "citeRegEx": "Laird et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Laird et al\\.", "year": 1986}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["R. Parr", "S. Russell"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Parr and Russell,? \\Q1997\\E", "shortCiteRegEx": "Parr and Russell", "year": 1997}, {"title": "Theoretical results on reinforcement learning with temporally abstract options", "author": ["D. Precup", "R. Sutton", "S. Singh"], "venue": "In 10th European Conference on Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Precup et al\\.", "year": 1998}, {"title": "A structure for plans and behavior", "author": ["E. Sacerdoti"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Sacerdoti,? \\Q1975\\E", "shortCiteRegEx": "Sacerdoti", "year": 1975}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R. Sutton"], "venue": "In 12th International Conference on Machine Learning,", "citeRegEx": "Sutton,? \\Q1995\\E", "shortCiteRegEx": "Sutton", "year": 1995}, {"title": "Reinforcement Learning: an Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).", "startOffset": 124, "endOffset": 187}, {"referenceID": 10, "context": "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).", "startOffset": 124, "endOffset": 187}, {"referenceID": 6, "context": "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).", "startOffset": 124, "endOffset": 187}, {"referenceID": 7, "context": "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).", "startOffset": 124, "endOffset": 187}, {"referenceID": 13, "context": "A closed-loop policy that is followed for some number of steps, and stops according to a termination condition that also depends on the state, is known as an option (Sutton et al., 1999).", "startOffset": 165, "endOffset": 186}, {"referenceID": 11, "context": "An option model describes the distribution of outcome states that would result from following the option (Sutton, 1995).", "startOffset": 105, "endOffset": 119}, {"referenceID": 9, "context": "Option models can also be composed together into more abstract option models (Precup et al., 1998).", "startOffset": 77, "endOffset": 98}, {"referenceID": 13, "context": "Option models are either constructed from primitive actions, in an approach known as intra-option model learning; or they are used to compute a value function, in an approach known as inter-option (or SMDP) planning (Sutton et al., 1999).", "startOffset": 216, "endOffset": 237}, {"referenceID": 11, "context": "(1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sutton, 1995) or to fixed policies without control (Sutton et al.", "startOffset": 162, "endOffset": 176}, {"referenceID": 13, "context": "(1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sutton, 1995) or to fixed policies without control (Sutton et al., 1999).", "startOffset": 214, "endOffset": 235}, {"referenceID": 2, "context": "(1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sutton, 1995) or to fixed policies without control (Sutton et al., 1999). We present a Bellman optimality equation for state probabilities in Markov decision processes, including actions and control. Second, Precup et al. (1998) provided Bellman equations for composing option models into policies, but not into options.", "startOffset": 36, "endOffset": 392}, {"referenceID": 2, "context": "These architectures, including Dietterich\u2019s MAXQ (2000), and Parr and Russell\u2019s HAMs (1997; 2002), do construct the solution to one subproblem from the solution to other subproblems.", "startOffset": 31, "endOffset": 56}, {"referenceID": 2, "context": "This algorithm is called value iteration (Bellman, 1957).", "startOffset": 41, "endOffset": 56}, {"referenceID": 11, "context": "We now formalise these ideas, following Sutton (1995).", "startOffset": 40, "endOffset": 54}, {"referenceID": 11, "context": "This block matrix notation for models and block vector notation for rasps are known as homogeneous coordinates (Sutton, 1995).", "startOffset": 111, "endOffset": 125}, {"referenceID": 13, "context": "A state value function V (s) can be represented by composition with the corresponding value model, sV; an action value function can be represented by sAV; and an inter-option value function (Sutton et al., 1999) can be represented by sOV.", "startOffset": 190, "endOffset": 211}, {"referenceID": 3, "context": "The hierarchically optimal policy model max \u03a0 \u03a0\u03c0 | supp(\u03c0)\u2286\u03a9 is the max model over this set; it is analogous to a hierarchically optimal value function (Dietterich, 2000), i.", "startOffset": 152, "endOffset": 170}, {"referenceID": 3, "context": "In contrast, recursive optimality (Dietterich, 2000) is a weaker, local optimality condition that assumes all suboptions are fixed.", "startOffset": 34, "endOffset": 52}, {"referenceID": 3, "context": "In this paper we have focused on planning with table lookup models; however, similar to MAXQ (Dietterich, 2000), HAMs (Andre & Russell, 2002) or skills (Konidaris & Barto, 2009), substantial efficiency improvements may be generated when each option model is provided with its own state abstraction.", "startOffset": 93, "endOffset": 111}], "year": 2012, "abstractText": "In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macrooperators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intraand inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals.", "creator": "LaTeX with hyperref package"}}}