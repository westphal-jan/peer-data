{"id": "1206.6474", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Estimation of Simultaneously Sparse and Low Rank Matrices", "abstract": "The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves $\\ell_1$-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (632kb)", "http://arxiv.org/abs/1206.6474v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.DS cs.LG cs.NA stat.ML", "authors": ["pierre-andr\u00e9 savalle", "emile richard", "nicolas vayatis"], "accepted": true, "id": "1206.6474"}, "pdf": {"name": "1206.6474.pdf", "metadata": {"source": "META", "title": "Estimation of Simultaneously Sparse and Low Rank Matrices", "authors": ["Emile Richard", "Pierre-Andr\u00e9 Savalle"], "emails": ["emile.richard@cmla.ens-cachan.fr", "pierre-andre.savalle@ecp.fr", "nicolas.vayatis@cmla.ens-cachan.fr"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Setup and motivations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Problem formulation and notations", "text": "For a matrix S = (Si, j) i, j, we define the following matrix norms: \"S\" 1 = \"i,\" \"i\" and \"S\" = \"1,\" where \"i\" are the singular values of \"S\" and \"S\" is the rank of \"S.\" Let us consider the following constellation. Let A \"Rn\" n \"be a fixed matrix and\" a loss function over matrices. \"We introduce the following optimization problem:\" arg \"min\" S \"(S, A) +\" S \"1 +.\" The matrix (M) + is the component positive part of the matrix \"M\" and \"sgn\" (M) the projection of a matrix Z on \"S\" is denoted by PS (Z)."}, {"heading": "2.2. Main examples", "text": "The underlying assumption in this paper is that the unknown matrix to be recovered has a block diagonal structure. We will now describe the most important modeling options using the following motivating examples: \u2022 Covariance matrix estimate - matrix A is a noisy estimate of the true covariance matrix obtained, for example, with very few observations; the search space is S = S + n of the class of positive semi-defined matrices; for loss, we consider the square standard '(S, A) = VP-2F. \u2022 Denoising diagram - matrix A is the adaptation matrix of a loud graph with both irrelevant and missing edges; the search space is entirely S = Rn \u00b7 n and the coefficients of a candidate matrix estimate S as sign values for adding / removing edges from the original matrix A; again, we use' (S, A) = VP-2F."}, {"heading": "3. Oracle inequality", "text": "The next result shows how the matrix recovery is regulated by the trade-off between rank and splitter index of the unknown target matrix or by its convex surrogates: the track norm and the \"1 norm.\" Sentence 1. Let us allow S0-Rn \u00b7 n and A = S0 + with i.i.d. entries with zero mean value. Let us assume that some entries have 0-1 mean value. Then, let us assume that 2\u03b1-Rn \u00b7 n and A-Rn \u00b7 n with i.i.d. entries with zero mean value."}, {"heading": "4. Generalization error in link prediction", "text": "We pause for a moment on the task of link prediction to illustrate how rank and thriftiness constraints can be helpful in this context. Faced with a subset of observed edges from a graph adjacence matrix A (0, 1) n \u00b7 n, we have begun to predict unobserved links by finding a sparse rank r predictor S (Rn) n with small zero-one loss (S, A) = 1n2 (i, j) n 1 (Ai, j \u2212 1 / 2) \u00b7 Si, j \u2264 0} by minimizing the empirical zero-one-loss capability 'E (S, A) = 1-one capability (i, j) of E1 (Ai, j \u2212 1 / 2) \u00b7 Si, j \u2264 0}.The goal of a generalization is the generalization tied to the relation' (S, A) with 'E (S, A)."}, {"heading": "5. Algorithms", "text": "(S, A) convex and differentiable in S. We assume that its gradient Lipschitz is constant L and can be efficiently calculated, especially for the aforementioned Frobenius quadratic standard and for other classical decisions such as the loss of hinges."}, {"heading": "5.1. Proximal operators", "text": "We code the presence of a constraint S using the indicator function 1S (S), which is zero if S-S and + vice versa, resulting in S-S = arg min S-Rn \u00b7 n {'(S, A) + \u03b3 | | | S-S | | 1 + 1S (S). This formulation includes the sum of a convex differentiable loss and convex non-differentiable regulator, which makes the problem non-trivial. In the case where the optimal solution is easy to calculate when each regulator is considered in isolation, a number of algorithms have been developed, formally corresponding to cases where the proximal operator for a convex regulator R: Rn \u00b7 n \u2192 R at one point Z byxR (Z) = arg min S-Rn \u00b7 oldix = 2 | | 2F + R (S).is easy to calculate."}, {"heading": "5.2. Generalized Forward-Backward splitting", "text": "The family of forward-backward splitting methods are iterative algorithms that are applicable when there is only one non-differentiable regulator, alternating a gradient step and a proximal step, resulting in updates of formSk + 1 = prox\u03b8R (Sk \u2212 \u03b8 gradS '(S, A). In particular, this corresponds to a projected gradient descent when R is the indicator function of a convex set. On the other hand, Douglas-Rachford splitting deals with the case of q \u2265 2 terms, but does not benefit from differentiability. A generalization of these two constellations was recently proposed in (Raguetet al., 2011) under the name Generalized ForwardBackward, which we specialize in our problem in algorithm 1. The proximal operators are applied in parallel, and the resulting (Z1, Z2, Z3) is projected on the condition that Z2 = conciliation is simple."}, {"heading": "5.3. Incremental Proximal Descent", "text": "Although algorithm 1 performs well in practice, the O (n2) memory imprint with a large guide constant can be a disadvantage in some cases due to the parallel updates. Consequently, we mention a suitable serial algorithm (algorithm 2), which was introduced in (Bertsekas, 2011) and has a similar taste to a stochastic gradient descent with multiple passes. We present here a version where updates are performed in cyclic order, although a random selection of the order of updates is also possible. Algorithm 2 Incremental proximal descent S = A-RepeatSet S = S \u2212 \u03b8, A-Set S = proxdependent | |."}, {"heading": "5.4. PSD constraint", "text": "The simple form of the trace standard allows the positive semi-defined limitation to be taken into account at no additional cost, since the shrinkage process and the projection onto the convex cone of positive semi-defined matrices can be combined into a single operation. Lemma 1. For \u03c4 \u2265 0 and S \u0445 Rn \u00b7 n applies: prox\u03c4 |. |. | \u043a + 1S + n (S) = arg minZ 01 2 | | Z \u2212 S | | 2F + \u03c4 | | Z | | \u0445 = PS + n (S \u2212 \u03c4In)."}, {"heading": "6. Numerical experiments", "text": "We present numerical experiments to emphasize the advantages of our method. For efficiency reasons, we use the serial proximal descendancy algorithm (algorithm 2)."}, {"heading": "6.1. Synthetic data", "text": "Covariance matrix estimation. We use N vectors xi \u0445 N (0, \u03a3) for a block diagonal covariance matrix \u0440 Rn \u00b7 n. We use r blocks of random size and shape vv >, where the entries of v i.i.d. are drawn from the even distribution to [\u2212 1, 1]. Finally, we add Gaussian noise N (0, \u03c32) to each entry. In our experiments r = 5, N = 20, n = 100, \u03c3 = 0.6. We apply our method (SPLR) as well as the trace norm regulation (LR) and the \"1 norm regulation (SP) to the empirical covariance matrix and give average results over ten passes. Figure 1 shows the RMSE normalized by the norm for different values from vice versa. Note that the effect of the mixed penalty is visible as the minimum RMSE is achieved within the region, although we support the small R results for restoring the same R values."}, {"heading": "6.2. Real data sets", "text": "Protein interactions. We use data from (Hu et al., 2009), in which protein interactions in Escherichia coli bacteria are evaluated according to their strength in [0, 2]. Data is, by nature, sparse. Furthermore, it is often suggested that the interactions between two proteins are regulated by a small set of factors, such as superficially accessible amino acid side chains (Bock & Gough, 2001), which motivate the estimation of a low representation. If the data is presented as a weighted graph, we filter only the 10% of all 4394 proteins that exhibit the most interactions, measured by weighted degree. We consistently corrupt 10% of the entries of the adjacency matrix selected according to random noise in [0, \u03b7]. Parameters are selected by cross-validation and algorithms are evaluated based on the mean RMSE between estimated and original adjacency matrices over 25 runs."}, {"heading": "7. Discussion", "text": "The methods presented in this publication, for example, are not useful for evaluating a correlation, but always as an equivalent case. (The methods presented in this publication are extended to non-square matrices, depending, for example, on the requirement that there be no linear measurements of the target matrix or graph, which are often used for the evaluation of precision matrices (Friedman et al., 2008), in which case the loss in the attribute space can be defined. Our method does not directly apply to the estimation of precision matrices, often used for Gaussian model structures (Friedman et al., 2008), and the application of conditional independence structures generated by low ranks and possibly economical models. (Note that the trace norm conint is for some specific classes of positive semi-matrices)."}, {"heading": "Appendix- Sketch of proof for Prop. 1", "text": "For all S in S and in accordance with the optimisability of S + 1, and in accordance with the optimisability of S + 1, and in accordance with the optimisability of S + 2, and in accordance with the optimisability of S + 2, and in accordance with the optimisability of S + 2, and in accordance with the optimisability of S + 2, and in accordance with the optimisation of S + 2, and in accordance with the standards of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale of scale"}], "references": [{"title": "Model selection through sparse maximum likelihood estimation", "author": ["O. Banerjee", "L. El Ghaoui", "A. d\u2019Aspremont"], "venue": "Machine Learning Research", "citeRegEx": "Banerjee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2007}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal of Imaging Sciences,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: a survey", "author": ["D.P. Bertsekas"], "venue": "Optimization for Machine Learning, pp", "citeRegEx": "Bertsekas,? \\Q2011\\E", "shortCiteRegEx": "Bertsekas", "year": 2011}, {"title": "Sparse estimation of a covariance matrix", "author": ["J. Bien", "R. Tibshirani"], "venue": null, "citeRegEx": "Bien and Tibshirani,? \\Q2010\\E", "shortCiteRegEx": "Bien and Tibshirani", "year": 2010}, {"title": "Predicting protein\u2013 protein interactions from primary", "author": ["J.R. Bock", "D.A. Gough"], "venue": "structure. Bioinformatics,", "citeRegEx": "Bock and Gough,? \\Q2001\\E", "shortCiteRegEx": "Bock and Gough", "year": 2001}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.F. Cai", "E.J. Candes", "Z. Shen"], "venue": "Arxiv preprint Arxiv:0810.3286,", "citeRegEx": "Cai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2008}, {"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Arxiv preprint ArXiv:0912.3599,", "citeRegEx": "Candes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2009}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM J. Opt.,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Proximal splitting methods in signal processing. Fixed-Point Algorithms for Inverse Problems", "author": ["P.L. Combettes", "J.C. Pesquet"], "venue": "Science and Engineering,", "citeRegEx": "Combettes and Pesquet,? \\Q2011\\E", "shortCiteRegEx": "Combettes and Pesquet", "year": 2011}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. D\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM review,", "citeRegEx": "D.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "D.Aspremont et al\\.", "year": 2007}, {"title": "Operator norm consistent estimation of large-dimensional sparse covariance matrices", "author": ["N. El Karoui"], "venue": "Annals of Statistics,", "citeRegEx": "Karoui,? \\Q2009\\E", "shortCiteRegEx": "Karoui", "year": 2009}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["E. Hazan"], "venue": "In Proceedings of the 8th Latin American conference on Theoretical informatics,", "citeRegEx": "Hazan,? \\Q2008\\E", "shortCiteRegEx": "Hazan", "year": 2008}, {"title": "Non-negative Matrix Factorization with Sparseness Constraints", "author": ["P.O. Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoyer,? \\Q2004\\E", "shortCiteRegEx": "Hoyer", "year": 2004}, {"title": "Global functional atlas of escherichia coli encompassing previously uncharacterized proteins", "author": ["P. Hu", "S.C. Janga", "M. Babu", "J.J. D\u0131\u0301az-Mej\u0301\u0131a", "G. Butland", "W. Yang", "O. Pogoutse", "X. Guo", "S. Phanse", "P Wong"], "venue": "PLoS biology,", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Convex optimization without projection steps", "author": ["M. Jaggi"], "venue": "Arxiv preprint arXiv:1108.1170,", "citeRegEx": "Jaggi,? \\Q2011\\E", "shortCiteRegEx": "Jaggi", "year": 2011}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "ICML \u201911,", "citeRegEx": "Jalali et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2011}, {"title": "Sparse nonnegative matrix factorization for clustering", "author": ["J. Kim", "H. Park"], "venue": "Technical report, Georgia Institute of Technology,", "citeRegEx": "Kim and Park,? \\Q2008\\E", "shortCiteRegEx": "Kim and Park", "year": 2008}, {"title": "Nuclear norm penalization and optimal rates for noisy matrix completion", "author": ["V. Koltchinskii", "K. Lounici", "A. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "Koltchinskii et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii et al\\.", "year": 2011}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "Seung", "H.S"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "Journal of the American society for information science and technology,", "citeRegEx": "Liben.Nowell and Kleinberg,? \\Q2007\\E", "shortCiteRegEx": "Liben.Nowell and Kleinberg", "year": 2007}, {"title": "High dimensional low rank and sparse covariance matrix estimation via convex minimization", "author": ["X. Luo"], "venue": "Arxiv preprint arXiv:1111.1133,", "citeRegEx": "Luo,? \\Q2011\\E", "shortCiteRegEx": "Luo", "year": 2011}, {"title": "A multivariate tchebycheff inequality", "author": ["I. Olkin", "J.W. Pratt"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Olkin and Pratt,? \\Q1958\\E", "shortCiteRegEx": "Olkin and Pratt", "year": 1958}, {"title": "Generalized forward-backward splitting", "author": ["H. Raguet", "J. Fadili", "G. Peyr\u00e9"], "venue": "Arxiv preprint arXiv:1108.4404,", "citeRegEx": "Raguet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Raguet et al\\.", "year": 2011}, {"title": "Link discovery using graph feature tracking", "author": ["E. Richard", "N. Baskiotis", "Evgeniou", "Th", "N. Vayatis"], "venue": "Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Richard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Richard et al\\.", "year": 2010}, {"title": "Learning with matrix factorizations", "author": ["N. Srebro"], "venue": "PhD thesis,", "citeRegEx": "Srebro,? \\Q2004\\E", "shortCiteRegEx": "Srebro", "year": 2004}, {"title": "Maximummargin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational and Graphical Statistics, pp", "citeRegEx": "Zou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2004}, {"title": "Regularization and variable selection via the elastic net", "author": ["Zou", "Hui", "Hastie", "Trevor"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 25, "context": "Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Srebro, 2004; Cai et al., 2008)).", "startOffset": 162, "endOffset": 194}, {"referenceID": 5, "context": "Indeed, the notion of sparsity assumption has been transposed into the concept of low-rank matrices and opened the way to numerous achievements (see for instance (Srebro, 2004; Cai et al., 2008)).", "startOffset": 162, "endOffset": 194}, {"referenceID": 27, "context": "Efficient procedures developed in the context of sparse model estimation mostly rely on the use of `1-norm regularization (Tibshirani, 1996).", "startOffset": 122, "endOffset": 140}, {"referenceID": 0, "context": "These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien & Tibshirani, 2010) and graphical model structure learning (Banerjee et al., 2007; Friedman et al., 2008).", "startOffset": 185, "endOffset": 231}, {"referenceID": 11, "context": "These methods are readily adapted to matrix valued data and have been applied to covariance estimation (El Karoui, 2009; Bien & Tibshirani, 2010) and graphical model structure learning (Banerjee et al., 2007; Friedman et al., 2008).", "startOffset": 185, "endOffset": 231}, {"referenceID": 26, "context": "In the low-rank matrix completion problem, the standard relaxation approach leads to the use of the trace norm as the main regularizer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative proximal solutions (Combettes & Pesquet, 2011; Beck & Teboulle, 2009) (for general classes of losses).", "startOffset": 170, "endOffset": 218}, {"referenceID": 18, "context": "In the low-rank matrix completion problem, the standard relaxation approach leads to the use of the trace norm as the main regularizer within the optimization procedures (Srebro et al., 2005; Koltchinskii et al., 2011) and their resolution can either be obtained in closed form (loss measured in terms of Frobenius norm) or through iterative proximal solutions (Combettes & Pesquet, 2011; Beck & Teboulle, 2009) (for general classes of losses).", "startOffset": 170, "endOffset": 218}, {"referenceID": 6, "context": "In Robust PCA (Candes et al., 2009) and related literature, the signal S is assumed to have an additive decomposition S = X + Y where X is sparse and Y low-rank.", "startOffset": 14, "endOffset": 35}, {"referenceID": 7, "context": ", in (Chandrasekaran et al., 2011).", "startOffset": 5, "endOffset": 34}, {"referenceID": 16, "context": "This technique has been successfully applied to background substraction in image sequences, to graph clustering (Jalali et al., 2011) and covariance estimation (Luo, 2011).", "startOffset": 112, "endOffset": 133}, {"referenceID": 21, "context": ", 2011) and covariance estimation (Luo, 2011).", "startOffset": 34, "endOffset": 45}, {"referenceID": 18, "context": "The techniques used in the proof (see the Appendix) are very similar to those introduced in (Koltchinskii et al., 2011).", "startOffset": 92, "endOffset": 119}, {"referenceID": 18, "context": "In fact, for \u03b1 = 0, \u03c4 can be set to zero, and we get a sharp bound for Lasso, while the tracenorm regression bounds of (Koltchinskii et al., 2011) are obtained for \u03b1 = 1.", "startOffset": 119, "endOffset": 146}, {"referenceID": 25, "context": "In the case of the sole rank constraint, (Srebro, 2004) remarked that all low-rank matrices with the same sign pattern are equivalent in terms of loss and applied a standard argument for generalization in classes of finite cardinality.", "startOffset": 41, "endOffset": 55}, {"referenceID": 25, "context": "By upper bounding the number of sign configurations for a fixed sparsity pattern in (U, V ) using an argument similar to (Srebro, 2004), a union bound gives", "startOffset": 121, "endOffset": 135}, {"referenceID": 23, "context": "A generalization of these two setups has been recently proposed in (Raguet et al., 2011) under the name of Generalized ForwardBackward, which we specialize to our problem in Algorithm 1.", "startOffset": 67, "endOffset": 88}, {"referenceID": 2, "context": "As a consequence, we mention a matching serial algorithm (Algorithm 2) introduced in (Bertsekas, 2011) that has a flavor similar to multi-pass stochastic gradient descent.", "startOffset": 85, "endOffset": 102}, {"referenceID": 14, "context": "We use data from (Hu et al., 2009), in which protein interactions in Escherichia coli bacteria are scored by strength in [0, 2].", "startOffset": 17, "endOffset": 34}, {"referenceID": 24, "context": "A useful example that links our work to the matrix completion framework is when linear measurements of the target matrix or graph are available, or can be predicted as in (Richard et al., 2010).", "startOffset": 171, "endOffset": 193}, {"referenceID": 11, "context": "Due to the low-rank assumption, our method does not directly apply to the estimation of precision matrices often used for gaussian graphical model structure learning (Friedman et al., 2008), and the applications of conditional independence structures generated by low-rank and possibly sparse models is to be discussed.", "startOffset": 166, "endOffset": 189}, {"referenceID": 26, "context": ", (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U, V \u2208 Rn\u00d7r loss functions of the form `((U, V ), A) = ||UV T \u2212A||F for some target maximum rank r.", "startOffset": 2, "endOffset": 37}, {"referenceID": 25, "context": ", (Srebro et al., 2005; Srebro, 2004)), thus jointly optimizing in U, V \u2208 Rn\u00d7r loss functions of the form `((U, V ), A) = ||UV T \u2212A||F for some target maximum rank r.", "startOffset": 2, "endOffset": 37}, {"referenceID": 19, "context": "Nonnegative Matrix Factorization (NMF) (Lee et al., 1999) imposes non negativity constraints on the coefficients of U and V to enhance interpretability by allowing only for additive effects and tends to produce sparse factor matrices U, V , although this a rather indirect effect.", "startOffset": 39, "endOffset": 57}, {"referenceID": 13, "context": "There is no strong guarantee on the sparsity achieved by NMF nor is it easy to set the target sparsity and different methods for sparse NMF have been proposed in (Hoyer, 2004; Kim & Park, 2008).", "startOffset": 162, "endOffset": 193}, {"referenceID": 29, "context": "SPCA proposed in (Zou et al., 2004) penalizes the `1 norm of the principal components and can be reduced to solving independent elastic-nets.", "startOffset": 17, "endOffset": 35}, {"referenceID": 9, "context": "A different formulation using SDP programming is introduced in (D\u2019Aspremont et al., 2007) with good empirical results.", "startOffset": 63, "endOffset": 89}, {"referenceID": 15, "context": "A trace norm constraint alone can be taken into account without projection or relaxation into a penalized form by casting the problem as a SDP as proposed in (Jaggi, 2011).", "startOffset": 158, "endOffset": 171}, {"referenceID": 12, "context": "The special form of this SDP can be leveraged to use the efficient resolution technique from (Hazan, 2008).", "startOffset": 93, "endOffset": 106}], "year": 2012, "abstractText": "The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves `1-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.", "creator": "LaTeX with hyperref package"}}}