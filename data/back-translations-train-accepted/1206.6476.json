{"id": "1206.6476", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Similarity Learning for Provably Accurate Sparse Linear Classification", "abstract": "In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to state-of-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (230kb)", "http://arxiv.org/abs/1206.6476v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["aur\u00e9lien bellet", "amaury habrard", "marc sebban"], "accepted": true, "id": "1206.6476"}, "pdf": {"name": "1206.6476.pdf", "metadata": {"source": "META", "title": "Similarity Learning for Provably AccurateSparse Linear Classification", "authors": ["Aur\u00e9lien Bellet", "Amaury Habrard", "Marc Sebban"], "emails": ["aurelien.bellet@univ-st-etienne.fr", "amaury.habrard@univ-st-etienne.fr", "marc.sebban@univ-st-etienne.fr"], "sections": [{"heading": "1. Introduction", "text": "The notion of (dis) similarity plays an important role in many machine learning problems such as classification, clustering, or ranking. (For this reason, researchers have explored in practical and formal ways what it means for a pair similarity function to be \"good.\" Since the manual adjustment of such functions can be difficult and tedious to solve real problems, we must learn in the real world, resulting in verified similarity and metric learning. Generally, we address these approaches on the basis of reasonable intuition that a good similarity function should assign a large (or small) score to pairs of the same class (different class)."}, {"heading": "2. Notations and Related Work", "text": "We refer to vectors with bold lowercase letters (x) and matrices with bold uppercase letters (A). We consider labeled points as z = (x,) drawn from an unknown distribution P over Rd \u00b7 {\u2212 1, 1}. A similarity function is defined by K: Rd \u00b7 Rd \u2192 [\u2212 1, 1]. We refer to the L2 standard as B \u00b7 2 and the Frobenius standard as B \u00b7 E. Finally, [1 \u2212 c] + = max (0, 1 \u2212 c) denotes the loss of hinges."}, {"heading": "2.1. Metric and Similarity Learning", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2.2. Learning with Good Similarity Functions", "text": "In the most recent work, Balcan et al. (2008) have introduced a new theory of learning with good parallelism functions, which are not based on the following definition. (Definition 1 (Balcan et al., 2008) A similarity function K is a (anyway) -good similarity problem in relation to a learning problem P, if there is a (random) indicator function R (x), which defines a (probable) set of \"reasonable points,\" so that the following conditions apply: 1. E (x,), 2. P [1 \u2212 g (x) / g] +], where g (x) = E (x), \u00b2)."}, {"heading": "3. Learning Good Similarity Functions for Linear Classification", "text": "We consider KA (x, x \u2032) = xTAx \u2032, a bilinear similarity function parameterized by the matrix A-Rd \u00b7 d, which is not limited to being PSD or symmetrical. This form of similarity function has been successfully used in the context of large-scale learning of image similarities on the Internet (Chechik et al., 2009). KA has the advantage of being efficiently calculable when the inputs are x and x \u2032 sparse vectors. To meet the condition KA-1 [\u2212 1, 1] we assume that inputs are normalized so that | x | | 2 \u2264 1, and we need | | A | F \u2264 1."}, {"heading": "3.1. Similarity Learning Formulation", "text": "In practice, it is a subset of T with NR = 0, 1). In the absence of background knowledge, it can be drawn randomly, according to certain criteria (e.g. diaspora)."}, {"heading": "3.2. Kernelization of SLLC", "text": "The framework presented in the previous section is well founded in theory in relation to Balcan et al. \"s theory and has some generalization guarantees, as we will see in the next section. Moreover, it has the advantage of being very simple: we learn a global linear similarity and use it to form a global linear classification. To learn more powerful similarities (and therefore classifiers), we propose to nuclearize the approach by introducing it into the nonlinear attribute space generated by a kernel. Kernel ization enables linear classifiers such as Vector Machines or some remote algorithms (e.g. Shalev-Shwartz et al., Davis et al., 2007) to learn nonlinear decision boundaries or transformations."}, {"heading": "4. Theoretical Analysis", "text": "In this section, we present a theoretical analysis of our approach: Our main result is the derivation of a generalization limit (Theorem 3) that guarantees the consistency of the SLLC and thus the \"quality\" of the task under consideration. In our framework, the similarity is optimized based on a series of reasonable points resulting from the training sample. Therefore, these reasonable points cannot follow the distribution from which the training sample was generated. To address this situation, we propose to derive a generalization limit according to the framework of uniform stability (Bousquet & Elisseeff, 2002), which does not presuppose a i.e. a tie at the pair level."}, {"heading": "4.1. Uniform Stability", "text": "Broadly speaking, an algorithm is stable if its performance does not change significantly under a small modification of the training sample, which variation must be limited in O (1 / NT) with respect to infinite normality. Definition 2 (Bousquet & Elisseeff, 2002) A learning algorithm has uniform stability in \u03baNT w.r.t. of a loss function L, with a positive constant if we have a positive constant, if we learn it from sample T, 1 \u2264 i \u2264 NT, sup z | L (MT, z) \u2212 L (MT i, z) \u2212 \u2264 \u0445NT, where MT is the model we learned from sample T."}, {"heading": "MT i the model learned from the sample T", "text": "i. T i is obtained from T by replacing the ith example zi-T with another example z-i, which is independent of T and is derived from P. L (M, z) is the loss of an example. In this definition, T i characterizes the concept of small modification of the training sample. If this definition is met, Bousquet & Elisseeff (2002) have shown that the following generalization limits persist. Theorem 2 (Bousquet & Elisseeff, 2002) Let it be g > 0 and NT > 1. For each algorithm with uniform stability, we use a loss function limited by 1, where the probability 1 \u2212 \u043c is higher than the random draw of T: L (MT) < L-T (MT) + KNT + (2 + 1), with L (MT) being the expected loss and L-T (MT) its empirical estimate above T."}, {"heading": "4.2. Generalization Bound", "text": "For the sake of simplicity, in a bilinear model KA with AR, we denote both the similarity defined by the matrix A and the reasonable points associated with it R (if it is apparent from the context, we can omit the subscript R). In the case of a similarity AR, V (AR, z, R), the loss function plays by an example. The loss above sample T is defined as the result T = (AR) = 1 NT \u2211 NT i = 1 V (AR, zi, R) and corresponds to the empirical quality. Finally, in our case, the expected loss over the true distribution is given as the result of what we produce as a result of what we produce (AR) = (x, l) \u2022 PV (AR, z, R) and corresponds to the goodness in the generalization. If it is clear from the context, we can simply use T and T to prove the uniform stability characteristic that we need to show that it belongs to z."}, {"heading": "P2: |KA(x,x\u2032)\u2212KA\u2032(x,x\u2032)| \u2264 \u2016A\u2212A\u2032\u2016F ,", "text": "P3: V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) \u2212 V (A) (A) \u2212 V (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A)"}, {"heading": "5. Experiments", "text": "We propose a comparative study of our method and two widely used Mahalanobis remote learning algorithms: Large Margin Nearest Neighbor (LMNN) by Weinberger & Saul (2009) and Information-Theoretic Metric Learning (ITML) by Davis et al. (2007).2 Roughly speaking, LMNN optimizes the k-NN error on the training set (with a safety distance), while ITML aims to best satisfy pair-based constraints, while 2We uses the code provided by the authors. Minimizing the LogDet divergence between the learned matrix M and the identity matrix. We are conducting this experimental study on seven classic binary datasets of different domains, sizes and difficulty, most of which have been taken from the UCI Machine Learning Repository. Their properties are summarized in Table 1."}, {"heading": "5.1. Experimental Setup", "text": "We compare the following methods: (i) cosmic similarity AI in KPCA space, (v) ITML in original space, and (vi) ITML in KPCA space. (3) All attributes are scaled to [\u2212 1 / d; 1 / d] To create a new attribute space using KPCA, we use the Gaussian kernel with parameters that correspond to the mean of all paired training data (a standard heuristic used, for example, by Kar & Jain (2011)). Ideally, we would like to project the data onto the maximum size attribute space (equal to the number of training examples)."}, {"heading": "5.2. Results", "text": "The results we have achieved with the sparse linear classifier of the problem (1) are considered by Balcan et al. (2008), but also the results obtained with 3-NN, as LMNN and ITML are considered by the LLC experts to be too high to ensure that their values become [\u2212 1].4Note that the amount of detected deviations is more than 90% used for all datasets.For k-NN. In the linear classification (Table 2), SLLC achieves the highest accuracy on 5 of 7 datasets and competitive performance on the remaining 2."}, {"heading": "6. Conclusion", "text": "In this thesis, we presented SLLC, a novel approach to the problem of similarity learning, using both the theory of Balcan et al. on good similarity functions and the KPCA trick. We derived a generalization based on the notion of a uniform stability independent of the size of the entrance space and thus of the number of dimensions selected by KPCA. It guarantees the quality in generalizing the learned similarity and thus the accuracy of the resulting linear classifier for the task under consideration. We demonstrated experimentally the effectiveness of SLLC and also demonstrated that the learned similarities induce extremely sparse classifiers. Combined with the independence of dimensionality and robustness to overhaul, it makes the approach very efficient and suitable for high-dimensional data. Future work could include a \"complete\" kernelization of SLLC (i.e. the problem solely related to internal products expressing the influence of solving technology on other solving processes)."}, {"heading": "Acknowledgments", "text": "We would like to acknowledge the support of the ANR LAMPADA 09-EMER-007-02 project and the PASCAL 2 Network of Excellence."}], "references": [{"title": "Improved Guarantees for Learning via Similarity Functions", "author": ["Balcan", "M.-F", "A. Blum", "N. Srebro"], "venue": "In COLT, pp", "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "A new kernelization framework for Mahalanobis distance learning", "author": ["R. Chatpatanasiri", "T. Korsrilabutr", "P. Tangchanachaianan", "B. Kijsirikul"], "venue": "algorithms. Neurocomputing,", "citeRegEx": "Chatpatanasiri et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chatpatanasiri et al\\.", "year": 2010}, {"title": "An Online Algorithm for Large Scale Image Similarity Learning", "author": ["G. Chechik", "U. Shalit", "V. Sharma", "S. Bengio"], "venue": "In NIPS,", "citeRegEx": "Chechik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2009}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML, pp", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Online Metric Learning and Fast Similarity Search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "In NIPS,", "citeRegEx": "Jain et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "Similarity-based Learning via Data Driven Embeddings", "author": ["P. Kar", "P. Jain"], "venue": "In NIPS,", "citeRegEx": "Kar and Jain,? \\Q2011\\E", "shortCiteRegEx": "Kar and Jain", "year": 2011}, {"title": "Generalized Cosine and Similarity Metrics: A supervised learning approach based on nearestneighbors", "author": ["A.M. Qamar"], "venue": "PhD thesis, University of Grenoble,", "citeRegEx": "Qamar,? \\Q2010\\E", "shortCiteRegEx": "Qamar", "year": 2010}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "Mller", "K.-R"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Learning a Distance Metric from Relative Comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "In NIPS,", "citeRegEx": "Schultz and Joachims,? \\Q2003\\E", "shortCiteRegEx": "Schultz and Joachims", "year": 2003}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2004}, {"title": "Distance Metric Learning for Large Margin", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Nearest Neighbor Classification. JMLR,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Sparse Metric Learning via Smooth Optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": "In NIPS,", "citeRegEx": "Ying et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ying et al\\.", "year": 2009}, {"title": "1-norm Support Vector Machines", "author": ["J. Zhu", "S. Rosset", "T. Hastie", "R. Tibshirani"], "venue": "In NIPS,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.", "startOffset": 51, "endOffset": 189}, {"referenceID": 3, "context": "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.", "startOffset": 51, "endOffset": 189}, {"referenceID": 4, "context": "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.", "startOffset": 51, "endOffset": 189}, {"referenceID": 11, "context": "Among these methods, Mahalanobis distance learning (Schultz & Joachims, 2003; Shalev-Shwartz et al., 2004; Davis et al., 2007; Jain et al., 2008; Weinberger & Saul, 2009; Ying et al., 2009) has attracted a lot of interest, because it has a nice geometric interpretation: the goal is to learn a positive semi-definite (PSD) matrix which linearly projects the data into a new feature space where the standard Euclidean distance performs well.", "startOffset": 51, "endOffset": 189}, {"referenceID": 2, "context": "Some work has also gone into learning arbitrary similarity functions with no PSD constraint to make the problem easier to solve (Chechik et al., 2009; Qamar, 2010).", "startOffset": 128, "endOffset": 163}, {"referenceID": 6, "context": "Some work has also gone into learning arbitrary similarity functions with no PSD constraint to make the problem easier to solve (Chechik et al., 2009; Qamar, 2010).", "startOffset": 128, "endOffset": 163}, {"referenceID": 0, "context": "Recently, Balcan et al. (2008) introduced the formal notion of (\u01eb, \u03b3, \u03c4)-good similarity function, which does not require positive semi-definiteness and is less restrictive than local pair-based constraints.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "Furthermore, by using the Kernel Principal Component Analysis (KPCA) trick (Chatpatanasiri et al., 2010), we are able to kernelize our algorithm and thereby learn more powerful similarity functions and classifiers in the nonlinear feature space induced by a kernel.", "startOffset": 75, "endOffset": 104}, {"referenceID": 9, "context": "There also exist purely online methods (Shalev-Shwartz et al., 2004; Jain et al., 2008).", "startOffset": 39, "endOffset": 87}, {"referenceID": 4, "context": "There also exist purely online methods (Shalev-Shwartz et al., 2004; Jain et al., 2008).", "startOffset": 39, "endOffset": 87}, {"referenceID": 2, "context": "There has also been some interest in learning arbitrary similarity functions with no PSD requirement (Chechik et al., 2009; Qamar, 2010).", "startOffset": 101, "endOffset": 136}, {"referenceID": 6, "context": "There has also been some interest in learning arbitrary similarity functions with no PSD requirement (Chechik et al., 2009; Qamar, 2010).", "startOffset": 101, "endOffset": 136}, {"referenceID": 2, "context": "Davis et al. (2007) regularize using the LogDet divergence (for its automatic enforcement of PSD) while Ying et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Davis et al. (2007) regularize using the LogDet divergence (for its automatic enforcement of PSD) while Ying et al. (2009) use the (2,1)-norm (which favors a low-rank M).", "startOffset": 0, "endOffset": 123}, {"referenceID": 0, "context": "In recent work, Balcan et al. (2008) introduced a new theory of learning with good similarity functions, based on the following definition.", "startOffset": 16, "endOffset": 37}, {"referenceID": 0, "context": "Definition 1 (Balcan et al., 2008) A similarity function K is an (\u01eb, \u03b3, \u03c4)-good similarity function in hinge loss for a learning problem P if there exists a (random) indicator function R(x) defining a (probabilistic) set of \u201creasonable points\u201d such that the", "startOffset": 13, "endOffset": 34}, {"referenceID": 0, "context": "Theorem 1 (Balcan et al., 2008) Let K be an (\u01eb, \u03b3, \u03c4)-good similarity function in hinge loss for a learning problem P .", "startOffset": 10, "endOffset": 31}, {"referenceID": 12, "context": "Note that Problem (1) is essentially an L1-regularized linear SVM (Zhu et al., 2003) with an empirical similarity map (Balcan et al.", "startOffset": 66, "endOffset": 84}, {"referenceID": 0, "context": ", 2003) with an empirical similarity map (Balcan et al., 2008), and can be efficiently The original formulation proposed by Balcan et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 0, "context": ", 2003) with an empirical similarity map (Balcan et al., 2008), and can be efficiently The original formulation proposed by Balcan et al. (2008) was actually L1-constrained.", "startOffset": 42, "endOffset": 145}, {"referenceID": 2, "context": "This form of similarity function was successfully used in the context of large-scale online image similarity learning (Chechik et al., 2009).", "startOffset": 118, "endOffset": 140}, {"referenceID": 3, "context": "Kernelization allows linear classifiers such as Support Vector Machines or some Mahalanobis distance learning algorithms (e.g., Shalev-Shwartz et al., 2004; Davis et al., 2007) to learn nonlinear decision boundaries or transformations.", "startOffset": 121, "endOffset": 176}, {"referenceID": 7, "context": "The idea is to use Kernel Principal Component Analysis (Sch\u00f6lkopf et al., 1998) to project the data into a new space using a nonlinear kernel function, and to keep only a chosen number of dimensions (those that capture best the overall variance of the data).", "startOffset": 55, "endOffset": 79}, {"referenceID": 1, "context": "For these reasons, we instead use the KPCA trick, recently proposed by Chatpatanasiri et al. (2010). It provides a straightforward way to kernelize a metric learning algorithm while performing dimensionality reduction at no additional cost.", "startOffset": 71, "endOffset": 100}, {"referenceID": 1, "context": "For these reasons, we instead use the KPCA trick, recently proposed by Chatpatanasiri et al. (2010). It provides a straightforward way to kernelize a metric learning algorithm while performing dimensionality reduction at no additional cost. The idea is to use Kernel Principal Component Analysis (Sch\u00f6lkopf et al., 1998) to project the data into a new space using a nonlinear kernel function, and to keep only a chosen number of dimensions (those that capture best the overall variance of the data). The data are then projected into this new feature space, and the (unchanged) metric learning algorithm can be used to learn a metric in that space. Chatpatanasiri et al. (2010) showed that the KPCA trick is theoretically sound for unconstrained metric and similarity learning algorithms (they proved representer theorems), which includes SLLC.", "startOffset": 71, "endOffset": 677}, {"referenceID": 3, "context": "We propose a comparative study of our method and two widely-used Mahalanobis distance learning algorithms: Large Margin Nearest Neighbor (LMNN) from Weinberger & Saul (2009) and Information-Theoretic Metric Learning (ITML) from Davis et al. (2007). Roughly speaking, LMNN optimizes the k-NN error on the training set (with a safety margin) whereas ITML aims at best satisfying pair-based constraints while We used the code provided by the authors.", "startOffset": 228, "endOffset": 248}, {"referenceID": 0, "context": "Classification performance We report the results obtained with the sparse linear classifier of Problem (1) suggested by Balcan et al. (2008) but also those obtained with 3-NN since LMNN and ITML are designed KI , LMNN and ITML are normalized to ensure their values belong to [\u22121, 1].", "startOffset": 120, "endOffset": 141}], "year": 2012, "abstractText": "In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of (\u01eb, \u03b3, \u03c4)-good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to stateof-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.", "creator": "LaTeX with hyperref package"}}}