{"id": "1206.6477", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Discovering Support and Affiliated Features from Very High Dimensions", "abstract": "In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (543kb)", "http://arxiv.org/abs/1206.6477v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yiteng zhai", "mingkui tan", "ivor w tsang", "yew-soon ong"], "accepted": true, "id": "1206.6477"}, "pdf": {"name": "1206.6477.pdf", "metadata": {"source": "META", "title": "Discovering Support and Affiliated Features from Very High Dimensions", "authors": ["Yiteng Zhai", "Mingkui Tan", "Ivor W. Tsang", "Yew-Soon Ong"], "emails": ["YZHAI1@NTU.EDU.SG", "TANM0097@NTU.EDU.SG", "IVORTSANG@NTU.EDU.SG", "ASYSONG@NTU.EDU.SG"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2. Preliminaries and Related Works", "text": "In this paper, we designate a data point according to xi-Rn and the data set according to X = [x1,.., xn] = [f-1,.., f-m-Rm \u00b7 n, where fj represents a row vector corresponding to the jth character of the data points in X. Each xi is associated with output yi-1. We also define y as the vector of the name for the data. The symbols 0 and 1 are the column vectors with all zeros and ones, respectively. For each vector f, we designate the mean and standard deviation of the inputs in f as \u00b5f and \u03c3f, respectively. Additionally, we designate the elementary product between two matrices A and B as A B. Finally, we designate | S | as the size of a set of S. As mentioned above, the feature correlation is of particular interest in our current research. In recent decades, a large branch of feature selection has aimed at reducing redundancy among the selected features, so that the selected feature represents the minimum feature in relation to the other feature."}, {"heading": "2.1. Feature Correlation Measures", "text": "To date, various criteria have been introduced to define the correlation between characteristics: For example, a widely used correlation criterion of the Pearson correlation coefficient (PCC), which measures the linear correlation between variables, can be defined as follows for two characteristic vectors fj and fk: \u03c1 (fj, fk) = cov (fj, fk) \u03c3fj\u03c3fk = 1 n (fj \u2212 \u00b5fj1 \u2032) (fk \u2212 \u00b5fk1 \u2032) \u2032 \u03c3fj\u03c3fk. (1) Note that \u03c1 is a symmetrical measure that moves in [\u2212 1, 1]. If two variables are completely independent of each other, \u03c1 = 0. On the other hand, if the two variables correlate completely, namely one variable can accurately predict another variable, we have other metrics such as the information gain IG (fj | fk) and the symmetrical uncertainty (Yfk) in Lifu (SU) (2003)."}, {"heading": "2.2. Feature Redundancy Reduction", "text": "Based on these correlation yardsticks, several methods have attempted to reduce redundancy between the selected features, for example, in Fast Correlation Based Filters (FCBF) (Yu & Liu, 2003), the meaning of the features and the correlation of the features are assessed using SU measurement. FCBF first selects a number of dominant features relevant to the output labels, then retains the informative features while removing the correlated features using some elegantly designed intuitive rules (Yu & Liu, 2003). Another notable redundancy reduction method is Minimum Redundancy Maximum Relevance (mRMR) (Peng et al, 2005), which selects the most correlated features to the labels so that they are far apart from each other by maximizing the dependence between the common distribution of the selected features and the output labels."}, {"heading": "3. Group Discovery Machine", "text": "In this section we present an efficient method for automatically grouping characteristics with which groups of discriminating but correlated characteristics are identified. Similar to Guyon, 2008, a vector \u03b4 = [\u03b41,.., \u03b4m] \u04320, 1} m is introduced to indicate whether the corresponding characteristic is selected (\u03b4j = 1) or not (\u03b4j = 0), so that the decision-making function is defined as: f (x) = w \u00b2 (x \u03b4), whereby w = [w1, wm] is a weight vector. In order to limit the number of selected characteristics to less than B, the \"0-restriction\" is used to limit the number of selected characteristics to less than B, a \"0-restriction\" is introduced."}, {"heading": "3.1. Correlation Constraints", "text": "To control the correlation between the selected characteristics, we explicitly introduce the following constraint, which states that each selected pair of characteristics should not be correlated as long as its coefficient defined in (1) is not higher than (1), where it is (0, 1). We also define (0, 1) as the domain of \u03b4 (1); (2) defines quadratic constraints with m integer variables, whereby the solution is associated with combinatorial subset selections, which results in high calculation costs, especially for high dimensions m."}, {"heading": "3.2. Proposed Formulation", "text": "For simplicity purposes, we use the square hinge loss in SVM and arrive at the following problem: min-3, g-1, g-2, g-2, g-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-3, c-2, c-c, c-2, c-2, c-c, c-2, c-2, c-2, c-c-c, c-c-c-c, c-c-2, c-c-3, c-3, c-2, c-2, c-2, c-2, c-c, c-c, c-c, c-c-3, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-c, c-c, c-c, c-2, c-2, c-2, c-2, c-2, c-3, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-3, c-2, c-2, c-2, c-3, c-2, c-2, c-2, c-2, c-2, c, c-2, c-2, c-2, c, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-2, c-"}, {"heading": "3.3. Cutting Plane Algorithm", "text": "To tackle this, we must first minimize the inner minimization in (3), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), (Q), Q (Q), Q (Q), Q (Q, Q (Q), Q (Q), Q (Q), Q (Q, Q (Q), Q (Q), Q (Q), Q (Q), Q (Q), Q (Q), Q (Q), (Q), Q (Q), (Q, Q (Q), Q (Q), Q (Q), (Q (Q), Q (Q), (Q (Q), Q (Q), Q (Q (Q), Q (Q), (Q (Q), Q (Q (Q), Q (Q), Q (Q (Q), Q (Q (Q), Q (Q), Q (Q (Q), Q (Q (Q), Q (Q (Q), Q (Q), Q (Q (Q), Q (Q (Q), Q (Q (Q), Q (Q), Q (Q (Q), Q (Q (Q), Q (), Q (Q (Q), Q (), Q (Q (), Q (), Q (Q (), Q (), Q (Q (), Q (), Q (), Q (Q (Q (), Q (), Q (), Q (), Q (Q (Q (, Q (), Q (, Q (), Q (), Q (), Q (Q"}, {"heading": "3.4. Correlation Redundancy Matching", "text": "In this subsection, we discuss the worst-case analysis of the problem (4) (i.e., the equivalent of searching for the most violated constraints), which plays the key role in processing algorithms (Mutapcic & Boyd, 2009). However, in our setting, it is difficult to solve this problem since 1 2 is the solution to the following integer optimization problems: 1 the solution to the following integer optimization problems: 1 the solution to the following integer optimization problems (xi); 2 the solution to the following integer optimization problems: 1 the answer to the question about properties 1 (xi); 2 the answer to the question about properties 1 (xi); 2 the answer to the question about properties 1 (xi); 2 the answer to the question about properties 1 (xi); 2 the answer to the question about properties 1 (xi); 2 the answer to the question about properties 1 (xi)."}, {"heading": "3.5. Complexity Analysis", "text": "Since locating the most violated \u03b4 can be precisely determined by the correlation redundancy matching algorithm, which first sorts the m-characteristics, then scans the B-characteristics, and then calculates the PCC w.r.t. of the other characteristics. In this case, sorting is done by O (m logm) and searching for the support and associated characteristics consumes O (Bmn). Therefore, the total time complexity for CRM for existing T-iterations is O (T (m logm + Bmn). Since there is convergence within 10 iterations for most TB-selected characteristics, the training time complexity to resolve (5) isO (TBn) is."}, {"heading": "4. Experiments", "text": "In this section, we conduct experiments to examine the selection performance of several state-of-the-art methods, including: 1) ReliefF (Robnik-Sikonja & Kononenko, 2003), 2) mRMR2 (Peng et al., 2005), 3) FCBF3 (Yu & Liu, 2003), 4) RCFS (Zhou et al., 2010), 5) SVMRFE (Guyon & Elisseeff, 2003), 6) L1-SVM4 (Yuan et al., 2011), 7) FGM5 (Tan et al., 2010), and 8) our proposed GDM features only using support features for prediction. The first four algorithms belong to filter methods. SVM-RFE is a wrapper method, while the last three methods are embedded. For fair comparisons, all methods except ReliefF are integrated into MATLAB R1b 201b."}, {"heading": "4.1. Evaluation on Synthetic Data", "text": "To illustrate the mechanisms of the proposed method, we first conduct a study of a synthetic dataset in which the correlated characteristics of the soil are known in advance; the data contain 2,048 observations and 10,000 characteristics, and the predefined predictive characteristics are categorized as 200 characteristic groups of different sizes, while others serve as noise agents. In addition, each of the 30 of 200 characteristic groups contains some highly correlated characteristics. While the remaining 170 groups have only one characteristic per group, the predictive ability of each group follows a normal distribution N (0, 1)."}, {"heading": "4.2. Evaluation on Real-world Data", "text": "In fact, most of them are able to play by the rules they have established over the past five years."}, {"heading": "5. Discussion and Conclusion", "text": "In this paper, we have presented a comprehensive study of potentially correlated characteristics leading to the concepts of support characteristics and associated characteristics. While superior predictive performance is achieved through support characteristics, maintaining some feature redundancies as associated characteristics can be useful for improving the interpretation of learning tasks while improving the robustness of predictions. By using the strategy of the cutting plane, the proposed GDM can handle very high-dimensional problems efficiently. Specifically, the associated characteristics in the proposed method are constructed at no additional cost as they are generated together with the support characteristics. Below, with further details on the interpretation of the proposed GDM algorithm, together with the associated characteristics achieved in Figure 6. In terms of the digit recognition result of usps8, the regions highlighted by the associated characteristics (129 characteristics) may be useful to support the group of previously discussed syllables in the group 1, \"wherein the signatory of the initial identification of the user is\" with a large number 1."}], "references": [{"title": "Benefitting from the variables that variable selection discards", "author": ["R. Caruana", "V.R. de Sa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Caruana and Sa,? \\Q2003\\E", "shortCiteRegEx": "Caruana and Sa", "year": 2003}, {"title": "Practical Feature Selection: from Correlation to Causality", "author": ["I. Guyon"], "venue": null, "citeRegEx": "Guyon,? \\Q2008\\E", "shortCiteRegEx": "Guyon", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "Correlation-based Feature Selection for Machine Learning", "author": ["M.A. Hall"], "venue": "PhD thesis, The University of Waikato,", "citeRegEx": "Hall,? \\Q1999\\E", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Optimizing performance measures for feature selection", "author": ["Q. Mao", "I.W. Tsang"], "venue": "In ICDM,", "citeRegEx": "Mao and Tsang,? \\Q2011\\E", "shortCiteRegEx": "Mao and Tsang", "year": 2011}, {"title": "Cutting-set methods for robust convex optimization with pessimizing oracles", "author": ["A. Mutapcic", "S. Boyd"], "venue": "Optimization Methods & Software,", "citeRegEx": "Mutapcic and Boyd,? \\Q2009\\E", "shortCiteRegEx": "Mutapcic and Boyd", "year": 2009}, {"title": "Trace ratio criterion for feature selection", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan"], "venue": "In AAAI,", "citeRegEx": "Nie et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nie et al\\.", "year": 2008}, {"title": "Featureboost: A meta learning algorithm that improves model robustness", "author": ["J. O\u2019Sullivan", "J. Langford", "R. Caruana", "A. Blum"], "venue": "In ICML,", "citeRegEx": "O.Sullivan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "O.Sullivan et al\\.", "year": 2000}, {"title": "Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Theoretical and empirical analysis of relieff and rrelieff", "author": ["M. Robnik-Sikonja", "I. Kononenko"], "venue": "Mach. Learn.,", "citeRegEx": "Robnik.Sikonja and Kononenko,? \\Q2003\\E", "shortCiteRegEx": "Robnik.Sikonja and Kononenko", "year": 2003}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "In ICML,", "citeRegEx": "Tan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2010}, {"title": "Feature selection for high-dimensional data: A fast correlation-based filter solution", "author": ["L. Yu", "H. Liu"], "venue": "In ICML,", "citeRegEx": "Yu and Liu,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu", "year": 2003}, {"title": "Redundancy based feature selection for microarray data", "author": ["L. Yu", "H. Liu"], "venue": "In SIGKDD,", "citeRegEx": "Yu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Yu and Liu", "year": 2004}, {"title": "An improved glmnet for l1-regularized logistic regression and support vector machines", "author": ["Yuan", "G.-X", "Ho", "C.-H", "Lin", "C.-J"], "venue": "In SIGKDD,", "citeRegEx": "Yuan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2011}, {"title": "Advancing feature selection research", "author": ["Z. Zhao", "F. Morstatter", "S. Sharma", "S. Alelyani", "A. Anand", "H. Liu"], "venue": "Technical report,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Feature selection with redundancy-constrained class separability", "author": ["L. Zhou", "L. Wang", "C. Shen"], "venue": "IEEE Trans. Neural Netw.,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}, {"title": "Markov blanket-embedded genetic algorithm for gene selection", "author": ["Z. Zhu", "Y.S. Ong", "M. Dash"], "venue": "Pattern Recognition,", "citeRegEx": "Zhu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "In general, these methods have been categorized as three core themes (Guyon, 2008): filter methods (Yu & Liu, 2003; Peng et al.", "startOffset": 69, "endOffset": 82}, {"referenceID": 8, "context": "In general, these methods have been categorized as three core themes (Guyon, 2008): filter methods (Yu & Liu, 2003; Peng et al., 2005), wrapper methods (Guyon & Elisseeff, 2003; Zhu et al.", "startOffset": 99, "endOffset": 134}, {"referenceID": 16, "context": ", 2005), wrapper methods (Guyon & Elisseeff, 2003; Zhu et al., 2007) and embedded methods (Yuan et al.", "startOffset": 25, "endOffset": 68}, {"referenceID": 13, "context": ", 2007) and embedded methods (Yuan et al., 2011; Tan et al., 2010; Mao & Tsang, 2011).", "startOffset": 29, "endOffset": 85}, {"referenceID": 10, "context": ", 2007) and embedded methods (Yuan et al., 2011; Tan et al., 2010; Mao & Tsang, 2011).", "startOffset": 29, "endOffset": 85}, {"referenceID": 1, "context": "two sets of parameters: parameter of the learning machine, and parameter to control the feature sparsity (Guyon, 2008).", "startOffset": 105, "endOffset": 118}, {"referenceID": 3, "context": "In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012).", "startOffset": 101, "endOffset": 151}, {"referenceID": 1, "context": "In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012).", "startOffset": 101, "endOffset": 151}, {"referenceID": 14, "context": "In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012).", "startOffset": 101, "endOffset": 151}, {"referenceID": 7, "context": "As also discussed in (O\u2019Sullivan et al., 2000; Caruana & de Sa, 2003; Xu et al., 2012), such feature redundancy has the benefits of bringing about stable generalization performances.", "startOffset": 21, "endOffset": 86}, {"referenceID": 8, "context": "Another notable redundancy reduction method is Minimum Redundancy Maximum Relevance (mRMR) (Peng et al., 2005), which selects the most correlated features to the labels such that they are mutually far apart from each other by maximizing the dependency between the joint distribution of the selected features and the output labels.", "startOffset": 91, "endOffset": 110}, {"referenceID": 6, "context": "Next, a feature subset is further identified from the selected features in each cluster group using graph based feature selection criteria (Nie et al., 2008) that capture the global and local intrinsic structures of the data.", "startOffset": 139, "endOffset": 157}, {"referenceID": 6, "context": "Next, a feature subset is further identified from the selected features in each cluster group using graph based feature selection criteria (Nie et al., 2008) that capture the global and local intrinsic structures of the data. This strategy however is heavily sensitive to the choice of graph Laplacian matrices used. For example, the Laplacian score is usually constructed using K nearest neighbor (KNN). In practice, on very high dimensional problems, KNNs can be very far away from each other in reality due to the effect of the curse of dimensionality. Besides, the high computational cost of feature clustering on high dimensional data and graph based methods (taking O(nm)) make this approach less attractive on large scale data. Recently, Zhao et al. (2012) proposed a framework to unify different criteria for removing feature redundancies.", "startOffset": 140, "endOffset": 764}, {"referenceID": 1, "context": "Similar to (Guyon, 2008), a vector \u03b4 = [\u03b41, .", "startOffset": 11, "endOffset": 24}, {"referenceID": 6, "context": "To limit the number of selected features to be less than B, the `0 constraint \u2016\u03b4\u20160 \u2264 B is imposed for the purpose of feature selection (Nie et al., 2008).", "startOffset": 135, "endOffset": 153}, {"referenceID": 10, "context": "Inspired by (Tan et al., 2010), by applying the minimax optimization theory, one can obtain a tight convex relaxation to (3), which is in the form of the following Quadratically Constrained Quadratic Programming (QCQP) problem:", "startOffset": 12, "endOffset": 30}, {"referenceID": 10, "context": "a reduced active constraint set C can be solved by some efficient QCQP solvers (Tan et al., 2010).", "startOffset": 79, "endOffset": 97}, {"referenceID": 10, "context": "The proof can be adapted from (Tan et al., 2010).", "startOffset": 30, "endOffset": 48}, {"referenceID": 8, "context": "In this section, we conduct experiments to study the feature selection performances of several state-of-the-art methods, including: 1) ReliefF (Robnik-Sikonja & Kononenko, 2003), 2) mRMR2 (Peng et al., 2005), 3) FCBF3 (Yu & Liu, 2003), 4) RCFS (Zhou et al.", "startOffset": 188, "endOffset": 207}, {"referenceID": 15, "context": ", 2005), 3) FCBF3 (Yu & Liu, 2003), 4) RCFS (Zhou et al., 2010), 5) SVMRFE (Guyon & Elisseeff, 2003), 6) L1-SVM4 (Yuan et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 13, "context": ", 2010), 5) SVMRFE (Guyon & Elisseeff, 2003), 6) L1-SVM4 (Yuan et al., 2011), 7) FGM5 (Tan et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 10, "context": ", 2011), 7) FGM5 (Tan et al., 2010), and 8) our proposed GDM using only support features for prediction.", "startOffset": 17, "endOffset": 35}, {"referenceID": 10, "context": "Moreover, as FGM imposes a tight convex approximation in the `0-model (Tan et al., 2010), it can be observed from Figure 2(a) that both FGM and GDM achieve competitive accuracy result when the number of selected features approaches the ground truth.", "startOffset": 70, "endOffset": 88}, {"referenceID": 3, "context": "This also implies that GDM can identify a good feature subset (Hall, 1999).", "startOffset": 62, "endOffset": 74}], "year": 2012, "abstractText": "In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method.", "creator": "LaTeX with hyperref package"}}}