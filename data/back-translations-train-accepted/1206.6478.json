{"id": "1206.6478", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Maximum Margin Output Coding", "abstract": "In this paper we study output coding for multi-label prediction. For a multi-label output coding to be discriminative, it is important that codewords for different label vectors are significantly different from each other. In the meantime, unlike in traditional coding theory, codewords in output coding are to be predicted from the input, so it is also critical to have a predictable label encoding.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (347kb)", "http://arxiv.org/abs/1206.6478v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yi zhang 0010", "jeff g schneider"], "accepted": true, "id": "1206.6478"}, "pdf": {"name": "1206.6478.pdf", "metadata": {"source": "META", "title": "Maximum Margin Output Coding", "authors": ["Yi Zhang", "Jeff Schneider"], "emails": ["yizhang1@cs.cmu.edu", "schneide@cs.cmu.edu"], "sections": [{"heading": null, "text": "In order to find output codes that are both discriminatory and predictable, we first propose a maximum margin formulation that captures these two characteristics in a natural way, and then convert them into a metric learning formulation, but with an exponentially large number of constraints common to structured prediction problems. Without a labeling structure for comprehensible conclusions, we use over-generating (i.e. relaxation) techniques combined with the cutting plane method for optimization. In our empirical study, the proposed output coding scheme outperforms a variety of existing multi-label prediction methods for image, text and music classification."}, {"heading": "1. Introduction", "text": "In traditional channel encoding (Cover & Thomas, 1991; Costello & Forney, 2007), a message is encoded into an alternative (and usually redundant) representation so that it can be accurately restored after being transmitted through a noisy channel. Error Correction Output Techniques (ECOC) apply the idea of channel encoding to multi-class classification (Dietterich & Bakiri, 1995; appearing in proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012). Copyright 2012 by the author (s) / owner (s). Allwein et al.) and more recently to multi-class prediction (Hsu et al., 2009; Tai & Lin, 2011): We encode the output into a codeword, learn models for predicting the code word to predict the codeword, and recover the correct output of noisy predictions."}, {"heading": "2. Multi-Label Output Codes: Framework and Existing Methods", "text": "In this section, we present the general framework for multi-label edition coding and review three recently proposed output codes (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011) where coding is based on random projections, principal component analyses, or canonical correlation analyses."}, {"heading": "2.1. Framework", "text": "In this case we can encode the encoding transformations in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner and manner in which they are able, in the manner in which they are in the manner and manner in which they are able, in the manner in which they are in the manner and manner in which they are able, in the manner in the manner in which they are in the manner and manner in the manner in which they are able, in the manner in the manner in the manner in which they are in the manner and manner in the manner in which they are able, in the manner in the manner in which they are in the manner and manner in the manner in the manner in which they are able, in the manner in the manner in which they are in the manner and manner in the manner in which they are in the manner in the manner in the manner and manner in which they are able, in the manner in the manner in the manner in the manner in which they are in the manner and manner in the manner in the manner in which they are in the manner in the manner in the manner in the manner in the manner in which they are and manner in the manner in the manner in the manner in which they are in the manner in the manner in the manner in the manner in the manner in which they are and in the manner in the manner in the manner in the manner in the manner in which they are and in the manner in the manner in the manner in the manner in the manner in which they are and in the manner in the manner in the manner in the manner in the manner in which they are and in the manner in the manner in the manner in the manner in the manner in the manner in the manner in"}, {"heading": "2.2. Coding with compressed sensing", "text": "For encryption, each projection vector is randomly generated with Gaussian or Bernoulli entries, so that the code word z = (vT1 y,.., v T d y) T random predictions of the label vector y.Note: The decoding follows the sparse approximation algorithms used in compressed capture. Two popular classes are convex relaxation such as \"1 penalizes least squares (Drop, 2006), and iterative greedy decoding such as CoSaMP (Needell & Tropp, 2008), which is effective only when the word\" 1 penalizes least square \"(Drop, 2006), and iterative greedy decoding such as CoSaMP (Needell & Tropp, 2008)."}, {"heading": "2.3. Coding with principal component analysis", "text": "Considering the n-training examples, the transformation of the principle of the label space (Tai & Lin, 2010) uses the uppermost d-main components in the label space as encoding projectors. The code word z = (vT1 y,.., v T y) T contains the uppermost d-coordinates of y in the main component space. Considering the predicted code word z = (m-1 (x),... m-d (x) T for a test sample x, the encoding is performed by projecting z-y back to the coordinates in the original label space and then rounding it up to 0s and 1s: y-round (Vz-1) (13). Note that coding with major components may produce discriminatory code words."}, {"heading": "2.4. Coding with canonical correlation analysis", "text": "Predictability for multi-label output codes is discussed in the latest paper (Zhang & Schneider, 2011), where output projections are determined by canonical correlation analysis. CCA tries to find an input projection u-Rp in the feature space and an output projection v-Rq in the label space, so that the projected variables uTx and vTy are maximally correlated: argmax u-Rp, v-RquTXTYv (uTXTXu) (vTYv) (14) This can be solved as a generalized eigenvalue problem, and the uppermost d pairs of eigenvectors {(uk, vk)} dk = 1 contain the coding requirements {vk} dk = 1.The codeword in this method is z = (y1,., yq, v T 1 y-labels,."}, {"heading": "3. Maximum Margin Output Coding", "text": "In this section, we propose a maximum margin encoding scheme where encoding transforms the discriminability and predictability of code words."}, {"heading": "3.1. A Max-Margin Formulation", "text": "As before, codewords are predicted using regression: M (x) = (m) q (x),..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.2. Metric Learning Formulation", "text": "Problem (22) is a quadratic program with quadratic constraints, and we first turn it into a metric learning problem. Let us define the q \u00b7 q matrix Q: Q = VVT (23), which is the Mahalanobis distance metric induced by V. Let us also define a series of new attribute vectors: \u03c6iy = P Tx (i) \u2212 y, in the form of a metric learning problem as follows: argmin Q-S + q, {0, 1} ni = 11 2 tracks (Q) + C n (i)."}, {"heading": "3.3. Incorporating Original Labels and Their Classifiers", "text": "As shown in eq. (3), the codeword may also contain q = q = = Q = = (y1,.., yq, v T 1 y,.., v T d y) T. In this case, the coding of Qj = 1 can be learned to predict original terms as in (5), and the decoding algorithm can use both regression and classification results, e.g. as in eq. (15). In this case, the coding projection should also use the original terms (y1,..., yq) in the codeword Tix, so that the projection part (vT1 y y,.) can provide complementary information. In order to adapt our Max Margin formulation (25) to this new information, we assume that classifiers {p, j} qj = 1 have already been learned, and therefore we know the classification results for each sample."}, {"heading": "3.4. Cutting Plane Method with Overgenerating", "text": "In this section, we look at how to solve the problem (27), which involves an exponentially large number of constraints (27) that could be solved due to the combinatorial nature of the marker space (0, 1) (27). As in the structured prediction (Tsochantaridis et al., 2004; Taskar et al. (Tsochantaridis et al., 2004), the problem (27) could be solved efficiently if a computer-tractable separate prediction exists to determine which of the exponentially many constraints is most violated (Tsochantaridis et al., 2004). However, without a specific structure (e.g. a chain or tree) in the marker space to enable efficient conclusions, the separate oracle for the problem (27) is computer-intractable. To address this problem, we use overgenerations (i.e., relation) (Fins & Joachim2008)."}, {"heading": "3.5. Encoding and Decoding", "text": "After solving Q in (31), one gets encoding projections as (26), and one can select d, the number of projections, by keeping only the first d columns of V in (26) for each d \u2264 q. The codeword as in (3) contains original names, and the encoding is performed as (15)."}, {"heading": "4. Empirical Study", "text": "In fact, it is that it is a matter of a way in which people put themselves and themselves at the centre, in which they put themselves and the world at the centre. (...) It is not that they feel they are able to understand themselves. (...) It is not that they are able to understand themselves. (...) It is not that they are able to identify themselves. (...) It is not that they are able to identify themselves. (...) It is not that they are able to identify themselves. (...) It is that they are not able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is not as if they want to. \"(...) It is not as if they want to. (...) It is not as if they want to.\" (...) It is as if they do not want to. \"(...) It is as if they do not want to."}, {"heading": "5. Related Work", "text": "Our work follows the direction of multi-label output encoding (Hsu et al., 2009) and is motivated by the recent success of encoding with PCA (Tai & Lin, 2010) and CCA (Zhang & Schneider, 2011) and their links to code removal and code predictability. Our maximum margin formulation is being transformed into a metric learning problem, as in (Weinberger et al., 2006), but with a metric defined for the label space and an exponential number of constraints caused by label combinations."}, {"heading": "6. Conclusion", "text": "Discriminability and predictability are equally important for output codes. In this paper, we propose a maximum margin formulation for multi-label output coding that promotes both discriminatory and predictable codes. We transform this formulation into a metric learning problem in the label space, combining over-generation with the cutting plane method for optimization. Our method surpasses many existing methods for multi-label image, text and music records."}, {"heading": "Allwein, Erin L., Schapire, Robert E., and Singer, Yoram.", "text": "Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers. JMLR, 1: 113-141, 2001.Candes, E. J. Compressive Sampling. In Proceedings of International Congress of Mathematicians, 2006."}, {"heading": "Costello, Daniel J. and Forney, G. David. Channel coding:", "text": "The Road to Channel Capacity. Proceedings of the IEEE, 95 (6): 1150-1177, 2007.Cover, Thomas M. and Thomas, Joy A. Elements of Information Theory. Wiley-Interscience, New York, 1991.Dietterich, Thomas G. and Bakiri, Ghulum. Solving multiclass learning problems via error-correcting output codes. JAIR, 2: 263-286, 1995.Donoho, D. L. Compressed Sensing. IEEE Trans. Information Theory, 52 (4): 1289-1306, 2006.Finley, Thomas and Joachims, Thorsten. Training structural svms when exact inference is intractable. In ICML, pp. 304-311, 2008."}, {"heading": "Fu\u0308rnkranz, Johannes, Hu\u0308llermeier, Eyke, Loza Menc\u0301\u0131a,", "text": "Eneldo, and Brinker, Klaus. Multi-label classification via calibrated label ranking. Mach. Learn., 73 (2): 133-153, 2008."}, {"heading": "Hsu, Daniel, Kakade, Sham, Langford, John, and Zhang,", "text": "Tong. Multi-label prediction via compressed sensing. In NIPS, pp. 772-780. 2009.Needell, D. and Tropp, J. A. Cosamp: Iterative signal extraction from incomplete and inaccurate samples. Applied and computerized harmonic analysis, 26 (3), 2008.Tai, Farbound and Lin, Hsuan-Tien. Multi-label classification with principal label space transformation. In Second International Workshop on learning from MultiLabel Data. 2010."}, {"heading": "Taskar, Benjamin, Guestrin, Carlos, and Koller, Daphne.", "text": "Max-margin markov networks. In NIPS, 2003.Tropp, Joel A. Just relax: convex programming methods for identification sparse signals in noise. IEEE Transactions on Information Theory, 52 (3): 1030-1051, 2006.Tsochantaridis, Ioannis, Hofmann, Thomas, Joachims, Thorsten, and Altun, Yasemin. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004."}, {"heading": "Weinberger, K.Q., Blitzer, J., and Saul, L. Distance metric", "text": "Learning for large margin next neighbor class classification. In NIPS. 2006."}, {"heading": "Zhang, Min-Ling and Zhang, Kun. Multi-label learning by", "text": "Exploiting label dependence. In KDD, 2010. Zhang, Yi and Schneider, Jeff. Multi-label output codes using canonical correlation analysis. In AISTATS, 2011."}], "references": [{"title": "Reducing multiclass to binary: a unifying approach for margin", "author": ["Allwein", "Erin L", "Schapire", "Robert E", "Singer", "Yoram"], "venue": "classifiers. JMLR,", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "Compressive Sampling", "author": ["E.J. Candes"], "venue": "In Proceedings of International Congress of Mathematicians,", "citeRegEx": "Candes,? \\Q2006\\E", "shortCiteRegEx": "Candes", "year": 2006}, {"title": "Channel coding: The road to channel capacity", "author": ["Costello", "Daniel J", "Forney", "G. David"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Costello et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Costello et al\\.", "year": 2007}, {"title": "Solving multiclass learning problems via error-correcting output", "author": ["Dietterich", "Thomas G", "Bakiri", "Ghulum"], "venue": "codes. JAIR,", "citeRegEx": "Dietterich et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1995}, {"title": "Compressed Sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Training structural svms when exact inference is intractable", "author": ["Finley", "Thomas", "Joachims", "Thorsten"], "venue": "In ICML, pp", "citeRegEx": "Finley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finley et al\\.", "year": 2008}, {"title": "Multilabel classification via calibrated label ranking", "author": ["F\u00fcrnkranz", "Johannes", "H\u00fcllermeier", "Eyke", "Loza Men\u0107\u0131a", "Eneldo", "Brinker", "Klaus"], "venue": "Mach. Learn.,", "citeRegEx": "F\u00fcrnkranz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "F\u00fcrnkranz et al\\.", "year": 2008}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Langford", "John", "Zhang", "Tong"], "venue": "In NIPS, pp", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Needell and Tropp,? \\Q2008\\E", "shortCiteRegEx": "Needell and Tropp", "year": 2008}, {"title": "Multi-label classification with principal label space transformation", "author": ["Tai", "Farbound", "Lin", "Hsuan-Tien"], "venue": "In Second International Workshop on learning from MultiLabel Data", "citeRegEx": "Tai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2010}, {"title": "Max-margin markov networks", "author": ["Taskar", "Benjamin", "Guestrin", "Carlos", "Koller", "Daphne"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Just relax: convex programming methods for identifying sparse signals in noise", "author": ["Tropp", "Joel A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp and A.,? \\Q2006\\E", "shortCiteRegEx": "Tropp and A.", "year": 2006}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Tsochantaridis", "Ioannis", "Hofmann", "Thomas", "Joachims", "Thorsten", "Altun", "Yasemin"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L. Saul"], "venue": "In NIPS", "citeRegEx": "Weinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2006}, {"title": "Multi-label learning by exploiting label dependency", "author": ["Zhang", "Min-Ling", "Kun"], "venue": "In KDD,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Multi-label output codes using canonical correlation analysis", "author": ["Zhang", "Yi", "Schneider", "Jeff"], "venue": "In AISTATS", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": ", 2001) and more recently to multi-label prediction (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011): we encode the output into a codeword, learn models to predict the codeword, and then recover the correct output from noisy predictions.", "startOffset": 52, "endOffset": 112}, {"referenceID": 7, "context": "Then we review three recently-proposed output codes (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011), where the encoding is based on random projections, principal component analysis and canonical correlation analysis, respectively.", "startOffset": 52, "endOffset": 112}, {"referenceID": 7, "context": "Following previous work (Hsu et al., 2009; Tai & Lin, 2010; Zhang & Schneider, 2011), we consider linear encoding.", "startOffset": 24, "endOffset": 84}, {"referenceID": 7, "context": ", relaxing y into a continuous domain and then rounding the relaxed solution (Hsu et al., 2009; Tai & Lin, 2010) or using approximate inference (Zhang & Schneider, 2011).", "startOffset": 77, "endOffset": 112}, {"referenceID": 7, "context": "Multi-label compressed sensing (Hsu et al., 2009) is one of the earliest works that formally defines a multilabel output code.", "startOffset": 31, "endOffset": 49}, {"referenceID": 4, "context": ", d) is randomly generated as in compressed sensing (Donoho, 2006; Candes, 2006), e.", "startOffset": 52, "endOffset": 80}, {"referenceID": 1, "context": ", d) is randomly generated as in compressed sensing (Donoho, 2006; Candes, 2006), e.", "startOffset": 52, "endOffset": 80}, {"referenceID": 12, "context": "As studied in structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), problem (27) could be solved efficiently, e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 10, "context": "As studied in structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), problem (27) could be solved efficiently, e.", "startOffset": 36, "endOffset": 86}, {"referenceID": 12, "context": ", by the cutting-plane method, if a computationally tractable separate oracle exists to determine which of the exponentially many constraints is most violated (Tsochantaridis et al., 2004).", "startOffset": 159, "endOffset": 188}, {"referenceID": 7, "context": "\u2022 Coding with compressed sensing (CodingCS) (Hsu et al., 2009).", "startOffset": 44, "endOffset": 62}, {"referenceID": 6, "context": "\u2022 Calibrated label ranking (CLR) (F\u00fcrnkranz et al., 2008).", "startOffset": 33, "endOffset": 57}, {"referenceID": 7, "context": "Our work follows the direction of multi-label output coding (Hsu et al., 2009) and is motivated by the recent success of coding with PCA (Tai & Lin, 2010) and CCA (Zhang & Schneider, 2011) and their connections to code distance and code predictability.", "startOffset": 60, "endOffset": 78}, {"referenceID": 13, "context": "Our maxmargin formulation is converted into a metric learning problem, as in (Weinberger et al., 2006), but with a metric defined for the label space and an exponential number of constraints caused by label combinations.", "startOffset": 77, "endOffset": 102}, {"referenceID": 12, "context": "The optimization technique developed for structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), more specically the cutting plane method with overgenerating (Finley & Joachims, 2008), is used to solve our metric learning problem.", "startOffset": 63, "endOffset": 113}, {"referenceID": 10, "context": "The optimization technique developed for structured prediction (Tsochantaridis et al., 2004; Taskar et al., 2003), more specically the cutting plane method with overgenerating (Finley & Joachims, 2008), is used to solve our metric learning problem.", "startOffset": 63, "endOffset": 113}], "year": 2012, "abstractText": "In this paper we study output coding for multi-label prediction. For a multi-label output coding to be discriminative, it is important that codewords for different label vectors are significantly different from each other. In the meantime, unlike in traditional coding theory, codewords in output coding are to be predicted from the input, so it is also critical to have a predictable label encoding. To find output codes that are both discriminative and predictable, we first propose a max-margin formulation that naturally captures these two properties. We then convert it to a metric learning formulation, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. Without a label structure for tractable inference, we use overgenerating (i.e., relaxation) techniques combined with the cutting plane method for optimization. In our empirical study, the proposed output coding scheme outperforms a variety of existing multi-label prediction methods for image, text and music classification.", "creator": "LaTeX with hyperref package"}}}