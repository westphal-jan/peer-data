{"id": "1206.6479", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "The Landmark Selection Method for Multiple Output Prediction", "abstract": "Conditional modeling x \\to y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset y_L of the dimensions of y, and proceed by modeling (i) x \\to y_L and (ii) y_L \\to y. Composing these two models, we obtain a conditional model x \\to y that possesses convenient statistical properties. Multi-label classification and multivariate regression experiments on several datasets show that this model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (170kb)", "http://arxiv.org/abs/1206.6479v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["krishnakumar balasubramanian", "guy lebanon"], "accepted": true, "id": "1206.6479"}, "pdf": {"name": "1206.6479.pdf", "metadata": {"source": "CRF", "title": "The Landmark Selection Method for Multiple Output Prediction", "authors": ["Krishnakumar Balasubramanian", "Guy Lebanon"], "emails": ["krishnakumar3@gatech.edu", "lebanon@cc.gatech.edu"], "sections": [{"heading": "1. Introduction", "text": "Specific cases include classification, where y is a discrete random variable, and regression, where y is a continuous random variable. Much of the attention in recent years has focused on the case where x is a high-dimensional vector. In this case, traditional statistical methods are inefficient due to overmatch. Proposed alternatives for high-dimensional x include power selection and regulated models. Instead, we consider the case of a high-dimensional y, where x is either low-dimensional or high-dimensional. The basic approach in this case is to construct models independently x 7 \u2192 yi dimensional R for i = 1,., k (assuming y is a k-dimensional real vector). This approach has the advantage of drawing from a wide variety of available individual production models, including linear and non-linear regression, logistic regression and supporting vector machines."}, {"heading": "2. Related Work", "text": "In the regression situation, most approaches focus on the retirement of the regression matrix or input space sharing. For example, (Izenman, 1975) a low retirement of the regression matrix was introduced, which is directly applicable to the multi-output prediction. (Yuan & Lin, 2006) Although these methods are popular and widely applicable, they do not model correlations in the high dimensions that can be used to reduce the complexity of the problem."}, {"heading": "3. The landmark selection method:", "text": "We consider a general scenario in which x-Rd and y-Y, where Y is either Rk (regression) or {0, 1} k (classification), and k, d are high dimensional. We designate the data matrices containing n-th examples, of which X-Rn \u00b7 d and Y-Rn \u00b7 k. Step 1: Selecting the map L and modeling (2) A convenient way to select the map L, and to model (2) is simultaneously the following regularized least square regression modelA (argmin A)."}, {"heading": "4. Theory", "text": "In this section, we provide a brief theoretical analysis of the proposed approach in the regression, in which we can highlight the advantages of the proposed approach. (We assume that there is a true \"Landmark\" subset under which there could be a \"Landmark\" solution. (We assume that there is a \"Landmark\" ------------------L \"-Rn and\" Y \"-Rn\" --Rn \"--K\" -Rn \"-K\" -K \"-Rn\" -K \"-K\" -Rn \"-K\" -Rn \"-K (\" -K \"-K\" -K \"-Rn (\" -K \"-K\" -K \"-K (\" -K \"-K\" -K \"-Rn) (\" -K \"-K\" (\"-K\" -K \"(\" n) (\"K\" -K \"-K\" (\"-K\" n) (\"K\" -K (\"-K\" -Rn) (\"K\" -Rn) (\"K\" -K \"-Rn\" -K \"-K\" -K \"-K\" -K \"-K\" -K \"-K\" -K \"-K\" -Rn \"-K\" -K \"-K\" -K \"-K\" -K \"-Rn\" -K (\"-K\" -K \"-K\" -K \"-K\" n) (\"Rn) (\" -K \"-K\" -K \"-K\" -K \"-K\" -K \"-K\" -K \"-K\" -K \"-K\" -K \"-K\" -K \"-Rn\" -Rn \"-Rn\" (\"-Rn) (\" K \"Rn\" K \"K\" -Rn) (\"Rn) (\" K \"K\" K \"K\" K \"K\" Rn) (\"Rn\" Rn \"Rn\" K \"K\" K \"K\" K \"K\" K \"-Rn) (\" Rn \"K\" K \"K\" Rn) (\"K\" K \"Rn)."}, {"heading": "5. Optimization procedure", "text": "The spaRSA method, recently proposed in (Wright et al., 2009), is a solution to the optimization problems of the formin-a-Rp-f (a) + \u03bb\u03c6 (a), where f is a convex loss function and \u03c6 is a convex regulator. The main advantage of the spaRSA method is that if the regulator is group splittable, the problem will disintegrate across the group. Using vectorization and block diagonalization, it can be shown that (3) falls within this framework. Initially, the block diagonalization operation appears to complicate the solver by increasing the size of the data matrix. However, below we describe a variation of the spaRSA form that works directly with the Y and A. A similar approach has been applied in (Sprechmann et al., 2011) for the problem of collaborative dictional learning with hierarchical punishment."}, {"heading": "6. Experiments", "text": "In this section, we compare our Landmark Selection Method, hereinafter referred to as Moplms, with alternative baselines to classification and regression problems. In our experiments, we used code from (Wright et al., 2009) to perform mixed standard selection (group lasso and lasso) Landmark selection. Regularization parameters were determined by cross-validation."}, {"heading": "6.1. Synthetic experiments", "text": "We conducted an experiment on synthetic regression data with k = 500 (dimensionality of y), d = 500 (dimensionality of x). The number of landmark outputs s was varied in quantity {50, 100, 200}. Data were simulated from the above model, including the specified landmark error outputs. Figure 1 (left) shows the action of the test MSE prediction errors as a function of the sample size for different values of the parameter s / k. From Section 4 we have that if the landmark output selection methods are not used, with a linear regression model for x \u2192 y, the Frobenious standard errors between the true and estimated matrix scales as O (kd n). Where error 1 is scaled as with the landmark output selection methods, the landmark selection method is not used, with a matrix scale model as the norm error O (the Frobenious error, the Frobenix-ious error)."}, {"heading": "6.2. Real-world data sets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1. Classification", "text": "We experimented with the following two multiple output classification datasets.1. del.icio.us This dataset consists of data from del.icio.us, a social bookmarking site that labels web pages with multiple contextual tags. It contains about 16,000 labeled web pages and 983 unique labels. We follow the experimental setup followed by (Hsu et al., 2009) and represent web pages as a Boolean word threshold vector, using a combination of frequency threshold and a ranking of properties, resulting in 500 features.2. Image dataset contains 68,000 images, with about 22,000 unique word thresholds for each image. Following (Hsu et al., 2009) we retained the 1000 most common labels. We represented each image via codes compressed with a learned dictionary (size 1024) over a sparse encoding (Yang et al., 2009) we compressed 10 of Hamfish specifications."}, {"heading": "6.2.2. Regression", "text": "In the regression environment, we consider the prediction of the share prices of several companies based on prior values on the so-called \"Landmark Select Approach\" based on the SP 500 dataset. Specifically, the data are closing prices of the 500 companies in the S & P Index for the period from August 21, 2009 to August 20, 2010 (a total of 245 entries). We assume the following auto-regressive 1 or AR (1) -modeling L = Byt \u2212 1L + E (6), whereby log StSt is the share price at the time t) for day t and E is the noise settings. The problem is motivated by the observation in the financial world that several companies have stock prices that share identical stochastic trends (cointegration). We compare our Landmark Settings approach to the low-level multivariate settings settings settings settings settings (using the spurnorm regression settings) and the group-based settings settings we use as settings in our settings settings settings."}, {"heading": "7. Discussion", "text": "By selecting a subset of the output dimensions (landmarks) and focusing on modelling the dependence of this subset of y on x, we significantly reduce sample complexity, which is most noticeable when the output dimensionality is high and the various components have a high correlation. Our experiments show that the proposed method exceeds the standard multi-output methods in both classification and regression scenarios. Results in this paper raise several interesting questions and follow instructions. First, a detailed analysis is required to characterize the improvements in the proposed methods over competing methods. Second, it is interesting to consider cases where the label vector exhibits a pattern of inconsistency."}], "references": [{"title": "Multi-label classification on tree and dag structured hierarchies", "author": ["W. Bi", "J. Kwok"], "venue": null, "citeRegEx": "Bi and Kwok,? \\Q2011\\E", "shortCiteRegEx": "Bi and Kwok", "year": 2011}, {"title": "Predicting multivariate responses in multiple linear regression", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the Royal Statistical Society:B,", "citeRegEx": "Breiman and Friedman,? \\Q1997\\E", "shortCiteRegEx": "Breiman and Friedman", "year": 1997}, {"title": "Incremental algorithms for hierarchical classification", "author": ["N. Cesa-Bianchi", "C. Gentile", "L. Zaniboni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": null, "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["A.J. Izenman"], "venue": "Journal of multivariate analysis,", "citeRegEx": "Izenman,? \\Q1975\\E", "shortCiteRegEx": "Izenman", "year": 1975}, {"title": "Nonparametric regression and classification with joint sparsity constraints", "author": ["H. Liu", "J. Lafferty", "L. Wasserman"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2008}, {"title": "I. support union recovery in high-dimensional multivariate regression", "author": ["G. Obozinski", "M.J. Wainwright", "M. Jordan"], "venue": "Annals of statistics,", "citeRegEx": "Obozinski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2011}, {"title": "Multivariate reducedrank regression: theory and applications", "author": ["G.C. Reinsel", "R.P. Velu"], "venue": null, "citeRegEx": "Reinsel and Velu,? \\Q1998\\E", "shortCiteRegEx": "Reinsel and Velu", "year": 1998}, {"title": "In defense of one-vs-all classification", "author": ["R. Rifkin", "A. Klautau"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Rifkin and Klautau,? \\Q2004\\E", "shortCiteRegEx": "Rifkin and Klautau", "year": 2004}, {"title": "Estimation of highdimensional low-rank matrices", "author": ["A. Rohde", "A.B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Rohde and Tsybakov,? \\Q2011\\E", "shortCiteRegEx": "Rohde and Tsybakov", "year": 2011}, {"title": "C-hilasso: A collaborative hierarchical sparse modeling framework", "author": ["P. Sprechmann", "I. Ram\u0131\u0301rez", "G. Sapiro", "Y.C. Eldar"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Sprechmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2011}, {"title": "Multi-label classification with principle label space transformation", "author": ["F. Tai", "H.T. Lin"], "venue": "Neural Computation,", "citeRegEx": "Tai and Lin,? \\Q2012\\E", "shortCiteRegEx": "Tai and Lin", "year": 2012}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2006}, {"title": "Mining multi-label data. Data mining and knowledge discovery handbook", "author": ["G. Tsoumakas", "I. Katakis", "Vlahavas"], "venue": null, "citeRegEx": "Tsoumakas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2010}, {"title": "Decision trees for hierarchical multilabel classification", "author": ["C. Vens", "J. Struyf", "L. Schietgat", "S. D\u017eeroski", "H. Blockeel"], "venue": "Machine Learning,", "citeRegEx": "Vens et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vens et al\\.", "year": 2008}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "For example (Izenman, 1975) introduced low-rank penalization of the regression matrix, which was analyzed in (Reinsel & Velu, 1998) in the low-dimensional setting.", "startOffset": 12, "endOffset": 27}, {"referenceID": 13, "context": "A summary of improvements over the one-versus-all method is available in (Tsoumakas et al., 2010).", "startOffset": 73, "endOffset": 97}, {"referenceID": 2, "context": "Alternative approaches assume a class hierarchy on the output space (Cesa-Bianchi et al., 2006), graph structure on the output space (Vens et al.", "startOffset": 68, "endOffset": 95}, {"referenceID": 14, "context": ", 2006), graph structure on the output space (Vens et al., 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 12, "context": ", 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al., 2006).", "startOffset": 90, "endOffset": 119}, {"referenceID": 3, "context": "A paper related to our proposed method is (Hsu et al., 2009), which consider multi-label prediction in a sparse high-dimensional output space.", "startOffset": 42, "endOffset": 60}, {"referenceID": 3, "context": "Furthermore, the approach by (Hsu et al., 2009) might not be applicable in the regression setting, as output sparsity assumption does not hold for regression in practice.", "startOffset": 29, "endOffset": 47}, {"referenceID": 3, "context": "Recently proposed variations on (Hsu et al., 2009) include (Tai & Lin, 2012) that propose to reduce the dimensionality of the output space by PCA, and (Bi & Kwok, 2011) that propose to reduce the label space by preserving a graph structure hierarchy on y.", "startOffset": 32, "endOffset": 50}, {"referenceID": 19, "context": "Our approach also has a close connection to sparse PCA (Zou et al., 2006).", "startOffset": 55, "endOffset": 73}, {"referenceID": 5, "context": "Handling non-linear output relationship: In order to select and learn non-linear relationships between the outputs and the landmarks, one could use functional joint sparsity models with L1/L\u221e constraints as proposed by (Liu et al., 2008) or with L1+ L1/L2 constraints (appropriately defined on a function space).", "startOffset": 219, "endOffset": 237}, {"referenceID": 6, "context": "developed in (Obozinski et al., 2011) for random design linear regression with group Lasso regularization, we can get a lower bound on the number of samples needed for recovering the support of the subset L of the landmark labels.", "startOffset": 13, "endOffset": 37}, {"referenceID": 6, "context": "The proof follows from the corresponding proof in (Obozinski et al., 2011).", "startOffset": 50, "endOffset": 74}, {"referenceID": 15, "context": "The spaRSA method, proposed recently in (Wright et al., 2009), is a solver for optimization problems of the form", "startOffset": 40, "endOffset": 61}, {"referenceID": 10, "context": "A similar approach was used in (Sprechmann et al., 2011) for the problem of collaborative dictionary learning with hierarchical penalty.", "startOffset": 31, "endOffset": 56}, {"referenceID": 15, "context": "We refer the reader to (Wright et al., 2009) for a complete description of the general procedure.", "startOffset": 23, "endOffset": 44}, {"referenceID": 10, "context": "The solutions for each of these sub-problems are available in closed form (similar to (Sprechmann et al., 2011)) as follows:", "startOffset": 86, "endOffset": 111}, {"referenceID": 15, "context": "In our experiments we used code from (Wright et al., 2009) for performing the mixed norm penalty (group lasso and lasso) landmark selection.", "startOffset": 37, "endOffset": 58}, {"referenceID": 3, "context": "Multilabel compressive sensing (mlcs): This approach was proposed in (Hsu et al., 2009) where the label vector is projected to a random m dimensional sub-space followed by regression on the compressed subspace.", "startOffset": 69, "endOffset": 87}, {"referenceID": 3, "context": "We follow the experimental setup followed in (Hsu et al., 2009) and represent web page as a boolean bag-of-words vector, with the vocabulary chosen using a combination of frequency thresholding and \u03c7 feature ranking, resulting in 500 features.", "startOffset": 45, "endOffset": 63}, {"referenceID": 3, "context": "Following (Hsu et al., 2009) we retained the 1000 most frequent labels.", "startOffset": 10, "endOffset": 28}, {"referenceID": 16, "context": "We represented each image via codes computed with a learned dictionary (of size 1024) via sparse coding (Yang et al., 2009).", "startOffset": 104, "endOffset": 123}], "year": 2012, "abstractText": "Conditional modeling x 7\u2192 y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset yL of the dimensions of y, and proceed by modeling (i) x 7\u2192 yL and (ii) yL 7\u2192 y. Composing these two models, we obtain a conditional model x 7\u2192 y that possesses convenient statistical properties. Multi-label classification and multivariate regression experiments on several datasets show that this method outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}