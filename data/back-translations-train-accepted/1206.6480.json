{"id": "1206.6480", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Dantzig Selector Approach to Temporal Difference Learning", "abstract": "LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, L1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are well-suited for high-dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed--point problem, its integration with L1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (549kb)", "http://arxiv.org/abs/1206.6480v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matthieu geist", "bruno scherrer", "alessandro lazaric", "mohammad ghavamzadeh"], "accepted": true, "id": "1206.6480"}, "pdf": {"name": "1206.6480.pdf", "metadata": {"source": "META", "title": "A Dantzig Selector Approach to Temporal Difference Learning", "authors": ["Matthieu Geist", "Bruno Scherrer", "Alessandro Lazaric", "Mohammad Ghavamzadeh"], "emails": ["matthieu.geist@supelec.fr", "bruno.scherrer@inria.fr", "firstname.lastname@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "2. LSTD and Related Work", "text": "A Markov reward process1 (MRP) is a tuple {S, P, R, \u03b3} in which S is a finite state space, P = (p (s) | s) 1 \u2264 s, s '\u2264 | S | is the transition matrix, R = (r (s) 1 \u2264 s \u2264 | S | with B = R \u2264 rmax is the reward vector and \u03b3 is a discount factor. Value function V is defined as the expected cumulative reward from a given state s (i.e. the reward R and the transitions P) are unknown and only a series of n transitions {(si, ri, s \u2032 i) 1 \u2264 i \u2264 n}. In many practical applications, the model of MRP (i.e. the reward R and the transitions P) is unknown, and only a series of n transitions {s (si, s \u2032 i) p is available."}, {"heading": "2.1. LSTD", "text": "If D\u00b5 is the diagonal matrix with the elements micro (s) and micro (p), it can easily be extended to Markovian decision-making processes, which are reduced to MRPs for fixed policies. Function is the fixed point of the Bellman operator T, the LSTD algorithm calculates the fixed point of the common micro-T operator: V (p). Let us define A (p) and b (p) as the fixed point of the Bellman operator T and b (b) as the fixed point of the common micro operator: V (p). In the following, we assume that A and M\u00b5 (p) are indispensable. It can be shown by simple algebra that V (p) is the fixed point of the micro (p)."}, {"heading": "2.2. Related Work", "text": "If the number of samples is close to or less than the number of characteristics, the matrix A-2 can be summarized as the number of characteristics. (This matrix A-2 is insufficiently conditioned and some form of regulation should be applied to solve the LSTD problem. (The formulation of LSTD in Eq. 1 is particularly helpful for understanding the different regulatory programs that could be applied to LSTD.) Specifically, each of the minimizations relating to the operators is adjustable and T can therefore be regulated. (The formulation of LSTD in Eq. 1 is particularly helpful for understanding the different regulatory programs that could be applied to LSTD. (Each of the minimizations relating to the operators and T can be regulated so as to obtain: {S-2 = argmino-2 = argmino-3).With this formulation, all of the regulation programs relating to LSTD \"(except STD-1) can be summarized as the number of characteristics."}, {"heading": "3. Dantzig-LSTD", "text": "The Dantzig LSTD (short for D-LSTD) algorithm, which we propose in this essay, provides an estimate \u03b8d (i.e., a value function V\u03b8d, \u03bb) with a low \"1 standard under the condition that the Bellman rest (R-LSTD), i.e. the correlated Bellman rest (B-LSTD) = b-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A"}, {"heading": "3.1. A Finite Sample Analysis", "text": "In this section, we will examine how good the D-LSTD solution \u03b8d is, \u03bb compares with the results of development, i.e., the model-based LSTD solution meets the distribution requirements. The analysis follows steps similar to those in Pires (2011) for \"1-LSTD.\" Following, we will use the assumption that the samples are i.e. generated from an arbitrary sample distribution. We will leave as future work the extension of the Markov design (i.e. if the samples are generated from a single track of policy within the evaluation). Theorem 1. Let us consider the B results = maxs-S results (s) as future work that the DLSTD solution (Eq. 2) is satisfactory in that the samples are generated from a single track of policy within the evaluation."}, {"heading": "3.2. Comparison to Other Algorithms", "text": "Similar to \"1-PBR\" and \"2,1-LSTD,\" D-LSTD is based on a clearly defined standard problem of convex optimization, which does not require A-LSTD to be a P matrix (as opposed to LASSO-TD), and this can be solved with the help of all the harder solutions. Nevertheless, D-LSTD has only one meta parameter (instead of two), and generally has lower computing costs w.r.t. Solving the nested optimization problems of \"1-PBR\" and \"2,1-LSTD.D\" is also related to LASSO-TD: Proposition 2. The LASSO-TD solution of the problem (if it exists) meets the D-LSTD limitations: \"A-LSTBR\" and \"2,1-LSTD.LSTD\" are also related to LASSO-TD: Proposition 2. The optimum conditions of the LSTD-STD can be achieved by ensuring that LASSD-STD is one of LSTD."}, {"heading": "4. Discussion", "text": "In this section we will discuss how the error | | A\u03b8 \u2212 b | | is related to the prediction error of the value function and how the regulator \u03bb can be selected in practice."}, {"heading": "4.1. From the Parameters to the Value", "text": "Similar to Yu & Bertsekas (2010), we can V \u2212 V-V-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p)."}, {"heading": "4.2. Cross Validation", "text": "The result of Theorem 1 applies to an oracle optimization problem (which is an overarching problem). In practice, the choice of the model can only be determined by the available data. This problem is of great practical importance, although it is not often discussed in the RL literature (especially for \"1-punished LSTD variations\"). However, in the supervised learning procedure, algorithms minimize a risk defined as the (empirical) expectation of a loss function. Cross-validation consists in using an independent sample to estimate the true risk function, and the meta parameter is selected as the one that minimizes the estimated true risk. However, there is no such risk for estimating the value function, and cross-validation cannot be used. A general model selection method for the value function was derived from Farahmand & Szepesva \"ri (2011). However, we can develop an ad hoc (simple solution) for STD-STD."}, {"heading": "5. Illustration and Experiment", "text": "Paragraph 5.1 provides an example that shows that D-LSTD mitigates the potential problem of non-political learning. Paragraph 5.2 reports on a more complex corrupt chain that illustrates the case of n p in an on and off-policy environment, as well as on studies of (heuristic) cross-validation."}, {"heading": "5.1. A Pathological MDP", "text": "The transition matrix and the reward vector are P = (0 1 0 1) and R = (0 \u2212 1) >. Therefore, the optimal value function is V = \u2212 11 \u2212 \u03b3 (\u03b3 1) > with \u03b3 the discount factor. Consider the one-feature approach \u03a6 = (1 2) >. Compare the (asymptotic) regulatory pathways of LASSO-TD (Kolter & Ng, 2009), '1-LSTD and D-LSTD, in cases where LASSO-TD fails, in cases where politics fails. In cases where politics fails, the sample distribution \u00b5 > = (0 1).The regulatory pathways for each algorithm can be easily calculated by analyzing politics (there is only one parameter)."}, {"heading": "5.2. Corrupted Chain", "text": "We look at the same problem as in Kolter & Ng. (2009) and Hoffman et al. (2011) We have the status s of EU member states in the EU and in the EU. (2011) We have the EU member states in the EU and in the EU. (2011) We have the EU and the EU in the EU. (2011) We have the EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012) The EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012) We have the EU and the EU in the EU. (2012)"}, {"heading": "6. Conclusion", "text": "The D-LSTD estimation is a good approximation to the asymptotic LSTD solution in the sense of theorem 1. It is also close to the LASSO-TD estimation (whenever well defined) in the sense of Prop. 2. In fact, D-LSTD inherits the same difference as the Dantzig selector in the sense of theorem 1. LASSO. Our preliminary experiments also show that D-LSTD is at least as good as LASSO-TD estimation (whenever well defined) in the sense of Prop. 2. There are still a number of questions that require further investigation. As discussed in Sec. 4, when we move from the linear equation system to the forecast plan, our preliminary experiments show that D-LSTD works at least as well as LASSO-TD. There are still a number of questions that are subject to further investigation."}, {"heading": "Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous", "text": "Analysis of Lasso and Dantzig selector. The Annals of Statistics, 37 (4): 1705-1732, 2009.Bradtke, S. J. and Barto, A. G. Linear Least-Squares algorithms for temporal difference learning. Machine learning, 22: 33-57, 1996.Candes, E. and Tao, T. The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics, 35 (6): 2313-2351, 2007."}, {"heading": "Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R.", "text": "Least Angle Regression. Annals of Statistics, 32 (2): 407- 499, 2004."}, {"heading": "Farahmand, A., Ghavamzadeh, M., Szepesva\u0301ri, C., and", "text": "Mannor, S. Regularized Policy Iteration. In Proc of NIPS 21, 2008."}, {"heading": "Farahmand, A. M. and Szepesva\u0301ri, C. Model selection in", "text": "Machine Learning Journal, 85 (3): 299-332, 2011.Geist, M. and Scherrer, B. '1-1-1 projected Bellman residue. In Proc of EWFL 9, 2011."}, {"heading": "Ghavamzadeh, M., Lazaric, A., Munos, R., and Hoffman,", "text": "M. Finite-Sample Analysis of Lasso-TD. In Proc. of ICML, 2011."}, {"heading": "Hoffman, M. W., Lazaric, A., Ghavamzadeh, M., and", "text": "Munos, R. Regularized Least Squares Temporal Difference learning with nested '2 and' 1. In Proc. of EWFL 9, 2011. Johns, J., Painter-Wakefield, C., and Parr, R. Linear Complementarity for Regularized Policy Evaluation and Improvement. In Proc. of NIPS 23, 2010."}, {"heading": "Kolter, J. Z. and Ng, A. Y. Regularization and Feature Selection in Least-Squares Temporal Difference Learning.", "text": "s thesis, University of Alberta, 2011.Romberg, J. '1-magic matlab library. http: / / users.ece. gatech.edu / ~ justin / l1magic /, 2005.Sutton, R. S. and Barto, A. G. Reinforcement Learning: an Introduction. The MIT Press, 1998."}, {"heading": "Tibshirani, R. Regression Shrinkage and Selection via the", "text": "Lasso. Journal of the Royal Statistical Society, 58 (1): 267-288, 1996.Yu, H. and Bertsekas, D. P. Error Bounds for Approximations from Projected Linear Equations. Mathematics of Operations Research, 35: 306-329, 2010."}], "references": [{"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Linear Least-Squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "Bradtke and Barto,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Candes", "T. Tao"], "venue": "Annals of Statistics,", "citeRegEx": "Candes and Tao,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao", "year": 2007}, {"title": "Least Angle Regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Regularized Policy Iteration", "author": ["A. Farahmand", "M. Ghavamzadeh", "C. Szepesv\u00e1ri", "S. Mannor"], "venue": "In Proc. of NIPS", "citeRegEx": "Farahmand et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2008}, {"title": "Model selection in reinforcement learning", "author": ["A.M. Farahmand", "C. Szepesv\u00e1ri"], "venue": "Machine Learning Journal,", "citeRegEx": "Farahmand and Szepesv\u00e1ri,? \\Q2011\\E", "shortCiteRegEx": "Farahmand and Szepesv\u00e1ri", "year": 2011}, {"title": "`1-penalized projected Bellman residual", "author": ["M. Geist", "B. Scherrer"], "venue": "In Proc. of EWRL", "citeRegEx": "Geist and Scherrer,? \\Q2011\\E", "shortCiteRegEx": "Geist and Scherrer", "year": 2011}, {"title": "Finite-Sample Analysis of Lasso-TD", "author": ["M. Ghavamzadeh", "A. Lazaric", "R. Munos", "M. Hoffman"], "venue": "In Proc. of ICML,", "citeRegEx": "Ghavamzadeh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2011}, {"title": "Regularized Least Squares Temporal Difference learning with nested `2 and `1 penalization", "author": ["M.W. Hoffman", "A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "In Proc. of EWRL", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Linear Complementarity for Regularized Policy Evaluation and Improvement", "author": ["J. Johns", "C. Painter-Wakefield", "R. Parr"], "venue": "In Proc. of NIPS", "citeRegEx": "Johns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2010}, {"title": "Regularization and Feature Selection in Least-Squares Temporal Difference Learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "In Proc. of ICML,", "citeRegEx": "Kolter and Ng,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Statistical analysis of `1-penalized linear estimation with applications", "author": ["B.A. Pires"], "venue": "Master\u2019s thesis, University of Alberta,", "citeRegEx": "Pires,? \\Q2011\\E", "shortCiteRegEx": "Pires", "year": 2011}, {"title": "Reinforcement Learning: an Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Regression Shrinkage and Selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Error Bounds for Approximations from Projected Linear Equations", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Yu and Bertsekas,? \\Q2010\\E", "shortCiteRegEx": "Yu and Bertsekas", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero.", "startOffset": 81, "endOffset": 99}, {"referenceID": 13, "context": "In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero. However, LASSO-TD is not derived from a proper convex optimization problem, and thus, it requires some assumptions that might not hold in an off-policy setting. Although other algorithms have been proposed to overcome these drawbacks (e.g., `1-PBR by Geist & Scherrer (2011)), other disadvantages may appear.", "startOffset": 82, "endOffset": 470}, {"referenceID": 4, "context": "This corresponds to \u03bb1pen1(\u03c9) = \u03bb\u2016\u03c9\u20162 and \u03bb2 = 0 and it has been generalized by Farahmand et al. (2008) with `2,2-LSTD, where both penalty terms use an `2-norm regularization.", "startOffset": 80, "endOffset": 104}, {"referenceID": 3, "context": "This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad\u2013hoc variation of the LARS algorithm (Efron et al., 2004).", "startOffset": 154, "endOffset": 174}, {"referenceID": 3, "context": "This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad\u2013hoc variation of the LARS algorithm (Efron et al., 2004). For LARS-TD to find a solution, \u00c3 must be a P-matrix. Unfortunately, this may not be true when the sampling and stationary distributions are different (off-policy learning). Although this does not always affect the performance in practice (see some of the experiments reported in Kolter & Ng 2009), it would be desirable to remove or relax this condition. The LARSTD idea is further developed by Johns et al. (2010), where LASSO-TD is reframed as a linear complementary problem.", "startOffset": 155, "endOffset": 592}, {"referenceID": 8, "context": "In order to alleviate the P-matrix problem, the `1-PBR (Projected Bellman residual) (Geist & Scherrer, 2011) and the `2,1-LSTD (Hoffman et al., 2011) algorithms have been proposed.", "startOffset": 127, "endOffset": 149}, {"referenceID": 11, "context": "Finally, a novel approach has been introduced by Pires (2011). The idea is to consider the linear system formulation of LSTD (i.", "startOffset": 49, "endOffset": 62}, {"referenceID": 11, "context": "The analysis follows similar steps as in Pires (2011) for `1-LSTD.", "startOffset": 41, "endOffset": 54}, {"referenceID": 0, "context": "This is not surprising, since DLSTD relates to LASSO-TD in a similar way as DS does to LASSO (Bickel et al., 2009).", "startOffset": 93, "endOffset": 114}, {"referenceID": 11, "context": "A result similar to Theorem 1 exists for `1-LSTD (Pires, 2011):", "startOffset": 49, "endOffset": 62}, {"referenceID": 11, "context": "A different model selection strategy has been devised for `1-LSTD by Pires (2011). It consists in choosing \u03bb\u0302 = argmin[a,b] \u2016\u00c3\u03b81,\u03bb \u2212 b\u0303\u20162 + \u03bb\u2016\u03b81,\u03bb\u20161 with [a, b] an exponential grid and \u03bb\u2032 can be computed from data (no oracle choice).", "startOffset": 69, "endOffset": 82}, {"referenceID": 8, "context": "We consider the same chain problem as in Kolter & Ng (2009) and Hoffman et al. (2011). The state s has s\u0304 + 1 components s.", "startOffset": 64, "endOffset": 86}, {"referenceID": 8, "context": "For LASSO-TD and `2,\u2205-LSTD, these results are consistent with those published by Hoffman et al. (2011). Notice that there was more choice of regularization parameters for LASSO-TD, as the whole regularization path was computed.", "startOffset": 81, "endOffset": 103}, {"referenceID": 11, "context": "As for the choice of the regularization parameter, we plan to adapt the model selection scheme of `1-LSTD (Pires, 2011) to D-LSTD and test it.", "startOffset": 106, "endOffset": 119}], "year": 2012, "abstractText": "LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, `1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are wellsuited for high\u2013dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed\u2013point problem, its integration with `1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks.", "creator": "LaTeX with hyperref package"}}}