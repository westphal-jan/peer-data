{"id": "1206.6481", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Cross Language Text Classification via Subspace Co-regularized Multi-view Learning", "abstract": "In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (154kb)", "http://arxiv.org/abs/1206.6481v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["yuhong guo", "min xiao"], "accepted": true, "id": "1206.6481"}, "pdf": {"name": "1206.6481.pdf", "metadata": {"source": "CRF", "title": "Cross Language Text Classification via Subspace Co-Regularized Multi-View Learning", "authors": ["Yuhong Guo", "Min Xiao"], "emails": ["yuhong@temple.edu", "minxiao@temple.edu"], "sections": [{"heading": "1. Introduction", "text": "With the rapid growth of multilingual data in all aspects of human society, it is very common for documents in different languages to share the same categories. In such a multilingual learning scenario, the application of standard monolingual classification methods directly requires costly and time-consuming annotations in each language. Thus, the development of effective methods of cross-lingual text classification, which transfer the categorization knowledge in a brand-rich language, the source language, to classifications inAppearing in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, 2012. Copyright 2012 by the author (s) / owner. A label-scarcity language, the target language, is becoming increasingly important. Previous work on cross-border text classification focuses mainly on the use of automatic machine translation technologies."}, {"heading": "2. Related Work", "text": "Previous work on translingual text classification has largely relied on machine translation methods, translating the test data into the language of the available training data or vice versa, so that classification algorithms for monolingual texts can be applied (Bel et al., 2003; Shanahan et al., 2004). Although these methods are simple and intuitive, they suffer from the errors and errors introduced in machine translation and the discrepancy in data distribution across languages (Shi et al., 2010) Various methods have been proposed to address these problems on translated data in order to increase the accuracy of translinguistic text classification, including one introduced in machine translation (Ling et al., 2008), EM-based model translation techniques (Shi et al., 2005), and translinguistic adaptation methods. (Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan al et al al)."}, {"heading": "3. Cross Language Text Classification", "text": "In this paper, we combine the domain adaptation intuition of learning generalizable attribute representations with the co-regulation principle of multiview learning and develop a sub-space co-regulated multiview method for cross-language text classification. For simplicity, we look at binary classification tasks. We assume that there are documents in two languages, the source language and the target language, for the same classification task. We use the data in the labeled source language to help classifiers train the data in the low-label target language in addition to machine translation. In this section, we first present the basic notations and then present the proposed multiview learning method."}, {"heading": "3.1. Notations", "text": "Suppose there are n documents in the source language in which l documents are labeled and the remaining us documents are unlabeled. Suppose there are n documents in the target language in which lt (lt < ls) of them are labeled and the remaining ut documents are unlabeled. By machine translation, we can translate any document in the source language into a parallel document in the target language and vice versa. If we merge the original and translated data in each language, we get two parallel matrices, X1, IR n \u00b7 d1 in the source view and X2, IR n \u00b7 d2 in the target view, where n = ns + nt. The first l = ls + lt lines of X1 and X2 form the labeled submatrices, X1 and X 2 respectively. Their corresponding labels are given as column vector y {\u2212 1, + 1} l."}, {"heading": "3.2. Multi-View Training via Subspace Co-Regularization", "text": "We assume that in each case there is a deep-dimensional subspace representation of the data, while both perspectives are separated from each other. (The linear prediction functions are in the individual areas.) (The linear prediction functions in the areas 1 and 2 (the linear prediction functions in the areas 1 and 2). (The linear prediction functions in the areas 1 and 2). (The linear prediction functions in the areas 1 and 2 (the linear weight vectors, 2 and 2). (The transformation matrix has an orthogonal column, which is an identity matrix, where I am an identity matrix.) Since the same classification task is shared between the two views, the underlying prediction functions in the parallel representations of the two views should be very similar."}, {"heading": "3.3. Optimization Algorithm", "text": "The non-convex optimization problem (8) is generally difficult to optimize (Q = 1) due to the orthogonal constraints (Wen & Yin, 2010).In this thesis, we use a curvilinear search process to solve it for a local optimal solution.In each iteration of the process (1) (taking into account the current point (1, 2), the gradients can be calculated using (9), such as G1 = 1 L (1, 2), G2 = 2L (2). (10) We then calculate two skew-symmetric matricesF1 = G1P 1 \u2212 1G 1, F2 = G2P 2 2, Q2 2P 2 (2) (2). (11) It is easy to see F 1 = \u2212 F1 and F 2 = \u2212 F2. The next point can be searched for as a curvilinear function of a variable level (2)."}, {"heading": "3.4. Multi-View Testing", "text": "After the semi-monitored multi-view training, we obtain two prediction models defined in the same way (1) with model parameters (i, wi, bi) 2 i = 1. We then perform multi-view tests on new documents. Specifically, we translate a test document, x, IRd2, in the target language into the source language in order to obtain x, IRd1. We then calculate the prediction values using the two prediction models f1 (x) = x, 1w1 + b1, (18) f2 (x) = x, 2w2 + b2. (19) The prediction reliability of each prediction model can be calculated using the steps | f1 (x) and | f2 (x). Finally, we set the prediction label for x as the predicted Q1 and the predicted Q2 as the predicted Q2, i.e. y = {character (f1 (x)) if."}, {"heading": "4. Experiments", "text": "In this section, we report on our empirical results on a number of cross-language text classification tasks."}, {"heading": "4.1. Experimental Setting", "text": "The experiments were conducted on cross-language text classification (CLTC) tasks built from a comparable multilingual corpus used in (Amini et al., 2009), which includes newswire articles written in 5 languages (English (E), French (F), German (G), Italian (I), Spanish (S), spread over 6 classes (C15, CCAT, E21, ECAT, GCAT, M11). In this multilingual corpus, each original document was translated into the other 4 languages using a statistical machine translation system. Our first set of experiments aims to evaluate CLTC tasks in different languages. We constructed a series of 20 binary cross-language classification tasks across all possible source languages, using two large classes, CCAT and ECAT, as shown in Table 1. E2F designates the task that uses English as the source language and French as the target language."}, {"heading": "4.2. Experiment I", "text": "The first set of experiments are performed on the 20 CLTC tasks mentioned above. For each task, we randomly selected 900 labeled and 2,100 unlabeled original documents from the source language, and selected 100 labeled and 2,900 unlabeled original documents from the target language domain for classification model training. Thus, we had a total of 1,000 labeled documents and 5,000 unlabeled documents in each language formation. We used the remaining 1,000 original documents in the target language as test data. Based on this random data distribution method, we repeated the E2F experiment 3 times to select the model parameter selection for MVCC and the proposed SCMV parameters for the MVCC group."}, {"heading": "4.3. Experiment II", "text": "The second set of experiments is based on the 36 CLTC tasks constructed with 1-against-all classification problems. We used the same distribution method for random data and the same model parameters specified in ExperimentI. For the proposed SCMV method, we used 10 as a subspace dimension size. We repeated each experiment ten times and the average test results are in Table 2. Similar to the first experiment, the proposed approach outperforms all the other five methods in all 36 tasks, and the improvements are significant for 22 tasks compared to the best comparative results. All these results suggest that identifying consistent subspace representations with two views based on both the original and translated data in two languages can effectively overcome linguistic divergence and achieve good predictive models."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a novel multilingual learning method to address cross-language text classifications. By jointly training two subregion-based prediction models in two language views while punishing the distance between the two projected subspace representations of both marked and blank instances, the underlying discriminatory subspace representations can be identified to create prediction models with better generalization performance. We developed a gradient descend algorithm with curvilinear search to solve the proposed common optimization problem for a local optimal solution. Our extensive empirical results on a large number of cross-language text classification tasks showed the superior performance of the proposed method compared to a few inductive methods, domain adjustment methods, and multivisional learning methods."}], "references": [{"title": "A co-classification approach to learning from multilingual corpora", "author": ["M. Amini", "C. Goutte"], "venue": "Machine Learning,", "citeRegEx": "Amini and Goutte,? \\Q2010\\E", "shortCiteRegEx": "Amini and Goutte", "year": 2010}, {"title": "Cross-lingual text", "author": ["N. Bel", "C. Koster", "M. Villegas"], "venue": "Neural Information Process. Systems", "citeRegEx": "Bel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bel et al\\.", "year": 2009}, {"title": "Coregularization based semi-supervised domain adaptation", "author": ["H. Daum\u00e9 III", "A. Kumar", "A. Saha"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "III et al\\.,? \\Q2010\\E", "shortCiteRegEx": "III et al\\.", "year": 2010}, {"title": "Can chinese web pages be classified with english data source", "author": ["X. Ling", "G. Xue", "W. Dai", "Y. Jiang", "Q. Yang", "Y. Yu"], "venue": "In Proc. of the international conference on World Wide Web,", "citeRegEx": "Ling et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2008}, {"title": "Cross-language text classification using structural correspondence learning", "author": ["P. Prettenhofer", "B. Stein"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Prettenhofer and Stein,? \\Q2010\\E", "shortCiteRegEx": "Prettenhofer and Stein", "year": 2010}, {"title": "An EM based training algorithm for cross-language text categorization", "author": ["L. Rigutini", "M. Maggini"], "venue": "In Proc. of the Web Intelligence Conference,", "citeRegEx": "Rigutini and Maggini,? \\Q2005\\E", "shortCiteRegEx": "Rigutini and Maggini", "year": 2005}, {"title": "Mining multilingual opinions through classification and translation", "author": ["J.G. Shanahan", "G. Grefenstette", "Y. Qu", "D.A. Evans"], "venue": "In Proc. of AAAI\u201904 Spring Symp. on Explor. Attitude and Affect in Text,", "citeRegEx": "Shanahan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shanahan et al\\.", "year": 2004}, {"title": "Cross language text classification by model translation and semisupervised learning", "author": ["L. Shi", "R. Mihalcea", "M. Tian"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "An RKHS for multiview learning and manifold co-regularization", "author": ["V. Sindhwani", "D. Rosenberg"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "Sindhwani and Rosenberg,? \\Q2008\\E", "shortCiteRegEx": "Sindhwani and Rosenberg", "year": 2008}, {"title": "A coregularization approach to semi-supervised learning with multiple views", "author": ["V. Sindhwani", "P. Niyogi", "M. Belkin"], "venue": "In Proc. of the Workshop on Learning with Multiple Views,", "citeRegEx": "Sindhwani et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2002}, {"title": "Bi-weighting domain adaptation for cross-language text classification", "author": ["C. Wan", "R. Pan", "J. Li"], "venue": "In Proc. of the Interational Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Wan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2011}, {"title": "Cross lingual adaptation: an experiment on sentiment classifications", "author": ["B. Wei", "C. Pal"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Wei and Pal,? \\Q2010\\E", "shortCiteRegEx": "Wei and Pal", "year": 2010}, {"title": "A feasible method for optimization with orthogonality constraints", "author": ["Z. Wen", "W. Yin"], "venue": "Technical report, Rice University,", "citeRegEx": "Wen and Yin,? \\Q2010\\E", "shortCiteRegEx": "Wen and Yin", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": ", the work in (Shi et al., 2010; Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011), they nevertheless suffer from the information loss and translation error introduced in machine translation process without direct access to the original documents.", "startOffset": 14, "endOffset": 95}, {"referenceID": 10, "context": ", the work in (Shi et al., 2010; Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011), they nevertheless suffer from the information loss and translation error introduced in machine translation process without direct access to the original documents.", "startOffset": 14, "endOffset": 95}, {"referenceID": 6, "context": "Previous work on cross language text classification mostly relied on machine translation methods, by translating the test data into the language of the training data or vice versa, so that classification algorithms for monolingual texts can be applied (Bel et al., 2003; Shanahan et al., 2004).", "startOffset": 252, "endOffset": 293}, {"referenceID": 7, "context": "Although simple and intuitive, these methods suffer from the error and noise introduced in machine translation and the discrepancy of data distribution across languages (Shi et al., 2010).", "startOffset": 169, "endOffset": 187}, {"referenceID": 3, "context": "Various methods have been proposed to tackle these issues on translated data to increase cross language text classification accuracy, including an information bottleneck method (Ling et al., 2008), EM-based model translation techniques (Shi et al.", "startOffset": 177, "endOffset": 196}, {"referenceID": 7, "context": ", 2008), EM-based model translation techniques (Shi et al., 2010; Rigutini & Maggini, 2005), and cross language domain adaptation methods (Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al.", "startOffset": 47, "endOffset": 91}, {"referenceID": 10, "context": ", 2010; Rigutini & Maggini, 2005), and cross language domain adaptation methods (Wei & Pal, 2010; Prettenhofer & Stein, 2010; Wan et al., 2011).", "startOffset": 80, "endOffset": 143}, {"referenceID": 10, "context": "(Wan et al., 2011) presents a feature and instance bi-weighting adaptation method for cross language text classification.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": ", 2009), and the multi-view coclassification method (Amini & Goutte, 2010) which is an instance of the co-regularized multi-view classification (Sindhwani et al., 2002; Sindhwani & Rosenberg, 2008).", "startOffset": 144, "endOffset": 197}], "year": 2012, "abstractText": "In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}