{"id": "1206.6482", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Modeling Images using Transformed Indian Buffet Processes", "abstract": "Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (612kb)", "http://arxiv.org/abs/1206.6482v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["ke zhai 0001", "yuening hu", "jordan l boyd-graber", "sinead williamson"], "accepted": true, "id": "1206.6482"}, "pdf": {"name": "1206.6482.pdf", "metadata": {"source": "META", "title": "Modeling Images using Transformed Indian Buffet Processes", "authors": ["Yuening Hu", "Ke Zhai", "Sinead Williamson"], "emails": ["YNHU@CS.UMD.EDU", "ZHAIKE@CS.UMD.EDU", "SINEAD@CS.CMU.EDU", "JBG@UMIACS.UMD.EDU"], "sections": [{"heading": "1. Introduction", "text": "These models typically assume that all characteristics of a characteristic are common to all data points - that is, each characteristic appears in exactly the same way across all observations. This is often a reasonable assumption. For example, microarray data are designed in such a way that each cell consistently meets a certain condition, but this does not apply to images. Consider the collection of images of a rolling ball. If a model produces new characteristics in the procedures of the 29th International Conference on Machine Learning, it will pay less attention to other aspects of the image and will not be able to generalize them."}, {"heading": "2. Background", "text": "In this section, we look at the Indian buffet process and how its extension, the transformed IBP, models simple images. Then, we describe probability models for images. These models are a prelude to the models we present in Section 3."}, {"heading": "2.1. The Indian Buffet Process", "text": "The Indian Buffet Process (IBP, Griffiths & Ghahramani, 2005) is a distribution of binary matrices with interchangeable rows and an infinite number of columns that a non-parametric latent attribute model with an infinite number of attributes can define, which often coincides with our intuitions: we do not know how many latent attributes we expect to find in our data, nor do we expect to see all possible latent attributes in a given dataset. To use IBP to model data, we need to select a probability model that determines the shape of the attributes corresponding to columns of Z, and how the attributes selected by a series of Z combine to form a data point."}, {"heading": "2.2. The Transformed IBP", "text": "Most IBP-based feature models assume that a feature is identical in each data point in which it appears, excluding image modeling, in which (for example) a car moves. (1) We follow the convention that zn is the n-th row of a matrix Z and znk is the nth element of the vector zn.from place to place or where a person can be either in the foreground or background. Na\u0131ve models would learn different features for each location where a car appears; a more appropriate model would learn that each observation is actually a transformation of a common trait. The transformed IBP (tIBP, Austerweil & Griffiths, 2010) expands the IBP to accommodate data in different places. tIBP associates each column of an IBP-distributed matrix Z with a feature (as before)."}, {"heading": "2.3. Likelihoods for Latent Feature Image Models", "text": "In addition to the noise OR, another probability used in IBP is a linear Gaussian model, which assumes that images are generated by a linear overlay of features (Griffiths & Ghahramani, 2005).Each IBP line selects a subset of features and generates an observation by additively overlaying these features and adding Gaussian noise. This model can be expanded by adding weights to the non-zero elements of the IBP distributed matrix (Knowles & Ghahramani, 2007) and including a pointed noise model suitable for corrupt images (Zhou et al., 2011).If we want to model images in which features can obscure each other, linear Gaussian models are inappropriate. In the visual community, images are often represented by overlapping layers (Wang & Adelson, 1994), including in generative probability models (Jojic, Frejic Williams, 2001, each feature being Gausy)."}, {"heading": "3. Modeling Real-valued Images", "text": "In this section we use the tIBP to create models that combine non-parametric features with more useful and realistic probability functions for real image functions. We start by providing a general representation for the transformed IBP with an arbitrary perceptual probability. 3. For each image n we use for the transformations. 3., N) For each image n {1. Example of a transformation rnk p (r). \u2022 Example of a transformation rnk p (r). \u2022 Example of an image xn (x). We assume that an image xn (r). The distribution over transformations p (r), the function probability p (r)."}, {"heading": "4. Inference", "text": "For each iteration, we examine the Gaussian distributed characters A, the IBP distributed binary matrix Z, the transformations R, the hyperparameters \u03b1, \u03c3x and \u03c3a, and for M-tIBP, the binary masks S and the order \u03c9."}, {"heading": "4.1. Sampling Indicators, Transformations, and Masks", "text": "In all the years in which we have been able to define the distribution between countries and regions, it is not possible to determine the quality of distribution outcomes (as discussed in Section 5); instead, we must explore the distribution outcomes between countries and regions through a Metropolis-Hastings proposal; the effectiveness of a Metropolis-Hastings distribution depends on the quality of distribution outcomes. (Tu & Zhu, 2002) q (znk) qz (znk) qr (rnk) qs (snk) qs (snk) qs (snk) qs (qs) based on an established distribution technology that provides a high probability of plausibility."}, {"heading": "4.2. Resampling Transformation and Masks", "text": "In addition to the common sampling of znk, rnk and sn, k, we also collect rnk (and, for M-tIBP, sn, k) for values of n and k, for znk = 1. We collect rnk together using a Metropolis Hastings step with the supply distribution qr (rnk) (or qr (rnk) qs (snk))). For M-tIBP, we also collect the binary masks using the conditional distribution p (sdn, k | sd \u2212 (n, k), xn, z, rn, A) p (xn | sdn, k, zn, rn, A) \u00b7 p (sdn, k | sd \u2212 (n, k)), (10) where p (sdn, k | sd \u2212 (n, k) is specified in Eqn. (9)."}, {"heading": "4.3. Sampling the Feature Order", "text": "We assume that the order of the characteristics \u03c9 is determined from a uniform distribution using permutations. We try out the order of the characteristics using a Metropolis-Hastings step, in which we consistently select two consecutive characteristics and propose an order exchange."}, {"heading": "4.4. Sampling Features and Hyperparameters", "text": "For the M-tIBP sample, the dth pixel of the kth attribute asakd | Z, R, S, X \u0445 N (F \u03c32x \u2211 N = 1M d n, kxn, rnk (d), F), (11) where F = (\u03c3 \u2212 2a + \u03c3 \u2212 2 x \u2211 N = 1M d n, k) \u2212 1.The hyperparameters \u03b1, \u03c3x and \u03c3a can be sampled via form equations in closed form (Doshi-Velez, 2009)."}, {"heading": "4.5. Modeling Color Images", "text": "The above derivative assumes that each pixel is a single real number. However, natural images typically have color information that is represented as a three-dimensional vector for each pixel. In our model, all colors contribute to the image probability. Similarly, the supply distribution is an elementary sum of all possible channels, q (r | ak, x-ak, k), x-exp {\u2211 c (x-c n, k? a c k) (r)}, (12) where x-cn, k, and c-k are each a c-channel contribution of x-cn, k, and ak. In the case of M-tIBP for attribute k in picture n, we assume that all channels have a common mask sn, k."}, {"heading": "5. Computational Complexity", "text": "The main motivation behind the algorithm proposed in Section 4 is to enable the application of the transformed IBP to large amounts of data.Austerweil & Griffiths (2010) calculates the probability of the data for each possible transformation.Replacing this naive approach with the sampler presented above, an acceleration of at least O (Dmin (SR, K / logD) can be achieved, with R being the number of rotations considered, S the number of scales considered, D the number of pixels and K the number of non-zero elements in z.The evaluation of LG-tIBP and M-tIBP probabilities for a single image requires O (DK) calculations. Since the number of possible transformations is 3 O (D), the probability for all possible transformations in O (SRD2K) can be calculated, resulting in a total complexity per iteration in O (ND2K2) for the logging method used by Austerweil."}, {"heading": "6. Experimental Evaluation", "text": "We evaluate the LG-tIBP and M-tIBP models4 on both simulated and real data sets against the linear Gaussian IBP3Because functions can be centered outside the image, the total number of translations is actually greater than the number of pixels.4http: / / www.cs.umd.edu / \u02dc ynhu / code / mtibp (IBP), the noisy-OR transformed IBP (NO-tIBP) and the sprite model (SPRITE, Jojic Frey, 2001). Simulated data experiments show that both LG-tIBP and M-tIBP restore the underlying functions and locations more effectively than IBP. All data sets have been reduced to zero mean and unit variance for linear Gaussian models.Simulated Data To qualitatively assess the ability of LGtIBP and M-tIBP to find translated functions."}, {"heading": "7. Discussion and Future Work", "text": "In this section, we discuss further applications of this paradigm and possible extensions of our models, which use scoring functions from classical image analysis to guide the spread of metaphors, and combine the robustness and compatibility of a well-established pattern with the flexibility of probability models, based on other classical pattern recognition techniques (Tu & Zhu, 2002; Tu et al., 2005), which point to a number of models for recognizing probabilities. An alternative to image modeling is the use of codes (Li-Fei & Perona, 2005)."}, {"heading": "8. Acknowledgments", "text": "The authors thank Joseph Austerweil, Frank Wood, Finale Doshi-Velez, and Michalis K. Titsias for publishing implementations that were supported by NSF Scholarship # 1018625. Jordan Boyd-Graber is also supported by the Army Research Laboratory through the ARL Cooperative Agreement W911NF-09-2-0072. Sinead Williamson is supported by the NIH Scholarship # R01GM087694 and the AFOSR Scholarship # FA9550010247."}], "references": [{"title": "Learning invariant features using the transformed Indian buffet process", "author": ["J.L. Austerweil", "T.L. Griffiths"], "venue": "In NIPS,", "citeRegEx": "Austerweil and Griffiths,? \\Q2010\\E", "shortCiteRegEx": "Austerweil and Griffiths", "year": 2010}, {"title": "The Indian buffet process: Scalable inference and extensions", "author": ["F. Doshi-Velez"], "venue": "Master\u2019s thesis, University of Cambridge,", "citeRegEx": "Doshi.Velez,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez", "year": 2009}, {"title": "Pattern classification and scene analysis", "author": ["R.O. Duda", "P.E. Hart"], "venue": null, "citeRegEx": "Duda and Hart,? \\Q1973\\E", "shortCiteRegEx": "Duda and Hart", "year": 1973}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2005}, {"title": "Learning flexible sprites in video layers", "author": ["N. Jojic", "B.J. Frey"], "venue": "In CVPR,", "citeRegEx": "Jojic and Frey,? \\Q2001\\E", "shortCiteRegEx": "Jojic and Frey", "year": 2001}, {"title": "Infinite sparse factor analysis and infinite independent component analysis", "author": ["D. Knowles", "Z. Ghahramani"], "venue": "In ICA,", "citeRegEx": "Knowles and Ghahramani,? \\Q2007\\E", "shortCiteRegEx": "Knowles and Ghahramani", "year": 2007}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["Li Fei-Fei", "Perona", "Pietro"], "venue": "In CVPR,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2005}, {"title": "Describing visual scenes using the transformed Dirichlet process", "author": ["E.B. Sudderth", "A. Torralba", "W.T. Freeman", "A.S. Willsky"], "venue": "In NIPS,", "citeRegEx": "Sudderth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2005}, {"title": "The infinite gamma-Poisson feature model", "author": ["M. Titsias"], "venue": "In NIPS,", "citeRegEx": "Titsias,? \\Q2007\\E", "shortCiteRegEx": "Titsias", "year": 2007}, {"title": "Sequential learning of layered models from video", "author": ["M.K. Titsias", "C.K.I. Williams"], "venue": "In Toward Category-Level Object Recognition,", "citeRegEx": "Titsias and Williams,? \\Q2006\\E", "shortCiteRegEx": "Titsias and Williams", "year": 2006}, {"title": "Image segmentation by data-driven Markov chain Monte Carlo", "author": ["Z. Tu", "Zhu", "S.-C"], "venue": "TPAMI, 24:657\u2013673,", "citeRegEx": "Tu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2002}, {"title": "Image parsing: Unifying segmentation, detection, and recognition", "author": ["Z. Tu", "X. Chen", "A.L. Yuille", "S.C. Zhu"], "venue": "In ICCV,", "citeRegEx": "Tu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2005}, {"title": "Representing moving images with layers", "author": ["J.Y.A. Wang", "E.H. Adelson"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang and Adelson,? \\Q1994\\E", "shortCiteRegEx": "Wang and Adelson", "year": 1994}, {"title": "A non-parametric Bayesian method for inferring hidden causes", "author": ["F. Wood", "T.L. Griffiths", "Z. Ghahramani"], "venue": "In UAI,", "citeRegEx": "Wood et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2006}, {"title": "Dependent hierarchical beta processes for image interpolation and denoising", "author": ["M. Zhou", "H. Yang", "G. Sapiro", "D. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "In the original tIBP paper, features were generated and combined using noisy-OR (Wood et al., 2006); we refer to this model as the noisy-OR tIBP (NO-tIBP), which allows the same feature to appear in different locations, scales, and orientations.", "startOffset": 80, "endOffset": 99}, {"referenceID": 14, "context": "This model can be extended by adding weights to the non-zero elements of the IBP-distributed matrix (Knowles & Ghahramani, 2007) and incorporating a spiky noise model (Zhou et al., 2011) appropriate for corrupted images.", "startOffset": 167, "endOffset": 186}, {"referenceID": 1, "context": "The hyperparameters \u03b1, \u03c3x and \u03c3a can be Gibbs sampled via closed form equations (Doshi-Velez, 2009).", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "This approach, or similar methods based on other classical pattern recognition techniques (Tu & Zhu, 2002; Tu et al., 2005), can be applied across a range of Bayesian models to improve inference in large state spaces.", "startOffset": 90, "endOffset": 123}, {"referenceID": 7, "context": "Other techniques have used transformed Bayesian nonparametric models to build high-performing vision systems using fixed codewords (Sudderth et al., 2005); a combination of these models would allow for a joint model to infer transformations, codewords, and feature cooccurrence patterns.", "startOffset": 131, "endOffset": 154}, {"referenceID": 8, "context": "The infinite gamma-Poisson process (Titsias, 2007) is a distribution over infinite non-negative integer valued matrices.", "startOffset": 35, "endOffset": 50}, {"referenceID": 8, "context": "This idea was used to speed up inference in the SPRITE implementation of Titsias & Williams (2006). Incorporating spatial information into the mask distribution would also lead to more coherent feature appearances and counteract some of the \u201cspotty\u201d features observed for MtIBP.", "startOffset": 73, "endOffset": 99}], "year": 2012, "abstractText": "Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the crosscorrelation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images.", "creator": "LaTeX with hyperref package"}}}