{"id": "1206.6485", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Greedy Algorithms for Sparse Reinforcement Learning", "abstract": "Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and applicability of RL. One approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on $L_1$ regularization has adapted techniques from the supervised learning literature for use with RL. Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features. Such algorithms have many of the good properties of the $L_1$ regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data. This paper considers variants of orthogonal matching pursuit (OMP) applied to reinforcement learning. The resulting algorithms are analyzed and compared experimentally with existing $L_1$ regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (555kb)", "http://arxiv.org/abs/1206.6485v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["christopher painter-wakefield", "ronald parr"], "accepted": true, "id": "1206.6485"}, "pdf": {"name": "1206.6485.pdf", "metadata": {"source": "META", "title": "Greedy Algorithms for Sparse Reinforcement Learning", "authors": ["Christopher Painter-Wakefield", "Ronald Parr"], "emails": ["PAINT007@CS.DUKE.EDU", "PARR@CS.DUKE.EDU"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Framework and Notation", "text": "We aim to determine exact or good approximation functions for Markov reward processes (MRPs = > reward functions): M \u03b2 = (S, P, R, \u03b3). Given a state s \u00b2 S, the probability of a transition to a state s \u00b2 S of P (s \u00b2 | s) is given and leads to an expected reward of R (s). We do not address the question of optimizing the guidelines for a Markov decision process, although we point out that policy evaluation, in which P = Pp \u00b2, through a specific policy approach, represents an important intermediate step in many algorithms. A discount factor casts future benefits, so that the current value of a trajectory st = 0. st = n is that the policy approach t = 0 \u03b3 tR (st). The true value function V \u00b2 over states fulfills the Bellman equation: V = TV \u00b2 R +, PV \u00b2 where PV \u00b2, where the Bellman T \u00b2 is the operator and the fixer V \u00b2."}, {"heading": "3. Prior Art", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. OMP for Regression", "text": "Algorithm 1 is the classic OMP algorithm for regression. It is greedy because it myopically selects the attribute with the highest correlation to the remaining characteristics and never discards characteristics. We say that target y in X is m-sparse if there is an Xopt consisting of m columns of X and corresponding wopt, so that y = Xoptwopt and Xopt is minimal in the sense that there is no X consisting of fewer columns of X that can satisfy y = X \u00b2 w. Of the many results for a sparse recovery, Tropps (2004) is perhaps the simplest: Theorem 1 If y in X is m-sparse, and maximizes i 6 if i-sparse in X \u00b2 optxi, 1 < 1, then OMP is called X, y, and \u03b2 = 0 will return in m iterations. In maximizing xi, the vectors xi correspond to the columns of X that are not needed to reconstruct the tropals of 2009, requiring the exact reconstruction of these small words."}, {"heading": "3.2. L1 Regularization in RL", "text": "In counterpoint to the greedy methods based on OMP, which we will examine in the next section, many of the recent work on feature selection in RL is based on the least quadratic methods with L1 regularization. For regression, Tibshirani (1996) introduced the LASSO method, which removes matrix X and target vector y and searches for a vector w that minimizes the properties of \"y-Xw-2\" that applies a restriction to LASSO method 1. While Tibshirani applies a hard restriction, this is equivalent to the minimization of \"y-Xw-2 + \u03b2,\" (3) for some values of \"R +.Loth et al.\" (2007) the application of LASSO to the residual minimization of Bellman."}, {"heading": "4. OMP for RL", "text": "We present two algorithms for policy assessment: OMPBRM and OMP-TD.1 As the names suggest, the first algorithm is based on Bellman's residual minimization (BRM), while the second is based on the linear TD fixed point. Johns (2010) introduced algorithm 2, OMP-BRM, which is simpler in the sense that it essentially performs OMP with properties (BRM) using the reward vector as the target. OMPBRM differs from Johns \"OMP-BR algorithm (2010), which selected basic functions. Algorithm 3, OMP-TD, applies the basic OMP approach to form a feature set for LSTD.2 OMP-TD."}, {"heading": "4.1. Sparse Recovery in OMP-TD", "text": "Theorem 2 Even if V \u043d m-sparse on orthonormal basis, OMP-TD cannot guarantee an exact restoration of V \u043d in m-iterations. PROOF (by counter-example) Consider the Markov chain in Figure 1. The arcs indicate deterministic transitions. Suppose R (S2) = R (S3) = R (S4) = 1, R (S5) = 0 and R (S1) = \u2212 (\u03b3 + \u03b32 + \u03b33), then V \u0432 = [0, 1 + \u03b3 2, 1 + \u03b3, 1, 0]. With orthonorthonormal basis, the use of indicator function i (s) = I (s = si), V \u0432 is 3-sparse in comparison with opt = {2, 3, 4}. Starting with the empty series of characteristics, the residual vector S3 is only R. OMP-TD will add the vector point to the problem with the residual function, which becomes a residual function in 1."}, {"heading": "4.2. Sparse Recovery in OMP-BRM", "text": "Lemma 1: (I \u2212 GP) -1R: (I \u2212 GP) -1R: (I \u2212 GP) -1R: (I \u2212 GP) -1R: (I \u2212 GP) -1R: (I \u2212 GP) -1R: (E = DP = DP = DP = PDP = (DP). 2It follows that we can perform OMP on the basis (DP) -1R: (I \u2212 GP) -1R: (R = DP) and that if there is a sparse representation of R on the basis, we will also receive a sparse representation of V \u00b2. This enables a sparse recovery claim for OMP-BRM, which stands in stark contrast to the negative results for OMP-TD. Theorem 3: If V \u00b2: m: sparse on the basis, andmax i / P: (P): (P): (P): 1 < (4): MP-BRM: (P), then P-BRM: (P) (P): (R): (P.P) -1R: (P)."}, {"heading": "4.3. Sparse Recovery Behavior", "text": "We created experiments to validate the theory of exact restoration of OMP-BRM and to investigate the behavior of OMP-TD under similar circumstances. First, we created a basis for the 50 state chain problem (see Section 5), in which the first three basic functions provide an exact reconstruction of V-BRM, and the remaining 997 features are randomly generated, which fulfills Equation (4). (Generating such a basis first required the generation of a much larger (50 x 3000) matrix, then the ejection of features that violated the exact condition of restoration, and finally the cutting of the matrix to 1000 features. The first three features were constructed by finding two random features that correlate highly with V \u0445, and then adding a third feature, namely the reconstruction residues using the first two features.) In the 200 BRM we selected the resulting basis of BRM in front of the BRM data with OMP exact state OMP."}, {"heading": "5. Experiments", "text": "In fact, most of us are capable of outdoing ourselves."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, we examined the theoretical and practical applications of OMP to RL. We analyzed variants of OMP and compared them experimentally with existing regulated L1 approaches. We showed that perhaps the most natural scenario in which one might hope to achieve a sparse recovery fails; however, one variant, OMP-BRM, offers promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, empirically outperforms previous methods both in terms of approximation accuracy and efficiency for several benchmark problems. There are two natural directions for further development of this work. Our theoretical results for OMP-BRM are based on the simplest results for a sparse recovery in regression and cannot be directly applied to more realistic scenarios that involve noise. Stronger results may be possible, building on the work of Zhang (2009). A more interesting, but also more challenging future direction of theoretical recovery would be able to explain the theoretical performance of OMP in spite of extremely poor practice."}, {"heading": "Acknowledgments", "text": "This work has been supported by NSF IIS-1147641. Opinions, findings, conclusions or recommendations on this matter come from the authors and not necessarily from NSF. The authors would also like to thank Susan Murphy and Eric Laber for helpful discussions."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In ICML,", "citeRegEx": "Baird,? \\Q1995\\E", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["Bradtke", "Steven J", "Barto", "Andrew G"], "venue": "Machine Learning,", "citeRegEx": "Bradtke et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke et al\\.", "year": 1996}, {"title": "Least angle regression", "author": ["Efron", "Bradley", "Hastie", "Trevor", "Johnstone", "Iain", "Tibshirani", "Robert"], "venue": "The Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Regularized policy iteration", "author": ["Farahmand", "Amir Massoud", "Ghavamzadeh", "Mohammad", "Szepesv\u00e1ri", "Csaba", "Mannor", "Shie"], "venue": "In NIPS, pp", "citeRegEx": "Farahmand et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2008}, {"title": "Finite-sample analysis of Lasso-TD", "author": ["Ghavamzadeh", "Mohammad", "Lazaric", "Alessandro", "Munos", "R\u00e9mi", "Auer", "Peter"], "venue": "In ICML,", "citeRegEx": "Ghavamzadeh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2011}, {"title": "Orthogonal matching pursuit with replacement", "author": ["P. Jain", "A. Tewari", "I.S. Dhillon"], "venue": "Technical Report arXiv:1106.2774, Arxiv preprint,", "citeRegEx": "Jain et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2011}, {"title": "Basis Construction and Utilization for Markov Decision Processes using Graphs", "author": ["Johns", "Jeffrey"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "Johns and Jeffrey.,? \\Q2010\\E", "shortCiteRegEx": "Johns and Jeffrey.", "year": 2010}, {"title": "Linear complementarity for regularized policy evaluation and improvement", "author": ["Johns", "Jeffrey", "Painter-Wakefield", "Christopher", "Parr", "Ronald"], "venue": "In NIPS, pp", "citeRegEx": "Johns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2010}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "In ICML, pp", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Sparse temporal difference learning using LASSO", "author": ["Loth", "Manuel", "Davy", "Preux", "Philippe"], "venue": "ADPRL", "citeRegEx": "Loth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loth et al\\.", "year": 2007}, {"title": "Value function approximation with diffusion wavelets and Laplacian eigenfunctions", "author": ["Mahadevan", "Sridhar", "Maggioni", "Mauro"], "venue": null, "citeRegEx": "Mahadevan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2006}, {"title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes", "author": ["Mahadevan", "Sridhar", "Maggioni", "Mauro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mahadevan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2007}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["Mallat", "St\u00e9phane G", "Zhang", "Zhifeng"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mallat et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mallat et al\\.", "year": 1993}, {"title": "Analyzing feature generation for valuefunction approximation", "author": ["Parr", "Ronald", "Painter-Wakefield", "Christopher", "Li", "Lihong", "Littman", "Michael L"], "venue": "In ICML, pp", "citeRegEx": "Parr et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Parr et al\\.", "year": 2007}, {"title": "Feature selection using regularization in approximate linear programs for Markov decision processes", "author": ["Petrik", "Marek", "Taylor", "Gavin", "Parr", "Ronald", "Zilberstein", "Shlomo"], "venue": "In ICML, pp", "citeRegEx": "Petrik et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Petrik et al\\.", "year": 2010}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["Tropp", "Joel A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Tropp and A.,? \\Q2004\\E", "shortCiteRegEx": "Tropp and A.", "year": 2004}, {"title": "On the consistency of feature selection using greedy least squares regression", "author": ["Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang and Tong.,? \\Q2009\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2009}, {"title": "Regularization and variable selection via the elastic net", "author": ["Zou", "Hui", "Hastie", "Trevor"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),", "citeRegEx": "Zou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 13, "context": "cability of RL (Parr et al., 2007; Mahadevan & Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011).", "startOffset": 15, "endOffset": 121}, {"referenceID": 7, "context": "cability of RL (Parr et al., 2007; Mahadevan & Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011).", "startOffset": 15, "endOffset": 121}, {"referenceID": 4, "context": "cability of RL (Parr et al., 2007; Mahadevan & Maggioni, 2007; Johns, 2010; Johns et al., 2010; Ghavamzadeh et al., 2011).", "startOffset": 15, "endOffset": 121}, {"referenceID": 3, "context": "Very often (though not always (Farahmand et al., 2008)) sparseness is viewed as a desirable goal or side effect of regularization.", "startOffset": 30, "endOffset": 54}, {"referenceID": 14, "context": "Favoring sparsity can lead to faster algorithms in some cases (Petrik et al., 2010).", "startOffset": 62, "endOffset": 83}, {"referenceID": 13, "context": "It is related to BEBFs (Parr et al., 2007), but it differs in that it selects features from a finite dictionary.", "startOffset": 23, "endOffset": 42}, {"referenceID": 13, "context": "It is related to BEBFs (Parr et al., 2007), but it differs in that it selects features from a finite dictionary. OMP for RL was explored by Johns (2010) in the context of PVFs (Mahadevan & Maggioni, 2007) and diffusion wavelets (Mahadevan & Maggioni, 2006), but aside from this initial exploration of the topic, we are not aware of any efforts to bring the theoretical and empirical understanding of OMP for reinforcement learning to parity with the understanding of OMP as", "startOffset": 24, "endOffset": 153}, {"referenceID": 0, "context": "This is the Bellman residual minimization (BRM) approach espoused by Baird (1995).", "startOffset": 69, "endOffset": 82}, {"referenceID": 2, "context": "is very similar to LARS (Efron et al., 2004).", "startOffset": 24, "endOffset": 44}, {"referenceID": 7, "context": "Johns et al. (2010) followed LARS-TD with an algorithm, LC-TD, which solves for the L1 regularized linear fixed point as a linear complementarity problem.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "2 OMP-TD is similar in approach to the approximate BEBF algorithm of Parr et al. (2007), in which each new feature is an approximation to the current Bellman residual.", "startOffset": 69, "endOffset": 88}, {"referenceID": 5, "context": "Such constructions can defeat modifications to OMP-TD that use a window of features and discard gratuitous ones (Jain et al., 2011) for any fixedsize window.", "startOffset": 112, "endOffset": 131}, {"referenceID": 9, "context": "LARS-BRM: This is our implementation of the algorithm of Loth et al. (2007), which effectively treats BRM as a regression problem to be solved using LARS.", "startOffset": 57, "endOffset": 76}], "year": 2012, "abstractText": "Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and applicability of RL. One approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on L1 regularization has adapted techniques from the supervised learning literature for use with RL. Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features. Such algorithms have many of the good properties of the L1 regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data. This paper considers variants of orthogonal matching pursuit (OMP) applied to reinforcement learning. The resulting algorithms are analyzed and compared experimentally with existing L1 regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMPTD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems.", "creator": "LaTeX with hyperref package"}}}