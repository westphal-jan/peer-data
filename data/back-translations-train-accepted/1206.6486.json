{"id": "1206.6486", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Flexible Modeling of Latent Task Structures in Multitask Learning", "abstract": "Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the \"right\" latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, mean-regularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and real-world datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (229kb)", "http://arxiv.org/abs/1206.6486v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alexandre passos", "piyush rai", "jacques wainer", "hal daum\u00e9 iii"], "accepted": true, "id": "1206.6486"}, "pdf": {"name": "1206.6486.pdf", "metadata": {"source": "CRF", "title": "Flexible Modeling of Latent Task Structures in Multitask Learning", "authors": ["Alexandre Passos", "Piyush Rai", "Jacques Wainer"], "emails": ["apassos@cs.umass.edu", "piyush@cs.utah.edu", "wainer@ic.unicamp.br", "hal@umiacs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "In these cases, there is often not enough data to learn a good model for each task - real-world examples that make it possible to prioritize e-mail messages across multiple user boxes (Aberdeen et al., 2011) and recommend tasks to users on websites (Ning & Karypis, 2012). In these settings, it is beneficial to transfer or share information across tasks. Multitask Learning (MTL, 1997) includes a set of techniques to share statistical strength for different tasks and allows learning, even if the amount of marked data for each task is very small."}, {"heading": "2. Background", "text": "In the context of MTL, the standard solution is to try many different models that cover many assumptions of similarity with many settings of complexity for each model, and to select the model based on certain selection criteria. In this paper, we take a non-parametric Bayesian approach to this problem (using the Dirichlet process and the Indian buffet process as building blocks) so that the appropriate MTL model that captures the correct structure of task relationships and the complexity of the model for this model is learned in a data-driven way that sets aside the problems of model selection."}, {"heading": "2.1. The Dirichlet Process", "text": "The Dirichlet Process (DP) is a prior distribution over discrete distributions (Ferguson, 1973). Discreteness implies that when samples are taken from a distribution taken from the DP, the samples cluster: new samples take the same value as older samples with a certain positive probability. A DP is defined by two parameters: a concentration parameter \u03b1 and a base measurement G0. The sampling process that defines the DP draws the first sample from the base measurement G0. Each subsequent sample would assume a new value from G0 with a probability proportional to \u03b1, or reuse a previously drawn value whose probability is proportional to the number of samples that have this value. This property makes it suitable as a precursor for effectively infinite mixing models where the number of mixtures can grow when new samples are observed. Our mix of factor analyzers based on the MTL model uses the DP to modify the compounds so that we do not have to modify their number."}, {"heading": "2.2. The Indian Buffet Process", "text": "The Indian Buffet Process (IBP) (Griffiths & Ghahramani, 2006) and the closely related Beta Process (Thibaux & Jordan, 2007) define a distribution based on a collection of sparse binary vectors of unlimited size (or, equivalent, on sparse binary matrices with a fixed but unlimited dimension) such sparse structures are often used in applications such as Scanty Factor Analysis (Paisley & Carin, 2009), where we want to dissect a data matrix X in such a way that each observation matrix X is presented as a sparse combination of a series of K-D base vectors (or factors), but K is not specified a priori. Generative history in the finite case is (assuming a linear Gauss model): Xn, Nor (2, II XI) or Nor (0, 2 I) bkn, Ber (2, K) sparkk, a saving factor (K or regenerative K)."}, {"heading": "3. Mixture of Factor Analyzers based Generative Model for MTL", "text": "Our proposed model assumes that the parameters (i.e. the weight vector) of each task are taken from a mixture of factor analyzers (Ghahramani & Beal, 2000). Note that our model is defined by means of deferred weight vectors, while the standard mix of factor analyses is generally defined as a model for observed data. The weight vector is a sparse linear combination of K-base vectors represented by columns of individual columns, a \"basic task.\" The combination weights are presented by us as st, where a real valued vector and a binary valued vector are both of the size K. Our model uses a BetaBernouter Vector Vector Vector Control and IBP before the determination of K, the number of factors in each factor analyzer."}, {"heading": "3.1. Variational inference", "text": "Since this model has an infinite and combinatorial distribution, the exact conclusion is complicated, and sampling-based conclusion may take too long to come to a convergence (Doshi-Velez et al., 2009; Lead & Jordan, 2006). Therefore, we use a variable-center algorithm to perform conclusions in this model. To do this, we need to calculate the marginal log probability of the Y distribution using a fully factor-based distribution Q via the model parameters, z, s: logP (Y-X) = logEP (Y-X) = logEP (Y-X) = logical, p, p, p, p, p, p, p, p, p, s, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p. [logQ]. To do this, we approach the DP and IBP with a tractable distribution Q. For the DP, we use an infinite stick-breaking distribution based on the infinite stick representation of the 2006 (the DP)."}, {"heading": "4. Experiments", "text": "We present the results on both synthetic and real datasets, and on linear regression and classification settings. As a health test, we show that our model can correctly learn the underlying latent task structures. Each task is represented by a weight vector of length D = 20. Figure 4 (left) shows the true correlation structure of the tasks and Figure 4 (right) shows the restored structure by our model: it correctly infects the correct number (5) of tasks resulting in a classification accuracy of 83.2%, while independently learned tasks have resulted in an accuracy of 79.2%. Our next set of experiments compares our model with a number of baseline methods on several synthetic and real task classification problems."}, {"heading": "5. Related Work", "text": "Apart from the previous work on multitask learning discussed in Section 1, our model is based on a somewhat similar motivation to the model proposed in Section 1 (Argyriou et al., 2008). Your model assumes that tasks can be divided into groups and that tasks within each group have a common core. Your assumption is an extension of the previous work on multitask feature learning (Argyriouet al., 2007) (one of the baselines we used in our experiments), which assumes that all tasks share the common core. In their model, the authors assume that there is a single set of task foundation vectors (i.e. a task dictionary) and that each task is a sparse combination of these base vectors. In their model, the number of base vectors divided between two tasks (i.e. their \"overlaps\") can be viewed in the way that the paarchaotic tasks are viewed."}, {"heading": "6. Future Work and Discussion", "text": "We have proposed and evaluated a non-parametric Bayesian multitask learning model that usefully interpolates between many previously proposed models for estimating task parameters of multiple related learning problems, such as a common Gaussian Prior (Chelba & Acero, 2006), cluster structure (Xue et al., 2007), reduced dimensionality (Argyriou et al., 2007; Zhang et al., 2006), multiple structure (Ghosn & Bengio, 2003; Agarwal et al., 2010), and so on. For this model, we have presented a variational midfield algorithm that provides competitive results on a range of synthetic and real multitask learning datasets."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Emma Tosch for her helpful comments in compiling this manuscript. Piyush Rai was supported by the DARPA CSSG Grant P-1076-113840. Alexandre Passos was partially supported by the CIIR, partially supported by IARPA through DoI / NBC Contract # D11PC20152, partially through the UPenn NSF medium IIS-080384, partially through the DARPA) Machine Reading Program pursuant to AFRL Prime Contract # FA8750-09-C-0181. The U.S. government is authorized to reproduce and distribute the reprint for government purposes, regardless of the copyright comments contained therein. Any opinions, results, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors."}], "references": [{"title": "The learning behind gmail priority inbox", "author": ["D. Aberdeen", "O. Pacovsky", "A. Slater"], "venue": "In NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds,", "citeRegEx": "Aberdeen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Aberdeen et al\\.", "year": 2011}, {"title": "Learning multiple tasks using manifold regularization", "author": ["Agarwal", "Arvind", "Gerber", "Samuel", "Daum\u00e9 III", "Hal"], "venue": "In NIPS,", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": null, "citeRegEx": "Aharon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2010}, {"title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "author": ["R.K. Ando", "T. Zhang"], "venue": null, "citeRegEx": "Ando and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Multi-task feature learning", "author": ["Argyriou", "Andreas", "Evgeniou", "Theodoros", "Pontil", "Massimiliano"], "venue": "In NIPS,", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "An algorithm for transfer learning in a heterogeneous environment", "author": ["Argyriou", "Andreas", "Maurer", "Pontil", "Massimiliano"], "venue": "In ECML,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Variational inference for Dirichlet process mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei and Jordan,? \\Q2006\\E", "shortCiteRegEx": "Blei and Jordan", "year": 2006}, {"title": "Multi-task gaussian process prediction", "author": ["Bonilla", "Edwin V", "Chai", "Kian Ming A", "Williams", "Christopher K. I"], "venue": "In NIPS,", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Modeling transfer learning in human categorization with the hierarchical Dirichlet process", "author": ["K.R. Canini", "M.M. Shashkov", "T.L. Griffiths"], "venue": "In ICML,", "citeRegEx": "Canini et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Canini et al\\.", "year": 2010}, {"title": "Multitask Learning", "author": ["Caruana", "Rich"], "venue": "Machine Learning,", "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["C. Chelba", "A. Acero"], "venue": "Computer Speech & Language,", "citeRegEx": "Chelba and Acero,? \\Q2006\\E", "shortCiteRegEx": "Chelba and Acero", "year": 2006}, {"title": "Compressive Sensing on Manifolds Using a Nonparametric Mixture of Factor Analyzers: Algorithm and Performance Bounds", "author": ["M. Chen", "J. Silva", "J. Paisley", "C. Wang", "D. Dunson", "L. Carin"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Variational inference for the Indian buffet process", "author": ["F. Doshi-Velez", "K.T. Miller", "J. Van Gael", "Y.W. Teh", "G. Unit"], "venue": "In AISTATS,", "citeRegEx": "Doshi.Velez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez et al\\.", "year": 2009}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The annals of statistics,", "citeRegEx": "Ferguson,? \\Q1973\\E", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Variational inference for bayesian mixtures of factor analysers", "author": ["Ghahramani", "Zoubin", "Beal", "Matthew J"], "venue": "In NIPS,", "citeRegEx": "Ghahramani et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ghahramani et al\\.", "year": 2000}, {"title": "Infinite Latent Feature Models and the Indian Buffet Process", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2006\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2006}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["H. Ishwaran", "L.F. James"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ishwaran and James,? \\Q2001\\E", "shortCiteRegEx": "Ishwaran and James", "year": 2001}, {"title": "A variational approach to bayesian logistic regression models and their extensions", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "In AISTATS,", "citeRegEx": "Jaakkola and Jordan,? \\Q1996\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1996}, {"title": "Clustered multi-task learning: a convex formulation", "author": ["L. Jacob", "F. Bach"], "venue": "In NIPS,", "citeRegEx": "Jacob and Bach,? \\Q2008\\E", "shortCiteRegEx": "Jacob and Bach", "year": 2008}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In ICML,", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daum\u00e9 III"], "venue": "In ICML,", "citeRegEx": "Kumar and III,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III", "year": 2012}, {"title": "Multi-task learning for recommender systems", "author": ["X. Ning", "G. Karypis"], "venue": null, "citeRegEx": "Ning and Karypis,? \\Q2010\\E", "shortCiteRegEx": "Ning and Karypis", "year": 2010}, {"title": "Nonparametric factor analysis with beta process priors", "author": ["Paisley", "John", "Carin", "Lawrence"], "venue": "In ICML,", "citeRegEx": "Paisley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2009}, {"title": "Infinite predictor subspace models for multitask learning", "author": ["P. Rai", "H. Daum\u00e9 III"], "venue": "In AISTATS,", "citeRegEx": "Rai and III,? \\Q2010\\E", "shortCiteRegEx": "Rai and III", "year": 2010}, {"title": "Constructing informative priors using transfer learning", "author": ["R. Raina", "A.Y. Ng", "D. Koller"], "venue": "In ICML,", "citeRegEx": "Raina et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2006}, {"title": "Stick-breaking construction for the Indian buffet process", "author": ["Y.W. Teh", "D. G\u00f6r\u00fcr", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Hierarchical beta processes and the indian buffet process", "author": ["Thibaux", "Romain", "Jordan", "Michael I"], "venue": "In AISTATS,", "citeRegEx": "Thibaux et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Thibaux et al\\.", "year": 2007}, {"title": "Multitask Learning for Classification with Dirichlet Process Priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": null, "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}, {"title": "Learning multiple related tasks using latent independent component analysis", "author": ["J. Zhang", "Z. Ghahramani", "Y. Yang"], "venue": "In NIPS,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D. Yeung"], "venue": "In UAI,", "citeRegEx": "Zhang and Yeung,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Yeung", "year": 2010}, {"title": "L-BFGSB: Fortran subroutines for large-scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Transactions on Mathetmatical Software,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "task individually\u2014real-world examples are prioritizing email messages across many users\u2019 inboxes (Aberdeen et al., 2011) and recommending items to users on web sites (Ning & Karypis, 2010).", "startOffset": 97, "endOffset": 120}, {"referenceID": 27, "context": "Most MTL methods achieve this improved performance either by assuming some notion of similarity across tasks\u2014for example, that all task parameters are drawn from a shared Gaussian prior (Chelba & Acero, 2006), have a cluster structure (Xue et al., 2007; Jacob & Bach, 2008), live on a low-dimensional subspace (Rai & Daum\u00e9 III, 2010), share feature representations (Argyriou et al.", "startOffset": 235, "endOffset": 273}, {"referenceID": 4, "context": ", 2007; Jacob & Bach, 2008), live on a low-dimensional subspace (Rai & Daum\u00e9 III, 2010), share feature representations (Argyriou et al., 2007), or by modeling the task covariance matrix (Bonilla et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 7, "context": ", 2007), or by modeling the task covariance matrix (Bonilla et al., 2007; Zhang & Yeung, 2010).", "startOffset": 51, "endOffset": 94}, {"referenceID": 27, "context": "For example, task clustering (Xue et al., 2007), which fits a full-covariance Gaussian mixture model over the weight vectors, is prone to overfitting on high dimensional problems as the number of learning tasks is usually much smaller than the dimensionality, making it difficult to estimate the covariance matrix.", "startOffset": 29, "endOffset": 47}, {"referenceID": 28, "context": "Likewise, models based on task subspaces (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010; Agarwal et al., 2010) assume that the weight vectors of all the tasks live on or close to a single shared subspace, which is known to lead to negative transfer in the presence of outlier tasks.", "startOffset": 41, "endOffset": 106}, {"referenceID": 1, "context": "Likewise, models based on task subspaces (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010; Agarwal et al., 2010) assume that the weight vectors of all the tasks live on or close to a single shared subspace, which is known to lead to negative transfer in the presence of outlier tasks.", "startOffset": 41, "endOffset": 106}, {"referenceID": 13, "context": "The Dirichlet Process (DP) is a prior distribution over discrete distributions (Ferguson, 1973).", "startOffset": 79, "endOffset": 95}, {"referenceID": 27, "context": "\u2022 Cluster-based Assumption(F > 1,K=0): (Xue et al., 2007; Jacob & Bach, 2008).", "startOffset": 39, "endOffset": 77}, {"referenceID": 28, "context": "\u2022 Linear Subspace Assumption(F=1,K < D): (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010).", "startOffset": 41, "endOffset": 84}, {"referenceID": 4, "context": ", \u03b8T } being a rankK matrix (Argyriou et al., 2007).", "startOffset": 28, "endOffset": 51}, {"referenceID": 11, "context": "\u2022 Nonlinear Manifold Assumption: A mixture of linear subspaces allows modeling a nonlinear subspace (Chen et al., 2010) and can capture the case when the weight vectors live on a nonlinear manifold (Ghosn & Bengio, 2003; Agarwal et al.", "startOffset": 100, "endOffset": 119}, {"referenceID": 1, "context": ", 2010) and can capture the case when the weight vectors live on a nonlinear manifold (Ghosn & Bengio, 2003; Agarwal et al., 2010).", "startOffset": 86, "endOffset": 130}, {"referenceID": 2, "context": "From a non-probabilistic analogy, our model can be seen as doing dictionary learning/sparse coding (Aharon et al., 2010) over the latent weight vectors (albeit, using an undercomplete dictionary setting since we assume K \u2264 min{T,D}).", "startOffset": 99, "endOffset": 120}, {"referenceID": 12, "context": "As this model is infinite and combinatorial in nature, exact inference is intractable and sampling-based inference may take too long to converge (Doshi-Velez et al., 2009; Blei & Jordan, 2006).", "startOffset": 145, "endOffset": 192}, {"referenceID": 25, "context": "While there is a similar stick-breaking construction to the IBP (Teh et al., 2007), it is not in the exponential family and requires complicated approximations, so we represent the IBP by its finite Beta-Bernoulli approximation (Doshi-Velez et al.", "startOffset": 64, "endOffset": 82}, {"referenceID": 12, "context": ", 2007), it is not in the exponential family and requires complicated approximations, so we represent the IBP by its finite Beta-Bernoulli approximation (Doshi-Velez et al., 2009).", "startOffset": 153, "endOffset": 179}, {"referenceID": 30, "context": "While it is possible to update \u03bd\u03b8t analytically, the update requires inverting a matrix, and in our experiment this matrix was often ill-conditioned, so we updated \u03bd\u03b8t by optimizing the lower bound with the L-BFGS-B optimizer (Zhu et al., 1997).", "startOffset": 226, "endOffset": 244}, {"referenceID": 4, "context": "\u2022 Multitask Feature Learning - MTFL: assumes the tasks share a common set of features (Argyriou et al., 2007).", "startOffset": 86, "endOffset": 109}, {"referenceID": 28, "context": "\u2022 Single shared subspace - RANK (Zhang et al., 2006; Rai & Daum\u00e9 III, 2010): assumes the tasks live close to a linear subspace (also equivalent to the matrix of the weight vector being low-rank).", "startOffset": 32, "endOffset": 75}, {"referenceID": 27, "context": "\u2022 DP mixture model based task clustering - DPMTL (Xue et al., 2007): assumes the weight vectors are generated from a mixture model, each component being a full-rank Gaussian.", "startOffset": 49, "endOffset": 67}, {"referenceID": 19, "context": "\u2022 Learning with Whom to Share - LWS (Kang et al., 2011).", "startOffset": 36, "endOffset": 55}, {"referenceID": 19, "context": "we used three datasets - one synthetic dataset used in (Kang et al., 2011), and two real-world datasets used commonly in the multitask learning literature: (1) School: This dataset consists of the examination scores of 15362 students from 139 schools in London.", "startOffset": 55, "endOffset": 74}, {"referenceID": 19, "context": "For the synthetic data, we followed the similar procedure for train/test split as used by (Kang et al., 2011).", "startOffset": 90, "endOffset": 109}, {"referenceID": 27, "context": "For this, we chose two datasets: (1) Landmine: The landmine detection dataset is a subset of the dataset used in the symmetric multitask learning experiment by (Xue et al., 2007).", "startOffset": 160, "endOffset": 178}, {"referenceID": 24, "context": "(2) 20ng: We did the standard training/test split of 20 Newsgroups for multitask learning, following Raina et al. (2006), and used a 50/50 split for the landmine data.", "startOffset": 101, "endOffset": 121}, {"referenceID": 5, "context": "Apart from the prior work on multitask learning discussed in Section 1, our model is based on a somewhat similar motivation as the model proposed in (Argyriou et al., 2008).", "startOffset": 149, "endOffset": 172}, {"referenceID": 4, "context": "Their assumption is an extension of the earlier work on Multitask Feature Learning (Argyriou et al., 2007) (one of the baselines we used in our experiments) that assumes all tasks share the common kernel.", "startOffset": 83, "endOffset": 106}, {"referenceID": 4, "context": "Apart from the prior work on multitask learning discussed in Section 1, our model is based on a somewhat similar motivation as the model proposed in (Argyriou et al., 2008). Their model assumes that tasks can be partitioned into groups and tasks within each group share a kernel. Their assumption is an extension of the earlier work on Multitask Feature Learning (Argyriou et al., 2007) (one of the baselines we used in our experiments) that assumes all tasks share the common kernel. In (Kumar & Daum\u00e9 III, 2012), the authors assume that there is single set of task basis vectors (i.e., a task dictionary) and each task is a sparse combination of these basis vectors. In their model, the number of basis vectors shared between two tasks (i.e., their \u201coverlap\u201d) can be seen as the pairwise task similarity. In Kang et al. (2011), the authors proposed a model based on the assumption that the tasks exist in groups and the tasks within each group share features, which is again similar in spirit to our work (this model was one of our baselines in the experiments).", "startOffset": 150, "endOffset": 829}, {"referenceID": 8, "context": "Among other related work, Canini et al. (2010) propose hierarchical Dirichlet process models as good models for human categorical learning.", "startOffset": 26, "endOffset": 47}, {"referenceID": 27, "context": "We proposed and evaluated a nonparametric Bayesian multitask learning model that usefully interpolates between many different previously proposed models for estimating task parameters of multiple related learning problems, such as a shared Gaussian prior (Chelba & Acero, 2006), a clustering structure (Xue et al., 2007), reduced dimensionality (Argyriou et al.", "startOffset": 302, "endOffset": 320}, {"referenceID": 4, "context": ", 2007), reduced dimensionality (Argyriou et al., 2007; Zhang et al., 2006), manifold structure (Ghosn & Bengio, 2003; Agarwal et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 28, "context": ", 2007), reduced dimensionality (Argyriou et al., 2007; Zhang et al., 2006), manifold structure (Ghosn & Bengio, 2003; Agarwal et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 1, "context": ", 2006), manifold structure (Ghosn & Bengio, 2003; Agarwal et al., 2010), etc.", "startOffset": 28, "endOffset": 72}], "year": 2012, "abstractText": "Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the \u201cright\u201d latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, meanregularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and realworld datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}