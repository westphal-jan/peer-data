{"id": "1206.6487", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "An adaptive algorithm for finite stochastic partial monitoring", "abstract": "We present a new anytime algorithm that achieves near-optimal regret for any instance of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for both \"easy\" and \"hard\" problems. For easy problems, it additionally achieves logarithmic individual regret. Most importantly, the algorithm is adaptive in the sense that if the opponent strategy is in an \"easy region\" of the strategy space then the regret grows as if the problem was easy. As an implication, we show that under some reasonable additional assumptions, the algorithm enjoys an O(\\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al. (2011).", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (697kb)", "http://arxiv.org/abs/1206.6487v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.GT stat.ML", "authors": ["g\u00e1bor bart\u00f3k", "navid zolghadr", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1206.6487"}, "pdf": {"name": "1206.6487.pdf", "metadata": {"source": "META", "title": "An adaptive algorithm for finite stochastic partial monitoring", "authors": ["G\u00e1bor Bart\u00f3k", "Navid Zolghadr"], "emails": ["bartok@ualberta.ca", "zolghadr@ualberta.ca", "szepesva@ualberta.ca"], "sections": [{"heading": null, "text": "\u221a T) Regrets in dynamic pricing, which proved difficult by Barto \u0301 k et al. (2011)."}, {"heading": "1. Introduction", "text": "At each step in time, the learner selects an action and at the same time selects a result. Then, based on the action and the result, the learner suffers a certain loss and receives a certain regret. Neither the result nor the loss is revealed to the learner. Thus, a partial observation game with N actions and M results is defined with the pair G = (L, H), where L strategies are the loss matrix, and H matrix is the feedback matrix with some arbitrary sets of symbols. These matrices are announced to both the learner and the opponent before the game, where L strategies are the loss matrix, and H matrix is the feedback matrix with some arbitrary sets of symbols. These matrices are announced to both the learner and the opponent before the game begins."}, {"heading": "1.1. Related work", "text": "Two specific cases of partial monitoring were introduced by Piccoli & Schindelhauer (2001): full information games in which the feedback contains enough information for the learner to derive the result for each action outcome pair, and bandit games in which the learner receives the loss of the chosen action as feedback; since Vovk (1990) and Littlestone & Warmuth (1994), we have known that for full information games the minimax regret scales as such (T logN) have been tested; for bandit games, the minimax regret has proven to be true to scale (T) (Audibert & Bubeck, 2009).1 The individual regret of this type of game has also been investigated: Auer et al. (2002) showed that in the face of any opposing strategy, the expected regret can be limited by c-Outbound i-N."}, {"heading": "1.2. Contributions", "text": "In this paper, we expand on the findings of Barto \u0301 k et al. (2011). We introduce a new algorithm called CBP for \"Confidence Bound Partial Monitoring\" with various desirable properties. Firstly, while Balaton only works on simple games, CBP can be run on any non-hopeless game, and it achieves (up to logarithmic factors) the minimax repentance rates for both simple and hard games (see conclusions 3 and 2). Furthermore, it also achieves logarithmic problem-dependent repentance for simple games (see conclusion 1). It is also an \"anytime\" algorithm, which means that it does not need to know the time horizon, nor does it need to use the double trick to achieve the desired performance.The final and potentially most effective aspect of our algorithm is that it is possible to lower the minimax repentance even of hard games to the desired value through additional assumptions about the opponent's set of strategies."}, {"heading": "2. Definitions and notations", "text": "In this case, the choice of the opponent is governed by a sequence J1, J2. The distribution of these variables p / M is called a counter-strategy, in which the probability is expressed that the strategy expressing the probability is also the strategy. \"The distribution of these variables p / M is called a counter-strategy.\" \"The distribution of these variables p / M is called a counter-strategy.\" \"The distribution of these variables p / M is also called a counter-strategy.\" \"The distribution of these variables p / M is called a counter-strategy, where the probability is expressed.\" \"The distribution of these variables p / M is called a counter-strategy.\" The distribution of these variables p / M is called a counter-strategy. \"\" The distribution of these variables p / M is called a counter-strategy, where the probability is expressed. \""}, {"heading": "3. The proposed algorithm", "text": "Our algorithm is based on the basic idea underlying the algorithm. (2011), so we start with a short overview of Balaton. (2011), so we start with a short overview of Balaton. (2011), so we start with a short report about Balaton. (2011), so we start with a short report about Balaton. (2011), so we start with a short report about Balaton. (2011), so we start with a detailed report. (2011), so we start with a detailed report. (2011), so we end with a report. (2011), so we end with a report. (2011), in which it is sufficient to understand the need for action. (2012), so there will be a smaller expected loss, and so the action i is eliminated when the actions are reviewed. (2011), so we will assess these actions. (2011), so we start with a report. (2011), so we will end. (2011), so we will end. (2011, so we will end. (2011), so we will end."}, {"heading": "4. Analysis of the algorithm", "text": "The first theorem in this section is an individual upper limit of regretting CBP.Theorem 1. Let's (L, H) be a N of M. (D) Let's (L, H) be a N of M. (D) Let's (D, H) be a N of M. (D, H) Let's (D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D"}, {"heading": "5. Experiments", "text": "We show the results of the preceding sections using examples of dynamic pricing and a locally observable game. We compare the results of CBP with two other algorithms: Balaton (Barto \u0301 k et al., 2011), which, as mentioned earlier in the paper, is the first algorithm to achieve O (T 2 / 3) minimax regret for all locally observable finite stochastic partial surveillance games; and FeedExp3 (Piccolboni & Schindelhauer, 2001), which achieves O (T 2 / 3) minimax regret for all non-hopeless finite partial surveillance games, even against opposing opponents."}, {"heading": "5.1. A locally observable game", "text": "The game we use to compare CBP and Balaton has 3 actions and 3 results. The game is described with the loss and feedback matrices: L = 1 1 00 1 1 1 0 1; H = a b bb a b b b b b b a. We executed the algorithms 10 times for 15 different stochastic strategies. We averaged the results for each strategy and then determined the maximum across the 15 strategies. Figure 2 (a) shows the empirical Minimax repentance calculated in the manner described above. Furthermore, Figure 2 (b) shows the regret of the algorithms against one of the opponents, averaged over 100 runs. The results indicate that CBP outperforms both FeedExp and Balaton."}, {"heading": "5.2. Dynamic Pricing", "text": "In Dynamic Pricing, a seller (player) sets a price for his product at each step, while a buyer (opponent) secretly sets a maximum price that he is willing to pay. Feedback to the seller is \"buy\" or \"no-buy,\" while his loss is either a predetermined constant (no-buy) or the difference between prices (buy).The finite version of the game can be described with the following matrices: L = 0 1 \u00b7 \u00b7 \u00b7 N \u2212 1 c 0 \u00b7 \u00b7 \u00b7 N \u2212 2........... c \u00b7 \u00b7 c 0 H = y \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 y....... n \u00b7 \u00b7 \u00b7 n y This game is not locally observable and therefore \"hard\" (Barto-k et al., 2011). Simple linear algebra indicates that the locally observable shareholder pairs are the \"successive shareholder pairs.\""}], "references": [{"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Audibert", "J-Y", "S. Bubeck"], "venue": "COLT", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Toward a classification of finite partial-monitoring games", "author": ["G. Bart\u00f3k", "D. P\u00e1l", "Szepesv\u00e1ri", "Cs"], "venue": "ALT", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2010}, {"title": "Minimax regret of finite partial-monitoring games in stochastic environments", "author": ["G. Bart\u00f3k", "D. P\u00e1l", "Szepesv\u00e1ri", "Cs"], "venue": "In COLT,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2011}, {"title": "Regret minimization under partial monitoring", "author": ["N. Cesa-Bianchi", "G. Lugosi", "G. Stoltz"], "venue": "In Information Theory Workshop,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "No internal regret via neighborhood watch", "author": ["D.P. Foster", "A. Rakhlin"], "venue": "CoRR, abs/1108.6088,", "citeRegEx": "Foster and Rakhlin,? \\Q2011\\E", "shortCiteRegEx": "Foster and Rakhlin", "year": 2011}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1994}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["A. Piccolboni", "C. Schindelhauer"], "venue": "COLT", "citeRegEx": "Piccolboni and Schindelhauer,? \\Q2001\\E", "shortCiteRegEx": "Piccolboni and Schindelhauer", "year": 2001}, {"title": "Aggregating strategies", "author": ["V.G. Vovk"], "venue": "In Annual Workshop on Computational Learning Theory,", "citeRegEx": "Vovk,? \\Q1990\\E", "shortCiteRegEx": "Vovk", "year": 1990}], "referenceMentions": [{"referenceID": 2, "context": "As an implication, we show that under some reasonable additional assumptions, the algorithm enjoys an O( \u221a T ) regret in Dynamic Pricing, proven to be hard by Bart\u00f3k et al. (2011).", "startOffset": 159, "endOffset": 180}, {"referenceID": 7, "context": "Since Vovk (1990) and Littlestone & Warmuth (1994) we know that for full-information games, the minimax regret scales as \u0398( \u221a T logN).", "startOffset": 6, "endOffset": 18}, {"referenceID": 7, "context": "Since Vovk (1990) and Littlestone & Warmuth (1994) we know that for full-information games, the minimax regret scales as \u0398( \u221a T logN).", "startOffset": 6, "endOffset": 51}, {"referenceID": 1, "context": "The individual regret of these kind of games has also been studied: Auer et al. (2002) showed that given any opponent strategy, the expected regret can be upper bounded by c \u2211 i\u2208N :\u03b4i 6=0 1 \u03b4i log T , where \u03b4i is the expected difference between the loss of action i and an optimal action.", "startOffset": 68, "endOffset": 87}, {"referenceID": 4, "context": "Their upper bound for non-hopeless games was tightened to O(T ) by Cesa-Bianchi et al. (2006), who also showed that there exists a game with a matching lower bound.", "startOffset": 67, "endOffset": 94}, {"referenceID": 2, "context": "The first steps towards classifying partialmonitoring games were made by Bart\u00f3k et al. (2010), who characterized almost all games with two outcomes.", "startOffset": 73, "endOffset": 94}, {"referenceID": 1, "context": "The Exp3 algorithm due to Auer et al. (2003) achieves almost the same regret, with an extra logarithmic term.", "startOffset": 26, "endOffset": 45}, {"referenceID": 1, "context": "The Exp3 algorithm due to Auer et al. (2003) achieves almost the same regret, with an extra logarithmic term. spectively. They also found that there exist games that are easy, but can not easily be \u201ctransformed\u201d to a bandit or full-information game. Later, Bart\u00f3k et al. (2011) proved the same results for finite stochastic partial monitoring, with any finite number of outcomes.", "startOffset": 26, "endOffset": 278}, {"referenceID": 1, "context": "The Exp3 algorithm due to Auer et al. (2003) achieves almost the same regret, with an extra logarithmic term. spectively. They also found that there exist games that are easy, but can not easily be \u201ctransformed\u201d to a bandit or full-information game. Later, Bart\u00f3k et al. (2011) proved the same results for finite stochastic partial monitoring, with any finite number of outcomes. The condition that separates easy games from hard games is the local observability condition (see Definition 6). The algorithm Balaton introduced there works by eliminating actions that are thought to be suboptimal with high confidence. They conjectured in their paper that the same classification holds for nonstochastic games, without changing the condition. Recently, Foster & Rakhlin (2011) designed the algorithm NeighborhoodWatch that proves this conjecture to be true.", "startOffset": 26, "endOffset": 775}, {"referenceID": 2, "context": "In this paper, we extend the results of Bart\u00f3k et al. (2011). We introduce a new algorithm, called CBP for \u201cConfidence Bound Partial monitoring\u201d, with various desirable properties.", "startOffset": 40, "endOffset": 61}, {"referenceID": 2, "context": "The final, and potentially most impactful, aspect of our algorithm is that through additional assumptions on the set of opponent strategies, the minimax regret of even hard games can be brought down to \u0398\u0303( \u221a T )! While this statement may seem to contradict the result of Bart\u00f3k et al. (2011), in fact it does not.", "startOffset": 271, "endOffset": 292}, {"referenceID": 2, "context": "The following definitions, taken from Bart\u00f3k et al. (2011), are essential for understanding how the structure of L and H determines the \u201chardness\u201d of a game.", "startOffset": 38, "endOffset": 59}, {"referenceID": 3, "context": "Definition 6 (Local observability (Bart\u00f3k et al., 2011)).", "startOffset": 34, "endOffset": 55}, {"referenceID": 2, "context": "The main result of Bart\u00f3k et al. (2011) is that lo-", "startOffset": 19, "endOffset": 40}, {"referenceID": 2, "context": "Our algorithm builds on the core idea underlying algorithm Balaton of Bart\u00f3k et al. (2011), so we start with a brief review of Balaton.", "startOffset": 70, "endOffset": 91}, {"referenceID": 3, "context": "The upper bounds in Corollaries 2 and 3 both have matching lower bounds up to logarithmic factors (Bart\u00f3k et al., 2011), proving that CBP achieves near optimal regret in both locally observable and non-locally observable games.", "startOffset": 98, "endOffset": 119}, {"referenceID": 3, "context": "We compare the results of CBP to two other algorithms: Balaton (Bart\u00f3k et al., 2011) which is, as mentioned earlier in the paper, the first algorithm that achieves \u00d5( \u221a T ) minimax regret for all locally observable finite stochastic partial-monitoring games; and FeedExp3 (Piccolboni & Schindelhauer, 2001), which achieves O(T ) minimax regret on all non-hopeless finite partial-monitoring games, even against adversarial opponents.", "startOffset": 63, "endOffset": 84}, {"referenceID": 3, "context": "This game is not locally observable and thus it is \u201chard\u201d (Bart\u00f3k et al., 2011).", "startOffset": 58, "endOffset": 79}], "year": 2012, "abstractText": "We present a new anytime algorithm that achieves near-optimal regret for any instance of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for both \u201ceasy\u201d and \u201chard\u201d problems. For easy problems, it additionally achieves logarithmic individual regret. Most importantly, the algorithm is adaptive in the sense that if the opponent strategy is in an \u201ceasy region\u201d of the strategy space then the regret grows as if the problem was easy. As an implication, we show that under some reasonable additional assumptions, the algorithm enjoys an O( \u221a T ) regret in Dynamic Pricing, proven to be hard by Bart\u00f3k et al. (2011).", "creator": "LaTeX with hyperref package"}}}