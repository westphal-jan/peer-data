{"id": "1207.0099", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2012", "title": "Density-Difference Estimation", "abstract": "We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of first estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small error incurred in the first stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. The usefulness of the proposed method is also demonstrated experimentally.", "histories": [["v1", "Sat, 30 Jun 2012 14:21:46 GMT  (3469kb)", "http://arxiv.org/abs/1207.0099v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["masashi sugiyama", "takafumi kanamori", "taiji suzuki", "marthinus christoffel du plessis", "song liu", "ichiro takeuchi"], "accepted": true, "id": "1207.0099"}, "pdf": {"name": "1207.0099.pdf", "metadata": {"source": "CRF", "title": "Density-Difference Estimation", "authors": ["Masashi Sugiyama"], "emails": ["sugi@cs.titech.ac.jp", "kanamori@is.nagoya-u.ac.jp", "s-taiji@stat.t.u-tokyo.ac.jp", "christo@sg.cs.titech.ac.jp", "song@sg.cs.titech.ac.jp", "takeuchi.ichiro@nitech.ac.jp"], "sections": [{"heading": null, "text": "Key words Density difference, L2 removal, robustness, Kullback-Leibler divergence, estimation of core density."}, {"heading": "1 Introduction", "text": "In estimating a quantity consisting of two elements, a two-step approach of the first estimation of the two elements was performed separately and then the approximation of the target size based on the estimates of the two elements was often poorly, because the first stage is performed without taking into account the second stage and therefore a small error in the first stage can cause a major error in the second stage. To address this problem, it would be more appropriate to estimate the target size in a single process without estimating the two elements separately. A pioneering example following this general pattern is the recognition by the support engine (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998). Instead of estimating two probability distributions of patterns for positive and negative classes, the support vector machine learns directly the boundary between the two classes sufficient for pattern recognition."}, {"heading": "2 Density-Difference Estimation", "text": "In this section we propose a one-line method to estimate the difference between two probability densities from samples and to analyse their theoretical properties."}, {"heading": "2.1 Problem Formulation and Naive Approach", "text": "First, we formulate the problem of estimating the density difference. Suppose we are given two sets of independent and identically distributed estimates, namely X: = {xi} ni = 1 and X: = {x \"i\" n \"i\" = 1, which consist of probability distributions on R d with densities p (x) and p \"(x) or: X: = {xi} ni = 1 i.i.d. x (x), X: = {x\" i \"n\" i \"i\" = 1 i.i.d. p \"(x). Our goal is to estimate the difference f (x) between p (x) and p\" (x) from samples X and X: f (x): p (x). A naive approach to estimating the density difference and difference is the use of kernel density estimators p (KDEs) (Silverman, 1986)."}, {"heading": "2.2 Least-Squares Density-Difference Estimation", "text": "In our proposed approach, we fit a density difference model g (x) to the true density difference function f (x) under the squared loss: argmin g (g) \u2212 f (x) \u2212 2 dx. (1) We use the following linear in-parameter model as g (x): g (x) = b (x) = 1 x (x). (2), where b denotes the number of base functions, (x) = (x),. (b), (x), (x), (x), (x), is a bdimensional base function vector, (x). (2), where b is a b-dimensional parameter vector, and denotes transpose. In practice, we use the following non-parametric Gaussian model as g (x): g (x)."}, {"heading": "2.3 Theoretical Analysis", "text": "Here we theoretically examine the behaviour of the LSDD estimator."}, {"heading": "2.3.1 Parametric Convergence", "text": "First, we consider a linear parametric structure in which the basic functions are fixed in our density difference model (2). Suppose that n / (n + n \") legally converges to the normal distribution with meaning 0 and covariance matrixH \u2212 1 (((1 \u2212 \u03b7) V p + throuV p\") H \u2212 1, where V p denotes the covariance matrix of p (x) under the probability density p (x): V p: = 1 (x) \u2212 p) (3) p (x) dx, (6), and p the expectation of p (x) under the probability density p (x):."}, {"heading": "2.3.2 Non-Parametric Error Bound", "text": "Next, we will consider a non-parametric setup in which a density differential function is learned in a Gaussian reproduction core Hilbert-Raum (RKHS) (Aronszajn, 1950). Let us consider a slightly modified LSDD estimator, which is more suitable for a non-parametric error analysis: For n \u00b2 (x, x \u00b2), f \u00b2: = argmin g \u00b2 = exp (2L2 \u2212 2 (1nn \u00b2).LSDD estimation is a slightly modified LSDD estimator, which is more suitable for a non-parametric error analysis: For n \u00b2, f \u00b2: = argmin g \u00b2 -H3 (1nn \u00b2).LD standard denotes the optimal L2 standard and for the non-optimal LSD standard in RKHS H3. Then we can prove that there is a constant in all cases in which K -standard exists."}, {"heading": "2.4 Model Selection by Cross-Validation", "text": "The above mentioned theoretical analyses showed the superiority of LSDD. However, the practical performance of LSDD depends on the choice of models (i.e. the core width \u03c3 and the regularization parameter \u03bb). Here we show that the model can be optimized by cross-validation (CV). More specifically, we first divide the samples X = {xi} ni = 1 and X \u2032 = {x \u2032 i \u2032 n \u2032 i \u2032 (i.e. all samples without Xt and X \u2032 t) Tt = 1 and {X \u2032 t} Tt = 1. Then we get a density differential estimate f (x) of X\\ t and X \u2032 n \u2032 t (i.e. all samples without Xt and X \u2032 t) and calculate the duration error for Xt and X \u2032 t} t asCV (t): = = f (x)."}, {"heading": "3.1 Basic Form", "text": "If we replace f (p, p) with an LSDD estimator f (x) dx (x) and approximate the expectations with empirical averages, we obtain the following L2-distance estimator: L2 (p, p) \u2248 h-estimator. (9) Similarly, for another LSDD estimator, L2 (p, p) = f (x) 2dx, if f (x) is replaced by an LSDD estimator f (x), another L2-distance estimator results: L2 (p, p \u00b2) A L2-distance estimator (p, p \u00b2) A L2-distance estimator (p, p \u00b2) A L2-distance estimator (p, p \u00b2) A L2-distance estimator (p, p \u00b2) A L2-distance estimator (p, p \u00b2) A L2-distance estimator (p, p) A-distance estimator (p, p)"}, {"heading": "3.2 Reduction of Bias Caused by Regularization", "text": "Equation (9) and Equation (10) themselves yield approximations to L2 (p, p). Nevertheless, we argue that the use of their combination, defined by L \u00b2 2 (X, X): = 2h = 2h = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 2q = 1 (2). To explain the reason, we consider a generalized L2 distance estimator of the following form: \u03b2h = 2p (1 \u2212 \u03b2)."}, {"heading": "4 Experiments", "text": "In this section we evaluate the performance of LSDD experimentally."}, {"heading": "4.1 Numerical Examples", "text": "First, we show numerical examples using artificial data sets."}, {"heading": "4.1.1 LSDD vs. KDE", "text": "We are experimentally comparing the behavior of LSDD and the KDE-based method. Letp (x) = more accurate estimates (x; (\u00b5, 0,.., 0), (4\u03c0) \u2212 1Id), p \u2032 (x) = N (x; (0, 0,.., 0), (4\u03c0) \u2212 1Id), where N (\u00b5, \u03a3) indicates the normal multidimensional density with the mean vector \u00b5 and the variance covariance matrix with respect to x, and Id indicates the d-dimensional identity matrix. We will first demonstrate how LSDD and KDE behave under d = 1 and n \u2032 = 200. Figure 1 shows the data strings, densities and density differences estimated by KDE, and the density difference estimated by LSDD for \u00b5 = 0 (i.e., f (x) = p (x))."}, {"heading": "4.2 Applications", "text": "Next, we apply LSDD to semi-monitored class balance estimates in the context of class changes and detection of change points in time series."}, {"heading": "4.2.1 Semi-Supervised Class-Balance Estimation", "text": "In real pattern recognition tasks, changes in the class balance are often observed. Then, significant estimation distortions can be caused, since the class balance in the training data set does not reflect that of the test data sets. In this context, we consider a pattern recognition task in the classification of patterns x-Rd to class y-Rd (Chapelle et al., 2006). Our goal is to learn the class balance of a test data set in a semi-supervised learning setup, in which unlabeled test samples are provided in addition to the labeled training samples (Chapelle et al., 2006). The class balance in the test set can be estimated by selecting a mixture of class-wise training input densities, hyptrain (x-y = + 1) + (1 \u2212 \u03c0) ptrain (x-y = \u2212 1) with the test input density test (x) (Saerens et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al."}, {"heading": "4.2.2 Unsupervised Change Detection", "text": "The goal of change detection is to detect abrupt property changes behind time series data. Let y (t), y (t), y (t + 1), by which time-dependent information can be a subsequence of time at time t with length k. We treat the subsequence Y (t) as a sample, instead of a single dot y (t) through which time-dependent information can be absorbed naturally (Kawahara & Sugiyama, 2012). Let Y (t) be a set of r retrospective subsequence samples starting at time t: Y (t), Y (t + 1), three (t + 1), the. \"We (t + r \u2212 1), the.\""}, {"heading": "5 Conclusions", "text": "In this paper, we proposed a method for directly estimating the difference between two probability density functions without density estimation.The proposed method, called the Least-squares Density Difference (LSDD), was derived within the framework of estimating the least squares of the core, and its solution can be calculated analytically in a calculation-4http: / / research.nii.ac.jp / src / en / CENSREC-1-C.html 5http: / hasc.jp / hc2011 / ally efficient and stable. Furthermore, LSDD is equipped with cross-validation, and thus all tuning parameters such as core width and regulation parameters can be systematically and objectively optimized. We demonstrated the asymptotic normality of LSDD in a parametric setup and derived LSDD experiments tied to a finite sample."}, {"heading": "Acknowledgments", "text": "The authors thank Wittawat Jitkrittum for his comments. Masashi Sugiyama was supported by MEXT KAKENHI 23300069, Takafumi Kanamori by MEXT KAKENHI 24500340, Taiji Suzuki by MEXT KAKENHI 22700289 and the Aihara Project, the first JSPS program initiated by CSTP, Marthinus Christoffel du Plessis was supported by MEXT Scholarship, Song Liu by JST PRESTO program and Ichiro Takeuchi by MEXT KAKENHI 23700165."}, {"heading": "A Technical Details of Non-Parametric Convergence", "text": "Analysis in section 2.3.2Initially we define the linear operators Pn, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P, P,"}], "references": [{"title": "Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates", "author": ["N. Anderson", "P. Hall", "D. Titterington"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Anderson et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 1994}, {"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Transactions of the American Mathematical Society, 68, 337\u2013404.", "citeRegEx": "Aronszajn,? 1950", "shortCiteRegEx": "Aronszajn", "year": 1950}, {"title": "Non-rigid medical image registration by maximisation of quadratic mutual information", "author": ["J. Atif", "X. Ripoche", "A. Osorio"], "venue": "IEEE 29th Annual Northeast Bioengineering Conference (pp. 32\u201340)", "citeRegEx": "Atif et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Atif et al\\.", "year": 2003}, {"title": "Multiple regression and estimation of the mean of a multivariate normal distribution (Technical Report 51)", "author": ["A.J. Baranchik"], "venue": "Department of Statistics, Stanford University, Stanford, CA, USA.", "citeRegEx": "Baranchik,? 1964", "shortCiteRegEx": "Baranchik", "year": 1964}, {"title": "Robust and efficient estimation by minimising a density power divergence", "author": ["A. Basu", "I.R. Harris", "N.L. Hjort", "M.C. Jones"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Basu et al\\.", "year": 1998}, {"title": "Integrated squared error estimation of normal mixtures", "author": ["P. Besbeas", "B.J.T. Morgan"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Besbeas and Morgan,? \\Q2004\\E", "shortCiteRegEx": "Besbeas and Morgan", "year": 2004}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (pp", "citeRegEx": "Boser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Boser et al\\.", "year": 1992}, {"title": "Semi-supervised learning of class balance under class-prior change by distribution matching", "author": ["M.C. Du Plessis", "M. Sugiyama"], "venue": "Proceedings of 29th International Conference on Machine Learning", "citeRegEx": "Plessis and Sugiyama,? \\Q2012\\E", "shortCiteRegEx": "Plessis and Sugiyama", "year": 2012}, {"title": "Highest density difference region estimation with application to flow cytometric data", "author": ["T. Duong", "I. Koch", "M.P. Wand"], "venue": "Biometrical Journal,", "citeRegEx": "Duong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2009}, {"title": "Optimal learning rates for least squares SVMs using Gaussian kernels", "author": ["M. Eberts", "I. Steinwart"], "venue": "Advances in neural information processing systems", "citeRegEx": "Eberts and Steinwart,? \\Q2011\\E", "shortCiteRegEx": "Eberts and Steinwart", "year": 2011}, {"title": "An introduction to the bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1993}, {"title": "On the best obtainable asymptotic rates of convergence in estimation of a density function at a point", "author": ["R.H. Farrell"], "venue": "The Annals of Mathematical Statistics, 43, 170\u2013180.", "citeRegEx": "Farrell,? 1972", "shortCiteRegEx": "Farrell", "year": 1972}, {"title": "Quadratic mutual information for dimensionality reduction and classification", "author": ["D.M. Gray", "J.C. Principe"], "venue": "Proceedings of SPIE (p", "citeRegEx": "Gray and Principe,? \\Q2010\\E", "shortCiteRegEx": "Gray and Principe", "year": 2010}, {"title": "Covariate shift by kernel mean matching", "author": ["A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning,", "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "On nonparametric discrimination using density differences", "author": ["P. Hall", "M.P. Wand"], "venue": null, "citeRegEx": "Hall and Wand,? \\Q1988\\E", "shortCiteRegEx": "Hall and Wand", "year": 1988}, {"title": "Nonparametric and semiparametric models", "author": ["W. H\u00e4rdle", "M. M\u00fcller", "S. Sperlich", "A. Werwatz"], "venue": null, "citeRegEx": "H\u00e4rdle et al\\.,? \\Q2004\\E", "shortCiteRegEx": "H\u00e4rdle et al\\.", "year": 2004}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Statistical analysis of kernel-based least-squares density-ratio estimation", "author": ["T. Kanamori", "T. Suzuki", "M. Sugiyama"], "venue": "Machine Learning,", "citeRegEx": "Kanamori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2012}, {"title": "Sequential change-point detection based on direct density-ratio estimation", "author": ["Y. Kawahara", "M. Sugiyama"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "Kawahara and Sugiyama,? \\Q2012\\E", "shortCiteRegEx": "Kawahara and Sugiyama", "year": 2012}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Kullback and Leibler,? \\Q1951\\E", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Probability density difference-based active contour for ultrasound image segmentation", "author": ["B. Liu", "H.D. Cheng", "J. Huang", "J. Tian", "X. Tang", "J. Liu"], "venue": "Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Change-point detection in timeseries data by relative density-ratio estimation (Technical Report 1203.0453)", "author": ["S. Liu", "M. Yamada", "N. Collier", "M. Sugiyama"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Detection of activities and events without explicit categorization. Proceedings of the 3rd International Workshop on Video Event Categorization, Tagging and Retrieval for Real-World Applications (VECTaR2011) (pp. 1532\u20131539)", "author": ["M. Matsugu", "M. Yamanaka", "M. Sugiyama"], "venue": null, "citeRegEx": "Matsugu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Matsugu et al\\.", "year": 2011}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "On the estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The Annals of Mathematical Statistics, 33, 1065\u20131076.", "citeRegEx": "Parzen,? 1962", "shortCiteRegEx": "Parzen", "year": 1962}, {"title": "Kullback-Leibler divergence estimation of continuous distributions", "author": ["F. P\u00e9rez-Cruz"], "venue": "Proceedings of IEEE International Symposium on Information Theory (pp. 1666\u20131670). Nice, France.", "citeRegEx": "P\u00e9rez.Cruz,? 2008", "shortCiteRegEx": "P\u00e9rez.Cruz", "year": 2008}, {"title": "Inferences for case-control and semiparametric two-sample density ratio models", "author": ["J. Qin"], "venue": "Biometrika, 85, 619\u2013630.", "citeRegEx": "Qin,? 1998", "shortCiteRegEx": "Qin", "year": 1998}, {"title": "Linear statistical inference and its applications", "author": ["C.R. Rao"], "venue": "New York, NY, USA: Wiley.", "citeRegEx": "Rao,? 1965", "shortCiteRegEx": "Rao", "year": 1965}, {"title": "Regularized least-squares classification. Advances in Learning Theory: Methods, Models and Applications (pp. 131\u2013154)", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": null, "citeRegEx": "Rifkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2003}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton, NJ, USA: Princeton University Press.", "citeRegEx": "Rockafellar,? 1970", "shortCiteRegEx": "Rockafellar", "year": 1970}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure", "author": ["M. Saerens", "P. Latinne", "C. Decaestecker"], "venue": "Neural Computation,", "citeRegEx": "Saerens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2002}, {"title": "Parametric statistical modeling by minimum integrated square error", "author": ["D.W. Scott"], "venue": "Technometrics, 43, 274\u2013285.", "citeRegEx": "Scott,? 2001", "shortCiteRegEx": "Scott", "year": 2001}, {"title": "Information divergence estimation based on datadependent partitions", "author": ["J. Silva", "S.S. Narayanan"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Silva and Narayanan,? \\Q2010\\E", "shortCiteRegEx": "Silva and Narayanan", "year": 2010}, {"title": "Density estimation for statistics and data analysis", "author": ["B.W. Silverman"], "venue": "London, UK: Chapman and Hall.", "citeRegEx": "Silverman,? 1986", "shortCiteRegEx": "Silverman", "year": 1986}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": null, "citeRegEx": "Steinwart and Christmann,? \\Q2008\\E", "shortCiteRegEx": "Steinwart and Christmann", "year": 2008}, {"title": "Fast rates for support vector machines using Gaussian kernels", "author": ["I. Steinwart", "C. Scovel"], "venue": "The Annals of Statistics,", "citeRegEx": "Steinwart and Scovel,? \\Q2004\\E", "shortCiteRegEx": "Steinwart and Scovel", "year": 2004}, {"title": "Density ratio estimation in machine learning", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": null, "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Density ratio matching under the Bregman divergence: A unified framework of density ratio estimation", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": "Annals of the Institute of Statistical Mathematics", "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Direct importance estimation for covariate shift adaptation", "author": ["M. Sugiyama", "T. Suzuki", "S. Nakajima", "H. Kashima", "P. von B\u00fcnau", "M. Kawanabe"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "Feature extraction by non-parametric mutual information maximization", "author": ["K. Torkkola"], "venue": "Journal of Machine Learning Research, 3, 1415\u20131438.", "citeRegEx": "Torkkola,? 2003", "shortCiteRegEx": "Torkkola", "year": 2003}, {"title": "Statistical learning theory", "author": ["V.N. Vapnik"], "venue": "New York, NY, USA: Wiley.", "citeRegEx": "Vapnik,? 1998", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "Divergence estimation of contiunous distributions based on data-dependent partitions", "author": ["Q. Wang", "S.R. Kulkarmi", "S. Verd\u00fa"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "A seminal example that follows this general idea is pattern recognition by the support vector machine (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998): Instead of separately estimating two probability distributions of patterns for positive and negative classes, the support vector machine directly learns the boundary between the two classes that is sufficient for pattern recognition.", "startOffset": 102, "endOffset": 159}, {"referenceID": 40, "context": "A seminal example that follows this general idea is pattern recognition by the support vector machine (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998): Instead of separately estimating two probability distributions of patterns for positive and negative classes, the support vector machine directly learns the boundary between the two classes that is sufficient for pattern recognition.", "startOffset": 102, "endOffset": 159}, {"referenceID": 26, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 38, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 13, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 16, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 23, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 17, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 30, "context": "Density differences are useful for various purposes such as class-balance estimation under class-prior change (Saerens et al., 2002; Du Plessis & Sugiyama, 2012), change-point detection in time series (Kawahara & Sugiyama, 2012; Liu et al.", "startOffset": 110, "endOffset": 161}, {"referenceID": 21, "context": ", 2002; Du Plessis & Sugiyama, 2012), change-point detection in time series (Kawahara & Sugiyama, 2012; Liu et al., 2012), feature extraction (Torkkola, 2003), video-based event detection (Matsugu et al.", "startOffset": 76, "endOffset": 121}, {"referenceID": 39, "context": ", 2012), feature extraction (Torkkola, 2003), video-based event detection (Matsugu et al.", "startOffset": 28, "endOffset": 44}, {"referenceID": 22, "context": ", 2012), feature extraction (Torkkola, 2003), video-based event detection (Matsugu et al., 2011), flow cytometric data analysis (Duong et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 8, "context": ", 2011), flow cytometric data analysis (Duong et al., 2009), ultrasound image segmentation (Liu et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 20, "context": ", 2009), ultrasound image segmentation (Liu et al., 2010), non-rigid image registration (Atif et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 2, "context": ", 2010), non-rigid image registration (Atif et al., 2003), and image-based target recognition (Gray", "startOffset": 38, "endOffset": 57}, {"referenceID": 0, "context": "We also apply LSDD to L-distance estimation and show that it is more accurate than the difference of KDEs, which tends to severely under-estimate the L-distance (Anderson et al., 1994).", "startOffset": 161, "endOffset": 184}, {"referenceID": 4, "context": "Compared with the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951), the L-distance is more robust against outliers (Basu et al., 1998; Scott, 2001; Besbeas & Morgan, 2004).", "startOffset": 126, "endOffset": 182}, {"referenceID": 31, "context": "Compared with the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951), the L-distance is more robust against outliers (Basu et al., 1998; Scott, 2001; Besbeas & Morgan, 2004).", "startOffset": 126, "endOffset": 182}, {"referenceID": 33, "context": "A naive approach to density-difference estimation is to use kernel density estimators (KDEs) (Silverman, 1986).", "startOffset": 93, "endOffset": 110}, {"referenceID": 27, "context": "Then the central limit theorem (Rao, 1965) asserts that \u221a nn n+n (\u03b8\u0302\u2212 \u03b8) converges in law to the normal distribution with mean 0 and covariance matrix", "startOffset": 31, "endOffset": 42}, {"referenceID": 1, "context": "2 Non-Parametric Error Bound Next, we consider a non-parametric setup where a density-difference function is learned in a Gaussian reproducing kernel Hilbert space (RKHS) (Aronszajn, 1950).", "startOffset": 171, "endOffset": 188}, {"referenceID": 11, "context": "It is known that, if the naive KDE with a Gaussian kernel is used for estimating a probability density with regularity \u03b1 > 2, the optimal learning rate cannot be achieved (Farrell, 1972; Silverman, 1986).", "startOffset": 171, "endOffset": 203}, {"referenceID": 33, "context": "It is known that, if the naive KDE with a Gaussian kernel is used for estimating a probability density with regularity \u03b1 > 2, the optimal learning rate cannot be achieved (Farrell, 1972; Silverman, 1986).", "startOffset": 171, "endOffset": 203}, {"referenceID": 24, "context": "To achieve the optimal rate by KDE, we should choose a kernel specifically tailored to each regularity \u03b1 (Parzen, 1962).", "startOffset": 105, "endOffset": 119}, {"referenceID": 9, "context": "More precise statement of the result and its complete proof are provided in Appendix A, where we utilize the mathematical technique developed in Eberts and Steinwart (2011) for a regression problem.", "startOffset": 145, "endOffset": 173}, {"referenceID": 29, "context": "convex duality (Rockafellar, 1970):", "startOffset": 15, "endOffset": 34}, {"referenceID": 3, "context": "Following the same line as Baranchik (1964), the positive-part estimator may be more accurate:", "startOffset": 27, "endOffset": 44}, {"referenceID": 0, "context": "Such smoother density estimates are accurate as density estimates, but the difference of smoother density estimates yields a smaller L-distance estimate (Anderson et al., 1994).", "startOffset": 153, "endOffset": 176}, {"referenceID": 38, "context": "Next, we draw n = n = 100 samples from p(x) and p(x), and estimate the L-distance by LSDD and the KL-divergence by the Kullback-Leibler importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 176, "endOffset": 220}, {"referenceID": 23, "context": "Next, we draw n = n = 100 samples from p(x) and p(x), and estimate the L-distance by LSDD and the KL-divergence by the Kullback-Leibler importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 176, "endOffset": 220}, {"referenceID": 41, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 38, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 25, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 23, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 4, "context": "This result implies that the L-distance is less sensitive to outliers than the KL-divergence, which well agrees with the observation given in Basu et al. (1998). Next, we draw n = n = 100 samples from p(x) and p(x), and estimate the L-distance by LSDD and the KL-divergence by the Kullback-Leibler importance estimation procedure (KLIEP) (Sugiyama et al.", "startOffset": 142, "endOffset": 161}, {"referenceID": 30, "context": "\u03c0ptrain(x|y = +1) + (1\u2212 \u03c0)ptrain(x|y = \u22121), with the test input density ptest(x) (Saerens et al., 2002), where \u03c0 \u2208 [0, 1] is a mixing coefficient to learn.", "startOffset": 81, "endOffset": 103}, {"referenceID": 28, "context": "Figure 10 plots the mean and standard error of the squared difference between true and estimated class balances \u03c0 and the misclassification error by a weighted regularized least-squares classifier (Rifkin et al., 2003) over 1000 runs.", "startOffset": 197, "endOffset": 218}, {"referenceID": 38, "context": "estimated by the KL importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 60, "endOffset": 104}, {"referenceID": 23, "context": "estimated by the KL importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 60, "endOffset": 104}], "year": 2012, "abstractText": "We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of first estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small error incurred in the first stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. The usefulness of the proposed method is also demonstrated experimentally.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}