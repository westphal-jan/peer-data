{"id": "1207.3859", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2012", "title": "Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning", "abstract": "We consider the estimation of an i.i.d. vector $\\xbf \\in \\R^n$ from measurements $\\ybf \\in \\R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. A novel method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\\xbf$ is presented. The proposed algorithm is a generalization of a recently-developed technique by Vila and Schniter that uses expectation-maximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The proposed methodology can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.", "histories": [["v1", "Tue, 17 Jul 2012 01:50:46 GMT  (105kb,D)", "https://arxiv.org/abs/1207.3859v1", "16 pages, 3 figures"], ["v2", "Thu, 19 Jul 2012 13:35:50 GMT  (101kb,D)", "http://arxiv.org/abs/1207.3859v2", "10 pages, 3 figures"], ["v3", "Sat, 1 Dec 2012 23:30:36 GMT  (180kb,D)", "http://arxiv.org/abs/1207.3859v3", "14 pages, 3 figures"]], "COMMENTS": "16 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.IT cs.LG math.IT", "authors": ["ulugbek kamilov", "sundeep rangan", "alyson k fletcher", "michael unser"], "accepted": true, "id": "1207.3859"}, "pdf": {"name": "1207.3859.pdf", "metadata": {"source": "CRF", "title": "Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning", "authors": ["Ulugbek S. Kamilov", "Sundeep Rangan", "Alyson K. Fletcher"], "emails": ["ulugbek.kamilov@epfl.ch)", "srangan@poly.edu)", "afletcher@ucsc.edu)", "michael.unser@epfl.ch)"], "sections": [{"heading": null, "text": "I. INTRODUCTION Consider the estimation of a random vector x-Rn from the measurement model illustrated in Figure 1. The random vector x, which is used to have independent and identically distributed (i.i.d.) components xj \u0445 PX, is passed through a known linear transform that outputs z = Ax. The components of y-Rm are generated by a componentwise transfer function PY | Z. This work addresses the cases where the distributions PX and PY have some unknown parameters. S. Rangan's work was supported by the National Science Foundation under Grant No. 1116589. U.S. Kamilov and M. Unser were supported by the European Commission under Grant ERC-2010-AdG 267439-FUN-SP.U. S. Kamilov (email: ulugbek.kamilov @ epfl.ch) is associated with Biomedical Imaging Group, E-cole Technik F\u00e9d\u00e9pend.U."}, {"heading": "A. Related Literature", "text": "As I said, the adaptive GAMP method proposed here can be considered a generalization of EM methods [22] - [23], the previous PX is described by a generic L-term mixture (GM) whose parameters are identified by an EM method [26]. The \"anticipation\" or E-step is performed by the GAMP, which can approximately determine the marginal posterior distributions of the components."}, {"heading": "B. Outline of the Paper", "text": "The paper is structured as follows: In Section II we discuss the non-adaptive GAMP and the corresponding state development equations. In Section III we present adaptive GAMP and describe the learning of ML parameters. In Section IV we present the main theories that characterize the asymptotic performance of the adaptive GAMP and demonstrate its consistency. In Section V we provide numerical experiments to demonstrate the application of the method. Section VI concludes the paper. A conference version of this paper appeared in [30]."}, {"heading": "II. REVIEW OF GAMP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. GAMP Algorithm", "text": "Before describing the adaptive GAMP algorithm, it is useful to review the basic (non-adaptive) GAMP functions as specific GAMP functions = other basic GAMP functions. Consider the estimation problem in Fig. 1, where the component-wise distributions to the inputs and outputs have some parametric estimation problems. [18] The GAMP algorithm of some parameter sets.The GAMP algorithm of [18] can be considered a class of methods for estimating the MP vectors MP-x and z-estimates of the MP-z estimation problems of the distributions and x-z estimation problems of some parameter sets.The GAMP algorithm of [18] can be considered a class of methods for estimating the MP vectors MP-x and z-estimates of the MP-z estimation problems. In contrast, the adaptive GAMP method discussed in Section III can be considered a common estimation of the parameter GP vectors and the GAMP estimation algorithm of the class 18."}, {"heading": "B. State Evolution Analysis", "text": "In addition to its mathematical simplicity and universality, a key motivation of the GAMP algorithm is the assumption that its asymptotic behavior can be accurately characterized if A is a large Gaussian transform. Asymptotic behavior is described by what is called a state evolution (SE) analysis. Meanwhile, there is a large number of random realizations of GAMP algorithms indexed by dimension n, taking into account the following assumptions: (a) For each n based on the framework in [16]. Assumption 1: Consider a sequence of random realizations of GAMP algorithms indexed by dimension n that fulfills the following assumptions."}, {"heading": "III. ADAPTIVE GAMP", "text": "As described in the previous paragraph, the standard GAMP algorithm of [18] considers the case when the parameters \u03bbx and \u03bbz are known in the distributions in (1). The adaptive GAMP method proposed in this paper and shown in Algorithm 1 is an extension of the standard GAMP method, which includes the simultaneous identification of the parameters \u03bbx and \u03bbz along with the estimation of the vectors x and z. The ekey modification is the introduction of the two matching functions: Htz (p t, y, \u03c4 tp) and H t x (rt, \u03c4 tr). In each iteration, these functions output estimates according to the data pt, y, rt, \u03c4 tp and \u0442. We saw that the standard GAMP method corresponds to the matching functions in (2), which output fixed values that do not depend on the data and can be used when the true parameters for which we do not know the maximum probability are known."}, {"heading": "A. ML Parameter Estimation", "text": "To understand how to estimate parameters via the fit functions, we note that from theorem 1, we know that the distribution of the components of rt is identical to the empirical distribution of the components of rt in (8). To this end, the distribution of Rt depends only on three parameters - \u03b1tr, \u0435t r and \u03bbx. It is therefore natural to try to try to determine these parameters from the empirical distribution of the components of rt.To achieve this, let the right side be the probability density of a random variable R with the distribution variable Y (r, \u03bbx, \u03b1r, \u03b1r) = log pR (r, p), (12), where the right side is the probability density of a random variable R with the distribution variable Y with the distribution variable R = x, X \u00b2 PX \u00b2 PX \u00b2 PX (\u00b2 Pimperx), \u03b1x, P (0 \u00b2 Pimperprospect Kr).Then we can try to perform a maximum probability Z with the distribution variable R."}, {"heading": "B. Relation to EM-GAMP", "text": "It is useful to briefly compare the above ML parameters with the EM GAMP method used by Vila and Schniter in [22], [23] and Krzakala et. al. in [24], [25]. Both of these methods combine the Bavarian AMP method [14], [15] or GAMP algorithms [18] with a standard EM method [26] as follows: Firstly, the algorithms use the sum product version of the AMP / GAMP algorithms so that the results can provide an estimate of the posterior distributions on the components of the x given current parameter estimates. Specifically, for each iteration t, define the distribution of the PMP algorithms (xj)."}, {"heading": "IV. CONVERGENCE AND ASYMPTOTIC CONSISTENCY WITH GAUSSIAN TRANSFORMS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. General State Evolution Analysis", "text": "Before demonstrating the asymptotic consistency of the adaptive GAMP method with ML adjustment, we will first demonstrate the following general convergence results: (a) Assumption 2: Consider the adaptive GAMP algorithm, which is based on a sequence of problems indexed by dimension n and fulfills the following assumptions: (a) Assumption 1 (a) to (c) with k = 2. (b) For each t, the adaptation function Htx (r, percr) can function as a functional over r satisfaction of the following weak pseudo-Lipschitz continuity property: Consider a sequence of vectors r = r (n) and a sequence of scalars (n)."}, {"heading": "B. Asymptotic Consistency with ML Adaptation", "text": "We use Theorem 2 to prove the asymptotic consistency of the adaptive GAMP method with the ML parameter estimation described in Section III-A. The following two assumptions can be considered as identification conditions. (Definition 1: Consider a family of distributions, {PX (x | \u03bbx), a set of parameters Sx (\u03b1r, \u0445r) of a Gaussian channel and a function \u03c6x (r). We say that PX (x | \u03bbx) is identifiable with Gaussian outputs with parameter set Sx and function \u0445r-x, if: (a) The sentences Sx and \u0432\u0430\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441simpsimpsimpsimpsimpsimpsimpsimpsimpsimpsimpsimpsimpsimpsimppoint point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point point"}, {"heading": "V. NUMERICAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Estimation of a Gauss-Bernoulli input", "text": "It is well known that estimators like LASSO offer certain optimal Minmax performance over a large class of sparse distributions [22], which showed great gains of the EMGAMP method due to their ability to learn priority. Here we illustrate the performance and asymptotic consistency of the adaptive GAMP in a simple compressed example. Specifically, we consider the estimation of a sparse vector x Rn from m noisy measurementsy = Ax + w, where the additive noise is i.2x."}, {"heading": "B. Estimation of a Nonlinear Output Classification Function", "text": "As a second example, we consider the estimation of the linear-nonlinear Poisson (LNP) cascade model (32), which has been successfully used to characterize neuronal spike responses in early sensory pathways of the visual system. In the context of the LNP cascade model, the vector x-Rn represents the linear filter that models the linear receiver field of the neuron. AMP techniques combined with parameter estimation have recently been proposed for neural receptive field estimation and connectivity detection in [44]. As in Section V-A, we model x as a Gauss-Bernoulli vector of unknown parameters."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "We have presented an adaptive GAMP method for estimating i.i.d. vectors x, which is observed by a known linear transformation, followed by an arbitrary, component-by-component random transformation. The method, which represents a generalization of the EMGAMP methodology of [22] - [25], which transforms both the vector x and parameters in the source and component-by-component. In the case of large i.i.d. Gaussian transformations, the adaptive GAMP method is shown to be asymptotically consistent, since the parameter estimates converge with the true values. This convergence result includes a large class of models with essentially arbitrarily complex parameterizations. Moreover, the algorithm is computationally efficient because it reduces the vector-weighted estimation problem to a sequence of scalar estimation problems in Gaussian noise. We believe that this method is not applicable to a large class of linear-computational non-linear learning models with detectable problems in a guaranteed spectrum of Gaussian noise."}, {"heading": "APPENDIX A SUM-PRODUCT GAMP EQUATIONS", "text": "As described in [18], the sum product estimation can be implemented with the estimation functions Gtx (r, \u03c4r, \u03bb x): = E [X | R = \u03c4r, \u03c4r, \u03bb \u0445 x], (33a) Gtz (p, y, \u03c4p, \u03bb \u0445 z): = E [Z | P = p, Y = y, \u03c4p, \u03bb z], (33b) Gts (p, y, \u03c4p, \u03bbp, \u03bbz): = 1\u03c4p (Gtz (p, y, \u03c4p, \u03bbp, \u03bb z) \u2212 p (33c), where the expectations regarding the scalar random variables P = X + Vx, Vx \u00b2 N (0, \u03c4r), X \u00b2 x), (34a)."}, {"heading": "APPENDIX B CONVERGENCE OF EMPIRICAL DISTRIBUTIONS", "text": "Bayati and Montanari use certain deterministic models on the vectors in their analysis in [16] and then prove convergence properties of related empirical distributions. To apply the same analysis here, we must check some of their definitions. We say a function \u03c6: Rr \u2192 Rs is pseudo-lipschitz of the order k > 1, if there is a L > 0 for each x, y, x, x, x, x, x, x, x, x, k, k, k, k, k, k, k, 1, 1, x, y, x, y,. Now we assume that for each n, 2,... v (n) a series of vectorsv (n) = {vi (n), i = 1,. \"(n)}, (37) where each element vi (n), Rs, and\" (n) is the number of elements in the set."}, {"heading": "APPENDIX C PROOF OF THEOREM 2", "text": "The proof follows along the adaptation argument of [41]. We use the tilde superscripts on quantities such as x-t, r-tr, p-tr, p-tp, s-tp, and z-tp, s-1 = s-1 = 0). The non-adaptive GAMP algorithm has the same initial conditions as the adaptive algorithm (i.e. x-tp, s-tx, s-1 = s-1 = 0), but with the same initial conditions as the adaptive algorithm (i.e. x-tp), i.e. we replace lines 7, 8, and 15 withz = G t (p-t)."}, {"heading": "APPENDIX D PROOF OF THEOREM 3", "text": "Part (a) of theory 3 is a direct application of the general result, theorem IV-A. To apply the general result, it must first be observed that assumption 3 (a) and (c) directly imply the corresponding items in assumption 2. Thus, we only need to check the continuity condition in assumption 2 (b) for the fit functions in (13) and (15). We begin by proving the continuity of Htz (n). Fix t, and let (n), p (n))) be a sequence of vectors and (n) p, a sequence of scalars such as thatlim n (n), p) PL (p) = (Y) lim n, p)."}], "references": [{"title": "Sparse Bayesian learning and the relevance vector machine", "author": ["M. Tipping"], "venue": "J. Machine Learning Research, vol. 1, pp. 211\u2013244, Sep. 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian factor regressionm models in the \u201clarge p, small n\u201d paradigm", "author": ["M. West"], "venue": "Bayesian Statistics, vol. 7, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparse Bayesian learning for basis selection", "author": ["D. Wipf", "B. Rao"], "venue": "IEEE Trans. Signal Process., vol. 52, no. 8, pp. 2153\u20132164, Aug. 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Bayesian compressive sensing", "author": ["S. Ji", "Y. Xue", "L. Carin"], "venue": "IEEE Trans. Signal Process., vol. 56, pp. 2346\u20132356, Jun. 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with compressible priors", "author": ["V. Cevher"], "venue": "Proc. NIPS, Vancouver, BC, Dec. 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Identification of systems containing linear dynamic and static nonlinear elements", "author": ["S. Billings", "S. Fakhouri"], "venue": "Automatica, vol. 18, no. 1, pp. 15\u201326, 1982.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1982}, {"title": "The identification of nonlinear biological systems: Wiener and Hammerstein cascade models", "author": ["I.W. Hunter", "M.J. Korenberg"], "venue": "Biological Cybernetics, vol. 55, no. 2\u20133, pp. 135\u2013144, 1986.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1986}, {"title": "Spiketriggered neural characterization", "author": ["O. Schwartz", "J.W. Pillow", "N.C. Rust", "E.P. Simoncelli"], "venue": "J. Vision, vol. 6, no. 4, pp. 484\u2013507, Jul. 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Iterative multiuser joint decoding: Unified framework and asymptotic analysis", "author": ["J. Boutros", "G. Caire"], "venue": "IEEE Trans. Inform. Theory, vol. 48, no. 7, pp. 1772\u20131793, Jul. 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Approximate belief propagation, density evolution, and neurodynamics for CDMA multiuser detection", "author": ["T. Tanaka", "M. Okada"], "venue": "IEEE Trans. Inform. Theory, vol. 51, no. 2, pp. 700\u2013706, Feb. 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Asymptotic mean-square optimality of belief propagation for sparse linear systems", "author": ["D. Guo", "C.-C. Wang"], "venue": "Proc. IEEE Inform. Theory Workshop, Chengdu, China, Oct. 2006, pp. 194\u2013198.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Random sparse linear systems observed via arbitrary channels: A decoupling principle", "author": ["\u2014\u2014"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Nice, France, Jun. 2007, pp. 946\u2013950.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Message-passing algorithms for compressed sensing", "author": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proc. Nat. Acad. Sci., vol. 106, no. 45, pp. 18 914\u201318 919, Nov. 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Message passing algorithms for compressed sensing I: motivation and construction", "author": ["\u2014\u2014"], "venue": "Proc. Info. Theory Workshop, Jan. 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Message passing algorithms for compressed sensing II: analysis and validation", "author": ["\u2014\u2014"], "venue": "Proc. Info. Theory Workshop, Jan. 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "The dynamics of message passing on dense graphs, with applications to compressed sensing", "author": ["M. Bayati", "A. Montanari"], "venue": "IEEE Trans. Inform. Theory, vol. 57, no. 2, pp. 764\u2013785, Feb. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimation with random linear mixing, belief propagation and compressed sensing", "author": ["S. Rangan"], "venue": "Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2010, pp. 1\u20136.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized approximate message passing for estimation with random linear mixing", "author": ["\u2014\u2014"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Saint Petersburg, Russia, Jul.\u2013Aug. 2011, pp. 2174\u20132178.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical model concepts in compressed sensing", "author": ["A. Montanari"], "venue": "Compressed Sensing: Theory and Applications, Y. C. Eldar and G. Kutyniok, Eds. Cambridge Univ. Press, Jun. 2012, pp. 394\u2013438.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian inference and optimal design for the sparse linear model", "author": ["M. Seeger"], "venue": "J. Machine Learning Research, vol. 9, pp. 759\u2013813, Sep. 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Expectation-maximization Bernoulli-Gaussian approximate message passing", "author": ["J.P. Vila", "P. Schniter"], "venue": "Conf. Rec. 45th Asilomar Conf. Signals, Syst. & Comput., Pacific Grove, CA, Nov. 2011, pp. 799\u2013803.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Expectation-maximization Gaussian-mixture approximate message passing", "author": ["\u2014\u2014"], "venue": "Proc. Conf. on Inform. Sci. & Sys., Princeton, NJ, Mar. 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical physics-based reconstruction in compressed sensing", "author": ["F. Krzakala", "M. M\u00e9zard", "F. Sausset", "Y. Sun", "L. Zdeborov\u00e1"], "venue": "arXiv:1109.4424, Sep. 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic reconstruction in compressed sensing: Algorithms, phase diagrams, and threshold achieving matrices", "author": ["\u2014\u2014"], "venue": "arXiv:1206.3953, Jun. 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum-likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Roy. Statist. Soc., vol. 39, pp. 1\u201317, 1977.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1977}, {"title": "Informationtheoretically optimal compressed sensing via spatial coupling and approximate message passing", "author": ["D.L. Donoho", "A. Javanmard", "A. Montanari"], "venue": "December 2011, arXiv:1112.0708v1 [cs.IT].", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid generalized approximation message passing with applications to structured sparsity", "author": ["S. Rangan", "A.K. Fletcher", "V.K. Goyal", "P. Schniter"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Cambridge, MA, Jul. 2012, pp. 1241\u20131245.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Minimax risk over `p-balls for `qerror.", "author": ["D.L. Donoho", "I.M. Johnstone"], "venue": "Probab. Theory and Relat. Fields, vol. 99,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Approximate message passing with consistent parameter estimation and applications to sparse learning", "author": ["U.S. Kamilov", "S. Rangan", "A.K. Fletcher", "M. Unser"], "venue": "Proc. NIPS, Lake Tahoe, NV, Dec. 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M. Figueiredo"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 7, pp. 2479\u20132493, Jul. 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Augmented Lagrangian and Operator- Splitting Methods in Nonlinear Mechanics, ser", "author": ["R. Glowinski", "P.L. Tallec"], "venue": "SIAM Studies in Applied Mathematics. Philadelphia, PA: SIAM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1989}, {"title": "A new inexact alternating directions method for monotone variational inequalities", "author": ["B. He", "L.-Z. Liao", "D. Han", "H. Yang"], "venue": "Math. Program., vol. 92, no. 1, Ser A, pp. 103\u2013108, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Alternating direction augmented Lagrangian methods for semidefinite programming", "author": ["Z. Wen", "D. Goldfarb", "W. Yin"], "venue": "Math. Program. Comp., vol. 2, no. 3\u20134, pp. 203\u2013230, 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian compressive sensing via belief propagation", "author": ["D. Baron", "S. Sarvotham", "R.G. Baraniuk"], "venue": "IEEE Trans. Signal Process., vol. 58, no. 1, pp. 269\u2013280, Jan. 2010.  14  APPROXIMATE MESSAGE PASSING WITH CONSISTENT PARAMETER ESTIMATION", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "A statistical-mechanics approach to large-system analysis of CDMA multiuser detectors", "author": ["T. Tanaka"], "venue": "IEEE Trans. Inform. Theory, vol. 48, no. 11, pp. 2888\u20132910, Nov. 2002.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Randomly spread CDMA: Asymptotics via statistical physics", "author": ["D. Guo", "S. Verd\u00fa"], "venue": "IEEE Trans. Inform. Theory, vol. 51, no. 6, pp. 1983\u20132010, Jun. 2005.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1983}, {"title": "Support recovery in compressed sensing: Information-theoretic bounds", "author": ["G. Caire", "S. Shamai", "A. Tulino", "S. Verd\u00fa"], "venue": "Proc. UCSD Workshop Inform. Theory & Its Applications, La Jolla, CA, Jan. 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Asymptotic analysis of MAP estimation via the replica method and applications to compressed sensing", "author": ["S. Rangan", "A. Fletcher", "V.K. Goyal"], "venue": "IEEE Trans. Inform. Theory, vol. 58, no. 3, pp. 1902\u20131923, Mar. 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1902}, {"title": "Iterative estimation of constrained rank-one matrices in noise", "author": ["S. Rangan", "A.K. Fletcher"], "venue": "Proc. IEEE Int. Symp. Inform. Theory, Cambridge, MA, Jul. 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressed sensing over `p-balls: Minimax mean square error", "author": ["D. Donoho", "I. Johnstone", "A. Maleki", "A. Montanari"], "venue": "Proc. ISIT, St. Petersburg, Russia, Jun. 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural reconstruction with approximate message passing (NeuRAMP)", "author": ["A.K. Fletcher", "S. Rangan", "L. Varshney", "A. Bhargava"], "venue": "Proc. Neural Information Process. Syst., Granada, Spain, Dec. 2011.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Distribution of eigenvalues for some sets of random matrices", "author": ["V.A. Mar\u010denko", "L.A. Pastur"], "venue": "Math. USSR\u2013Sbornik, vol. 1, no. 4, pp. 457\u2013 483, 1967.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1967}], "referenceMentions": [{"referenceID": 0, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 253, "endOffset": 256}, {"referenceID": 1, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 258, "endOffset": 261}, {"referenceID": 2, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 380, "endOffset": 383}, {"referenceID": 4, "context": "Such joint estimation and learning problems with linear transforms and componentwise nonlinearities arise in a range of applications, including empirical Bayesian approaches to inverse problems in signal processing, linear regression and classification [1], [2], and, more recently, Bayesian compressed sensing for estimation of sparse vectors x from underdetermined measurements [3]\u2013[5].", "startOffset": 384, "endOffset": 387}, {"referenceID": 5, "context": "Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]\u2013[8].", "startOffset": 259, "endOffset": 262}, {"referenceID": 7, "context": "Also, since the parameters in the output transfer function PY |Z can model unknown nonlinearities, this problem formulation can be applied to the identification of linear-nonlinear cascade models of dynamical systems, in particular for neural spike responses [6]\u2013[8].", "startOffset": 263, "endOffset": 266}, {"referenceID": 8, "context": "In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]\u2013[18].", "startOffset": 187, "endOffset": 190}, {"referenceID": 17, "context": "In recent years, there has been significant interest in so-called approximate message passing (AMP) and related methods based on Gaussian approximations of loopy belief propagation (LBP) [9]\u2013[18].", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 66, "endOffset": 69}, {"referenceID": 10, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "These methods originate from CDMA multiuser detection problems in [9]\u2013[11], and have received considerable recent attention in the context of compressed sensing [13]\u2013[17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 18, "context": "See, also the survey article [19].", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z.", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "The Gaussian approximations used in AMP are also closely related to standard expectation propagation techniques [20], [21], but with additional simplifications that exploit the linear coupling between the variables x and z.", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 223, "endOffset": 227}, {"referenceID": 11, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 229, "endOffset": 233}, {"referenceID": 15, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 235, "endOffset": 239}, {"referenceID": 16, "context": "The key benefits of AMP methods are their computational simplicity, large domain of application, and, for certain large random A, their exact asymptotic performance characterizations with testable conditions for optimality [11], [12], [16], [17].", "startOffset": 241, "endOffset": 245}, {"referenceID": 17, "context": "This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "This paper considers the so-called generalized AMP (GAMP) method of [18] that extends the algorithm in [13] to arbitrary output distributions PY |Z (many original formulations assumed additive white Gaussian noise (AWGN) measurements).", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "To overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al.", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "To overcome this limitation, Vila and Schniter [22], [23] and Krzakala et al.", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "[24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (\u03bbx, \u03bbz) along with the estimation of the vector x.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[24], [25] have recently proposed extension of AMP and GAMP based on Expectation Maximization (EM) that enable joint learning of the parameters (\u03bbx, \u03bbz) along with the estimation of the vector x.", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "\u2022 Generalization of the GAMP method of [18] to a class of algorithms we call adaptive GAMP that enables joint estimation of the parameters \u03bbx and \u03bbz along with vector x.", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]\u2013[25] as special cases.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "In addition, the adaptive GAMP methods include the EM-GAMP algorithms of [22]\u2013[25] as special cases.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "We show that, similar to the analysis of the AMP and GAMP algorithms in [11], [12], [16]\u2013 [18], the componentwise asymptotic behavior of adaptive GAMP can be described exactly by a simple scalar state evolution (SE) equations.", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]\u2013[25].", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "As mentioned above, the adaptive GAMP method proposed here can be seen as a generalization of the EM methods in [22]\u2013[25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "In [22], [23], the prior PX is described by a generic L-term Gaussian mixture (GM) whose parameters are identified by an EM procedure [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs.", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "A related EM-GAMP algorithm has also appeared in [24], [25] for the case of certain sparse priors and AWGN outputs.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "Simulations in [22], [23] show remarkably good performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Simulations in [22], [23] show remarkably good performance and computational speed for EM-GAMP over a wide class of distributions, particularly in the context of compressed sensing.", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "Also, using arguments from statistical physics, [24], [25] presents state evolution (SE) equations for the joint evolution of the parameters and vector estimates and confirms them numerically.", "startOffset": 54, "endOffset": 58}, {"referenceID": 23, "context": "In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods.", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "In particular, the current work provides a rigorous justification of the SE analysis in [24], [25] along with extensions to more general input and output channels and adaptation methods.", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 151, "endOffset": 155}, {"referenceID": 24, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 26, "context": "However, the methodology in [24], [25] in other ways is more general in that it can also study \u201cseeded\u201d or \u201cspatially-coupled\u201d matrices as proposed in [24], [25], [27].", "startOffset": 163, "endOffset": 167}, {"referenceID": 27, "context": "An alternate method for joint learning and estimation has been presented in [28], which assumes that the distributions on the source and output channels are themselves described by graphical models with the parameters \u03bbx and \u03bbz appearing as unknown variables.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "The method in [28], called Hybrid-GAMP, iteratively combines standard loopy BP with AMP methods.", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "This minimax approach [29] was proposed for AMP recovery of sparse signals in [13].", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "This minimax approach [29] was proposed for AMP recovery of sparse signals in [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "Indeed, this gap was the main justification of the EMGAMP methods in [22], [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "Indeed, this gap was the main justification of the EMGAMP methods in [22], [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "A conference version of this paper has appeared in [30].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "Before describing the adaptive GAMP algorithm, it is useful to review the basic (non-adaptive) GAMP algorithm of [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The GAMP algorithm of [18] can be seen as a class of methods for estimating the vectors x and z for the case when the parameters \u03bbx and \u03bbz are known.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "As described in [18], there are two important sets of choices for the estimation functions, resulting in two variants of GAMP:", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]\u2013 [35].", "startOffset": 112, "endOffset": 116}, {"referenceID": 33, "context": "The operations are similar in form to separable and proximal minimization methods widely used for such problems [31]\u2013 [35].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "More details, as well as the equations for max-sum GAMP can be found in [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "By now, there are a large number of SE results for AMP-related algorithms [9], [11]\u2013[18].", "startOffset": 74, "endOffset": 77}, {"referenceID": 10, "context": "By now, there are a large number of SE results for AMP-related algorithms [9], [11]\u2013[18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "By now, there are a large number of SE results for AMP-related algorithms [9], [11]\u2013[18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Here, we review the particular SE analysis from [18] which is based on the framework in [16].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Here, we review the particular SE analysis from [18] which is based on the framework in [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "of Bayati and Montanari\u2019s analysis in [16].", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "The main result of [18] shows that if we fix the iteration t, and let n\u2192\u221e, the asymptotic joint empirical distribution of the components of these two sets \u03b8 x and \u03b8 t z converges to random vectors of the form", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "With these definitions, we can state the main result from [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Theorem 1 ( [18]): Consider the random vectors \u03b8 x and \u03b8 t z generated by the outputs of GAMP under Assumption 1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "This scalar equivalent model appears in several analyses and can be thought of as a single-letter characterization [36] of the system.", "startOffset": 115, "endOffset": 119}, {"referenceID": 23, "context": "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]\u2013[40].", "startOffset": 97, "endOffset": 101}, {"referenceID": 35, "context": "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]\u2013[40].", "startOffset": 103, "endOffset": 107}, {"referenceID": 38, "context": "Similar singleletter characterizations can also be derived by arguments from statistical physics [24], [37]\u2013[40].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "As described in the previous section, the standard GAMP algorithm of [18] considers the case when the parameters \u03bbx and \u03bbz in the distributions in (1) are known.", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et.", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "It is useful to briefly compare the above ML parameter estimation with the EM-GAMP method proposed by Vila and Schniter in [22], [23] and Krzakala et.", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "in [24], [25].", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "in [24], [25].", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "Both of these methods combine the Bayesian AMP [14], [15] or GAMP algorithms [18] with a standard EM procedure [26] as follows.", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "For the sum-product AMP or GAMP algorithms, it is shown in [18] that the SE equations simplify so that \u03b1 r = 1 and \u03be r = \u03c4 t r, if the parameters were selected correctly.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "Some justification for this last step can be found in [11], [12], [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "Some justification for this last step can be found in [11], [12], [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Some justification for this last step can be found in [11], [12], [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "In [22], [23], the parameter update (17) is performed only once every few iterations to allow P\u0302 t to converge to the approximation of the posterior distribution of xj given the current parameter estimates.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "In [22], [23], the parameter update (17) is performed only once every few iterations to allow P\u0302 t to converge to the approximation of the posterior distribution of xj given the current parameter estimates.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "In [24], [25], the parameter estimate is updated every iteration.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [24], [25], the parameter estimate is updated every iteration.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "We thus see that the EM-GAMP procedures in [22], [23] and in [24], [25] are both special cases of the adaptive GAMP algorithm in Algorithm 1 with particular choices of the adaptation functions H x and H t z .", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "The proof is straightforward and is based on a continuity argument also used in [41].", "startOffset": 80, "endOffset": 84}, {"referenceID": 40, "context": "It is known that estimators such as LASSO offer certain optimal minmax performance over a large class of sparse distributions [43].", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "This gap was the main motivation for [22], [23] which showed large gains of the EMGAMP method due to its ability to learn the prior.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "where \u039bx = [0, 1]\u00d7 [0,+\u221e).", "startOffset": 11, "endOffset": 17}, {"referenceID": 7, "context": "As a second example, we consider the estimation of the linear-nonlinear-Poisson (LNP) cascade model [8].", "startOffset": 100, "endOffset": 103}, {"referenceID": 41, "context": "AMP techniques combined with the parameter estimation have been recently proposed for neural receptive field estimation and connectivity detection in [44].", "startOffset": 150, "endOffset": 154}, {"referenceID": 21, "context": "The procedure, which is a generalization of EMGAMP methodology of [22]\u2013[25] that estimates both the vector x as well as parameters in the source and componentwise output transform.", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "The procedure, which is a generalization of EMGAMP methodology of [22]\u2013[25] that estimates both the vector x as well as parameters in the source and componentwise output transform.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "As described in [18], the sum-product estimation can be implemented with the estimation functions", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "The paper [18] shows that the derivatives of these estimation functions for lines 9 and 16 are computed via the variances:", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "A key result of [18] is that, when the parameters are set to the true values (i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "Bayati and Montanari\u2019s analysis in [16] employs certain deterministic models on the vectors and then proves convergence properties of related empirical distributions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 39, "context": "The proof follows along the adaptation argument of [41].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "This non-adaptive algorithm is precisely the standard GAMP method analyzed in [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 42, "context": "components with zero mean and variance 1/m, it follows from the Mar\u010denko-Pastur Theorem [45] that that its 2-norm operator norm is bounded.", "startOffset": 88, "endOffset": 92}], "year": 2012, "abstractText": "We consider the estimation of an i.i.d. (possibly non-Gaussian) vector x \u2208 R from measurements y \u2208 R obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. A novel method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector x is presented. The proposed algorithm is a generalization of a recently-developed EM-GAMP that uses expectationmaximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The methodology can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. In addition, we show that when a certain maximum-likelihood estimation can be performed in each step, the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. Remarkably, this result applies to essentially arbitrary parametrizations of the unknown distributions, including ones that are nonlinear and non-Gaussian. The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.", "creator": "LaTeX with hyperref package"}}}