{"id": "1207.4404", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2012", "title": "Better Mixing via Deep Representations", "abstract": "It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation. To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples.", "histories": [["v1", "Wed, 18 Jul 2012 16:07:36 GMT  (561kb,D)", "http://arxiv.org/abs/1207.4404v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "gr\u00e9goire mesnil", "yann dauphin", "salah rifai"], "accepted": true, "id": "1207.4404"}, "pdf": {"name": "1207.4404.pdf", "metadata": {"source": "CRF", "title": "Better Mixing via Deep Representations", "authors": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin"], "emails": [], "sections": [{"heading": "1 Introduction and Background", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "2 Hypotheses", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to understand the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to understand the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to understand the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to break the rules that determine the rules of the market. (...) \"It is the case that the rules of the market and the rules of the market are to be observed.\" (...) \"It is the case that the rules of the market and the rules of the market are to be observed.\" (...) It is the case that the rules of the market and the rules of the market are (...) to be observed. (...)"}, {"heading": "3 Representation-Learning Algorithms", "text": "The learning algorithms used in this work to explore the previous hypotheses are the Deep Belief Network or DBN (Hinton et al., 2006), which was trained by stacking Restricted Boltzmann Machines or RBMs, and the Contractive Auto-Encoder or CAE (Rifai et al., 2011a), for which a sampling algorithm was proposed (Rifai et al., 2012). See Bengio (2009) for a detailed review of RBMs and DBNs. Each level of the DBN is trained as RBM, and a single-layer DBN is just an RBM model that projects a common distribution between a hidden layer h and a visible layer v. Gibbs sampling at the top level of the DBN is used to obtain samples from the model: the sampled representations at the top level are pockastically projected to lower levels."}, {"heading": "4 Experiments", "text": "The experiments were carried out with the MNIST digit dataset (LeCun et al., 1998) and with the Toronto Face Database (Susskind et al., 2010), TFD. The former has been widely used to evaluate many deep learning algorithms, while the latter is interesting for its diverse structure and for the control factors (such as emotions and identity). The DBNs tested at MNIST have layer sizes of 768-1024-1024 (28 x 28 input) and 2304-512-1024 (48 x 48 input), respectively, while the CAEs have sizes of 768-1000-1000 and 2304-1000-1000 at MNIST and TFD, respectively."}, {"heading": "4.1 Sampling at Different Depths", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Better Samples at Higher Levels", "text": "To test H1, we first plot sequences of samples at different depths. In Figure 1, it is possible to verify that samples obtained in deeper layers are visually more likely and mix faster. In addition, we measure the quality of the samples obtained using a comparison of sample generators described in Breuleux et al. (2011). It measures the logic probability of a test set below the density calculated from a Parzen Window Density Estimator based on 10,000 samples generated. Log probabilities for different models are shown in Table 1 (right columns) These results also indicate that the quality of samples is higher when the Markov chain process for sampling takes place in the upper layers. This observation agrees with H3 (b) that movement in higher-level representation spaces where diversity has been expanded delivers higher-quality samples than movement in the input region, where density may be difficult to maintain."}, {"heading": "4.1.2 Visualizing Representation-Space by Interpolating Between Neighbors", "text": "According to H3 (a), deeper layers tend to unfold the manifold areas near high densities of the entrance space locally, while according to H3 (b), the relative volume of plausible configurations in the representation space should be greater. Both would mean that convex combinations of adjacent examples in the representation space correspond to more likely input configurations. In fact, the interpolation between points on a flat manifold should remain on the manifold. Furthermore, when interpolating between examples of different classes (i.e. different modes), H3 (b) would suggest that most points in between (on the linear interpolation line) should correspond to plausible samples, which would not be the case in the entrance space. In Figure 2, we interpolate linearly between neighbors in the representation space and visualize the points obtained at different depths in the entrance space."}, {"heading": "4.2 Measuring Mixing by Counting Number of Classes Visited", "text": "Here we look at the possibility of mixing between different classes. We look at sequences of length 10, 20 or 100 and calculate histograms of the number of different classes attended in a sequence for the two different depths and learners on TFD. Since classes typically take place in different modes (diversity), counting how many different classes are attended in an MCMC run tells us how fast the chain mixes. Figure 3 (c, f) shows that the deeper architectures attend more classes and the CAE mixtures faster than the DBN."}, {"heading": "4.3 Occupying More Volume Around Data Points", "text": "In these experiments (Fig. 3 (a, b, d, e), we estimate the quality of the samples whose representation is in the vicinity of training examples on different representation levels. In the first case (Fig. 3 (a, b, e), the samples are interpolated in the middle between an example and its k-th nearest neighbour, with k being plotted on the x-axis. In the second case (Fig. 3 (d, e)), isotropic noise is added around an example, with standard deviations on the x-axis. In both cases, for each data point plotted on the images, 500 samples are generated, with the y-axis being the log probability introduced earlier, i.e. the estimation of the quality of the samples. At a higher level, both the CAE and the DBN are occupied by probable configurations, i.e. closer to the input space where the actual data are located, the distribution between the first and second distances (DN)."}, {"heading": "4.4 Discriminative Ability vs Volume Expansion", "text": "The proposed hypotheses could possibly correspond to a worse discriminatory force. If, in fact, at a higher level the different classes are \"closer\" to each other (which facilitates mixing between them), would this not mean that they are more confusable? We first confirm with the tested models (as a health test) that the deeper characteristics, despite their better generative abilities and better mixing, are conducive to better classification performance. We train a linear SVM by concatenating the raw input with the representations of the upper layers. The results presented in Table 1 show that the representation is more linear separable by increasing the depth of the architecture and the information added by each layer are helpful for classification. Also, fine-tuning an MLP initialized with these weights is still the best way to achieve the state of the art (results presented in Table 1). To explain the good discriminatory capabilities of the deeper layers (either by concatenating with lower layers, or better discriminating by a better adjustment of the various factors) would be helpful."}, {"heading": "5 Conclusion", "text": "The following hypotheses were tested: (1) deeper representations can yield better samples and better mixing; (2) this is due to better unbundling; (3) this is related to the unfolding of the manifold in which the data is concentrated, along with the increase in the volume that good samples take in the representation space; the experimental results were consistent with these hypotheses; they showed better samples and better mixing at higher levels, better samples obtained during interpolation between examples at higher levels, and better samples obtained when adding isotropic noise at higher levels. We also looked at the potential conflict between the third hypothesis and better discrimination (confirmed by the models tested) and explained it away as a result of the second hypothesis. This could be immediate good news for applications that need to generate MCMC samples: By transporting the problem to deeper representations, better and faster training results could be obtained even if they were to examine the relationship between the MC and the more interesting work of the MCMC."}], "references": [{"title": "A Bayesian/information theoretic model of learning via multiple task sampling", "author": ["J. Baxter"], "venue": "Machine Learning,", "citeRegEx": "Baxter,? \\Q1997\\E", "shortCiteRegEx": "Baxter", "year": 1997}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "On the expressive power of deep architectures", "author": ["Y. Bengio", "O. Delalleau"], "venue": null, "citeRegEx": "Bengio and Delalleau,? \\Q2011\\E", "shortCiteRegEx": "Bengio and Delalleau", "year": 2011}, {"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": null, "citeRegEx": "Bengio and LeCun,? \\Q2007\\E", "shortCiteRegEx": "Bengio and LeCun", "year": 2007}, {"title": "The curse of highly variable functions for local kernel machines", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "In NIPS\u20192005,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Quickly generating representative samples from an rbm-derived process", "author": ["O. Breuleux", "Y. Bengio", "P. Vincent"], "venue": "Neural Computation,", "citeRegEx": "Breuleux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Breuleux et al\\.", "year": 2011}, {"title": "Learning many related tasks at the same time with backpropagation", "author": ["R. Caruana"], "venue": "In NIPS\u201994,", "citeRegEx": "Caruana,? \\Q1995\\E", "shortCiteRegEx": "Caruana", "year": 1995}, {"title": "Algorithms for manifold learning", "author": ["L. Cayton"], "venue": "Technical Report CS2008-0923,", "citeRegEx": "Cayton,? \\Q2005\\E", "shortCiteRegEx": "Cayton", "year": 2005}, {"title": "Parallel tempering is efficient for learning restricted boltzmann machines", "author": ["K. Cho", "T. Raiko", "A. Ilin"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Tempered Markov chain monte carlo for training of restricted Boltzmann machine", "author": ["G. Desjardins", "A. Courville", "Y. Bengio", "P. Vincent", "O. Delalleau"], "venue": "In JMLR W&CP: Proc. AISTATS\u20192010,", "citeRegEx": "Desjardins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "Remapping somatosensory cortex after injury", "author": ["H. Flor"], "venue": "Advances in Neurology,", "citeRegEx": "Flor,? \\Q2003\\E", "shortCiteRegEx": "Flor", "year": 2003}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "A. Ng"], "venue": "In NIPS\u20192009,", "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Almost optimal lower bounds for small depth circuits", "author": ["J. H\u00e5stad"], "venue": "In STOC\u201986,", "citeRegEx": "H\u00e5stad,? \\Q1986\\E", "shortCiteRegEx": "H\u00e5stad", "year": 1986}, {"title": "On the power of small-depth threshold circuits", "author": ["J. H\u00e5stad", "M. Goldmann"], "venue": "Computational Complexity,", "citeRegEx": "H\u00e5stad and Goldmann,? \\Q1991\\E", "shortCiteRegEx": "H\u00e5stad and Goldmann", "year": 1991}, {"title": "Autoencoders, minimum description length, and helmholtz free energy", "author": ["G.E. Hinton", "R.S. Zemel"], "venue": null, "citeRegEx": "Hinton and Zemel,? \\Q1994\\E", "shortCiteRegEx": "Hinton and Zemel", "year": 1994}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "The development of the time-delay neural network architecture for speech recognition", "author": ["K.J. Lang", "G.E. Hinton"], "venue": "Technical Report CMU-CS-88-152,", "citeRegEx": "Lang and Hinton,? \\Q1988\\E", "shortCiteRegEx": "Lang and Hinton", "year": 1988}, {"title": "Mod\u00e8les connexionistes de l\u2019apprentissage", "author": ["Y. LeCun"], "venue": "Ph.D. thesis, Universite\u0301 de Paris VI", "citeRegEx": "LeCun,? \\Q1987\\E", "shortCiteRegEx": "LeCun", "year": 1987}, {"title": "Generalization and network design strategies", "author": ["Y. LeCun"], "venue": "Technical Report CRG-TR-89-4,", "citeRegEx": "LeCun,? \\Q1989\\E", "shortCiteRegEx": "LeCun", "year": 1989}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sample complexity of testing the manifold hypothesis", "author": ["H. Narayanan", "S. Mitter"], "venue": "In NIPS\u20192010", "citeRegEx": "Narayanan and Mitter,? \\Q2010\\E", "shortCiteRegEx": "Narayanan and Mitter", "year": 2010}, {"title": "Sampling from multimodal distributions using tempered transitions", "author": ["R.M. Neal"], "venue": "Technical Report 9421,", "citeRegEx": "Neal,? \\Q1994\\E", "shortCiteRegEx": "Neal", "year": 1994}, {"title": "Contracting auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "Learning deep Boltzmann machines using adaptive MCMC", "author": ["R. Salakhutdinov"], "venue": "In ICML\u20192010", "citeRegEx": "Salakhutdinov,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov", "year": 2010}, {"title": "Learning in Markov random fields using tempered transitions", "author": ["R. Salakhutdinov"], "venue": "In NIPS\u20192010", "citeRegEx": "Salakhutdinov,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov", "year": 2010}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In AISTATS\u20192009,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "The Toronto face dataset", "author": ["J. Susskind", "A. Anderson", "G.E. Hinton"], "venue": "Technical Report UTML TR 2010-001,", "citeRegEx": "Susskind et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Deep learning algorithms attempt to discover multiple levels of representation of the given data (see (Bengio, 2009) for a review), with higher levels of representation defined hierarchically in terms of lower level ones.", "startOffset": 102, "endOffset": 116}, {"referenceID": 14, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 15, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 4, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 3, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 2, "context": "Mathematical results in the case of specific function families have shown that choosing a sufficient depth of representation can yield exponential benefits, in terms of size of the model, to represent some functions (H\u00e5stad, 1986; H\u00e5stad and Goldmann, 1991; Bengio et al., 2006; Bengio and LeCun, 2007; Bengio and Delalleau, 2011).", "startOffset": 216, "endOffset": 330}, {"referenceID": 6, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 48, "endOffset": 105}, {"referenceID": 0, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 48, "endOffset": 105}, {"referenceID": 9, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 48, "endOffset": 105}, {"referenceID": 18, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 158, "endOffset": 194}, {"referenceID": 20, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989).", "startOffset": 158, "endOffset": 194}, {"referenceID": 13, "context": "Several observations have been made and reported that suggest that some deep learning algorithms indeed help to disentangle the underlying factors of variation (Goodfellow et al., 2009; Glorot et al., 2011).", "startOffset": 160, "endOffset": 206}, {"referenceID": 12, "context": "Several observations have been made and reported that suggest that some deep learning algorithms indeed help to disentangle the underlying factors of variation (Goodfellow et al., 2009; Glorot et al., 2011).", "startOffset": 160, "endOffset": 206}, {"referenceID": 7, "context": "In general the associated sampling algorithms involve a Markov Chain and MCMC techniques, and these can notoriously suffer from a fundamental problem of mixing: it is difficult for the Markov chain to jump from one mode of the distribution to another, when these are separated by large low-density regions, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010).", "startOffset": 380, "endOffset": 422}, {"referenceID": 22, "context": "In general the associated sampling algorithms involve a Markov Chain and MCMC techniques, and these can notoriously suffer from a fundamental problem of mixing: it is difficult for the Markov chain to jump from one mode of the distribution to another, when these are separated by large low-density regions, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010).", "startOffset": 380, "endOffset": 422}, {"referenceID": 0, "context": "ing work such as multi-task learning algorithms (Caruana, 1995; Baxter, 1997; Collobert and Weston, 2008) and learning algorithms involving parameter sharing (Lang and Hinton, 1988; LeCun, 1989). There is another \u2013 less commonly discussed \u2013 motivation for deep representations, introduced in Bengio (2009): the idea that they may, to some extent, help to disentangle the underlying factors of variation.", "startOffset": 64, "endOffset": 306}, {"referenceID": 23, "context": ", 2010; Salakhutdinov, 2010b,a) is tempering (Neal, 1994).", "startOffset": 45, "endOffset": 57}, {"referenceID": 29, "context": "The idea that deeper generative models produce not only better features for classification but also better quality samples (in the sense of better corresponding to the target distribution being learned) is not novel and several observations support this hypothesis already, some quantitatively (Salakhutdinov and Hinton, 2009), some more qualitative (Hinton et al.", "startOffset": 294, "endOffset": 326}, {"referenceID": 17, "context": "The idea that deeper generative models produce not only better features for classification but also better quality samples (in the sense of better corresponding to the target distribution being learned) is not novel and several observations support this hypothesis already, some quantitatively (Salakhutdinov and Hinton, 2009), some more qualitative (Hinton et al., 2006).", "startOffset": 350, "endOffset": 371}, {"referenceID": 11, "context": "Something similar is observed in the brain where different areas of somatosensory cortex correspond to different body parts, and the size of these areas adaptively depends (Flor, 2003) on usage of these (i.", "startOffset": 172, "endOffset": 184}, {"referenceID": 17, "context": "The learning algorithms used in this paper to explore the preceding hypotheses are the Deep Belief Network or DBN (Hinton et al., 2006), trained by stacking Restricted Boltzmann Machines or RBMs, and the Contractive Auto-Encoder or CAE (Rifai et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 26, "context": ", 2011a), for which a sampling algorithm was recently proposed (Rifai et al., 2012).", "startOffset": 63, "endOffset": 83}, {"referenceID": 19, "context": "An auto-encoder (LeCun, 1987; Hinton and Zemel, 1994) is parametrized through an encoder function f mapping an input-space vector x to a representation-space vector h, and a decoder function g mapping a representation-space vector h to an input-space reconstruction r.", "startOffset": 16, "endOffset": 53}, {"referenceID": 16, "context": "An auto-encoder (LeCun, 1987; Hinton and Zemel, 1994) is parametrized through an encoder function f mapping an input-space vector x to a representation-space vector h, and a decoder function g mapping a representation-space vector h to an input-space reconstruction r.", "startOffset": 16, "endOffset": 53}, {"referenceID": 1, "context": "See Bengio (2009) for a detailed review of RBMs and DBNs.", "startOffset": 4, "endOffset": 18}, {"referenceID": 26, "context": "A sampling algorithm was recently proposed for CAEs (Rifai et al., 2012).", "startOffset": 52, "endOffset": 72}, {"referenceID": 21, "context": "The experiments have been performed on the MNIST digits dataset (LeCun et al., 1998) and the Toronto Face Database (Susskind et al.", "startOffset": 64, "endOffset": 84}, {"referenceID": 30, "context": ", 1998) and the Toronto Face Database (Susskind et al., 2010), TFD.", "startOffset": 38, "endOffset": 61}, {"referenceID": 5, "context": "In addition, we measure the quality of the obtained samples, using a procedure for the comparison of sample generators described in Breuleux et al. (2011). It measures the log-likelihood of a test set under the density computed from a Parzen window density estimator built on 10, 000 generated samples.", "startOffset": 132, "endOffset": 155}], "year": 2012, "abstractText": "It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation. To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples.", "creator": "LaTeX with hyperref package"}}}