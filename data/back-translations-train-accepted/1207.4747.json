{"id": "1207.4747", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2012", "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "abstract": "We consider the use of Frank-Wolfe optimization algorithms on the dual formulation of structural SVMs. These yield simple algorithms which only need access to an approximate maximization oracle for the structured prediction problem and thus have wide applicability. This perspective provides insights on previous popular algorithms as we show that batch subgradient as well as the cutting plane algorithms are equivalent to versions of Frank-Wolfe algorithms, enabling us to improve on their convergence analysis by harvesting the Frank-Wolfe literature. Moreover, we propose a new stochastic coordinate descent version of Frank-Wolfe which yields a provably convergent optimization algorithm for structural SVMs with total run-time independent of the number of training examples, like Pegasos, but with duality gap certificate guarantees and step-size robustness thanks to the use of line-search. Our experiments on sequence prediction indicate that this simple algorithm outperforms all other optimization algorithms which only have access to the maximization oracle.", "histories": [["v1", "Thu, 19 Jul 2012 18:02:41 GMT  (2347kb,D)", "https://arxiv.org/abs/1207.4747v1", "13 pages main text + 13 pages appendix (short version). Under review"], ["v2", "Mon, 29 Oct 2012 18:03:32 GMT  (3198kb,D)", "http://arxiv.org/abs/1207.4747v2", "10 pages main text + 17 pages appendix. Under review"], ["v3", "Tue, 30 Oct 2012 19:25:10 GMT  (1599kb,D)", "http://arxiv.org/abs/1207.4747v3", "10 pages main text + 17 pages appendix. Under review. Changes from v1 to v3: 1) Re-organized text for clarity + changed title; 2) Added new experiments (more settings, online EG, matching dataset); 3) Added multiplicative approximation result in theorems; 4) Corrected typo for rate of online EG in Table 1. v2 was missing acknowledgment section"], ["v4", "Mon, 14 Jan 2013 13:26:51 GMT  (1895kb,D)", "http://arxiv.org/abs/1207.4747v4", "Appears in Proceedings of the 30th International Conference on Machine Learning (ICML 2013). 9 pages main text + 22 pages appendix. Changes from v3 to v4: 1) Re-organized appendix; improved &amp; clarified duality gap proofs; re-drew all plots; 2) Changed convention for Cf definition; 3) Added weighted averaging experiments + convergence results; 4) Clarified main text and relationship with appendix"]], "COMMENTS": "13 pages main text + 13 pages appendix (short version). Under review", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["simon lacoste-julien", "martin jaggi", "mark w schmidt", "patrick pletscher"], "accepted": true, "id": "1207.4747"}, "pdf": {"name": "1207.4747.pdf", "metadata": {"source": "META", "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "authors": ["Simon Lacoste-Julien", "Martin Jaggi", "Mark Schmidt"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This seems to be due to the difficulty of dealing with the exponential number of constraints in the primary problem fields, or the exponential number of variables in the dual problem fields."}, {"heading": "2. Structural Support Vector Machines", "text": "(bit.ly / JJJJJJh2c). (bit.ly) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.bit.e) (bit.bit.bit.bit.e) (bit.bit.e) (bit.bit.e) (bit.bit.e) (bit.e) (bit.bit.e) (bit.bit.e) (bit.e) (bit.e) (bit.e) (bit.bit.e) (bit.e) (bit.bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e) (bit.e (bit.e) (bit.e) (bit.e) (bit.e) (bit.e (bit.e) (bit.e) (bit.e) (bit.e) (bit.e (bit.e) (bit.e (bit.e) (bit.e) (bit.e (bit.e) (bit.e (bit.e) (bit..e) (bit...e (bit..e) (bit....e) (bit.e (bit.e) (bit.e) (bit....e (bit.e (bit..e) (bit..e) (bit....e) (bit"}, {"heading": "3. The Frank-Wolfe Algorithm", "text": "The Frank Wolfe Algorithm (1956) (shown in Algorithm 1) is an iterative optimization algorithm for such problems, which requires only an optimization of linear functions over M and therefore has a broader applicability than projected gradient algorithms, which require an optimization of the quadratic function over M. For each iteration, a feasible search corner s is found by minimizing the linearisation of f at the current iteration over M (see picture in the inset list)."}, {"heading": "4. Frank-Wolfe for Structural SVMs", "text": "In this section we will explain how the Frank Wolfe method (algorithm 1) can be applied efficiently to this dual problem, and discuss its relationship to other algorithms, the most important finding being that the linear subproblem that Frank Wolfe applies is actually directly identical to the lossy-magnified decoding problem (2) for each datapoint that can be efficiently solved (see Appendix B.1 for details). Note only that the optimization domain for the dual variables \u03b1 is the productivity algorithm 2 Batch dual Frank Wolfe algorithm for the structural SVMLet w (0): = 0, '(0): = 0 for k = 0."}, {"heading": "A Primal-Dual Frank-Wolfe Algorithm for the", "text": "Structural SVM Dual. Applying algorithm 1 with line search to the dual of the structural SVM (4), but only maintaining the corresponding primary iterate w (k): = A\u03b1 (k), yields the algorithm 2. We use the natural starting point \u03b1 (0): (ey1,.., eyn), which yields w (0) = 0 as quantitative i (yi) = 0 quantitative difference.The duality gap (5) for our structural SVM dual formulation (4) is given byg (\u03b1 s \u2032 M < eyn), which yields the structural difference (0) = 0 as quantitative difference of the structural SVM dual formulation (4)."}, {"heading": "Relationship with the Batch Subgradient", "text": "Surprisingly, the Stack Frank Wolfe Method (algorithm 2) is equivalent to the Stack Subgradient Method in the Primal, although Frank Wolfe allows a cleverer choice of step size since the line search can be used in the Dual. To see the equivalence, note that a subgradient of (3) is given by dsub = \u03bbw \u2212 1n \u2211 i (y \u0445 i) = \u03bb (w \u2212 ws), where y-and ws are defined as in Algorithm 2. Therefore, the Subgradient Method Update is given at a step size of \u03b2 to w (k + 1): = w (k) \u2212 \u03b2dsub = w (k) \u2212 \u03b2\u03bb (w (k) \u2212 ws) = (1 \u2212 \u03b2\u03bb) w (k) + \u03b2ws. If we compare this with Algorithm 2, we see that each Frank Wolfe step to the dual problem (4) with the step size is equivalent to a batch size of an equivalence between the equivalence and the equivalence in 1."}, {"heading": "Relationship with Cutting Plane Algorithms.", "text": "In each iteration, the cutting plane algorithm by Joachims et al. (2009) and the Frank Wolfe method (algorithm 2) solve the loss-magnified decoding problem for each data point by selecting the same new \"active\" dual variables to be added to the dual problem, the only difference being that the cutting plane algorithm does not simply move into the corner as in the classic Frank Wolfe, but re-optimizes using all previously added \"active\" dual variables (this task is a square program), showing that the method is exactly the \"fully corrective\" variant of Frank Wolfe, which re-optimizes all previously visited corners in each iteration (Clarkson, 2010; Shalev-Shwartz et al., 2010b)."}, {"heading": "5. Faster Block-Coordinate Frank-Wolfe", "text": "A major drawback of the standard Frank Wolfe method when applied to the structural SVM problem is that each iteration requires a complete run through the data, resulting in n calls to the maximization oracle. In this section, we present the most important new contribution of the paper: a block-coordinate generalization of the Frank Wolfe algorithm that maintains all the appealing properties of Frank Wolfe, but requires much cheaper iterations that only require a call to the maximize oracle in the context of structural SVMs. The new method is applied in algorithm 3 and applies to all the convex optimization limitations of the formmin optimization problem."}, {"heading": "6. Experiments", "text": "We compare our novel Frank Wolfe approach to existing algorithms for training structural SVMs on the OCR dataset (n = 6251, d = 4028) by Taskar et al. (2003) and the CoNLL dataset (n = 8936, d = 1643026) by Sang & Buchholz (2000), both of which are sequence marking tasks where the loss-magnified decoding problem can be solved precisely by the Viterbi algorithm. Our third application is a word alignment problem between sentences in different languages in the vicinity of Taskar et al. (2006) (n = 5000, d = 82). Here, the structured labels are bipartite matchings that require computational margins using labels as required by Collins et al. (2008); Zhang et al. (2011) the method is intractable, but loss-accumulated decoding can be performed efficiently."}, {"heading": "7. Related Work", "text": "The SMO algorithm has been generalized to structural SVMs (Taskar, 2004, Chapter 6), but its convergence rate scales poorly with the size of the output space: However, it was estimated as O (n | Y | / \u03bb\u03b5) in Zhang et al. (2011). Further, this method requires an expectation oracle to work with its factored dual parameterization. As in our algorithm, Rousu et al. (2006) suggest updating a training example at one time, but the use of multiple Frank Wolfe updates to optimize along the subspace. However, they do not get rate guarantees and their algorithm is less general because it again requires an expectation oracle. In the degenerated binary SVM case, our block coordinate Frank Wolfe algorithm is actually equivalent to the method of Hsieh et al. (2008), where each datable is."}, {"heading": "8. Discussion", "text": "This paper proposes a novel randomized block coordinate generalization of the classic Frank Wolfe algorithm for optimization with block separable constraints. Despite its potentially much lower iteration costs, the new algorithm achieves a similar convergence rate in the duality gap as the full Frank Wolfe method. For the dual structural SVM optimization problem, it leads to a simple online algorithm that provides a solution to a problem that is notoriously difficult for stochastic algorithms to address: no step size sequence needs to be matched, as the optimal step size can be efficiently calculated in closed form. Furthermore, at the expense of an additional run through the data (which could be performed in conjunction with a full Frank Wolfe iteration), it allows us to calculate a duality gap guarantee that can be used to decide when the algorithm should be terminated. Our experiments suggest that empirically, the problem is considered more constructural than SVM for other algorithms."}, {"heading": "Bach, F., Lacoste-Julien, S., and Obozinski, G. On the", "text": "Equivalence between herd and conditional gradient algorithms. In ICML, 2012."}, {"heading": "Balamurugan, P., Shevade, S., Sundararajan, S., and", "text": "Keerthi, S. A Sequential Dual Method for Structural SVMs. In SDM, 2011."}, {"heading": "Caetano, T.S., McAuley, J.J., Cheng, Li, Le, Q.V., and", "text": "Smola, A.J. Learning graph matching. IEEE PAMI, 31 (6): 1048-1058, 2009.Clarkson, K. Coresets, sparse greedy approximation and the Frank Wolfe algorithm. ACM Transactions on Algorithms, 6 (4): 1-30, 2010."}, {"heading": "Collins, M., Globerson, A., Koo, T., Carreras, X., and", "text": "Bartlett, P. L. Exponentiated gradient algorithms for conditional random fields and max-margin Markov networks. JMLR, 9: 1775-1822, 2008.Dunn, J.C. and Harshbarger, S. Conditional gradient algorithms with open loop step size rules. Journal of Mathematical Analysis and Applications, 62 (2): 432-444, 1978."}, {"heading": "Finley, T. and Joachims, T. Training structural SVMs", "text": "In ICML, 2008.Frank, M. and Wolfe, P. An Algorithm for Quadratic Programming. Naval Research Logistics Quarterly, 3: 95- 110, 1956.Ga \ufffd rtner, B. and Jaggi, M. Coresets for Polytope Distance. ACM Symposium on Computational Geometry, 2009.Hsieh, C., Chang, K., Lin, C., Keerthi, S., and Sundararajan, S. A Method for the Origin of Two Coordinates for Large-Area Linear SVM. In ICML, pp. 408-415, 2008.Jaggi, M. Sparse Convex Optimization Methods for Machine Learning. Doctoral Thesis, ETH Zu \ufffd rich, 2011."}, {"heading": "Jaggi, M. Revisiting Frank-Wolfe: Projection-free sparse", "text": "In ICML, 2013."}, {"heading": "Joachims, T., Finley, T., and Yu, C. Cutting-plane training", "text": "of structural SVMs. Machine Learn., 77 (1): 27-59, 2009."}, {"heading": "Lacoste-Julien, S., Schmidt, M., and Bach, F. A simpler", "text": "Technical Report 1212.2002v2 [cs.LG], arXiv, December 2012.Mangasarian, O.L. Machine learning through polyhedral concave minimization. Technical Report 95-20, University of Wisconsin, 1995.Nesterov, Yurii. Efficiency of coordinate descend methods in large-scale optimization problems. SIAM Journal on Optimization, 22 (2): 341-362, 2012.Ouyang, H. and Gray, A. Fast stochastic Frank Wolfe algorithms for nonlinear SVMs. SDM, 2010.Rakhlin, A., Shamir, O., and Sridharan, K. Making gradient descent optimal for strong convex stochastic optimization. In ICML, 2012."}, {"heading": "Ratliff, N., Bagnell, J. A., and Zinkevich, M. (Online)", "text": "Subgradient methods for structured predictions. In AISTATS, 2007."}, {"heading": "Rousu, J., Saunders, C., Szedmak, S., and Shawe-Taylor,", "text": "J. Kernel-based learning of hierarchical multi-label classification models. JMLR, 2006."}, {"heading": "Sang, E.F.T.K. and Buchholz, S. Introduction to the", "text": "CoNLL-2000 common task: Chunking, 2000."}, {"heading": "Shalev-Shwartz, S. and Zhang, T. Proximal stochastic", "text": "Technical Report 1211.2717v1 [stat.ML], arXiv, November 2012."}, {"heading": "Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter, A.", "text": "Pegasos: Primarily Estimated Sub-Gradient Solver for SVM. Mathematical Programming, 127 (1), 2010a.Shalev-Shwartz, S., Srebro, N., and Zhang, T. Trading accuracy for sparsity in optimization problems with sparsity constraints. SIAM Journal on Optimization, 20: 2807-2832, 2010b.Shamir, O. and Zhang, T. Stochastic gradient descent for non-smooth optimization: Convergence results and optimally averaging systems. In ICML, 2013.Taskar, B. Learning structured prediction models: A large margin approach. Dissertation, Stanford, 2004."}, {"heading": "Taskar, B., Guestrin, C., and Koller, D. Max-margin", "text": "Markov Networks. In NIPS, 2003.Taskar, B., Lacoste-Julien, S., and Jordan, M. I. Structured prediction, dual extragradients, and Bregman projections. JMLR, 7: 1627-1653, 2006."}, {"heading": "Teo, C.H., Vishwanathan, S.V.N., Smola, A.J., and Le,", "text": "The question we have to ask is whether we can solve the problem of the structural SVM problem at all. (...) We give more details on the application of the structural SVM algorithms to the structural SVM algorithms. (...) We give the structural SVM algorithms to the structural SVM algorithms. (...) We give more details on the application of the structural SVM algorithms to the structural SVM algorithms. (...) We give the structural SVM algorithms to the structural SVM algorithms. (...) We give the structural SVM algorithms to the structural SVM algorithms. (...) We give the structural SVM algorithms to the structural SVM algorithms. (...) We give the structural SVM algorithms to the structural SVM algorithms. We give the structural SVM algorithms to the SVM algorithms."}, {"heading": "Computing the Curvature Constant Cf in the SVM Case.", "text": "Lemma A.1. For the dual structural SVM objective function (4) over the domain M: = \u00b7 Y1 | \u00b7 \u00b7 \u00b7 Yn |, the curvature constant Cf, as defined in (7), is limited by Cf \u2264 4R2\u03bb, where R is the maximum length of a differential attribute vector, i.e. R: = maximum i-curvature constant Cf, as defined in (7). \u2212 Proof Lemma A.1. If the objective function is doubly differentiable, we can insert the second degree Taylor expansion of f into the above definition (7) of curvature, see e.g. (Jaggi, 2011, Inequality (2,12)) or (Clarkson, 2010, Section 4.1). In our case, the gradient at this point is Taylor expansion of f into the above definition (7) of curvature, so the Hessian laboratories of \u00b2, like the vector, like the vector of \u00b2, like the vector, like the vector of \u00b2, like the vector like the vector, like the vector of \u00b2, like the vector like the vector, like the vector of \u00b2, like the vector like the"}, {"heading": "Computing the Product Curvature Constant C\u2297f in the SVM Case.", "text": "Lemma A.2. For the dual SVM structural lens function (4) over domain M: = = = = = = Function (1). \u00b7 \u00b7 Yn: the total curvature constant C: \u2212 f on product domain M, as defined in (9), is actually the upper limit of C: \u00b7 f \u2264 4R2\u03bbnwhere R is the maximum length of a differential feature vector, i.e. R: = max i-th block, y-Yi (2).Proof. We follow the same lines as in the above proof of Lemma A.1, but now apply the same limit to the block-by-block definition (8) of curvature on the i-th block. Here, the change from x to y is limited to the coordinates in i-th block M (i). To simplify the notation, let us augment M [i] with the zero domain for all other blocks."}, {"heading": "B. More Details on the Algorithms for Structural SVMs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1. Equivalence of an Exact Frank-Wolfe Step and Loss-Augmented Decoding", "text": "To see that the proposed algorithm 2 actually corresponds exactly to the standard Frank Wolfe algorithm 1 applied to the SVM dual problem (4), we verify that the search direction s, which corresponds to the update w = As, is in fact an exact Frank Wolfe step that can be considered as follows: Lemma B.1. The sparse vector s-Rn constructed in the inner orbit loop of algorithm 2 is an exact solution to s = argmins \"M\" s, \"f (\u03b1 (k)) for the optimization problem (4). Evidence. About the product domain M =\" Y1 |... \"\u00b7 Minimizing mints\" M \"< s\" f \"(\u03b1) > decomposes as follows: Yi\" minsi. \"< si\" if. \""}, {"heading": "B.2. Relation between the Lagrange Duality Gap and the \u2018Linearization\u2019 Gap for the Structural SVM", "text": "We show here that the simple \"linearization gap\" (5), as measured by the structural SVM dual problem (4), actually corresponds to the standard convergence gap (max.) of our Frank Wolfe algorithms (these two duality gaps are generally not identical (4). This is important for the convergence rates of our Frank Wolfe algorithms in order to be transferable as original convergence rates to the original structural SVM target (3), which is the difference with statistical significance (for example with generalization error limits as in Taskar et al. (2003). Thus, let us consider the difference of our target for w: = A\u03b1 in the original problem (3) and the dual object4For example, the two gaps differ in the evaluation of the dual of the conditional random field objective (see for example f)."}, {"heading": "B.3. Convergence Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.3.1. Convergence of the Batch Frank-Wolfe Algorithm 2 on the Structural SVM Dual", "text": "Theorem '1. Algorithm 2 obtains an \u03b5 approximate solution to the structural SVM dual problem (4) and the duality gap g (\u03b1 (k)) \u2264 \u03b5 after at most O (R2\u03bb\u03b5) iterations, with each iteration costing n oracle calls. Proof. We apply the known convergence results to the standard Frank Wolfe algorithm 1, as in (Frank & Wolfe, 1956; Dunn & Harshbarger, 1978; Jaggi, 2013) or as stated in the paragraph shortly after the proof for theorem C.1: For each k \u2265 1, the \u03b1 (k) iteration of algorithm 1 (either using the predefined step sizes or using the line search) E [f (\u03b1 (k)) -f (\u03b1) -f (2 Cfk + 2, where an optimal solution to the problem (4) is set out."}, {"heading": "B.3.2. Convergence of the Block-Coordinate Frank-Wolfe Algorithm 4 on the Structural SVM Dual", "text": "Theorem 3: If Lmax \u2264 4R 2: 0 (i.e. h0 \u2264 4R 2: 0), then the expected error rate (so that h0 = 4R 2: 0), then algorithm 4: 0 (so h0), then algorithm 4: 0 (so h0), then algorithm 4: 0 (so h0), then algorithm 4: 0 (so h0), then the same error and duality gap is guaranteed, while the predefined step size variant requires an additional O (nLmax) size, then it will require at most an additional (constant) number of O (n log), (n log) to get the same error and duality gap, while the predefined step size variant will require an additional O (nLmax) size. Proof. Font h0 = f (0) \u2212 f (so h0), which we use for the error at the starting point, the convergence 2: Theorem."}, {"heading": "B.5. More details on the Kernelized Algorithm", "text": "Both algorithms 2 and 4 can be used with cores by explicitly maintaining the sparse dual variables \u03b1 (k) instead of the primary variables w (k). In this case, the classifier is given only implicitly as a sparse combination of the corresponding core functions, i.e. w = A\u03b1, where the number of non-zero dual variables is limited upwards by the number of iterations, and thus the time to take dot products grows square in the number of iterations. Algorithm B.1 Kernelized dual block coordinate Frank Wolfe for the structural SVMLet \u03b1 (0): = (ey1,..) knel grows in the number of iterations."}, {"heading": "C. Analysis of the Block-Coordinate Frank-Wolfe Algorithm 3", "text": "The main objective is to present the convergence results for the line search variant in Theorem C.4 and Theorem C.5, which we have presented in the convergence for the structural SVM case as above in Theorem 3.Coordinate Descent Methods. Despite their simplicity and very early appearance in the literature, surprisingly few results were known in the literature (and convergence rates in particular) of the coordination types methods. Recently, interest in these methods has grown again due to their good scalability to very large scale problems such as in machine learning, and also new theoretical results such as (Nesterov, 2012)."}, {"heading": "C.1. Setup for Convergence Analysis", "text": "The product structure of our domain has a decisive influence on the duality gap, namely that it splits into a sum using the n components of the domain. The \"linearization gap,\" as defined in (5) (see also Jaggi (2013) for each convex problem of the above form (10), for a fixed, practicable point x-M, is expressed in g (x): = max s-M < x-s, f (x) > = n-m (i) x (i) \u2212 s (i) -s (i), i) f (i) f (x) (n) =: n i = 1g (i) (x). Curvature can also be defined by the individual factors now."}, {"heading": "C.2. Primal Convergence on Product Domains", "text": "According to O (1) many iterations, algorithm C.2 (1) (3) (3) (3) (3) (3) (3) (3) (3) (3) (3) (3) (4) (4) (4) (4) (4) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5 (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5)"}, {"heading": "C.3. Obtaining Small Duality Gap", "text": "The following theory shows that according to O (1) many iterations, algorithm C.2 has found a solution with the least duality gap in anticipation. (1) Since the block coordinate Frank Wolfe algorithm considers only one block at a time, it does not know what its current duality gap is without making a complete (batch) gap in all blocks. (2) Without monitoring this quantity, the algorithm could miss a low duality gap. (2) This is why one is interested in having a good duality gap (as in the structural SVM application), then averaging in (14) and (15) will become interesting: the following theorem also says that the bound gap is held for each of the duality gaps if the duality gap is g, which is the case if f is a quadratic function.7 Theorem C.3 (Primal-Dual Convergence)."}, {"heading": "C.4. An Improved Convergence Analysis for the Line-Search Case", "text": "C.4.1. Improved primary convergence for line search If line search is used, we can improve the convergence results of theorem C.1 by showing a weaker dependence on the initial condition h0 due to the faster progress in the starting phase of the first iterations: theorem C.4 (Improved primary convergence for line search). For each k \u2265 k0, the iteration x (k) of the line search variant of algorithm C.2 (where the linear partial problem is solved with a multiplicative approximation quality (12) of 0 < \u03bd \u2264 1) is satisfactory E [f (x (k)) \u2212 f (x)] \u2264 1\u03bd 2nC f (k \u2212 k0) + 2n (24), where k0: = max {0, log (2) of 0 < \u03bd (x) < C f (< < < < < < n; < n) is the number of steps required to guarantee the problem."}, {"heading": "If the linear subproblem is solved with an additive approximation quality (11) of \u03b4 \u2265 0 instead, then replace all", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,"}, {"heading": "D. Equivalence of the \u2018Linearization\u2019-Duality Gap to a Special Case of Fenchel Duality", "text": "We assume that the objective function f (\u2212 f) x x is convex, but not necessarily differentiable. (30) In this case, the general \"linearization gap\" (5) proposed by (Jaggi, 2013) is byg (x; dx) = I assume that the objective function f (\u2212 dx) x x x is convex, but not necessarily differentiable. (30) Here dx is an arbitrary subgradient to f at candidate position x, and I (y): = sups M < s > the support function of group M. Convexity of f (x) implies that the linarization f (x) + s is always below the graph of the function."}, {"heading": "E. Derivation of the n-Slack Structural SVM Dual", "text": "For a coherent explanation of the Lagrange duality, we refer the reader to Boyd & Vandenberghe (2004, Section 5).The Lagrange model of (1) isL (w, \u03b1) = 2 < w, w > + 1 nn [n], y [n] n [n], y [n] n (y), i (y), \u2212 i (y) > + Li (y)), where \u03b1 = (\u03b11,..., n) [R | Y1 | \u00b7 \u00b7 R | Yn (Rm) are the corresponding (non-negative) Lagrange multipliers, where we have recalibrated the multipliers (dual variables) by a constant of 1n [n], corresponding to the multiplication of the corresponding original explicit constraint by 1n on both sides."}, {"heading": "F. Additional Experiments", "text": "In addition to the results presented in Figure 1 of Discussion 6 of the main paper, here we provide more experimental results as well as more information about the experimental setup used. Figure 2 presents results on OCR that set the step size by line search against the simpler predefined scheme of step size selection of \u03b3k = 2n / (k + 2n). There are BCFW with predefined step sizes similar to SSG, suggesting that most of the improvement in BCFW with line search via SSG comes from the optimal step size choice (and not from the Frank Wolfe formulation on the dual formulation). We also see that BCFW with predefined step sizes can do even worse than line search in the early iterations of small values. Figure 3 and Figure 4 show additional results of stochastic solvers for multiple values of COR CoLL and NLL."}, {"heading": "Supplementary References", "text": "Borwein, J. and Lewis, A. Convex analysis and nonlinear optimization: theory and examples. 2006.Boyd, S. and Vandenberghe, L. Convex optimization Theory and Applications, 72 (1): 7-35, 1992.Patriksson, M. Decomposition methods for differentiable block-coordinate descent methods over cartesian product sets. Computational Optimization and Applications, 9 (1): 5-42, 1998.Richta Chirik, P. and Taka Chip, M. Iteration Complexity of randomized block-coordinate descent methods for minimizing a composite function. Technical Report 1107.2848v1 [math.OC], arXiv, 2011.Teo, C.H., Smola, A.J. Vishwanathan, SVN, Le Q.Q.A, s.scalable Sv, SV.A scalable M7b, S. and Vandenberghe, L. Convex optimization."}], "references": [{"title": "Convex analysis and nonlinear optimization: theory and examples", "author": ["A. Lewis"], "venue": null, "citeRegEx": "Borwein and Lewis,? \\Q2006\\E", "shortCiteRegEx": "Borwein and Lewis", "year": 2006}, {"title": "On the convergence of the coordinate descent method for convex differentiable minimization", "author": ["Z Q Luo", "P. Tseng"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Luo and Tseng,? \\Q1992\\E", "shortCiteRegEx": "Luo and Tseng", "year": 1992}, {"title": "Decomposition methods for differentiable optimization problems over cartesian product sets", "author": ["M. Patriksson"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Patriksson,? \\Q1998\\E", "shortCiteRegEx": "Patriksson", "year": 1998}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Technical Report 1107.2848v1 [math.OC],", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d,? \\Q2011\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d", "year": 2011}, {"title": "A scalable modular convex solver for regularized risk minimization", "author": ["C.H. Teo", "A.J. Smola", "SVN Vishwanathan", "Q.V. Le"], "venue": "ACM SIGKDD, pp. 727\u2013736,", "citeRegEx": "Teo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teo et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 2, "context": "2 has already been proposed in Patriksson (1998), using a generalization of Frank-Wolfe iterations under the name \u2018cost approximation\u2019.", "startOffset": 31, "endOffset": 49}, {"referenceID": 2, "context": "2 has already been proposed in Patriksson (1998), using a generalization of Frank-Wolfe iterations under the name \u2018cost approximation\u2019. The analysis of Patriksson (1998) shows asymptotic convergence, but since the method goes through the blocks sequentially, no convergence rates could be proven so far.", "startOffset": 31, "endOffset": 170}, {"referenceID": 4, "context": "(2009, Theorem 5, see their Equation (23)) or in the appendix of Teo et al. (2007). We can solve the recurrence (26) by following the argument of Teo et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 4, "context": "(2009, Theorem 5, see their Equation (23)) or in the appendix of Teo et al. (2007). We can solve the recurrence (26) by following the argument of Teo et al. (2007), where it was pointed out that since hk is monotonically decreasing, we can upper bound hk by the solution to the corresponding differential equations h\u2032(t) = \u2212h2(t)/\u03b6, with initial condition h(k0) = hk0 .", "startOffset": 65, "endOffset": 164}], "year": 2013, "abstractText": "We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full FrankWolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate FrankWolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.", "creator": "LaTeX with hyperref package"}}}