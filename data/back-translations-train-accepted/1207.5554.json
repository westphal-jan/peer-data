{"id": "1207.5554", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jul-2012", "title": "Bellman Error Based Feature Generation using Random Projections on Sparse Spaces", "abstract": "We address the problem of automatic generation of features for value function approximation. Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error. Empirical results demonstrate the strength of this method.", "histories": [["v1", "Mon, 23 Jul 2012 22:39:51 GMT  (47kb,DS)", "http://arxiv.org/abs/1207.5554v1", null], ["v2", "Tue, 11 Sep 2012 22:53:01 GMT  (44kb,D)", "http://arxiv.org/abs/1207.5554v2", null], ["v3", "Fri, 21 Sep 2012 22:51:40 GMT  (45kb,D)", "http://arxiv.org/abs/1207.5554v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mahdi milani fard", "yuri grinberg", "amir-massoud farahmand", "joelle pineau", "doina precup"], "accepted": true, "id": "1207.5554"}, "pdf": {"name": "1207.5554.pdf", "metadata": {"source": "CRF", "title": "Bellman Error Based Feature Generation using Random Projections on Sparse Spaces", "authors": ["Mahdi Milani Fard", "Yuri Grinberg", "Joelle Pineau", "Doina Precup"], "emails": ["mmilan1@cs.mcgill.ca", "ygrinb@cs.mcgill.ca", "amirf@cs.mcgill.ca", "jpineau@cs.mcgill.ca", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "The accuracy of parameterized policy assessment depends on the quality of the characteristics used to estimate the value function. Therefore, the generation / selection of reinforcements of learning (RL) has received a lot of attention (e.g. [1, 2, 3, 4, 5]. We focus on methods aimed at generating characteristics in the direction of current estimates (Bellman Error Based or BEBF, characteristics).The successive addition of the exact BEBFs has shown that the error of a linear valuer is reduced at a rate of iteration [6]. In contrast to the adjusted value teration [7], which works with a fixed characteristic, the complexity of the hypotheses space increases by adding new characteristics and thus does not deviate as long as the error remains in generation."}, {"heading": "2 Notations and Background", "text": "Throughout this essay, column vectors are represented by bold lowercase letters, and matrices are represented by bold uppercase letters. |. | denotes the size of a set, and M (X) is the scale set to X. \u2022 0 is Donoho's zero standard, which indicates the number of non-zero elements in a vector."}, {"heading": "2.1 Markov Decision Process and Fast Mixing", "text": "A Markov Decision Process (MDP) M = (X, A, T, R) is defined by a (possibly infinite) set of states X, a set of actions A, a transition probability kernel T: X \u00b7 A \u2192 M (X), where T (. | x, a) defines the distribution of the next state, since action a is taken in state x, and a (possibly stochastic) reward function operator R: X \u00b7 A \u2192 M ([0, Rmax]). Throughout the paper, we focus on discounted reward methods MDPs, whereby the discounting factor designated by \u03b3 [0, 1). In discrete increments of time, the reinforcement learner selects an action and receives a reward. The environment then changes to a new state according to the transition core. A policy is a (possibly stochastic) function of states to actions. The value of a state x for policies designated by V-policy is the expected value of learning."}, {"heading": "2.2 Bellman Error Based Feature Generation", "text": "In high-dimensional state spaces, the direct estimation of the value function is not able to provide good results with a small number of projected transitions. Feature selection / extraction methods have therefore been used to create better approximation spaces for the value functions [1, 2, 3, 4, 5]. Among these, we focus on methods aimed at generating characteristics in the direction of the Bellman error, which is defined as: eV (.) = T V (.) error. (4) Let Sn = (xt, rt) nt = 1) be a random sample of the size n collected on a fixed policy. Faced with an estimate V of the value function, time differences (TD) are defined as being: Considered t = rt + V (xt + 1) \u2212 V (xt) It is easy to show that the expectation of the time difference corresponds to a point xt that will project the Bellman error at this point."}, {"heading": "2.3 Random Projections and Inner Product", "text": "Random projections of suitable dimensions are known to preserve sufficient information for an accurate reconstruction with high probability (see e.g. [16, 17]), because random projections are normal and distance-preserving in many classes of attribute spaces [17]. There are several types of random projection matrices that can be used. (6) Recently, random projections of suitable dimensions have been shown to maintain the linearity of a target function on sparse attribute spaces. A sample introduced in [18] and streamlined later shows that if a function is linear in a sparse space, it is almost linear in an exponentially smaller projected space."}, {"heading": "3 Compressed Linear BEBFs", "text": "Let Vm be an estimated value function described in a linear space defined by a feature defined by a feature {1,.}. They also show that if we can estimate the Bellman error within a constant angular error, the error will still shrink (with mild assumptions). Estimating the Bellman error by tracing it back to time differences in high-dimensional sparse spaces can lead to large prediction errors. However, as discussed in Lemma 1, random projections were shown to exponentially reduce the dimension of a sparse feature space, only at the cost of a controlled constant prediction."}, {"heading": "4 Empirical Analysis", "text": "We evaluate our method in a challenging domain in which the objective of the RL agent is to apply direct electrical neurostimulation as it suppresses epileptic behavior in neural tissues. We use a generative model based on real data collected on slices of rat brain tissues. [22] The model is available in the RL glue structure. Observations are generated via a 5-dimensional real state space. Discrete action selection corresponds to the selection of frequency in which neurostimulation is applied. The model is observed in 5 steps per second. Reward is 0 for steps in which seizure occurs, 1 / 41 for when appropriation occurs, 40 / 41 for each stimulation, and 1 else 4.One of the challenges of this domain is that it is difficult to prioritize how to construct good state representation."}, {"heading": "5 Discussion", "text": "In this paper, we provide a simple, fast and robust feature extraction algorithm for political evaluation in sparse and high-dimensional state spaces. On the basis of current results on the properties of random projections, we prove that in sparse spaces random projections of logarithmic quantities in the original dimension are sufficient to maintain linearity. Therefore, BEBFs can be produced on compressed spaces caused by small random projections. Our finite sample analysis provides guarantees for the reduction of errors after the addition of the discussed BEBFs. Empirical analysis in a high-dimensional space with an unknown value function structure shows that CBEBF far outperforms the LSTD with random projections and easily scales to larger problems. It is also more consistent in output and has a much lower memory complexity. We expect this behavior to occur under most common states with irregular functional structures, but such empirical analyses should be performed to confirm this hypothesis."}, {"heading": "6 Concentration Bounds for Mixing Chains", "text": "We give an extension of Bernstein's inequality on the basis of [20].Let x1,. \u00b7 jj (for all A-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "7 Proof of The Theorem 2", "text": "To begin with the proof of the main theorem, it must first be noted that we can write the TD errors as the sum of Bellman errors and some sound concepts. (These sound concepts form a series of martinal differences, since their expectation is the entire story up to this point 0. (22) By random projections we have in compressed space: (XB) (XB) (B), where b is the vector of bias due to the projection. (Let us leave bmax = (XB) prj mmax (W), w). We have from Lemma 1 that with the probability 1 \u2212 1, for all x mmTW is \u2212 X. (B), where b is the vector of bias due to the projection. (Let us leave bmax = (XB) prj mmax (TW)."}, {"heading": "7.1 Bounding the Regression to Bias Terms", "text": "Lemma 7. Under the conditions and with the probability defined in Theorem 2: max. (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT (xT) (xT) (xT (xT) (xT) (xT (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) (xT) ("}, {"heading": "7.2 Bounding the Regression to Noise Terms", "text": "To tie the regression to the noise, we need the following problem on martyrdom: Lemma 8. Let M be a matrix of magnitude l \u00b7 n, in which column t is a function of xt. Then, with a probability of 1 \u2212 2, we have the following answer to the question: \"Why?\" The following problem results directly from the addition of inner productivity. (35) Proof. The inner product between each line of M and each line of M can be limited by a concentration of inequality to martyrdom. (44) The following problem is based on mixing conditions to term.Lemma 9. With the conditions of the theorem, with probability 1 \u2212 4, there is a Yd with all elements in [1, 1] and thus a Yd element in [1, 1] and thus one based on probability."}, {"heading": "8 Proof of Error Contraction Lemmas", "text": "In this section, the proof of the Lemmata, which are presented in the essay, is completed."}, {"heading": "8.1 Proof of Lemma 3", "text": "Proof of Lemma 3. We have that V \u03c0 is the fixed point to the Bellman operator (i.e. T V \u03c0 = V \u03c0), and that the operator has a contraction in relation to the weighted L2 norm on the stationary distribution \u03c1 [25]: T V (x) -T V (x) -T V \"(x) \u2264 V (x) \u2212 V\" (x). (54) So we have: T V (x) \u2212 V (x) + E (x) (x) + E (x)) (x) - V (x) (x) - V (x) - V (x) - V (x) + E (T) (x) - V (x) - V (x) - V (x) (56) - T (x) - V (x) - V (x) - V (x) + E (9) + E (9) - V (9)."}, {"heading": "8.2 Proof of Lemma 4", "text": "The proof for Lemma 4. We have this: \u2022 V \u03c0 (x) \u2212 V (x) \u2264 T V \u03c0 (x) \u2212 T V \u03c0 (x) \u2212 T V (x) \u2022 T V (x) \u2212 V (x) \u2022 V (x) \u2022 V (x) \u2022 V (x) \u2022 V (x) \u2022 V (x) \u2212 V (x) \u2022 V (x) \u2022 V (x) \u2022 V (x), (61) and thus: \u2022 V \u03c0 (x) \u2212 V (x). If the contraction does not occur, then we must have due to Lemma 3: \u2022 V (x) \u2264 11 \u2212 \u03b3 T V (x) \u2212 V (x) \u2212 V (x) \u2012 V (x)."}], "references": [{"title": "Adaptive bases for reinforcement learning", "author": ["D. Di Castro", "S. Mannor"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Automatic basis function construction for approximate dynamic programming and reinforcement learning", "author": ["P.W. Keller", "S. Mannor", "D. Precup"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Extraction of reward-related feature space using correlation-based and reward-based learning methods", "author": ["P. Manoonpong", "F. W\u00f6rg\u00f6tter", "J. Morimoto"], "venue": "Neural Information Processing. Theory and Algorithms,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Online discovery of feature dependencies", "author": ["A. Geramifard", "F. Doshi", "J. Redding", "N. Roy", "J.P. How"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Analyzing feature generation for value-function approximation", "author": ["R. Parr", "C. Painter-Wakefield", "L. Li", "M. Littman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Generalization in reinforcement learning: Safely approximating the value function", "author": ["J. Boyan", "A.W. Moore"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "LSTD with random projections", "author": ["M. Ghavamzadeh", "A. Lazaric", "O.A. Maillard", "R. Munos"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Learning sparse image codes using a wavelet pyramid architecture", "author": ["B.A. Olshausen", "P. Sallee", "M.S. Lewicki"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton. GQ"], "venue": "In Third Conference on Artificial General Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Model selection in reinforcement learning", "author": ["A.M. Farahmand", "C. Szepesv\u00e1ri"], "venue": "Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Basis function adaptation in temporal difference reinforcement learning", "author": ["I. Menache", "S. Mannor", "N. Shimkin"], "venue": "Annals of Operations Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Detection and estimation with compressive measurements", "author": ["M.A. Davenport", "M.B. Wakin", "R.G. Baraniuk"], "venue": "Dept. of ECE, Rice University, Tech. Rep,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "An introduction to compressive sampling", "author": ["E.J. Cand\u00e8s", "M.B. Wakin"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Compressed least-squares regression on sparse spaces", "author": ["M.M. Fard", "Y. Grinberg", "J. Pineau", "D. Precup"], "venue": "In AAAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Random projections preserve linearity in sparse spaces", "author": ["M.M. Fard", "Y. Grinberg", "J. Pineau", "D. Precup"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Concentration of measure inequalities for Markov chains and \u03c6-mixing processes", "author": ["P.M. Samson"], "venue": "Annals of Probability,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Near-optimal signal recovery from random projections: Universal encoding strategies", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Dynamic Representations for Adaptive Neurostimulation Treatment of Epilepsy", "author": ["K. Bush", "J. Pineau", "A. Guez", "B. Vincent", "G. Panuccio", "M. Avoli"], "venue": "4th International Workshop on Seizure Prediction,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Regularized policy iteration", "author": ["A.M. Farahmand", "M. Ghavamzadeh", "C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Linear complementarity for regularized policy evaluation and improvement", "author": ["J. Johns", "C. Painter-Wakefield", "R. Parr"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Learning and value function approximation in complex decision processes", "author": ["B. Van Roy"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2, 3, 4, 5]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 1, "context": "[1, 2, 3, 4, 5]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "[1, 2, 3, 4, 5]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 3, "context": "[1, 2, 3, 4, 5]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "[1, 2, 3, 4, 5]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 5, "context": "Successive addition of exact BEBFs has been shown to reduce the error of a linear value estimator at a rate similar to value iteration [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "Unlike fitted value iteration [7] which works with a fixed feature set, iterative BEBF generation gradually increases the complexity of the hypothesis space by adding new features and thus does not diverge, as long as the error in the generation does not cancel out the contraction effect of the Bellman operator [6].", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "Unlike fitted value iteration [7] which works with a fixed feature set, iterative BEBF generation gradually increases the complexity of the hypothesis space by adding new features and thus does not diverge, as long as the error in the generation does not cancel out the contraction effect of the Bellman operator [6].", "startOffset": 313, "endOffset": 316}, {"referenceID": 4, "context": "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.", "startOffset": 128, "endOffset": 143}, {"referenceID": 0, "context": "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.", "startOffset": 128, "endOffset": 143}, {"referenceID": 3, "context": "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.", "startOffset": 128, "endOffset": 143}, {"referenceID": 5, "context": "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.", "startOffset": 128, "endOffset": 143}, {"referenceID": 2, "context": "A number of methods have been introduced in RL to generate features related to the Bellman error, with a fair amount of success [5, 1, 4, 6, 3], but many of them fail to scale to high dimensional state spaces.", "startOffset": 128, "endOffset": 143}, {"referenceID": 7, "context": "Our results indicate that the proposed method outperforms both gradient type methods, and also LSTD with random projections [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 8, "context": "image, audio and video signals [9]) and also from most discretization-based methods (e.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "Among these there are dynamic programming methods in which one iteratively applies the Bellman operator [10] to an initial guess of the optimal value function.", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "Least-squares temporal difference learning (LSTD) and its derivations [11, 12] are among the methods used to learn a value function based on a finite sample.", "startOffset": 70, "endOffset": 78}, {"referenceID": 11, "context": "Least-squares temporal difference learning (LSTD) and its derivations [11, 12] are among the methods used to learn a value function based on a finite sample.", "startOffset": 70, "endOffset": 78}, {"referenceID": 7, "context": "Using LSTD in spaces induced by random projections is a way of dealing with such domains [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 12, "context": "Stochastic gradient descent type method are also used for value function approximation in high dimensional state spaces, some with proofs of convergence in online and offline settings [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "[14]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].", "startOffset": 118, "endOffset": 133}, {"referenceID": 1, "context": "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].", "startOffset": 118, "endOffset": 133}, {"referenceID": 2, "context": "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].", "startOffset": 118, "endOffset": 133}, {"referenceID": 3, "context": "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].", "startOffset": 118, "endOffset": 133}, {"referenceID": 4, "context": "Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5].", "startOffset": 118, "endOffset": 133}, {"referenceID": 9, "context": "(5) It is easy to show that the expectation of the temporal difference given a point xt equals the Bellman error on that point [10].", "startOffset": 127, "endOffset": 131}, {"referenceID": 14, "context": "[15] introduced two algorithms to adapt basis functions as features for linear function approximation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] applied neighbourhood component analysis as a dimensionality reduction technique to construct a low dimensional state space based on the TD-error.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] later showed that any BEBF extraction method with small angular approximation error will provably tighten approximation error in the value function estimate.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] have recently introduced the incremental Feature Dependency Discovery (iFDD) as a fast online algorithm to extract non-linear binary feature for linear function approximation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16, 17]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[16, 17]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "This is because random projections are norm and distance-preserving in many classes of feature spaces [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "A bound introduced in [18] and later tightened in [19] shows that if a function is linear in a sparse space, it is almost linear in an exponentially smaller projected space.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "A bound introduced in [18] and later tightened in [19] shows that if a function is linear in a sparse space, it is almost linear in an exponentially smaller projected space.", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "An immediate lemma based on Theorem 2 of [19] bounds the bias induced by random projections: Lemma 1.", "startOffset": 41, "endOffset": 45}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] show that if we add a new BEBF \u03c8m+1 = eVm to the feature set, (with mild assumptions) the approximation error on the new linear space shrinks by a factor of \u03b3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "If the Markov chain \u201cforgets\u201d exponentially fast, one can bound the on-measure variance part of the error by a constant error with similar sizes of sampled transitions [20] (second and third line of the bound).", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "The norm of \u03a6 can be bounded using the bounds discussed in [21]; we have with probability 1\u2212 \u03b4\u03a6: \u2016\u03a6\u2016 \u2264 \u221a D/d+ \u221a (2 log(2/\u03b4\u03a6))/d+ 1 and \u2016\u03a6\u2020\u2016 \u2264 [\u221a D/d\u2212 \u221a (2 log(2/\u03b4\u03a6))/d\u2212 1 ]\u22121 .", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "We use a generative model constructed from real-world data collected on slices of rat brain tissues [22]; the model is available in the RL-Glue framework.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "We apply the best clinical fixed rate policy (stimulation is applied at a consistent 1Hz) to collect our sample set [22].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "See [22].", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "LSTD with random projections [8]) and online stochastic gradient type methods (e.", "startOffset": 29, "endOffset": 32}, {"referenceID": 12, "context": "GQ (\u03bb) algorithm [13]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 7, "context": "LSTD with random projections (Compressed LSTD, CLSTD), discussed in [8], is a simple algorithm in which one applies random projections to reduce the dimension of the state space to a manageable size, and then applies LSTD on the compressed space.", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "Among the gradient type methods, we chose the GQ (\u03bb) algorithm [13], as it was expected to provide good consistency.", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "mountain car [10]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "Compared to other regularization approaches to RL [2, 23, 24], our random projection method does not require complex optimization, and thus is faster and more scalable.", "startOffset": 50, "endOffset": 61}, {"referenceID": 22, "context": "Compared to other regularization approaches to RL [2, 23, 24], our random projection method does not require complex optimization, and thus is faster and more scalable.", "startOffset": 50, "endOffset": 61}, {"referenceID": 23, "context": "Compared to other regularization approaches to RL [2, 23, 24], our random projection method does not require complex optimization, and thus is faster and more scalable.", "startOffset": 50, "endOffset": 61}, {"referenceID": 5, "context": "see [6]).", "startOffset": 4, "endOffset": 7}, {"referenceID": 19, "context": "We give an extension of Bernstein\u2019s inequality based on [20].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "For further discussion on this, see [14].", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "The following result from [14] is a trivial corollary of Theorem 2 of Samson [20] (Theorem 2 is stated for empirical processes and can be considered as a generalization of Talagrand\u2019s inequality to dependent random variables): Theorem 5.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "The following result from [14] is a trivial corollary of Theorem 2 of Samson [20] (Theorem 2 is stated for empirical processes and can be considered as a generalization of Talagrand\u2019s inequality to dependent random variables): Theorem 5.", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "[18]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "T V \u03c0 = V ), and that the operator is a contraction with respect to the weighted L norm on the stationary distribution \u03c1 [25]: \u2016T V (x)\u2212 T V (x)\u2016\u03c1(x) \u2264 \u2016V (x)\u2212 V (x)\u2016\u03c1(x) .", "startOffset": 121, "endOffset": 125}], "year": 2017, "abstractText": "We address the problem of automatic generation of features for value function approximation. Bellman Error Basis Functions (BEBFs) have been shown to improve the error of policy evaluation with function approximation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections to generate BEBFs for sparse feature spaces. We provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space are enough to guarantee contraction in the error. Empirical results demonstrate the strength of this method.", "creator": "LaTeX with hyperref package"}}}