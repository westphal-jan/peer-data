{"id": "1208.3561", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2012", "title": "Efficient Active Learning of Halfspaces: an Aggressive Approach", "abstract": "We study pool-based active learning of halfspaces, in which a learner receives a pool of unlabeled examples, and iteratively queries a teacher for the labels of examples from the pool, in order to identify all the labels of pool examples. We revisit the idea of greedily selecting examples to label, and use it to derive an efficient algorithm, called ALuMA, that approximates the optimal label complexity for a given pool in $\\reals^d$. We show that ALuMA obtains an $O(d^2 \\log(d))$ approximation factor if the examples in the pool are numbers with a finite accuracy. We further prove a result for general hypothesis classes, showing that a slight change to the greedy approach leads to an improved target-dependent guarantee on the label complexity. In particular, we conclude a better guarantee for ALuMA if the target hypothesis has a large margin. We further compare our approach to other common active learning strategies, and provide a theoretical and empirical evaluation of the advantages and disadvantages of the approach.", "histories": [["v1", "Fri, 17 Aug 2012 09:49:31 GMT  (1756kb,D)", "http://arxiv.org/abs/1208.3561v1", null], ["v2", "Wed, 19 Dec 2012 15:38:37 GMT  (1266kb,DS)", "http://arxiv.org/abs/1208.3561v2", "Full version of: Gonen, Sabato and Shalev-Shwartz, \"Efficient Active Learning of Halfspaces: an Aggressive Approach\", ICML 2013"], ["v3", "Sat, 25 May 2013 19:23:14 GMT  (2877kb,D)", "http://arxiv.org/abs/1208.3561v3", "Full version of: Gonen, Sabato and Shalev-Shwartz, \"Efficient Active Learning of Halfspaces: an Aggressive Approach\", ICML 2013"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alon gonen", "sivan sabato", "shai shalev-shwartz"], "accepted": true, "id": "1208.3561"}, "pdf": {"name": "1208.3561.pdf", "metadata": {"source": "CRF", "title": "Efficient Pool-Based Active Learning of Halfspaces", "authors": ["Alon Gonen"], "emails": ["alongnn@cs.huji.ac.il", "sabato@cs.huji.ac.il", "shais@cs.huji.ac.il"], "sections": [{"heading": null, "text": "000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053"}, {"heading": "1 Introduction", "text": "In fact, it is a matter of a way in which people are able to determine for themselves what they want to do. (...) It is as if people are able to decide for themselves what they want to do. (...) It is as if they are able to determine for themselves. (...) It is as if they are able to determine for themselves. (...) It is as if they are able to determine for themselves what they want. (...) It is as if they are able to determine for themselves what they want. (...) It is as if they are able to determine for themselves. (...) It is as if they are able to determine for themselves what they want. (...)"}, {"heading": "2 Main Results", "text": "Let P have a distribution over a hypothesis class H. Given the pool X = {x1,., xm}, and some h. \"(We assume) that the probability that such a distribution will occur is higher than the probability that such a distribution will occur. (We assume that there will be such a distribution. (We assume that there will be such a distribution.) (We assume that there will be such a distribution.) (We assume that there will be such a distribution.) (We assume that there will be such a distribution.) P.\" (V \u2212 1), x. \""}, {"heading": "3 The ALuMA algorithm", "text": "We describe our algorithms, which we have listed below as Alg. 1, and explain why Lemma 3 cannot solve the problem of semi-fixed spaces. We call the algorithm Active Learning under a Margin Assumption or ALuMA. Its inputs are the unlabeled examples X, the label oracle L, the maximum allowed number of label queries T, and the desired trustworthiness. (0, 1) It returns the labels of all the examples in X. (2) Two blocks are required to implement an algorithm with the desired guarantees for half-spaces. (First, we must be able to select a pool example that approximates P (V 1t, x) \u00b7 P (V \u2212 1t, x). Both of these sets are able to output a conversion space high enough to reach the label level. (For the first block, we must have the volumes of the sets, V, V 1t, and V \u2212 x)."}, {"heading": "4 Proof Sketch for Theorem 1", "text": "Here we give the most important steps of the proof for theorem 1. Fix a Pool X. Indicate for each algorithm the average version space reduction of Alg after n queries by Favg (Alg, n) = 1 \u2212 Eh \u0445 P [P (Vn (Alg, h))]. Golovin and Krause [2010] prove that A after A is absolutely greedy for any pool-based algorithm, favor (A, n) laboratories (Alg, k) \u2212 favor (Alg, k) \u2212 exp (\u2212 n / approximate). (1) Let yourself opt for an algorithm that achieves OPTmax. It can be shown that for each hypothesis h (H) and each active learner laboratory of V\u03b1h, favor (OPTmax) \u2212 favor (alg, n).P (h) (P) (approximate) that the VTmax (A) (V\u03b1) is sufficient."}, {"heading": "5 On the difficulties in greedy active-learning for halfspaces", "text": "At first glance, it may seem that there are simpler ways to implement an efficient strategy for half-spaces by applying a different distribution P over the hypotheses. Thus, if there are examples in d dimensions, Sauer's problem may be that the effective size of the hypothesis class of half-spaces will be the most md. Thus, one can use the uniform distribution over this finite class, and it is not clear whether this approach can reduce the number of possible hypotheses in ambient space by obtaining a d log (m) factor in relation to the optimal label complexity. However, a direct implementation of this method will be exponential in d, and it is not clear whether this approach has a polynomial implementation. 270 271 272 275 277 277 278 278 282 284 284 284 284 285 285 285 287 288 289 290 291 292 293 303 303 304 307 307 307 307 3010 314 314 314 311 314 314."}, {"heading": "6 Other Approaches: Theoretical and Empirical Comparison", "text": "We are now comparing the effectiveness of the approach implemented by ALuMA with other active learning strategies. ALuMA can be characterized by two characteristics: (1) its \"goal\" is to reduce the volume of version space, and (2) at each iteration, it aggressively selects an example from the pool to minimize its target (roughly) as much as possible (in a greedy sense); we discuss the implications of these characteristics by comparing it to other strategies; property (1) is in contrast to strategies that focus on increasing the number of examples whose designation is known; property (2) is in contrast to strategies that are \"soft\" because their criterion for retrieving examples is softer; and much research has been devoted to the challenge of substantially improving label complexity over regular \"passive\" learning for half-spaces in Rd."}, {"heading": "6.1 Theoretical Comparison", "text": "The complexity of the above-mentioned algorithms is usually analyzed in the PAC environment, so we translate our guarantees into the PAC environment and into comparison. We define the (, m, D) label complexity of an active learning algorithm to be the number of label queries required 324 327 328 329 331 332 333 335 336 338 340 342 344 347 348 353 354 356 357 359 361 363 364 365 366 366 366 368 371 374 375 347 to guarantee that, given a sample of m unlabeled examples from D, the error of the learned classifier is the most (with a probability of at least 1 \u2212 3)."}, {"heading": "6.2 Empirical Comparison", "text": "Figure 1: MNIST (3 vs. 5) We performed an empirical comparison between the 443 algorithms mentioned above. Our goal is twofold: firstly, to evaluate ALuMA in practice, and secondly, to compare the performance of aggressive strategies versus mild strategies; the aggressive strategies are represented in this evaluation by ALuMA and one of the heuristics suggested by Tong and Koller; the mild strategy is represented by CAL. QBC represents a middle ground between aggressive and weak; we also compare a passive ERM algorithm using randomly described examples; we evaluated the algorithms using synthetic and real data sets, and compared their label complexity; details of our implementations and additional results are presented in Appendix D. In the first experiment, the data from the numbers 3 and 5 of the MNIST datasets were 4readable."}, {"heading": "A Proofs", "text": "For the classHline hypothesis, the possible version is spaces after a partial run of an active learner (< v = 1) all of the form [a, b] and [b]. First, it is easy to see that the binary search on the pool identifies any hypothesis in [0, 1] that uses the log (m) example, i.e. OPTmax = log (m). Consider an active learning algorithm that fulfills the following properties: \u2022 If the current conversion space [a, b], it will select x x so that x = min {x \u2212 a) (b \u2212 x)."}, {"heading": "B Randomization in the Optimal Algorithm", "text": "Remember that ALuMA is allowed to use randomization, and it cannot succeed in printing the correct label with the probability \u03b4. (In contrast, the definition of OPTmax requires that the optimal algorithm always be successful, in the effectiveness that makes it deterministic. It can be suggested that the approximation factor that we achieve for ALuMA in Theorem 4 arises from this apparent advantage for ALuMA 4. We now show that this is not the case - the same approximation factor can be achieved if ALuMA and the optimal algorithm allow the same probability of failure. Let m be the size of the pool and let d be the dimension of the examples correct, and set it to 0 = 12md. denote of NB, h) the number of calls to L that occur before the output (L (x1),."}, {"heading": "D Additional Experiments", "text": "This year it is more than ever before."}], "references": [{"title": "Queries and concept learning", "author": ["D. Angluin"], "venue": "Machine learning,", "citeRegEx": "Angluin.,? \\Q1988\\E", "shortCiteRegEx": "Angluin.", "year": 1988}, {"title": "Decision trees for geometric models", "author": ["E.M. Arkin", "H. Meijer", "J.S.B. Mitchell", "D. Rappaport", "S.S. Skiena"], "venue": "In Proceedings of the ninth annual symposium on Computational geometry,", "citeRegEx": "Arkin et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Arkin et al\\.", "year": 1993}, {"title": "Margin based active learning", "author": ["M.F. Balcan", "A. Broder", "T. Zhang"], "venue": "Learning Theory, pages", "citeRegEx": "Balcan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2007}, {"title": "Counting linear extensions is #p-complete", "author": ["G. Brightwell", "P. Winkler"], "venue": "In Proceedings of the twenty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Brightwell and Winkler.,? \\Q1991\\E", "shortCiteRegEx": "Brightwell and Winkler.", "year": 1991}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning,", "citeRegEx": "Cohn et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Dasgupta.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta.", "year": 2005}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Dasgupta.,? \\Q2006\\E", "shortCiteRegEx": "Dasgupta.", "year": 2006}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A. Kalai", "C. Monteleoni"], "venue": "Learning Theory,", "citeRegEx": "Dasgupta et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2005}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine learning,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Active learning for smooth problems", "author": ["E. Friedman"], "venue": "In Proceedings of the 22nd Conference on Learning Theory,", "citeRegEx": "Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Friedman.", "year": 2009}, {"title": "Query by committee made real", "author": ["R. Gilad-Bachrach", "A. Navot", "N. Tishby"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Gilad.Bachrach et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gilad.Bachrach et al\\.", "year": 2005}, {"title": "Adaptive submodularity: A new approach to active learning and stochastic optimization", "author": ["D. Golovin", "A. Krause"], "venue": "In Proceedings of International Conference on Learning Theory (COLT),", "citeRegEx": "Golovin and Krause.,? \\Q2010\\E", "shortCiteRegEx": "Golovin and Krause.", "year": 2010}, {"title": "Teaching dimension and the complexity of active learning", "author": ["S. Hanneke"], "venue": "In COLT,", "citeRegEx": "Hanneke.,? \\Q2007\\E", "shortCiteRegEx": "Hanneke.", "year": 2007}, {"title": "Adaptive rates of convergence in active learning", "author": ["S. Hanneke"], "venue": "In COLT,", "citeRegEx": "Hanneke.,? \\Q2009\\E", "shortCiteRegEx": "Hanneke.", "year": 2009}, {"title": "On the size of weights for threshold gates", "author": ["J. H\u00e5stad"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "H\u00e5stad.,? \\Q1994\\E", "shortCiteRegEx": "H\u00e5stad.", "year": 1994}, {"title": "Hit-and-run mixes fast", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical Programming,", "citeRegEx": "Lov\u00e1sz.,? \\Q1999\\E", "shortCiteRegEx": "Lov\u00e1sz.", "year": 1999}, {"title": "Lectures on discrete geometry, volume 212", "author": ["J. Matou\u0161ek"], "venue": null, "citeRegEx": "Matou\u0161ek.,? \\Q2002\\E", "shortCiteRegEx": "Matou\u0161ek.", "year": 2002}, {"title": "Employing em in pool-based active learning for text classification", "author": ["A. McCallum", "K. Nigam"], "venue": "In Proceedings of ICML-98, 15th International Conference on Machine Learning,", "citeRegEx": "McCallum and Nigam.,? \\Q1998\\E", "shortCiteRegEx": "McCallum and Nigam.", "year": 1998}, {"title": "Theory of majority decision elements", "author": ["S. Muroga", "I. Toda", "S. Takasu"], "venue": "Journal of the Franklin Institute,", "citeRegEx": "Muroga et al\\.,? \\Q1961\\E", "shortCiteRegEx": "Muroga et al\\.", "year": 1961}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "In Proceedings of the fifth annual workshop on Computational learning theory,", "citeRegEx": "Seung et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Seung et al\\.", "year": 1992}, {"title": "Probability of error for optimal codes in a gaussian channel", "author": ["C.E. Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "Shannon.,? \\Q1959\\E", "shortCiteRegEx": "Shannon.", "year": 1959}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Tong and Koller.,? \\Q2002\\E", "shortCiteRegEx": "Tong and Koller.", "year": 2002}, {"title": "For both ALuMA and QBC, we used a fixed number of mixing iterations for hit-and-run", "author": ["Gilad-Bachrach"], "venue": null, "citeRegEx": "Gilad.Bachrach,? \\Q2005\\E", "shortCiteRegEx": "Gilad.Bachrach", "year": 2005}], "referenceMentions": [{"referenceID": 17, "context": "Pool-based active learning [McCallum and Nigam, 1998] is useful in many data-laden applications, where unlabeled data is abundant but labeling is expensive.", "startOffset": 27, "endOffset": 53}, {"referenceID": 4, "context": "For example, the CAL algorithm [Cohn et al., 1994] selects an instance at random and queries its label only if there are two hypotheses in the version space that disagree on its label.", "startOffset": 31, "endOffset": 50}, {"referenceID": 4, "context": "For example, the CAL algorithm [Cohn et al., 1994] selects an instance at random and queries its label only if there are two hypotheses in the version space that disagree on its label. Tong and Koller [2002] proposed a more aggressive greedy selection policy for halfspaces: query the instance from the pool that splits the version space as evenly as possible, in terms of volume in R.", "startOffset": 32, "endOffset": 208}, {"referenceID": 1, "context": "They also showed that the worst-case label complexity of an approximate greedy rule is at most O(log(1/pmin) \u00b7OPTmax), thus exending a result of Arkin et al. [1993].", "startOffset": 145, "endOffset": 165}, {"referenceID": 18, "context": "In particular, by proving a variant of a result due to Muroga et al. [1961], we show that if the examples in the pool X are stored using number of a finite accuracy 1/c, then pmin \u2265 (c/d) 2 .", "startOffset": 55, "endOffset": 76}, {"referenceID": 2, "context": "Balcan et al. [2007] proposed an active learning algorithm with dimension-independent guarantees under a margin assumption.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "When P (h) pmin, the bound in Theorem 1 is stronger than the guarantee \u2200h \u2208 H, N(A, h) \u2264 O(log(1/pmin) \u00b7 OPTmax), obtained by Golovin and Krause [2010]. Importantly, the following theorem shows that this improved bound cannot be obtained for a general approximate-greedy algorithm, even in a simple case such as the problem of thresholds on the line.", "startOffset": 126, "endOffset": 152}, {"referenceID": 11, "context": "We can consider the minimal possible margin, \u03b3 = minh\u2208W \u03b3(h), and deduce from Theorem 4, or from the results of Golovin and Krause [2010], a uniform approximation factor of O(d log(1/\u03b3)).", "startOffset": 112, "endOffset": 138}, {"referenceID": 17, "context": "3, is an adaptation of a classic result due to Muroga et al. [1961]. We conclude that pmin = \u03a9((c/d) 2 ), and deduce an approximation factor of d log(d/c) for the worst-case label complexity of ALuMA.", "startOffset": 47, "endOffset": 68}, {"referenceID": 14, "context": "The exponential dependence of the minimal margin on d here is necessary: As shown in H\u00e5stad [1994], the minimal margin can indeed be exponentially small, even if the points are taken only from {\u00b11}.", "startOffset": 85, "endOffset": 99}, {"referenceID": 3, "context": "The problem of calculating the volume of such convex sets in R is #P-hard if d is not fixed [Brightwell and Winkler, 1991].", "startOffset": 92, "endOffset": 122}, {"referenceID": 16, "context": "Moreover, deterministically approximating the volume is NP-hard in the general case [Matou\u0161ek, 2002].", "startOffset": 84, "endOffset": 100}, {"referenceID": 3, "context": "The problem of calculating the volume of such convex sets in R is #P-hard if d is not fixed [Brightwell and Winkler, 1991]. Moreover, deterministically approximating the volume is NP-hard in the general case [Matou\u0161ek, 2002]. Luckily, it is possible to approximate this volume using randomization. Specifically, in Kannan et al. [1997] a randomized algorithm is provided such that for any convex body K \u2286 R with an efficient separation oracle, with probability at least 1 \u2212 \u03b4 the algorithm returns a non-negative number \u0393 such that (1 \u2212 )\u0393 < P (K) < (1 + )\u0393.", "startOffset": 93, "endOffset": 336}, {"referenceID": 15, "context": "We can efficiently draw a hypothesis approximately uniformly from V , by using the hit-and-run algorithm [Lov\u00e1sz, 1999].", "startOffset": 105, "endOffset": 119}, {"referenceID": 11, "context": "Golovin and Krause [2010] prove that since A is \u03b1-approximately greedy, for any pool-based algorithm alg, favg(A, n) \u2265 favg(alg, k)\u2212 exp(\u2212n/\u03b1k).", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Note that the CAL algorithm Cohn et al. [1994], which we discuss in Section 6, can be seen as implementing a mellow version of this approach, since it decreases the so-called \u201cdisagreement region\u201d in each iteration.", "startOffset": 28, "endOffset": 47}, {"referenceID": 5, "context": "This is not the case, as evident by the following example, which is an adaptation of an example from Dasgupta [2005].", "startOffset": 101, "endOffset": 117}, {"referenceID": 20, "context": "It was shown in Shannon [1959] that for any m \u2264 O(1/\u03b3), there exists a set of points that satisfy the conditions above.", "startOffset": 16, "endOffset": 31}, {"referenceID": 4, "context": ", 1997], the CAL algorithm [Cohn et al., 1994], and the Active Perceptron [Dasgupta et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 7, "context": ", 1994], and the Active Perceptron [Dasgupta et al., 2005].", "startOffset": 35, "endOffset": 58}, {"referenceID": 6, "context": "To demonstrate the effect of the second property described above\u2014being aggressive versus being mellow, we consider the following example, adapted slightly from [Dasgupta, 2006].", "startOffset": 160, "endOffset": 176}, {"referenceID": 2, "context": "These examples strengthen the observation of Balcan et al. [2007] that in some cases a more aggressive approach is preferable.", "startOffset": 45, "endOffset": 66}, {"referenceID": 21, "context": "The aggressive strategies are represented in this evaluation by ALuMA and one of the heuristics proposed by Tong and Koller [2002]. The mellow strategy is represented by CAL.", "startOffset": 108, "endOffset": 131}, {"referenceID": 0, "context": "In the limit of an infinite number of unlabeled examples, if the distribution has a non-zero support on the entire domain, the pool-based setting becomes identical to the setting of membership queries [Angluin, 1988].", "startOffset": 201, "endOffset": 216}], "year": 2017, "abstractText": "We study pool-based active learning of halfspaces, in which a learner receives a pool of unlabeled examples, and iteratively queries a teacher for the labels of examples from the pool, in order to identify all the labels of pool examples. We revisit the idea of greedily selecting examples to label, and use it to derive an efficient algorithm, called ALuMA, that approximates the optimal label complexity for a given pool in R. We show that ALuMA obtains an O(d log(d)) approximation factor if the examples in the pool are numbers with a finite accuracy. We further prove a result for general hypothesis classes, showing that a slight change to the greedy approach leads to an improved target-dependent guarantee on the label complexity. In particular, we conclude a better guarantee for ALuMA if the target hypothesis has a large margin. We further compare our approach to other common active learning strategies, and provide a theoretical and empirical evaluation of the advantages and disadvantages of the approach.", "creator": "LaTeX with hyperref package"}}}