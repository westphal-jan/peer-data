{"id": "1209.1077", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2012", "title": "Learning Probability Measures with respect to Optimal Transport Metrics", "abstract": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.", "histories": [["v1", "Wed, 5 Sep 2012 19:10:09 GMT  (521kb,D)", "http://arxiv.org/abs/1209.1077v1", "13 pages, 2 figures. Advances in Neural Information Processing Systems, NIPS 2012"]], "COMMENTS": "13 pages, 2 figures. Advances in Neural Information Processing Systems, NIPS 2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["guillermo d ca\u00f1as", "lorenzo rosasco"], "accepted": true, "id": "1209.1077"}, "pdf": {"name": "1209.1077.pdf", "metadata": {"source": "CRF", "title": "Learning Probability Measures with respect to Optimal Transport Metrics", "authors": ["Guille D. Canas", "Lorenzo A. Rosasco"], "emails": ["guilledc@mit.edu", "lrosasco@mit.edu"], "sections": [{"heading": "1 Introduction and Motivation", "text": "In this paper, we examine the problem of learning random samples based on a variety of factors when measuring the learning error using transport metrics. Learning a probability distribution is classic in statistics and machine learning and is typically analyzed for distributions in X = Rd that have a density in terms of the Lebesgue measure, and L2 in terms of the common distances used to measure the proximity of two densities."}, {"heading": "2 Setup and Previous work", "text": "Consider the problem of learning a probability dimension \u03c1 defined in a roomM, from an i.i.d. example Xn = (x1,., xn) \u0445 \u03c1n of magnitude n. We assume that M would be a compact, smooth d-dimensional multiplicity with a C1 metric and a volume dimension \u03bbM embedded in the unit sphere of a divisible Hilbert space X with an inner product < \u00b7, \u00b7, induced standard, and distance d (for example M = Bd2 (1) the unit sphere in X = Rd.). Following [34, p. 94], we leave Pp (M) the Waterstone space of the order 1 \u2264 p < \u221e: Pp (M) metric: = {2 Waterstone ratio the unit sphere in X = Rd."}, {"heading": "2.1 Closeness of Empirical and Population Measures", "text": "According to the empirical law of large numbers, the empirical measure almost certainly converges with the population measure: \u043d n \u2192 \u03c1 in the sense of weak topology [33]. Since weak convergence and convergence in Wp in Pp (M) are equivalent, this means that the empirical measure \u03c1 in the Wp sense is an arbitrarily good approximation of \u03c1 as n \u00b2. A fundamental question is therefore how fast the convergence rate \u03c1 n \u00b2 \u03c1 is."}, {"heading": "2.1.1 Convergence in expectation", "text": "The mean convergence rate of n \u00b2 n \u00b2 n \u00b2 n has been extensively investigated in the past, resulting in higher limits of the order EW2 (n \u2212 1 / (d + 2) [19, 8] and lower limits of the order EW2 (n \u2212 1 / d) [29] (both assuming that the absolutely continuous part of the order A is 6 = 0, with possibly better rates). More recently, an upper limit of the order EWp (n \u2212 1 / d) = O (n \u2212 1 / d) [2] has been proposed, demonstrating a limit to the problem of optimal bidirectional fit (OBM) [1] and relating this problem to the expected distance EWp (n \u00b2 n)."}, {"heading": "2.1.2 Convergence in probability", "text": "The results for convergence in probability, one of the main results of this work, seem to be much more difficult to obtain. A fruitful way of analysis was the use of so-called transport or talagrand inequalities Tp, which can be used to prove inequalities in concentration on Wp [20]. In particular, we say that \u03c1 is a Tp (C) inequality with C > 0 iff Wp (p, p) 2 \u2264 CH (p) inequalities on Wp (M), where H (\u00b7) is relative entropy [20]. As shown in [6, 5], it is possible to obtain a probabilistic upper limit on Wp (p, p) 2 \u2264 CH (p) -n if it is known that a Tp inequality of the same magnitude is satisfied, thereby reducing the problem of limit Wp (p, p)."}, {"heading": "3 Learning probability measures, optimal trans-", "text": "We deal with the problem of learning a probability quantity when the only observation available to us is an i.i.d. sample Xn. We start by determining some notation and useful intermediate results.If a closed set S is M, we leave \u03c0 S = \u2211 q S 1Vq (S) \u00b7 q a next neighboring projection on S (a function mapping in X to its next point in S) where {Vq (S): q S} is a Borel-Voronoi division of X, so that Vq (S) X: x q X = minr S x \u2212 r \u00b2 (see for example [15]. Since S is closed and the x \u2212 x division is continuous and convex, each point x is in S. Since each x-division in S has a next point. Since {Vq (S): q S): q S is an optimal division in S, it follows."}, {"heading": "3.1 Lower bounds", "text": "If we look at the situation shown in Fig. 1, where a sample X4 = {x1, x2, x3, x4} is clearly drawn from a distribution gap from which we assume that it is absolutely continuous, the projection map sends points x to its nearest point in X4. The resulting voronoi resolution of supp (\u03c1) is drawn in shades of blue. According to Fig. 5,2 of [9], the pair-by-pair overlap of the preonoi regions has an ambient measurement value, and since this is absolutely continuous, the pushforward measurement in this case may be called \u03c0X4 \u03c1 = 4 j = 1 Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospective Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Prospect Pr"}, {"heading": "4 Unsupervised learning algorithms for learning", "text": "There cannot be a clear link between uncontrolled learning and the problem of the probability of learning in relation to a general problem that turns out to be a problem. (...) There cannot be a clear link between uncontrolled learning and the problem of the probability of learning in relation to the problem of the general probability of learning. (...) There cannot be a clear link between uncontrolled learning and the problem of the probability of learning in relation to the problem of the general probability of learning. (...) There cannot be a clear answer to this question. (...) There cannot be a clear answer to the question of the probability of learning. (...) An answer to this question is the question of the degree of learning. (...) An answer to the question of the degree of learning. (...) An answer to the question of the degree of learning. (...) An answer to the question of the degree of learning. (...) A question of the degree of learning. (...) A question of the degree of learning. (...) A question of the degree of learning. (...) A question of the degree of learning. (...) A question of the degree of learning."}, {"heading": "5 Learning rates", "text": "To analyze the performance of the k mean as a measure of learning algorithm and the convergence of empirical and population measures, we propose the decomposition shown in Fig. 2. The diagram encompasses all the measures considered in the thesis and shows the two decomposition quantities used to detect upper limits. The upper arrow (green) illustrates the decomposition used to limit the distance W2 (3, 4). This decomposition uses the measurements p and p as intermediate quantities to arrive at \u03c1 n, where Sk is a k point of optimal quantifier of p, that is, a quantity of Sk that minimizes Ex \u0445 \u03c1d (x, S) 2, in such a way that | Sk | = k. The lower arrow (blue) corresponds to the decomposition of W2 (3, 4, 5 and 5) (the performance of the k mean), while the simpler arrows begin with the individual boundaries."}, {"heading": "5.1 Convergence rates for the empirical law of large numbers", "text": "Let Sk be the optimal k-point quantizer of the order two [14, p. 31]. From the triangular inequality and identity (a + b + c) 2 \u2264 3 (a2 + b2 + c2) it follows that W2 (b + b + c2) 2 \u2264 3 [W2 (b + b + c) 2 + W2 (n) 2 + W2 (n) 2 + W2 (n). (3) This is the decomposition shown in the upper arrow of Fig. 2. Below point 3.1, the first term in the sum of Equation 3 is the optimal k-point quantization error of the order, which results from a d-manifold M, which is represented using newer techniques from [16] (see also [17, p. 491]) in the proof of ltm ability 5,1 (part a), which results from the order."}, {"heading": "5.2 Learning rates of k-means", "text": "The key element of the proof of theory d 5,1 is that the distance between population and empirical measures can be automatically limited (by selecting an average optimal quantization variable).In the analysis, the best limits for k < n.If the output of k means is close to an optimal quantizer (for example, if sufficient data is available), then we expect the best limits for k means to be a choice of k < n.The decomposition of the lower (blue) arc in Figure 2 leads to the following limit of probability. Theorem 5,2. Given the fact that Pp (M) with the absolutely continuous part of theory A 6 = 0, and 0 < g < 1, then for all sufficiently large n, and lettingk = C \u00b7 m \u00b7 nd / (2d + 4), it is holdsW2 (f)."}], "references": [{"title": "On optimal matchings", "author": ["M. Ajtai", "J. Komls", "G. Tusndy"], "venue": "Combinatorica, 4:259\u2013264", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1984}, {"title": "Combinatorial optimization over two random point sets", "author": ["Franck Barthe", "Charles Bordenave"], "venue": "Technical Report", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "The Gaussian isoperimetric inequality and transportation", "author": ["Gordon Blower"], "venue": "Positivity, 7:203\u2013224,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Exponential integrability and transportation cost related to logarithmic Sobolev inequalities", "author": ["S.G. Bobkov", "F. G\u00f6tze"], "venue": "Journal of Functional Analysis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Simple bounds for the convergence of empirical and occupation measures in 1-Wasserstein", "author": ["Emmanuel Boissard"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Quantitative concentration inequalities for empirical measures on non-compact spaces", "author": ["F. Bolley", "A. Guillin", "C. Villani"], "venue": "Probability Theory and Related Fields, 137(3):541\u2013593", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Weighted Csisz\u00e1r-Kullback-Pinsker inequalities and applications to transportation inequalities", "author": ["F. Bolley", "C. Villani"], "venue": "Annales de la Faculte des Sciences de Toulouse, 14(3):331\u2013352", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Deconvolution for the Wasserstein metric and geometric inference", "author": ["Claire Caillerie", "Fr\u00e9d\u00e9ric Chazal", "J\u00e9r\u00f4me Dedecker", "Bertrand Michel"], "venue": "Rapport de recherche RR-7678,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Building triangulations using -nets", "author": ["Kenneth L. Clarkson"], "venue": "In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Combinatorial methods in density estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Asymptotics for transportation cost in high dimensions", "author": ["V. Dobri", "J. Yukich"], "venue": "Journal of Theoretical Probability, 8:97\u2013118", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Vector Quantization and Signal Compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Kluwer International Series in Engineering and Computer Science. Kluwer Academic Publishers", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "On choosing and bounding probability metrics", "author": ["Alison L. Gibbs", "Francis E. Su"], "venue": "International Statistical Review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Foundations of quantization for probability distributions", "author": ["Siegfried Graf", "Harald Luschgy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Distortion mismatch in the quantization of probability measures", "author": ["Siegfried Graf", "Harald Luschgy", "Gilles Pages"], "venue": "Esaim: Probability and Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Optimum quantization and its applications", "author": ["Peter M. Gruber"], "venue": "Adv. Math,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Convex and discrete geometry", "author": ["P.M. Gruber"], "venue": "Grundlehren der mathematischen Wissenschaften. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel density estimation on riemannian manifolds: Asymptotic results", "author": ["Guillermo Henry", "Daniela Rodriguez"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Mean rates of convergence of empirical measures in the Wasserstein metric", "author": ["Joseph Horowitz", "Rajeeva L. Karandikar"], "venue": "J. Comput. Appl. Math.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "The Concentration of Measure Phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs. American Mathematical Society", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "K\u2013dimensional coding schemes in Hilbert spaces", "author": ["A. Maurer", "M. Pontil"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Ricci curvature of markov chains on metric spaces", "author": ["Yann Ollivier"], "venue": "J. Funct. Anal.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Submanifold density estimation", "author": ["Arkadas Ozakin", "Alexander Gray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "The probabilistic analysis of matching heuristics", "author": ["C. Papadimitriou"], "venue": "Proc. of the 15th Allerton Conf. on Communication, Control and Computing, pages 368\u2013378", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1978}, {"title": "Kernel density estimation on", "author": ["Bruno Pelletier"], "venue": "Riemannian manifolds. Statist. Probab. Lett.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements", "author": ["Xavier Pennec"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Information and information stability of random variables and processes", "author": ["M.S. Pinsker"], "venue": "San Francisco: Holden-Day", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1964}, {"title": "Quantization and the method of k-means", "author": ["David Pollard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1982}, {"title": "Probability metrics and the stability of stochastic models", "author": ["S.T. Rachev"], "venue": "Wiley series in probability and mathematical statistics: Applied probability and statistics. Wiley", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1991}, {"title": "Probability Theory and Combinatorial Optimization", "author": ["J.M. Steele"], "venue": "Cbms-Nsf Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Transportation cost for Gaussian and other product measures", "author": ["M. Talagrand"], "venue": "Geometric And Functional Analysis, 6:587\u2013600", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1996}, {"title": "Introduction to nonparametric estimation", "author": ["Alexandre B. Tsybakov"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "On the convergence of sample probability distributions", "author": ["V.S. Varadarajan"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1958}, {"title": "Optimal Transport: Old and New", "author": ["C. Villani"], "venue": "Grundlehren der Mathematischen Wissenschaften. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Manifold Parzen Windows", "author": ["P. Vincent", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "The problem of learning a probability distribution is classic in statistics and machine learning, and is typically analyzed for distributions in X = R that have a density with respect to the Lebesgue measure, with total variation, and L2 among the common distances used to measure closeness of two densities (see for instance [10, 32] and references therein.", "startOffset": 326, "endOffset": 334}, {"referenceID": 31, "context": "The problem of learning a probability distribution is classic in statistics and machine learning, and is typically analyzed for distributions in X = R that have a density with respect to the Lebesgue measure, with total variation, and L2 among the common distances used to measure closeness of two densities (see for instance [10, 32] and references therein.", "startOffset": 326, "endOffset": 334}, {"referenceID": 34, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 167, "endOffset": 179}, {"referenceID": 22, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 167, "endOffset": 179}, {"referenceID": 17, "context": "In particular, kernel density estimators on manifolds have been described in [35], and their pointwise consistency, as well as convergence rates, have been studied in [25, 23, 18].", "startOffset": 167, "endOffset": 179}, {"referenceID": 25, "context": "A discussion on several topics related to statistics on a Riemannian manifold can be found in [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "Interestingly, the problem of approximating measures with respect to transportation distances has deep connections with the fields of optimal quantization [14, 16], optimal transport [34] and, as we point out in this work, with unsupervised learning (see Sec.", "startOffset": 155, "endOffset": 163}, {"referenceID": 15, "context": "Interestingly, the problem of approximating measures with respect to transportation distances has deep connections with the fields of optimal quantization [14, 16], optimal transport [34] and, as we point out in this work, with unsupervised learning (see Sec.", "startOffset": 155, "endOffset": 163}, {"referenceID": 33, "context": "Interestingly, the problem of approximating measures with respect to transportation distances has deep connections with the fields of optimal quantization [14, 16], optimal transport [34] and, as we point out in this work, with unsupervised learning (see Sec.", "startOffset": 183, "endOffset": 187}, {"referenceID": 33, "context": "The space Pp(M) with the Wp metric is itself a complete separable metric space [34].", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "There are many possible choices of distances between probability measures [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 33, "context": "Among them, Wp metrizes weak convergence (see [34] theorem 6.", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "Wasserstein distances have been used to study the mixing and convergence of Markov chains [22], as well as concentration of measure phenomena [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Wasserstein distances have been used to study the mixing and convergence of Markov chains [22], as well as concentration of measure phenomena [20].", "startOffset": 142, "endOffset": 146}, {"referenceID": 32, "context": "By the empirical law of large numbers, the empirical measure converges almost surely to the population measure: \u03c1\u0302n \u2192 \u03c1 in the sense of the weak topology [33].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "The mean rate of convergence of \u03c1\u0302n \u2192 \u03c1 has been widely studied in the past, resulting in upper bounds of order EW2(\u03c1, \u03c1\u0302n) = O(n\u22121/(d+2)) [19, 8], and lower", "startOffset": 139, "endOffset": 146}, {"referenceID": 7, "context": "The mean rate of convergence of \u03c1\u0302n \u2192 \u03c1 has been widely studied in the past, resulting in upper bounds of order EW2(\u03c1, \u03c1\u0302n) = O(n\u22121/(d+2)) [19, 8], and lower", "startOffset": 139, "endOffset": 146}, {"referenceID": 28, "context": "bounds of order EW2(\u03c1, \u03c1\u0302n) = \u03a9(n\u22121/d) [29] (both assuming that the absolutely continuous part of \u03c1 is \u03c1A 6= 0, with possibly better rates otherwise).", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "More recently, an upper bound of order EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d) has been proposed [2] by proving a bound for the Optimal Bipartite Matching (OBM) problem [1], and relating this problem to the expected distance EWp(\u03c1, \u03c1\u0302n).", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "More recently, an upper bound of order EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d) has been proposed [2] by proving a bound for the Optimal Bipartite Matching (OBM) problem [1], and relating this problem to the expected distance EWp(\u03c1, \u03c1\u0302n).", "startOffset": 152, "endOffset": 155}, {"referenceID": 23, "context": "In particular, given two independent samples Xn, Yn, the OBM problem is that of finding a permutation \u03c3 that minimizes the matching cost n\u22121 \u2211 \u2016xi \u2212 y\u03c3(i)\u2016 [24, 30].", "startOffset": 156, "endOffset": 164}, {"referenceID": 29, "context": "In particular, given two independent samples Xn, Yn, the OBM problem is that of finding a permutation \u03c3 that minimizes the matching cost n\u22121 \u2211 \u2016xi \u2212 y\u03c3(i)\u2016 [24, 30].", "startOffset": 156, "endOffset": 164}, {"referenceID": 1, "context": "By Jensen\u2019s inequality, the triangle inequality, and (a+ b) \u2264 2p\u22121(ap + b), it holds EWp(\u03c1, \u03c1\u0302n) \u2264 EWp(\u03c1\u0302Xn , \u03c1\u0302Yn ) p \u2264 2EWp(\u03c1, \u03c1\u0302n), and therefore a bound of order O(n\u2212p/d) for the OBM problem [2] implies a bound EWp(\u03c1, \u03c1\u0302n) = O(n\u22121/d).", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "The matching lower bound is only known for a special case: \u03c1A constant over a bounded set of non-null measure [2] (e.", "startOffset": 110, "endOffset": 113}, {"referenceID": 10, "context": ") Similar results, with matching lower bounds are found for W1 in [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "One fruitful avenue of analysis has been the use of so-called transportation, or Talagrand inequalities Tp, which can be used to prove concentration inequalities on Wp [20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "In particular, we say that \u03c1 satisfies a Tp(C) inequality with C > 0 iff Wp(\u03c1, \u03bc) 2 \u2264 CH(\u03bc|\u03c1),\u2200\u03bc \u2208 Pp(M), where H(\u00b7|\u00b7) is the relative entropy [20].", "startOffset": 143, "endOffset": 147}, {"referenceID": 5, "context": "As shown in [6, 5], it is possible to obtain probabilistic upper bounds on Wp(\u03c1, \u03c1\u0302n), with p = 1, 2, if \u03c1 is known to satisfy a Tp inequality of the same order, thereby reducing the problem of bounding Wp(\u03c1, \u03c1\u0302n) to that of obtaining a Tp inequality.", "startOffset": 12, "endOffset": 18}, {"referenceID": 4, "context": "As shown in [6, 5], it is possible to obtain probabilistic upper bounds on Wp(\u03c1, \u03c1\u0302n), with p = 1, 2, if \u03c1 is known to satisfy a Tp inequality of the same order, thereby reducing the problem of bounding Wp(\u03c1, \u03c1\u0302n) to that of obtaining a Tp inequality.", "startOffset": 12, "endOffset": 18}, {"referenceID": 19, "context": "Note that, by Jensen\u2019s inequality, and as expected from the behavior ofWp, the inequality T2 is stronger than T1 [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "While it has been shown that \u03c1 satisfies a T1 inequality iff it has a finite square-exponential moment [4, 7], no such general conditions have been found for T2.", "startOffset": 103, "endOffset": 109}, {"referenceID": 6, "context": "While it has been shown that \u03c1 satisfies a T1 inequality iff it has a finite square-exponential moment [4, 7], no such general conditions have been found for T2.", "startOffset": 103, "endOffset": 109}, {"referenceID": 33, "context": "15 of [34], and the celebrated Csisz\u00e1r-Kullback-Pinsker inequality [27], for all \u03c1, \u03bc \u2208 Pp(M), it is", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "15 of [34], and the celebrated Csisz\u00e1r-Kullback-Pinsker inequality [27], for all \u03c1, \u03bc \u2208 Pp(M), it is", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "The T2 inequality has been shown by Talagrand to be satisfied by the Gaussian distribution [31], and then slightly more generally by strictly log-concave measures [3].", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "The T2 inequality has been shown by Talagrand to be satisfied by the Gaussian distribution [31], and then slightly more generally by strictly log-concave measures [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "However, as noted in [6], \u201ccontrary to the T1 case, there is no hope to obtain T2 inequalities from just integrability or decay estimates.", "startOffset": 21, "endOffset": 24}, {"referenceID": 14, "context": "Given a closed set S \u2286 M, let \u03c0 S = \u2211 q\u2208S 1Vq(S) \u00b7 q be a nearest neighbor projection onto S (a function mapping points in X to their closest point in S), where {Vq(S) : q \u2208 S} is a Borel Voronoi partition of X such that Vq(S) \u2286 {x \u2208 X : \u2016x \u2212 q\u2016 = minr\u2208S \u2016x \u2212 r\u2016} (see for instance [15].", "startOffset": 282, "endOffset": 286}, {"referenceID": 27, "context": "This close connection between optimal quantization and Wasserstein distance has been pointed out in the past in the statistics [28], optimal quantization [14, p.", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "33], and approximation theory literatures [16].", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "2 of [9], the pairwise intersections of Voronoi regions have null ambient measure, and since \u03c1 is absolutely continuous, the pushforward measure can be written in this case as \u03c0 X4 \u03c1 = \u22114 j=1 \u03c1(Vj)\u03b4xj , where Vj is the Voronoi region of xj .", "startOffset": 5, "endOffset": 8}, {"referenceID": 15, "context": "78] and [16].", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "As described in [21], several of the most widely used unsupervised learning algorithms can be interpreted to take as input a sample Xn and output a set \u015ck, where k is typically a free parameter of the algorithm, such as the number of means in k-means, the dimension of affine spaces in PCA, etc.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": ") This formulation is general enough to encompass k-means and PCA, but also k-flats, non-negative matrix factorization, and sparse coding (see [21] and references therein.", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "This connection between k-means and W2 measure approximation was, to the best of the authors\u2019 knowledge, first suggested by Pollard [28] though, as mentioned earlier, the argument carries over to many other unsupervised learning algorithms.", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "This effectively reduces the problem of learning a measure to that of finding a set, and is akin to how the fact that every optimal quantizer is a nearest-neighbor quantizer (see [15], [12, p.", "startOffset": 179, "endOffset": 183}, {"referenceID": 15, "context": "from [16] (see also [17, p.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "(see for instance [21]), for which non-matching lower bounds are known.", "startOffset": 18, "endOffset": 22}], "year": 2012, "abstractText": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.", "creator": "LaTeX with hyperref package"}}}