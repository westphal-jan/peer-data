{"id": "1209.1121", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2012", "title": "Learning Manifolds with K-Means and K-Flats", "abstract": "We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by k-means and k-flats, and analyze their performance. We extend previous results for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-flats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-flats, both the results and the mathematical tools are new.", "histories": [["v1", "Wed, 5 Sep 2012 21:18:03 GMT  (240kb,D)", "https://arxiv.org/abs/1209.1121v1", "13 pages, 2 figures"], ["v2", "Fri, 7 Sep 2012 17:11:23 GMT  (240kb,D)", "http://arxiv.org/abs/1209.1121v2", "13 pages, 2 figures; Advances in Neural Information Processing Systems, NIPS 2012"], ["v3", "Tue, 11 Sep 2012 16:00:03 GMT  (240kb,D)", "http://arxiv.org/abs/1209.1121v3", "13 pages, 2 figures; Advances in Neural Information Processing Systems, NIPS 2012"], ["v4", "Tue, 19 Feb 2013 17:53:17 GMT  (257kb,D)", "http://arxiv.org/abs/1209.1121v4", "19 pages, 2 figures; Advances in Neural Information Processing Systems, NIPS 2012"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["guillermo d ca\u00f1as", "tomaso a poggio", "lorenzo rosasco"], "accepted": true, "id": "1209.1121"}, "pdf": {"name": "1209.1121.pdf", "metadata": {"source": "CRF", "title": "Learning Manifolds with K-Means and K-Flats", "authors": ["Guillermo D. Canas", "Tomaso Poggio", "Lorenzo A. Rosasco"], "emails": ["guilledc@mit.edu", "tp@ai.mit.edu", "lrosasco@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "2 Learning Manifolds", "text": "Leave X through a Hilbert space with internal product < \u00b7, \u00b7 >, equipped with a Borel probability measure \u03c1, supported by a compact, smooth d-manifold M. We assume that the data are given by a training set in the form of samples Xn = (x1,., xn), which are identical and independently drawn with \u03c1. Our goal is to learn a set Sn, which approximates the manifold well. Approximation (learning error) is measured by the expected reconstruction error EIB (Sn): = \"M d2 X (x, x) d2 X (x, Sn), (1) where the distance to a set S X d2 X (x, S) = infx \u00b2 S 2 X (x), where dX (x, x) risk x \u2212 x \u00b2) d2 X (x, Sn) is the distance to a set S X (x), where the X (S) is equal to a set S (2) with a measure X (x) x x)."}, {"heading": "2.1 Using K-Means and K-Flats for Piecewise Manifold Approximation", "text": "In this paper, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12]. Although typically discussed in Euclidean space, their definition can easily be extended to a Hilbert space setting. Examining multiplicities embedded in a Hilbert space is of particular interest when looking at non-linear (core) versions of algorithms [20]. More generally, this setting can be considered a limit case when dealing with high-dimensional data. Of course, the more classical setting of an absolutely continuous distribution over d-dimensional euclidean space is simply a special case where X = Rd, and M is a domain with positive lebesgue measurement. Let H = Sk be the class of magnitudes k in X. In view of a training set Xn and a choice of k means, it is defined by minimizing via S."}, {"heading": "2.2 Learning a Manifold with K-means and K-flats", "text": "In practice, the means are often interpreted as cluster algorithms, with clusters defined by the voronoi diagrams of the mean values Sn, k. In this interpretation, equation 2 is simply rewritten by adding up the voronoi regions and adding up all the pairwise distances between the samples in the region (the intracluster-related distances).For example, this view is taken into account by examining k means from an information theory. K means can also be interpreted to minimize the encoding error associated with a nearest quantifier."}, {"heading": "3 Reconstruction Properties of k-Means", "text": "Since we are interested in the behavior of the expected reconstruction error itself over the hypotheses space H = Sk, we can define Sk apartments for varying k and n before analyzing this behavior, we consider what is currently known about this problem based on previous work. While k apartments is a relatively new algorithm whose behavior is not well understood, several properties of the k mean are currently known. Remember that k averages find a discrete amount Sn, k the size k that best approximates the samples in the sense of (2). Clearly, how k increases, the empirical reconstruction error En (Sn, k) cannot increase, and typically decreases, but we are ultimately interested in the expected reconstruction error and would therefore like to understand the behavior of E\u03c1 with varying k, n.In the context of optimal quantization, the behavior of the expected reconstruction error En, the Sk error, is taken into account."}, {"heading": "4 Main Results", "text": "contributions. Our work expands previous results in two different directions: (a) We provide an analysis of k averages for the case where the data-generating distribution is based on a multiplicity embedded in a Hilbert space. In particular, we derive new results from the approximation error in this setting, and (2) new sample complexity results (learning rates) resulting from the choice of k by optimizing the resulting boundary. We analyze the case where a solution is obtained from an approximation algorithm such as k averages + + [3] to include this calculation error within the boundaries. (b) We generalize the above results from k averages to k levels and derive learning rates from both statistical and approximation errors. To the best of our knowledge, these results provide the first theoretical analysis of k averages in both directions."}, {"heading": "4.1 Learning Rates for k-Means", "text": "The first result considers the idealized case in which we have access to an exact solution for k-means.Theorem \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4.2 Learning Rates for k-Flats", "text": "To examine k-flats, we must slightly strengthen acceptance 1 by supplementing it with the following basic assumption: (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance 2). (acceptance. (acceptance 2)."}, {"heading": "4.3 Discussion", "text": "In all results, the final performance does not depend on the dimensionality of the embedding space (which can indeed be infinite), but only on the intrinsic dimension of the space on which the data-generating distribution is defined. Key to these results is an approximate construction in which the voronoi regions on the manifold are guaranteed to have a flight diameter in the boundary between k and infinity. Under our construction, a hyper surface is efficiently approximated by tracking the variation of their tangible spaces through the use of the second basic form. Where this form disappears, the voronoi regions of an approximation are not maintained with a flight diameter of k into infinity unless some care is taken into account in the analyses. An important point of interest is that the approximations are controlled by averaged quantities, such as the total root curvature (k planes for surfaces one, which are intuitive)."}, {"heading": "A Methodology and Derivation of Results", "text": "Although both k averages and k flats optimize the same empirical risk, the measure of performance we are interested in is that of Equation 1. We can include it from above as follows: En (Sn, k) \u2264 | En (Sn, k) | + En (Sn, k) \u2212 En (S, k) + | En (S, k) \u00b7 E, k | + E samples, k (14) \u2264 2 \u00b7 supS-Sk | En (S) \u2212 En (S)."}, {"heading": "B K-Means", "text": "In order to derive explicit boundaries over the various error concepts, we need to combine in a novel way some earlier results and some new observations. However, the error E \u043d, k = infSk, E\u03c1 (Sk) is related to the problem of optimal quantization. The classic problem of optimal quantization is fairly well understood, going back to the fundamental work of [46, 44] on optimal quantization of data transmission, and more recently through the work of [24, 27, 26, 15]. In particular, it is known that it is for distributions with finite moment of order 2 + \u03bb, it is [24] lim, it is that it is E, k \u00b7 k2 / d = C {icher (x) pa (x) pa (x) d + 2) d / d (16), where the legue is part of the absolute unit of measurement."}, {"heading": "C K-Flats", "text": "We begin here with a series of gaps, which we will demonstrate in the next section."}], "references": [{"title": "Multiscale geometric methods for data sets ii: Geometric multi-resolution analysis", "author": ["William K Allard", "Guangliang Chen", "Mauro Maggioni"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Np\u2013hardness of euclidean sum-of-squares clustering", "author": ["Daniel Aloise", "Amit Deshpande", "Pierre Hansen", "Preyas Popat"], "venue": "Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "k\u2013means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Voronoi diagrams: A survey of a fundamental geometric data structure", "author": ["Franz Aurenhammer"], "venue": "ACM Comput. Surv.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "The minimax distortion redundancy in empirical quantizer design", "author": ["Peter L. Bartlett", "Tamas Linder", "Gabor Lugosi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., 15(6):1373\u20131396", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering", "author": ["Shai Ben-David"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "On the performance of clustering in hilbert spaces", "author": ["G\u00e9rard Biau", "Luc Devroye", "G\u00e1bor Lugosi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Statistical properties of kernel principal component analysis", "author": ["Gilles Blanchard", "Olivier Bousquet", "Laurent Zwald"], "venue": "Mach. Learn.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "k-plane clustering", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "J. of Global Optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Empirical risk approximation: An induction principle for unsupervised learning", "author": ["Joachim M. Buhmann"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Information theoretic model validation for clustering", "author": ["Joachim M. Buhmann"], "venue": "In International Symposium on Information Theory, Austin Texas", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On the optimization of weighted cubature formulae on certain classes of continuous functions", "author": ["E V Chernaya"], "venue": "East J. Approx", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Building triangulations using -nets", "author": ["Kenneth L. Clarkson"], "venue": "In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Set estimation", "author": ["A. Cuevas", "R. Fraiman"], "venue": "New perspectives in stochastic geometry, pages 374\u2013397. Oxford Univ. Press, Oxford", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Set estimation: an overview and some recent developments", "author": ["A. Cuevas", "A. Rod\u0155\u0131guez-Casal"], "venue": "Recent advances and trends in nonparametric statistics, pages 251\u2013264. Elsevier B. V., Amsterdam", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Random projection trees for vector quantization", "author": ["Sanjoy Dasgupta", "Yoav Freund"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Brian Kulis"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Foundations of Modern Analysis", "author": ["J. Dieudonne"], "venue": "Pure and Applied Mathematics. Hesperides Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Riemannian geometry", "author": ["M.P. DoCarmo"], "venue": "Theory and Applications Series. Birkh\u00e4user", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Vector quantization and signal compression", "author": ["Allen Gersho", "Robert M. Gray"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1991}, {"title": "Foundations of quantization for probability distributions", "author": ["Siegfried Graf", "Harald Luschgy"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "Asymptotic estimates for best and stepwise approximation of convex bodies i", "author": ["P.M. Gruber"], "venue": "Forum Mathematicum, 15:281\u2013297", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Optimum quantization and its applications", "author": ["Peter M. Gruber"], "venue": "Adv. Math,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Convex and discrete geometry", "author": ["P.M. Gruber"], "venue": "Grundlehren der mathematischen Wissenschaften. Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Intrinsic dimensionality estimation of submanifolds in rd", "author": ["Matthias Hein", "Jean-Yves Audibert"], "venue": "Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["V. De Silva J.B. Tenenbaum", "J.C. Langford"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Spectral surface reconstruction from noisy point clouds", "author": ["Ravikrishna Kolluri", "Jonathan Richard Shewchuk", "James F. O\u2019Brien"], "venue": "In Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "The Concentration of Measure Phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs. American Mathematical Society", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Mesh-independent surface interpolation", "author": ["David Levin"], "venue": "Geometric Modeling for Scientific Visualization,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Least squares quantization in pcm", "author": ["Stuart P. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "L. M. Le Cam and J. Neyman, editors, Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281\u2013297. University of California Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1967}, {"title": "The planar k\u2013means problem is np-hard", "author": ["Meena Mahajan", "Prajakta Nimbhorkar", "Kasturi Varadarajan"], "venue": "In Proceedings of the 3rd International Workshop on Algorithms and Computation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Online dictionary learning for sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "K\u2013dimensional coding schemes in hilbert spaces", "author": ["A. Maurer", "M. Pontil"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Sample complexity of testing the manifold hypothesis", "author": ["Hariharan Narayanan", "Sanjoy Mitter"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Strong consistency of k-means clustering", "author": ["David Pollard"], "venue": "Annals of Statistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1981}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["ST Roweis", "LK Saul"], "venue": "Science, 290:2323\u2013 2326", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2000}, {"title": "The one-sided barrier problem for Gaussian noise", "author": ["David Slepian"], "venue": "Bell System Tech. J.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1962}, {"title": "Nonparametric regression between general Riemannian manifolds", "author": ["Florian Steinke", "Matthias Hein", "Bernhard Sch\u00f6lkopf"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Information Science and Statistics. Springer, New York", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Sur la representation d \u0301une population in par une nombre d \u0301elements", "author": ["G Fejes Toth"], "venue": "Acta Math. Acad. Sci. Hungaricae", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1959}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike von Luxburg"], "venue": "Stat. Comput.,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}], "referenceMentions": [{"referenceID": 28, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 14, "endOffset": 25}, {"referenceID": 39, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 14, "endOffset": 25}, {"referenceID": 6, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 14, "endOffset": 25}, {"referenceID": 41, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 180, "endOffset": 183}, {"referenceID": 44, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 233, "endOffset": 236}, {"referenceID": 31, "context": "R), and the data are typically not sampled probabilistically, see for instance [32, 30].", "startOffset": 79, "endOffset": 87}, {"referenceID": 29, "context": "R), and the data are typically not sampled probabilistically, see for instance [32, 30].", "startOffset": 79, "endOffset": 87}, {"referenceID": 16, "context": "The problem of learning a manifold is also related to that of estimating the support of a distribution, (see [17, 18] for recent surveys.", "startOffset": 109, "endOffset": 117}, {"referenceID": 17, "context": "The problem of learning a manifold is also related to that of estimating the support of a distribution, (see [17, 18] for recent surveys.", "startOffset": 109, "endOffset": 117}, {"referenceID": 0, "context": "The reconstruction framework that we consider is related to the work of [1, 38], as well as to the framework proposed in [37], in which a manifold is approximated by a set, with performance measured by an expected distance to this set.", "startOffset": 72, "endOffset": 79}, {"referenceID": 37, "context": "The reconstruction framework that we consider is related to the work of [1, 38], as well as to the framework proposed in [37], in which a manifold is approximated by a set, with performance measured by an expected distance to this set.", "startOffset": 72, "endOffset": 79}, {"referenceID": 36, "context": "The reconstruction framework that we consider is related to the work of [1, 38], as well as to the framework proposed in [37], in which a manifold is approximated by a set, with performance measured by an expected distance to this set.", "startOffset": 121, "endOffset": 125}, {"referenceID": 35, "context": "This setting is similar to the problem of dictionary learning (see for instance [36], and extensive references therein), in which a dictionary is found by minimizing a similar reconstruction error, perhaps with additional constraints on an associated encoding of the data.", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "This is the same reconstruction measure that has been the recent focus of [37, 5, 38].", "startOffset": 74, "endOffset": 85}, {"referenceID": 4, "context": "This is the same reconstruction measure that has been the recent focus of [37, 5, 38].", "startOffset": 74, "endOffset": 85}, {"referenceID": 37, "context": "This is the same reconstruction measure that has been the recent focus of [37, 5, 38].", "startOffset": 74, "endOffset": 85}, {"referenceID": 33, "context": "1 Using K-Means and K-Flats for Piecewise Manifold Approximation In this work, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12].", "startOffset": 131, "endOffset": 139}, {"referenceID": 32, "context": "1 Using K-Means and K-Flats for Piecewise Manifold Approximation In this work, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12].", "startOffset": 131, "endOffset": 139}, {"referenceID": 11, "context": "1 Using K-Means and K-Flats for Piecewise Manifold Approximation In this work, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "The study of manifolds embedded in a Hilbert space is of special interest when considering non-linear (kernel) versions of the algorithms [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 8, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 36, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 9, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 36, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 3, "context": ",mk}, which induces a Dirichlet-Voronoi tiling of X : a collection of k regions, each closest to a common mean [4] (in our notation, the subscript n denotes the dependence of Sn,k on the sample, while k refers to its size.", "startOffset": 111, "endOffset": 114}, {"referenceID": 32, "context": "These two facts imply that it is possible to compute a local minimum of the empirical risk by using a greedy coordinate-descent relaxation, namely Lloyd\u2019s algorithm [33].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "Even though Lloyd\u2019s algorithm provides no guarantees of closeness to the global minimizer, in practice it is possible to use a randomized approximation algorithm, such as kmeans++ [3], which provides guarantees of approximation to the global minimum in expectation with respect to the randomization.", "startOffset": 180, "endOffset": 183}, {"referenceID": 13, "context": ") For instance, this point of view is considered in [14] where k-means is studied from an information theoretic persepective.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "K-means can also be interpreted to be performing vector quantization, where the goal is to minimize the encoding error associated to a nearest-neighbor quantizer [23].", "startOffset": 162, "endOffset": 166}, {"referenceID": 38, "context": "Interestingly, in the limit of increasing sample size, this problem coincides, in a precise sense [39], with the problem of optimal quantization of probability distributions (see for instance the excellent monograph of [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "Interestingly, in the limit of increasing sample size, this problem coincides, in a precise sense [39], with the problem of optimal quantization of probability distributions (see for instance the excellent monograph of [24].", "startOffset": 219, "endOffset": 223}, {"referenceID": 25, "context": ") As in the Euclidean setting, the limit of this problem with increasing sample size is precisely the problem of optimal quantization of distributions on manifolds, which is the subject of significant recent work in the field of optimal quantization [26, 27].", "startOffset": 250, "endOffset": 258}, {"referenceID": 26, "context": ") As in the Euclidean setting, the limit of this problem with increasing sample size is precisely the problem of optimal quantization of distributions on manifolds, which is the subject of significant recent work in the field of optimal quantization [26, 27].", "startOffset": 250, "endOffset": 258}, {"referenceID": 35, "context": "In this interpretation, the set of means can be seen as a dictionary of size k that produces a maximally sparse representation (the k-means encoding), see for example [36] and references therein.", "startOffset": 167, "endOffset": 171}, {"referenceID": 27, "context": "For example [28] report an average intrinsic dimension d for each digit to be between 10 and 13.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "For example in the case X = R, and under fairly general technical assumptions, it is possible to show that E\u03c1(Sk) = \u0398(k\u22122/d), where the constants depend on \u03c1 and d [24].", "startOffset": 164, "endOffset": 168}, {"referenceID": 36, "context": "In particular, this quantity has been studied for X = R, and shown to be, with high probability, of order \u221a kd/n, up-to logarithmic factors [37].", "startOffset": 140, "endOffset": 144}, {"referenceID": 36, "context": "The case where X is a Hilbert space has been considered in [37, 10], where an upper-bound of order k/ \u221a n is proven to hold with high probability.", "startOffset": 59, "endOffset": 67}, {"referenceID": 9, "context": "The case where X is a Hilbert space has been considered in [37, 10], where an upper-bound of order k/ \u221a n is proven to hold with high probability.", "startOffset": 59, "endOffset": 67}, {"referenceID": 8, "context": "The more general setting where X is a metric space has been studied in [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "The existence of such a tradeoff between the approximation, and the statistical errors may itself not be entirely obvious, see the discussion in [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 23, "context": "While the bound on E\u03c1(Sk) is known to be tight for k sufficiently large [24], the remaining terms (which are dominated by |E\u03c1(Sn,k)\u2212En(Sn,k)|) are derived by controlling the supremum of an empirical process sup S\u2208Sk |En(S)\u2212 E\u03c1(S)| (4)", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "and it is unknown whether available bounds for it are tight [37].", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "Indeed, it is not clear how close the distortion redundancy E\u03c1(Sn,k)\u2212 E\u03c1(Sk) is to its known lower bound of order d \u221a k1\u2212 4 d n (in expectation) [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "Indeed, as pointed out in [5], \u201cThe exact dependence of the minimax distortion redundancy on k and d is still a challenging open problem\u201d.", "startOffset": 26, "endOffset": 29}, {"referenceID": 30, "context": "Because d n, with high probability, the samples are nearly orthogonal: < x1, x2 >X' 0, while a third sample x drawn uniformly on S will also very likely be nearly orthogonal to both x1, x2 [31].", "startOffset": 189, "endOffset": 193}, {"referenceID": 2, "context": "We analyze the case in which a solution is obtained from an approximation algorithm, such as k-means++ [3], to include this computational error in the bounds.", "startOffset": 103, "endOffset": 106}, {"referenceID": 42, "context": "In fact, following the ideas in [43] Section 6.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "Assume the manifoldM to have metric of class C, and finite second fundamental form II [22].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 133, "endOffset": 141}, {"referenceID": 15, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 133, "endOffset": 141}, {"referenceID": 0, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 271, "endOffset": 278}, {"referenceID": 37, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 271, "endOffset": 278}, {"referenceID": 0, "context": "[1] William K Allard, Guangliang Chen, and Mauro Maggioni.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] David Arthur and Sergei Vassilvitskii.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Franz Aurenhammer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Peter L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Shai Ben-David.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] G\u00e9rard Biau, Luc Devroye, and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Gilles Blanchard, Olivier Bousquet, and Laurent Zwald.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Joachim M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Joachim M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] E V Chernaya.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Kenneth L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Sanjoy Dasgupta and Yoav Freund.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Inderjit S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Allen Gersho and Robert M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Siegfried Graf and Harald Luschgy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Peter M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Matthias Hein and Jean-Yves Audibert.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Ravikrishna Kolluri, Jonathan Richard Shewchuk, and James F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] David Levin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Stuart P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] Hariharan Narayanan and Sanjoy Mitter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] David Pollard.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] ST Roweis and LK Saul.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] David Slepian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] Florian Steinke, Matthias Hein, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] G Fejes Toth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] Ulrike von Luxburg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 107, "endOffset": 115}, {"referenceID": 23, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 26, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 25, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 14, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 23, "context": "In particular, it is known that, for distributions with finite moment of order 2 +\u03bb, for some \u03bb > 0, it is [24]", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "We note that, by setting \u03bc to be the uniform distribution over the unit cube [0, 1], it clearly is", "startOffset": 77, "endOffset": 83}, {"referenceID": 26, "context": "and thus, by making use of Zador\u2019s asymptotic formula [46], and combining it with a result of B\u00f6r\u00f6czky (see [27], p.", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "The approximation error E\u2217 \u03c1,k = infSk\u2208Sk E\u03c1(Sk) of k-means is related to the problem of optimal quantization on manifolds, for which some results are known [26].", "startOffset": 157, "endOffset": 161}, {"referenceID": 36, "context": "The statistical error of Equation 14, which uniformly bounds the difference between the empirical, and expected error, has been widely-studied in recent years in the literature [37, 38, 5].", "startOffset": 177, "endOffset": 188}, {"referenceID": 37, "context": "The statistical error of Equation 14, which uniformly bounds the difference between the empirical, and expected error, has been widely-studied in recent years in the literature [37, 38, 5].", "startOffset": 177, "endOffset": 188}, {"referenceID": 4, "context": "The statistical error of Equation 14, which uniformly bounds the difference between the empirical, and expected error, has been widely-studied in recent years in the literature [37, 38, 5].", "startOffset": 177, "endOffset": 188}, {"referenceID": 36, "context": "with probability 1 \u2212 \u03b4 [37].", "startOffset": 23, "endOffset": 27}, {"referenceID": 38, "context": "Clearly, this implies convergence En(S) \u2192 E\u03c1(S) almost surely, as n \u2192 \u221e; although this latter result was proven earlier in [39], under the less restrictive condition that p have finite second moment.", "startOffset": 123, "endOffset": 127}, {"referenceID": 1, "context": "In practice, the k-means problem is NP-hard [2, 19, 35], with the original Lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of Equation 2.", "startOffset": 44, "endOffset": 55}, {"referenceID": 18, "context": "In practice, the k-means problem is NP-hard [2, 19, 35], with the original Lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of Equation 2.", "startOffset": 44, "endOffset": 55}, {"referenceID": 34, "context": "In practice, the k-means problem is NP-hard [2, 19, 35], with the original Lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of Equation 2.", "startOffset": 44, "endOffset": 55}, {"referenceID": 2, "context": "However, practical approximations, such as the k-means++ algorithm [3], exist.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "This randomized seeding has been shown by [3] to output a set that is, in expectation, within a 8 (ln k + 2)-factor of the optimal.", "startOffset": 42, "endOffset": 45}, {"referenceID": 10, "context": "Noticing that d X (x, \u03c0x) = \u2016x\u2016 \u2212 \u2016\u03c0x\u2016 = \u2016x\u2016 \u2212 \u3008xx, \u03c0\u3009 F for any orthogonal projection \u03c0 (see for instance [11], Sec.", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "where the first inequality follows from Equation 23 and Slepian\u2019s Lemma [41], and the second from Equation 24.", "startOffset": 72, "endOffset": 76}, {"referenceID": 5, "context": "Finally, by Theorem 8 of [6], it is:", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "At every point x \u2208 M, define the metric Qx := |IIx|+ \u03b1(x)Ix, where a) I and II are, respectively, the first and second fundamental forms on M [22].", "startOffset": 142, "endOffset": 146}, {"referenceID": 25, "context": "The following theorem, adapted from [26], characterizes the relation between k and the quantization error fQ,p(Pk) on a Riemannian manifold.", "startOffset": 36, "endOffset": 40}, {"referenceID": 25, "context": "[[26]] Given a smooth compact Riemannian d-manifold M with metric Q of class C, and a continuous function w :M\u2192 R, then", "startOffset": 1, "endOffset": 5}, {"referenceID": 15, "context": "1 in [16], and is borrowed from [26, 25].", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "1 in [16], and is borrowed from [26, 25].", "startOffset": 32, "endOffset": 40}, {"referenceID": 24, "context": "1 in [16], and is borrowed from [26, 25].", "startOffset": 32, "endOffset": 40}, {"referenceID": 25, "context": "[[26, 25], [16]] Given M as above, and \u03bb > 0 then, for every p \u2208 M there is an open neighborhood V\u03bb(p) 3 p in M such that, for all x, y \u2208 V\u03bb(p), it is d X (x, TyM) \u2264 (1 + \u03bb)d|II|(x, y) (30)", "startOffset": 1, "endOffset": 9}, {"referenceID": 24, "context": "[[26, 25], [16]] Given M as above, and \u03bb > 0 then, for every p \u2208 M there is an open neighborhood V\u03bb(p) 3 p in M such that, for all x, y \u2208 V\u03bb(p), it is d X (x, TyM) \u2264 (1 + \u03bb)d|II|(x, y) (30)", "startOffset": 1, "endOffset": 9}, {"referenceID": 15, "context": "[[26, 25], [16]] Given M as above, and \u03bb > 0 then, for every p \u2208 M there is an open neighborhood V\u03bb(p) 3 p in M such that, for all x, y \u2208 V\u03bb(p), it is d X (x, TyM) \u2264 (1 + \u03bb)d|II|(x, y) (30)", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "We may, however, use Weierstrass\u2019 approximation theorem (see for example [21] p.", "startOffset": 73, "endOffset": 77}], "year": 2013, "abstractText": "We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by k-means and k-flats, and analyze their performance. We extend previous results for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-flats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-flats, both the results and the mathematical tools are new.", "creator": "LaTeX with hyperref package"}}}