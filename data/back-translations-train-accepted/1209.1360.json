{"id": "1209.1360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2012", "title": "Multiclass Learning with Simplex Coding", "abstract": "In this paper we discuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework, a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized method with training/tuning complexity which is independent to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper.", "histories": [["v1", "Thu, 6 Sep 2012 18:22:25 GMT  (238kb,D)", "https://arxiv.org/abs/1209.1360v1", null], ["v2", "Fri, 14 Sep 2012 14:14:53 GMT  (170kb,D)", "http://arxiv.org/abs/1209.1360v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["youssef mroueh", "tomaso a poggio", "lorenzo rosasco", "jean-jacques e slotine"], "accepted": true, "id": "1209.1360"}, "pdf": {"name": "1209.1360.pdf", "metadata": {"source": "CRF", "title": "Multiclass Learning with Simplex Coding", "authors": ["Youssef Mroueh", "Tomaso Poggio", "Lorenzo Rosasco", "Jean-Jacques E. Slotine"], "emails": ["lrosasco@mit.edu", "jjs@mit.edu", "tp@ai.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to achieve our goals, and that we are able to achieve our goals."}, {"heading": "2 Problem Statement and Previous Work", "text": "Leave (X, Y) two random variables with values in two measurable spaces X = \u2192 X = {1. T}, T \u2265 2. Indicate by \u03c1X, the law from X to X, and by \u03c1j (x), the conditional probabilities for j-Y. The data is a sample S = (xi, yi) ni = 1, of n identical and independent copies of (X, Y).We can imagine X as a set of possible inputs and Y as a set of labels describing a set of semantic categories / classes to which the input may belong. A classification rule is a map b: X \u2192 Y, and its error is caused by the miscassification risk R (b) = P (X) 6 = Y) = E (1I [b) 6 = y] (X, Y)).The optimal classification rule that R minimizes is the maxy rule (x) = x."}, {"heading": "2.1 Relaxation Error Analysis", "text": "Since we replace the misclassification loss with a convex substitute function (this property is almost certainly related to the concept of calibration), we are effectively changing the problem: the misclassification risk is replaced by the expected loss, E (f) = E (p) = E (- Y). The expected loss can act as a functional loss on a wide range of functions F = FV, \u03c1, which depends on V and \u03c1. Its minimizer, which is denoted by f\u03c1, replaces the Bayes rule as the target of our algorithm. The question arises as to the price we pay taking into account a relaxation approach: \"What is the relationship between FIS and BIS?\" More generally speaking: \"What is the approximation we accept by estimating the expected risk instead of the misclassification risk?\" The relativization error for a given loss function can be quantified by the following tweeted equipments: 1) Fisher consistency is a loss function = Fisher."}, {"heading": "3 A Relaxation Approach to Multicategory Classification", "text": "In this section, we propose a natural extension of the relaxation approach that avoids the restriction of the class of functions to be taken into account and allows explicit comparative inequalities to be derived. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4 Relaxation Error Analysis", "text": "If we look at the simplex encoding, a function f that takes values in RT \u2212 1 and the decoding operator D = q, the misclassification risk can also be described as follows: R (F) = 0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0"}, {"heading": "5 Computational Aspects and Regularization Algorithms", "text": "In this section we will discuss some computational implications of the framework we have presented. Regularized kernel methods (> x). We will consider regulated methods of form (1), induced by simplex loss functions and where the hypotheses space is a vector that reproduces spaces (VV-RKHSs) and the regulator of the corresponding standard. See Appendix D.2 for a brief introduction to VV-RKHSs. Below we will consider a class of nuclei such that the corresponding RKHS H is given by the completion of the chip {f)."}, {"heading": "5.1 Comparison of Computational Complexity", "text": "The cost of solving S-RLS for fixed \u03bb is in the worst case O (n3) (for example, by choleski decomposition). If we are interested in calculating the regularization path for N regularization parameter values, then, as mentioned in [15], it might be convenient to perform a self-decomposition of the kernel matrix instead of solving the systems N times. For explicit feature maps, the cost is O (np2), so the cost of calculating the regularization path for simple RLS algorithms O (min (n3, np2) and thus independent of T. One can compare this complexity with that of a na ive One Versus all (OVa) approach, which would result in an O (Nn3T) complexity. Simple SVMs can be solved with solutions available for binary SVMs that support the strict SVM, as opposed to the strict SVM (S) complexity classes (S) and SVM (S) that are actually SVO (nx3) complexity classes."}, {"heading": "6 Numerical Results", "text": "We perform several experiments to evaluate the performance of our batch and online algorithms using 5 UCI datasets listed in Table 6, as well as Caltech101 and Pubfig83. We compare the performance of our algorithms with on compared to all svm (libsvm), as well as with the simplex-based increase [16]. For UCI datasets we use the raw properties, for Caltech101 we use hierarchical characteristics (loo). For the Selection of 1 in S-LS, 100 values are selected in the range [\u03bbmin, \u03bbmax] (the parameter selection is based on either a hold out (ho) (80% training \u2212 20% validation) or an outlet error (loo). For the Selection of \u03bb in S-LS, 100 values are selected in the range [\u03bbmin, \u03bbmax] (with the smallest and largest eigenvalues of K corresponding between the two methods)."}], "references": [{"title": "Reducing multiclass to binary: a unifying approach for margin classifiers", "author": ["Erin L. Allwein", "Robert E. Schapire", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Optimal rates for regularized least-squares algorithm", "author": ["A. Caponnetto", "E. De Vito"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Consistency of multiclass empirical risk minimization methods based in convex loss", "author": ["D. Chen", "T. Sun"], "venue": "Journal of machine learning, X,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["Thomas G. Dietterich", "Ghulum Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Vc theory of large margin multi-category classiers", "author": ["Yann Guermeur"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "A framework for kernel-based multi-category classification", "author": ["Simon I. Hill", "Arnaud Doucet"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "A correspondence between bayesian estimation of stochastic processes and smoothing by splines", "author": ["G. Kimeldorf", "G. Wahba"], "venue": "Ann. Math. Stat., 41:495\u2013502,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1970}, {"title": "Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data", "author": ["Lee.Y", "L.Yin", "Wahba.G"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "On learning vector\u2013valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation, 17:177\u2013204,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Scaling-up biologically-inspired computer vision: A case-study on facebook", "author": ["N. Pinto", "Z. Stone", "T. Zickler", "D.D. Cox"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Composite binary losses", "author": ["M.D. Reid", "R.C. Williamson"], "venue": "JMLR, 11, September", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiclass boosting: Theory and algorithms", "author": ["Saberian.M", "Vasconcelos .N"], "venue": "In NIPS 2011,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In Proceedings of the 24th ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Information Science and Statistics. Springer, New York,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "A moment bound for multicategory support vector machines", "author": ["Van de Geer.S Tarigan.B"], "venue": "JMLR 9,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "On the consistency of multiclass classification methods", "author": ["A. Tewari", "P.L. Bartlett"], "venue": "Proceedings of the 18th Annual Conference on Learning Theory, volume 3559, pages 143\u2013157. Springer,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, 6(2):1453\u20131484,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["Alexandre B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Composite multiclass losses", "author": ["Elodie Vernet", "Robert C. Williamson", "Mark D. Reid"], "venue": "In Proceedings of Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Spline models for observational data, volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics", "author": ["G. Wahba"], "venue": "SIAM, Philadelphia, PA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Support vector machine for multi class pattern recognition", "author": ["Weston", "Watkins"], "venue": "Proceedings of the seventh european symposium on artificial neural networks,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Multicategory vertex discriminant analysis for high-dimensional data", "author": ["Tong Tong Wu", "Kenneth Lange"], "venue": "Ann. Appl. Stat.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "On early stopping in gradient descent learning", "author": ["Y. Yao", "L. Rosasco", "A. Caponnetto"], "venue": "Constructive Approximation, 26(2):289\u2013315,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical analysis of some multi-category large margin classification methods", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research, 5:1225\u20131251,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Tong Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 23, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 11, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 25, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 248, "endOffset": 263}, {"referenceID": 14, "context": "In this context, results in statistical learning theory quantify the error incurred by relaxation and in particular derive comparison inequalities explicitly relating the excess misclassification risk with the excess expected loss, see for example [2, 27, 14, 29] and [18] Chapter 3 for an exhaustive presentation as well as generalizations.", "startOffset": 268, "endOffset": 272}, {"referenceID": 8, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 4, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 21, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 0, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 17, "context": "Over the years, several computational solutions have been proposed (among others, see [10, 6, 5, 25, 1, 21].", "startOffset": 86, "endOffset": 107}, {"referenceID": 24, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 45, "endOffset": 53}, {"referenceID": 16, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 45, "endOffset": 53}, {"referenceID": 5, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 64, "endOffset": 75}, {"referenceID": 19, "context": "Results in this sense have been pioneered by [28, 20], see also [11, 7, 23].", "startOffset": 64, "endOffset": 75}, {"referenceID": 3, "context": "More quantitative results in terms of comparison inequalities are given in [4] under similar restrictions (see also [19]).", "startOffset": 75, "endOffset": 78}, {"referenceID": 15, "context": "More quantitative results in terms of comparison inequalities are given in [4] under similar restrictions (see also [19]).", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "A loss function is Fisher consistent if sign(f\u03c1(x)) = b\u03c1(x) almost surely (this property is related to the notion of classification-calibration [2]).", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "The relaxation error in the binary case has been thoroughly studied in [2, 14].", "startOffset": 71, "endOffset": 78}, {"referenceID": 11, "context": "The relaxation error in the binary case has been thoroughly studied in [2, 14].", "startOffset": 71, "endOffset": 78}, {"referenceID": 1, "context": "In particular, Theorem 2 in [2] shows that if a large margin surrogate loss is convex, differentiable and decreasing in a neighborhood of 0, then the loss is Fisher consistent.", "startOffset": 28, "endOffset": 31}, {"referenceID": 18, "context": "The comparison inequality for the square loss can be improved for a suitable class of probability distribution satisfying the so called Tsybakov noise condition [22], \u03c1X ({x \u2208 X , |f\u03c1(x)| \u2264 s}) \u2264 Bqs, s \u2208 [0, 1], q > 0.", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "The comparison inequality for the square loss can be improved for a suitable class of probability distribution satisfying the so called Tsybakov noise condition [22], \u03c1X ({x \u2208 X , |f\u03c1(x)| \u2264 s}) \u2264 Bqs, s \u2208 [0, 1], q > 0.", "startOffset": 205, "endOffset": 211}, {"referenceID": 1, "context": "In this case the comparison inequality for the square loss is given by \u03c8(t) = cqt q+1 q+2 , see [2, 27].", "startOffset": 96, "endOffset": 103}, {"referenceID": 23, "context": "In this case the comparison inequality for the square loss is given by \u03c8(t) = cqt q+1 q+2 , see [2, 27].", "startOffset": 96, "endOffset": 103}, {"referenceID": 8, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 21, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 0, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 17, "context": "Among others, we mention for example [10, 6, 5, 25, 1, 21].", "startOffset": 37, "endOffset": 58}, {"referenceID": 16, "context": "As we previously mentioned from a theoretical perspective a general account of a large class of multiclass methods has been given in [20], building on results in [2] and [28].", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "As we previously mentioned from a theoretical perspective a general account of a large class of multiclass methods has been given in [20], building on results in [2] and [28].", "startOffset": 162, "endOffset": 165}, {"referenceID": 24, "context": "As we previously mentioned from a theoretical perspective a general account of a large class of multiclass methods has been given in [20], building on results in [2] and [28].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "These results, see also [11, 23], are developed in a setting where a classification rule is found by applying a suitable prediction/decoding map to a function f : X \u2192 R where f is found considering a loss function V : Y \u00d7 R \u2192 R.", "startOffset": 24, "endOffset": 32}, {"referenceID": 3, "context": "More quantitative results in terms of explicit comparison inequality are given in [4] and (see also [19]), but also need to to impose the \u201dsum to zero\u201d constraint on the considered function class.", "startOffset": 82, "endOffset": 85}, {"referenceID": 15, "context": "More quantitative results in terms of explicit comparison inequality are given in [4] and (see also [19]), but also need to to impose the \u201dsum to zero\u201d constraint on the considered function class.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "The latter loss function is related to the one considered in the multiclass SVM proposed in [10].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "The simplex coding has been considered in [8],[26], and [16].", "startOffset": 42, "endOffset": 45}, {"referenceID": 22, "context": "The simplex coding has been considered in [8],[26], and [16].", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "The simplex coding has been considered in [8],[26], and [16].", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "In particular, a kind of SVM loss is considered in [8] where V (y, f(x)) = \u2211 y\u2032 6=y |\u03b5\u2212 \u3008f(x), vy\u2032(y)\u3009|+ and vy\u2032(y) = cy\u2212cy\u2032 \u2016cy\u2212cy\u2032\u2016 , with \u03b5 = \u3008cy, vy \u2032(y)\u3009 =", "startOffset": 51, "endOffset": 54}, {"referenceID": 22, "context": "More recently [26] considered the loss function V (y, f(x)) = |\u03b5\u2212 \u2016cy \u2212 f(x)\u2016|+, and a simplex multi-class boosting loss was introduced in [16], in our notation V (y, f(x)) = \u2211 j 6=y e \u2212\u3008cy\u2212cy\u2032 ,f(x)\u3009.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "More recently [26] considered the loss function V (y, f(x)) = |\u03b5\u2212 \u2016cy \u2212 f(x)\u2016|+, and a simplex multi-class boosting loss was introduced in [16], in our notation V (y, f(x)) = \u2211 j 6=y e \u2212\u3008cy\u2212cy\u2032 ,f(x)\u3009.", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "While all those losses introduce a certain notion of margin that makes use of the geometry of the simplex coding, it is not to clear how to derive explicit comparison theorems and moreover the computational complexity of the resulting algorithms scales linearly with the number of classes in the case of the losses considered in [16, 26] and O((nT )), \u03b3 \u2208 {2, 3} for losses considered in [8] .", "startOffset": 329, "endOffset": 337}, {"referenceID": 22, "context": "While all those losses introduce a certain notion of margin that makes use of the geometry of the simplex coding, it is not to clear how to derive explicit comparison theorems and moreover the computational complexity of the resulting algorithms scales linearly with the number of classes in the case of the losses considered in [16, 26] and O((nT )), \u03b3 \u2208 {2, 3} for losses considered in [8] .", "startOffset": 329, "endOffset": 337}, {"referenceID": 6, "context": "While all those losses introduce a certain notion of margin that makes use of the geometry of the simplex coding, it is not to clear how to derive explicit comparison theorems and moreover the computational complexity of the resulting algorithms scales linearly with the number of classes in the case of the losses considered in [16, 26] and O((nT )), \u03b3 \u2208 {2, 3} for losses considered in [8] .", "startOffset": 388, "endOffset": 391}, {"referenceID": 0, "context": "where s \u2208 [0, 1].", "startOffset": 10, "endOffset": 16}, {"referenceID": 5, "context": "Standard techniques for deriving sample complexity bound in binary classification extended for multi-class SVM losses could be found in [7] and could be adapted to our setting.", "startOffset": 136, "endOffset": 139}, {"referenceID": 14, "context": "The obtained bound are not known to be tight, better bounds akin to those in [18], will be subject to future work.", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "It is known [12, 3] that the representer theorem [9] can be easily extended to a vector valued setting, so that that minimizer of a simplex version of Tikhonov regularization is given by f S (x) = \u2211n j=1 k(x, xj)aj , aj \u2208 RT\u22121, for all x \u2208 X , where the explicit expression of the coefficients depends on the considered loss function.", "startOffset": 12, "endOffset": 19}, {"referenceID": 2, "context": "It is known [12, 3] that the representer theorem [9] can be easily extended to a vector valued setting, so that that minimizer of a simplex version of Tikhonov regularization is given by f S (x) = \u2211n j=1 k(x, xj)aj , aj \u2208 RT\u22121, for all x \u2208 X , where the explicit expression of the coefficients depends on the considered loss function.", "startOffset": 12, "endOffset": 19}, {"referenceID": 7, "context": "It is known [12, 3] that the representer theorem [9] can be easily extended to a vector valued setting, so that that minimizer of a simplex version of Tikhonov regularization is given by f S (x) = \u2211n j=1 k(x, xj)aj , aj \u2208 RT\u22121, for all x \u2208 X , where the explicit expression of the coefficients depends on the considered loss function.", "startOffset": 49, "endOffset": 52}, {"referenceID": 20, "context": "Interestingly, the classical results from [24] can be extended to show that the value fSi(xi), obtained computing the solution fSi removing the i\u2212 th point from the training set (the leave one out solution), can be computed in closed", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Following [17] we can alternate a step of stochastic descent on a data point : Wtmp = (1 \u2212 \u03b7i\u03bb)Wi \u2212 \u03b7i\u2202(V (yi, fWi(xi))) and a projection on the Frobenius ball Wi = min(1, 1 \u221a \u03bb||Wtmp||F )Wtmp (See Algorithn C.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "We compare the performance of our algorithms to on versus all svm (libsvm) , as well as the simplex based boosting [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "For UCI datasets we use the raw features, on Caltech101 we use hierarchical features1 , and on Pubfig83 we use the feature maps from [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "60% Simplex boosting [16] 86.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "More generally, we see that rbf S- LS has the best performance among the simplex methods including the simplex boosting [16].", "startOffset": 120, "endOffset": 124}], "year": 2012, "abstractText": "In this paper we discuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework, a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized method with training/tuning complexity which is independent to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper.", "creator": "TeX"}}}