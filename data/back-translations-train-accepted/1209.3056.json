{"id": "1209.3056", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2012", "title": "Parametric Local Metric Learning for Nearest Neighbor Classification", "abstract": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this \"independence\" approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several large-scale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner.", "histories": [["v1", "Thu, 13 Sep 2012 22:47:07 GMT  (2191kb,D)", "http://arxiv.org/abs/1209.3056v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun wang 0017", "alexandros kalousis", "adam woznica"], "accepted": true, "id": "1209.3056"}, "pdf": {"name": "1209.3056.pdf", "metadata": {"source": "CRF", "title": "Parametric Local Metric Learning for Nearest Neighbor Classification", "authors": ["Jun Wang"], "emails": ["Jun.Wang@unige.ch", "Adam.Woznica@unige.ch", "Alexandros.Kalousis@hesge.ch"], "sections": [{"heading": null, "text": "Most previous work on local metric learning has learned a number of local, non-contiguous metrics. While this \"independence approach\" offers increased flexibility, its drawback is the considerable risk of overadjustment. We present a new parametric local metric learning method, in which we learn a smooth metric matrix function across the data diversity. Using an approximation error linked to the metric matrix function, we learn local metrics as linear combinations of base metrics defined at anchor points across different regions of the instance space. We limit the metric matrix function by imposing manifold regulation on the linear combinations, which allows the learned metric matrix function to vary smoothly along the geodesics of data diversity. Our metric learning method has excellent performance both in terms of predictive force and on the totality of multiple, large-scale SVM experiments."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Preliminaries", "text": "We use X to denote the n \u00b7 dmatrix of the learning instances, whose i-th row is the xTi-Rd instance, and y = (y1,.., yn) T, yi-xj,..., c} is the vector of the class names. However, the square Mahalanobis distance between two instances in the entrance room is given by: d2M (xi, xj) TM (xi \u2212 xj), where M is a PSD metrix instance (M 0). A linear metric learning method learns a Mahalanobis metric M by optimizing a cost function among the PSD constraints for M and a series of additional constraints on the picturesque instance distances. Depending on the actual metric learning method, different types of constraints between the individual distances are used. The most successful are the large margin of the triplet metrix constraints, which we are projected by the problem of the xxi instance (the xxi), which we often do not have."}, {"heading": "3 Parametric Local Metric Learning", "text": "We assume that there is a Lipschitz metric function f (x), the result of which is the vectorized local metrix matrix of the instance x. (...) We assume that there is a Lipschitz metrix function f (x). (...) We assume that there is a Lipschitz metrix function. (...) We assume that there is a Lipschitz metrix function. (...) We assume that there is a real metrix function. (...) We assume that there is a Lipschitz metrix function. (...) We assume that there is a Lipschitz metrix function. (...) We assume that we have a real metrix function. (...) We assume that we have a real metrix function. (...) We assume that we have a real function. (...)"}, {"heading": "3.1 Smooth Local Linear Weighting", "text": "The first term states that we should be close to its linear approximation, and the second that the weighting should be local. In addition, we want to vary the local metrics that we are able to do. To achieve this, we rely on manifold regularization and limit the weight vectors of neighboring instances. To simplify the objective function, we use the term we are able to use the basic metrics by minimizing the margin of error of (1) together with a regularization term that controls the weight variation of similar instances. To simplify the objective function, we use the term we are able to use the concepts that we are capable of, the concepts that we are capable of, the concepts that we are capable of, the concepts of the concepts that we are capable of, the concepts that we are capable of, the concepts that we are capable of, the concepts that we are capable of, the concepts that concepts, the concepts that concepts, the concepts that concepts, the concepts that concepts, the concepts that concepts, the concepts that concepts, the concepts, the concepts that concepts, the concepts, the concepts that concepts, the concepts, the concepts, the concepts that concepts, the concepts, the concepts, the concepts, the concepts of the concepts, the concepts, the concepts, the concepts, the concepts, the concepts, the concepts, the concepts, the concepts of the"}, {"heading": "3.2 Large Margin Basis Metric Learning", "text": "In this section, we define a large margin based on algorithms to determine the base metrics Mb1,., Mbm. Given the W matrix of base metrics that we obtained using algorithm 1, the local metric Mi of an instance in (2) is linear in relation to the base metrics Mb1,., Mbm. We define the relative distance of instances xi, xj and xk as: d2Mi (xi, xj) \u2212 d 2 Mi (xi, xj) \u2212 d in relation to the base metrics c (xi, xj, xk), the relative distance d2Mi (xi, xk) is required to be greater than d2Mi (xi, xj) \u2212 d 2 Mi (xi, xj) otherwise an error is generated. Note: This relative definition differs from that defined in LMNN-MM [15]."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the performance of PLML and compare it with a number of relevant baseline methods on six datasets with a large number of instances ranging from 5K to 70K. These datasets are letters, USPS, pendigits, opdigrites, isolet and MNIST. We will determine whether adding multiple regulations to the local metrics improves the predictive performance of local metric learning processes and whether the number of local metric learning processes through learning is improved with single global metrics. We will compare PLML against six baseline methods. The first, SML, is a variant of PLML where a single global metric is learned, i.e. we will put the number of fundamentals in (6). The second, cluster-based LML (CBLML), is also a variant of PLML without weight learning."}, {"heading": "4.1 Results", "text": "In fact, most of us are able to put ourselves at the top in the way they do, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "5 Conclusions", "text": "In this paper, we introduced PLML, a local metric learning method that regulates local metrics so that they can vary smoothly across data diversity. Our method scales learning problems with tens of thousands of instances and avoids the overpass problems that plague other local metric learning methods. Experimental results show that PLML significantly exceeds the level of metric learning methods and performs significantly better or equivalent to SVM with automatic core selection."}, {"heading": "Acknowledgments", "text": "Support for the EU projects DebugIT (FP7-217139) and e-LICO (FP7-231519) and COST Action BM072 (\"Urine and Renal Proteomics\") is also appreciated."}], "references": [{"title": "Gradient-based algorithms with applications to signal-recovery problems", "author": ["A. Beck", "M. Teboulle"], "venue": "Convex Optimization in Signal Processing and Communications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Integrating constraints and metric learning in semisupervised clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "In ICML, page", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A metric learning perspective of svm: on the relation of svm and lmnn", "author": ["H. Do", "A. Kalousis", "J. Wang", "A. Woznica"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Image retrieval and classification using local distance functions", "author": ["A. Frome", "Y. Singer", "J. Malik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Discriminant adaptive nearest neighbor classification", "author": ["T. Hastie", "R. Tibshirani"], "venue": "IEEE Trans. on PAMI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Metric and kernel learning using a linear transformation", "author": ["P. Jain", "B. Kulis", "J.V. Davis", "I.S. Dhillon"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Generative local metric learning for nearest neighbor classification", "author": ["Y.K. Noh", "B.T. Zhang", "D.D. Lee"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A scalable dual approach to semidefinite metric learning", "author": ["C. Shen", "J. Kim", "L. Wang"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Positive semidefinite metric learning using boostinglike", "author": ["C. Shen", "J. Kim", "L. Wang", "A. Hengel"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Metric learning with multiple kernels", "author": ["J. Wang", "H. Do", "A. Woznica", "A. Kalousis"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Locally smooth metric learning with application to image retrieval", "author": ["D.Y. Yeung", "H. Chang"], "venue": "In ICCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Sparse metric learning via smooth optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Nonlinear learning using local coordinate coding", "author": ["K. Yu", "T. Zhang", "Y. Gong"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 13, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 7, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 8, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 15, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 12, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 6, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 1, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 13, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 5, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 6, "context": "One of the first local metric learning works, Discriminant Adaptive Nearest Neighbor classification [8], DANN, learns local metrics by shrinking neighborhoods in directions orthogonal to the local decision boundaries and enlarging the neighborhoods parallel to the boundaries.", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": "The authors of LMNN-Multiple Metric (LMNN-MM) [15] significantly limited the number of learned metrics and constrained all instances in a given region to share the same metric in an effort to combat overfitting.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "In the supervised setting they fixed the number of metrics to the number of classes; a similar idea has been also considered in [3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 14, "context": "The authors of [16] learn local metrics using a least-squares approach by minimizing a weighted sum of the distances of each instance to apriori defined target positions and constraining the instances in the projected space to preserve the original geometric structure of the data in an effort to alleviate overfitting.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "In another effort to overcome the overfitting problem of the discriminative methods [8, 15], Generative Local Metric Learning, GLML, [11], propose to learn local metrics by minimizing the NN expected classification error under strong model assumptions.", "startOffset": 84, "endOffset": 91}, {"referenceID": 13, "context": "In another effort to overcome the overfitting problem of the discriminative methods [8, 15], Generative Local Metric Learning, GLML, [11], propose to learn local metrics by minimizing the NN expected classification error under strong model assumptions.", "startOffset": 84, "endOffset": 91}, {"referenceID": 9, "context": "In another effort to overcome the overfitting problem of the discriminative methods [8, 15], Generative Local Metric Learning, GLML, [11], propose to learn local metrics by minimizing the NN expected classification error under strong model assumptions.", "startOffset": 133, "endOffset": 137}, {"referenceID": 0, "context": "To improve scalability and efficiency we employ a fast first-order optimization algorithm, FISTA [2], to learn the linear combinations as well as the basis metrics of the anchor points.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "In most cases we learn a local metric for each learning instance [8, 11], however we can also learn a local metric for some part of the instance space in which case the number of learned metrics can be considerably smaller than n, e.", "startOffset": 65, "endOffset": 72}, {"referenceID": 9, "context": "In most cases we learn a local metric for each learning instance [8, 11], however we can also learn a local metric for some part of the instance space in which case the number of learned metrics can be considerably smaller than n, e.", "startOffset": 65, "endOffset": 72}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Nevertheless, in accordance to the standard practice we will continue to use the term local metric learning following [15, 11].", "startOffset": 118, "endOffset": 126}, {"referenceID": 9, "context": "Nevertheless, in accordance to the standard practice we will continue to use the term local metric learning following [15, 11].", "startOffset": 118, "endOffset": 126}, {"referenceID": 16, "context": "[18] have shown that any Lipschitz smooth real function f(x) defined on a lower dimensional manifold can be approximated by a linear combination of function values f(u),u \u2208 U, of a set U of anchor points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "1 in [18]; for lack of space we omit its presentation.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "Most often the similarity matrix S is constructed using k-nearest neighbors graph [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "Such a projection operator can be efficiently implemented with a complexity of O(nm log(m)) [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "To speed up the optimization procedure we employ a fast first-order optimization method FISTA, [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "Note that, this relative comparison definition is different from that defined in LMNN-MM [15].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "First we exploit the connection between LMNN and SVM shown in [5] under which the squared Frobenius norm of the metric matrix is related to the SVM margin.", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "Second because adding this term leads to an easy-to-optimize dual formulation of (6) [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "Unlike many special solvers which optimize the primal form of the metric learning problem [15, 13], we follow [12] and optimize the Lagrangian dual problem of (6).", "startOffset": 90, "endOffset": 98}, {"referenceID": 11, "context": "Unlike many special solvers which optimize the primal form of the metric learning problem [15, 13], we follow [12] and optimize the Lagrangian dual problem of (6).", "startOffset": 90, "endOffset": 98}, {"referenceID": 10, "context": "Unlike many special solvers which optimize the primal form of the metric learning problem [15, 13], we follow [12] and optimize the Lagrangian dual problem of (6).", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Since metric learning is computationally expensive for datasets with large number of features we followed [15] and reduced the dimensionality of the USPS, Isolet and MINIST datasets by applying PCA.", "startOffset": 106, "endOffset": 110}, {"referenceID": 17, "context": "The Laplacian matrix L is constructed using the six nearest neighbors graph following [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "For LMNN and LMNN-MM we use their default settings, [15], in which the triplet constraints are constructed by the three nearest same-class neighbors and all different-class samples.", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "As in [11], GLML uses the Gaussian distribution to model the learning instances from the same class.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "The poor performance of LMNNMM is not in agreement with the results reported in [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "In [15], 30% of the training instance of each dataset were used as a validation set to avoid overfitting.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "As in [15], we use the two main principal components to learn.", "startOffset": 6, "endOffset": 10}], "year": 2012, "abstractText": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this \u201dindependence\u201d approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several largescale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner.", "creator": "LaTeX with hyperref package"}}}