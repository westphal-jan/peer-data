{"id": "1209.3129", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2012", "title": "Analog readout for optical reservoir computers", "abstract": "Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.", "histories": [["v1", "Fri, 14 Sep 2012 08:56:19 GMT  (107kb,D)", "http://arxiv.org/abs/1209.3129v1", "to appear in NIPS 2012"]], "COMMENTS": "to appear in NIPS 2012", "reviews": [], "SUBJECTS": "cs.ET cs.LG cs.NE physics.optics", "authors": ["anteo smerieri", "fran\u00e7ois duport", "yvan paquot", "benjamin schrauwen", "marc haelterman", "serge massar"], "accepted": true, "id": "1209.3129"}, "pdf": {"name": "1209.3129.pdf", "metadata": {"source": "CRF", "title": "Analog readout for optical reservoir computers", "authors": ["A. Smerieri", "F. Duport", "Y. Paquot", "B. Schrauwen", "M. Haelterman", "S. Massar"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2 Reservoir computing and time multiplexing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Principles of Reservoir Computing", "text": "The main component of a reservoir computer (RC) is a recursive network of nonlinear elements, usually referred to as \"nodes\" or \"neurons.\" Network output is typically done in discrete time, and the state of each node in each time step is a function of the input value in this time step and the states of neighboring nodes in the previous time step. Network output is generated by an read-out layer - a series of linear nodes providing a linear combination of instantaneous node states with fixed coefficients. The equation describing the development of the reservoir computer is isxi (n) = f (\u03b1miu (n) + \u03b2 N \u2211 j = 1 wijxj (n \u2212 1)) (1), where xi (n) is the state of the i-th node at discrete time, N is the total number of nodes, u (n) is the output set of dynamic, the mixing layer and the connecting layer in general is the dynamic (1), where the dynamic is (1)."}, {"heading": "2.2 Time multiplexing", "text": "The number of nodes in a reservoir computer determines an upper limit on the performance of the reservoir [17]; this may be an obstacle to the conception of physical implementations of RCs that should contain a high number of interconnected nonlinear units. A solution to this problem proposed in [7, 8] is time multiplexing: the xi (n) are computed one by one by a single nonlinear element that receives a combination of the input state u (n) and a previous state xj (n \u2212 1). Additionally, an input mask mi is applied to the input signal u (n) to enrich the dynamics of the reservoir. The value of xi (n) is then stored in a delay line that can be used at a later date n + 1. Interaction between different neurons can be ensured either by a slow nonlinear element that xi is connected to the previous states xi \u2212 1, xi \u2212 2, non-coupled [8] or by the use of an inline [8]."}, {"heading": "2.3 Hardware RC with digital readout", "text": "The hardware reservoir computer we use in the present paper is identical to the one given in [10] (see also [9]). It uses time multiplexing using the desynchronization technique described in the previous paragraph. We give a brief description of the experimental system shown in the left part of Figure 1. It uses a LiNbO3 Mach-Zehnder (MZ) modulator, which operates with a constant power of 1560 nm laser, as a non-linear component. A MZ modulator is a voltage-controlled optoelectronic device; the amount of light it transmits is a sine function of the voltage applied to it. The resulting state xi (n) is encoded in a light intensity level at the MZ output. It is then stored in a coil of the optical fiber acting as a delay line of duration T = 8.5\u00b5s, while all subsequent states are calculated by the modulxi (MZ)."}, {"heading": "3 Analog readout", "text": "The procedure described in Section 2 is to design a device that retrieves the reservoir states shown in Figure 2a directly from their output power. However, this is not easy to do, as obtaining good performance requires only positive and negative readout weights. In optical implementations [10, 9], the states are encoded as light intensities that are always positive so that they cannot be subtracted from others. Furthermore, summing up the states must include only the values of the maximum readout time, which is difficult in time-multiplexed reservoirs where the states xN (n) and x1 (n + 1) follow. Here we show how to solve both difficulties by using the scheme shown in the right figure."}, {"heading": "4 Results", "text": "As a benchmark for our analog evaluations, three quantities are presented: The first is the digital performance of the Jair networks (introduced in 1994) to test adaptive bilinear filtering and then used by Jaeger [16] to demonstrate the capabilities of reservoir computing, which becomes a standard benchmark task in the reservoir computing community, and was used, for example, in [20]. It consists in restoring a sequence of symbols transmitted along a radio channel, in the presence of multiple reflections, noise and nonlinear distortion; a more detailed description of the task can be found in the appendix. The performance of the reservoir is normally measured in Symbol Error Rate (SER), i.e. the rate of misinterpreted symbols, as a function of the amount of noise in the wireless channels. Figure 3 shows the performance of the experimental setup of [10] for a network of 64 nodes, for different sums of noise."}, {"heading": "5 Discussion", "text": "It is the first time that a country has been able to pay its debts, it is the first time that a country has been able to pay its debts, it is the second time that a country has been able to pay its debts, it is the second time that a country has been able to pay its debts, it is the third time that a country has been able to pay its debts, it is the first time that a country has been able to pay its debts."}, {"heading": "Acknowledgments", "text": "This research was supported by the Interuniversity Attraction Poles Programme of the Belgian Science Policy Office, under the auspices of IAP P7-35 \"photonics @ be.\""}, {"heading": "Appendix: Nonlinear Channel Equalization task", "text": "What follows is a detailed description of the channel balancing task used to test the performance of the container. The object of this task is to reconstruct a sequence d (n) of symbols from {\u2212 3, \u2212 1, 1, 3}. The symbols in d (n) are given in a new sequence q (n) by q (n) = 0.08 d (n + 2) \u2212 0.12d (n + 1) + 0.18 d (n \u2212 1) \u2212 0.1d (n \u2212 1) \u2212 0.1d (8) + 0.09 1d (n \u2212 3) -0.05 d (n \u2212 4) + 0.04 d (n \u2212 5) + 0.03 d (n \u2212 6) + 0.01 d (n \u2212 7), which model a radio signal that reaches a receiver through different paths with different runtimes. A loud, distorted version u (n) of the mixed signal output signal q (n) and a simulation of the nonlinearities and the sources in the receiver is then generated by d."}], "references": [{"title": "The \"echo state\" approach to analysing and training recurrent neural networks", "author": ["H. Jaeger"], "venue": "Technical report, Technical Report GMD Report 148, German National Research Center for Information Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Real-time computing without stable states: A new framework for neural computation based on perturbations", "author": ["W. Maass", "T. Natschlager", "H. Markram"], "venue": "Neural computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "An overview of reservoir computing: theory, applications and implementations", "author": ["B. Schrauwen", "D. Verstraeten", "J. Van Campenhout"], "venue": "In Proceedings of the 15th European Symposium on Artificial Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Lukosevicius", "H. Jaeger"], "venue": "Computer Science Review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Pattern recognition in a bucket", "author": ["C. Fernando", "S. Sojakka"], "venue": "Advances in Artificial Life,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Edge of chaos computation in mixed-mode vlsi - a hard liquid", "author": ["F. Schurmann", "K. Meier", "J. Schemmel"], "venue": "In In Proc. of NIPS. MIT Press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Reservoir computing: a photonic neural network for information processing. volume 7728, page 77280B", "author": ["Y. Paquot", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Information processing using a single dynamical node as complex system", "author": ["L. Appeltant", "M.C. Soriano", "G. Van der Sande", "G. Danckaert", "S. Massar", "J. Dambre", "B. Schrauwen", "C.R. Mirasso", "I. Fischer"], "venue": "Nature Communications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Photonic information processing beyond Turing: an optoelectronic implementation of reservoir computing", "author": ["L. Larger", "M.C. Soriano", "D. Brunner", "L. Appeltant", "J.M. Gutierrez", "L. Pesquera", "C.R. Mirasso", "I. Fischer"], "venue": "Optics Express,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Optoelectronic reservoir computing", "author": ["Y. Paquot", "F. Duport", "A. Smerieri", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar"], "venue": "Scientific reports,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "What makes a dynamical system computationally powerful", "author": ["R. Legenstein", "W. Maass"], "venue": "New Directions in Statistical Signal Processing: From Systems to Brain. MIT Press,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Photonic reservoir computing: A new approach to optical information processing", "author": ["K. Vandoorne", "M. Fiers", "D. Verstraeten", "B. Schrauwen", "J. Dambre", "P. Bienstman"], "venue": "In 2010 12th International Conference on Transparent Optical Networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Optical computing: Photonic neural networks", "author": ["D. Woods", "T.J. Naughton"], "venue": "Nature Physics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Generating coherent patterns of activity from chaotic neural networks. Neuron", "author": ["D. Sussillo", "L.F. Abbott"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Optimization and applications of echo state networks with leaky-integrator neurons", "author": ["H. Jaeger", "M. Lukosevicius", "D. Popovici", "U. Siewert"], "venue": "Neural networks : the official journal of the International Neural Network Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": "Science, 304(5667):78\u201380,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Memory versus nonlinearity in reservoirs", "author": ["D. Verstraeten", "J. Dambre", "X. Dutoit", "B. Schrauwen"], "venue": "In The 2010 International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Stable output feedback in reservoir computing using ridge regression", "author": ["F. Wyffels", "B. Schrauwen"], "venue": "Artificial Neural Networks-ICANN,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Adaptive algorithms for bilinear filtering", "author": ["J. Mathews. V"], "venue": "Proceedings of SPIE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Minimum complexity echo state network", "author": ["A. Rodan", "P. Tino"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "A rewardmodulated hebbian learning rule can explain experimentally observed network reorganization in a brain control task", "author": ["R. Legenstein", "S.M. Chase", "A.B. Schwartz", "W. Maass"], "venue": "The Journal of neuroscience : the official journal of the Society for Neuroscience,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Jaeger [1] and by W.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "Maass [2].", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "The result is a powerful system that can outperform other techniques on a range of tasks (see for example the ones reported in [3, 4]), and is significantly easier to train than recurrent neural", "startOffset": 127, "endOffset": 133}, {"referenceID": 3, "context": "The result is a powerful system that can outperform other techniques on a range of tasks (see for example the ones reported in [3, 4]), and is significantly easier to train than recurrent neural", "startOffset": 127, "endOffset": 133}, {"referenceID": 4, "context": "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].", "startOffset": 59, "endOffset": 68}, {"referenceID": 5, "context": "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].", "startOffset": 59, "endOffset": 68}, {"referenceID": 6, "context": "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].", "startOffset": 59, "endOffset": 68}, {"referenceID": 7, "context": "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].", "startOffset": 203, "endOffset": 213}, {"referenceID": 8, "context": "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].", "startOffset": 203, "endOffset": 213}, {"referenceID": 9, "context": "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].", "startOffset": 203, "endOffset": 213}, {"referenceID": 10, "context": "The last requirement essentially means that the dynamics allows the exploration of a large number of network states when new inputs come in, while at the same time retaining for a finite time information on the previous inputs [11].", "startOffset": 227, "endOffset": 231}, {"referenceID": 0, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 119, "endOffset": 132}, {"referenceID": 1, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 119, "endOffset": 132}, {"referenceID": 4, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 119, "endOffset": 132}, {"referenceID": 11, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 119, "endOffset": 132}, {"referenceID": 6, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 198, "endOffset": 211}, {"referenceID": 7, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 198, "endOffset": 211}, {"referenceID": 8, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 198, "endOffset": 211}, {"referenceID": 9, "context": "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].", "startOffset": 198, "endOffset": 211}, {"referenceID": 9, "context": "Very recently, optoelectronic reservoir computers have been demonstrated by different research teams [10, 9], conjugating good computational performances with the promise of very high operating speeds.", "startOffset": 101, "endOffset": 108}, {"referenceID": 8, "context": "Very recently, optoelectronic reservoir computers have been demonstrated by different research teams [10, 9], conjugating good computational performances with the promise of very high operating speeds.", "startOffset": 101, "endOffset": 108}, {"referenceID": 12, "context": "An analog readout for experimental reservoirs would remove this major bottleneck, as pointed out in [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Moreover, an analog readout opens the possibility of feeding back the output of the reservoir into the reservoir itself, which in turn allows the use of different training techniques [14] and to apply reservoir computing to new categories of tasks, such as pattern generation [15, 16].", "startOffset": 183, "endOffset": 187}, {"referenceID": 14, "context": "Moreover, an analog readout opens the possibility of feeding back the output of the reservoir into the reservoir itself, which in turn allows the use of different training techniques [14] and to apply reservoir computing to new categories of tasks, such as pattern generation [15, 16].", "startOffset": 276, "endOffset": 284}, {"referenceID": 15, "context": "Moreover, an analog readout opens the possibility of feeding back the output of the reservoir into the reservoir itself, which in turn allows the use of different training techniques [14] and to apply reservoir computing to new categories of tasks, such as pattern generation [15, 16].", "startOffset": 276, "endOffset": 284}, {"referenceID": 9, "context": "The mechanism has been tested experimentally using the experimental reservoir reported in [10], and compared to a digital readout.", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Although the results are preliminary, they are promising: while not as good as those reported in [10], they are however already better than non-reservoir methods for the same task [16].", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Although the results are preliminary, they are promising: while not as good as those reported in [10], they are however already better than non-reservoir methods for the same task [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 16, "context": "The number of nodes in a reservoir computer determines an upper limit to the reservoir performance [17]; this can be an obstacle when designing physical implementations of RCs, which should contain a high number of interconnected nonlinear units.", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "A solution to this problem proposed in [7, 8], is time multiplexing: the xi(n) are computed one by one by a single nonlinear element, which receives a combination of the input u(n) and a previous state xj(n \u2212 1).", "startOffset": 39, "endOffset": 45}, {"referenceID": 7, "context": "A solution to this problem proposed in [7, 8], is time multiplexing: the xi(n) are computed one by one by a single nonlinear element, which receives a combination of the input u(n) and a previous state xj(n \u2212 1).", "startOffset": 39, "endOffset": 45}, {"referenceID": 7, "context": "[8], or by using an instantaneous nonlinear element and desynchronizing the input with respect to the delay line [10].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[8], or by using an instantaneous nonlinear element and desynchronizing the input with respect to the delay line [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "The hardware reservoir computer we use in the present work is identical to the one reported in [10] (see also [9]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "The hardware reservoir computer we use in the present work is identical to the one reported in [10] (see also [9]).", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "In the experiment reported in [10] a portion of the light coming out of the MZ is deviated to a second photodiode (not shown in Figure 1), that converts it", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "The optimal readout weights Wi and bias Wb are then calculated on a computer from a subset (training set) of the recorded states, using ridge regression [18], and the output y(n) is then calculated using equation 2 for all the states collected.", "startOffset": 153, "endOffset": 157}, {"referenceID": 9, "context": "In optical implementations [10, 9] the states xi are encoded as light intensities which are always positive, so they cannot be subtracted one from another.", "startOffset": 27, "endOffset": 34}, {"referenceID": 8, "context": "In optical implementations [10, 9] the states xi are encoded as light intensities which are always positive, so they cannot be subtracted one from another.", "startOffset": 27, "endOffset": 34}, {"referenceID": 18, "context": "As a benchmark for our analog readout, we use a wireless channel equalization task, introduced in 1994 [19] to test adaptive bilinear filtering and subsequently used by Jaeger [16] to show the capabilities of reservoir computing.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "As a benchmark for our analog readout, we use a wireless channel equalization task, introduced in 1994 [19] to test adaptive bilinear filtering and subsequently used by Jaeger [16] to show the capabilities of reservoir computing.", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "This task is becoming a standard benchmark task in the reservoir computing community, and has been used for example in [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 9, "context": "Figure 3 shows the performance of the experimental setup of [10] for a network of 28 nodes and one of 64 nodes, for different amounts of noise.", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "The first is the performance of the reservoir with a digital readout (blue triangles), identical to the one used in [10].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "However, it is already better than the non-reservoir methods reported in [19] and used by Jaeger as benchmarks in [16].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "However, it is already better than the non-reservoir methods reported in [19] and used by Jaeger as benchmarks in [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "We expect to extend easily this approach to different tasks, already studied in [9, 10], including a spoken digit recognition task on a standard dataset.", "startOffset": 80, "endOffset": 87}, {"referenceID": 9, "context": "We expect to extend easily this approach to different tasks, already studied in [9, 10], including a spoken digit recognition task on a standard dataset.", "startOffset": 80, "endOffset": 87}, {"referenceID": 20, "context": "Adaptive training algorithms, such as the ones mentioned in [21], could also take into account nonidealities in the readout components.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "Indeed, removing the need for slow, offline postprocessing is indicated in [13] as one of the major challenges in the field.", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "second [10]; however, in the case of a digital readout, the node states must be recovered and postprocessed to obtain the reservoir outputs.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "5 \u03bcs per input symbol, five orders of magnitude faster than the electronic reservoir reported in [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 14, "context": "This opens the way for different tasks to be performed [15] or different training techniques to be employed [14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "This opens the way for different tasks to be performed [15] or different training techniques to be employed [14].", "startOffset": 108, "endOffset": 112}], "year": 2012, "abstractText": "Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.", "creator": "LaTeX with hyperref package"}}}