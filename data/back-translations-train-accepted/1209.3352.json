{"id": "1209.3352", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2012", "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs", "abstract": "Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is perhaps the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of $\\tilde{O}(\\frac{1}{\\sqrt{\\epsilon}}\\sqrt {T^{1+\\epsilon}} d)$ in time $T$ for any $0&lt;\\epsilon &lt;1$, where $d$ is the dimension of each context vector and $\\epsilon$ is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of $\\Omega(\\sqrt{Td})$ for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem.", "histories": [["v1", "Sat, 15 Sep 2012 03:27:11 GMT  (23kb)", "http://arxiv.org/abs/1209.3352v1", null], ["v2", "Fri, 22 Feb 2013 18:35:56 GMT  (30kb)", "http://arxiv.org/abs/1209.3352v2", null], ["v3", "Thu, 30 Jan 2014 07:00:54 GMT  (30kb)", "http://arxiv.org/abs/1209.3352v3", "Improvements from previous version: (1) dependence on d improved from d^2 to d^{3/2} (2) Simpler and more modular proof techniques"], ["v4", "Mon, 3 Feb 2014 07:09:03 GMT  (30kb)", "http://arxiv.org/abs/1209.3352v4", "Improvements from previous version: (1) dependence on d improved from d^2 to d^{3/2} (2) Simpler and more modular proof techniques (3) bounds in terms of log(N) added"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["shipra agrawal", "navin goyal"], "accepted": true, "id": "1209.3352"}, "pdf": {"name": "1209.3352.pdf", "metadata": {"source": "CRF", "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs", "authors": ["Shipra Agrawal"], "emails": ["shipra@microsoft.com", "navingo@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 120 9.33 52v1 [cs.LG] 1 5Se p20 12,000 \u221a T 1 + \u0439d) in time T for each 0 < B < 1, where d is the dimension of each context vector and the algorithm uses one parameter.Our results provide the first theoretical guarantees for the contextual version of Thompson sampling and are close to the lower limit of what applies to this problem. This essentially solves the open COLT problem of Chapelle and Li [COLT 2012] regarding the limits of regret for Thompson sampling for contextual bandits problems.Our version of Thompson sampling uses Gaussian Pre- and Gaussian probability functionality. Our novel analysis techniques based on martyr principles also allow simple extensions of the use of more general distributions, thus satisfying certain general conditions.0"}, {"heading": "1 Introduction", "text": "There are many versions of multi-armed bandit problems; a particularly useful version is the contextual multi-armed bandit problem. In this problem, a learner is confronted with the choice of one of the two actions called N weapons. Before making the choice of the arm he plays, the learner sees a d-dimensional trait vector bi, related to the \"context\" associated with each arm i. The learner uses these traits together with the traits vectors and rewards of the arms played by them in the past, to make the choice of the arms. Over time, the learner gathers enough information about how the traits vectors and rewards are related so that it can predict with some certainty what will give the best reward by looking at the characteristics of the predictors."}, {"heading": "1.1 Our Results", "text": "The formal problem appears in Sec. 2.1.Theorem 1. For the contextual bandit problem with linear payouts, with probability 1 \u2212 \u03b4, total regret in time T for Thompson sampling algorithm is limited by O (d \u221a NT 1 + 2 \u00b7 \u00b7 \u00b7 = \u00b5N = \u00b5, i.e. there is a single underlying d-dimensional parameter \u00b5, then with probability 1 \u2212 \u03b4, total regret in time T for Thompson sampling is limited by O (d \u221a T 1 \u00b7 \u00b7 \u00b7 lnT ln 1). Comment 1. Here 0 < < 1 can be selected to be a constant. If T is known, one could choose whether the total time T for Thompson sampling is limited by O (d \u221a T 1 \u00b7 lnT ln 1). Comment 1. Here 0 < < 1 can be selected to be a constant."}, {"heading": "1.2 Related Work", "text": "The contextual bandit problem with linear payouts is a widely studied problem in statistics and machine learning often under different names, as by Chu et al. [9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5]. The name contextual bandits was coined in Langford and Zhang [17].Chu et al. [9] shows that for any algorithm, regret is essentially coined by the N-armed contextual bandits and individual parameters. Auer [4] and Chu et al. [9] SupLinUCB, a complicated algorithm that uses UCB as subroutine, for this problem. Chu et al. achieve a regret tied to O (slow payouts and individual parameters)."}, {"heading": "2 Problem setting and algorithm description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem setting", "text": "There are N Arms. At the time t = 1, 2,.. becomes for each arm a context vector bi (t). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (Regrets). (....). (......... (Regrets). (............. (....). (Regrets. (..... (....). (..... (Regrets). (..... (....). (Regrets). (..... (....). (....). (Regrets. (....). (....). (..... (....). (Regrets). (..... (....). (....). (....). (Regrets. (....). (..... (....). (....). (....). (....). (Regrets. (..... (....). (....). (..... (....). (....). (....). (....). (....). (Regrets). (..... (....). (..... (....). (....). (....). (..... (....). (....). (....). (....)."}, {"heading": "2.2 Thompson Sampling algorithm", "text": "Here we describe the algorithm for setting individual parameters. (For the case of N different parameters, see Section 4.) Since there is a single underlying parameter, TS maintains a common previous distribution via this parameter. We use the Gaussian probability function and Gaussian previous distribution N (bi (t) T\u00b5, v2). More specifically, we assume that the probability of the reward ri (t) at time t, given context bi (t) and parameter \u00b5, by the pdf of the Gaussian distribution N (bi (t) T\u00b5, v2). Here, v = R 6 (1)."}, {"heading": "2.3 Challenges and solution outline", "text": "The contextual version of the multi-armed bandit problem presents new challenges for analyzing the TS algorithm, and the techniques used so far to analyze the basic multi-armed bandit problem do not seem to be directly applicable. Let's describe some of these difficulties and our novel solutions to solve them. In the basic MAB problem, there are N arms, each arm of which is associated with a parameter, but beyond that, it is always associated with a sub-optimal arm associated with a context bi (t), so that the reward bi (t) \u00b5i is the best arm i (t) at a time when each arm i is associated with a parameter \u00b5i, and the regret for the game is poor i is bi (t).That means reward bi (t) \u00b5i, the best arm i (t) at a time t is the arm with the highest mean at a time t, and the regret for the game is bi (t)."}, {"heading": "3 Regret Analysis: Proof of Theorem 2", "text": "Definition (T) = R + D (T) = D + D (T) = D (T) = D (T) = D (T) = D (T)) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T).Definition (T): D) (T). Definition (T).Definition (T): D) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T)).). Definition (T): D) (T) (T) (T)."}, {"heading": "3.1 Proof of Theorem 2", "text": "Note that for Super-martingale Yt, | Yt \u2212 Yt \u2212 1 (i) i (i) i (i) i (li) i (li) i (li) i (li) i (li) i (li) i (li) i (li) i (li) i (li) i (li) i (t) \u2212 st, i (t) \u2212 5pT 2 | \u2264 7p. The last inequality applies because for each i, st, i \u2212 bi (t) TB \u2212 1 (t) bi (t) i (t) (t) i (n) | bi (t) i (t) \u2212 st, i (n) i (t) i (t) i (n) i (sp) i (t) i (t) (t) \u2212 p) i (t) i (n) (sp) i (t) i (p) (n) (n) (p) (n) (n) (i) (p) i (i) i i i (i) i (i) i) i (i), i (t) i (p) (p) (n) (p) (n) (p) (n) (n) (i) (p) i (i) i (i) i (i) i i i (i) i (i) i (i) (p) (p) (p)."}, {"heading": "4 Extensions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 N different parameters", "text": "Theorem 1 considers the setting in which each arm i is associated with a parameter \u00b5i, possibly \u00b5i 6 = \u00b5i \u2032 for two different arms i and i \u2032. In this case, Thompson Sampling would maintain a separate estimate of the mean value of \u00b5 i (t) and Bi (t) for each arm i, which would be updated only at the time when i is played. In the case of Lemma 1 and the supermartyr property specified by Lemma 2, the same will apply as for the new definitions. The only difference will be in the limit for \"t st, i (t),\" which will be used in the proof of Theorem 2. In the case of \"N,\" we will get at this amount of O (\u221a NTd lnT) instead of O (\u221a Td lnT), which will result in the additional N factor in the limit in Theorem 1 compared to Theorem 2. Details of the algorithm in the case of N different parameters and the changes in the analysis provided for Theorem 1 will be required in Theorem D."}, {"heading": "4.2 General distributions", "text": "The only distribution-specific properties we have used in the analysis are the concentration and concentration inequalities for Gaussian distributed random variables mentioned in Lemma 4. Concentration inequality was used to prove that E-value (t) is highly likely to occur in Lemma 1, and the concentration inequality was used to reduce the probability that the Gaussian distributed random variable successi (t) (t) (t) (t) exceeds its mean by several factors of its standard deviation in Lemma 3. If another distribution exhibits similar tail inequalities, these inequalities can be used as a black box in the analysis, and the limits of regret can be reproduced for this distribution."}, {"heading": "5 Conclusions", "text": "Our results solve many open questions regarding the theoretical guarantees for Thompson Sampling and state that TS itself achieves remorse limits comparable to the state of the art for the contextual version of the stochastic MAB problem. We used novel martyrdom-based analysis techniques that are simpler and expandable than those investigated in the previous work on TS [3, 15]. Indeed, the techniques introduced in this paper could also be used to provide a simpler proof of the optimal expected remorse limit for TS for the fundamental MAB problem investigated in [3, 15]. Evidence of this claim will appear elsewhere. Several questions remain open. A stricter analysis that can eliminate dependence on this article is desirable. We believe that our techniques would adapt to create such limits for the expected remorse. Other ways to explore are contextual bandits with generalized linear models that do not relate to the Emissions being considered in the case."}, {"heading": "A Posterior distribution computation", "text": "Pr (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) (p) ("}, {"heading": "B Proof of Theorem 2", "text": "B.1 Gaussian Concentration Formula 7.1.13 from [2] can be used to derive the following concentration and anticoncentration inequalities for Gaussian distributed variables: \u2212 \u2212 \u2212 \u2212 \u2212 For a Gaussian distributed random variable Z with mean m and variance \u03c32, for any z-number of 1.12 \u221a \u2212 z 2 / 2 \u2264 Pr (Z \u2212 m | > z\u03c3) \u2264 1 \u221a z e \u2212 z 2 / 2.B.2 Proof Lemma 1We will use the following problem (implied by theorem 1 in [1]: Lemma 5. [1] Let us (F \u00b2 t; t) be a filtration, (mt; t \u00b2 1) be an Rd-weighted stochastic process so that mt (F \u2032 t \u2212 1) cannot be measured, (t \u00b2 1), (t \u00b2 1) be a real process where the difference is not (F \u00b2)."}, {"heading": "C Proof of Lemma 3", "text": "Since Gauss' random variable inequality in Lemma 4, Pr (2) (t) (t) (t) (t) (t) (t) (t) (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t) and standard deviation vst, i (t) (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t) Ft \u2212 1) Pr (t) (t) (t) T (t) T (t) T (t) T (t) T (t) T (t) T (t), T (t) T (t) T (t), T (t) T (t) T (t), T (t) T (t) T (t), T (t (t) T (t) T (t), T (t) T (t) T (t) T (t), T (t) T (t) T (t), T (t (t) T (t) T (t) T (t), T (t (t) T (t) T (t), T (t (t) T (t), T (t (t) T (t) T (t (t) T (t) T (t (t) T (t) T (t (t) T (t) T (t (t) T (t), T (t (t (t) T (t) T (t (t (t) T (t) T (t), T (t (t (t)."}, {"heading": "D N different parameters: Proof of Theorem 1", "text": "In this case Thompson Sampling would provide a separate estimate of the mean value of \"i (t)\" (t) = ibi (u) bi (u) T\u00b5 (t) bi (u) T\u00b5 (t) bi (u) T\u00b5 (t) T\u00b5 (t) i (t) = Bi (t) = Bi (t) = Bi (t) = Bi (t) \u2212 1t \u2212 1 x T (t) i (t) i (t) i (t) \u2212 1 x T (t) i (t) = Bi (t) \u2212 1 x T (t) i (t) i (t) i (t) i (t), v2 x (t) i (t) i (t) i (t)."}], "references": [{"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables", "author": ["Milton Abramowitz", "Irene A. Stegun"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1964}, {"title": "Analysis of Thompson Sampling for the Multi-armed Bandit Problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs", "author": ["Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "The Nonstochastic Multiarmed Bandit Problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi", "Sham M. Kakade"], "venue": "Proceedings of the 25th Conference on Learning Theory (COLT),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "An Empirical Evaluation of Thompson Sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Open Problem: Regret Bounds for Thompson Sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In COLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Contextual Bandits with Linear Payoff Functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E. Schapire"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic Linear Optimization under Bandit Feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Parametric Bandits: The Generalized Linear Case", "author": ["Sarah Filippi", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Web-Scale Bayesian Click-Through rate Prediction for Sponsored Search Advertising in Microsoft\u2019s Bing Search Engine", "author": ["Thore Graepel", "Joaquin Qui\u00f1onero Candela", "Thomas Borchert", "Ralf Herbrich"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian Learning Automaton", "author": ["O.-C. Granmo"], "venue": "International Journal of Intelligent Computing and Cybernetics (IJICC),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Associative Reinforcement Learning: Functions in k-DNF", "author": ["Leslie Pack Kaelbling"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information", "author": ["John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Linearly Parametrized Bandits", "author": ["Pedro A. Ortega", "Daniel A. Braun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "One-armed badit problem with covariates", "author": ["Jyotirmoy Sarkar"], "venue": "The Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "A modern Bayesian look at the multi-armed bandit", "author": ["S. Scott"], "venue": "Applied Stochastic Models in Business and Industry,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Experienceefficient learning in associative bandit problems", "author": ["Alexander L. Strehl", "Chris Mesterharm", "Michael L. Littman", "Haym Hirsh"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A Bayesian Framework for Reinforcement Learning", "author": ["Malcolm J.A. Strens"], "venue": "In ICML, pages 943\u2013950,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R. Thompson"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1933}, {"title": "A one-armed bandit problem with a concomitant variable", "author": ["Michael Woodroofe"], "venue": "Journal of the American Statistics Association,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1979}], "referenceMentions": [{"referenceID": 3, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 10, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 8, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 0, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 21, "context": "The first version of this Bayesian heuristic is around 80 years old, dating to Thompson (1933) [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": ", in [27, 20, 24].", "startOffset": 5, "endOffset": 17}, {"referenceID": 20, "context": ", in [27, 20, 24].", "startOffset": 5, "endOffset": 17}, {"referenceID": 12, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 18, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 11, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 6, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 18, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB.", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB.", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "TS has also been used in an industrial scale application for CTR prediction of search ads on search engines [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "[13, 18] provided weak guarantees, namely, a bound of o(T ) on expected regret in time T .", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "More recently, some significant progress was made by [3, 15], who provided near-optimal problem-dependent bounds on the expected regret of TS for the basic (i.", "startOffset": 53, "endOffset": 60}, {"referenceID": 7, "context": "Some of these questions were formally raised as a COLT 2012 open problem [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "This essentially solves the COLT 2012 open problem [8] for linear contextual bandits.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "The contextual MAB problem does not seem easily amenable to the techniques used so far for analyzing the basic MAB problem by [3, 15].", "startOffset": 126, "endOffset": 133}, {"referenceID": 8, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 37, "endOffset": 45}, {"referenceID": 17, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 37, "endOffset": 45}, {"referenceID": 13, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 116, "endOffset": 123}, {"referenceID": 19, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 116, "endOffset": 123}, {"referenceID": 4, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 15, "context": "The name contextual bandits was coined in Langford and Zhang [17].", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "[9] show that for any algorithm the regret is \u03a9( \u221a Td) for d2 \u2264 T for the N -armed contextual bandits problem with linear payoffs and single parameter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Auer [4] and Chu et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "[9] SupLinUCB, a complicated algorithm using UCB as a subroutine, for this problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Td ln(NT ln(T )/\u03b4)) with probability at least 1\u2212 \u03b4 (Auer [4] proves similar results).", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm.", "startOffset": 43, "endOffset": 49}, {"referenceID": 8, "context": "By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm.", "startOffset": 43, "endOffset": 49}, {"referenceID": 9, "context": "[10, 1].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[10, 1].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[1] analyze a UCB-style algorithm for that problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] as \u03a9(d \u221a T ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The state-of-the-art bounds for linear bandits problem in case of finite N are given by [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Our bounds are essentially within a factor of \u221a d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within \u221a lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.", "startOffset": 105, "endOffset": 111}, {"referenceID": 3, "context": "Our bounds are essentially within a factor of \u221a d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within \u221a lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.", "startOffset": 105, "endOffset": 111}, {"referenceID": 5, "context": "Our bounds are essentially within a factor of \u221a d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within \u221a lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "While significant recent progress was made in analyzing it for basic MAB [3, 15], it was not clear how to extend that to contextual bandits problem, for which no regret bounds were available.", "startOffset": 73, "endOffset": 80}, {"referenceID": 7, "context": "There were considerable difficulties in extending the existing techniques to this case, some of which were also pointed out in [8].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "1 of [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "3 Challenges and solution outline The contextual version of the multi-armed bandit problem presents new challenges for the analysis of TS algorithm, and the techniques used so far for analyzing the basic multi-armed bandit problem by [3, 15] do not seem directly applicable.", "startOffset": 234, "endOffset": 241}, {"referenceID": 3, "context": "t st,i(t) = O( \u221a Td) (derived along the lines of [4]), to get the desired regret bound.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "The probability bound for E(t) will be proven using the concentration inequality given by Theorem 1 in [1]).", "startOffset": 103, "endOffset": 106}, {"referenceID": 1, "context": "The probability bound for \u1ebci(t) will be proven using a concentration inequality for Gaussian random variables from [2] stated as Lemma 4 in Appendix B.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Also, 1 p \u2211T t=1 I(i(t) = i (t))st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT = 1 p \u2211 t:i(t)=i(t) st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT \u2264 1 p \u2211T t=1 st,i(t) + \u2211T t=1 st,i(t) + 5 pT = O( \u221a T \u01eb \u221a Td lnT ) For the last inequality, we use that \u2211T t=1 st,i(t) \u2264 5 \u221a dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4].", "startOffset": 303, "endOffset": 306}, {"referenceID": 3, "context": "Also, 1 p \u2211T t=1 I(i(t) = i (t))st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT = 1 p \u2211 t:i(t)=i(t) st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT \u2264 1 p \u2211T t=1 st,i(t) + \u2211T t=1 st,i(t) + 5 pT = O( \u221a T \u01eb \u221a Td lnT ) For the last inequality, we use that \u2211T t=1 st,i(t) \u2264 5 \u221a dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4].", "startOffset": 325, "endOffset": 328}, {"referenceID": 2, "context": "We used novel martingale-based analysis techniques which are simpler than those in the past work on TS [3, 15], and amenable to extensions.", "startOffset": 103, "endOffset": 110}, {"referenceID": 2, "context": "In fact, the techniques introduced in this paper could also be used to provide a simpler proof for the optimal expected regret bounds for TS for the basic MAB problem studied in [3, 15].", "startOffset": 178, "endOffset": 185}, {"referenceID": 10, "context": "Other avenues to explore are contextual bandits with generalized linear models considered in [11], the setting with delayed and batched feedbacks, and the agnostic case of contextual bandits with linear payoffs.", "startOffset": 93, "endOffset": 97}], "year": 2016, "abstractText": "Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is perhaps the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of \u00d5( 1 \u221a \u01eb \u221a T d) in time T for any 0 < \u01eb < 1, where d is the dimension of each context vector and \u01eb is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of \u03a9( \u221a Td) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem. Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of more general distributions, satisfying certain general conditions.", "creator": "LaTeX with hyperref package"}}}