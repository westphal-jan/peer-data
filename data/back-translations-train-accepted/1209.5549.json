{"id": "1209.5549", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2012", "title": "Towards a learning-theoretic analysis of spike-timing dependent plasticity", "abstract": "This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli.", "histories": [["v1", "Tue, 25 Sep 2012 09:23:41 GMT  (123kb,D)", "http://arxiv.org/abs/1209.5549v1", "To appear in Adv. Neural Inf. Proc. Systems"]], "COMMENTS": "To appear in Adv. Neural Inf. Proc. Systems", "reviews": [], "SUBJECTS": "q-bio.NC cs.LG stat.ML", "authors": ["david balduzzi", "michel besserve"], "accepted": true, "id": "1209.5549"}, "pdf": {"name": "1209.5549.pdf", "metadata": {"source": "CRF", "title": "Towards a learning-theoretic analysis of spike-timing dependent plasticity", "authors": ["David Balduzzi"], "emails": ["david.balduzzi@inf.ethz.ch", "michel.besserve@tuebingen.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2 The selectron", "text": "We. We. We. We. We. We. We. We. We. We. We. We. We. \"We.\" We. \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"We.\" \"We.\" \"We.\" We. \"We.\" We. \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \").\" We. \"We.\" We. \"We.\" We. \").\" We. \"We.\" We. \"We.\"). \"We.\" We. \"We.\""}, {"heading": "3 Relation to leaky integrate-and-fire neurons equipped with STDP", "text": "Similarly, there is a large menagerie of models of synaptic plasticity [19]. We look at two well-established models: Gerstner's Spike Response Model (SRM), which generalizes leaky integrate-and-fire neurons (SRM), and the original spike timing dependent plasticity rule proposed by Song et al [5], and show that the selectron appears in the fast time constant of the two models. First, let us recall SRM. Suppose neuron nk last outted a spike at time tk and receives input spikes at times tj from neuron nj. Neuron nk spikes or according to the haaviside function applied to the membrane potential Mw: fw (t) = H (Mw)."}, {"heading": "4 An error bound", "text": "In fact, it recursively justifies the inclusion of spikes in the reward function, which refers to the quality of spikes as an estimated value, where the outputs of many weak learners are aggregated into a system. [22] It is therefore crucial to provide guarantees of the quality of spikes as an estimated value, where the outputs of many weak learners are aggregated into a system that is remarkably resistant to overadjustment as the number of learners increases. [23] Cortical learning may be angous to boosting: individual neurons have access to a tiny fraction of the total brain state, and so are weak learners; and in the fast time limit, neurons are essential aggregators.We sharpen the analogy using selectronic approaches."}, {"heading": "5 A bound on the efficacy of inter-neuronal communication", "text": "It is therefore crucial to provide guarantees of the usefulness of spikes. In this section we quantify the effects of selectron's spikes on selectron's expected reward. We show that the effectiveness of selectron nj on selectron's expected reward is lower than the effectiveness of selectron nk's: = Rk-xj = 1: 0. The effectiveness of spikes from selectron nj on selectron nk's expected reward is weak."}, {"heading": "6 Experiments", "text": "It is therefore important that what neurons learn is resilient to changing input factors [26, 27]. In this section, as proof of the principle, we examine a simple random pattern of classic STDP with offline regularization. We show that it improves robustness when neurons have more than one pattern. We observe that regulating the optimization problem (3) improves the effects of classic STDP with offline regularization (x) when neurons have more than one pattern.2 (11) Learning rule: \"We must learn.\" We must continuously reevaluate the sum of synaptic weights."}, {"heading": "7 Discussion", "text": "We hope that the selectron and related models will lead to an improved understanding of the principles underlying learning in the cortex; it remains to be seen whether other STDP-based models also have traceable, discrete time analogs; the selectron is an interesting model in itself: it embeds reward estimates in peaks and maximizes a margin that improves error limits; it imposes a limitation on synaptic weights that: focus rewards / peaks, tighten error limits and improve guarantees for the effectiveness of spikes; while the analysis is not directly applicable to continuous time models, experiments show that a Tweak inspired by our analysis improves the performance of a more realistic model; an important possibility for future research is exploring the role of spiking effectiveness; although the analysis is not directly applied to continuous time models, experiments show that a Tweak inspired by our analysis improves the performance of a weak model."}, {"heading": "Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 The perceptron", "text": "We describe the perceptron to facilitate comparison with the selectron. The loss function and the learning rule of the perceptron are expressed most naturally when inputs and outputs take values in {\u00b1 1}. We therefore represent the perceptron in both \u00b1 1 and 0 / 1 \"coordinate systems.\" However, we first replace inputs and outputs from 0 / 1 to \u00b1 1: A = 2X \u2212 1 and B = 2Y \u2212 1. In view of the input a, the perceptron output after tob = 0 / 1 (a) is determined: = character (wa), where w is a real weighted N vector specifying the synaptic properties of the perceptron."}, {"heading": "A.2 Proof of Theorem 1", "text": "The theorem requires the calculation of the second moment of the total flow of a selectron, which is given as a function of the input I.i.d. Bernoulli inputs. We also calculate the expectation and the variance, since these are of intrinsic interest.Lemma 5 (moments for I.i.d. Bernoulli inputs).For Bernoulli i.i.d. inputs on synapses, i.e. P (xj = 0) = p for all j, we have E [< w, x > \u2212 p] = p (1 \u2212 p, x > 2] = p (1 \u2212 p) \u00b7 p (1 \u2212 p) \u00b7 p (1 \u2212 22 + p2 \u00b7 p \u00b7 p \u00b7 w = p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 w = 2p."}, {"heading": "A.3 Proof of Theorem 2", "text": "amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp; amp;"}, {"heading": "A.4 Proof of Theorem 3", "text": "To prove the theory, we first recall an error limited by [24]. To indicate its result, we need the following notation (letter f = f (x) = character N = 1 aj \u2212 gj (x). \u2212 f (letter f). \u2212 f (letter f). \u2212 f (letter f). \u2212 f (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (letter f). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ()."}, {"heading": "A.6 Proof of Theorem 4", "text": "It is helpful to introduce some notation."}], "references": [{"title": "A free energy principle for the brain", "author": ["K Friston", "J Kilner", "L Harrison"], "venue": "J. Phys. Paris", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["F Rosenblatt"], "venue": "Psychol Rev", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1958}, {"title": "Learning representations by back-propagating errors", "author": ["DE Rumelhart", "GE Hinton", "RJ Williams"], "venue": "Nature", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1986}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G Hinton", "S Osindero", "YW Teh"], "venue": "Neural Computation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Competitive Hebbian learning through spike-timing-dependent synaptic plasticity", "author": ["S Song", "KD Miller", "LF Abbott"], "venue": "Nature Neuroscience", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission", "author": ["HS Seung"], "venue": "Neuron", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Reducing spike train variability: A computational theory of spike-timing dependent plasticity", "author": ["SM Bohte", "MC Mozer"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "A criterion for the convergence of learning with spike timing dependent plasticity", "author": ["R Legenstein", "W Maass"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Simplified rules and theoretical analysis for information bottleneck optimization and PCA with spiking neurons", "author": ["L Buesing", "W Maass"], "venue": "In Adv in Neural Information Processing Systems (NIPS)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Theoretical analysis of learning with reward-modulated spiketiming-dependent plasticity", "author": ["R Legenstein", "D Pecevski", "W Maass"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "The information bottleneck method", "author": ["N Tishby", "F Pereira", "W Bialek"], "venue": "In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "What can neurons do for their brain? Communicate selectivity with spikes", "author": ["D Balduzzi", "G Tononi"], "venue": "To appear in Theory in Biosciences", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Metabolic cost as an organizing principle for cooperative learning", "author": ["D Balduzzi", "PA Ortega", "M Besserve"], "venue": "Under review,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "A neuromorphic architecture for object recognition and motion anticipation using burst-STDP", "author": ["A Nere", "U Olcese", "D Balduzzi", "G Tononi"], "venue": "PLoS One 2012,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Spike timing-dependent plasticity as dynamic filter", "author": ["J Schmiedt", "C Albers", "K Pawelzik"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Neural Network Learning: Theoretical Foundations", "author": ["M Anthony", "PL Bartlett"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "RE: Large Margin Classification Using the Perceptron Algorithm", "author": ["Freund Y", "Schapire"], "venue": "Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Decorrelated neuronal firing in cortical microcircuits", "author": ["AS Ecker", "P Berens", "GA Keliris", "M Bethge", "NK Logothetis", "AS Tolias"], "venue": "Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Spike timing-dependent plasticity of neural circuits", "author": ["Y Dan", "MM Poo"], "venue": "Neuron", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Time structure of the activity in neural network models", "author": ["W Gerstner"], "venue": "Phys. Rev. E", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Neural Networks and the Bias/Variance Dilemma", "author": ["S Geman", "E Bienenstock", "R Doursat"], "venue": "Neural Comp", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "RE: Experiments with a New Boosting Algorithm", "author": ["Freund Y", "Schapire"], "venue": "In Machine Learning: Proceedings of the Thirteenth International Conference", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods", "author": ["RE Schapire", "Y Freund", "P Bartlett", "WS Lee"], "venue": "The Annals of Statistics", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Theory of classification: A survey of some recent advances", "author": ["S Boucheron", "O Bousquet", "G Lugosi"], "venue": "ESAIM: PS", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "Metabolic cost as a unifying principle governing neuronal biophysics", "author": ["A Hasenstaub", "S Otte", "E Callaway", "TJ Sejnowski"], "venue": "Proc Natl Acad Sci U S A", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Cascade Models of Synaptically Stored Memories", "author": ["S Fusi", "P Drew", "L Abbott"], "venue": "Neuron", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Limits on the memory storage capacity of bounded synapses", "author": ["S Fusi", "L Abbott"], "venue": "Nature Neuroscience", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Sleep function and synaptic homeostasis", "author": ["G Tononi", "C Cirelli"], "venue": "Sleep Med Rev", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Molecular and electrophysiological evidence for net synaptic potentiation in wake and depression in sleep", "author": ["VV Vyazovskiy", "C Cirelli", "M Pfister-Genskow", "U Faraguna", "G Tononi"], "venue": "Nat Neurosci", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Cortical firing and sleep homeostasis", "author": ["VV Vyazovskiy", "U Olcese", "Y Lazimy", "U Faraguna", "SK Esser", "JC Williams", "C Cirelli", "G Tononi"], "venue": "Neuron", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Sleep and waking modulate spine turnover in the adolescent mouse cortex", "author": ["S Maret", "U Faraguna", "AB Nelson", "C Cirelli", "G Tononi"], "venue": "Nat Neurosci", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Unsupervised learning of visual features through spike timing dependent plasticity", "author": ["T Masquelier", "SJ Thorpe"], "venue": "PLoS Comput Biol", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "An elegant suggestion is that global objective functions may be optimized during learning [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "A successful approach to this question has been Rosenblatt\u2019s perceptron [2] and its extension to multilayer perceptrons via backpropagation [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "A successful approach to this question has been Rosenblatt\u2019s perceptron [2] and its extension to multilayer perceptrons via backpropagation [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "Similarly, (restricted) Boltzmann machines, constructed from simple stochastic units, have provided a remarkably powerful approach to organizing distributed optimization across many layers [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 4, "context": "By contrast, although there has been significant progress in developing and understanding more biologically realistic models of neuronal learning [5\u201310], these do not match the performance of simpler, more analytically and computationally tractable models in learning tasks.", "startOffset": 146, "endOffset": 152}, {"referenceID": 5, "context": "By contrast, although there has been significant progress in developing and understanding more biologically realistic models of neuronal learning [5\u201310], these do not match the performance of simpler, more analytically and computationally tractable models in learning tasks.", "startOffset": 146, "endOffset": 152}, {"referenceID": 6, "context": "By contrast, although there has been significant progress in developing and understanding more biologically realistic models of neuronal learning [5\u201310], these do not match the performance of simpler, more analytically and computationally tractable models in learning tasks.", "startOffset": 146, "endOffset": 152}, {"referenceID": 7, "context": "By contrast, although there has been significant progress in developing and understanding more biologically realistic models of neuronal learning [5\u201310], these do not match the performance of simpler, more analytically and computationally tractable models in learning tasks.", "startOffset": 146, "endOffset": 152}, {"referenceID": 8, "context": "By contrast, although there has been significant progress in developing and understanding more biologically realistic models of neuronal learning [5\u201310], these do not match the performance of simpler, more analytically and computationally tractable models in learning tasks.", "startOffset": 146, "endOffset": 152}, {"referenceID": 9, "context": "By contrast, although there has been significant progress in developing and understanding more biologically realistic models of neuronal learning [5\u201310], these do not match the performance of simpler, more analytically and computationally tractable models in learning tasks.", "startOffset": 146, "endOffset": 152}, {"referenceID": 5, "context": "The work closest in spirit to our own is Seung\u2019s \u201chedonistic\u201d synapses, which seek to increase average reward [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "Another related line of research derives from the information bottleneck method [9,11] which provides an alternate constraint to the one considered here.", "startOffset": 80, "endOffset": 86}, {"referenceID": 10, "context": "Another related line of research derives from the information bottleneck method [9,11] which provides an alternate constraint to the one considered here.", "startOffset": 80, "endOffset": 86}, {"referenceID": 11, "context": "An information-theoretic perspective on synaptic homeostasis and metabolic cost, complementing the results in this paper, can be found in [12, 13].", "startOffset": 138, "endOffset": 146}, {"referenceID": 12, "context": "An information-theoretic perspective on synaptic homeostasis and metabolic cost, complementing the results in this paper, can be found in [12, 13].", "startOffset": 138, "endOffset": 146}, {"referenceID": 13, "context": "Simulations combining synaptic renormalization with burst-STDP can be found in [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Important aspects of plasticity that we have not considered here are properties specific to continuoustime models, such as STDP\u2019s behavior as a temporal filter [15], and also issues related to convergence [8, 10].", "startOffset": 160, "endOffset": 164}, {"referenceID": 7, "context": "Important aspects of plasticity that we have not considered here are properties specific to continuoustime models, such as STDP\u2019s behavior as a temporal filter [15], and also issues related to convergence [8, 10].", "startOffset": 205, "endOffset": 212}, {"referenceID": 9, "context": "Important aspects of plasticity that we have not considered here are properties specific to continuoustime models, such as STDP\u2019s behavior as a temporal filter [15], and also issues related to convergence [8, 10].", "startOffset": 205, "endOffset": 212}, {"referenceID": 15, "context": "The learning-theoretic properties of neural networks have been intensively studied, mostly focusing on perceptrons, see for example [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "A non-biologically motivated \u201clarge-margin\u201d analog of the perceptron was proposed in [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "is the Heaviside function and w is a [0, 1] \u2282 R valued N -vector specifying the selectron\u2019s synaptic weights.", "startOffset": 37, "endOffset": 43}, {"referenceID": 7, "context": "The Bernoulli regime is the discrete-time analog of the homogeneous Poisson setting used to prove convergence of reward-modulated STDP in [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 17, "context": "inputs are unrealistic, note that recent neurophysiological evidence suggests neuronal firing \u2013 even of nearby neurons \u2013 is uncorrelated [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Similarly, there is a large menagerie of models of synaptic plasticity [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "We consider two well-established models: Gerstner\u2019s Spike Response Model (SRM) which generalizes leaky integrate-and-fire neurons [20] and the original spike-timing dependent plasticity learning rule proposed by Song et al [5], and show that the selectron arises in the fast time constant limit of the two models.", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "We consider two well-established models: Gerstner\u2019s Spike Response Model (SRM) which generalizes leaky integrate-and-fire neurons [20] and the original spike-timing dependent plasticity learning rule proposed by Song et al [5], and show that the selectron arises in the fast time constant limit of the two models.", "startOffset": 223, "endOffset": 226}, {"referenceID": 4, "context": "The original STDP update rule [5] is", "startOffset": 30, "endOffset": 33}, {"referenceID": 20, "context": "However, in a large system where estimates pile on top of each other there is a tendency to overfit, leading to poor generalizations [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 21, "context": "Boosting algorithms, where the outputs of many weak learners are aggregated into a classifier [22], are remarkably resistant to overfitting as the number of learners increases [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 22, "context": "Boosting algorithms, where the outputs of many weak learners are aggregated into a classifier [22], are remarkably resistant to overfitting as the number of learners increases [23].", "startOffset": 176, "endOffset": 180}, {"referenceID": 23, "context": "As a first step towards understanding how the cortex combats overfitting, we adapt a theorem developed to explain the effectiveness of boosting [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 24, "context": "However, since the cortex contains many neurons and spiking is metabolically expensive [25], we propose a conservative loss that only penalizes errors of commission (\u201cfirst, do no harm\u201d) and does not penalize specialization.", "startOffset": 87, "endOffset": 91}, {"referenceID": 25, "context": "It is therefore important that what neurons learn is robust to changing inputs [26, 27].", "startOffset": 79, "endOffset": 87}, {"referenceID": 26, "context": "It is therefore important that what neurons learn is robust to changing inputs [26, 27].", "startOffset": 79, "endOffset": 87}, {"referenceID": 27, "context": "It has recently been proposed that a function of NREM sleep may be to regulate synaptic weights [28].", "startOffset": 96, "endOffset": 100}, {"referenceID": 28, "context": "Indeed, neurophysiological evidence suggests that average cortical firing rates increase during wakefulness and decrease during sleep, possibly reflecting synaptic strengths [29, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 29, "context": "Indeed, neurophysiological evidence suggests that average cortical firing rates increase during wakefulness and decrease during sleep, possibly reflecting synaptic strengths [29, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 30, "context": "Experimental evidence also points to a net increase in dendritic spines (synapses) during waking and a net decrease during sleep [31].", "startOffset": 129, "endOffset": 133}, {"referenceID": 31, "context": "We used Gerstner\u2019s SRM model, recall \u00a73, with parameters chosen to exactly coincide with [32]: \u03c4m = 10, \u03c4s = 2.", "startOffset": 89, "endOffset": 93}, {"referenceID": 31, "context": "7 also taken from [32].", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Synaptic weights were clipped to fall in [0, 1].", "startOffset": 41, "endOffset": 47}, {"referenceID": 4, "context": "Classical STDP has a depotentiation bias to prevent runaway potentiation feedback loops leading to seizures [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 29, "context": "3 This is in line with experimental evidence showing increased cortical activity during waking [30].", "startOffset": 95, "endOffset": 99}], "year": 2012, "abstractText": "This paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning. We introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (STDP) and (ii) is amenable to theoretical analysis. We show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights. Moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength. Finally, based on our analysis, we propose a regularized version of STDP, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli.", "creator": "LaTeX with hyperref package"}}}