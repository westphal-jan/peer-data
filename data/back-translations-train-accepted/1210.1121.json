{"id": "1210.1121", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2012", "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations", "abstract": "We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via non-parametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semi-supervised sparse coding.", "histories": [["v1", "Wed, 3 Oct 2012 14:26:59 GMT  (28kb)", "http://arxiv.org/abs/1210.1121v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["krishnakumar balasubramanian", "kai yu", "guy lebanon"], "accepted": true, "id": "1210.1121"}, "pdf": {"name": "1210.1121.pdf", "metadata": {"source": "CRF", "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations", "authors": ["Krishnakumar Balasubramanian", "Kai Yu"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 121 0.11 21v1 [st at.M L] 3O ctWe propose and analyze a novel framework for learning economical representations based on two statistical techniques: core smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating trait similarity or temporal information present in data sets by means of non-parametric core smoothing. We provide general limits for learning sparse codes and demonstrate how sample complexity depends on the L1 standard of the core function used. We also propose to use marginal regression to obtain sparse coding, which greatly improves speed and allows easy scaling to large dictionary sizes. We demonstrate the benefits of the proposed approach in terms of both accuracy and speed by extensively experimenting with multiple real data sets. In addition, we show how the proposed coding approach could be used to improve semi-economization."}, {"heading": "1 Introduction", "text": "In fact, we're going to be able to hide, and we're going to be able to put ourselves on top, \"he said.\" We've got to be able to put ourselves on top, \"he said.\" We've got to put ourselves on top, \"he said.\" We've got to put ourselves on top, \"he said.\" We've got to put ourselves on top, \"he said.\" We've got to put ourselves on top, \"he said.\" But we've got to put ourselves on top. \""}, {"heading": "2 Related work", "text": "Our approach refers to the local regression method [13, 7]. Recent similar work [15] uses smoothing techniques in high-dimensional lasso regression in the context of temporal data. Another approach proposed by [26] achieves code locality by approximating data points using a linear combination of nearby baseline points. The main difference is that traditional local regression techniques do not include basic learning. In this work, we propose to learn the base or dictionary along with local regression coefficients. In contrast to previous sparse coding work, we propose to use marginal regression to learn the regression coefficients, resulting in significant compressive acceleration without loss of accuracy. Marginal regression is a relatively old technique that has recently resurfaced as a more compressed alternative to lasso regression [5]. See also [6] for a comparison of lasso regression statistical and regression."}, {"heading": "3 Smooth Sparse Coding", "text": "The notations x and X correspond to vectors and matrices, each in appropriately defined dimensions; the notation [1] corresponds to the Lp norm of a vector (we usually use p = 1, 2 in this work); the notation [2] corresponds to the Frobenius norm of a matrix; the notation [3] corresponds to the Lp norm of the function f: (3]. The notation xi, i = 1,. n corresponds to the data samples in which we assume that each sample xi is a d-dimensional vector. The explanation below uses L1 norm for simplification. The method applies more generally to all structured regulators, for example [3, 8]. The standard thrift coding consists of solving the following optimization problems, min D-Rd \u00d7 K \u03b2i."}, {"heading": "3.1 The choice of smoothing kernel", "text": "There are several ways to determine the weight function w. A common choice for the core function is the Gaussian core, whose bandwidth is selected by cross-validation. Other common choices for the core are the triangular, uniform and tricube-shaped cores. The bandwidth can be set throughout the entrance space or vary to take advantage of unequal samples. In our experiment, we use the tricube-shaped kernel with constant bandwidth. The distance function \u03c1 (\u00b7, \u00b7) can be one of the standard distance functions (for example, based on the Lp standard). Alternatively, \u03c1 (\u00b7, \u00b7) can be expressed by domain experts learned from data prior to sparse coding training, or can be learned together with the dictionary and codes during sparse coding training."}, {"heading": "3.2 Spatio-Temporal smoothing", "text": "In spatio-temporal applications, we can also extend the core by a term that reflects the distance between the corresponding time or space (xj, xi) = 1 h1 K1 (\u03c1 (xj, xi) h1) 1 h2 K2 (j \u2212 i h2).Above, K2 is a unified symmetric core with the bandwidth parameter h2. A similar approach to this situation is based on the fused lasso, which penalizes the absolute difference between the codes for adjacent points. The main disadvantage of this approach is that one can only have the temporal component and must omit the first term, which contains the distance function between the representation of the characteristics, while the coefficient learning step in this situation is based on the fused lasso, which penalizes the absolute difference between the codes for adjacent points."}, {"heading": "4 Marginal Regression for Smooth Sparse Coding", "text": "The question is whether it is even possible for us to communicate in this section in a way that enables better encodings. (i) To do this, we must first give a brief description of marginal regression. (i) In this section, we will show how marginal regression could be used to obtain better encodings. (i) To do this, we will first give a brief description of marginal regression. (i) In this section, we will show how marginal regression could be used to obtain better encodings. (i) To do this, we will first give a brief description of marginal regression. (i)"}, {"heading": "5 Sample Complexity of Smooth sparse coding", "text": "In this section, we analyze the complexity of the proposed sparse coding environments, and we define the coding environments so that there is no detectable algorithm that approaches the global minimum of the optimization problem in Equation (1). We start by extending the smooth, sparse coding problem in the smooth, sparse coding setting, and the main difficulty for the smooth, sparse coding setting is to obtain a covering number for an appropriately defined class of functions (see Theorem 1 for more details). We begin by presenting the smooth, sparse coding problem setting in a convenient format for analysis. Let's leave x1, xn be independent random variables with a common probability measurement P with a density of Pn."}, {"heading": "6 Experiments", "text": "We show the advantage of the proposed approach in terms of both acceleration and accuracy over traditional sparse coding. A detailed description of all real-world data sets used in the experiments can be found in the appendix."}, {"heading": "6.1 Speed comparison", "text": "We conducted synthetic experiments to investigate the acceleration associated with sparse coding with marginal regression, and the data were generated from a 100-dimensional mixture of two Gaussian distributions, which fulfilled a covariance matrix of 1-2-3 (with identity covariance matrices).The dictionary size was set at 1024. We compared the proposed sparse coding algorithm, the standard sparse coding with lasso [11] and marginal regression actualizations with a relative reconstruction error, namely a convergence criterion of X-D, B, F and X-F. We experimented with different values with different values of the relative reconstruction error (less than 10%) and gave the average time. Table 1 shows that a smooth sparse coding with marginal regression takes significantly less time to achieve a reconstruction error due to the use of a reconstruction spatial structure."}, {"heading": "6.2 Experiments with Kernel in Feature space", "text": "We conducted several experiments demonstrating the benefits of the proposed encoding scheme in different environments, focusing on face and object recognition from static images, evaluating the performance of the proposed approach along with Standard Sparse Encoding and LLC [26], another method for obtaining sparse features based on locality, and conducting activity detection experiments from videos based on both spatial and time-based cores."}, {"heading": "6.2.1 Image classification", "text": "We performed image classification experiments using CMU multipie, 15 Scene and Caltech-101 datasets. Following [24], we used the following approach to generate a sparse image representation: We randomly selected 16 x 16 image fields from pixel-level images on an 8-pixel Gird, calculated SIFT characteristics, and then calculated the corresponding sparse codes via a 1024-pixel dictionary. We used maximum pooling to obtain the final representation of the image based on the codes for the patches. The process was repeated with various randomly selected training and test images, and we report the average detection rates per class (along with its standard deviation estimate) based on the one-against-all SVM classification. We used cross validation to select the regulation and bandwidth parameters. As Table 2 shows, our smooth coding of algorithms leads to a higher codification accuracy LC."}, {"heading": "6.2.2 Action recognition:", "text": "Similar to the static image case, we perform an activity detection experiment from videos with KTH action and YouTube dataset (see Appendix). Similar to the static image case, we follow the standard approach to generate sparse representations for videos as in [21]. We sample 16 x 16 x 10 blocks from the video and extract the HoG-3d [10] characteristics from the sampled blocks. Subsequently, we use smooth, sparse encoding and max pooling to generate the video representation (the dictionary size was set to 1024 and cross validation was used to select the regulation and bandwidth parameters). Previous approaches include sparse encoding, vector quantization, and k means on the top of the HoG-3d feature set (see [21] for a comprehensive evaluation). As shown in Table 4, smooth, sparse encoding leads to a higher level of coding than previously reported (see Codification accuracy for both)."}, {"heading": "6.2.3 Discriminatory power", "text": "In this section, we describe another experiment that contrasts the codes obtained by sparse encoding and smooth sparse encoding in the context of a later classification task. As in [25], we first calculate the codes in both cases on the basis of patches and combine them with max pooling to obtain the representation at image level. Then, we calculate the Fischer discriminant value (ratio between class variance and class variance) for each dimension as a measure of the discriminatory power realized by the representations. Figure 1, shows a histogram of the ratio of smooth sparse encoding Fisher score compared to conventional sparse encoding Fisher score R (d) = F1 (d) / F2 (d) for 15-scene datasets (left) and Youtube datasets (right). Both histograms demonstrate the improved discriminatory power of smooth sparse encoding versus regular sparse encoding."}, {"heading": "6.3 Experiments using Temporal Smoothing", "text": "In this section, we describe an experiment conducted with the temporal smoothing core of the Youtube person dataset. We extracted SIFT descriptors for all 16 x 16 patches sampled on a step-size 8 grid and used smooth, sparse time core encoding to learn the codes and maximum pooling to obtain the final video representation. We avoided pre-processing steps such as face extraction or face tracking. Note that in the previous action detection video experiment, video blocks were densely sampled and used for the extraction of HoG 3D features. In this experiment, on the other hand, we extracted SIFT features from individual frames and used the time cores to include the temporal information in the sparse encoding process. In this case, we also compared the more standard lassobased approach [18]. Note that in this experiment, in addition to this standardized 1 penalty, in addition to the L1 penalty, we used a standard one."}, {"heading": "7 Semi-supervised smooth sparse coding", "text": "One of the primary difficulties in image editing is the lack of availability of described data and in some cases blank data (for certain areas).The motivation for such approaches is that the data from a related domain may have some visual patterns similar to the problem at hand. Therefore, learning a high-level dictionary based on data from other domains is helpful. We suggest that the smooth coding approach could be useful in this environment.The motivation is as follows: in half of the monitored, typically not all cases where the coding takes place."}, {"heading": "8 Discussion and Future work", "text": "We proposed a simple framework for integrating similarities in attribute space and space or time into sparse encodings; the codes obtained through sparse encoding are significantly more discriminatory than traditional sparse encoding, resulting in vastly improved classification accuracy as measured by several different image and video classification tasks; we also propose in this paper to modify the sparse encoding by replacing the level of lasso optimization with marginal regression and adding a constraint to the enforcement of incoherent dictionaries; the resulting algorithm is much faster (an acceleration of about two orders of magnitude over the standard sparse encoding); this facilitates scaling of the sparse encoding framework to large dictionaries, an area that is normally limited by insoluble compression; and we explore promising extensions for temporal smoothing, semi-supervised encoding, and transfer of learning content."}, {"heading": "9 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Data set Description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1.1 CMU Multi-pie face recognition:", "text": "The Face Recognition Experiment was performed using the CMU Multi-PIE dataset, which is challenging due to the large number of subjects and is one of the standard datasets for Face Recognition Experiments. The dataset contains 337 subjects on simultaneous variations in pose, expression and lighting. We ignore the 88 subjects who were considered outliers in [24] and use the rest of the images for our Face Recognition Experiments. We follow [24] and use the 7 frontal extreme lights from the first session as a traction set and a further 20 lights from sessions 2-4 as a test set."}, {"heading": "9.1.2 15 Scenes Categorization:", "text": "In addition, we conducted scene classification experiments with the dataset 15-scenes. This dataset consists of 4485 images from 15 categories, with the number of images per category varying from 200 to 400. Categories correspond to scenes from different scenarios such as kitchen, living room, etc. Similar to the previous experiment, we extracted patches from the images and calculated the SIFT features that correspond to the patches."}, {"heading": "9.1.3 Caltech-101 Data set:", "text": "The Caltech 101 dataset consists of images from 101 classes such as animals, vehicles, flowers, etc. The number of images per category varies from 30 to 800. Most images are medium resolution (300 x 300). All images are used in grayscale images. Following previous experimental default settings for the Caltech 101 dataset, we use 30 images per category and test the rest."}, {"heading": "9.1.4 Activity recognition", "text": "The KTH action dataset consists of 6 human action classes. Each action is performed several times by 25 subjects and is recorded in four different scenarios. In total, the data consists of 2391 video samples. The YouTube action dataset comprises 11 action categories and is more complex and demanding [12]. It includes 1168 video sequences with different lighting, background, resolution, etc. We sample (400 cuboids) video blocks from the data sample and extract HOG 3D features and construct the video features as described above."}, {"heading": "9.1.5 Youtube person data set", "text": "Similar to the experiments with the Smoothing Kernel feature, in this section we report on the results of experiments carried out with the Time Smoothed Kernel. Specifically, we used YouTube's personal data set [9] to identify people using a time-based kernel-smooth spare encoding; the data set contains 1910 sequences of 47 subjects; the architecture of this data set is similar [23]."}], "references": [{"title": "Local rademacher complexities", "author": ["P.L. Bartlett", "O. Bousquet", "S. Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Group sparse coding", "author": ["S. Bengio", "F. Pereira", "Y. Singer", "D. Strelow"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Learning efficient structured sparse models", "author": ["A. Bronstein", "P. Sprechmann", "G. Sapiro"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Combinatorial methods in density estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Sure independence screening for ultrahigh dimensional feature space", "author": ["J. Fan", "J. Lv"], "venue": "JRSS: B(Statistical Methodology),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A comparison of the lasso and marginal regression", "author": ["C.R. Genovese", "J. Jin", "L. Wasserman", "Z. Yao"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Local regression: Automatic kernel carpentry", "author": ["T. Hastie", "C. Loader"], "venue": "Statistical Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Face tracking and recognition with visual constraints in real-world videos", "author": ["M. Kim", "S. Kumar", "V. Pavlovic", "H. Rowley"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A spatio-temporal descriptor based on 3d-gradients", "author": ["A. Kl\u00e4ser", "M. Marsza lek", "C. Schmid"], "venue": "In BMVC,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Recognizing realistic actions from videos in the wild", "author": ["J. Liu", "J. Luo", "M. Shah"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Local regression and likelihood", "author": ["C. Loader"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Smoothing l1-penalized estimators for high-dimensional time-course data", "author": ["L. Meier", "P. B\u00fchlmann"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Sparse modeling with universal priors and learned incoherent dictionaries", "author": ["I. Ram\u0131rez", "F. Lecumberry", "G. Sapiro"], "venue": "Tech Report, IMA, University of Minnesota,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "JRSS:B,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "Information Theory, IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "The sample complexity of dictionary learning", "author": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Evaluation of local spatio-temporal features for action recognition", "author": ["H. Wang", "M.M. Ullah", "A. Klaser", "I. Laptev", "C. Schmid"], "venue": "In BMVC,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE PAMI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In CVPR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Supervised translation-invariant sparse coding", "author": ["J. Yang", "K. Yu", "T. Huang"], "venue": "In CVPR. IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Nonlinear learning using local coordinate coding", "author": ["K. Yu", "T. Zhang", "Y. Gong"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Nonlinear programming: a unified approach", "author": ["W.I. Zangwill"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1969}], "referenceMentions": [{"referenceID": 20, "context": "While efficient algorithms for such cases exist [22, 11], their scalability for large dictionaries remains a challenge.", "startOffset": 48, "endOffset": 56}, {"referenceID": 10, "context": "While efficient algorithms for such cases exist [22, 11], their scalability for large dictionaries remains a challenge.", "startOffset": 48, "endOffset": 56}, {"referenceID": 18, "context": "We further develop theory that extends the sample complexity result of [20] for dictionary learning using standard sparse coding to the smooth sparse coding case.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "2 Related work Our approach is related to the local regression method [13, 7].", "startOffset": 70, "endOffset": 77}, {"referenceID": 6, "context": "2 Related work Our approach is related to the local regression method [13, 7].", "startOffset": 70, "endOffset": 77}, {"referenceID": 13, "context": "More recent related work is [15] that uses smoothing techniques in high-dimensional lasso regression in the context of temporal data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "Another recent approach proposed by [26] achieves code locality by approximating data points using a linear combination of nearby basis points.", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "Marginal regression is a relatively old technique that has recently reemerged as a computationally faster alternative to lasso regression [5].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "See also [6] for a statistical comparison of lasso regression and marginal regression.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": ", [3, 8].", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": ", [3, 8].", "startOffset": 2, "endOffset": 8}, {"referenceID": 3, "context": "The smooth sparse coding setting leads to codes that represent a neighborhood rather than an individual sample and that have lower mean square reconstruction error (with respect to a given dictionary), due to lower estimation variance (see for example the standard theory of smoothed empirical process [4]).", "startOffset": 302, "endOffset": 305}, {"referenceID": 4, "context": "A comparison of the statistical properties of marginal regression and lasso is available in [5, 6].", "startOffset": 92, "endOffset": 98}, {"referenceID": 5, "context": "A comparison of the statistical properties of marginal regression and lasso is available in [5, 6].", "startOffset": 92, "endOffset": 98}, {"referenceID": 2, "context": "Note that the same approach could be used with structured regularizers too, for example [3, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": "Note that the same approach could be used with structured regularizers too, for example [3, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 5, "context": "In the linear regression setting, marginal regression performs much better with orthogonal data [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 17, "context": "In the context of sparse coding, this corresponds to having uncorrelated or incoherent dictionaries [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "We use the method of optimal directions update [17] to solve the above optimization problem.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "A sequence of such updates corresponding to step (i) and step (ii) converges to a stationary point of the optimization problem (this can be shown using Zangwill\u2019s theorem [27]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "We leverage the analysis for dictionary learning in the standard sparse coding setting by [20] and extend it to the smooth sparse coding setting.", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "We assume a dictionary of bounded babel function, which holds as a result of the relaxed orthogonality constraint used in the Algorithm 1 (see also [17]).", "startOffset": 148, "endOffset": 152}, {"referenceID": 3, "context": "By Young\u2019s inequality [4] we have, |Kh1 \u2217 (s1 \u2212 s2)|p \u2264 |Kh1 |1|s1 \u2212 s2|p, 1 \u2264 p \u2264 \u221e for any Lp integrable functions s1 and s2.", "startOffset": 22, "endOffset": 25}, {"referenceID": 18, "context": "From [20], we have that the the class F \u2032 \u03bb has \u01eb covers of size at most ( 4\u03bb \u01eb(1\u2212\u03b3)) dK .", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "4 n The above theorem follows from the previous covering number bound and the following lemma for generalization bound that is based on the result in [20] concerning | \u00b7 |\u221e covering numbers.", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "In the case of \u03ba > 0, it is possible to obtain faster rates of O(n\u22121) for smooth sparse coding, similar to derivations in [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 18, "context": "The above theorem follows from the covering number bound above and Proposition 22 from [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "We compare the proposed smooth sparse coding algorithm, standard sparse coding with lasso [11] and marginal regression updates respectively, with a relative reconstruction error \u2016X \u2212 D\u0302B\u0302\u2016F /\u2016X\u2016F convergence criterion.", "startOffset": 90, "endOffset": 94}, {"referenceID": 24, "context": "Concentrating on face and object recognition from static images, we evaluated the performance of the proposed approach along with standard sparse coding and LLC [26], another method for obtaining sparse features based on locality.", "startOffset": 161, "endOffset": 165}, {"referenceID": 22, "context": "Following [24] , we used the following approach for generating sparse image representation: we densely sampled 16 \u00d7 16 patches from images at the pixel level on a gird with step size 8 pixels, computed SIFT features, and then computed the corresponding sparse codes over a 1024-size dictionary.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "In fact, the reported performance is better than previous reported results using unsupervised sparse coding techniques [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "Similar to the static image case, we follow the standard approach for generating sparse representations for videos as in [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "We densely sample 16\u00d7 16\u00d7 10 blocks from the video and extract HoG-3d [10] features from the sampled blocks.", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "Previous approaches include sparse coding, vector quantization, and k-means on top of the HoG-3d feature set (see [21] for a comprehensive evaluation).", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "As indicated by Table 4, smooth sparse coding results in higher classification accuracy than previously reported state-of-the-art and standard sparse coding on both datasets (see [21, 12] for a description of the alternative techniques).", "startOffset": 179, "endOffset": 187}, {"referenceID": 11, "context": "As indicated by Table 4, smooth sparse coding results in higher classification accuracy than previously reported state-of-the-art and standard sparse coding on both datasets (see [21, 12] for a description of the alternative techniques).", "startOffset": 179, "endOffset": 187}, {"referenceID": 23, "context": "As in [25], we first compute the codes in both case based on patches and combine it with max-pooling to obtain the image level representation.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "For this case, we also compared to the more standard fused-lasso based approach [18].", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "10 [21] 92.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "2 [12] 72.", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "for example [9].", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "This motivated semi-supervised learning and transfer learning without labels [16] respectively.", "startOffset": 77, "endOffset": 81}, {"referenceID": 1, "context": "Other approach to handle a lower number of labeled samples include collaborative modeling or multi-task approaches which impose a shared structure on the codes for several tasks and use data from all the tasks simultaneously, for example group sparse coding [2].", "startOffset": 258, "endOffset": 261}, {"referenceID": 24, "context": "Table 6 shows the test set error rate and compares it to standard sparse coding and LLC [26].", "startOffset": 88, "endOffset": 92}], "year": 2012, "abstractText": "We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via non-parametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semi-supervised sparse coding.", "creator": "LaTeX with hyperref package"}}}