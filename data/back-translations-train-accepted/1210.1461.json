{"id": "1210.1461", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2012", "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound", "abstract": "The CUR matrix decomposition is an important extension of Nystr\\\"{o}m approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.", "histories": [["v1", "Thu, 4 Oct 2012 14:23:34 GMT  (105kb)", "http://arxiv.org/abs/1210.1461v1", "accepted by NIPS 2012"]], "COMMENTS": "accepted by NIPS 2012", "reviews": [], "SUBJECTS": "cs.LG cs.DM stat.ML", "authors": ["shusen wang", "zhihua zhang"], "accepted": true, "id": "1210.1461"}, "pdf": {"name": "1210.1461.pdf", "metadata": {"source": "CRF", "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound\u2217", "authors": ["Shusen Wang", "Zhihua Zhang", "Jian Li"], "emails": ["wss@zju.edu.cn", "zhzhang@zju.edu.cn", "lijian@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 121 0.14 61v1 [cs.LKeywords: Large-scale matrix calculations, low-level matrix approximation, decomposition of the CUR matrix, randomized algorithms"}, {"heading": "1. Introduction", "text": "Most efforts focus on the manipulation, understanding and interpretation of large-format data matrices. In many cases, matrix factorization methods are used to construct compressed and informative representations to facilitate computation and interpretation, and a basic approach is the way in which the unspoken individual values are decompressed (SVD), which finds the best approximation to a data matrix. SVD applications such as eigenface (Sirovich and Kirby, 1987, Turk and Pentland, 1991) and latent semantic analysis (Deerwester et al, 1990) are very successful, but the base vectors resulting from SVD have little concrete significance, making it very difficult for us to understand and interpret the data in question."}, {"heading": "2. Notations", "text": "For a matrix A = [aij], in which one (i) is the i-th row and one (j) is the j-th column. Let the i-th column and the i-th be the Frobenius norm. Furthermore, allow Im to denote the m-m identity matrix and 0 to denote the zero matrix, the size of which depends on the context. Let the i-th column be rank (A) and k-th column be written as A = 1-Ax-2."}, {"heading": "3. Previous Work in CUR Matrix Decomposition", "text": "This section discusses two recent developments in CUR algorithms: Section 3.1 introduces an additive error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative error CUR algorithms in Drineas et al. (2008)."}, {"heading": "3.1 The Linear-Time CUR Algorithm", "text": "The linear time CUR algorithm is proposed by Drineas et al. (2006). It is a highly efficient algorithm. Given a matrix A and a constant k < rank (A), the resulting CUR decomposition by scanning c = 64kB \u2212 4 columns and r = 4kB \u2212 2 rows of A and calculating an intersection matrix U fulfills the following additive error limit: A \u2212 CUR, F \u2264. Proposal 1 (The linear time CUR algorithm). In addition, the decomposition also fulfills the rank (CUR) \u2264 k. Here, we give the main results (Theorem 4 by Drineas et al., 2006) in the following position. Proposal 1 (The linear time CUR algorithm) Give a matrix A, Rm \u00d7 n, let us leave pi = a (i), 22K and qj = 22 / qm A \u00b2 probability UA, the probabilities A \u00b2 (time C, A \u00b2 and 4K) with the time (4K) probabilities."}, {"heading": "3.2 The Subspace Sampling CUR Algorithm", "text": "Drineas et al. (2008) proposed a two-stage randomized CUR algorithm that has a relative error limit. (h.p) In the first stage, the algorithm c tries columns of A to construct C, and in the second stage, it tries r lines of A and C simultaneously to construct R and U. In the first stage, the sample probabilities are proportional to the square 2 norm of the lines of UC, k. Therefore, it is called the \"subspace sampling algorithm.\" Here, we show the main results of the subspace sampling algorithms in the following proposition. (The subspace sampling series of CUR algorithm) Proposition 2 (The subspace sampling series of CUR algorithm) Proposition 2 (The subspace sampling series of CUR algorithm) becomes an optimal one (Rm \u00b7 n and integer series of CUR, Ugthm series of CUR)."}, {"heading": "4. Theoretical Backgrounds", "text": "Section 4.1 deals with the relationship between the problem of column selection and the problem of decomposition of the CUR matrix. Section 4.2 introduces an almost optimal column selection algorithm with relative error. Our proposed CUR algorithm is motivated and partly based on the nearly optimal column selection algorithm."}, {"heading": "4.1 Connections between Column Selection and CUR Matrix Decomposition", "text": "Column selection is a well-established problem that has been extensively studied in the literature: (Frieze et al., 2004, Deshpande et al., 2006, Drineas et al., 2008, Deshpande and Rademacher, 2010, Boutsidis et al., 2011b, Guruswami and Sinop, 2012) Column selection aims to select the c columns of A to construct C columns of A so that A-CC and A-F reach the minimum. As there are (nc) possible choices for the construction of C, selecting the best subset is a difficult problem. In recent years, many polynomial-time approximate algorithms have been proposed, among which we are particularly interested in these algorithms with relative error limits."}, {"heading": "4.2 The Near-Optimal Column Selection Algorithm", "text": "In recent years the number of algorithms in the USA has multiplied. < / M > M (1 + o (1)) D (1 + 1) D (1 + 1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1) D (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) D (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) D (1 D) (1 D) (1 D) (1 D) D (1 D) (1 D) D (1 D) (1 D) (1 D) (1 D) D (1 D) (1 D) (1 D) (1 D) D (1 D) (1 D) D (1 D) (1 D) (1 D) (1 D) (1 D) D (1 D) (1 D) (1 D) D (1 D) (1 D) (1 D (1 D) (1 D) (1 D) (1 D) (1 D (1 D) D (1 D) (1 D) (1 D) (1 D) D (1 D) (1 D (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1 D) (1) D (1 D) (1 D (1 D) (1 D) (1 D (1 D) (1 D) (1 D) (1 D (1 D) (1 D)"}, {"heading": "5. Main Results", "text": "In this section we develop a novel CUR algorithm, which we call the fast CUR algorithm due to its lower time complexity compared to SVD. We describe the procedure in Algorithm 1 and give theoretical analysis in Theorem 9. The main results of our work are formally presented in this section in three theorems, the proofs of which are put aside in Appendix B. Theorem 9 is based on Lemma 3 and Theorem 8, and Theorem 8 is based on Theorem 7. Theorem 7 is a generalization of Lemma 6, and Theorem 8 is a generalization of Lemma 3."}, {"heading": "5.1 Adaptive Sampling", "text": "The adaptive sampling algorithm with relative error was established in Theorem 2.1 by Deshpande et al. (2006). The algorithm is based on the following idea: After selecting a portion of columns from A to C1 by an arbitrary algorithm, the algorithms randomly sampled additional c2 columns according to the residual A \u2212 C1C \u2020 1A. Boutsidis et al. (2011a) used the adaptive sampling algorithm to reduce the remainder of the dual sparsification algorithm and achieved a (1 +) relative error ratio. Here, we prove a new boundary for the same adaptive sampling algorithm. Interestingly, this new boundary is a generalization of the original algorithm in Theorem 2.1 by Deshpande et al. (2006)."}, {"heading": "5.2 The Fast CUR Algorithm", "text": "Based on the randomized SVD algorithm of Lemma 4, the dual set-sparsification algorithm of Lemma 5, and the adaptive sampling algorithm of Theorem 7, we develop a randomized algorithm to solve the second stage of the CUR problem. We present the results of the algorithm in the following theorem.Theorem 8 (The Fast Row Selection Algorithm) Given a matrix A (1 + m) and a matrix C (Rm), rank (C) = rank (CC \u2020 A) = rank (A) = rank (A) = rank (A), and a target rank (A)."}, {"heading": "6. Empirical Analysis", "text": "In this section, we perform empirical comparisons between the CUR algorithms of relative error on multiple datasets. We specify the relative error rate and runtime of each algorithm on each dataset. The relative error ratio is defined by relative error ratio (CUR, F, A, Ak, F), where k is a set target value."}, {"heading": "6.1 Datasets", "text": "The Redrock and Edinburgh (Agarwala, 2007) are two large-format natural images. Arcene and Dexter both come from UCI datasets (Frank and Asuncion, 2010). Arcene is a biology dataset with 900 instances and 10,000 attributes. Dexter is a bag of words with a 2,000 vocabulary and 2600 documents. The PicasaWeb dataset (Wang et al., 2012) contains 6.8 million PicasaWeb images. We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50,000 images; the features provided by Wang et al. (2012) are all 3,000 dimensions. Each dataset is actually presented as a data matrix to which we apply the CUR attributes."}, {"heading": "6.2 Setup", "text": "We implement the subspace scanning algorithm and our fast CUR algorithm in MATLAB 7.10.0. We do not compare it with the linear time CUR algorithm for the following reason. There is an implicit projection operation in the linear time CUR algorithm, so the result does not rank (CUR) \u2264 k. However, this inequality does not apply to the subspace scanning algorithm and the fast CUR algorithm. Therefore, the comparison of the design error between the three CUR algorithms for the linear time CUR algorithm is very unfair. In fact, the design error of the linear time CUR algorithm is much worse than the other two algorithms. We conduct experiments on a workstation with 12 Intel Xeon 3.47 GHz CPUs, 12GB memory and Ubuntu 10.04 systems, in which the experiments in Dras are set to c, \u03b1 and \u03b1, respectively (this and \u03b1), and \u03b1, respectively."}, {"heading": "6.3 Result Analysis", "text": "The results show that the fast CUR algorithm has a much lower relative error ratio than the subspace scanning algorithm. The experimental results are well in line with our theoretical analyses in Section 5. In terms of runtime, the fast CUR algorithm is more efficient when c and r are small. However, when c and r become large, the fast CUR algorithm becomes less efficient. This is because the temporal complexity of the fast CUR algorithm is linear, with large c and r implying small numbers. However, the purpose of CUR is to select a small number of columns and rows from the data matrix, i.e. c \u00b2 n and r \u00b2 m. Therefore, we are not interested in the cases where c and r are large compared to m and n, e.g. k = 20 and \u03b1 = 10."}, {"heading": "7. Discussions", "text": "In this paper, we have proposed a novel randomized algorithm for the CUR matrix decomposition problem. (<) This algorithm is faster, more scalable, and more accurate than the state-of-the-art algorithm, i.e., the subspace sampling algorithm. (To achieve the same relative error rate, the subspace sampling algorithm requires only c = 2 (1 + o (1)) columns and r = 2 (1 + o (1)) rows to achieve the same relative error rate, the subspace sampling algorithm requires c = O (kp \u2212 2 log k) columns and r = O (cp \u2212 2 log c) rows selected from the original matrix. Our algorithm also beats the subspace sampling algorithms in time complexity."}, {"heading": "Appendix A. The Dual Set Sparsification Algorithm", "text": "For the sake of completeness, we add here the dual set sparsification algorithm and describe some implementation details. The dual set sparsification algorithms are deterministic algorithms established in Boutsidis et al. (2011a). The fast CUR algorithm also calls up the dual set spectral Frobenius sparsification algorithm (Lemma 13 in Boutsidis et al., 2011a) in both stages. We show this algorithm in Algorithm 2 and its limitations in Lemma 10.Lemma 10 (Dual Set Spectral-Frobenius Sparsification). Leave U = {x1, \u00b7 \u00b7 \u00b7, xn}."}, {"heading": "Appendix B. Proofs", "text": "B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B."}], "references": [{"title": "Efficient gradient-domain compositing using quadtrees", "author": ["Aseem Agarwala"], "venue": "SIGGRAPH", "citeRegEx": "Agarwala.,? \\Q2007\\E", "shortCiteRegEx": "Agarwala.", "year": 2007}, {"title": "Generalized Inverses: Theory and Applications", "author": ["Adi Ben-Israel", "Thomas N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "Ben.Israel and Greville.,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville.", "year": 2003}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "CoRR, abs/1103.0995,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Near optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman"], "venue": "Journal of The American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["Amit Deshpande", "Luis Rademacher"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Deshpande and Rademacher.,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher.", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["Amit Deshpande", "Luis Rademacher", "Santosh Vempala", "Grant Wang"], "venue": "Theory of Computing,", "citeRegEx": "Deshpande et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2006}, {"title": "Pass-efficient algorithms for approximating large matrices", "author": ["Petros Drineas"], "venue": "Proceeding of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms,", "citeRegEx": "Drineas.,? \\Q2003\\E", "shortCiteRegEx": "Drineas.", "year": 2003}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["Petros Drineas", "Michael W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Fast monte-carlo algorithms for finding low-rank approximations", "author": ["Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "A theory of pseudoskeleton approximations", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Pseudo-skeleton approximations by matrices of maximal volume", "author": ["S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov"], "venue": "Mathematical Notes,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["Venkatesan Guruswami", "Ali Kemal Sinop"], "venue": "In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Guruswami and Sinop.,? \\Q2012\\E", "shortCiteRegEx": "Guruswami and Sinop.", "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Computer Science Theory for the Information Age", "author": ["John Hopcroft", "Ravi Kannan"], "venue": null, "citeRegEx": "Hopcroft and Kannan.,? \\Q2012\\E", "shortCiteRegEx": "Hopcroft and Kannan.", "year": 2012}, {"title": "Vector algebra in the analysis of genome-wide expression data", "author": ["Finny G. Kuruvilla", "Peter J. Park", "Stuart L. Schreiber"], "venue": "Genome Biology,", "citeRegEx": "Kuruvilla et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kuruvilla et al\\.", "year": 2002}, {"title": "Object recognition from local scale-invariant features", "author": ["David G. Lowe"], "venue": "In Proceedings of the International Conference on Computer Vision, ICCV", "citeRegEx": "Lowe.,? \\Q1999\\E", "shortCiteRegEx": "Lowe.", "year": 1999}, {"title": "Divide-and-conquer matrix factorization", "author": ["Lester Mackey", "Ameet Talwalkar", "Michael I. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mackey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["Michael W. Mahoney", "Petros Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Mahoney and Drineas.,? \\Q2009\\E", "shortCiteRegEx": "Mahoney and Drineas.", "year": 2009}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["Thomas Serre", "Lior Wolf", "Stanley Bileschi", "Maximilian Riesenhuber", "Tomaso Poggio"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Serre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Serre et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Applications of SVD such as eigenface (Sirovich and Kirby, 1987, Turk and Pentland, 1991) and latent semantic analysis (Deerwester et al., 1990) have been illustrated to be very successful.", "startOffset": 119, "endOffset": 144}, {"referenceID": 20, "context": "The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009).", "startOffset": 129, "endOffset": 156}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.", "startOffset": 4, "endOffset": 288}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound.", "startOffset": 4, "endOffset": 1620}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows.", "startOffset": 4, "endOffset": 1704}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel.", "startOffset": 4, "endOffset": 1930}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m\u00d7 n matrix A and a target rank k \u2264 min{m,n}, the state-of-the-art CUR algorithm \u2014 the subspace sampling algorithm in Drineas et al. (2008) \u2014 requires exactly O(k4\u01eb\u22126) rows or O(k\u01eb\u22124 log k) rows in expectation to achieve (1 + \u01eb) relative-error ratio w.", "startOffset": 4, "endOffset": 2276}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m\u00d7 n matrix A and a target rank k \u2264 min{m,n}, the state-of-the-art CUR algorithm \u2014 the subspace sampling algorithm in Drineas et al. (2008) \u2014 requires exactly O(k4\u01eb\u22126) rows or O(k\u01eb\u22124 log k) rows in expectation to achieve (1 + \u01eb) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices. In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008). The rest of this paper is organized as follows.", "startOffset": 4, "endOffset": 2908}, {"referenceID": 7, "context": "in (Drineas et al., 2008, Mahoney and Drineas, 2009) has well shown this viewpoint; that is, the vector [(1/2)age\u2212 (1/ \u221a 2)height+ (1/2)income], the sum of the significant uncorrelated features from a dataset of people\u2019s features, is not particularly informative. Kuruvilla et al. (2002) have also claimed: \u201cit would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.\u201d Therefore, it is of great interest to represent a data matrix in terms of a small number of actual columns and/or actual rows of the matrix. The CUR matrix decomposition provides such techniques, and it has been shown to be very useful in high dimensional data analysis (Mahoney and Drineas, 2009). Given a matrix A, the CUR technique selects a subset of columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and computes a matrix U such that \u00c3 = CUR best approximates A. The typical CUR algorithms (Drineas, 2003, Drineas et al., 2006, 2008) work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2 does row selection from A and C simultaneously. Thus, implementing Stage 2 is much more difficult than doing Stage 1. The CUR matrix decomposition problem is widely studied in the literature (Goreinov et al., 1997a,b, Tyrtyshnikov, 2000, Drineas, 2003, Drineas and Mahoney, 2005, Drineas et al., 2006, 2008, Mahoney and Drineas, 2009, Mackey et al., 2011, Hopcroft and Kannan, 2012). Among the existing work, several recent work are of particular interest. Drineas et al. (2006) proposed a CUR algorithm with additive-error bound. Later on, Drineas et al. (2008) devised randomized CUR algorithms with relative error by sampling sufficiently many columns and rows. Particularly, the algorithm has (1 + \u01eb) relative-error ratio with high probability (w.h.p.). Recently, Mackey et al. (2011) established a divide-and-conquer method which solves the CUR problem in parallel. Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be chosen. For example, for an m\u00d7 n matrix A and a target rank k \u2264 min{m,n}, the state-of-the-art CUR algorithm \u2014 the subspace sampling algorithm in Drineas et al. (2008) \u2014 requires exactly O(k4\u01eb\u22126) rows or O(k\u01eb\u22124 log k) rows in expectation to achieve (1 + \u01eb) relative-error ratio w.h.p. Moreover, the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is, O(min{mn2, nm2}).1 The algorithms are therefore impractical for large-scale matrices. In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory and experiments. In particular, we show in Theorem 9 a novel randomized CUR algorithm with lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR algorithm in Drineas et al. (2008). The rest of this paper is organized as follows. Section 2 lists some notations that will be used in this paper and Section 3 reviews two classes of CUR algorithms. Section 4 mainly introduces a column selection algorithm to which our work is closely related. Section 5 describes and analyzes our novel CUR algorithm. Section 6 empirically compares our proposed algorithm with the state-of-the-art algorithm. All proofs are deferred to Appendix B. 1. Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time, they are all numerical unstable. See Halko et al. (2011) for more discussions.", "startOffset": 4, "endOffset": 3513}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003).", "startOffset": 77, "endOffset": 108}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXUXA \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y \u2208 Rm\u00d7n is the projection of A onto the row space of Y. Finally, given an integer k \u2264 p, we define the matrix \u03a0X,k(A) \u2208 Rm\u00d7n as the best approximation to A within the column space of X that has rank at most k. We have \u03a0X,k(A) = X\u1e90 where \u1e90 = argminrank(Z)\u2264k \u2016A\u2212XZ\u2016F . We also have that \u2016A\u2212XXA\u2016F \u2264 \u2016A\u2212\u03a0X,k(A)\u2016F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.", "startOffset": 78, "endOffset": 755}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXUXA \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y \u2208 Rm\u00d7n is the projection of A onto the row space of Y. Finally, given an integer k \u2264 p, we define the matrix \u03a0X,k(A) \u2208 Rm\u00d7n as the best approximation to A within the column space of X that has rank at most k. We have \u03a0X,k(A) = X\u1e90 where \u1e90 = argminrank(Z)\u2264k \u2016A\u2212XZ\u2016F . We also have that \u2016A\u2212XXA\u2016F \u2264 \u2016A\u2212\u03a0X,k(A)\u2016F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008). 3.", "startOffset": 78, "endOffset": 841}, {"referenceID": 1, "context": "Furthermore, let A \u2020 = UA,\u03c1\u03a3 \u22121 A,\u03c1V T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003). Given matrices A \u2208 Rm\u00d7n, X \u2208 Rm\u00d7p, and Y \u2208 Rq\u00d7n, XX\u2020A = UXUXA \u2208 Rm\u00d7n is the projection of A onto the column space of X, and AY\u2020Y = AVYV T Y \u2208 Rm\u00d7n is the projection of A onto the row space of Y. Finally, given an integer k \u2264 p, we define the matrix \u03a0X,k(A) \u2208 Rm\u00d7n as the best approximation to A within the column space of X that has rank at most k. We have \u03a0X,k(A) = X\u1e90 where \u1e90 = argminrank(Z)\u2264k \u2016A\u2212XZ\u2016F . We also have that \u2016A\u2212XXA\u2016F \u2264 \u2016A\u2212\u03a0X,k(A)\u2016F . 3. Previous Work in CUR Matrix Decomposition This section discusses two recent developments of the CUR algorithms. Section 3.1 introduces an additive-error CUR algorithm in Drineas et al. (2006), and Section 3.2 describes two relative-error CUR algorithms in Drineas et al. (2008). 3.1 The Linear-Time CUR Algorithm The linear-time CUR algorithm is proposed by Drineas et al. (2006). It is a highly efficient algorithm.", "startOffset": 78, "endOffset": 943}, {"referenceID": 3, "context": "2 The Subspace Sampling CUR Algorithm Drineas et al. (2008) proposed a two-stage randomized CUR algorithm which has a relativeerror bound w.", "startOffset": 38, "endOffset": 60}, {"referenceID": 15, "context": "The near-optimal algorithm has three steps: the approximate SVD via random projection (Halko et al., 2011), the dual set sparsification algorithm (Boutsidis et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": ", 2011a), and the adaptive sampling algorithm (Deshpande et al., 2006).", "startOffset": 46, "endOffset": 70}, {"referenceID": 2, "context": "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2k\u01eb\u22121(1 + o(1)) columns to achieve the expected relative-error ratio (1 + \u01eb).", "startOffset": 56, "endOffset": 81}, {"referenceID": 2, "context": "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2k\u01eb\u22121(1 + o(1)) columns to achieve the expected relative-error ratio (1 + \u01eb). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = k\u01eb\u22121 columns are selected to achieve the (1 + \u01eb) ratio.", "startOffset": 56, "endOffset": 239}, {"referenceID": 2, "context": "2 The Near-Optimal Column Selection Algorithm Recently, Boutsidis et al. (2011a) proposed a randomized algorithm which selects only c = 2k\u01eb\u22121(1 + o(1)) columns to achieve the expected relative-error ratio (1 + \u01eb). Boutsidis et al. (2011a) also proved the lower bound of the column selection problem; that is, at least c = k\u01eb\u22121 columns are selected to achieve the (1 + \u01eb) ratio. Thus this algorithm is near optimal. Though an optimal algorithm recently proposed by Guruswami and Sinop (2012) achieves the the lower bound, the optimal algorithm is quite inefficient compared with the near-optimal algorithm.", "startOffset": 56, "endOffset": 491}, {"referenceID": 15, "context": "(2011a) employed an approximation SVD algorithm (Halko et al., 2011) to speedup computation.", "startOffset": 48, "endOffset": 68}, {"referenceID": 2, "context": "Since SVD is time consuming, Boutsidis et al. (2011a) employed an approximation SVD algorithm (Halko et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 2, "context": "The second step of the near-optimal column selection algorithm is the dual set sparsification proposed by Boutsidis et al. (2011a). When ones take A and the top k (approximate) right singular vectors of A as inputs, the dual set sparsification algorithm can deterministically selects c1 columns of A to construct C1.", "startOffset": 106, "endOffset": 131}, {"referenceID": 6, "context": "After sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error.", "startOffset": 106, "endOffset": 130}, {"referenceID": 6, "context": "After sampling c1 columns of A, the near-optimal column selection algorithm uses the adaptive sampling of Deshpande et al. (2006) to select c2 columns of A to further reduce the construction error. We present Theorem 2.1 in Deshpande et al. (2006) in the following lemma.", "startOffset": 106, "endOffset": 248}, {"referenceID": 4, "context": "1 of Deshpande et al. (2006). The algorithm is based on the following idea: after selecting a proportion of columns from A to form C1 by an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the residual A \u2212 C1C\u20201A.", "startOffset": 5, "endOffset": 29}, {"referenceID": 2, "context": "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + \u01eb) relative-error ratio.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + \u01eb) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.", "startOffset": 0, "endOffset": 354}, {"referenceID": 2, "context": "Boutsidis et al. (2011a) used the adaptive sampling algorithm to decrease the residual of the dual set sparsification algorithm and obtained an (1 + \u01eb) relative-error ratio. Here we prove a new bound for the same adaptive sampling algorithm. Interestingly, this new bound is a generalization of the original one in Theorem 2.1 of Deshpande et al. (2006). In other words, Theorem 2.1 of Deshpande et al. (2006) is a direct corollary of our following theorem when C = Ak.", "startOffset": 0, "endOffset": 410}, {"referenceID": 2, "context": ", Theorem 5 of Boutsidis et al. (2011a), is a special case of Theorem 8 when C = Ak.", "startOffset": 15, "endOffset": 40}, {"referenceID": 0, "context": "The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images.", "startOffset": 26, "endOffset": 42}, {"referenceID": 21, "context": "We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 18, "context": ", 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al.", "startOffset": 30, "endOffset": 42}, {"referenceID": 0, "context": "The Redrock and Edinburgh (Agarwala, 2007) are two large size natural images. Arcene and Dexter are both from the UCI datasets (Frank and Asuncion, 2010). Arcene is a biology dataset with 900 instances and 10000 attributes. Dexter is a bag of words dataset with a 20000vocabulary and 2600 documents. PicasaWeb image dataset (Wang et al., 2012) contains 6.8 million PicasaWeb images. We use the HMAX features (Serre et al., 2007) and the SIFT features (Lowe, 1999) of the first 50000 images; the features provided by Wang et al. (2012) are all of 3000 dimensions.", "startOffset": 27, "endOffset": 535}, {"referenceID": 7, "context": "According to the analysis in Drineas et al. (2008) and this paper, k, c, and r should be integers much less than m and n.", "startOffset": 29, "endOffset": 51}, {"referenceID": 2, "context": "First, what is the lower bound for the CUR problem? Second, is there any algorithm achieving such a lower bound? Boutsidis et al. (2011b) proved a lower bound for the column selection problem: \u2016A\u2212CC\u2020A\u20162F \u2016A\u2212Ak\u2016 2 F \u2265 1+ kc .", "startOffset": 113, "endOffset": 138}, {"referenceID": 2, "context": "The dual set sparsification algorithms are deterministic algorithms established in Boutsidis et al. (2011a). The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm (Lemma 13 in Boutsidis et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 2, "context": "Here we would like to mention the implementation of Algorithm 2, which is not described by Boutsidis et al. (2011a) in details.", "startOffset": 91, "endOffset": 116}, {"referenceID": 2, "context": "In order to stick to the column space convention of Boutsidis et al. (2011a), we prove Theorem 11 instead of Theorem 7.", "startOffset": 52, "endOffset": 77}, {"referenceID": 2, "context": "2 The Proof of Theorem 8 Boutsidis et al. (2011a) proposed a randomized algorithm which achieves the expected relative-error bound in Lemma 13.", "startOffset": 25, "endOffset": 50}, {"referenceID": 2, "context": "Lemma 13 (Boutsidis et al. (2011a), Theorem 4) Given a matrix A \u2208 Rm\u00d7n of rank \u03c1, a target rank 2 \u2264 k < \u03c1, and 0 < \u01eb0 < 1, there is a randomized algorithm to select c1 > k columns of A and form a matrix C1 \u2208 Rm\u00d7c1 such that E\u2016A\u2212C1C\u20201A\u2016F \u2264 (1 + \u01eb0) (", "startOffset": 10, "endOffset": 35}, {"referenceID": 15, "context": "Proof This randomized algorithm has three steps: approximate SVD via randomized projection (Halko et al., 2011), deterministic column selection via dual set sparsification algorithm (Boutsidis et al.", "startOffset": 91, "endOffset": 111}], "year": 2012, "abstractText": "The CUR matrix decomposition is an important extension of Nystr\u00f6m approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR algorithm with an expected relative-error bound. The proposed algorithm has the advantages over the existing relative-error CUR algorithm that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory. Finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}