{"id": "1210.2085", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2012", "title": "Privacy Aware Learning", "abstract": "We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure.", "histories": [["v1", "Sun, 7 Oct 2012 18:27:03 GMT  (63kb)", "https://arxiv.org/abs/1210.2085v1", "51 pages"], ["v2", "Thu, 10 Oct 2013 17:53:36 GMT  (75kb)", "http://arxiv.org/abs/1210.2085v2", "60 pages"]], "COMMENTS": "51 pages", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["john c duchi", "michael i jordan", "martin j wainwright"], "accepted": true, "id": "1210.2085"}, "pdf": {"name": "1210.2085.pdf", "metadata": {"source": "CRF", "title": "Privacy Aware Learning", "authors": ["John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright"], "emails": ["jduchi@eecs.berkeley.edu", "jordan@eecs.berkeley.edu", "wainwrig@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 121 0.20 85v2 [st at.M L] 10 Oct 2"}, {"heading": "1 Introduction", "text": "Natural tensions between learning and privacy arise whenever a learner needs to aggregate data across multiple individuals. However, the learner wants to make optimal use of each data point, while the data providers want to limit the detailed burden on either the learner or other individuals. However, characterizing such tensions in the form of quantitative trade-offs is of great use: it can influence the public discourse around the design of systems that learn from data, and the trade-offs can be exploited as controllable degrees of freedom whenever such a system is used. In this work, we approach this problem from the perspective of statistical decision theory. The decision theory perspective offers a number of advantages. First, the use of loss functions and risk functions provides a compelling formal basis for the definition of \"learning,\" which dates back to forest [46] and which has seen continuous development in the context of research on machine learning over the past two decades."}, {"heading": "1.1 Prior work", "text": "There is a long history of research at the interface of privacy and statistics going back at least to the 1960s, when Warner proposed privacy-preserving methods of data collection, and to later work related to the collection and presentation of tabular data (e.g., [20]). More recently, there has been a great deal of computationally oriented work on data protection [17, 15, 51, 48, 24, 11, 7, 42]. We review some of the key terms in this section, but cannot hope to do justice to the large body of relevant work, and refer the reader to the comprehensive collection of dwork [15] and the statistical treatment by Wasserman and Zhou [48] for background and references."}, {"heading": "1.2 Our setting", "text": "The goal of many types of privacy is to guarantee that the output of information about each Xi date based on the data cannot be used exclusively to discover information about privacy in statistical literature [47], relying only on the communication of some hidden views about each true sample of Xi. In this setting, for example, the natural variant of alpha-differential privacy (2) is the non-interactive (in the sense that Zi depends only on Xi and not on other private variables) local privacy warranesup S sup x, x \u00b2 Q (Zi-S | Xi x)."}, {"heading": "1.3 Outline and techniques", "text": "We spend the rest of the work deriving the boundaries (5) and (6). Our path to achieving these boundaries is based on a two-part analysis. First, we consider the saddle points of mutual information I (X; Z) when we consider these saddle performances as a function of the P of X distribution and the Q (\u00b7 X) of Z conditional distribution, under natural constraints that still allow estimation. We consider related saddle performances for different private conditional distributions. After calculating these saddle performances, we can apply information theory techniques to achieve lower limits for estimation and optimization [49, 1] to prove the results of the form (5a) or (6a). Our upper saddle performances then follow the application of known convergence rates for computer-efficient methods, such as stochastic gradient and mirror descent of algorithms [36, 37]. We provide complete evidence - except for the technical results - that are transferred to our appeals and a complete one."}, {"heading": "2 Problem Formulation", "text": "We start with a formal description of the communication protocol through which information about the X random variables is transmitted to Method M. Then we define the notion of optimal local privacy, which is examined in this paper, and the Minimax framework, in which we present our main results."}, {"heading": "2.1 Communication protocol", "text": "In this paper, we focus on statistical learning processes that have access to data through the undergradients of the loss functions. Formally speaking, method M gets access to a random vector Zi at each turn, which shows that the inclusion of the undergradient (7) is not only intuitively attractive, but in a certain sense necessary. In detail, our communication protocol consists of the following three steps: \u2022 method M sends the parameter vector vector vector vector vector Xi; \u2022 owner i computes a subgradient g, which is not only intuitively appealing, but in a certain sense necessary. \u2022 The communication protocol consists of the following three steps: \u2022 method M sends the parameter vector vector vector vector vector g Xi; \u2022 owner i computes a subgradient g (Xi, TB) to be communicated privately."}, {"heading": "2.2 Optimal local privacy", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "2.3 Minimax error", "text": "Considering an estimate based on n samples X from a distribution P, we evaluate their quality with respect to the risk function (1), i.e. R (\u03b8) = E [(X, \u0445). In this section, we describe the Minimax framework for obtaining uniform boundaries across all possible estimates. Let M designate any statistical procedure or method based on stochastic gradient samples, and let n indicate the output of M after obtaining n such samples. The excess risk of method M to risk R (\u03b8) after obtaining n sample gradients is n (M, \u0445, \u0445, P): = R (2001) \u2212 inf."}, {"heading": "3 Optimal Learning Rates and Tradeoffs", "text": "Once the basic framework has been established, we will now turn to the statements of our main results. We will begin by imposing certain (weak) conditions on the families of the loss functions considered, and then turn to the main results of this section (theories 1 and 2, which apply to information-based privacy, and theories 3-5, which apply to \u03b1-differential privacy) and some of their consequences (conclusions 1, 2 and 3). Having described in section 4 the optimal distributions for maintaining privacy, we will provide evidence of the results in this section in section 5."}, {"heading": "3.1 Families of loss functions and stochastic gradient methods", "text": "To illustrate this definition, let us give a few examples: we assume that our loss functions obey certain natural black ice conditions."}, {"heading": "3.2 Minimax error bounds under privacy", "text": "We will now present our most important theorems and discuss some of their consequences. All the evidence will be put aside in section 5."}, {"heading": "3.2.1 Minimax errors with mutual information-based privacy", "text": "Our first two main results consider the privacy mechanisms Q as an optimal local private life, if we first refer the theorems to their dependence on the geometry of the subdifferential sets (in which the subgradients live); in fact, we assume that these decisions must correspond to certain mutual guarantees of information on privacy. Our first theory refers to the class of (L,) loss functions given in Definition 3. For this theorem, we assume that the amount to which the disturbed data must belong to Z is [\u2212 M, M] d, where M \u00b2 d, where M \u00b2 L \u00b2 n, this corresponds to the notation of Definition 1, C = [\u2212 L] d and D = [\u2212 M \u00b2 d, M \u00b2 d] d. We give two variants of the first theorem, as one version gives some Sharper results for an important specific case."}, {"heading": "3.2.2 Minimax errors under differential privacy", "text": "We focus on two privacy settings: in the first (Theorem 3), we assume that the communication with respect to privacy is optimal, as defined in Definition 2. (3) For the second two results, we change the setting slightly, assuming only that the mechanism by which the private quantity Zi is communicated to Method M is differently private and non-interactive. (3) Optimal local differential privacy We start with the result that the optimal local differential privacy is assumed. We use the same collection of loss functions L as in Theorem 1, that is, in Theorem. (L) -loss functions also assume that the set to which the affected data belongs. [\u2212 M] d, although the specific value of M \u00b2 is not important for the statement of theorem.Theorem 3."}, {"heading": "4 Optimal privacy-preserving distributions", "text": "In this section, we examine the conditions for a Q * distribution to obtain the optimal local privacy according to definitions 1 and 2. Our results can be considered as distortion theorems [23, 8, 10] (with source P and channel Q) for certain compact alphabets, although, as far as we know, they are all new. Therefore, we refer to the conditional distribution Q, which is designed to preserve the privacy of the data through communication of Z, interchangeable as the data preserving distribution or channel distribution. Note that since we want to bind I (X; Z) for general losses as covered by the definitions of source P (C) and the communication chain Q (C, D) in equality (10a) and (10b), we must address the case when (X; Z) a general loss occurs, as in the definitions of source P (C) and the communication chain Q (C) and the communication chain Q (C) and the communication chain Q (C, D)."}, {"heading": "4.1 General saddle point characterizations", "text": "We start with a general characterization by first defining the types of sets C and D, which we place in our characterization of Q = Q = lowered. Such sets are reasonable for many applications (recall section 3.1). We focus on the case if the compact sets C and D are (appropriately symmetrical) norms: Definition 4. Let C = Rd create a compact convex sets with extreme points Ui = Rd, i \u00b2 I for any index set I. Then C is a rotation invariant through their extreme points, if privacy 2 = E \u00b2 2 for each i, j \u00b2 and for each uniform matrix U so that Uui = uj for some i = j, then UC = C. Some examples of convex sets are rotation invariant through their extreme points, if privacy 2 = E \u00b2 2 for each i, j \u00b2 and for each uniform matrix U, so that Uui = C = some invaric points for their rotation points, then Sexi = some examples for their rotation points, if privacy 2 = E \u00b2 for each i, and for each of their extreme matrix U matrix, so that Uui = some invaric points for some invarii = some invarii = some examples for their rotation points."}, {"heading": "4.2 Specific saddle point computations", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "4.3 Saddle points for differentially private communication", "text": "Our final result in this section characterizes segments for distributions that fulfill the definition. \u2212 k = > Q = > Q = > Q = > q = q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = ="}, {"heading": "5 Proofs of Statistical Rates", "text": "In this section we will prove theorems 1-5 and conclusions 1, 2 and 3. Our proofs are based on classical information theoretical techniques from the statistical minimax theory [49, 50] and also use some additional results based on Agarwal et al. [1]. At a high level, our approach consists of the following steps. Starting with an appropriately selected finite quantity V, we assign to each member a risk function Rv. The resulting recording of the risk functions {Rv} v-V is selected so that they represent \"separate\" points in the set V, which means that if \u03b8 is a point that approximately minimizes the function Rv, then for each w 6 = v, the point \u03b8 cannot also be an approximate minimizer of Rw. This separation property allows us to conclude that the statistical estimate implies the existence of a test method that differs v from w for w 6 = v."}, {"heading": "5.1 Reduction to testing", "text": "We start by describing the reduction, which indicates the Minimax error rate of 23%. (This assumes a pre-defined collection of risk capacity (V). (V) We expect the risk capacity (V) to be shortened from 1% (V). (V) We expect the risk capacity (V) to be shortened from 1% (V). (V) We expect the risk capacity (V) to be shortened from 1% (V). (V) We expect the risk capacity (V) to be shortened from 1% (V). (V) We expect the risk capacity (V) to be shortened from 1% (V). (V) We expect the risk capacity (V) to be shortened from 1% (V)."}, {"heading": "5.2 Proof of Theorem 1", "text": "We provide the most detailed evidence for this theorem, as it represents the very blueprint by which we prove the other results."}, {"heading": "5.2.1 Constructing well-separated losses", "text": "The first step in the verification of our minimax limits is to construct a family of well separated risks. < p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p"}, {"heading": "5.2.2 Bounding the mutual information", "text": "As outlined in Section 5.1, the second step in our subordinate proofs is to bind the mutual information (Z1,.., Zn; V), with Zi being the private views available to the learning method. At this point, we provide the reciprocal information boundaries for the linear loss family (Z1,.., Zn; V) and the median-based losses (Zi). Each of these reciprocal information boundaries - and our subsequent boundaries for mutual information - we continue by preferring the problem of estimating the mutual information I (Z; V | \u03b8) for a single randomized gradient sample Z. Then, a careful calculation of the distribution of Z | V leads to the final inequalities. Since the evidence is somewhat long and technical, we move it to Appendix B. Lemma 4. Let V be uniformly assigned to the random principle of V = {\u00b1 ei} di = 1. Let X tie the distribution (26) to V = v and assume that Appendix B < Z."}, {"heading": "5.2.3 Applying testing inequalities", "text": "After determining the two families of loss functions, we consider and the resulting reciprocal information boundaries, it remains to be seen whether we perform Lemma 1 and an examination of inequality. We begin by examining the part (a) of theory 1 (a) We divide the proof of the part (a) of theory into two parts: one assuming the dimension d + 9 and the other assuming d < 9. For the first, we first use Fano's inequality (24), while for the second an application of Le Cam's method (25) completes the result. For both results, we use the median type loss (X, 5) = L'rX \u2212 rX \u2212 vD of the beginning of the previous section, indicating the following application of Lemma 1 and Fano's inequality (24)."}, {"heading": "5.3 Proof of Theorem 2", "text": "The proof of Theorem 2 is quite similar to that of Theorem 1, also following our sketch from Section 5.1. However, in this section we construct a different family of loss functions that require a new mutual information binding."}, {"heading": "5.3.1 Constructing well-separated losses", "text": "We construct loss families that are limited for the analysis of the case of stochastic undergradients = > Risk (1-norm) (1-norm). As was the case with the mean losses (Section 5.2.1), we can use V \u2212 \u2212 1, 1} d / 4 packs of the hypercube in category 0 norm; we know that there is such a set with cardinality. \u2212 As our sampling process for the data shows, we choose X from the 2d positive and negative defaults for vectors \u00b1 ej, namelyosis index j 1,.., d} uniformly randomly, and set X = {ej w.p. 1 + 2 \u2212 ej w.p. 1 \u2212 vj2, (32) where we have set the values (0, 1]. For a fixed L > 0, we define the hinges loss, generally in category problems, x,."}, {"heading": "5.3.2 Bounding the mutual information", "text": "For theorem 2, we need a reasonably careful limitation of the mutual information between the subgradients and the unknown index. We have the following problem, the proof of which we provide in Appendix B.3. Lemma 7. Let V be evenly randomly constructed from a series of V-1, 1, 1, d. Define the distribution P (\u00b7 | A) to X as in the random sample scheme (32) and use the loss (33). Let Z be constructed according to the conditional distribution specified in sentence 2, where Z-z-Rd: z-1 \u2264 M1}, and define M = M1 / L. ThenI (Z1,..., Zn; V)."}, {"heading": "5.3.3 Applying testing inequalities", "text": "The rest of the evidence is similar to that of theorem 1, except that we see lemmas 7 instead of lemmas 4 or 5. In fact, following identical steps as in the proof of theorem 1 with the specified packaging V = 1, 1} d of size | V | \u2265 exp (d / 8) (recall Eq. (30) 4rL\u03b4 EP, Q [1 \u2212 Log 2 log | V | \u2212 n \u00b2 2 log | V | 3 Log | 1 \u2212 6 d \u2212 8) 2 d. Consequently, for all d = 9 we have the lower limit 4 rL\u03b4EP, Q [2] n (M, P) \u2265 15, or equivalentlyEP, Q [3} n (M, P]."}, {"heading": "5.4 Proof of Theorem 3", "text": "In our proof for Theorem 3, we are slightly shorter than the previous two, although we repeat the same steps to emphasize our technique."}, {"heading": "5.4.1 Constructing well-separated losses", "text": "Let's start with the selection of the family of loss functions we need: Since our optimization range uses the linear losses from Section 5.2.1 as in Theorem 1, we use the scanning scheme (26) as in Theorem 1. Thus, if we use the packing set V = {\u00b1 ei} di = 1, we find that f (V) = Lr\u03b4 and consequently 2Lr\u03b4 EP, Q [\u0441 n (M,,, P)] \u2265 1 \u2212 log 2 log (2d) \u2212 I (Z1,..., Zn; V) log (2d) as before."}, {"heading": "5.4.2 Bounding the mutual information", "text": "The mutual information bound in this theorem is somewhat more complicated than the previous limits, because the optimal data protection distribution (Proposition 3) is more complex. Let's start by specifying a lemma.Lemma 8. Let's let V be evenly randomly selected from V = {\u00b1 ei} di = 1. Let's let X | V be randomly selected according to the distribution (26), and let Z | X = x define the constants Cd (k) and Q (k) evenly according to the random principle of V = {exp (\u03b1), if z x > k1, if z x kfor some k 0. Let's define the constants Cd (k) and Q (k) and the constants Cd \u2212 d \u2212 x, k) byCd (k): = Map {z {\u2212 1, 1} d: < z > k} Properties (k) = < z > k} = Properties (k), x (d) \u2212 d \u2212 d."}, {"heading": "5.4.3 Applying testing inequalities", "text": "As a result of the diagram (34), we have the lower limit 2Lr\u03b4 EP, Q [\u0394n (M,, \u0445, P)] \u2265 1 2 \u2212 max kn \u0445 (\u03b4, \u03b1, d, k) 2log (2d) \u2265 1 2 \u2212 4n\u03b42\u03b12d log (2d).By selecting \u03b4 = min {\u221a d log (2d) / 4\u03b1 \u221a n, 1}, we find out that 2Lr\u03b4 EP, Q [\u0424n (M, \u0445, P)] \u2265 1 4 corresponds to the limit given in the theorem."}, {"heading": "5.5 Proof of Theorem 4", "text": "In our proofs of theorems 4 and 5, we use some of our own current results [12] on the contractive properties of mutual information and KL divergence under local differential conditions. To represent these results, a few definitions are required. As usual, we have an indexed set of probability scales {Pv} v \u00b2 V, and we leave Qn (\u00b7 x1,. \u2212 xn) the common probability of n published private random variables Z1,.., Zn. For each v \u00b2 V, we then define the boundary distribution Mnv (A): = 1 \u00b2 Qn (A \u2212 x1,., xn) dPnv (x1,., xn) for A \u00b2 n (Zn). (35) Duchi et al. [12] specify two results, the boundaries for the Mnv \u2212 Mnw \u00b2 TV and I (Z1,.,.)."}, {"heading": "5.5.1 Constructing well-separated losses", "text": "Our lower limit uses an identical construction as in Section 5.2.1. We allow the loss function (x, \u03b8) = L < x, \u03b8 >, and we use the distribution (26) to x; that is, we have V = {\u00b1 ej} dj = 1, and for \u04210 [0, 1 / 2] we take vectors from X = {\u2212 1, 1} d with the probability Pv (X = x) = (1 + \u03b4v x) / 2d. We then have \u0430 (V) = Lr\u03b4 and | V | = 2d (remember Lemma 2)."}, {"heading": "5.5.2 Bounding the mutual information", "text": "In constructing the (almost) uniform scanning scheme, we face the following problem [12, Lemma 7].Lemma 9. Under the conditions of the previous paragraph, we let \u03b4 \u2264 1 and V be scanned uniformly by {\u00b1 ej} dj = 1. For each non-interactive \u03b1-differentiated private channel Q, I (Z1,..., Zn; V) \u2264 n e\u03b14d (e\u03b1 \u2212 e \u2212 \u03b1) 2 \u03b42."}, {"heading": "5.5.3 Applying testing inequalities", "text": "With the help of Lemma 9 we can provide an almost immediate proof of the lower limit in Theorem 4 (b). In fact, Fano's inequality (24), Lemmas 1 and 9 and the separation of Lemma 2 (b) provide us with proof of the lower limit. As long as d) n (L), Setting (2d) n (n) n (1) n (e\u03b1 (e\u03b1 \u2212 e \u2212 \u03b1) 2\u03b42 / 4d + log 2 (2d)))). As long d) = 2, Setting (n) n (2d) n (e\u03b1 \u2212 e), 1) and pointing out that e\u03b1 = O (1) and e\u03b1 \u2212 e \u2212 k\u03b1 for a universal constant c (1) complete the proof. If d = 1, an argument about Le Cam's method (25) yields an identical result."}, {"heading": "5.6 Proof of Theorem 5", "text": "The proof of this theorem follows the sketch in section 5.1, as well as the previous results. We move the results of accessibility to Appendix C.2."}, {"heading": "5.6.1 Constructing well-separated losses", "text": "Before constructing the well-separated loss functions, we show the quantity V, which is the basis of our scanning strategy = > q = = q = q = q distributions. The following problem shows the presence of a special packaging of the Boolean Hypercubes [12, Lemma 5]: Lemma 10. There is a packaging V of the d-dimensional Hypercubes {\u2212 1, 1} d with the packaging V \u2212 w \u00b2 1 \u00b2 d / 2 for each v, w \u2212 V with V = w, so that the cardinality of V is at least exp (d / 16) and 1 | V | v \u00b2 v \u00b2 v \u00b2 Vvv \u00b2 25Id \u00b7. With this packaging V we leave the V \u00b2 V \u00b2 V-V, as usual, and condition the X = v \u00b2 -dj} dj = 1 according to the pattern (32).Linear losses (for the bound (18)). First we consider the case that the loss functions L-Lipitz are related to the Norm."}, {"heading": "5.6.2 Bounding the mutual information", "text": "With Lemma 10 we can bind the mutual information between the samples Z from a certain distribution and a random sample V from a sentence V of the form in Lemma 10. Actually, V should be a packaging of the d-dimensional hypercube specified in Lemma 10. Subject to V = v = v = 1, 1} d we sample the random vector X = 1 according to the sample scheme (32). Then we have the following problem [12, Lemma 6], which applies as long as the channel Q is not interactive and \u03b1-locally differentiated privat.Lemma 11. Let Zi be differentiated private for Xi and the conditions of the previous paragraph. ThenI (Z1,..., Zn; V) \u2264 n 25e\u03b116\u03b42d (e\u03b1 \u2212 e \u2212 \u03b1) 2."}, {"heading": "5.6.3 Applying testing inequalities", "text": "Our last step is the application of the usual test inequalities. We first prove the lower limit in inequality (18). Let us leave Llin = Llin (Bq (rq); L, p). Then, by applying Lemma 11 and Fano's inequality (24) to Lemma 1 - using the separation (38) - we obtain proof of quality (Llin, \u03b1); 3rqL\u03b4d1 p \u2212 110 (e\u03b1 \u2212 e \u2212 \u03b1) 2 / 16d + log 2 d / 16). As long as we have 16, we have 16 protocols 2 / d \u2264 log 2 < 7 / 10. Thus, we provide proof of quality (s)."}, {"heading": "5.7 Proof of Corollary 1", "text": "Since the limit (14a) guarantees that the mirror parentage attains a convergence rate O (M \u00b2 r \u221a log (2d) / \u221a n), which is consistent with the second statement of theorem 1. Now, we correct our desired amount of mutual information I \u00b2. From the remarks in sentence 1, it emerges that (due to the uniqueness of the optimal data protection distribution Q) we must guarantee for each distribution P and loss function P \u00b2 (P, Q) that we have (due to the uniqueness of the optimal data protection distribution Q) a certain level of privacy. (39) Until a higher order, in order to guarantee a level of privacy with mutual information I \u00b2, we must allow gradient noise up to one level M \u00b2 = L \u00b2 d / I \u00b2. Equality (39) stipulates that for a certain level of permitted mutual information I \u00b2, if an optimal local data protection limit exists, then M \u00b2 L \u00b2 and I \u00b2."}, {"heading": "5.8 Proof of Corollary 2", "text": "In accordance with the conditions of optimal local privacy, if we need to guarantee that I use \u2265 supP I (P, Q) for any loss function whose slopes are limited by L, we must have I \u0445 dL 22M21, using conclusion 4 according to the statement of sentence 2. If we paraphrase this, we see that we must have M1 = L \u221a d / 2I * (at higher conditions) in order to be able to guarantee a degree of privacy I \u0445. As in the case of Aubameyang, we have a division between the multiplier M1 and the amount of information I need, and can apply similar techniques. Let us now remember the convergence guarantee (14b) given by stochastic gradient. Since the n-ball of the radius r is contained in the case of 2-ball of the radius r2 = r-d, stochastic gradient guarantees that the ball of the radius r = d is contained, and that it applies to all Rd-1 gradients."}, {"heading": "5.9 Proof of Corollary 3", "text": "Without loss of generality (by scaling), we assume that L = 1. Now we are looking at q > q = > = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "6 Discussion", "text": "We have studied privacy protection methods in general statistical risk mitigation problems and described general techniques to achieve sharp compromises between privacy protection and estimation rates, the latter being a natural measure of the benefit of statistical problems. We believe that there are a number of interesting open questions and areas for future work. First, we have investigated procedures that access data Zi of Subgradient XII (Xi, \u03b8) only once and through a disturbed view, which is natural in the context of convex risk minimization. A natural question is whether there are limitations to the class of loss functions so that a transformed version (Z1,.., Zn) of the data is sufficient to draw conclusions. For example, other researchers [51, 52] have investigated applications where a data matrix X = [X1 \u00b7 \u00b7 Xn] of privacy functions requires that a transformed version (Z1,., Zn) of the data be sufficient to ensure statistical dissemination."}, {"heading": "A Unbiasedness", "text": "In this appendix we show that, if an optimization procedure shows biased undergrades, it is possible to be willfully wrong, by constructing a simple problem instance. Do you fix a bias of the form in the first place and look at the following one-dimensional problem: Minimize f (\u03b8): = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "B Calculation of the Mutual Information for Sampling Strategies", "text": "This appendix is dedicated to the proofs of our mutual information: Lemma 4, Lemma 5, Lemma 7, and Lemma 8. Before proving the Lemmas, we make an observation that allows us to tensorize the mutual information, thus simplifying our arguments (we only need to calculate the individual observation information (Z1; V). For each individual cell Zi 4, 5, 7, and 8, we remember that Zn is constructed on the basis of an evaluation of the subgradient set on the basis of the individual observation information (Xi, \u03b8), where Xi are independent samples according to a distribution P (\u00b7 V). Then, the samples Zi are conditionally independent of V given Xi and the parameters."}, {"heading": "C Achievability by stochastic mirror descent", "text": "In this appendix, we provide further details on the algorithm used to achieve the upper limits in Theorems 4 and 5. Both of our results depend on the stochastic gradient mechanisms, and their main ingredient is a conditional distribution Q, which fulfills the local differential privacy. In particular, if g-Rd is a (sub) gradient of loss, we construct Z-Rd in such a way that E [Z | g] = g. Each of the accessibility guarantees consists of the description of an \u03b1-differential private sampling distribution, which is then the expected norm of Z and the application of one of the convergence guarantees (14).C.1 Availability in Theorem 4The sampling strategy we use is essentially identical to the uniform used in Corollary 3 (and the optimal proposition 3); see also Strategy B in our work [12]."}, {"heading": "D Background on Conditional Probabilities", "text": "In this appendix, we begin with a precise definition of a regular condition. (F) We assume that we have certain quantities (F). (F) We assume that we must apply certain quantities (F). (F) We assume that we must apply certain quantities (F). (F) We assume that we must apply certain quantities (F). (D) We assume that we must apply certain quantities (F). (D) We must adjust these quantities (F). (D) We must adjust these quantities (F). (D) We must adjust these quantities (D). (D) We must adjust these quantities (D). We must adjust these quantities (D) dimensions of the Krein Milman theorem (B) theorem (B) dimensions (D) dimensions (D) dimensions (D) dimensions (D) dimensions."}, {"heading": "E Proofs of Minimax Mutual Information Characterizations", "text": "In this section, we provide the proofs of the results given in section 4, all of which follow a broadly similar sketch. We use Lemma 15 to guarantee that any conditional distribution Q can minimize the mutual information. (P, Q) must be based on the extreme points of set D. This allows us to reduce the maximum entropies and minimum mutual information values to finite dimensional convex programs, the optimality of which we can verify using results from convex analyses and optimizations. (P, Q) 1 Proof Theorem 6We start by considering supP, where Q \"is defined as in the statement of the theorem. Since the support of Q is\" finite \"(it is extreme points of D), we have haveI\" = I (X; Z) = H (Z) \u2212 H (Z) \u2212 H (Z) \u2264 Log (m) \u2212 H (Z) \u2212 H (Z) (Z) \u2212 H (Z) (Z) (log) (we are \u2212 H (x) for each distribution."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>We study statistical risk minimization problems under a privacy model in which the data<lb>is kept confidential even from the learner. In this local privacy framework, we establish sharp<lb>upper and lower bounds on the convergence rates of statistical estimation procedures. As a<lb>consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and<lb>the utility, as measured by convergence rate, of any statistical estimator or learning procedure.", "creator": "LaTeX with hyperref package"}}}