{"id": "1210.2771", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2012", "title": "Cost-Sensitive Tree of Classifiers", "abstract": "Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test time must be budgeted and accounted for. In this paper, we address the challenge of balancing the test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across eatures. We decrease this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. By only computing features for inputs that benefit from them the most, our cost sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost.", "histories": [["v1", "Tue, 9 Oct 2012 22:17:42 GMT  (1590kb,D)", "https://arxiv.org/abs/1210.2771v1", null], ["v2", "Wed, 14 Nov 2012 21:45:56 GMT  (1590kb,D)", "http://arxiv.org/abs/1210.2771v2", null], ["v3", "Mon, 22 Apr 2013 17:56:54 GMT  (1588kb,D)", "http://arxiv.org/abs/1210.2771v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["zhixiang eddie xu", "matt j kusner", "kilian q weinberger", "minmin chen"], "accepted": true, "id": "1210.2771"}, "pdf": {"name": "1210.2771.pdf", "metadata": {"source": "META", "title": "Cost-Sensitive Tree of Classifiers", "authors": ["Zhixiang (Eddie", "Matt J. Kusner", "Kilian Q. Weinberger"], "emails": ["xuzx@cse.wustl.edu", "mkusner@wustl.edu", "kilian@wustl.edu", "mchen@wustl.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world, \"he said in an interview with the\" New York Times. \""}, {"heading": "2. Related Work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "3. Cost-sensitive classification", "text": "In order to reduce the costs, which in this case we do not put in a non-linear space with the \"boosting trick\" (Friedman, 2001), before optimizing our optimization, we first have to deal with a quadratic loss (Friedman, 2001)."}, {"heading": "4. Cost-sensitive tree", "text": "We start with the introduction of basic concepts of the CSTC tree and derive a global loss function from it (5). Similar to the previous section, we first derive the exact cost concept and then relax it with the mixed standard. Finally, we describe how we can efficiently optimize this function and undo some of the inaccuracies caused by the mixed normative relaxation. We assume that instances with similar designations can use similar traits. 11 For example, traits generated by browser statistics are typically predictable only for highly relevant pages, as the user can spend considerable time on the page and interact with the root."}, {"heading": "4.1. Tree loss", "text": "We derive a single global loss function across all nodes in the CSTC because each parent has exactly one child. (...) We have the probability that a child will go from one parent to one parent. (...) The probability that a child will go from one parent to one parent is very low. (...) The probability that a child will go from one parent to one parent is very low. (...) The probability that a child will go from one parent to one parent is very high. (...) The probability that a child will come from one parent to one parent is very low. (...) The probability that a child will come from one parent to one parent is very low. (...) The probability that a child will come from one parent to one parent is very low. (...) The probability that a child will come from one parent to one parent is very low. (...) The probability that a child will be reached by one parent is very low. (...) The probability that a child will be reached by one parent is very low."}, {"heading": "4.2. Optimization Details", "text": "There are many techniques to minimize the loss in (5)."}, {"heading": "5. Results", "text": "In this section, we evaluate CSTC using a carefully constructed synthetic dataset to test our hypothesis that CSTC learns specialized classifiers based on different attribute subsets, and then evaluate the performance of CSTC on the large scale of the Yahoo! Learning to Rank Challenge dataset and compare it to state-of-the-art algorithms."}, {"heading": "5.1. Synthetic data", "text": "We construct a synthetic regression dataset that is sampled from the four quadrants of the X, Z plane, where X = Z = [\u2212 1, 1] can be used; the characteristics belong to two categories: cheap characteristics, characters (x), characters (z) with costs c = 1, which can be used to identify the quadrant of an input; and four expensive characteristics y + +, y + \u2212, y \u2212 +, y \u2212 with costs c = 10, which represent the exact designation of an input if it comes from the corresponding region (otherwise a random number). Since we do not transform the attribute space in this synthetic dataset, we have? (x) = x, and F (the weak attribute usage variable for learners) is the identity matrix 6. Constructively, a perfect classifier can use the two cheap characteristics to identify the subregion of an instance and then make the correct eigenness of each minimum instance to extract the two instances of a perfect precursor, respectively."}, {"heading": "5.2. Yahoo! Learning to Rank", "text": "To evaluate the performance of CSTC on real-world tasks, a multi-level ranking is provided when we evaluate the results of public Yahoo analysis on two levels. (There is only one cost indication) Each query document pair consists of 519 characteristics. An extraction cost, which refers to a value in the sentence {1, 2, 1, 0} refers to the relevance of a document to its corresponding query, with 4 references to a perfect match. (2012) We do not hide the number of irrelevant documents (by counting them 10 times). We measure performance using NDCG @ 5 (Yes \u00bc rvelin Keka \u00bc inen, 2002)."}, {"heading": "6. Conclusions", "text": "We introduce Cost-Sensitive Tree of Classifiers (CSTC), a novel learning algorithm that explicitly addresses the trade-off between accuracy and expected test-time CPU costs. The CSTC tree splits the entry area into sub-regions and identifies the most cost-effective features for each of these regions - so that it matches the high accuracy of the current state of the art at a small fraction of the cost. We obtain the CSTC algorithm by formulating the expected test-time costs of an instance that passes through a classification tree and loosening it into a continuous cost function. This cost function can be minimized while learning the parameters of all NPCs in the tree together. By explicitly formulating the test-time costs versus the accuracy compromises, we enable high-performance classifiers that fit into computational budgets and can reduce unnecessary energy consumption in large industrial applications."}], "references": [{"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "Classification and regression trees", "author": ["L. Breiman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Breiman,? \\Q1984\\E", "shortCiteRegEx": "Breiman", "year": 1984}, {"title": "Fast classification using sparse decision dags", "author": ["R. Busa-Fekete", "D. Benbouzid", "B K\u00e9gl"], "venue": "In ICML,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2012}, {"title": "Early exit optimizations for additive machine learned ranking systems", "author": ["B.B. Cambazoglu", "H. Zaragoza", "O. Chapelle", "J. Chen", "C. Liao", "Z. Zheng", "J. Degenhardt"], "venue": "In WSDM\u20193,", "citeRegEx": "Cambazoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cambazoglu et al\\.", "year": 2010}, {"title": "Yahoo! learning to rank challenge overview", "author": ["O. Chapelle", "Y. Chang"], "venue": "In JMLR: Workshop and Conference Proceedings,", "citeRegEx": "Chapelle and Chang,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Chang", "year": 2011}, {"title": "Boosted multi-task learning", "author": ["O. Chapelle", "P. Shivaswamy", "S. Vadrevu", "K. Weinberger", "Y. Zhang", "B. Tseng"], "venue": "Machine learning,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Classifier cascade for minimizing feature evaluation cost", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "O. Chapelle"], "venue": "In AISTATS,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Fast and balanced: Efficient label tree learning for large scale object recognition", "author": ["J. Deng", "S. Satheesh", "A.C. Berg", "L. Fei-Fei"], "venue": "In NIPS,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "Learning fast classifiers for image spam", "author": ["M. Dredze", "R. Gevaryahu", "A. Elias-Bachrach"], "venue": "In proceedings of the Conference on Email and Anti-Spam (CEAS),", "citeRegEx": "Dredze et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2007}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Finding naked people", "author": ["M. Fleck", "D. Forsyth", "C. Bregler"], "venue": "ECCV, pp. 593\u2013602,", "citeRegEx": "Fleck et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Fleck et al\\.", "year": 1996}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Active classification based on value of classifier", "author": ["T. Gao", "D. Koller"], "venue": "In NIPS, pp", "citeRegEx": "Gao and Koller,? \\Q2011\\E", "shortCiteRegEx": "Gao and Koller", "year": 2011}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["A. Grubb", "J.A. Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Grubb and Bagnell,? \\Q2012\\E", "shortCiteRegEx": "Grubb and Bagnell", "year": 2012}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM TOIS,", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen", "year": 2002}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural computation,", "citeRegEx": "Jordan and Jacobs,? \\Q1994\\E", "shortCiteRegEx": "Jordan and Jacobs", "year": 1994}, {"title": "Timely object recognition", "author": ["S. Karayev", "T. Baumgartner", "M. Fritz", "T. Darrell"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Karayev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2012}, {"title": "Sparse regression using mixed norms", "author": ["M. Kowalski"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Kowalski,? \\Q2009\\E", "shortCiteRegEx": "Kowalski", "year": 2009}, {"title": "Joint cascade optimization using a product of boosted classifiers", "author": ["L. Lefakis", "F. Fleuret"], "venue": "In NIPS, pp", "citeRegEx": "Lefakis and Fleuret,? \\Q2010\\E", "shortCiteRegEx": "Lefakis and Fleuret", "year": 2010}, {"title": "Using classifier cascades for scalable e-mail classification", "author": ["J. Pujara", "H. Daum\u00e9 III", "L. Getoor"], "venue": "In CEAS,", "citeRegEx": "Pujara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pujara et al\\.", "year": 2011}, {"title": "Boosting classifier cascades", "author": ["M. Saberian", "N. Vasconcelos"], "venue": null, "citeRegEx": "Saberian and Vasconcelos,? \\Q2010\\E", "shortCiteRegEx": "Saberian and Vasconcelos", "year": 2010}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT press,", "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2001}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "IJCV, 57(2):137\u2013154,", "citeRegEx": "Viola and Jones,? \\Q2004\\E", "shortCiteRegEx": "Viola and Jones", "year": 2004}, {"title": "Local supervised learning through space partitioning", "author": ["J. Wang", "V. Saligrama"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wang and Saligrama,? \\Q2012\\E", "shortCiteRegEx": "Wang and Saligrama", "year": 2012}, {"title": "Feature hashing for large scale multitask learning", "author": ["K.Q. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "In ICML,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Z. Xu", "K. Weinberger", "O. Chapelle"], "venue": "In ICML, pp", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "A general boosting method and its application to learning ranking functions for web search", "author": ["Z. Zheng", "H. Zha", "T. Zhang", "O. Chapelle", "K. Chen", "G. Sun"], "venue": "In NIPS,", "citeRegEx": "Zheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "Machine learning algorithms are widely used in many real-world applications, ranging from emailspam (Weinberger et al., 2009) and adult content filtering (Fleck et al.", "startOffset": 100, "endOffset": 125}, {"referenceID": 10, "context": ", 2009) and adult content filtering (Fleck et al., 1996), to web-search engines (Zheng et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 26, "context": ", 1996), to web-search engines (Zheng et al., 2008).", "startOffset": 31, "endOffset": 51}, {"referenceID": 9, "context": "Different from prior work, which reduces the total cost for every input (Efron et al., 2004) or which stages the feature extraction into linear cascades (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 19, "context": ", 2004) or which stages the feature extraction into linear cascades (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), a CSTC tree incorporates input-dependent feature selection into training and dynamically allocates higher feature budgets for infrequently traveled tree-paths.", "startOffset": 68, "endOffset": 184}, {"referenceID": 6, "context": ", 2004) or which stages the feature extraction into linear cascades (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012), a CSTC tree incorporates input-dependent feature selection into training and dynamically allocates higher feature budgets for infrequently traveled tree-paths.", "startOffset": 68, "endOffset": 184}, {"referenceID": 9, "context": "A basic approach to control test-time cost is the use of l1-norm regularization (Efron et al., 2004), which results in a sparse feature set, and can significantly reduce the feature cost during test-time (as unused features are never computed).", "startOffset": 80, "endOffset": 100}, {"referenceID": 19, "context": "There is much previous work that extends single classifiers to classifier cascades (mostly for binary classification) (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012).", "startOffset": 118, "endOffset": 234}, {"referenceID": 6, "context": "There is much previous work that extends single classifiers to classifier cascades (mostly for binary classification) (Viola & Jones, 2004; Lefakis & Fleuret, 2010; Saberian & Vasconcelos, 2010; Pujara et al., 2011; Chen et al., 2012).", "startOffset": 118, "endOffset": 234}, {"referenceID": 11, "context": "It should be noted that, in spite of the high accuracy achieved by these techniques, the algorithms are based heavily on stage-wise regression (gradient boosting) (Friedman, 2001), and are less likely to work with more general weak learners.", "startOffset": 163, "endOffset": 179}, {"referenceID": 24, "context": "Grubb & Bagnell (2012) and Xu et al. (2012) focus on training a classifier that explicitly trades-off test-time cost and accuracy.", "startOffset": 27, "endOffset": 44}, {"referenceID": 0, "context": "(2011), who speed up the training and evaluation of label trees (Bengio et al., 2010), by avoiding many binary one-vs-all classifier evaluations.", "startOffset": 64, "endOffset": 85}, {"referenceID": 2, "context": "Possibly most similar to our work are (Busa-Fekete et al., 2012), who learn a directed acyclic graph via a Markov decision process to select features for different instances, and (Wang & Saligrama, 2012), who adaptively partition the feature space and learn local region-specific classifiers.", "startOffset": 38, "endOffset": 64}, {"referenceID": 5, "context": "Recent tree-structured classifiers include the work of Deng et al. (2011), who speed up the training and evaluation of label trees (Bengio et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 0, "context": "(2011), who speed up the training and evaluation of label trees (Bengio et al., 2010), by avoiding many binary one-vs-all classifier evaluations. Differently, we focus on problems in which feature extraction time dominates the test-time cost which motivates different algorithmic setups. Dredze et al. (2007) combine the cost to select a feature with the mutual information of that feature to build a decision tree that reduces the feature extraction cost.", "startOffset": 65, "endOffset": 309}, {"referenceID": 11, "context": "Throughout this paper, we focus on linear classifiers but in order to allow non-linear decision boundaries we map the input into a non-linear feature space with the \u201cboosting trick\u201d (Friedman, 2001; Chapelle et al., 2011), prior to our optimization.", "startOffset": 182, "endOffset": 221}, {"referenceID": 5, "context": "Throughout this paper, we focus on linear classifiers but in order to allow non-linear decision boundaries we map the input into a non-linear feature space with the \u201cboosting trick\u201d (Friedman, 2001; Chapelle et al., 2011), prior to our optimization.", "startOffset": 182, "endOffset": 221}, {"referenceID": 11, "context": "In particular, we first train gradient boosted regression trees with a squared loss penalty (Friedman, 2001), H \u2032(xi) = \u2211T t=1 ht(xi), where each function ht(\u00b7) is a limited-depth CART tree (Breiman, 1984).", "startOffset": 92, "endOffset": 108}, {"referenceID": 1, "context": "In particular, we first train gradient boosted regression trees with a squared loss penalty (Friedman, 2001), H \u2032(xi) = \u2211T t=1 ht(xi), where each function ht(\u00b7) is a limited-depth CART tree (Breiman, 1984).", "startOffset": 190, "endOffset": 205}, {"referenceID": 17, "context": "described by (Kowalski, 2009).", "startOffset": 13, "endOffset": 29}, {"referenceID": 9, "context": "Note that for a single element this relaxation relaxes the l0 norm to the l1 norm, \u2016aij\u20160 \u2192 \u221a (aij) = |aij |, and recovers the commonly used approximation to encourage sparsity (Efron et al., 2004; Sch\u00f6lkopf & Smola, 2001).", "startOffset": 177, "endOffset": 222}, {"referenceID": 11, "context": "Stage\u2212wise regression (Friedman, 2001) Single cost\u2212sensitive classifier Early exit s=0.", "startOffset": 22, "endOffset": 38}, {"referenceID": 6, "context": "In contrast to Chen et al. (2012), we do not inflate the number of irrelevant documents (by counting them 10 times).", "startOffset": 15, "endOffset": 34}, {"referenceID": 11, "context": "The plot shows different stages in our derivation of CSTC: the initial cost-insensitive ensemble classifier H \u2032(\u00b7) (Friedman, 2001) from section 3 (stage-wise regression), a single cost-sensitive classifier as described in eq.", "startOffset": 115, "endOffset": 131}, {"referenceID": 3, "context": "For competing algorithms, we include Early exit (Cambazoglu et al., 2010) which improves upon stagewise regression by short-circuiting the evaluation of unpromising documents at test-time, reducing the overall test-time cost.", "startOffset": 48, "endOffset": 73}, {"referenceID": 6, "context": "For Cronus (Chen et al., 2012), we use a cascade with a maximum of 10 nodes.", "startOffset": 11, "endOffset": 30}, {"referenceID": 6, "context": "The only feature that has cost 200 is extracted at all depths\u2014which seems essential to obtain high NDCG (Chen et al., 2012).", "startOffset": 104, "endOffset": 123}], "year": 2013, "abstractText": "Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test-time must be budgeted and accounted for. In this paper, we address the challenge of balancing the test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction\u2014which can vary drastically across features. We decrease this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific subpartition of the input space. By only computing features for inputs that benefit from them the most, our cost-sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost.", "creator": "LaTeX with hyperref package"}}}