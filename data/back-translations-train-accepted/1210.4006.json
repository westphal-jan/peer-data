{"id": "1210.4006", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2012", "title": "The Perturbed Variation", "abstract": "We introduce a new discrepancy score between two distributions that gives an indication on their similarity. While much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions. The new score gives an intuitive interpretation of similarity; it optimally perturbs the distributions so that they best fit each other. The score is defined between distributions, and can be efficiently estimated from samples. We provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions. The statistical power of this procedures is presented in simulations. We also compare the score's capacity to detect similarity with that of other known measures on real data.", "histories": [["v1", "Mon, 15 Oct 2012 12:43:03 GMT  (88kb,D)", "http://arxiv.org/abs/1210.4006v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["maayan harel", "shie mannor"], "accepted": true, "id": "1210.4006"}, "pdf": {"name": "1210.4006.pdf", "metadata": {"source": "CRF", "title": "The Perturbed Variation", "authors": ["Maayan Harel", "Shie Mannor"], "emails": ["maayanga@tx.technion.ac.il", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "The question of similarity between two sets of examples is common to many fields, including statistics, data mining, machine learning and computer vision. For example, in machine learning, a standard assumption is that the training and test data are generated by the same distribution. However, in some scenarios, such as domain adaptation (DA), this is not the case and the distributions are only assumed to be similar. It is quite intuitive to call two inputs similar in nature, but the following question remains open: Given two examples, how do we test whether they were generated by similar distributions? The main focus of this work is to provide a similarity score and a corresponding statistical procedure that provides a possible answer to this question. Discretion between distributions has been studied for decades, and a wide variety of distance values have been proposed. However, not all the proposed results can be used to test similarity."}, {"heading": "2 The Perturbed Variation", "text": "The continuous distribution PV is defined as follows: Definition 1. Let P > Q = two distributions on a Banach space be X, and let M (P, Q) be the set of all common distributions on X \u00b7 X with the mariginals P and Q. The PV, in relation to a distance function d: X \u00b7 X \u2192 R and, is defined by PV (P, Q, d). = inf \u00b5 \u00b2 M (P, Q) P\u00b5 [d, Y) >], (1) over all pairs (X, Y) as the marginal of X is P and the marginal of Y is Q.Put in words, Equation (1) defines the common distribution of two distributions, so that the probability of the event of a pair (X, Y) lying at a distance from a ridge is minimized. The solution to (1) is a special case of the classic mass transport problem of Monge [1] and its cantor version (X)."}, {"heading": "2.1 The Perturbed Variation on Discrete Distributions", "text": "It can be shown that for two discrete distributions problem (1) corresponds to the following problem. Definition 2. Let \u00b51 and \u00b52 be two discrete distributions on the uniform carrier {a1,..., aN}. Define the neighborhood of ai as ng (ai,) = {z; d (z, ai) \u2264}. The PV (\u00b51, \u00b52,, d) between the two distributions is: min wi \u2265 0, vi \u2265 0, Zij \u2265 012 N = 1 wi + 1 2 N = 1 vj (2) s.t. The PV (\u00b51, \u00b52, d) between the two distributions is: min wi \u2265 0, vi \u2265 0, Zij + vj = \u00b52 (aj).jZij = 0, (i, j).t."}, {"heading": "2.2 Estimation of the Perturbed Variation", "text": "In this case, the examples we give for the difference in the order of magnitude of the two samplesar programs are identical to (2), as in this case the examples we give for the difference in the order of magnitude of the two samplesar programs. (2) It is not as if the difference is in the order of magnitude (2), as in the order of magnitude (2), as in the order of magnitude (3), as in the order of magnitude (3), as in the order of magnitude (3), as in the order of magnitude (3), as in the order of magnitude (3), as in the order of magnitude (2), as in the order of magnitude (3), as in the order of magnitude (3)."}, {"heading": "3 Related Work", "text": "In fact, most of us are able to play by the rules we set ourselves, \"he said in an interview with the German Press Agency.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said.\" We have to play by the rules, \"he said,\" but we have to play by the rules. \""}, {"heading": "4 Analysis", "text": "We present the convergence analysis of the PV. The proofs of the theorems are provided in the supplementary material. If no clarity is lost, we omit d from the notation. \u2212 Our main proposition is as follows: Theorem 3. Suppose we get two i.i.d. samples S1 = {x1,..., xn}, and S2 = {y1,..., ym}, which are generated by the distributions P and Q. Suppose that the ground distance d = and let N () be the cardinality of a disjoint coverage of the distributions. \"Then, for any other type of distributions (0, 1), N = min (m), and p = 2 (log (2 (2N)) + log (1 / 3), we will be the cardinality of a disjoint coverage of the distribution support."}, {"heading": "5 Statistical Inference", "text": "We construct two types of complementary procedures for hypotheses testing of similarity and dissimilarity, but only on the basis of the presented hypotheses tests. In the first type of procedures given 0 \u2264 \u03b8 < 1, we distinguish between the null hypothesis H (1) 0: PV (P, Q, d) \u2264 \u03b8, which implies similarity, and the alternative hypothesis H (1) 1: PV (P, Q, d) > \u03b8. Note that in the second type of procedures, this test is a relaxed version of the TSP. Using PV (P, Q) = 0 instead of P = Q as zero, a distinction between the distributions is possible, which gives the necessary relaxation to detect the similarity. In the second type of procedures, we test whether two distributions are similar. To do this, we spin the role of the zero and the alternative. Note that there is no equivalence of this form for the TSP, therefore we cannot derive similarity, but only use the similarity under P."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Synthetic Simulations", "text": "In our first experiment, we investigated the effect of the choice on the statistical strength of the test. To this end, we applied significance tests for similarity to two univariate uniform distributions: P \u0445 U [0, 1] and Q \u0445 U [\u2206 (), where \u2206 () represents a different magnitude of the error. We took into account values of = [0.1, 0.2, 0.3, 0.4, 0.5] and sample sizes of up to 5000 samples from each distribution. In this way, we tested the ability of the PV to detect similarities for different sizes of disturbances. The percentage in which the zero hypothesis was wrongly rejected, i.e. the type 1 error was kept at a statistical level of = 0.05. The percentage of the zero hypothesis based on a certain magnitude of disturbances was based on the expected convergence of 0.05."}, {"heading": "6.2 Comparing Distance Measures", "text": "In fact, most people who are in a position to put themselves in the world are not in a position to put themselves in the world in which they live, but in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they are, in which they are, in which they, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are, in which they, in which they live, in which they are, in which they, in which they live, in which they are, in which they are, in which they are, in which they are, in which they live, in which they are, in which they, in which they are, in which they are, in which they, in which they are, in which they, in which they are, in which they are, in which they are, in which they are, in which they are, in which they are,"}, {"heading": "7 Discussion", "text": "We have proposed a new score that measures the similarity between two multivariate distributions and assigns it a value in the range of 0.1. The sensitivity of the score, reflected by the parameter, allows flexibility that is essential for quantifying the concept of similarity. PV is efficiently estimated by sampling; its low computational complexity is based on its simple binary classification of points as neighbors or non-neighbors, so that optimizing the distances from distant points is not necessary. In this way, the PV only gathers the essential information to describe the similarity. Although it is not a measurement, our experiments show that it captures the distance between similar distributions as well as known distribution distances. Our work also includes convergence analyses of the PV. On the basis of this analysis, we offer hypotheses tests that give statistical meaning to the resulting score. While our limits depend on the statistical dimension, if the intrinsic dimension of the data is smaller than the intuitive dimension of the intuition of the similarity."}, {"heading": "A Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Hypothesis Testing Procedures", "text": "The statistical tests in this section are based on the convergence boundaries in Section 4.Notations."}, {"heading": "For two samples S1 \u223c P and S2 \u223c Q the score is", "text": "The next theorem represents the convergence of P-PVK to zero for distributions with PV (P, Q) = 0, theorem 9. Let P and Q be two distributions in space ([0, 1] d, d), and S1 = 1.0,... xn} p and S2 = 1.0, ym} Q two i.i.d. samples (N = min (n, m). Perform K i.i.d. random projections of S1 and S2 to one dimension. If PV (P, Q, d) = 0, then for all distributions of Q two i.i.d. samples (N = min (m), m)."}, {"heading": "A.3 Proof of Theorem 3", "text": "We present the theory of the theorems of the theorems immediately after the proof of the theorems. We assume that the spatial relations are completely different from the spatial relations. We assume that we define the spatial relations completely differently from the spatial relations. (...) We assume that we do not understand the spatial relations. (...) We assume that we do not understand the spatial relations. (...) We assume that S1 (...) S1 (...) S1 (...) S1 (...) S1 (...) S2 (...) S2 (...) S2 (...) S2 (...) S2) (...) S1 (...) S2 (...), S2 (...), S2 (...) (...), S2 (...), S2 (...), S2 (...), S2 (...), S2 (...), S2 (...), S2 (...), S2 (...)."}, {"heading": "Proofs of Lemmas 13,15", "text": "The proof that Lemma 13Let sample xi + 1) S1 belongs to the element ak (p) n (p) n (p) n (p) n (p) n (p) n (ak, p) n (ak, p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n n (p) n n (p) n n n n (n) n n n n n (n) n n n (n) n n (p) n (n) n n n (n (n) n (p) n n n n (n) n n n n (n) n n n n (p) n n n n (n) n (n) n (p) n) n (p) n n n n n (n (n) n) n (p) n) n) n (p) n (p) n (p) n) n n n n n n (p) n n n n (p) n n n n n (p) n n n n (p) n) n) n n n (p) n) n n n n n n (p) n (p) n n) n n n n n (p) n n n n n n n n n (p) n n n n n n n n n (p) n) n (p) n) n n n n (p) n) n n (p) n n n (p) n n n n n n (p) n (p) n) n n n) n (p) n n n n (p) n n n n (p) n n n n (p) n n n n (p) n) n n n (p) n n n n (p) n (p) n n n n (p) n n n n n n (p) n n n n n n n n n n n n n (p) n n"}, {"heading": "A.4 Proof of Theorem 4", "text": "Let P = Q be the even distribution on Sd \u2212 1, a unit (d \u2212 1) -dimensional hypersphere. Let S1 = {x1,..., xN} \u0445 P and S2 = {y1,..., yN} \u0445 Q be two i.i.d. samples for any, \u2032, \u03b4 (0, 1), 0 \u2264 \u03b7 < 2 / 3 and sample size protocol (1 / \u03b4) 2 (1 \u2212 3\u03b7 / 2) 2 \u2264 N \u2082 / 2ed (1 \u2212 22) / 2 we have PV (P, Q, \u2032) = 0 \u2212 P (P \u0412V (S1, S2,) > \u03b7) \u2265 1 \u2212 \u03b4. (24) Proof. We use the following definitions and lemmas.Definition 18. The spherical cap of the radius r by one point x is C (r, x) = {z, x."}, {"heading": "Proof.", "text": "p = P (ngS2 (x) \u2212 P (ngS2) = > P (ngS2 (x) 6 = \u03b7 (n) = 1 \u2212 P (yj S2; yj C (, xi)) \u2212 NP (y C (, x)) \u2265 1 \u2212 Ne \u2212 d (1 \u2212 2) / 2, the first inequality being due to the composite boundary, and the second to Lemma 20.We consider the probability that the P-V number is greater than some 0 \u2264 \u03b7 < 1. Note that since PV (P, Q) = 0 this is also the difference between the empirical and distributional PV. Leave e = {xi S1: ngS2 (xi) = 4.2 = 4.2 samples in S1 without neighbors, and Ne its cardinal inequality. P (P V (S1, S2,) S2 \u2212 d \u2212 d."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "We introduce a new discrepancy score between two distributions that gives an indi-<lb>cation on their similarity. While much research has been done to determine if two<lb>samples come from exactly the same distribution, much less research considered<lb>the problem of determining if two finite samples come from similar distributions.<lb>The new score gives an intuitive interpretation of similarity; it optimally perturbs<lb>the distributions so that they best fit each other. The score is defined between<lb>distributions, and can be efficiently estimated from samples. We provide conver-<lb>gence bounds of the estimated score, and develop hypothesis testing procedures<lb>that test if two data sets come from similar distributions. The statistical power of<lb>this procedures is presented in simulations. We also compare the score\u2019s capacity<lb>to detect similarity with that of other known measures on real data.", "creator": "LaTeX with hyperref package"}}}