{"id": "1210.5644", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2012", "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials", "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.", "histories": [["v1", "Sat, 20 Oct 2012 17:41:23 GMT  (3893kb,DS)", "http://arxiv.org/abs/1210.5644v1", "NIPS 2011"]], "COMMENTS": "NIPS 2011", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["philipp kr\u00e4henb\u00fchl", "vladlen koltun"], "accepted": true, "id": "1210.5644"}, "pdf": {"name": "1210.5644.pdf", "metadata": {"source": "META", "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials", "authors": ["Philipp Kr\u00e4henb\u00fchl", "Vladlen Koltun"], "emails": ["philkr@cs.stanford.edu", "vladlen@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "2 The Fully Connected CRF Model", "text": "In our setting, it is as if it is a possible input image of size N and X with possible captions. Ij is the color vector of pixel j and Xj is associated with the label. (I, X) is a random random random field (I, X) is characterized by a random random random field (I, X). (I) is the random random field (I, X) is characterized by a random random field (I, X) is characterized by a random random field (I, I). (I) is the distribution of Gibbs P (I) = 1Z exp (I). (I), where G = (V, E) is a graph on X and each clip cin a series of clicks CG induces a potential random [15]."}, {"heading": "3 Efficient Inference in Fully Connected CRFs", "text": "Our algorithm is based on an average field approach to the CRF distribution. This approach results in an iterative message forwarding algorithm to approximate conclusion. Our main observation is that the delivery of messages in the presented model can be performed using Gaussian filtering in the attribute space. This allows us to use highly efficient approaches for high-dimensional filtering that reduce the complexity of transmitting square to linear messages, resulting in an approximate inference algorithm for fully connected CRFs that is linear in the number of variables N and sublinear in the number of edges in the model."}, {"heading": "3.1 Mean Field Approximation", "text": "Instead of calculating the exact distribution P (X), the mean field approximation calculates a distribution Q (X) that minimizes the KL divergence D (Q) between all distributions Q, which can be expressed as a product of independent marginals. Minimizing the KL divergence, while limiting Q (X) and Qi (Xi) requires valid distributions, results in the following iterative update equation: Qi (xi = l) = 1Zi exp \u2212 u (xi) \u2212 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 m = 1 w \u2212 i k (m) (fi, fj) Qj \u00b2 l \u00b2). (4) A detailed derivation of Equation 4 is given in the supplementary material."}, {"heading": "3.2 Efficient Message Passing Using High-Dimensional Filtering", "text": "From a signal processing point of view, the message transmitted step by step can be expressed as a convolution with a Gaussian core Q (Q) Q (m) in the attribute space: Q (m) i (l) = \u2211 j (V) (fi, fj) Qj (l) \u2212 Qi (l) (m) Q (l)]]] (fi) (m) -Qi (l) We subtract Qi (l) from the convolution function Q (m) i (l) because the convolution is summed up across all variables, while the transmission of the message is not via Qi. This convolution performs a low-pass filter, the essentially band-limiting Q (m) i (l). By mutating the theorem, this function can be reconstructed from a series of samples whose volume proportion is proportional to the standard deviation of the filter."}, {"heading": "4 Learning", "text": "We learn the parameters of the model through piecemeal training. First, the improved simple classifiers are trained using the JointBoost algorithm [21], using the characteristics described in Section 5. Next, we learn the parameters of the appearance core w (1), \u03b8\u03b1, and \u03b8\u03b2 for the Potts model. (w (1) can be efficiently found by a combination of expectation maximization and high-dimensional filtering. Unfortunately, the kernel widths \u03b8\u03b1 and \u03b8\u03b2 cannot be calculated effectively with this approach, as their gradient includes a sum of non-Gaussian cores (1) that are not suitable for the same acceleration techniques. We found that it is more efficient to apply the grid search to a complete validation that is applicable to all three kernel parameters w (1), success\u03b1, and success\u03b1."}, {"heading": "5 Implementation", "text": "The simple potentials used in our implementation come from TextonBoost [19, 13]. We use the 17-dimensional filter bank proposed by Shotton et al. [19] and follow Ladicky \u0301 et al. [13] by adding color, histogram oriented gradients (HOG) and pixel location characteristics. Our evaluation on the MSRC 21 dataset uses this enhanced version of TextonBoost for the simple potentials. For the VOC 2010 dataset, we include the response of Bounding Box object detectors [4] for each object class as 20 additional characteristics. This increases the performance of the simple classifiers on the VOC 2010 from 13% to 22%. We gain an additional 5% by building a logistic regression classifier to the responses of the increased classification [4] for each object class."}, {"heading": "6 Evaluation", "text": "The first is the MSRC-21 dataset, which consists of 591 color images of size 320 x 213; the second is the PASCAL VOC dataset, which contains 1928 color images of size 500 x 400, with a total of 20 object classes and a background class; the presented approach was used alongside the adjacence (grid) by Shotton et al. [19] and the rugged Pn CRF by Kohli al. [9], which uses publicly available reference implementations to ensure that all models described in Section 5. All experiments were conducted on an Intel i7-930 processor clocked at 2.80GHz. Eight CPU cores were used for training; all other experiments were performed."}], "references": [{"title": "Fast high-dimensional filtering using the permutohedral lattice", "author": ["A. Adams", "J. Baek", "M.A. Davis"], "venue": "Computer Graphics Forum, 29(2),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian kd-trees for fast high-dimensional filtering", "author": ["A. Adams", "N. Gelfand", "J. Dolson", "M. Levoy"], "venue": "ACM Transactions on Graphics, 28(3),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "The PASCAL Visual Object Classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Cascade object detection with deformable part models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D.A. McAllester"], "venue": "Proc. CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Class segmentation and object localization with superpixel neighborhoods", "author": ["B. Fulkerson", "A. Vedaldi", "S. Soatto"], "venue": "Proc. ICCV,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Object categorization using co-occurrence, location and appearance", "author": ["C. Galleguillos", "A. Rabinovich", "S. Belongie"], "venue": "Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-class segmentation with relative location prior", "author": ["S. Gould", "J. Rodgers", "D. Cohen", "G. Elidan", "D. Koller"], "venue": "IJCV, 80(3),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiscale conditional random fields for image labeling", "author": ["X. He", "R.S. Zemel", "M.A. Carreira-Perpinan"], "venue": "Proc. CVPR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["P. Kohli", "L. Ladick\u00fd", "P.H.S. Torr"], "venue": "IJCV, 82(3),", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "What energy functions can be minimized via graph cuts? PAMI", "author": ["V. Kolmogorov", "R. Zabih"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "A hierarchical field framework for unified context-based classification", "author": ["S. Kumar", "M. Hebert"], "venue": "Proc. ICCV,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Associative hierarchical crfs for object class image segmentation", "author": ["L. Ladick\u00fd", "C. Russell", "P. Kohli", "P.H.S. Torr"], "venue": "Proc. ICCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Graph cut based inference with co-occurrence statistics", "author": ["L. Ladick\u00fd", "C. Russell", "P. Kohli", "P.H.S. Torr"], "venue": "Proc. ECCV,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proc. ICML,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "A fast approximation of the bilateral filter using a signal processing approach", "author": ["S. Paris", "F. Durand"], "venue": "IJCV, 81(1),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "RF) \u2013 random forest random field", "author": ["N. Payet", "S. Todorovic"], "venue": "Proc. NIPS.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Objects in context", "author": ["A. Rabinovich", "A. Vedaldi", "C. Galleguillos", "E. Wiewiora", "S. Belongie"], "venue": "Proc. ICCV,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["J. Shotton", "J.M. Winn", "C. Rother", "A. Criminisi"], "venue": "IJCV, 81(1),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "The scientist and engineer\u2019s guide to digital signal processing", "author": ["S.W. Smith"], "venue": "California Technical Publishing,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Sharing visual features for multiclass and multiview object detection", "author": ["A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "PAMI, 29(5),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Random field model for integration of local information and global information", "author": ["T. Toyoda", "O. Hasegawa"], "venue": "PAMI, 30,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Scene segmentation with crfs learned from partially labeled images", "author": ["J.J. Verbeek", "B. Triggs"], "venue": "Proc. NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "A common approach is to pose this problem as maximum a posteriori (MAP) inference in a conditional random field (CRF) defined over pixels or image patches [8, 12, 18, 19, 9].", "startOffset": 155, "endOffset": 173}, {"referenceID": 11, "context": "A common approach is to pose this problem as maximum a posteriori (MAP) inference in a conditional random field (CRF) defined over pixels or image patches [8, 12, 18, 19, 9].", "startOffset": 155, "endOffset": 173}, {"referenceID": 17, "context": "A common approach is to pose this problem as maximum a posteriori (MAP) inference in a conditional random field (CRF) defined over pixels or image patches [8, 12, 18, 19, 9].", "startOffset": 155, "endOffset": 173}, {"referenceID": 18, "context": "A common approach is to pose this problem as maximum a posteriori (MAP) inference in a conditional random field (CRF) defined over pixels or image patches [8, 12, 18, 19, 9].", "startOffset": 155, "endOffset": 173}, {"referenceID": 8, "context": "A common approach is to pose this problem as maximum a posteriori (MAP) inference in a conditional random field (CRF) defined over pixels or image patches [8, 12, 18, 19, 9].", "startOffset": 155, "endOffset": 173}, {"referenceID": 18, "context": "Basic CRF models are composed of unary potentials on individual pixels or image patches and pairwise potentials on neighboring pixels or patches [19, 23, 7, 5].", "startOffset": 145, "endOffset": 159}, {"referenceID": 22, "context": "Basic CRF models are composed of unary potentials on individual pixels or image patches and pairwise potentials on neighboring pixels or patches [19, 23, 7, 5].", "startOffset": 145, "endOffset": 159}, {"referenceID": 6, "context": "Basic CRF models are composed of unary potentials on individual pixels or image patches and pairwise potentials on neighboring pixels or patches [19, 23, 7, 5].", "startOffset": 145, "endOffset": 159}, {"referenceID": 4, "context": "Basic CRF models are composed of unary potentials on individual pixels or image patches and pairwise potentials on neighboring pixels or patches [19, 23, 7, 5].", "startOffset": 145, "endOffset": 159}, {"referenceID": 7, "context": "In order to improve segmentation and labeling accuracy, researchers have expanded the basic CRF framework to incorporate hierarchical connectivity and higher-order potentials defined on image regions [8, 12, 9, 13].", "startOffset": 200, "endOffset": 214}, {"referenceID": 11, "context": "In order to improve segmentation and labeling accuracy, researchers have expanded the basic CRF framework to incorporate hierarchical connectivity and higher-order potentials defined on image regions [8, 12, 9, 13].", "startOffset": 200, "endOffset": 214}, {"referenceID": 8, "context": "In order to improve segmentation and labeling accuracy, researchers have expanded the basic CRF framework to incorporate hierarchical connectivity and higher-order potentials defined on image regions [8, 12, 9, 13].", "startOffset": 200, "endOffset": 214}, {"referenceID": 12, "context": "In order to improve segmentation and labeling accuracy, researchers have expanded the basic CRF framework to incorporate hierarchical connectivity and higher-order potentials defined on image regions [8, 12, 9, 13].", "startOffset": 200, "endOffset": 214}, {"referenceID": 8, "context": "This limits the ability of region-based approaches to produce accurate label assignments around complex object boundaries, although significant progress has been made [9, 13, 14].", "startOffset": 167, "endOffset": 178}, {"referenceID": 12, "context": "This limits the ability of region-based approaches to produce accurate label assignments around complex object boundaries, although significant progress has been made [9, 13, 14].", "startOffset": 167, "endOffset": 178}, {"referenceID": 13, "context": "This limits the ability of region-based approaches to produce accurate label assignments around complex object boundaries, although significant progress has been made [9, 13, 14].", "startOffset": 167, "endOffset": 178}, {"referenceID": 17, "context": "Fully connected CRFs have been used for semantic image labeling in the past [18, 22, 6, 17], but the complexity of inference in fully connected models has restricted their application to sets of hundreds of image regions or fewer.", "startOffset": 76, "endOffset": 91}, {"referenceID": 21, "context": "Fully connected CRFs have been used for semantic image labeling in the past [18, 22, 6, 17], but the complexity of inference in fully connected models has restricted their application to sets of hundreds of image regions or fewer.", "startOffset": 76, "endOffset": 91}, {"referenceID": 5, "context": "Fully connected CRFs have been used for semantic image labeling in the past [18, 22, 6, 17], but the complexity of inference in fully connected models has restricted their application to sets of hundreds of image regions or fewer.", "startOffset": 76, "endOffset": 91}, {"referenceID": 16, "context": "Fully connected CRFs have been used for semantic image labeling in the past [18, 22, 6, 17], but the complexity of inference in fully connected models has restricted their application to sets of hundreds of image regions or fewer.", "startOffset": 76, "endOffset": 91}, {"referenceID": 8, "context": "(c) Classification produced by the Robust P CRF [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "(d) Classification produced by MCMC inference [17] in a fully connected pixel-level CRF model; the algorithm was run for 36 hours and only partially converged for the bottom image.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "This allows us to reduce the computational complexity of message passing from quadratic to linear in the number of variables by employing efficient approximate high-dimensional filtering [16, 2, 1].", "startOffset": 187, "endOffset": 197}, {"referenceID": 1, "context": "This allows us to reduce the computational complexity of message passing from quadratic to linear in the number of variables by employing efficient approximate high-dimensional filtering [16, 2, 1].", "startOffset": 187, "endOffset": 197}, {"referenceID": 0, "context": "This allows us to reduce the computational complexity of message passing from quadratic to linear in the number of variables by employing efficient approximate high-dimensional filtering [16, 2, 1].", "startOffset": 187, "endOffset": 197}, {"referenceID": 16, "context": "Figure 1(d) shows the results of approximate MCMC inference in fully connected CRFs on these images [17].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "We have also experimented with graph cut inference in the fully connected models [11], but it did not converge within 72 hours.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "in a set of cliques CG in G induces a potential \u03c6c [15].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "The smoothness kernel removes small isolated regions [19].", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "Instead of computing the exact distribution P (X), the mean field approximation computes a distribution Q(X) that minimizes the KL-divergence D(Q\u2016P ) among all distributions Q that can be expressed as a product of independent marginals, Q(X) = \u220f iQi(Xi) [10].", "startOffset": 254, "endOffset": 258}, {"referenceID": 19, "context": "By the sampling theorem, this function can be reconstructed from a set of samples whose spacing is proportional to the standard deviation of the filter [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "We can thus perform the convolution by downsampling Q(l), convolving the samples with G\u039b(m) , and upsampling the result at the feature points [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 15, "context": "This implies that approximate message passing can be performed in O(N) time [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "structure that tiles the feature space with simplices arranged along d+1 axes [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 20, "context": "First, the boosted unary classifiers are trained using the JointBoost algorithm [21], using the features described in Section 5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "The unary potentials used in our implementation are derived from TextonBoost [19, 13].", "startOffset": 77, "endOffset": 85}, {"referenceID": 12, "context": "The unary potentials used in our implementation are derived from TextonBoost [19, 13].", "startOffset": 77, "endOffset": 85}, {"referenceID": 18, "context": "[19], and follow Ladick\u00fd et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] by adding color, histogram of oriented gradients (HOG), and pixel location features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "For the VOC 2010 dataset we include the response of bounding box object detectors [4] for each object class as 20 additional features.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "For efficient high-dimensional filtering, we use a publicly available implementation of the permutohedral lattice [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 18, "context": "The first is the MSRC-21 dataset, which consists of 591 color images of size 320 \u00d7 213 with corresponding ground truth labelings of 21 object classes [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 2, "context": "The second is the PASCAL VOC 2010 dataset, which contains 1928 color images of size approximately 500 \u00d7 400, with a total of 20 object classes and one background class [3].", "startOffset": 168, "endOffset": 171}, {"referenceID": 18, "context": "[19] and the Robust P CRF of Kohli et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9], using publicly available reference implementations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "We use the standard split of the dataset into 45% training, 10% validation and 45% test images [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 18, "context": "We report the standard measures of multi-class segmentation accuracy: \u201cglobal\u201d denotes the overall percentage of correctly classified image pixels and \u201caverage\u201d is the unweighted average of per-category classification accuracy [19, 9].", "startOffset": 227, "endOffset": 234}, {"referenceID": 8, "context": "We report the standard measures of multi-class segmentation accuracy: \u201cglobal\u201d denotes the overall percentage of correctly classified image pixels and \u201caverage\u201d is the unweighted average of per-category classification accuracy [19, 9].", "startOffset": 227, "endOffset": 234}, {"referenceID": 8, "context": "[9], we manually produced accurate segmentations and labelings for a set of images from the MSRC-21 dataset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] for evaluating segmentation accuracy around boundaries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Segmentation accuracy was measured using the standard VOC measure [3].", "startOffset": 66, "endOffset": 69}], "year": 2012, "abstractText": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.", "creator": "Unknown"}}}