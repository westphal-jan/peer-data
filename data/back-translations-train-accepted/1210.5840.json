{"id": "1210.5840", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2012", "title": "Supervised Learning with Similarity Functions", "abstract": "We address the problem of general supervised learning when data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multi-class classification problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification. We give a \"goodness\" criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using \"good\" similarity functions. We demonstrate the effectiveness of our model on three important super-vised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness definition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs.", "histories": [["v1", "Mon, 22 Oct 2012 08:55:13 GMT  (945kb,DS)", "http://arxiv.org/abs/1210.5840v1", "To appear in the proceedings of NIPS 2012, 30 pages"]], "COMMENTS": "To appear in the proceedings of NIPS 2012, 30 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["purushottam kar", "prateek jain 0002"], "accepted": true, "id": "1210.5840"}, "pdf": {"name": "1210.5840.pdf", "metadata": {"source": "CRF", "title": "Supervised Learning with Similarity Functions", "authors": ["Purushottam Kar", "Prateek Jain"], "emails": ["purushot@cse.iitk.ac.in", "prajain@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2 Problem formulation and Preliminaries", "text": "The goal in the similarity-based monitoring of the domain is to approximate narrowly to a goal predictor (\"X \u2192 Y over any domain X.\"). (\"K.\" (\"K.\"): \"X.\" (\"K.\") \"X.\" (\"K.\") \"X.\" (\"K.\"). \"(\" K. \").\" (\"K.\"). \"(\" K. \").\" X. \"(\" K. \")\" X. \"(\" K. \").\" (\"K.\"). \"(\" K. \").\" (\"K.\"). \"(\" K. \").\" (\"K.\"). \"(\" K. \").\" (\"K.\"). \"(\" K. \").\" (\"K.\"). \"(\" K. \").\" (\"K.\" (\").\" (\"K.\"). \"(\" K. \").\" (\"(\" K. \").\" (\").\" (\"K.\"). \"(\" (\").\" (\"K.\"). \"(\" (\").\" (\").\" (\"K.\"). \"(\"). \"(\" (\").\" (\").\" (\").\" (\"(\" K.).). \"(\" (\").). (\").). (\"(\").). (.).). (\"(.). (.). (.).). (.). (.).). (.). (.).). (.). (. (.).). (.). (.). (.).). (.). (.). (.). (.).). (.). (). (.).). (.). ().). (). (). (.). ().).). (.). (.). (.). (.). ().).). ().). (). ().).)."}, {"heading": "2.1 Utility", "text": "Definition 3 (User Instruction). A similarity function K is described as follows: 0-useful w.r.t. a loss function \"actual\" (\u00b7, \u00b7) if the following applies: There is a learning algorithm A, which for any 1, \u03b4 > 0, specifying poly (1 / 1, log (1 / 3)) \"labeled\" and \"unlabeled\" samples from the input distribution D, with a probability of at least 1 \u2212 \u03b4, is a hypothesis f (x; K) s.t. Ex-Dr's actual (f (x), y (x)) z \u2264 0 + 1.Note that f (x; K) on the data exclusively by K.Here, the term 0 covers the mismatch or bias of the similarity function with respect to the learning problem. Note that the above user requirement definition allows learning from unlabeled data points and thus integrating our approach into the semi-supervised learning frame.All of our instruction manuals, we first proceed to a blank instruction manual, using an unlabeled instruction manual."}, {"heading": "2.2 Admissibility", "text": "In order to show that our models are not too rigid, we would prove that they allow good PSD cores. To us, the idea of a good PSD kernel will be one that corresponds to a dominant technique with large margins for the given problem. Generally, most concepts correspond to the existence of a linear operator in the RKHS of the kernel, which exhibits small losses with large margins. Formally, Definition 4 (Good PSD kernel) is considered. In a learning task y: X \u2192 Y over any distribution D, a PSD kernel K: X \u00b7 X \u2192 R with associated RKHSHK and canonical characteristic card \u03a6K: X \u2192 HK in relation to a loss function \"K: R \u00d7 Y \u2192 R, if there is W \u0445 HK, so that the results of W \u0442 = 1 and E x \u0445 Ds'K (< W \u0445 K (x) > GP, y (x) -R look like that for all tasks that we are considered to be a similar function, we will not be treated with any good results (PSEL = 1)."}, {"heading": "3 Applications", "text": "We will now use usefulness and admissibility guarantees to lead the general learning model described above to a real evaluated regression, ordinal regression and precedence order. Due to lack of space, we refer all the evidence and the discussion about precedence to the supplementary material (Appendix F)."}, {"heading": "3.1 Real-valued Regression", "text": "In the following, we present algorithms for performing real-rated regression using non-PSD-like measures. We consider the problem with \"actual (a, b) = | a \u2212 b | to be the true loss function. (For surrogates\" S \"and\" K, \"we choose the -insensitive loss function [1], which is defined as follows:\" (a, b) = \"(a \u2212 b) =\" 0 if | a \u2212 b | < | a \u2212 b |, otherwise. \"The above loss function automatically gives us ideas of good cores and similarity functions by invoking definitions 4 and 2, respectively. It is easy to apply error limits with respect to absolute errors to those relating to mean squared errors (MSE), which are a commonly used performance measure for real-rated regression."}, {"heading": "3.2 Sparse regression models", "text": "The problem is that the relative frequency of such nodes is low, then random selection would force us to choose a large number of landmarks before enough \"informative\" landmarks are used. [8, 12] However, this greatly increases training and testing times due to the increased cost of constructing the landmark space. [8, 12] In contrast, we guarantee that our predictor will select a small number of landmarks while selecting a limited number of landmarks, while this requires careful restructuring of the learning model to incorporate the \"informativeness\" of landmarks."}, {"heading": "3.3 Ordinal Regression", "text": "The problem of ordinal regression requires an accurate prediction of the (discrete) labels coming from a finite ordered set is always greater than the absolute error rate [r] = {1, 2,., r}. The problem is similar to both classification and regression, but has some different characteristics, due to which it has received independent attention [16, 17] in areas such as product ratings, etc. The most popular performance measure for this problem is the absolute loss, which is the absolute difference between the predicted and the true labels. A natural and rather seductive way to solve this problem is to relax the problem of real rated regression and threshold, the output of the learned predictor using predefined thresholds b1,., br to obtain discrete labels is prevalent in the literature [17], since the discussion in the supplementary material shows that this leads to poor generalization guarantees in our model."}, {"heading": "4 Experimental Results", "text": "In fact, it is such that most of us are in a position to embark on a search for a solution that is in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "5 Conclusion", "text": "In this paper, we looked at the general problem of supervised learning with non-PSD-like functions, while at the same time showing that our criterion is not too restrictive for each learning task as it allows all good PSD cores. We then focused on the problem of identifying influential milestones with the goal of learning sparse predictors. Finally, we presented a model that formalized the intuition that typically only a small fraction of milestones influence a particular learning problem. We adapted existing sparse vector recovery algorithms within our model to learn demonstrably sparse predictors with limited generalization errors. Finally, we empirically evaluated our learning algorithms using benchmark regression tasks and ordinary regression tasks. In all cases, our learning methods, especially the sparse recovery algorithm, consistently outperformed the large generalization errors."}, {"heading": "Acknowledgments", "text": "P. K. is supported by a Microsoft Research India Ph.D. Fellowship Award. Part of this work was done during his time as an intern at Microsoft Research Labs India, Bangalore."}, {"heading": "Appendix A Proofs of supplementary theorems", "text": "In this section, we provide evidence of certain generic results that would be used in the supply and admissibility testing. < / p > The first result, given as Lemma 15, allows us to analyze the Landmark Step (Step 1 of Algorithm 1) and allows us to reduce the learning problem to learning a linear predictor over the Landmark Space. < p > p > p (Point 2) gives us a concise representation of the Landmark Error Margins that would be used in testing usability boundaries. The third result, given as Lemma 17, is a technical result that helps us prove the admissibility of our Landmark Definitions. & ltr (Point 15) Lemma 15 (Landmark Approximation Guarantee [8]). Given a similarity function K over a domain X and a limited function of the form f (x) = Ex-D Yw (x)."}, {"heading": "Appendix B Justifying Double-dipping", "text": "All our analyses (as well as the analyses we have presented in [6, 7, 8], use some data as reference points and then require a new set of training points to learn a classifier on the map. [7, 8] This is especially true for [7, 8], which require a uniform approach to the map. [8] We present a generic argument similar to Lemma 16 to the various learning problems considered in this paper.To make the presentation easier, we set some notations f, let Lf = Ex-D J '(f x), y (x) K and for each training S of size."}, {"heading": "Appendix C Regression with Similarity Functions", "text": "In this section we provide evidence of utility and admissibility results for our similarity based on learning models (32 x) (32 x) (32 x) (32 x) (32 x) (32 x) (32 x) (32 x))."}, {"heading": "Appendix D Sparse Regression with Similarity functions", "text": "(1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2) (2) (1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (3) () () () () () () () () () () ()) () () () () () () () () () () () () () () ()) () () () ()) () () ()) () () ()) () () ()) () () ()) () () ()) ()) () ()) () () ()) ()) () () ())) () ()) () () ()) () ()) () ()) () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () () (() () () () (() () () (() () () (() () () (() () () (() () () (() () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()"}, {"heading": "Appendix E Ordinal Regression", "text": "In this section we give missing evidence of usefulness and admissibility for the similarity-based learning model for ordinal regression."}, {"heading": "Appendix F Ranking", "text": "The problem of precedence arises from the need to sort a series of points based on their relevance. In the model considered here, each precedence consists of m documents (pages) (p1,.., pm) of any universe P along with their relevance to any particular precedence Q, which are given as relevance values of any set DP. Thus, for each z = (p, q).P \u00b7 Q, let r (z).R denote the true relevance of the document p to query q.For each Relevant vector r: Rm, let r-be be the vector with elements of r sorted in descending order and ascending order we will be the permutation that induces this sort."}, {"heading": "Appendix G Supplementary Experimental Results", "text": "In the following, we present further experimental results for regression and ordinal regression problems.G.1 Regression experiments We present results for various benchmark data sets, which are taken into account in Section 4 for Gaussian K (x, y) = exp (\u2212 x \u2212 y, 2 22\u03c32) and Euclidean: K (x, y) = \u2212 x \u2212 y \u00b2 22 nuclei. In accordance with standard practice, we find that wefixed \u043c is the average pair distance between data points in the training set. G.2 Ordinary regression experiments We present results for various benchmark data sets, which are taken into account in Section 4 for Gaussian K (x, y) = exp (\u2212 x \u2212 y \u00b2 2 22\u03c32) and Manhattan: K (x, y) = \u2212 x \u2212 y \u00b2 1 nuclei."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "We address the problem of general supervised learning when data can only be ac-<lb>cessed through an (indefinite) similarity function between data points. Existing<lb>work on learning with indefinite kernels has concentrated solely on binary/multi-<lb>class classification problems. We propose a model that is generic enough to handle<lb>any supervised learning task and also subsumes the model previously proposed for<lb>classification. We give a \u201cgoodness\u201d criterion for similarity functions w.r.t. a given<lb>supervised learning task and then adapt a well-known landmarking technique to<lb>provide efficient algorithms for supervised learning using \u201cgood\u201d similarity func-<lb>tions. We demonstrate the effectiveness of our model on three important super-<lb>vised learning problems: a) real-valued regression, b) ordinal regression and c)<lb>ranking where we show that our method guarantees bounded generalization error.<lb>Furthermore, for the case of real-valued regression, we give a natural goodness<lb>definition that, when used in conjunction with a recent result in sparse vector re-<lb>covery, guarantees a sparse predictor with bounded generalization error. Finally,<lb>we report results of our learning algorithms on regression and ordinal regression<lb>tasks using non-PSD similarity functions and demonstrate the effectiveness of<lb>our algorithms, especially that of the sparse landmark selection algorithm that<lb>achieves significantly higher accuracies than the baseline methods while offering<lb>reduced computational costs.", "creator": "LaTeX with hyperref package"}}}