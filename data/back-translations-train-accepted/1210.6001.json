{"id": "1210.6001", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2012", "title": "Reducing statistical time-series problems to binary classification", "abstract": "We show how binary classification methods developed to work on i.i.d. data can be used for solving statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. The algorithms that we construct for solving these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the proposed algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.", "histories": [["v1", "Mon, 22 Oct 2012 19:02:21 GMT  (28kb,D)", "http://arxiv.org/abs/1210.6001v1", null], ["v2", "Mon, 28 Jan 2013 10:25:38 GMT  (28kb,D)", "http://arxiv.org/abs/1210.6001v2", null], ["v3", "Fri, 7 Jun 2013 09:45:45 GMT  (28kb,D)", "http://arxiv.org/abs/1210.6001v3", "In proceedings of NIPS 2012, pp. 2069-2077"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["daniil ryabko", "j\u00e9r\u00e9mie mary"], "accepted": true, "id": "1210.6001"}, "pdf": {"name": "1210.6001.pdf", "metadata": {"source": "CRF", "title": "Reducing statistical time-series problems to binary classification", "authors": ["Daniil Ryabko"], "emails": ["daniil@ryabko.net", "Jeremie.Mary@inria.fr"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before in the history of the city, where it is so far that it is a place, where it is a place, where it is a place."}, {"heading": "2 Notation and definitions", "text": "Let (X, FX) be a measurable space (the domain). Time series (or process) distributions are probability measures on the space (XN, FN) of an infinite one-way line (where FN is the Borel sigma algebra of XN). We use the abbreviation X1.. k for X1,..., Xk. All sets and functions presented below (especially the setsHk and its elements) are assumed to be measurable. A stationary distribution is stationary if \u03c1 (X1.. k, A) = \u03c1 (Xn + 1.. n + k, A) for all A-FXk, k, n-N (where FXk is the sigma algebra of Xk). A stationary distribution is called (stationary) ergodically if limn \u2192 \u221e 1n, i = 1.. n \u2212 k + 1 IXi.. i + k, n-a.s (A) for each Xk algebra of Xk)."}, {"heading": "3 A distance between time-series distributions", "text": "We start with a distance between the distributions to X, and then we will expand them to distributions to X (X). For two probability distributions P and Q to (X, F) and a series of measurements to X, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D"}, {"heading": "4 Calculating D\u0302H using binary classification methods", "text": "The methods for solving various statistical problems that we propose are all based on Q-H. The main attraction of this approach is that D-H can be calculated using binary classification methods. (2) The definition (2) of DH includes the calculation of l-K problems (where l: = min {n, m}), that the output h-K + K-K + K-K-K-K (1), like the definition (1) -K-K + K-1) -K-K errors (where l: = 1 h (Yi.. i + K-1), like the definition of h-K + K-K-K-K-K (1), like the definition of h-K-K-K-K-1), like the definition of h-K-K-K, like the K-K-K definition of h-K-K-K-K-K-1), like the definition of H-K-K-K, like the K-K-K-K, like the K-K-K-K definition of K, like the K-K-K, like the K-K-K-K."}, {"heading": "5 The three-sample problem", "text": "We start with a conceptually simple problem known in statistics as the tripartite distribution problem (sometimes referred to as time series classification). It is known that X and Y were generated by different time series distributions, whereas Z was generated by the same distribution as X or Y. It is necessary to find out which of these is the case. Both distributions are assumed to be stationary ergodic, but no further assumptions are made about them (no independence, mixing or memory assumptions). The three-sample problem for dependent time series was found in [9] for Markov processes and in [23] for stationary ergodic time series. The latter work uses an approach based on distribution spaces.In fact, it is sufficient to have consistent estimates for certain time series distributions."}, {"heading": "6 Clustering time series", "text": "It is not possible to group the N samples into k-groups (clusters), i.e. to give a division of {X1.. XN} into k-sets into k-sets into k-sets. While there can be many different approaches to defining what good clustering is (and, in general, deciding what good clustering is, is a difficult problem), for the problem of classifying time series samples there is a natural choice, proposed in [21]: these samples should be put together that were generated by the same distribution. Thus, we define the target clusters as the partitioning in which these and only those samples were generated."}, {"heading": "7 Speed of convergence", "text": "The results found so far are asymptotic out of necessity: they are generated on the assumption that the distributions involved are stationary ergodic, which is too general to allow any meaningful finite performance guarantees. Furthermore, some statistical problems, such as homogeneity tests or clustering when the number of clusters is unknown, are demonstrably impossible to solve under this assumption [22]. Although it is interesting to find consistency results under such general assumptions, it is also interesting to see what results can be achieved under stronger assumptions. Since it is usually not known in advance whether the available data satisfies given assumptions or not, it seems important to have methods that both guarantee asymptotic consistency in the general environment and guarantee end-time performance under stronger assumptions."}, {"heading": "7.1 Speed of convergence of D\u0302", "text": "Suppose that a sample X1.. n is generated by a distribution that is uniformly \u03b2-mixing with coefficients \u03b2 (\u03c1, k). < n < p > p > p > p > p > p > p > p (2) The general tool we use to obtain performance guarantees in this section is the following limit that results from the results of [12].qn (\u03c1, Hk, \u03b5): = p (sup h, Hk)."}, {"heading": "7.2 Homogeneity testing", "text": "Given two samples X1.. n and Y1.. m generated by distributions \u03c1X and \u03c1Y, respectively, the problem of homogeneity testing (or the two-sample problem) is to decide whether \u03c1X = \u03c1Y. A test is called (asymptotically) consistent if its probability of error is zero, as n \u2032: = min {m, n} goes infinitely. Generally, there is no asymptotically consistent test for homogeneity for stationary ergodic time series distributions [22], so stronger assumptions are in order.Homogeneity testing is one of the classic problems of mathematical statistics, and one of the most studied. Extensive literature exits to homogeneity tests for i.i.d. data and also for dependent processes. We are not trying to examine this literature here. Our contribution to this line of research is to show that this problem can be reduced (via telescope spacing) to binary classification."}, {"heading": "7.3 Clustering with a known or unknown number of clusters", "text": "If the distributions that produce the samples meet certain mixing conditions, then we can supplement theorems 3 and 4 with guarantees for the performance of finite samples. Theorem 6. Let the distributions \u03c11,.., \u03c1k that generate the samples X1 = (X11,.., X 1 n1),.., X N = (XN1,.., X N nN) meet the conditions of Lemma 2. Define \u03b4: = mini, j = 1.. N, i6 = j DH (\u03c1i, \u03c1j) and n: = mini = 1.. N ni. Then with a probability of least1 \u2212 N (N \u2212 1)."}, {"heading": "8 Experiments", "text": "For the experimental evaluation, we chose the problem of clustering time series. Mean clustering is used, where the telescope distance between samples is calculated using SVM, as in Section 4. In all experiments, SVM with radial base core is used, with standard parameters of libsvm [5]."}, {"heading": "8.1 Synthetic data", "text": "For the artificial setting we have chosen highly dependent time series distributions, which have the same one-dimensional marginals and which cannot be easily approximated by finite or countable state models. Distributions \u03c1 (\u03b1), \u03b1 (0, 1) are constructed as follows: The time series (X1, X2,...) is then determined from ri by drawing a point from a distribution lawN1 if ri < 0.5 and from N2 otherwise. N1 is a three-dimensional Gauss with the mean 0 and the covariance matrix Id \u00d7 1 / 4. N2 is equal, but with the mean 1. If \u03b1 is irrational 1, then the1in experiments simulated by a long sequence."}, {"heading": "8.2 Real data", "text": "To demonstrate the applicability of the proposed methods to realistic scenarios, we chose the brain computer interface data from the BCI competition III [17]. The data set consists of (pre-processed) BCI recordings of mental images: A person reflects on one of three topics (left foot, right foot, a random letter). Originally, each time series consisted of several consecutive sequences of different classes, and the problem was monitored: three time series for training and one for testing. We divided each of the original time series into classes and then used our clustering algorithm in a completely unattended environment. The original problem is 96-dimensional, but we used only the first 3 dimensions (using all 96 results in inferior performance). The typical sequence length is 300. Performance is reported in Table 1, labeled TSSVM. All compilations for this experiment take about 6 minutes on a standard laptop. The following methods were used for comparison."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We show how binary classification methods developed to work on i.i.d. data can<lb>be used for solving statistical problems that are seemingly unrelated to classifi-<lb>cation and concern highly-dependent time series. Specifically, the problems of<lb>time-series clustering, homogeneity testing and the three-sample problem are ad-<lb>dressed. The algorithms that we construct for solving these problems are based<lb>on a new metric between time-series distributions, which can be evaluated using<lb>binary classification methods. Universal consistency of the proposed algorithms<lb>is proven under most general assumptions. The theoretical results are illustrated<lb>with experiments on synthetic and real-world data.", "creator": "LaTeX with hyperref package"}}}