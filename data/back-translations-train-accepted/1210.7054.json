{"id": "1210.7054", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2012", "title": "Large-Scale Sparse Principal Component Analysis with Application to Text Data", "abstract": "Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.", "histories": [["v1", "Fri, 26 Oct 2012 05:35:26 GMT  (46kb,D)", "http://arxiv.org/abs/1210.7054v1", "Appeared in the proceedings of NIPS 2011; The Neural Information Processing Systems Conference (NIPS), Granada, Spain, December 2011"]], "COMMENTS": "Appeared in the proceedings of NIPS 2011; The Neural Information Processing Systems Conference (NIPS), Granada, Spain, December 2011", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["youwei zhang", "laurent el ghaoui"], "accepted": true, "id": "1210.7054"}, "pdf": {"name": "1210.7054.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Sparse Principal Component Analysis with Application to Text Data", "authors": ["Youwei Zhang"], "emails": ["zyw@eecs.berkeley.edu", "elghaoui@eecs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "The sparse Principal Component Analysis (Sparse PCA) problem is a variant of the classic PCA problem, which creates a trade-off between the declared variance along a normalized vector and the number of non-zero components of that vector. Sparse PCA not only provides a better interpretation [1], but also provides statistical regulation [2] when the number of samples is less than the number of features. Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholds [4], to greedy algorithms [5, 6] which include other algorithms SCoTLASS through [7], the regulated SVD method through [9] and the generalized power method through [10]. These algorithms are based on non-convex formulations and can only approximate to a local optimum."}, {"heading": "2 Safe Feature Elimination", "text": "In view of a n \u00b0 \u00b0 n positive-semidefinitive Matrixproblems is the \"sparse PCA\" -problem, which was introduced in [1], following the following steps:?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "3 Block Coordinate Ascent Algorithm", "text": "In this section we will develop a block coordinate algorithm with better dependence on the problem size (O (n3)), which in practice is much faster.Failure of a direct method. We will try to apply a row-by-row algorithm by which we update each row / column pair, one at a time. This algorithm appeared in the specific context of the frugal covariance estimation in [13], and extended to a large class of SDPs in [14]. Specifically, it applies to problems of formmin Xf (X) \u2212 \u03b2 log detX, X 0, (4) whereX = XT is a large class of SDPs in [14]."}, {"heading": "4 Numerical Examples", "text": "In this section, we analyze two publicly available large datasets, the NYTimes News article data and the PubMed abstract data provided by the UCI Machine Learning Repository [16]. BothAlgorithm 1 Block Coordinate Ascent Algorithm Input: The covariance matrix \u03a3, and a parameter \u03c1 > 0. 1: Set X (0) = I 2: repeat 3: for j = 1 to n do 4: Let X (j \u2212 1) denotes the current iteration. Solve the box-limited square programR2: = min u uTX (j \u2212 1)\\ j _ j u: \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s."}, {"heading": "5 Conclusion", "text": "The method we propose works particularly well when the target cardinality of the result is small, which is often the case in applications where human interpretability is key. The algorithm we propose has better computational complexity and converges much faster in practice than the first-order algorithm developed in [1]. Our experiments with text data also show that the sparse PCA can be a promising approach to summarizing and organizing a large body of text."}], "references": [{"title": "A direct formulation of sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M. Jordan", "G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["A.A. Amini", "M. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Rotation of principal components: choice of normalization constraints", "author": ["I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Loadings and correlations in the interpretation of principal components", "author": ["J. Cadima", "I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Spectral bounds for sparse PCA: exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "A modified principal component technique based on the LASSO", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Sparse Principal Component Analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational & Graphical Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["Haipeng Shen", "Jianhua Z. Huang"], "venue": "J. Multivar. Anal.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Sparse PCA: Convex relaxations, algorithms and applications", "author": ["Y. Zhang", "A. d\u2019Aspremont", "L. El Ghaoui"], "venue": "Handbook on Semide nite, Cone and Polynomial Optimization: Theory, Algorithms, Software and Applications. Springer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On the quality of a semidefinite programming bound for sparse principal component analysis", "author": ["L. El Ghaoui"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data", "author": ["O.Banerjee", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Row by row methods for semidefinite programming", "author": ["Zaiwen Wen", "Donald Goldfarb", "Shiqian Ma", "Katya Scheinberg"], "venue": "Technical report, Dept of IEOR, Columbia University,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Sparse PCA not only brings better interpretation [1], but also provides statistical regularization [2] when the number of samples is less than the number of features.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Sparse PCA not only brings better interpretation [1], but also provides statistical regularization [2] when the number of samples is less than the number of features.", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 4, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 5, "context": "Various researchers have proposed different formulations and algorithms for this problem, ranging from ad-hoc methods such as factor rotation techniques [3] and simple thresholding [4], to greedy algorithms [5, 6].", "startOffset": 207, "endOffset": 213}, {"referenceID": 6, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Other algorithms include SCoTLASS by [7], SPCA by [8], the regularized SVD method by [9] and the generalized power method by [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "The `1-norm based semidefinite relaxation DSPCA, as introduced in [1], does guarantee global convergence and as such, is an attractive alternative to local methods.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS and SPCA algorithms, often underperform DSPCA.", "startOffset": 30, "endOffset": 40}, {"referenceID": 1, "context": "In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS and SPCA algorithms, often underperform DSPCA.", "startOffset": 30, "endOffset": 40}, {"referenceID": 10, "context": "In fact, it has been shown in [1, 2, 11] that simple ad-hoc methods, and the greedy, SCoTLASS and SPCA algorithms, often underperform DSPCA.", "startOffset": 30, "endOffset": 40}, {"referenceID": 0, "context": "However, the first-order algorithm for solving DSPCA, as developed in [1], has a computational complexity ofO(n \u221a log n), with n the number of", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Then we develop a block coordinate ascent algorithm, with a computational complexity of O(n) to solve DSPCA, which is much faster than the firstorder algorithm proposed in [1].", "startOffset": 172, "endOffset": 175}, {"referenceID": 0, "context": "Given a n\u00d7 n positive-semidefinite matrix \u03a3, the \u201csparse PCA\u201d problem introduced in [1] is : \u03c6 = max Z Tr\u03a3Z \u2212 \u03bb\u2016Z\u20161 : Z 0, TrZ = 1 (1) where \u03bb \u2265 0 is a parameter encouraging sparsity.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "By viewing problem (1) as a convex approximation to the non-convex problem (2), we can leverage the safe feature elimination theorem first presented in [6, 12] for problem (2): Theorem 2.", "startOffset": 152, "endOffset": 159}, {"referenceID": 11, "context": "By viewing problem (1) as a convex approximation to the non-convex problem (2), we can leverage the safe feature elimination theorem first presented in [6, 12] for problem (2): Theorem 2.", "startOffset": 152, "endOffset": 159}, {"referenceID": 0, "context": "The first-order algorithm developed in [1] to solve problem (1) has a computational complexity of O(n \u221a log n).", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "This algorithm appeared in the specific context of sparse covariance estimation in [13], and extended to a large class of SDPs in [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "This algorithm appeared in the specific context of sparse covariance estimation in [13], and extended to a large class of SDPs in [14].", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "The authors in [14] propose an augmented Lagrangian method to deal with such constraints, with a complication due to the choice of appropriate penalty parameters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "SDP theory ensures that if \u03b2 = /n, then a solution to the above problem is -suboptimal for the original problem [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "Therefore, the convergence result from [14] readily applies and hence every limit point that our block coordinate ascent algorithm converges to is the global optimizer.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "Hence our algorithm has better dependence on the problem size compared to O(n \u221a log n) required of the first order algorithm developed in [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "On the right, the covariance matrix comes from a \u201dspiked model\u201d similar to that in [2], with \u03a3 = uu +V V T /m, where u \u2208 R is the true sparse leading eigenvector, with Card(u) = 0.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "The algorithm we proposed has better computational complexity, and in practice converges much faster than, the first-order algorithm developed in [1].", "startOffset": 146, "endOffset": 149}], "year": 2012, "abstractText": "Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.", "creator": "LaTeX with hyperref package"}}}