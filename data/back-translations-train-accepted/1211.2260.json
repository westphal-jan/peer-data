{"id": "1211.2260", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2012", "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization", "abstract": "Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R^n. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x^* are known in advance. We present algorithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of x^*. In particular, regret with respect to x^* = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting.", "histories": [["v1", "Fri, 9 Nov 2012 22:13:10 GMT  (25kb,D)", "http://arxiv.org/abs/1211.2260v1", "To appear"]], "COMMENTS": "To appear", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthew j streeter", "h brendan mcmahan"], "accepted": true, "id": "1211.2260"}, "pdf": {"name": "1211.2260.pdf", "metadata": {"source": "CRF", "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization", "authors": ["Matthew Streeter", "H. Brendan McMahan"], "emails": ["matt@duolingo.com", "mcmahan@google.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, this approach has crystallized as a fundamental tool for solving problems in machine learning (see also: \"This is a problem we can't solve.\"). (\"This is a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we need to solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\" (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\" (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\" (\"It's a problem we need to solve.\") (\"It's a problem we need to solve.\") (\"It's a problem we need to solve.\") (\"It's a problem we can't solve.\" (\"It's a problem we can't solve.\") (\"It's a problem we can't solve.\" (\") (\" It's a problem we can't solve. (\"It's a problem we can't solve.\") (\"It's a problem we can't solve. (\" It's a problem we can't solve. \")"}, {"heading": "2 Reward and Regret", "text": "In this section, we present a general result that converts lower limits of reward into upper limits of regret, for a one-dimensional linear online optimization. In the unrestricted environment, this result will be sufficient to provide guarantees for a general n-dimensional online convex optimization. Theorem 1. Consider an algorithm for a one-dimensional online linear optimization that, when executed on a sequence of gradients g1, g2,..., guarantees that gT, with gt [\u2212 1, 1] for all t, guarantees reward success."}, {"heading": "3 Gradient Descent with Increasing Learning Rates", "text": "In this section we show that increasing the learning rate with gradient descent sometimes leads to new theoretical guarantees. To build intuition, we consider linear online optimization in one dimension, with gradients g1, g2,..., gT, everything in [\u2212 1, 1]. In this setting, the reward with unrestricted gradient descent has a simple closed form: Lemma 2. Consider the unlimited gradient descent in one dimension, with learning rate \u03b7. In the round, this algorithm plays the point xt = \u03b7g1: t \u2212 1. Letters G = | g1: t | and H = \u2211 T = 1 g 2 t, the cumulative reward of the algorithm is exact Reward = \u03b72 (G2 \u2212 H). We give a simple direct proof in Appendix A. Perhaps this result implies that the reward is completely independent of the order of the linear functions selected by the opponent. < If we use the expression Lem2, we see the optimal learning rate at G2 > the reward."}, {"heading": "3.1 Analysis in One Dimension", "text": "In this section we analyze the algorithms REWARD-DOUBLING-1D (algorithm 1), which consist of a series of epochs. We currently assume that an upper reward will be accumulated in the current epoch. \u2212 Let us see the total reward accumulated in the current epoch. \u2212 Let us see a double reward and let us begin a new epoch (return to the origin and forgetting all previous grades except the most recent.).Lemma 3. Applied to a sequence of gradations g1, g2,., gT, all in [\u2212 1, 1], whereH = 1 g, all previous gradations except the most recent epoch."}, {"heading": "3.2 Extension to n dimensions", "text": "In order to extend our results to the general convex optimization, it is sufficient to execute a separate copy of REWARD-DOUBLING-1D-GUESS for each coordinate, as it is done in REWARD-DOUBLING (algorithm 2). The key to analyzing this algorithm is that the general regret is simply the sum of regrets about n one-dimensional sub-problems that can be analyzed independently. Theorem 5. Given a sequence of convex loss functions f1, f2,., fT from Rn to R, REWARD-DOUBLING with i = n has regret by regret (amounx) xi + c n."}, {"heading": "4 An Epoch-Free Algorithm", "text": "In this section, we will analyze SMOOTH-REWARD-DOUBLING, a simple algorithm that reaches the limits comparable to those of Theorem 4 without guessing-and-doubling. We will consider only the 1-d problem, since the technique of Theorem 5 can be applied to n dimensions. (Given a parameter \u03b7 > 0, we will reach Regret \u2264 R \u221a T (log (RT 3 / 2\u03b7) \u2212 1) + 1.76\u03b7, (7) for all T and R, which is better (by constant factors) than Theorem 4, if gt: 1, 1} (which implies that T = H) can be worse if H < T. The idea of the algorithm is to maintain the invariant that our cumulative reward as a function of g1: t and t: Satisfies reward of n (g1: t, t: t, t: t: t), some fixed function for N."}, {"heading": "5 Lower Bounds", "text": "As with our previous results, it is sufficient to indicate a lower limit in one dimension, since it can then be independently replicated in each coordinate to obtain an n-dimensional limit. Note: Our lower limit contains the factor protocol (1), which can be negative if x-Q is small relative to T, so it is important to hold x-Z and consider the behavior as T-Z. Here, we only specify a protocol in which there is complete proof. Theorem 7. Consider the problem of unlimited linear optimization in one dimension, and an online algorithm that guarantees regrets of origin in most cases. Then, for each fixed comparator x-Z and each integer T0, there is a gradient sequence."}, {"heading": "6 Future Work", "text": "This work leaves many interesting questions unanswered. It should be possible to apply our techniques to problems that have restricted feasible sets; for example, it is natural to apply the problem of unrestricted experts to the positive orthant. Although we believe that this extension is simple, dealing with arbitrary, non-axis oriented constraints will be more difficult. Another possibility is to develop an algorithm that has limits relating to H instead of T that does not employ a conjecture and duplication approach."}, {"heading": "A Proofs", "text": "This appendix gives the reward left out in the body of the paper, with the corresponding lemas and theorems redefined for convenience. (Theorem 1) Consider an algorithm for a one-dimensional linear optimization that, when executed on a sequence of gradients, G1, g2,., gT, with any comparator x. (\u2212 R], we haveRegret (\u0441.x)."}], "references": [{"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Algorithms for portfolio management based on the Newton method", "author": ["Amit Agarwal", "Elad Hazan", "Satyen Kale", "Robert E. Schapire"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In COLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "On stochastic and worst-case models for investing", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["Robert A. Jacobs"], "venue": "Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors", "author": ["Jyrki Kivinen", "Manfred Warmuth"], "venue": "Journal of Information and Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Optimal stochastic search and adaptive momentum", "author": ["Todd K. Leen", "Genevieve B. Orr"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H. Brendan McMahan", "Matthew Streeter"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Gradient descent: Second order momentum and saturating error", "author": ["Barak Pearlmutter"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": ", [3, 12] for an introduction).", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": ", [3, 12] for an introduction).", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": ", [3]), there are n experts, and on each round t the player selects an expert (say i), and obtains reward gt,i from a bounded interval (say [\u22121, 1]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "It is useful to contrast our results in this setting to previous applications of online convex optimization to portfolio management, for example [6] and [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "It is useful to contrast our results in this setting to previous applications of online convex optimization to portfolio management, for example [6] and [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Table 1 compares the bounds for REWARD-DOUBLING (this paper) to those of two previous algorithms: online gradient descent [13] and projected exponentiated gradient descent [8, 12].", "startOffset": 172, "endOffset": 179}, {"referenceID": 11, "context": "Table 1 compares the bounds for REWARD-DOUBLING (this paper) to those of two previous algorithms: online gradient descent [13] and projected exponentiated gradient descent [8, 12].", "startOffset": 172, "endOffset": 179}, {"referenceID": 0, "context": "Gradient descent is minimax-optimal [1] when the comparator point is contained in a hypershere whose radius is known in advance (\u2016x\u030a\u20162 \u2264 R) and gradients are sparse (\u2016gt\u20162 \u2264 1, top table).", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "Related Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9].", "startOffset": 155, "endOffset": 165}, {"referenceID": 10, "context": "Related Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9].", "startOffset": 155, "endOffset": 165}, {"referenceID": 8, "context": "Related Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9].", "startOffset": 155, "endOffset": 165}, {"referenceID": 7, "context": "In Follow-The-Regularized-Leader terms, the exponentiated gradient descent algorithm with unnormalized weights of Kivinen and Warmuth [8] plays xt+1 = arg minx\u2208Rn+ g1:t \u00b7 x+ 1 \u03b7 (x log x\u2212 x), which has closed-form solution xt+1 = exp(\u2212\u03b7g1:t).", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Hazan and Kale [5] give regret bounds in terms of the variance of the gt.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "One of the motivations for this work is the observation that the state-of-the-art online gradient descent algorithms adjust their learning rates based only on the observed value ofH (or its upper bound T ); for example [4, 10].", "startOffset": 219, "endOffset": 226}, {"referenceID": 9, "context": "One of the motivations for this work is the observation that the state-of-the-art online gradient descent algorithms adjust their learning rates based only on the observed value ofH (or its upper bound T ); for example [4, 10].", "startOffset": 219, "endOffset": 226}], "year": 2012, "abstractText": "Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x\u030a are known in advance. We present algorithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of x\u030a. In particular, regret with respect to x\u030a = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting.", "creator": "LaTeX with hyperref package"}}}