{"id": "1211.2532", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2012", "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation", "abstract": "The L1-regularized maximum likelihood estimation problem has recently become a topic of great interest within the machine learning, statistics, and optimization communities as a method for producing sparse inverse covariance estimators. In this paper, a proximal gradient method (G-ISTA) for performing L1-regularized covariance matrix estimation is presented. Although numerous algorithms have been proposed for solving this problem, this simple proximal gradient method is found to have attractive theoretical and numerical properties. G-ISTA has a linear rate of convergence, resulting in an O(log e) iteration complexity to reach a tolerance of e. This paper gives eigenvalue bounds for the G-ISTA iterates, providing a closed-form linear convergence rate. The rate is shown to be closely related to the condition number of the optimal point. Numerical convergence results and timing comparisons for the proposed method are presented. G-ISTA is shown to perform very well, especially when the optimal point is well-conditioned.", "histories": [["v1", "Mon, 12 Nov 2012 08:35:26 GMT  (43kb,D)", "https://arxiv.org/abs/1211.2532v1", "25 pages, 1 figure, 4 tables. Conference paper"], ["v2", "Wed, 14 Nov 2012 01:22:30 GMT  (44kb,D)", "http://arxiv.org/abs/1211.2532v2", "25 pages, 1 figure, 4 tables. Conference paper"], ["v3", "Tue, 27 Nov 2012 04:48:51 GMT  (44kb,D)", "http://arxiv.org/abs/1211.2532v3", "25 pages, 1 figure, 4 tables. Conference paper"]], "COMMENTS": "25 pages, 1 figure, 4 tables. Conference paper", "reviews": [], "SUBJECTS": "stat.CO cs.LG stat.ML", "authors": ["benjamin t rolfs", "bala rajaratnam", "dominique guillot", "ian wong", "arian maleki"], "accepted": true, "id": "1211.2532"}, "pdf": {"name": "1211.2532.pdf", "metadata": {"source": "CRF", "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation", "authors": ["Dominique Guillot", "Bala Rajaratnam", "Benjamin T. Rolfs"], "emails": ["dguillot@stanford.edu", "brajarat@stanford.edu", "benrolfs@stanford.edu", "arian.maleki@rice.edu", "ianw@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "A basic example of this is the problem of estimating the covariance matrix resulting from the covariance matrix. [...] If the maximum probability of the covariance estimators is high in the covariance matrix (1), where S + + + denotes the space of the p + symmetrical, positive definitive matrices. [...] If n [...] the maximum probability of the covariance estimators in the covariance matrix (1) is high, the covariance matrix (i) X (i) T is a problem, but one that arises when n < p, due to which the Equal Contriors.ar Xiv: 121 1.25 32v3 [st at.C Oto the rank-deficiency] in S. In this example, there are several common uses."}, {"heading": "2 Prior Work", "text": "Although there are several excellent general convex solvers (e.g. [12] and [4]), they are not always adept at dealing with high-dimensional problems (e.g. p > 1000). As many modern datasets have several thousand variables, numerous authors have proposed efficient algorithms specifically designed to solve the \"1-penalized covariance estimation problem with low maximum probability (1), which can be roughly categorized as primary or dual methods. Following the literature, we refer to primary methods as such that directly solve problem (1) and provide a concentration estimate. Dual methods [1] provide a covariance matrix by solving the limited problem, minimizing the U-Rp-p-log det (S + U) \u2212 subject to the fact that the primary and dual variables are related to each other = (S + U) \u2212 1. Both the primary and dual problems can be solved with block methods (also referred to as\" sequential problems, \"in both cases)."}, {"heading": "2.1 Dual Methods", "text": "Banerjee et al. [1] consider a block coordinate descendant algorithm to solve the block dual problem, which reduces each optimization step to solving a box-confined quadratic program. Each of these quadratic programs corresponds to performing a \"lasso\" (\"1-regulated\") regression. Friedman et al. [11] solve the lasso regression iteratively as described in [1], but do so using a coordinate-based descent. Their widely used solver, known as a graphic lasso (\"Glasso\"), is implemented on CRAN. Global convergence rates of these block coordinate methods are unknown. D'Aspremont et al. [9] use Nesterov's soft approximation scheme, which produces an optimal solution in O (1 / \u03b5) iterations. A variant of Nesterov's smooth method has been shown to have an O (1 / \u043c) -Iteration."}, {"heading": "2.2 Primal Methods", "text": "Interest in primary methods for solving problem (1) has increased for many reasons, one important reason being the fact that convergence within a certain tolerance for the dual problem does not necessarily imply convergence within the same tolerance for the primary problem. Yuan and Lin [36] use internal point methods based on the maximum end problem investigated in [32]. Yuan [37] use a method with a changing direction, whereas Scheinberg et al. [30] suggests a similar method and has a sublinear convergence rate. Mazumder and Hastie [23] consider block coordinate descend approaches for the primary problem similar to the dual approach in [11]. Mazumder and Agarwal [22] also solve the primary problem with block coordinate descend, but each iteration results in a partial rather than a complete block optimization, which leads to a decreased computational complexity per iteration. [16] Conximal rates of these two primary methods have not been taken into account in the IC and one of the other literature is not guaranteed."}, {"heading": "3 Methodology", "text": "This section presents the graphical iterative shrinkage threshold algorithm (G-ISTA) for solving the primordial problem (1).There is a wealth of mathematical and numerical work on general iterative shrinkage thresholds and related methods; see in particular [3, 8, 24, 25, 26, 31]."}, {"heading": "3.1 General Iterative Shrinkage Thresholding (ISTA)", "text": "It is only a matter of time before a solution is found. (...) The function f is also often assumed to be the Lipschitz constant f, that is, there is a constant L > 0, that there is a constant L > 0, which is a constant L > 0. (...) The function f is also often assumed to be the Lipschitz constant f, that is, there is a constant L > 0, which is a constant L > 0. (...) For a given lower convex function g, the proximity operator of g, which is proxg. (...)"}, {"heading": "3.2.1 Choice of initial step size, \u03b60", "text": "Each iteration of algorithm 1 requires an initial step quantity, i.e. 0. The results of section 4 guarantee that in the next iteration any change of 0 \u2264 \u03bbmin (\u0441t) 2 will be accepted by the line search criteria of step 1. In practice, however, this choice of step is overcautious; often a much larger step can be taken. Our implementation of algorithm 1 chooses the Barzilai-Borwein step [2]. This step is also used in the SpaRSA algorithm [35] and approaches Hessian by T + 1. If a certain number of maximum traces does not lead to an accepted step, G-ISTA takes the safe step, namely min (\u0442t) 2. Such a safe step can be achieved from T + 1, whereby a maximum number of traces cannot be achieved in an approximate revolution."}, {"heading": "4 Convergence Analysis", "text": "This section discusses the linear convergence of algorithm 1."}, {"heading": "5 Numerical Results", "text": "In this section, we provide numerical results for the G-ISTA algorithm. In section 5.2, the theoretical results of section 4. section 5.3 compares the runtimes of the G-ISTA, Glasso [11], and QUIC [16] algorithms. All algorithms are implemented in C + + and run on an Intel i7 \u2212 2600k 3.40GHz \u00d7 8 core with 16GB of RAM."}, {"heading": "5.1 Synthetic Datasets", "text": "Synthetic data for this section were generated using the method used by [21, 22]: for a fixed p, a p-dimensional inverse covariance matrix with off-diagonally drawn entries was generated, i.e. from a uniform (\u2212 1, 1) distribution, which was set to zero with a certain fixed probability (in this case, either 0.97 or 0.85, to simulate a very sparse and somewhat sparse model), and finally, a multiple of identity was added to the resulting matrix, so that the smallest eigenvalue was equal to 1, ensuring that \"n\" was sparse, positively defined and well conditioned. Data sets of n samples were then generated by taking samples from an Np (0-1) distribution, i.e., n = 1.2p and n = 0.2p were tested for each value of p and the sparsity level to represent both n < p and n-p cases."}, {"heading": "5.2 Demonstration of Convergence Rates", "text": "The linear convergence rate derived for G-ISTA in Section 4 turned out to be highly dependent on the conditioning of the final estimator. To demonstrate these results, G-ISTA was executed on a synthetic dataset, as described in Section 5.1, with p = 500 and n = 300. Regularization parameters of \u03c1 = 0.75, 0.1, 0.125, 0.15, and 0.175 were used. Note: The increases of \u03c1 are generally better conditioned. For each value of \u03c1, the numerical optimum was calculated using G-ISTA to a duality gap of 10 \u2212 10. These values of \u03c1 resulted in savings levels of 81.80%, 89.67%, 94.97%, 97.82%, and 99.11%, respectively. G-ISTA was then reexecuted, and the Frobenius standard argument errors were stored for each iteration. These errors were recorded on a protocol scale for each number of the convergence to demonstrate the number 1, respectively."}, {"heading": "5.3 Timing Comparisons", "text": "The G-ISTA, Glasso, and QUIC algorithms were executed on synthetic datasets (real datasets are presented in the supplementary section) with different p, n, and different degree of regulation, \u03c1. All algorithms were executed to ensure a specified duality gap, estimated here at 10 \u2212 5. This comparison used efficient C + + implementations of all three algorithms studied. The implementation of G-ISTA was adapted from the publicly available C + + implementation by QUIC Hsieh et al. [16]. Runtime was recorded and will be presented in Table 1. Further comparisons are presented in the supplementary paragraph.Note 2. The three algorithms variable ability to use multiple processors is an important detail. The times shown in Table 1 are wall times, not CPU times. The comparisons were performed on a multicore processor, and it is important to note that both the Uolky and QIC inversions of QIC are required, as well as the Userts of QUICs."}, {"heading": "6 Conclusion", "text": "In this paper, a proximal gradient method was applied to the sparse inverse covariance problem. Linear convergence was discussed, with a fixed closed-form rate. Numerical values were also presented, comparing G-ISTA with the widely used Glasso algorithm and the newer but very fast QUIC algorithm. These results indicate that G-ISTA is competitive, especially in values that provide sparse, well-conditioned estimators. G-ISTA algorithm was very fast in the synthetic examples of Section 5.3, which were generated from well-conditioned models. QUIC is very competitive in poorly-conditioned models. The Supplemental section has two real data sets supporting this. However, for many practical applications it is important to obtain an estimator that is well-conditioned ([29, 34]). To conclude that the program is well-classified for the second-ARF."}, {"heading": "A Supplementary material", "text": "Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}], "references": [{"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivarate gaussian or binary data", "author": ["O. Banerjee", "L. El Ghaoui"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Two-Point Step Size Gradient Methods", "author": ["Jonathan Barzilai", "Jonathan M. Borwein"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Templates for convex cone problems with applications to sparse signal recovery", "author": ["S. Becker", "E.J. Candes", "M. Grant"], "venue": "Mathematical Programming Computation, 3:165\u2013218,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Uncertainty estimates in regional and global observed temperature changes: A new data set from 1850", "author": ["P. Brohan", "J.J. Kennedy", "I. Harris", "S.F.B. Tett", "P.D. Jones"], "venue": "Journal of Geophysical Research, 111,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Convergence rates in forward-backward splitting", "author": ["George H.G. Chen", "R.T. Rockafellar"], "venue": "Siam Journal on Optimization,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Signal recovery by proximal forwardbackward splitting", "author": ["Patrick L. Combettes", "Val\u00e9rie R. Wajs"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "First-order methods for sparse covariance selection", "author": ["Alexandre D\u2019Aspremont", "Onureena Banerjee", "Laurent El Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Hyper-markov laws in the statistical analysis of decomposable graphical models", "author": ["A.P. Dawid", "S.L. Lauritzen"], "venue": "Annals of Statistics, 21:1272\u20131317,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Biostatistics, 9:432\u2013441,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 1.21", "author": ["M. Grant", "S. Boyd"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Large-scale correlation screening", "author": ["A. Hero", "B. Rajaratnam"], "venue": "Journal of the American Statistical Association, 106(496):1540\u20131552,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Hub discovery in partial correlation graphs", "author": ["A. Hero", "B. Rajaratnam"], "venue": "IEEE Transactions on Information Theory, 58(9):6064\u20136078,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Matrix Analysis", "author": ["Roger A. Horn", "Charles R. Johnson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Sparse inverse covariance matrix estimation using quadratic approximation", "author": ["Cho-Jui Hsieh", "Matyas A. Sustik", "Inderjit S. Dhillon", "Pradeep K. Ravikumar"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Wishart distributions for decomposable covariance graph models", "author": ["K. Khare", "B. Rajaratnam"], "venue": "Annals of Statistics, 39(1):514\u2013555,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical models", "author": ["S.L. Lauritzen"], "venue": "Oxford Science Publications. Clarendon Press,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Wishart distributions for decomposable graphs", "author": ["G. Letac", "H. Massam"], "venue": "Annals of Statistics, 35(3):1278\u20131323,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Zhaosong Lu"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Adaptive first-order methods for general sparse inverse covariance selection", "author": ["Zhaosong Lu"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "A flexible, scalable and efficient algorithmic framework for the Primal graphical lasso", "author": ["Rahul Mazumder", "Deepak K. Agarwal"], "venue": "Pre-print,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The graphical lasso: New insights and alternatives", "author": ["Rahul Mazumder", "Trevor Hastie"], "venue": "Pre-print,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Yurii Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1983}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": "CORE discussion papers, Universite\u0301 catholique de Louvain, Center for Operations Research and Econometrics (CORE),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Integrated modeling of clinical and gene expression information for personalized prediction of disease outcomes", "author": ["Jennifer Pittman", "Erich Huang", "Holly Dressman", "Cheng-Fang F. Horng", "Skye H. Cheng", "Mei-Hua H. Tsou", "Chii-Ming M. Chen", "Andrea Bild", "Edwin S. Iversen", "Andrew T. Huang", "Joseph R. Nevins", "Mike West"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Flexible covariance estimation in graphical models", "author": ["B. Rajaratnam", "H. Massam", "C. Carvalho"], "venue": "Annals of Statistics, 36:2818\u20132849,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "A note on the lack of symmetry in the graphical lasso", "author": ["Benjamin T. Rolfs", "Bala Rajaratnam"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Sparse inverse covariance selection via alternating linearization methods", "author": ["Katya Scheinberg", "Shiqian Ma", "Donald Goldfarb"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["Paul Tseng"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Determinant maximization with linear matrix inequality constraints", "author": ["Lieven Vandenberghe", "Stephen Boyd", "Shao-Po Wu"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Graphical Models in Applied Multivariate Statistics", "author": ["J. Whittaker"], "venue": "Wiley,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1990}, {"title": "Condition number regularized covariance estimation", "author": ["J. Won", "J. Lim", "S. Kim", "B. Rajaratnam"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse reconstruction by separable approximation", "author": ["Stephen J. Wright", "Robert D. Nowak", "M\u00e1rio A.T. Figueiredo"], "venue": "IEE Transactions on Signal Processing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Model selection and estimation in the gaussian graphical model", "author": ["Ming Yuan", "Yi Lin"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Alternating direction method of multipliers for covariance selection models", "author": ["X.M. Yuan"], "venue": "Journal of Scientific Computing, pages 1\u201313,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 32, "context": "A related problem is the inference of a Gaussian graphical model ([33, 18]), that is, a sparsity pattern in the inverse covariance matrix, \u03a9.", "startOffset": 66, "endOffset": 74}, {"referenceID": 17, "context": "A related problem is the inference of a Gaussian graphical model ([33, 18]), that is, a sparsity pattern in the inverse covariance matrix, \u03a9.", "startOffset": 66, "endOffset": 74}, {"referenceID": 32, "context": "Specifically, if X = (Xi) p i=1 \u2208 R is distributed as X \u223c Np(0,\u03a3), then (\u03a3)ij = \u03a9ij = 0 \u21d0\u21d2 Xi \u22a5\u22a5 Xj|{Xk}k 6=i,j, where the notation A \u22a5\u22a5 B|C denotes the conditional independence of A and B given the set of variables C (see [33, 18]).", "startOffset": 223, "endOffset": 231}, {"referenceID": 17, "context": "Specifically, if X = (Xi) p i=1 \u2208 R is distributed as X \u223c Np(0,\u03a3), then (\u03a3)ij = \u03a9ij = 0 \u21d0\u21d2 Xi \u22a5\u22a5 Xj|{Xk}k 6=i,j, where the notation A \u22a5\u22a5 B|C denotes the conditional independence of A and B given the set of variables C (see [33, 18]).", "startOffset": 223, "endOffset": 231}, {"referenceID": 0, "context": "[1] proposed performing such sparse inverse covariance estimation by solving the `1-penalized maximum likelihood estimation problem,", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Moreover, the `1 penalty induces sparsity in \u0398\u03c1, as it is the closest convex relaxation of the 0 \u2212 1 penalty, \u2016\u0398\u20160 = \u2211 i,j I(\u0398ij 6= 0), where I(\u00b7) is the indicator function [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 9, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 12, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 13, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 16, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 18, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 27, "context": "The reader is referred to the theoretical work of [10, 13, 14, 17, 19, 28], among others, for greater detail.", "startOffset": 50, "endOffset": 74}, {"referenceID": 11, "context": "While several excellent general convex solvers exist (for example, [12] and [4]), these are not always adept at handling high dimensional problems (i.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "While several excellent general convex solvers exist (for example, [12] and [4]), these are not always adept at handling high dimensional problems (i.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Dual methods [1] yield a covariance matrix by solving the constrained problem,", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "[1] consider a block coordinate descent algorithm to solve the block dual problem, which reduces each optimization step to solving a box-constrained quadratic program.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] iteratively solve the lasso regression as described in [1], but do so using coordinate-wise descent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[11] iteratively solve the lasso regression as described in [1], but do so using coordinate-wise descent.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "[9] use Nesterov\u2019s smooth approximation scheme, which produces an \u03b5-optimal solution in O(1/\u03b5) iterations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "A variant of Nesterov\u2019s smooth method is shown to have a O(1/ \u221a \u03b5) iteration complexity in [20, 21].", "startOffset": 91, "endOffset": 99}, {"referenceID": 20, "context": "A variant of Nesterov\u2019s smooth method is shown to have a O(1/ \u221a \u03b5) iteration complexity in [20, 21].", "startOffset": 91, "endOffset": 99}, {"referenceID": 35, "context": "Yuan and Lin [36] use interior point methods based on the max-det problem studied in [32].", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "Yuan and Lin [36] use interior point methods based on the max-det problem studied in [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 36, "context": "Yuan [37] use an alternating-direction method, while Scheinberg et al.", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "[30] proposes a similar method and show a sublinear convergence rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Mazumder and Hastie [23] consider blockcoordinate descent approaches for the primal problem, similar to the dual approach taken in [11].", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "Mazumder and Hastie [23] consider blockcoordinate descent approaches for the primal problem, similar to the dual approach taken in [11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "Mazumder and Agarwal [22] also solve the primal problem with block-coordinate descent, but at each iteration perform a partial as opposed to complete block optimization, resulting in a decreased computational complexity per iteration.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "[16] propose a second-order proximal point algorithm, called QUIC, which converges superlinearly locally around the optimum.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 23, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 24, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 25, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 30, "context": "exists for general iterative shrinkage thresholding and related methods; see, in particular, [3, 8, 24, 25, 26, 31].", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "It is well known (for example, [8]) that x\u2217 \u2208 X is an optimal solution of problem (4) if and only if", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "See [3] for more details.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "Lemma 1 ([1, 20]).", "startOffset": 9, "endOffset": 16}, {"referenceID": 19, "context": "Lemma 1 ([1, 20]).", "startOffset": 9, "endOffset": 16}, {"referenceID": 7, "context": "where for some x \u2208 R, (x)+ := max(x, 0) (see [8]).", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "As in [3], the algorithm uses a backtracking line search for the choice of step size.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "Our implementation of Algorithm 1 chooses the Barzilai-Borwein step [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 34, "context": "is also used in the SpaRSA algorithm [35], and approximates the Hessian around \u0398t+1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "In particular, the useful, recent QUIC method [16] warrants a discussion.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "3 compares running times of the G-ISTA, glasso [11], and QUIC [16] algorithms.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "3 compares running times of the G-ISTA, glasso [11], and QUIC [16] algorithms.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Synthetic data for this section was generated following the method used by [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 21, "context": "Synthetic data for this section was generated following the method used by [21, 22].", "startOffset": 75, "endOffset": 83}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For many practical applications however, obtaining an estimator that is well-conditioned is important ([29, 34]).", "startOffset": 103, "endOffset": 111}, {"referenceID": 33, "context": "For many practical applications however, obtaining an estimator that is well-conditioned is important ([29, 34]).", "startOffset": 103, "endOffset": 111}], "year": 2012, "abstractText": "The `1-regularized maximum likelihood estimation problem has recently become a topic of great interest within the machine learning, statistics, and optimization communities as a method for producing sparse inverse covariance estimators. In this paper, a proximal gradient method (G-ISTA) for performing `1-regularized covariance matrix estimation is presented. Although numerous algorithms have been proposed for solving this problem, this simple proximal gradient method is found to have attractive theoretical and numerical properties. G-ISTA has a linear rate of convergence, resulting in an O(log \u03b5) iteration complexity to reach a tolerance of \u03b5. This paper gives eigenvalue bounds for the G-ISTA iterates, providing a closed-form linear convergence rate. The rate is shown to be closely related to the condition number of the optimal point. Numerical convergence results and timing comparisons for the proposed method are presented. G-ISTA is shown to perform very well, especially when the optimal point is well-conditioned.", "creator": "LaTeX with hyperref package"}}}