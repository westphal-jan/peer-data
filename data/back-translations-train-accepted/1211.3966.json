{"id": "1211.3966", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2012", "title": "Lasso Screening Rules via Dual Polytope Projection", "abstract": "Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose a fast and efficient screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there are currently no \"exact\" screening rules for group Lasso. We have evaluated our screening rule using both synthetic and real data sets. Results show that our rule is more effective to identify inactive predictors than existing state-of-the-art screening rules.", "histories": [["v1", "Fri, 16 Nov 2012 17:48:42 GMT  (479kb,D)", "https://arxiv.org/abs/1211.3966v1", null], ["v2", "Sat, 1 Feb 2014 00:12:10 GMT  (911kb,D)", "http://arxiv.org/abs/1211.3966v2", null], ["v3", "Wed, 15 Oct 2014 20:18:33 GMT  (445kb,D)", "http://arxiv.org/abs/1211.3966v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jie wang", "jiayu zhou", "peter wonka", "jieping ye"], "accepted": true, "id": "1211.3966"}, "pdf": {"name": "1211.3966.pdf", "metadata": {"source": "CRF", "title": "Lasso Screening Rules via Dual Polytope Projection", "authors": ["Jie Wang", "Peter Wonka", "Jieping Ye"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu"}, {"heading": "2 Screening Rules for Lasso via Dual Polytope Projections", "text": "In this section, we will present the details of the proposed DPP and EDPP screening rules for the lasso problem. Based on the geometric properties discussed in Section 2.1, we will then develop the basic DPP screening rule in Section 2.2. As a simple extension to deal with the model selection problems, we will also develop the sequential version of the DPP rules. In Section 2.3, we will improve the DPP rules by developing the so-called extended DPP rules. EDPP screening rules significantly exceed DPP rules when identifying the inactive features of the lasso problem."}, {"heading": "2.1 Basics", "text": "Unlike [36, 35] we do not assume that y and all xi have a unit length. The double problem (1) takes the form (to make the paper self-contained), we provide the detailed derivation of the dual form in the appendix. < < p}, (2), where it is the dual variable. In fact, we leave the optimal solution of the problem (2), so the optimal solution of the problem (2), (2), the optimal solution of the problem (2), so the optimal solution of the problem (1), (2), the optimal solution of the problem (1), the optimal solution of the problem (3), so the optimal solution of the problem (4), so on."}, {"heading": "2.2 Fundamental Screening Rules via Dual Polytope Projections (DPP)", "text": "In this section, we propose the so-called DPP screening rules to discard the inactive features for Lasso. As the name implies, the idea of the DPP depends heavily on the properties of the projection operators, e.g. non-expansivity [5]. We will follow the three steps indicated in Section 2.1 to develop the DPP screening rules. First, we need to find a region that contains the double optimal solution. Indeed, the result in (9) provides us with an important clue, which means that we may be able to estimate a possible region for the screening."}, {"heading": "2.3 Enhanced DPP Rules for Lasso", "text": "In this section, we improve the DPP rules set out in Section 2.2 by more careful analysis of the projection operators. In fact, from the three steps by which we develop the DPP rules, we can see that the first step is a key. In other words, estimating the optimal dual solution serves as a fundamentally important role in the development of the DPP rules. Furthermore, (R1 ') implies that the more accurate the estimate, the more effective the resulting screening rule is in discarding the inactive features. In fact, estimating the optimal dual solution in the DPP rules is a direct consequence of the non-expansiveness of the projection operators. Therefore, in order to improve the performance of the DPP rules in discarding the inactive features, we propose two different approaches to find more accurate estimates of the optimal dual solution. These two approaches are set out in detail in Section 2.3.1 and 2.3.2 respectively. By combining the ideas of these two approaches, we can further improve the estimate of the optimal dual solution."}, {"heading": "2.3.1 Improving the DPP rules via Projections of Rays", "text": "In the DPP screening rules, the double optimal solution for both cases is estimated to be within the ball (both). In this section, we show that the solution for both cases is centered in a ball that is centered in a smaller radius. In fact, it is known that the projection of any point on a non-empty, closed convexcursion C in a Hilbert room H always exists and is unique. However, the inversion is not true, i.e. it can exist w1, w2, and PC (w1) in such a way that w1, w2, and PC (w2) = PC (w2). Indeed, it is known that the following result applies: Lemma 6. [3] Let C have a non-empty closed convexcursion of a Hilbert room H. For a point w = H, let w = PC (w) + t (w), the following result applies: Lemma 6."}, {"heading": "2.3.2 Improving the DPP rules via Firmly Nonexpansiveness", "text": "In Section 2.3.1, we improve the estimation of the double optimal solution in DPP by using the projections of correctly selected beams. (R1 \") implies that the resulting screening rule in Improvement 1 is more effective if we discard the inactive properties as DPP. (R1\") In this section, we present another approach to improve the estimation of the double optimal solution in DPP by using the so-called absolute non-expansion of the projections on non-empty closed convex subset of a Hilbert room H. Then the projection operator defined in Eq is continuous and firmly non-expansive. (5) In other words, for each w1, w2 \"H, we have PC (w1) -PC (w2) -22 +..."}, {"heading": "2.3.3 The Proposed Enhanced DPP Rules", "text": "In sections 2.3.1 and 2.3.2 we present two different approaches to improve the assessment of the double optimal solution in DPP. In this section we give a more accurate estimate of the double optimal solution (R1) than those in theorems 7 and 13 by combining the above two approaches. The resulting screening rule for Lasso is the so-called extended DPP rule (EDPP). Again (R1) implies that EDPP is more effective in discarding the inactive properties than the screening rules in Improvements 1 and 2. We present several experiments to show that EDPP is able to identify more inactive properties than the screening rules in Improvements 1. and 2. Therefore, in the following sections we will focus on the generalizations and evaluations of EDPPP. In order to develop the EDPP rules, we follow three more steps in the proposed solution."}, {"heading": "3 Extensions to Group Lasso", "text": "In order to demonstrate the flexibility of the family of DPP rules, we extend the idea of EDPP in this section to the group lasso problem [37]. Although the lasso and group lasso problems differ greatly from each other, we will find that their dual problems have many similarities. For example, both dual problems can be formulated in such a way that they look for projections onto non-empty closed convex subsets of a Hilbert room. Let us remember that the EDPP rule for the lasso problem is based entirely on the properties of the projection operators. Therefore, the framework of the EDPP screening rule that we have developed for the group lasso problem is also applicable to the group lasso problem. In Section 3.1, we will briefly discuss some of the basics of the group lasso problem and examine the geometric properties of its dual problem. In Section 3.2, we will develop the EDPP rule for the group lasso problem."}, {"heading": "3.1 Basics", "text": "I'm not going to comment on this because I don't want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it, I want to talk about it."}, {"heading": "3.2 Enhanced DPP rule for Group Lasso", "text": "In view of (R2) we can see that the estimation of the double optimal solution is the key step to develop a screening rule for the group Lasso-Problem (max.). Since we are the projection rules (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) max. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) x. (max.) max. (max.) x. (max. (max.) x. (max. (max.) x. (max. (max.) x. (max. (max.) x. (max. (max.) x. (max.) x. (max. (max.) x. (max.) max. (max.) x. (max. (max.) x. (max.) max. (max.) x. (max. (max.) x. (max. (max.) x. (max.) max. (max.) x. (max. (max.) x. (max. (max.) x. (max) x. (max. (max) x. (max) max. (max. (max.) x."}, {"heading": "It is easy to see that \u2225\u2225XTg \u03b8\u2217(\u03bb)\u2225\u22252 \u2264 \u2016XTg (\u03b8\u2217(\u03bb)\u2212 o)\u20162 + \u2016XTg o\u20162 (73)", "text": "The second and third inequalities in (73) are due to (72) and theorem 19. Considering Equation (52) and theorem 20, we can derive the EDPP rule to discard the inactive groups for the group lasso problem as follows. Consequence 21. EDPP: For the group lasso problem (50), we can assume that we are given a sequence of parameter values: max = 5 > 1 >... > \u03bbK. For each integer 0 \u2264 k < K, we have \u03b2-g (5 + 1) = 0, if \u03b2-problem (50) is known and the following values apply:"}, {"heading": "4 Experiments", "text": "In this section, we evaluate the proposed EDPP rules for lasso and group lasso on both synthetic and real data sets. To measure the performance of our screening rules, we calculate the repulsion ratio and acceleration (details of which can be found in Section 2.3.3). Since the EDPP rule is safe, i.e. no active features / groups are mistakenly discarded, the repulsion ratio will be smaller than one.In Section 4.1, we conduct two series of tests to compare the performance of EDPP with several state-of-the-art screening methods. First, we compare the performance of the basic versions of EDPP, DOME, SAFE and Strong Rules. Then, we focus on the sequential versions of EDPP, SAFE and Strong Rules. Note that SAFE and EDPP are safe. However, strong rules can erroneously discard properties with non-zero coefficients in the solution. Although DOME is also safe for the lasso problem, it is unclear if there is a DOME sequential version of the problem."}, {"heading": "4.1 EDPP for the Lasso Problem", "text": "For the lasso problem, we first compare the performance of the base versions of EDPP, DOME, SAFE, and Strong Rule in Section 4.1.1. Then, we compare the performance of the sequential versions of EDPP, SAFE, and Strong Rule in Section 4.1.2."}, {"heading": "4.1.1 Evaluation of the Basic EDPP Rule", "text": "In this section, we perform experiments on six real records to compare the performance of the basic versions of SAFE, DOME, SDK, and SDK. Let's assume that we have a parameter-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value-value"}, {"heading": "4.1.2 Evaluation of the Sequential EDPP Rule", "text": "In this section, we compare the performance of the sequential versions of SAFE, Strong Rule and EDPP by the reject ratio and acceleration. We first conduct experiments on two synthetic data sets. We then apply the three screening rules to six real data sets. Synthetic Data SetsFirst, we conduct experiments on several synthetic problems that are commonly used in the sparse learning literature. [7, 39, 31] We simulate the data from the real data matrix X are basically standard Gaussian with paired correlation zero, i.e., we create two data sets of 250 \u00d7 10000 entries: Synthetic 1 and Synthetic 2. For synthetic 1, the entries of the data matrix X are i.e. We draw the results of the data matrix X with paired correlation zero, i.e."}, {"heading": "4.2 EDPP for the Group Lasso Problem", "text": "In this experiment, we evaluate the performance of EDPP and the strong rule with different group numbers. Data matrix X is fixed at 250 x 200000. Inputs of the response vector y and data matrix X are i.i.d. generated from a standard Gaussian distribution. For each experiment, we repeat the calculation 20 times and report the average results. Furthermore, we let ng indicate the number of groups and sg the average group size. If ng is, for example, 10000, then sg = p / ng = 20.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.7 0.7 0.7 0.9 1 00.20.40.61R ejec tion Rat ioStrong Rule EDPPPPPP0.1 0.4 0.5 0.6 0.7 0.9 0.9 0.60.81\u043c ejec i.40.81\u043c ejec i.6 Council ioStrong Rule EDPPPPPPPPP0.1 0.2 0.6 0.000 0.000 0000 group 80.000 00.7 0.0000 00,000,0000"}, {"heading": "5 Conclusion", "text": "In this paper, we develop new screening rules for the lasso problem by using the properties of the projection operators in relation to a closed convex set. In addition, our proposed methods, i.e. DPP screening rules, are able to effectively identify inactive predictors of the lasso problem, which significantly reduces the size of the optimization problem. In addition, we continue to improve the DPP rule and propose the extended DPP rule, which is more effective at discarding inactive characteristics than the DPP rule. It is worth mentioning that the idea of the family of DPP rules can be easily generalized to identify the inactive groups of the group lasso problem. In the future, we plan to conduct extensive numerical experiments on both synthetic and real data showing the effectiveness of the proposed rules. It is worth mentioning that the family of DPP rules can be combined with any lasso solver as a speedup tool. In the future, we plan to base our ideas on other sparse formulations of more generalized penalties, e.g., the tree structure."}, {"heading": "Appendix A.", "text": "In this appendix we give the detailed derivation of the double problem of Lasso."}, {"heading": "A1. Dual Formulation", "text": "Assuming that the data matrix is X-RN-p, the standard lasso problem (\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2-\u03b2 (\u03b2-\u03b2-\u03b2-\u03b2-\u03b2) results in the following: inf-\u03b2-\u03b2-\u03b2-\u03b2 (\u03b2-\u03b2-22 + \u03bb-\u03b2-1. (75) The double problem is therefore trivial and useless. (75) A common trick [8] is to introduce a new set of variables z = y-X\u03b2 of this problem (75): inf-\u03b21 2 x-z-z-z-\u03b2-\u03b2-1, (76) subject to z-y-\u03b2. By introducing the dual variables RN, we obtain the Lagrangian problem (76): L (\u03b2, z, \u03b7) = 12 x-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z-z."}, {"heading": "A2. The KKT Conditions", "text": "Problem (76) is clearly convex and its constraints are all affin. According to Slater's condition, as long as problem (76) is feasible, we have strong duality. Let us interpret \u03b2 *, z * and \u03b8 * as optimal primary and dual variables. Lagrange Island L (\u03b2, z, \u03b8) = 12 x z * 22 + 2 x 1 + 2 x T \u00b7 (y \u2212 X\u03b2 \u2212 z). (88) From the KKT condition, we have 0 x x \u03b2L (\u03b2, z, \u0456) = \u2212 XT + 1, where we v = 1 and vT\u03b2 = 1, (89) x zL = 1, (\u03b2, z \u00b2, \u03b8) = 0, (90) x-L = 0, (\u03b2, z \u00b2) = 89 x-V (E) = 1 x-V (T) = 0."}, {"heading": "XT \u03b8\u2217 = v\u2217, \u2016v\u2217\u2016\u221e \u2264 1 and (v\u2217)T\u03b2\u2217 = \u2016\u03b2\u2217\u20161,", "text": "(93) It is easy to deduce from Equation (93): (\u03b8) Txi sign (\u03b2-i) if \u03b2-i 6 = 0, [\u2212 1, 1] if \u03b2-i = 0. (94)"}, {"heading": "Appendix B.", "text": "In this appendix we present the detailed derivation of the dual problem of the Lasso group."}, {"heading": "B1. Dual Formulation", "text": "Assuming that the data matrix is Xg = RN \u00b7 ng and p = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG = RG RG = RG = RG RG = RG = RG = RG RG RG = RG RG RG = RG RG = RG RG RG RG = RG RG RG = RG RG RG RG RG = RG RG RG RG RG = RG RG RG RG RG RG RG RG = RG RG RG RG RG RG RG: RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG RG R"}, {"heading": "B2. The KKT Conditions", "text": "The problem (96) is convex and its constraints are all affinity. According to Slater's condition, as long as the problem (96) is feasible, we have strong duality. Denotes \u03b2, z and \u03b8 as optimal primary and dual variables. (106) The CCT condition implies that we have 0% \u03b2gL (\u03b2, z \u00b2) = 1% \u03b2g \u00b2 2 + 1% T \u00b7 (y \u2212 G \u00b2 g = 1 Xg\u03b2g \u2212 z). (106) From the CCT condition, we have 0% \u03b2gL (\u03b2 \u00b2, z \u00b2) = 1% \u03b2g \u00b2 2 + 1% T \u00b7 (y \u2212 G \u00b2 g \u00b2 g = 1 Xg\u03b2g \u2212 z). (106) The CCT condition implies that we have 0% \u03b2gL (\u03b2 \u00b2, z \u00b2, conceived) = \u2212 l \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2. (107)"}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "<lb>Lasso is a widely used regression technique to find sparse representations. When the di-<lb>mension of the feature space and the number of samples are extremely large, solving the Lasso<lb>problem remains challenging. To improve the efficiency of solving large-scale Lasso problems,<lb>El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify<lb>the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then,<lb>the inactive predictors or features can be removed from the optimization problem to reduce its<lb>scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive<lb>predictors include the set of inactive constraints on the optimal dual solution. In this paper,<lb>we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which<lb>is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to<lb>the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we<lb>show that our screening rule can be extended to identify inactive groups in group Lasso. To<lb>the best of our knowledge, there is currently no exact screening rule for group Lasso. We have<lb>evaluated our screening rule using synthetic and real data sets. Results show that our rule is<lb>more effective in identifying inactive predictors than existing state-of-the-art screening rules for<lb>Lasso.", "creator": "LaTeX with hyperref package"}}}