{"id": "1211.5063", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2012", "title": "On the difficulty of training recurrent neural networks", "abstract": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective. Our analysis is used to justify the simple yet effective solution of norm clipping the exploded gradient. In the experimental section, the comparison between this heuristic solution and standard SGD provides empirical evidence towards our hypothesis as well as it shows that such a heuristic is required to reach state of the art results on a character prediction task and a polyphonic music prediction one.", "histories": [["v1", "Wed, 21 Nov 2012 15:40:11 GMT  (353kb,D)", "http://arxiv.org/abs/1211.5063v1", null], ["v2", "Sat, 16 Feb 2013 00:35:48 GMT  (447kb,D)", "http://arxiv.org/abs/1211.5063v2", "Improved description of the exploding gradient problem and description and analysis of the vanishing gradient problem"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["razvan pascanu", "tomas mikolov", "yoshua bengio"], "accepted": true, "id": "1211.5063"}, "pdf": {"name": "1211.5063.pdf", "metadata": {"source": "CRF", "title": "Understanding the exploding gradient problem", "authors": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "emails": ["r.pascanu@gmail.com", "imikolov@fit.vutbr.cz", "bengioy@iro.umontreal.ca"], "sections": [{"heading": null, "text": "In this paper, we attempt to understand the fundamental problems underlying the exploding gradient problem by examining it from an analytical, geometric and dynamic system perspective. Our analysis serves to justify the simple but effective solution of the standard clipping of the exploded gradient. In the experimental section, the comparison between this heuristic solution and the standard SGD provides empirical evidence for our hypothesis and shows that such heuristics are necessary to reach the state of the art."}, {"heading": "1 Introduction", "text": "A recessionary network (RNE), as shown in Figure 1, is a neural network model that is able to detect the connections between the hidden units with a time delay. In this case, the model may contain information about past interventions, which makes it possible to detect the temporal relationships between the events (a crucial characteristic for proper learning)."}, {"heading": "1.1 Training recurrent networks", "text": "In the theoretical part of this thesis we will sometimes fall back on the specific parameterization given by Equation (2) to provide more precise conditions and intuitions about the everyday use case. Parameters of the model are given by the recurring weight matrix Wrec, the distortions b and input weight matrix Win, collected in the general case. xt and ut represent the state and input in the time t, in which x0 is provided by the user or set to zero (or is learned), and it is an elementary function (usually1) corresponding to the commonly known equation xt = 1 + Winut b."}, {"heading": "2 Exploding Gradients", "text": "As described in Bengio et al. (1994), the exploding problems with gradient relate to the sharp increase in the norm of gradient during training. Such events are caused by the explosion of long-term components, which can grow exponentially more strongly than short-term ones."}, {"heading": "2.1 The mechanics", "text": "To understand this phenomenon, we need to consider the shape of each temporal component, in particular the factors \u2202 xt \u2202 xk (see Equation (5)), which is the shape of a product of l-Jacobin matrices with l = t \u2212 k. Intuitively, these products with l can grow exponentially fast (in any direction v), resulting in the explosion of long-term components when l is large. Since the gradient is only a sum of these components, it follows that it should also grow exponentially fast after the long-term component with k = 0 (for l = t) is present. Below, we will try to formalize these intuitions (a similar derivative from Bengio et al. (1994), in which only a single hidden case was taken into account)."}, {"heading": "2.2 Linear model", "text": "Consider the term gTk = digit 6 = 0 and all digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = digits i = (WTrec) l (6) By using the same approach as the potential iteration method, we can show that under certain conditions the digits sequence of the digits q1, q2, qn that form a vector base grows exponentially. let Wrec write the eigenvalues 1,.., digits with digits 1 | > digits 2 | > digits sequence of digits q1, qn, qn that form a vector base. We can now write the row vector qEt digits digits sequence of that digit."}, {"heading": "2.3 Nonlinear model", "text": "To generalize this evidence for the nonlinear case, we define the concept of expanding and non-expanding matrices for a particular direction. (We say that the Jacobian matrix for a particular direction is not sufficient to achieve exponential growth, it is sufficient that most of the Jacobians expand so that their product exponentially expands with their number, and also that there is no exponential matrix to stall these exponentially large growth steps. (We formalize this by taking the set P of matrices that do not expand, and considering a lower limit as > 0 on how much these matrices shrink towards v). (See Equation (10)."}, {"heading": "2.4 The geometrical interpretation", "text": "s look at a simple hidden unit model (Equation (14), where we find an initial state x0 and a move to a certain target value after 50 steps."}, {"heading": "2.5 Drawing similarities with Dynamical Systems", "text": "Recurring networks are universal approximators of dynamic systems (see e.g. Siegelmann and Sontag (1995)), and as such it is sometimes useful to analyze them using dynamic system tools that provide better abstractions that we can use for reflecting on the behavior of the model. Dynamic system theory to explain the exploding gradient problem has already been done in Doya (1993); Bengio et al. (1993) In this paper we will expand and improve these previous observations. For each parameter mapping, depending on the initial state x0, which is the xt of an autonomous dynamic system, we will converge, using the map F repeatedly, to one of several possible different states of attraction (e.g. point attractors), describing the asymptotic behavior of the model. State space is divided into basins of attraction, one for each attractor."}, {"heading": "3 Dealing with the exploding", "text": "Slope"}, {"heading": "3.1 Previous solutions", "text": "One approach to avoiding exploding gradients is to use L1 or L2 punishment on the recurring weights. Since the model is initialized with small numbers, the spectral radius of Wrec is probably smaller than 1, which means that the gradient cannot explode (see necessary condition in Section 2.1). The regularization term can ensure that during training the spectral radius never exceeds 1. This approach limits the model to a simple regime (with a single point attractor at the origin) where any information inserted into the model must die out exponentially quickly in time. In such a regime, we cannot form a generator network, nor can we have long-term memory pathways. This suggests that solutions that use changes in the architecture to avoid disappearing gradients, such as LSTMs (Hochreiter and Schmidhuber, 1997) can deal with the exploding gradient model."}, {"heading": "3.2 Scaling down the gradients", "text": "As proposed in section 2.4, a simple mechanism for dealing with a sudden rise in the standard of gradients is to move them when they exceed a threshold (see algorithm 1). Algorithm 1 pseudo-code for cutting off the gradients whenever they explode g. \"---------------------------------------------- This algorithm is very similar to the one proposed by Tomas Mikolov and the only reason why we deviated from his original proposal to create a better theoretical basis (e.g., make sure we always move in a descending direction), although in practice they behave similarly. The proposed discount is easy to implement and mathematically efficient, but it introduces an additional hyperparameter, namely the threshold. A good heuristic method for setting this threshold is to look at statistics on the average standard, which relate to a sufficiently large number of updates."}, {"heading": "4 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Polyphonic music prediction", "text": "The first task we are considering is the polyphonic music prediction using the records Piano-midi.de, Nottingham and MuseData, which are described in Boulanger-Lewandowski et al. (2012). In this case, the vocabulary size is 88 different notes, with the distinction that multiple notes can be played simultaneously. We use a model with 100 Tanh units and distortions to stay close to the original setup (BoulangerLewandowski et al., 2012). Each song is divided into non-overlapping sequences with 100 steps, and the hidden state is transmitted only along the same song (and set to 0 for the first sequence of each song). We use a learning rate of.001 and a threshold of 120. The training and test results specified in Table 1 are average negative logic probability per time step. We use 5 different runs to estimate these values. These results represent an improvement over the state of the art, which was achieved using RNN models 0.0001, 0.00006 0.000.000.000.000.06, 2006 Boulanet Lewandowski 0.000.000.06,.000.000.04, 0.0004 0.000.0004, 0.000.0004, 0.000.0004, 0.000.000.000.000.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.000. These results represent an improvement over the state of the art, which was achieved using RNN models 0.0001 0.0004 0.0001, 0.0004, 0.0004, 0.0004, 0.000.000.000.000.0004, 0.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.04, 0.000.04, 0.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000"}, {"heading": "4.2 Next character prediction", "text": "The second task is the next character prediction on three different datasets: Penn Treebank Corpus, Wikipedia \"text8.\" The same datasets were considered in Mikolov et al. (2012) The model used is a 500 sigmoid unit RNN without bias to have a structure similar to that in Mikolov et al. (2012) Each gradient is calculated using non-overlapping sequences of 180 characters, with the hidden state being transferred from one sequence to the next. Table 2 returns the training and test error for the best validation value. These values are calculated using 5 different random initializations and we report entropy (bits per character) as a measure of error. We used a threshold of 20 for \"text8\" and 45 for Penn Treebank datasets. These results (together with the one on polyphonic music prediction) suggest that truncating the gradient optimizer is a general test that does not solve the results of both the regulator and the results of 2012."}, {"heading": "4.3 Temporal order task", "text": "In our final experiments, we consider the synthetic problem proposed by Hochreiter and Schmidhuber (1997) as problem 6a (see this paper for full details).We use a model with 50 sigmoid units with a learning rate of.001 and limit the number of training steps to 100k. We use a decrease in the gradient in the minibatch with 1000 examples per batch. The problem is considered solved if the model provides the correct answer for 1000 consecutive inputs. Figure 7 shows the success rate of the standard BPTT and the recalcification of the gradient for 50 different runs. Note that for sequences longer than 20, the problem of the disappearing gradient ensures that neither BPTT nor our algorithm can solve the problem. This task provides empirical evidence that the exploding gradient is associated with tasks that require long memory traces. We know that the model works first in the single-attractor regime (i.e. the gradation of the gradient is controlled in the quantity 1)."}, {"heading": "5 Summary and Conclusions", "text": "The problem of the exploding gradient is only one facet of the difficulty of training recurrent networks. We have presented different perspectives through which one can gain more insight into this problem, although these descriptions can be easily useful for understanding the problem of the disappearing gradient. We propose a solution in which the norm of the exploded gradient is cut off when they are too large. The algorithm is based on the assumption that when gradients explode, the curvature also explodes and we are dealing with a certain pattern in the fault surface, namely a valley with a single steep wall. In practice, we show that this approach improves performance in all 6 tasks tested."}], "references": [{"title": "New results on recurrent network training: Unifying the algorithms and accelerating convergence", "author": ["A.F. Atiya", "A.G. Parlos"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Atiya and Parlos,? \\Q2000\\E", "shortCiteRegEx": "Atiya and Parlos", "year": 2000}, {"title": "The problem of learning long-term dependencies in recurrent networks. pages 1183\u20131195, San Francisco", "author": ["Y. Bengio", "P. Frasconi", "P. Simard"], "venue": "IEEE Press. (invited paper)", "citeRegEx": "Bengio et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1993}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Bifurcations of recurrent neural networks in gradient descent learning", "author": ["K. Doya"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Doya,? \\Q1993\\E", "shortCiteRegEx": "Doya", "year": 1993}, {"title": "Adaptive synchronization of neural and physical oscillators", "author": ["K. Doya", "S. Yoshizawa"], "venue": null, "citeRegEx": "Doya and Yoshizawa,? \\Q1991\\E", "shortCiteRegEx": "Doya and Yoshizawa", "year": 1991}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["J. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman,? \\Q1990\\E", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Computer Science Review ,", "citeRegEx": "Luko\u0161evi\u010dius and Jaeger,? \\Q2009\\E", "shortCiteRegEx": "Luko\u0161evi\u010dius and Jaeger", "year": 2009}, {"title": "Statistical Language Models based on", "author": ["T. Mikolov"], "venue": "Neural Networks. Ph.D. thesis, Brno University of Technology", "citeRegEx": "Mikolov,? \\Q2012\\E", "shortCiteRegEx": "Mikolov", "year": 2012}, {"title": "Subword language modeling with neural networks. preprint (http://www.fit.vutbr.cz/ imikolov/rnnlm/char.pdf)", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "Le", "H.-S", "S. Kombrink", "J. Cernocky"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Neural networks with adaptive learning rate and momentum terms. Idiap-RR Idiap-RR-04-1995, IDIAP, Martigny, Switzerland", "author": ["M. Moreira", "E. Fiesler"], "venue": null, "citeRegEx": "Moreira and Fiesler,? \\Q1995\\E", "shortCiteRegEx": "Moreira and Fiesler", "year": 1995}, {"title": "A neurodynamical model for working memory", "author": ["R. Pascanu", "H. Jaeger"], "venue": "Neural Netw.,", "citeRegEx": "Pascanu and Jaeger,? \\Q2011\\E", "shortCiteRegEx": "Pascanu and Jaeger", "year": 2011}, {"title": "Learning representations by backpropagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "On the computational power of neural nets", "author": ["H.T. Siegelmann", "E.D. Sontag"], "venue": "JOURNAL OF COMPUTER AND SYSTEM SCIENCES ,", "citeRegEx": "Siegelmann and Sontag,? \\Q1995\\E", "shortCiteRegEx": "Siegelmann and Sontag", "year": 1995}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks,", "citeRegEx": "Werbos,? \\Q1988\\E", "shortCiteRegEx": "Werbos", "year": 1988}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. Williams", "D. Zipser"], "venue": "Neural Comput.,", "citeRegEx": "Williams and Zipser,? \\Q1989\\E", "shortCiteRegEx": "Williams and Zipser", "year": 1989}], "referenceMentions": [{"referenceID": 1, "context": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective.", "startOffset": 146, "endOffset": 167}, {"referenceID": 14, "context": "A recurrent neural network (RNN), see Figure 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series.", "startOffset": 95, "endOffset": 146}, {"referenceID": 7, "context": "A recurrent neural network (RNN), see Figure 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series.", "startOffset": 95, "endOffset": 146}, {"referenceID": 17, "context": "A recurrent neural network (RNN), see Figure 1, is a neural network model proposed in the 80\u2019s (Rumelhart et al., 1986; Elman, 1990; Werbos, 1988) for modeling time series.", "startOffset": 95, "endOffset": 146}, {"referenceID": 14, "context": "A plethora of training algorithms have been proposed in the literature, like Backpropagation Through Time (BPTT) (Rumelhart et al., 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 113, "endOffset": 151}, {"referenceID": 17, "context": "A plethora of training algorithms have been proposed in the literature, like Backpropagation Through Time (BPTT) (Rumelhart et al., 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 113, "endOffset": 151}, {"referenceID": 18, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 52, "endOffset": 79}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc.", "startOffset": 107, "endOffset": 131}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc. Most of these are gradient based, though alternative approaches are also available (for example see Luko\u0161evi\u010dius and Jaeger (2009)), and as shown in Atiya and Parlos (2000) most gradient-based methods behave qualitatively the same, providing little success in properly addressing complex tasks.", "startOffset": 108, "endOffset": 269}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc. Most of these are gradient based, though alternative approaches are also available (for example see Luko\u0161evi\u010dius and Jaeger (2009)), and as shown in Atiya and Parlos (2000) most gradient-based methods behave qualitatively the same, providing little success in properly addressing complex tasks.", "startOffset": 108, "endOffset": 311}, {"referenceID": 0, "context": ", 1986; Werbos, 1988), Real Time Recurrent Learning (Williams and Zipser, 1989), AtiyaParlos learning rule (Atiya and Parlos, 2000), etc. Most of these are gradient based, though alternative approaches are also available (for example see Luko\u0161evi\u010dius and Jaeger (2009)), and as shown in Atiya and Parlos (2000) most gradient-based methods behave qualitatively the same, providing little success in properly addressing complex tasks. Among the main reasons why this model is so unwieldy are the vanishing gradient and exploding gradient problems described in Bengio et al. (1994).", "startOffset": 108, "endOffset": 579}, {"referenceID": 16, "context": "that there are a few solutions proposed for this problem in the literature, amongst which the HessianFree learning (Sutskever et al., 2011), a second order method that seems promising, though more needs to be done to analyze its success, especially compared to other second order methods that seem to do less well in general.", "startOffset": 115, "endOffset": 139}, {"referenceID": 8, "context": "Long Short Term Memory networks (Hochreiter and Schmidhuber, 1997) is another approach, relying on a change in the structure of the model, and designed to help with the vanishing gradient problem.", "startOffset": 32, "endOffset": 66}, {"referenceID": 1, "context": "As introduced in Bengio et al. (1994), the exploding gradient problems refers to the large increase in the norm of the gradient during training.", "startOffset": 17, "endOffset": 38}, {"referenceID": 1, "context": "In what follows we will try to formalize these intuitions (extending a similar derivation done in Bengio et al. (1994) where only a single hidden unit case was considered).", "startOffset": 98, "endOffset": 119}, {"referenceID": 12, "context": "Siegelmann and Sontag (1995)) and as such it is sometimes useful to analyze them using dynamical systems tools which allow for better abstractions that we can use for reasoning about the behaviour of the model.", "startOffset": 0, "endOffset": 29}, {"referenceID": 2, "context": "Looking at dynamical systems theory for explaining the exploding gradient problem has been done before in Doya (1993); Bengio et al.", "startOffset": 106, "endOffset": 118}, {"referenceID": 1, "context": "Looking at dynamical systems theory for explaining the exploding gradient problem has been done before in Doya (1993); Bengio et al. (1993). In this paper we will extend and improve these previous observations.", "startOffset": 119, "endOffset": 140}, {"referenceID": 4, "context": "In Doya (1993) only crossing bifurcation boundaries are considered ignoring changes in the state or the position of the state relative a borders between basin of attraction.", "startOffset": 3, "endOffset": 15}, {"referenceID": 8, "context": "This suggest that solutions that exploit changes in the architecture to avoid vanishing gradients, such as LSTMs (Hochreiter and Schmidhuber, 1997) can deal with the exploding gradient by operating the recurrent model in a damping regime and relying exclusively on the highly specialized LSTM units to exhibit memory, thus justifying why the exploding gradient does not seem to be an issue in practice.", "startOffset": 113, "endOffset": 147}, {"referenceID": 13, "context": "It has been showed that in practice it can reduce the chance that gradients explode, and even allow training generator models or models that work with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991).", "startOffset": 178, "endOffset": 230}, {"referenceID": 5, "context": "It has been showed that in practice it can reduce the chance that gradients explode, and even allow training generator models or models that work with unbounded amounts of memory(Pascanu and Jaeger, 2011; Doya and Yoshizawa, 1991).", "startOffset": 178, "endOffset": 230}, {"referenceID": 10, "context": "Another approach was proposed by Tomas Mikolov recently described in his PhD thesis (Mikolov, 2012) involves clipping the gradient element-wise if the value exceeds in absolute value a fix threshold.", "startOffset": 84, "endOffset": 99}, {"referenceID": 6, "context": "Compared to other learning rate adaptation strategies, which focus on improving convergence by collecting statistics on the gradient (as for example in Duchi et al. (2011), or Moreira and Fiesler (1995) for an overview), we rely on the instantaneous gradient.", "startOffset": 152, "endOffset": 172}, {"referenceID": 6, "context": "Compared to other learning rate adaptation strategies, which focus on improving convergence by collecting statistics on the gradient (as for example in Duchi et al. (2011), or Moreira and Fiesler (1995) for an overview), we rely on the instantaneous gradient.", "startOffset": 152, "endOffset": 203}, {"referenceID": 3, "context": "de, Nottingham and MuseData described in Boulanger-Lewandowski et al. (2012). In this case the vocabulary size is 88 different notes, with the distinction that multiple notes can be played at the same time.", "startOffset": 41, "endOffset": 77}, {"referenceID": 10, "context": "The same datasets had been considered in Mikolov et al. (2012).", "startOffset": 41, "endOffset": 63}, {"referenceID": 10, "context": "The model used is a 500 sigmoid units RNN with no biases in order to have a similar setup as the one used in Mikolov et al. (2012). Each gradient is computed over non-overlapping sequences of 180 characters, where the hidden state is carried over from one sequence to the next one.", "startOffset": 109, "endOffset": 131}, {"referenceID": 11, "context": "Also results on Penn Treebank reach the state of the art (Mikolov et al., 2012), where those results were obtained using a different clipping algorithm similar to ours providing evidence that both behave similarly.", "startOffset": 57, "endOffset": 79}, {"referenceID": 8, "context": "In our final experiments we consider the synthetic problem proposed as task 6a in Hochreiter and Schmidhuber (1997) (see that paper for full details).", "startOffset": 82, "endOffset": 116}], "year": 2012, "abstractText": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective. Our analysis is used to justify the simple yet effective solution of norm clipping the exploded gradient. In the experimental section, the comparison between this heuristic solution and standard SGD provides empirical evidence towards our hypothesis as well as it shows that such a heuristic is required to reach state of the art results on a character prediction task and a polyphonic music prediction one.", "creator": "LaTeX with hyperref package"}}}