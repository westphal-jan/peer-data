{"id": "1212.1824", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2012", "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes", "abstract": "Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD \\emph{without} such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the \\emph{last} SGD iterate scales as O(\\log(T)/\\sqrt{T}) for non-smooth convex objective functions, and O(\\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in \\citet{RakhShaSri12arxiv} is not as simple to implement). Finally, we provide some experimental illustrations.", "histories": [["v1", "Sat, 8 Dec 2012 18:22:42 GMT  (24kb,D)", "http://arxiv.org/abs/1212.1824v1", "To appear in ICML 2013"], ["v2", "Fri, 28 Dec 2012 10:58:48 GMT  (24kb,D)", "http://arxiv.org/abs/1212.1824v2", "To appear in ICML 2013"]], "COMMENTS": "To appear in ICML 2013", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["ohad shamir", "tong zhang 0001"], "accepted": true, "id": "1212.1824"}, "pdf": {"name": "1212.1824.pdf", "metadata": {"source": "CRF", "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes", "authors": ["Ohad Shamir"], "emails": ["ohadsh@microsoft.com", "tzhang@stat.rutgers.edu"], "sections": [{"heading": null, "text": "\u221a T) for non-smooth convex lens functions and O (log (T) / T) for non-smooth strongly convex functions. To our knowledge, these are the first limits of their kind and are close to the minimax optimum rates that can be achieved by appropriate averaging programs. We also propose a new and simple mean scheme that not only achieves optimal rates, but can also be easily calculated on-the-fly (in contrast, the suffix mean scheme proposed in Rakhlin et al. (2011) is not so easy to implement). Finally, we offer some experimental illustrations."}, {"heading": "1 Introduction", "text": "This paper looks at one of the simplest and most popular optimization algorithms, namely the Stochastic Gradient Descent (SGD) of 2011. (SGD) This feature makes it very useful for learning problems, where our goal is to minimize generalization errors based on only a limited number of educational problems. (SGD) In addition, it is extremely simple and highly scalable, making it particularly suitable for large-scale learning problems. The algorithm itself proceeds in rounds and can be described in just a few lines: We assume that w1 = 0). In round t = 1, 2,.. we get a random estimate of a subgradient suffices gt."}, {"heading": "2 Preliminaries", "text": "We let F denote a convex function over a (closed) convex domain W, which is a subset of a strongly convex space with an induced norm. We assume that F is minimized at some W-W values. Besides general convex F values, we also consider the important subclass of strongly convex functions. Formally, we say that a function F values is strongly convex if for all W values W and all subgradients g from F at w values, it holds thatF (w values) \u2265 F (w values) + < g, w values 2 (w values).0 For a general convex function, the above-mentioned inequality can always be satisfied with \u03bb = 0. As discussed in the introduction, we consider the first-class stochastic optimization setting, where we have no direct access to F values."}, {"heading": "3 Convergence of Individual SGD Iterates", "text": "We begin by considering the case of strongly convex F, and prove the following connection to the expected error of each individual iterate wT = 32k =. In this theorem, as in later cases, we have not tried constants. Theorem 1. Suppose F is more than strongly convex, and that E [2 + log (T))) \u2264 G2 for all. Consider SGD with increments of T \u2212 \u2212 \u2212 \u2212 \u2212 T for each T > 1, it applies that E [F (wT) \u2212 F (w)] \u2264 17G2 (1 + log (T).Proof. The beginning of the proof is standard. Based on the conformity of W we have the following W: E [wt) \u2212 w + 1 \u2212 w [2] = E (wT)."}, {"heading": "4 Averaging Schemes", "text": "The limits shown in the previous section imply that the individual iterates have wT = assumed results (T) / T) of expected errors in the strongly convex case and O (log (T) / \u221a T) of expected errors in the convex case. These limits are close, but not equal to the minimax optimal rates, which are O (1 / T) and O (1 / \u221a T) respectively. In this section we will focus mainly on the convex case, since the simple averaging of all iterates is already known as optimal (up to constants) in the generally convex case. We will first examine the case of alpha suffix averages, defined as the average of the last implicit iterate (where the height of 0, 1), and assume that integers are one."}, {"heading": "5 Experiments", "text": "In this section, we examine the behavior of the polynomial decay averaging scheme on a few strongly convex optimization problems. We chose the same 3 binary classification datasets (ccat, cov1, and astro-ph) and experimental setup as in Rakhlin et al. (2011). For each dataset {xi, yi} mi = 1, we executed SGD on the support vector machine optimization problem, using a single randomly drawn training example (xi, yi) and calculating the gradient relative to this example, i.e. g-yi < xi, w >}, using the DomainW = Rd, where the stochastic gradient was calculated based on the training examples drawn (xi, yi) and the gradient relative to this example, i.e. g-yi < xi, w >} median gradient relative to the flow data \u2264 < < xi;"}, {"heading": "6 Discussion", "text": "In this paper, we examined the convergence behavior of SGD and the averaging programs required to achieve optimum performance. In particular, we looked at averaging the polynomial decay rate, which is as easy to calculate as the standard averaging of all iterates, but in fact achieves better performance both theoretically and in practice. We also expanded the existing analysis of the SGD by setting new limits on the descent of SGD iterates held without any smooth assumptions for both convex and strongly convex problems.Finally, we provided new limits on the averaging of suffix. While focusing on the standard descent of SGD, our techniques can be extended to the more general descent probability of SGD and non-euclidean norms. An important open question is whether the O (T) / T) rate we obtained on the individual Iterate G8 is valid for SGD, because this question is closely related to SGD."}], "references": [{"title": "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization", "author": ["A. Agarwal", "P. Bartlett", "P. Ravikumar", "M. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["F. Bach", "E. Moulines"], "venue": "In NIPS,", "citeRegEx": "Bach and Moulines,? \\Q2011\\E", "shortCiteRegEx": "Bach and Moulines", "year": 2011}, {"title": "Beyond the regret minimization barrier: An optimal algorithm for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "In COLT,", "citeRegEx": "Hazan and Kale,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2011}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["H. Kushner", "G. Yin"], "venue": null, "citeRegEx": "Kushner and Yin,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "Stochastic smoothing for nonsmooth minimizations: Accelerating sgd by exploiting structure", "author": ["H. Ouyang", "A. Gray"], "venue": "In ICML,", "citeRegEx": "Ouyang and Gray,? \\Q2012\\E", "shortCiteRegEx": "Ouyang and Gray", "year": 2012}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "CoRR, abs/1109.5647,", "citeRegEx": "Rakhlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2011}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Pegasos: primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Is averaging needed for strongly convex stochastic gradient descent? Open problem presented at COLT", "author": ["O. Shamir"], "venue": null, "citeRegEx": "Shamir,? \\Q2012\\E", "shortCiteRegEx": "Shamir", "year": 2012}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In ICML,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": "We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement).", "startOffset": 186, "endOffset": 208}, {"referenceID": 4, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al.", "startOffset": 46, "endOffset": 63}, {"referenceID": 2, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al. (2007); Hazan & Kale (2011).", "startOffset": 64, "endOffset": 84}, {"referenceID": 2, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al. (2007); Hazan & Kale (2011). However, Rakhlin et al.", "startOffset": 64, "endOffset": 105}, {"referenceID": 2, "context": "2), and O(1/ \u221a T ) error for general convex F Zinkevich (2003); Hazan et al. (2007); Hazan & Kale (2011). However, Rakhlin et al. (2011) showed that simple averaging is provably suboptimal in a stochastic setting.", "startOffset": 64, "endOffset": 137}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues.", "startOffset": 142, "endOffset": 164}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)).", "startOffset": 142, "endOffset": 431}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case.", "startOffset": 142, "endOffset": 593}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, .", "startOffset": 142, "endOffset": 1091}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F .", "startOffset": 142, "endOffset": 1600}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization.", "startOffset": 142, "endOffset": 2143}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting.", "startOffset": 142, "endOffset": 2453}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting. \u2022 We improve the existing expected error bound on the suffix averaging scheme of Rakhlin et al. (2011), from O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ) to O(log( 1 min{\u03b1,1\u2212\u03b1} )/T ).", "startOffset": 142, "endOffset": 2590}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting. \u2022 We improve the existing expected error bound on the suffix averaging scheme of Rakhlin et al. (2011), from O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ) to O(log( 1 min{\u03b1,1\u2212\u03b1} )/T ). \u2022 We propose a new and very simple running average scheme, called polynomial-decay averaging, and prove that it enjoys optimal rates of convergence. Unlike suffix-averaging, this new running average scheme can be easily computed on-the-fly. \u2022 We provide a simple experimental study of the averaging schemes discussed in the paper. We emphasize that although there exist other algorithms with O(1/T ) convergence rate in the strongly convex case (e.g. Hazan & Kale (2011); Ouyang & Gray (2012)), our focus in this paper is on the simple and widely-used SGD algorithm.", "startOffset": 142, "endOffset": 3123}, {"referenceID": 0, "context": "In comparison, in the non-smooth setting, there are \u03a9(1/ \u221a T ) and \u03a9(1/T ) lower bounds for convex and strongly-convex problems, respectively Agarwal et al. (2012). These results leave open several issues. First, they pertain to averaging significant parts of the iterates, although in practice averaging just over the last few iterates, or returning the last iterate wT , often works quite well (e.g. Shalev-Shwartz et al. (2011)). Unless F is smooth, the previous results cannot say much about the optimization error of individual iterates. For example, the results in Rakhlin et al. (2011) only imply an O(1/ \u221a T ) convergence rate for the last iterate wT with strongly-convex functions, and we are not aware of any results for the last iterate wT in the general convex case. In fact to the best of our knowledge, even for the simpler (non-stochastic) gradient descent method (where \u011dt = gt), we do not know any existing results that can guarantee the performance of each individual iterate wT . Second, the theoretically optimal suffix-averaging scheme proposed in Rakhlin et al. (2011) has some practical limitations, since it cannot be computed on-the-fly: unless we can store all iterates w1, . . . ,wT in memory, one needs to know the stopping time T beforehand, in order to know when to start computing the suffix average. In practice, T is often not known in advance. This can be partially remedied with a so-called doubling trick, but it is still not a simple or natural procedure compared to just averaging all iterates, and the latter was shown to be suboptimal in Rakhlin et al. (2011). In this paper, we investigate the convergence rate of SGD and the averaging schemes required to obtain them, with the following contributions: \u2022 We prove that the expected optimization error of every individual iterate wT is O(log(T )/T ) for strongly-convex F , and O(log(T )/ \u221a T ) for general convex F without smoothness assumptions on F . These results show that the suboptimality of the last iterate is not much worse than the optimal rates obtainable by averaging schemes, and partially addresses an open problem posed in Shamir (2012). Moreover, the latter result is (to the best of our knowledge) the first finite-sample bound on individual iterates of SGD for non-smooth convex optimization. The proof relies on a technique to reduce results on averages of iterates to results on individual iterates, which was implicitly used in Zhang (2004) for a somewhat different setting. \u2022 We improve the existing expected error bound on the suffix averaging scheme of Rakhlin et al. (2011), from O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ) to O(log( 1 min{\u03b1,1\u2212\u03b1} )/T ). \u2022 We propose a new and very simple running average scheme, called polynomial-decay averaging, and prove that it enjoys optimal rates of convergence. Unlike suffix-averaging, this new running average scheme can be easily computed on-the-fly. \u2022 We provide a simple experimental study of the averaging schemes discussed in the paper. We emphasize that although there exist other algorithms with O(1/T ) convergence rate in the strongly convex case (e.g. Hazan & Kale (2011); Ouyang & Gray (2012)), our focus in this paper is on the simple and widely-used SGD algorithm.", "startOffset": 142, "endOffset": 3145}, {"referenceID": 7, "context": "It is well-known that this framework can be applied to learning problems (see for instance Shalev-Shwartz et al. (2009)): given a hypothesis class W and a set of T i.", "startOffset": 91, "endOffset": 120}, {"referenceID": 3, "context": "Now comes the crucial trick: instead of picking w = w\u2217, as done in standard analysis (Hazan et al. (2007); Rakhlin et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 3, "context": "Now comes the crucial trick: instead of picking w = w\u2217, as done in standard analysis (Hazan et al. (2007); Rakhlin et al. (2011)), we instead pick w = wT\u2212k.", "startOffset": 86, "endOffset": 129}, {"referenceID": 3, "context": "Now comes the crucial trick: instead of picking w = w\u2217, as done in standard analysis (Hazan et al. (2007); Rakhlin et al. (2011)), we instead pick w = wT\u2212k. We also use the fact that E [ \u2016wt \u2212w\u2217\u20162 ] \u2264 4G 2 \u03bb2t (Rakhlin et al. (2011), Lemma 1), which implies that for any t \u2265 T \u2212 k, E[\u2016wt \u2212wT\u2212k\u2016] \u22642E [ \u2016wt \u2212w\u2217\u20162 + \u2016wT\u2212k \u2212w\u2217\u20162 ] \u2264 8G 2 \u03bb2 ( 1 t + 1 T \u2212 k ) \u2264 16G 2 \u03bb2(T \u2212 k) \u2264 32G 2 \u03bb2T .", "startOffset": 86, "endOffset": 233}, {"referenceID": 6, "context": "E[ST/2] is the expected average value of the last bT/2c iterates, which was already analyzed in (Rakhlin et al. (2011), Theorem 5), yielding a bound of", "startOffset": 97, "endOffset": 119}, {"referenceID": 10, "context": "We note that a similar technique was used in Zhang (2004), but for a different algorithm (one with constant learning rate), and the result was less explicit.", "startOffset": 45, "endOffset": 58}, {"referenceID": 6, "context": "In Rakhlin et al. (2011), it was shown that this averaging scheme results in an optimization error of O((1 + log( 1 1\u2212\u03b1 ))/\u03b1T ), which is optimal in terms of T , but increases rapidly as we make \u03b1 smaller.", "startOffset": 3, "endOffset": 25}, {"referenceID": 6, "context": "If \u03b1 is larger, we can use the existing analysis (Rakhlin et al. (2011), Theorem 5), and get that E[F (w\u0304 T )\u2212 F (w\u2217)] is at most ( 4 + 5 log ( 1 1 + 1/T \u2212 \u03b1 )) G \u03bbT .", "startOffset": 50, "endOffset": 72}, {"referenceID": 6, "context": "(9) is provably suboptimal and can harm performance Rakhlin et al. (2011). Alternatively, we can easily maintain and return the current iterate wt, but the bound we have for it is only O(log(t)/t), worse than the minimax-optimal O(1/t) we can get with suffix averaging.", "startOffset": 52, "endOffset": 74}, {"referenceID": 6, "context": "We chose the same 3 binary classification datasets ((ccat,cov1 and astro-ph) and experimental setup as in Rakhlin et al. (2011). For each dataset {xi, yi}i=1, we ran SGD on the support vector machine optimization problem", "startOffset": 106, "endOffset": 128}, {"referenceID": 6, "context": "Compared to these schemes, a simple average of all iterates is significantly suboptimal, matching the results of Rakhlin et al. (2011).", "startOffset": 113, "endOffset": 135}], "year": 2017, "abstractText": "Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the last SGD iterate scales as O(log(T )/ \u221a T ) for non-smooth convex objective functions, and O(log(T )/T ) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some experimental illustrations.", "creator": "LaTeX with hyperref package"}}}