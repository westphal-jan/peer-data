{"id": "1212.2006", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2012", "title": "A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization", "abstract": "Both supervised learning methods and LDA based topic model have been successfully applied in the field of query focused multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experiments on TAC2008 and TAC2009 demonstrate the effectiveness of our approach.", "histories": [["v1", "Mon, 10 Dec 2012 09:41:12 GMT  (316kb)", "http://arxiv.org/abs/1212.2006v1", "Submitted on 1 Oct 2012; Accepted on 8 Dec 2012"], ["v2", "Fri, 27 Dec 2013 17:28:14 GMT  (0kb,I)", "http://arxiv.org/abs/1212.2006v2", "This paper has been withdrawn by the author due to a crucial sign error in equation"]], "COMMENTS": "Submitted on 1 Oct 2012; Accepted on 8 Dec 2012", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jiwei li", "sujian li"], "accepted": true, "id": "1212.2006"}, "pdf": {"name": "1212.2006.pdf", "metadata": {"source": "CRF", "title": "A Novel Feature-based Bayesian Model for Query Focused Multi- document Summarization", "authors": ["Jiwei Li", "Sujian Li"], "emails": ["jl3226@cornell.edu", "lisujian@pku.edu.cn"], "sections": [{"heading": null, "text": "In this paper, we propose a novel supervised approach, which in principle can integrate rich sentence characteristics into Bayesian theme models, using both topic-model and function-based supervised learning methods. Experiments on TAC2008 and TAC2009 show the effectiveness of our approach."}, {"heading": "1 Introduction", "text": "In fact, most people who have chosen such a model will actually do it. (...) It's not that they understand it as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it. (...) It's as if they were doing it, as if they were doing it, as if they were doing it, as if they were doing it, as if they were doing it, as if they were doing it, if they were doing it, if they were doing it, if they were doing it, if they were doing it. (...)"}, {"heading": "2 Related Work", "text": "A variety of approaches have been proposed for topic-focused multi-document summaries, such as unsupervised (semi-supervised) approaches, supervised approaches, and Bayesian approaches. Most supervised approaches view summary summaries as a sentence-level two class classification problem, distinguishing desired summary sentences from undesirable ones in terms of sentence characteristics. Supervised machine learning methods such as Support Vector Machine (SVM) (Li, et al., 2009), Maximum Entropy (Osborne, 2002), Conditional Random Field (Shen et al., 2007), and Regression Models (Ouyang et al., 2010) have been introduced to utilize the rich sentence features for summary. Recently, the Bayesian theme model has shown its power in the summary for its clear likely interpretation. Daume and Marcu (2006) suggested that the model for sentence extraction be elaborated based on queries."}, {"heading": "3 Model description", "text": "In this section, we start with two basic topic models, LDA and sLDA, and then classify how we propose our sentence-based S-sLDA model."}, {"heading": "3.1 LDA and sLDA", "text": "The hierarchical Bayessche LDA (Blei et al., 2003) models the probability of a corpus on hidden topics as shown in Fig. 1 (a). Let K be the number of topics, M be the number of documents in the corpus, and V be the number of terms in the vocabulary. The topic distribution of each document m comes from a previous Dirichlet distribution Dir (\u03b1), and each document word mn w is scanned from a topic-word distribution z specified by a character from the theme-document distribution m. It is a K * M dimension matrix, and each k is a distribution via the V terms. For a document m, the generation process of the LDA is illustrated in Fig.2. m. m is a mixing ratio over the topics of the document m, and mnz is a K dimension variable that presents the topic allocation of the word mnw. Supervised LDA (Blei and McAuliffe 2007)."}, {"heading": "3.2 S-sLDA", "text": "The generative approach of S-sLDA is shown in Fig.3. We can see that the process of generating a sentence involves not only generating words in the current sentence, but also sentence characteristics. In S-sLDA, we assume that words in the same sentence are all generated from the same topic proposed by Gruber (2007). Let msnz be the hidden variable that indicates the topic to which the current sentence belongs. In S-sLDA, we assume that words in the same sentence are all generated from the same topic proposed by Gruber (2007). Let msnz assign the topic wordmsnw. In our view, msn msz z for each arbitrary [1,] msn n n n. msY denotes the feature vector of the current sentence and represents information such as the position of the current sentence in the document, the semantic similarity between the current sentence and the query, etc. The feature space is described in detail in section 4.2."}, {"heading": "3.3 Posterior Inference and Estimation", "text": "In view of a document and labels for each sentence, the posterior distribution of the latent variables is: 1: 1 1 11: 1 11: 1: (|) (|) (|,) (|) (|) (|,) (,) (,) (,) (,) (,) m msm (msNM s N m ms msn ms Km s n Nm m m ms ms ms ms ms ms ms ms ms Kz s nN Kp p p z p p p p p z z z z z z z z z (2) Equation (2) cannot be efficiently implemented."}, {"heading": "4 Our Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Learning", "text": "In this section we describe how we use the following strategies to calculate Qk1 (Qk). (D) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K) K (K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K) K (K) K (K) K) K (K) K) K (K) K (K) K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K (K) K) K (K) K (K) K (K) K (K) K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K (K) K) K (K) K (K) K (K (K) K (K) K) K (K (K) K (K) K (K) K) K (K) K (K) K (K) K (K) K (K) K) K (K (K) K (K) K) K (K (K) K) K) K (K) K (K"}, {"heading": "4.2 Feature Space", "text": "Many features have proved useful for summarizing (Louis et al., 2010).Here we discuss several types of features adopted in the S-sLDA model. Characteristics are either binary or normalized to the interval [0,1] divided by the normalized value. Characteristics used in S-sLDA are as follows: sentence position and paragraph position. Kosine similarity with query: Kosine similarity is based on the ti-idf value of terms. Local Inner-Document Specific Word: Innerdocument Degree is the number of edges between content and sentences from the same document. Local Inner-Document Degree Order is a binary feature that indicates whether the Inner-Dogree of the sentence is the largest among its neighbors. Document Specific Word: 1, if a sentence contains a specific word, otherwise 0. Average Unigram Sample (Nenkova Vanderwende 2005, Celil set; also the lever set of 2010 and the lever of D)."}, {"heading": "4.3 Sentence Selection Strategy", "text": "Next, we explain our sentence selection strategy. In accordance with our teaching that the desired summary should have a small KL divergence with the query topic, we propose a function to evaluate a set of sentences sum, which is a subset relative to the sentence set S.k k1 '() (| |') Log K sumsumsumk Q kScore Sum Q (16) ksum can be calculated using Eqn. (10) and \"Q k can be calculated by Eqn. (9). The evaluation of Sum is negative, but it does not affect our sentence selection process. Let * Sum name the optimal summary of updates. We can * Sum that maximizes the scoring function. * & () arg max. () Sum S Words Sum L Sum Score Sum (17) A greedy algorithm is applied by finding the sentence one by one."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experiment Set-up", "text": "The persons mentioned are able to comply with the rules that they have established in previous years, in the way that they have been able to comply with the rules, in the way that they have been able to comply with the rules, in the way that they have complied with the rules, in the way that they have been able to comply with the rules, in the way that they have complied with the rules, in the way that they have complied with the rules, in the way that they have complied with the rules, in the way that they have complied with the rules, in the way that they have complied with the rules."}, {"heading": "5.3 Comparison with other Bayesian models.", "text": "In this subsection, we compare our model with other Bayesian baselines such as Hybhsum (Celikyilmaz and Hakkani-Tur 2010), HierSum (Haghighi and Vanderwende 2009), KLsum (Lin et al., 2006). In Hybhsum, we set 0: 1 as Celikyilmaz and Hakkani-Tur. All summaries are shortened to the same length of 100 words. From Table 1 and Table 2, we can see that Hybhsum achieves the best result among all Bayesian baselines, further illustrating the advantages of combining theme model and supervised method. We can also see that Rouge results from the S-sLDA model are only slightly better than results from Hybhsum. Further analyses between the two models are performed in the last part of the paper."}, {"heading": "5.4 Comparison with other baselines.", "text": "In order to further demonstrate the primacy of our system over other existing monitored and unsupervised approaches, we select several widely used summation algorithms for comparison. Here, the basic systems consist of uncontrolled methods, including Personalized Pagerank (Haveliwala, 2002), Manifold Ranking (Manifold) (Wan et al., 2007) and a monitored method - Support Vector Machine (SVM) (Vapnik 1995). These three algorithms are implemented by ourselves and their parameters are determined with the best ROUGE-2 results. Simultaneously, we also present the three best participating systems in terms of ROUGE-2 on TAC2008 and TAC2009 for comparison, which are referred to as SysRank 1., 2 and 3. (Gillick et al., 2008; Zhang et al., 2008; Varma et al. 2009)."}, {"heading": "5.5 Manual Evaluations", "text": "Overall quality, focus and responsiveness, SsLDA model significantly outcomes Hybhsum (based on t-test at 90% confidential level). Fig. 8 shows the sample summaries generated respectively by S-sLDA and Hybhsum for document collection D0824E-A of TAC2008. The query for D0824E-A is: \"Describe the causes and therapies being investigated to reduce breast cancer.\" As we can see from Figure 8 (a), there are four summary sentences generated from S-sLDA: sentence (1) and (4) talk about current therapies; (2) talks about the causes of breast cancer and (3) mentions both. Summaries generated by Hybhsum mostly talk about therapies and rarely mention the cause of breast cancer. It is not difficult to explain why S-sLDA can achieve better results than Hybhsum."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel supervised approach based on a revised supervised topic model for the query-focused summary of multiple documents. By its very nature, our approach combines the Bayesian topic model with a supervised method and enjoys the advantages of both models. Experiments with benchmarks show a good performance of our model."}], "references": [{"title": "Supervised topic models", "author": ["David Blei", "Jon McAuliffe"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Blei and McAuliffe.,? \\Q2007\\E", "shortCiteRegEx": "Blei and McAuliffe.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["David Blei", "Andrew Ng", "Micheal Jordan"], "venue": "In The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A class of methods for solving nonlinear simultaneous equations", "author": ["Charles Broyden"], "venue": "In Math. Comp.,", "citeRegEx": "Broyden.,? \\Q1965\\E", "shortCiteRegEx": "Broyden.", "year": 1965}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development", "citeRegEx": "Carbonell and Goldstein.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "A Hybrid hierarchical model for multi-document summarization", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur"], "venue": "In Proceeding of the", "citeRegEx": "Celikyilmaz and Hakkani.Tur.,? \\Q2010\\E", "shortCiteRegEx": "Celikyilmaz and Hakkani.Tur.", "year": 2010}, {"title": "DualSum: a topic-model based approach for update summarization", "author": ["Jean-Yves Delort", "Enrique Alfonseca"], "venue": "In Proceeding of EACL", "citeRegEx": "Delort and Alfonseca.,? \\Q2012\\E", "shortCiteRegEx": "Delort and Alfonseca.", "year": 2012}, {"title": "Summarizing Text Documents: Sentence Selection and Evaluation Metrics", "author": ["Jade Goldstein", "Mark Kantrowitz", "Vibhu Mittal", "Jaime Carbonell"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research", "citeRegEx": "Goldstein et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 1999}, {"title": "Bayesian Query-Focused Summarization", "author": ["Hal Daume", "Daniel Marcu H"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association", "citeRegEx": "Daume and H.,? \\Q2006\\E", "shortCiteRegEx": "Daume and H.", "year": 2006}, {"title": "Lexrank: graph-based lexical centrality as salience in text summarization", "author": ["Gune Erkan", "Dragomir Radev"], "venue": "In J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Erkan and Radev.,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "The ICSI/UTD Summarization System at TAC", "author": ["Dan Gillick", "Benoit Favre", "Dilek Hakkani-Tur", "Berndt Bohnet", "Yang Liu", "Shasha Xie"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Haghighi and Vanderwende.,? \\Q2009\\E", "shortCiteRegEx": "Haghighi and Vanderwende.", "year": 2009}, {"title": "The summarization systems at tac 2010", "author": ["Feng Jin", "Minlie Huang", "Xiaoyan Zhu"], "venue": "In Proceedings of the third Text Analysis Conference,", "citeRegEx": "Jin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2010}, {"title": "Enhancing diversity, coverage and balance for summarization through structure learning", "author": ["Liangda Li", "Ke Zhou", "Gui-Rong Xue", "Hongyuan Zha", "Yong Yu"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "An information-theoretic approach to automatic evaluation of summaries", "author": ["Chin-Yew Lin", "Guihong Gao", "Jianfeng Gao", "JianYun Nie"], "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American", "citeRegEx": "Lin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2006}, {"title": "Discourse indicators for content selection in summarization", "author": ["Annie Louis", "Aravind Joshi", "Ani Nenkova"], "venue": "In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Louis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "Multi-document summarization using minimum distortion", "author": ["Tengfei Ma", "Xiaojun Wan"], "venue": "Proceedings of International Conference of Data Mining", "citeRegEx": "Ma and Wan.,? \\Q2010\\E", "shortCiteRegEx": "Ma and Wan.", "year": 2010}, {"title": "Extractive multi-document summaries should explicitly not contain document-specific content", "author": ["Rebecca Mason", "Eugene Charniak"], "venue": "In proceeding of ACL HLT,", "citeRegEx": "Mason and Charniak.,? \\Q2011\\E", "shortCiteRegEx": "Mason and Charniak.", "year": 2011}, {"title": "The impact of frequency on summarization", "author": ["Ani Nenkova", "Lucy Vanderwende"], "venue": "In Tech. Report MSRTR-2005-101,", "citeRegEx": "Nenkova and Vanderwende.,? \\Q2005\\E", "shortCiteRegEx": "Nenkova and Vanderwende.", "year": 2005}, {"title": "A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization", "author": ["Ani Nenkova", "Lucy Vanderwende", "Kathleen McKeown"], "venue": "In Proceedings of the 29th annual International ACM SIGIR Conference", "citeRegEx": "Nenkova et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2006}, {"title": "Using maximum entropy for sentence extraction", "author": ["Miles Osborne"], "venue": "In Proceedings of the ACL-02 Workshop on Automatic Summarization,", "citeRegEx": "Osborne.,? \\Q2002\\E", "shortCiteRegEx": "Osborne.", "year": 2002}, {"title": "Using random walks for question-focused", "author": ["Jahna Otterbacher", "Gunes Erkan", "Dragomir Radev"], "venue": null, "citeRegEx": "Otterbacher et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Otterbacher et al\\.", "year": 2005}, {"title": "Applying regression models to query-focused multidocument summarization", "author": ["You Ouyang", "Wenjie Li", "Sujian Li", "Qin Lua"], "venue": "In Information Processing \\& Management,", "citeRegEx": "Ouyang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2011}, {"title": "Developing learning strategies for topic-based summarization", "author": ["You Ouyang", "Sujian. Li", "Wenjie. Li"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Ouyang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2007}, {"title": "Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora", "author": ["Daniel Ramage", "David Hall", "Ramesh Nallapati", "Christopher Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural", "citeRegEx": "Ramage et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2009}, {"title": "Document summarization using conditional random fields", "author": ["Dou She", "Jian-Tao Sun", "Hua Li", "Qiang Yang", "Zheng Chen"], "venue": "In Proceedings of International Joint Conference on Artificial Intelligence,", "citeRegEx": "She et al\\.,? \\Q2007\\E", "shortCiteRegEx": "She et al\\.", "year": 2007}, {"title": "Multi-document Summarization using cluster-based link analysis", "author": ["Xiaojun Wan", "Jianwu Yang"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Wan and Yang.,? \\Q2008\\E", "shortCiteRegEx": "Wan and Yang.", "year": 2008}, {"title": "Manifold-ranking based topic-focused multidocument summarization", "author": ["Xiaojun Wan", "Jianwu Yang", "Jianguo Xiao"], "venue": "In Proceedings of International Joint Conference on Artificial Intelligence,", "citeRegEx": "Wan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2007}, {"title": "Exploiting Query-Sensitive Similarity for GraphBased Query-Oriented Summarization", "author": ["Furu Wei", "Wenjie Li", "Qin Lu", "Yanxiang He"], "venue": "In Proceedings of the 31st annual International ACM SIGIR Conference on Research and Development", "citeRegEx": "Wei et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2008}, {"title": "ICTCAS's ICTGrasper at TAC 2008: Summarizing Dynamic Information with Signature Terms Based Content Filtering, TAC", "author": ["Jin Zhang", "Xueqi Cheng", "Hongbo Xu", "Xiaolei Wang", "Yiling Zeng"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Ranking on Data Manifolds", "author": ["Dengzhong Zhou", "Jason Weston", "Arthur Gretton", "Olivier Bousquet", "Bernhard Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2003}, {"title": "Conditional Topic Random Fields", "author": ["Jun Zhu", "Eric Xing"], "venue": "In Proceeding of the 27th International Conference on Machine Learning,", "citeRegEx": "Zhu and Xing.,? \\Q2010\\E", "shortCiteRegEx": "Zhu and Xing.", "year": 2010}, {"title": "Semi-supervised Learning using Gaussian Fields and Harmonic Functions", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of a set of documents and has been proven to be an effective way consume massive information.", "startOffset": 43, "endOffset": 104}, {"referenceID": 26, "context": "Query-focused multi-document summarization (Nenkova et al., 2006; Wan et al., 2007; Ouyang et al., 2010) can facilitate users to grasp the main idea of a set of documents and has been proven to be an effective way consume massive information.", "startOffset": 43, "endOffset": 104}, {"referenceID": 1, "context": "Recently, LDA-based (Blei et al., 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 10, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 11, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 16, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 5, "context": ", 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012).", "startOffset": 228, "endOffset": 355}, {"referenceID": 30, "context": "LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and are do not explore more useful text features such as position, word order etc (Zhu and Xing, 2010).", "startOffset": 191, "endOffset": 211}, {"referenceID": 1, "context": "Recently, LDA-based (Blei et al., 2003) Bayesian topic models have successfully been applied to multi-document summarization in that Bayesian approaches can offer clear and rigorous probabilistic interpretations for summaries that other techniques do not have (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al., 2010; Mason and Charniak, 2011; Delort and Alfonseca, 2012). Exiting Bayesian approaches label sentences or words with topics and sentences which are closely related with query or can highly generalize documents can be selected into summaries. LDA topic model suffers from the intrinsic disadvantages that it only uses word frequency for topic modeling and are do not explore more useful text features such as position, word order etc (Zhu and Xing, 2010). For example, the first sentence in a paragraph or a passage may be important for summary since it is more likely to give a global generalization about a paragraph or a passage. But LDA model is hard to consider such information, making useful information lost. It naturally comes to our minds that we can improve summarization performance by making full use of both more useful text features and the latent semantic structures generated by LDA topic models. One related work is by Celikyilmaz and Hakkani-Tur (2010). They built a hierarchical topic model called Hybhsum based on LDA for topic discovery and assumed this model can produce appropriate scores for evaluating sentences.", "startOffset": 21, "endOffset": 1301}, {"referenceID": 19, "context": ", 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al.", "startOffset": 25, "endOffset": 40}, {"referenceID": 17, "context": ", 2009), Maximum Entropy (Osborne, 2002) , Conditional Random Field (Shen et al., 2007) and regression models (Ouyang et al., 2010) have been adopted to leverage the rich sentence features for summarization. Recently, Bayesian topic model has shown its power in summarization for its clear probabilistic interpretation. Daume and Marcu (2006) proposed Bayesum model for sentence extraction based on query expansion concept in information retrieval.", "startOffset": 26, "endOffset": 343}, {"referenceID": 9, "context": "Haghighi and Vanderwende (2009) proposed topicsum and hiersum which use a LDA-like topic model and assign each sentence a distribution over background topic, doc-specific topic and content topics.", "startOffset": 0, "endOffset": 32}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization.", "startOffset": 0, "endOffset": 35}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and Hakkani-Tur\u2019s model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al.", "startOffset": 0, "endOffset": 779}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and Hakkani-Tur\u2019s model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al. (2009) proposed Labeled LDA by defining a one to one correspondence between latent topic and user tags.", "startOffset": 0, "endOffset": 846}, {"referenceID": 4, "context": "Celikyilmaz and Hakkani-Tur (2010) made a good step in combining topic model with supervised feature based regression for sentence scoring in summarization. In their model, the score of training sentences are firstly got through a novel hierarchical topic model. Then a featured based support vector regression (SVR) is used for sentence score prediction. The problem of Celikyilmaz and Hakkani-Tur\u2019s model is that topic model and feature based regression are two separate processes and the score of training sentences may be biased because their topic model only consider word frequency and fail to consider other important features. Supervised feature based topic models have been proposed in recent years to incorporate different kinds of features into LDA model. Blei (2007) proposed sLDA for document response pairs and Daniel et al. (2009) proposed Labeled LDA by defining a one to one correspondence between latent topic and user tags. Zhu and Xing(2010) proposed conditional topic random field(CTRF) which addresses feature and independent limitation in LDA.", "startOffset": 0, "endOffset": 962}, {"referenceID": 1, "context": "The hierarchical Bayesian LDA (Blei et al., 2003) models the probability of a corpus on hidden topics as in Fig.", "startOffset": 30, "endOffset": 49}, {"referenceID": 1, "context": "The first, third and forth terms of equation (3)are identical to the corresponding terms for unsupervised LDA(Blei et al., 2003).", "startOffset": 109, "endOffset": 128}, {"referenceID": 14, "context": "Lots of features have been proven to be useful for summarization(Louis et al., 2010).", "startOffset": 64, "endOffset": 84}, {"referenceID": 6, "context": "To avoid topic redundancy in the summary, we also revise the MMR strategy (Goldstein et al., 1999; Ouyang et al., 2007) in the process of sentence selection.", "startOffset": 74, "endOffset": 119}, {"referenceID": 22, "context": "To avoid topic redundancy in the summary, we also revise the MMR strategy (Goldstein et al., 1999; Ouyang et al., 2007) in the process of sentence selection.", "startOffset": 74, "endOffset": 119}, {"referenceID": 10, "context": "In order to obtain a more comprehensive measure of summary quality, we also conduct manual evaluation on TAC data with the reference to (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011; Delort and Alfonseca, 2011).", "startOffset": 136, "endOffset": 231}, {"referenceID": 13, "context": "In this subsection, we compare our model with other Bayesian baselines such as Hybhsum(Celikyilmaz and Hakkani-Tur 2010), HierSum(Haghighi and Vanderwende 2009), KLsum (Lin et al., 2006).", "startOffset": 168, "endOffset": 186}, {"referenceID": 26, "context": "Here, the baseline systems are composed of unsupervised methods including Personalized Pagerank (PageRank) (Haveliwala, 2002), Manifold Ranking (Manifold) (Wan et al., 2007), and a supervised method - Support Vector Machine(SVM) (Vapnik 1995).", "startOffset": 155, "endOffset": 173}, {"referenceID": 28, "context": "At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for comparison, denoted as (denoted as SysRank 1, 2 and 3)(Gillick et al., 2008; Zhang et al., 2008; Gillick et al., 2009; Varma et al., 2009).", "startOffset": 174, "endOffset": 258}, {"referenceID": 9, "context": "At the same time, we also present the top three participating systems with regard to ROUGE-2 on TAC2008 and TAC2009 for comparison, denoted as (denoted as SysRank 1, 2 and 3)(Gillick et al., 2008; Zhang et al., 2008; Gillick et al., 2009; Varma et al., 2009).", "startOffset": 174, "endOffset": 258}], "year": 2012, "abstractText": "Both supervised learning methods and LDA based topic model have been successfully applied in the field of query focused multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experiments on TAC2008 and TAC2009 demonstrate the effectiveness of our approach.", "creator": "Microsoft Office Word 2007"}}}