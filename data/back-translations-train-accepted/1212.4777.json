{"id": "1212.4777", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2012", "title": "A Practical Algorithm for Topic Modeling with Provable Guarantees", "abstract": "Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model inference have been based on a maximum likelihood objective. Efficient algorithms exist that approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for topic model inference that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.", "histories": [["v1", "Wed, 19 Dec 2012 18:14:51 GMT  (187kb,D)", "http://arxiv.org/abs/1212.4777v1", "26 pages"]], "COMMENTS": "26 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["sanjeev arora", "rong ge 0001", "yonatan halpern", "david m mimno", "ankur moitra", "david sontag", "yichen wu", "michael zhu"], "accepted": true, "id": "1212.4777"}, "pdf": {"name": "1212.4777.pdf", "metadata": {"source": "CRF", "title": "A Practical Algorithm for Topic Modeling with Provable Guarantees", "authors": ["Sanjeev Arora", "Rong Ge", "Yoni Halpern", "David Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu"], "emails": ["arora@cs.princeton.edu.", "rongge@cs.princeton.edu.", "halpern@cs.nyu.edu.", "mimno@cs.princeton.edu.", "moitra@ias.edu.", "dsontag@cs.nyu.edu.", "yichenwu@princeton.edu.", "mhzhu@princeton.edu."], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that it is a matter of a way in which people are able to change and change the world, \"he said in an interview with the\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York Times, \"\" New York, \"the\" New York Times, \"the\" the \"New York,\" the \"New York,\" the \"New York,\" the \"New York,\" the \"New York,\" the \"New York,\" the \"the\" New York, \"the\" the \"New York,\" the \"the\" the \"New York Times,\" the \"the\" the \""}, {"heading": "2 Background", "text": "Examples of such distributions are latent Dirichlet allocation [Lead et al., 2003], correlated topic models [Lead & Lafferty, 2007], and Pachinko allocation [Li & McCallum, 2007]. We refer to the number of words in the vocabulary by V and the number of topics by K. Associated topic to each topic k is a multinomial distribution over the words in the vocabulary that we will call column vector Ak of length V. Each of these topic models postulates a certain prior distribution over the topic of a document. For example, latent Dirichlet allocation (LDA) is a Dirichlet distribution and for the related topic a model that is a logistic normal distribution."}, {"heading": "3 A Probabilistic Approach to Exploiting Separability", "text": "Dre rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf the rf\u00fc the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the r"}, {"heading": "4 A Combinatorial Algorithm for Finding Anchor Words", "text": "Here we look at the anchor selection step of the algorithm, where our goal is to find the anchor words, whether they are linear or not. In the infinite data case, where we have an infinite number of documents, the convex body of the lines in row Q is only an approximation of their expectation. We therefore have a series of V points d1, d2,... dV, each with a perturbation of a1, a2,... aV, whose convex hull P defines a simplex. We would like to find an approximation of the depressions of P. See Arora et al. [2012b] for more details on this problem. [2012a] We give a polynomial time algorithm based on the anchor words."}, {"heading": "5 Experimental Results", "text": "We compare three parameter recovery methods, Recover [Arora et al., 2012b], RecoverKL, and RecoverL2, with a quick implementation of Gibbs sampling [McCallum, 2002].2 Linear programmable word finding is too slow to be comparable, so we use FastAnchorWords for all three recovery algorithms. Using Gibbs sampling, we obtain word-theme distributions by tracing 1000 burn-in iterations over 10 stored states, each separated by 100 iterations."}, {"heading": "5.1 Methodology", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to hide. We have to be able to hide, \"he said.\" We have to be able to hide, \"he said."}, {"heading": "5.2 Efficiency", "text": "The recovery algorithms in Python are faster than a highly optimized Java Gibbs sampling implementation [Yao et al., 2009]. Fig. 1 shows the time in which models on synthetic corpora are trained on a single machine. Gibbs sampling is linear in body size. RecoverL2 is also linear (\u03c1 = 0.79), but varies only between 33 and 50 seconds. The estimate of Q is linear, but takes only 7 seconds for the largest corpora. FastAnchorWords takes less than 6 seconds for all corpora."}, {"heading": "5.3 Semi-synthetic documents", "text": "The new algorithms have good '1 reconstruction errors for semi-synthetic documents, especially for larger corpora. Results for semi-synthetic corpora derived from topics covered in New York Times articles are shown in Fig. 2 for body sizes from 50k to 2M of synthetic documents. In addition, we report results for the three \"infinite data\" recovery algorithms, that is, the true Q matrix from the model used to generate the documents. Error bars show discrepancies between the topics. Recover performs poorly in all but the silent, infinite data setting. Gibbs sampling has a lower' 1 for smaller corpora, while the new algorithms achieve better recovery and lower discrepancies for more data (although more sampling could further reduce the MCMC error). Results for semi-synthetic corpora extracted from NIPS topics are shown in lower corpora, while the new algorithms achieve a better recovery and lower discrepancies for more data (although more sampling could further reduce the MCMC error). Results for semi-synthetic corpora extracted from NIPS topics are shown in lower corpora, but are shown better in corpora, but is shown in Fig. 3 for corpora, however, the reca is shown poorer at Reca = 40.000."}, {"heading": "5.4 Effect of separability", "text": "The non-negative algorithms are more resistant to separability violations than the original recovery algorithm. In Fig. 3, Recover does not achieve zero-1 error even on noiseless \"infinite\" data. Here, we show that this is due to a lack of separability. In our semi-synthetic corpora, documents are generated from the LDA model, but the theme-word distributions are learned from data and may not meet the anchor word assumption. We test the sensitivity of algorithms to separability violations by adding a synthetic anchor word to each topic that is constructively unique to the topic. We assign a probability to the synthetic anchor word that corresponds to the most likely word in the original topic, resulting in the distribution adding up to more than 1.0, so we renormalize. The results are shown in Fig. 4."}, {"heading": "5.5 Effect of correlation", "text": "The theoretical guarantees of the new algorithms apply even when the topics are correlated. To test how algorithms respond to correlation, we have generated new synthetic corpora from the same K = 100 model that was trained on New York Times articles. Instead of a symmetrical dirichlet distribution, we use a normal logistic distribution with a block-structured covariance matrix. We divide the topics into 10 groups. For each topic pair in a group, we add a non-zero deviating element to the covariance matrix. This block structure is not necessarily realistic, but shows the effect of the correlation. The results for two levels of covariance (\u03c1 = 0.05, \u03c1 = 0.1) are in Fig. 5. The results for recall in both cases are much worse than the dirichlet-generated corpora in Fig. 2. The other three algorithms, in particular Gibbs sampling, are more robust for the correlation, but the correlation improves as the correlation improves and the correlation with a larger correlation improves."}, {"heading": "5.6 Real documents", "text": "The new algorithms produce comparable quantitative and qualitative results on real data. Fig. 6 shows three metrics for both corpora. Error bars show the distribution of the log probabilities between held-up documents (top panel) and coherence and unique words on topics (middle and bottom panel). Sustained sentences are 230 documents for NIPS and 59k for NY Times. For the small NIPS corpus, we produce an average of over 5 non-overlapping pull / test splits. Matrix inversion in Recover failed for the smaller corpus (NIPS). In the larger corpus (NY Times), Recover produces a significantly lower log probability per token than the other algorithms. Scanning Gibbs yields the sustained probability (p < 0.0001 under a paired t test), but the difference is in the range of variability between the documents. We have tried several methods of comparing the Y parameters, but not changing the differences between the observed parameters)."}, {"heading": "6 Conclusions", "text": "The runtime of these algorithms is effectively independent of the size of the corpus. Empirical results suggest that the sample complexity of these algorithms is slightly greater than that of the MCMC, but especially for the \"2 variant, they deliver comparable results in a fraction of the time. We have tried to use the results of our algorithms as an initialization for further optimizations (e.g. using MCMC), but have not yet found a hybrid that surpasses both methods alone. Finally, although we postpone parallel implementations for future work, these algorithms are parallelizable and potentially support web-based theme conclusions."}, {"heading": "A Proof for Anchor-Words Finding Algorithm", "text": "Remember that the correctness of the algorithm depends on the following lemmas: Lemma A.1. There is a vertex vi whose distance from the span (S) is at least \u03b3 / 2.Lemma A.2. The point \u0394j found by the algorithm must be equal = O (/ \u03b3 2) near a vertex vi.To prove that Lemma A.1, we use a volume argument. First, we show that the volume of a robust simplex cannot be changed too much when the vertices perturbed.Lemma A.3. Assumption {v1, v2,..., vK} are the vertices of a robust Simplex S. Let S be a simplex with vertices {v \u00b2 1, v \u00b2 2, each of the vertices v \u00b2 i is a perturbation of vi and vice versa."}, {"heading": "B Proof for Nonnegative Recover Procedure", "text": "To show that we know the parameters even if the lines of Q = Q = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0: 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 ="}, {"heading": "C Empirical Results", "text": "This section contains diagrams for \"1, sustained probability, coherence and uniqueness for all semi-synthetic data sets. Up is better for all metrics except\" 1 error. C.1 Sample topic tables 2, 3 and 4 show 100 topics trained using real New York Times articles using the RecoverL2 algorithm. Each topic is followed by the most similar topic (measured by \"1 spacing) from a model learned using Gibbs samples from the same documents. If the anchor word is likely to be among the first six words, it is highlighted in bold. Note that the anchor word is often not the most prominent word."}, {"heading": "D Algorithmic Details", "text": "D \"i, ie T zd, i is the i-th word of d.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D.D. \"D.D.\" D. \"D.\" D.D. \"D.D.\" D.D. \"D.D.\" D.D. \"D.D.\" D.D. \"D.D.\" D.D. \"D.\" D.D. \"D.\" D.D. \"D.\" D. D. \"D.\" D. \"D.D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D.D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D.D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D"}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Topic models provide a useful method for dimensionality reduction and exploratory data<lb>analysis in large text corpora. Most approaches to topic model inference have been based on<lb>a maximum likelihood objective. Efficient algorithms exist that approximate this objective,<lb>but they have no provable guarantees. Recently, algorithms have been introduced that provide<lb>provable bounds, but these algorithms are not practical because they are inefficient and not ro-<lb>bust to violations of model assumptions. In this paper we present an algorithm for topic model<lb>inference that is both provable and practical. The algorithm produces results comparable to the<lb>best MCMC implementations while running orders of magnitude faster.", "creator": "LaTeX with hyperref package"}}}