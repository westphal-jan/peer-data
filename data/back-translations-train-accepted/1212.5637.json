{"id": "1212.5637", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2012", "title": "Random Spanning Trees and the Prediction of Weighted Graphs", "abstract": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.", "histories": [["v1", "Fri, 21 Dec 2012 23:51:21 GMT  (146kb,D)", "http://arxiv.org/abs/1212.5637v1", "Appeared in ICML 2010"]], "COMMENTS": "Appeared in ICML 2010", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nicol\u00f2 cesa-bianchi", "claudio gentile", "fabio vitale", "giovanni zappella"], "accepted": true, "id": "1212.5637"}, "pdf": {"name": "1212.5637.pdf", "metadata": {"source": "CRF", "title": "Random Spanning Trees and the Prediction of Weighted Graphs", "authors": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile"], "emails": ["nicolo.cesa-bianchi@unimi.it", "claudio.gentile@uninsubria.it", "fabio.vitale@unimi.it", "giovanni.zappella@unimi.it"], "sections": [{"heading": "1 Introduction", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in"}, {"heading": "1.1 Preliminaries and Basic Notation", "text": "Let G = (V, E, W) is an undirected, connected, and weighted graph with n nodes and positive edge weights wi, j > 0 for (i, j).A The caption of G is any arrangement y = (y1)...., yn). (1, + 1) n of the binary labels to its nodes. We use (G, y) to mark the resulting number. The online learning protocol for prediction (G, y) can be defined as the next game that lies between a (possibly randomized) learner and an opponent. The game is parameterized by the graph G = (V, E, W)."}, {"heading": "G C2C1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Related Work", "text": "With the above notation and the precursor data in hand, it is as expected easy to gather the results in the existing literature most closely related to this work. Further comments are made online at the end of Section 5.Standard linear learners, such as the Perceptron algorithm, are applied to the general (weighted) graph prediction problem by adding the n depressions of the graph in Rn through a map i 7 \u2192 K \u2212 1 / 2ei, where ei-Rn is the i-th vector in the canonical base of Rn, and K is a positively defined n \u00b7 n matrix. The graph perctron algorithm [17, 16] uses K = LG + 11 >, where LG is the (weighted) laplacian of G and 1 = (1, 1)."}, {"heading": "3 A General Lower Bound", "text": "This section contains our general lower limit. We show that each prediction algorithm must be error-free at least 12 E\u03a6T (y) times on each weighted graph. (Theorem 1: LetG = (V, E, W) is a weighted undirected graph with n nodes and weights. (PG = 0 for (i, j).Then for all K \u2264 n there is a randomized labeling y of G, so for all (deterministic or randomized) algorithms A the expected number of prediction errors made by A is at least K / 2, while E\u03a6T (y) < K.Proof. The opponent uses the weighting P induced by W and defined by pi, j = wi, jrWi, j the probability is that the edge (i, j) to a random tree T of G. Let Pi = Pi, j pi, j pi, the sum of the induced weights of all events is node."}, {"heading": "4 The Weighted Tree Algorithm", "text": "We will now describe the Weighted Tree Algorithm (WTA) to predict the labels of a weighted tree. In Section 5, we will show how to apply WTA to the more general weighted graph prediction problem. WTA first transforms the tree into a line graph (i.e., a list), then executes a quick neighboring method to predict the labels of each node in the line. Although this technique is similar to the one used by [18], the fact that the tree is weighted makes analysis much more difficult, and the practical scope of our algorithm is much broader. Our experimental comparison in Section 8 confirms that using the weight information in the real world is often advantageous. Given a marked weighted tree (T, y), the algorithm first generates a weighted line graph containing some duplicates of the nodes in T. Then, each duplicate node is replaced in T (together with its preedges)."}, {"heading": "4.1 Analysis of WTA", "text": "The following problem gives an error bound to WTA on each weighted line chart. Faced with any marked chart (G, y), we at RWG denote the sum of the resistances of the \"F\" -free edges in G, RWG = \"F\" edge, j. \"Even if we make errors in the error-free edge E\" F \"-E\u03c6\" F \"edge, we define RWG (E\" E \") as the sum of the resistances of all the\" F \"-free edges in E\" (E \"), RWG (E\") = \"F\" edge, \"i\" J, \"E\" 1wi. \"Note that RWG (E\") \u2264 RWG \"because we drop some edges from the definition formula. Finally, we use f O = g as the abbreviation for f = O (g). The following problem is the starting point of our theoretical investigation - see appendix A for Proofs.Lemma 2. If WTA is weighted on a diagram (G) in G (O)."}, {"heading": "5 Predicting a Weighted Graph", "text": "To solve the more general problem of predicting a weighted diagram G = 1 error, one can first create an spanning tree T of G and then run WTA directly on T. (In this case, it is possible to reformulate theorem 3 with respect to the properties of G.) Note that for each spanning tree T of G, \u03a6WT (y) and \u03a6T (y), as the individual decisions of the spanning tree T differ, the amounts in the margin of error of theorem 3. For example, a minimal spanning tree tends to decrease the value of R-WT, betting on the fact that the spanning edges are light. The next theorem relies on random trees.Theorem 4 If WTA is executed on a random tree spanning a labeled curve (G, y), then satisfy the total number of errors.EmG = E (y)."}, {"heading": "6 The Robustness of WTA to Label Perturbation", "text": "In this section, we show that WTA is tolerant to noise, i.e., the number of errors made by WTA on most designated graphs (G, y) does not change significantly if a small number of labels are disturbed before executing the algorithm, especially if the input graph G is polynomically connected (see section 5 for a definition). As in previous sections, we start from the case where the input graph is a tree, and then expand the result to generic graphs based on random trees.Assuming that the labels y in the tree (T, y) are used as input to the algorithm, we actually assumed a different labeling y of the labeling y of the label."}, {"heading": "7 Implementation", "text": "As explained in Section 4, the WTA runs in two phases: (i) a random series of experiments spanning any pair. (ii) The tree is linearized and the labels are predicted sequentially. As discussed in Section 1.1, Wilson's algorithm can draw a random experimental tree from \"most\" unweighted graphs in the expected time O (n). Analyzing runtimes on weighted graphs is much more complex and outside the scope of this paper. A naive implementation of the second phase of WTA runs in the time O (n log n log n) and requires linear memory space when operating on a tree with n nodes. We will now describe how to implement the second phase to run in the time O (n), i.e., in constant amortized time per prediction. Once the given tree T is linearized, we will be embedded in an n-node to the left, we will first pass through L."}, {"heading": "8 Experiments", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "9 Conclusions and Open Questions", "text": "We have introduced and analyzed WTA, a randomized online prediction algorithm for weighted diagram predictions that uses random tree circumferences and has near-optimal performance guarantees in terms of expected predictive accuracy; the expected runtime of WTA is optimal when the random tree is drawn and edge weights ignored; and, thanks to its linearization phase, the algorithm is also demonstrably robust for marking notes. Our experimental evaluation shows that WTA outperforms other previously proposed online predictors; in addition, WTA, in combination with an aggregation of random tree circumferences, also tends to beat standard batch predictors, such as the propagation of labels; these features make WTA (and its combinations) suitable for large-scale applications; there are two main directions in which this work can be improved; firstly, previous analyses [7] show that analysis of WTA is loose, at least if the input tree is a weighted diagram."}, {"heading": "Appendix A", "text": "This appendix contains the proofs of Lemma 2, Theorem 3, Theorem 4, Corollary 5, Theorem 6, Theorem 7, Corollary 8, Theorem 9 and Corollary 10. Notation and references are like in the main text. We start by giving Lemma 2.Lemma 2. Let a cluster be any maximum subline of L, whose edges are all free. Then, L contains exactly this cluster L (y) + 1 cluster, which we enumerate continuously, starting with one of the two terminal nodes. Consider the k-th cluster ck, whose label is predicted by WTA."}, {"heading": "Appendix B", "text": "This appendix summarizes all of our experimental results. For each combination of dataset, algorithm, and tensile test distribution, we provide macro-averaged error rates and F measurements on the test set. The algorithms are WTA, NWWTA, and GPA (all combined with different spanning trees), WMV, LABPROP, and WTA run with rejects of random spanning trees. WEBSPAM was too large a dataset to conduct a thorough investigation. Therefore, we report only test results on the four algorithms WTA, WMV, LABPROP, and WTA with a reject of 7 (non-weighted) random spanning trees."}], "references": [{"title": "Many random walks are faster than one", "author": ["N. Alon", "C. Avin", "M. Kouck\u00fd", "G. Kozma", "Z. Lotker", "M.R. Tuttle"], "venue": "Proc. of the 20th Annual ACM Symposium on Parallel Algorithms and Architectures, pages 119\u2013128. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Regularization and semi-supervised learning on large graphs", "author": ["M. Belkin", "I. Matveeva", "P. Niyogi"], "venue": "Proc. of the 17th Annual Conference on Learning Theory, pages 624\u2013638. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Semi-Supervised Learning, pages 193\u2013216. MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "Proc. of the 18th International Conference on Machine Learning, pages 19\u201326. Morgan Kaufmann", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Semi-supervised learning using randomized mincuts", "author": ["A. Blum", "J. Lafferty", "M. Rwebangira", "R. Reddy"], "venue": "Proc. of the 21st International Conference on Machine Learning, pages 97\u2013104", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Generating random spanning trees", "author": ["A. Broder"], "venue": "Proc. of the 30th Annual Symposium on Foundations of Computer Science, pages 442\u2013447. IEEE Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Fast and optimal prediction of a labeled tree", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale"], "venue": "Proceedings of the 22nd Annual Conference on Learning Theory. Omnipress", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Random spanning trees and the prediction of weighted graphs", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale", "G. Zappella"], "venue": "Proceedings of the 27th International Conference on Machine Learning (27th ICML)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Active learning on trees and graphs", "author": ["N. Cesa-Bianchi", "C. Gentile", "F. Vitale", "G. Zappella"], "venue": "Proceedings of the 23rd Conference on Learning Theory (23rd COLT)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Graph Laplacian kernels for object classification from a single example", "author": ["H. Chang", "D.Y. Yeung"], "venue": "Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 2011\u20132016. IEEE Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Low congestion online routing and an improved mistake bound for online prediction of graph labeling", "author": ["J. Fakcharoenphol", "B. Kijsirikul"], "venue": "CoRR, abs/0809.2075", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Functional organization of the yeast proteome by systematic analysis of protein complexes", "author": ["A.-C. Gavin"], "venue": "Nature, 415(6868):141\u2013147,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Seeing stars when there aren\u2019t many stars: Graph-based semisupervised learning for sentiment categorization", "author": ["A. Goldberg", "X. Zhu"], "venue": "HLT-NAACL 2006 Workshop on Textgraphs: Graph-based algorithms for Natural Language Processing", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Exploiting cluster-structure to predict the labeling of a graph", "author": ["M. Herbster"], "venue": "Proc. of the 19th International Conference on Algorithmic Learning Theory, pages 54\u201369. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Predicting the labelling of a graph via minimum p-seminorm interpolation", "author": ["M. Herbster", "G. Lever"], "venue": "Proc. of the 22nd Annual Conference on Learning Theory. Omnipress", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Prediction on a graph with the Perceptron", "author": ["M. Herbster", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems 21, pages 577\u2013584. MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Online learning over graphs", "author": ["M. Herbster", "M. Pontil", "L. Wainer"], "venue": "Proc. of the 22nd International Conference on Machine Learning, pages 305\u2013312. ACM Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Online prediction on large diameter graphs", "author": ["M. Herbster", "G. Lever", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems 22, pages 649\u2013656. MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast prediction on a tree", "author": ["M. Herbster", "M. Pontil", "S. Rojas-Galeano"], "venue": "Advances in Neural Information Processing Systems 22, pages 657\u2013664. MIT Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "A comprehensive two-hybrid analysis to explore the yeast protein interactome", "author": ["T. Ito", "T. Chiba", "R. Ozawa", "M. Yoshida", "M. Hattori", "Y. Sakaki"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 98(8):4569\u20134574", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Global landscape of protein complexes in the yeast", "author": ["N.J. Krogan"], "venue": "Saccharomyces cerevisiae. Nature,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Probability on trees and networks", "author": ["R. Lyons", "Y. Peres"], "venue": "Manuscript", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Association analysis-based transformations for protein interaction networks: a function prediction case study", "author": ["G. Pandey", "M. Steinbach", "R. Gupta", "T. Garg", "V. Kumar"], "venue": "Proc. of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 540\u2013549. ACM Press", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "The FunCat", "author": ["A. Ruepp"], "venue": "a functional annotation scheme for systematic classification of proteins from whole genomes. Nucleic Acids Research, 32(18):5539\u20135545", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Graph sparsification by effective resistances", "author": ["D.A. Spielman", "N. Srivastava"], "venue": "Proc. of the 40th annual ACM symposium on Theory of computing, pages 563\u2013568. ACM Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Protein functional class prediction with a combined graph", "author": ["H. Shin K. Tsuda", "B. Sch\u00f6lkopf"], "venue": "Expert Systems with Applications,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "A comprehensive analysis of protein-protein interactions in Saccharomyces cerevisiae", "author": ["P. Uetz"], "venue": "Nature, 6770(403):623\u2013627,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "See the tree through the lines: the Shazoo algorithm", "author": ["F. Vitale", "N. Cesa-Bianchi", "C. Gentile", "G. Zappella"], "venue": "Proc. of the 25th Annual Conference on Neural Information Processing Systems, pages 1584-1592. Curran Associates", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating random spanning trees more quickly than the cover time", "author": ["D.B. Wilson"], "venue": "Proc. of the 28th ACM Symposium on the Theory of Computing, pages 296\u2013303. ACM Press", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "ICML Workshop on the Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "This technique for coding input data has been applied to several domains, including Web spam detection [19], classification of genomic data [27], face recognition [10], and text categorization [13].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 15, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 13, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 14, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 7, "context": ", when all edges have the same common weight), it was found in previous works [17, 16, 14, 15, 8] that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes, i.", "startOffset": 78, "endOffset": 97}, {"referenceID": 6, "context": "This naturally led to the idea of solving the prediction problem on a spanning tree of the graph [7, 18, 19], whose number of edges is exactly equal to the number of nodes minus one.", "startOffset": 97, "endOffset": 108}, {"referenceID": 17, "context": "This naturally led to the idea of solving the prediction problem on a spanning tree of the graph [7, 18, 19], whose number of edges is exactly equal to the number of nodes minus one.", "startOffset": 97, "endOffset": 108}, {"referenceID": 18, "context": "This naturally led to the idea of solving the prediction problem on a spanning tree of the graph [7, 18, 19], whose number of edges is exactly equal to the number of nodes minus one.", "startOffset": 97, "endOffset": 108}, {"referenceID": 6, "context": "This suggests to pick a tree at random among all spanning trees of the graph so as to prevent the adversary from concentrating the cutsize on the chosen tree [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "Although the results of [7] yield a mistake bound for arbitrary unweighted graphs in terms of the cutsize of a random spanning tree, no general lower bounds are known for online unweighted graph prediction.", "startOffset": 24, "endOffset": 27}, {"referenceID": 15, "context": "The scenario gets even more uncertain in the case of weighted graphs, where the only previous papers we are aware of [16, 14, 15] essentially contain only upper bounds.", "startOffset": 117, "endOffset": 129}, {"referenceID": 13, "context": "The scenario gets even more uncertain in the case of weighted graphs, where the only previous papers we are aware of [16, 14, 15] essentially contain only upper bounds.", "startOffset": 117, "endOffset": 129}, {"referenceID": 14, "context": "The scenario gets even more uncertain in the case of weighted graphs, where the only previous papers we are aware of [16, 14, 15] essentially contain only upper bounds.", "startOffset": 117, "endOffset": 129}, {"referenceID": 6, "context": "Following the ideas of [7], our algorithm first extracts a random spanning tree of the original graph.", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "Then, it predicts all nodes of this tree using a generalization of the method proposed by [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "As in [18], our algorithm first linearizes the tree, and then operates on the resulting line graph via a nearest neighbor rule.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "In particular, we test our algorithm against the Perceptron algorithm with Laplacian kernel by [16, 19], and against a version of the label propagation algorithm by [31].", "startOffset": 95, "endOffset": 103}, {"referenceID": 18, "context": "In particular, we test our algorithm against the Perceptron algorithm with Laplacian kernel by [16, 19], and against a version of the label propagation algorithm by [31].", "startOffset": 95, "endOffset": 103}, {"referenceID": 29, "context": "In particular, we test our algorithm against the Perceptron algorithm with Laplacian kernel by [16, 19], and against a version of the label propagation algorithm by [31].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": ", the monograph of [22].", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "Hence the ratio 1 n\u22121E\u03a6T (y) \u2208 [0, 1] provides a density-independent measure of the cutsize in G, and even allows to compare labelings on different graphs.", "startOffset": 31, "endOffset": 37}, {"referenceID": 13, "context": "A different way of taking into account graph connectivity is provided by the covering ball approach taken by [14, 15] \u2013see the next section.", "startOffset": 109, "endOffset": 117}, {"referenceID": 14, "context": "A different way of taking into account graph connectivity is provided by the covering ball approach taken by [14, 15] \u2013see the next section.", "startOffset": 109, "endOffset": 117}, {"referenceID": 16, "context": "The graph Perceptron algorithm [17, 16] uses K = LG + 11>, where LG is the (weighted) Laplacian of G and 1 = (1, .", "startOffset": 31, "endOffset": 39}, {"referenceID": 15, "context": "The graph Perceptron algorithm [17, 16] uses K = LG + 11>, where LG is the (weighted) Laplacian of G and 1 = (1, .", "startOffset": 31, "endOffset": 39}, {"referenceID": 18, "context": "The idea of using a spanning tree to reduce the cutsize ofG has been investigated by [19], where the graph Perceptron algorithm is applied to a spanning tree T ofG.", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "[19] suggest to apply the graph Perceptron algorithm to the spanning tree T with smallest geodesic diameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] introduce a different technique showing its application to the case of unweighted graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "By running a Nearest Neighbor (NN) predictor on S, [18] prove a mistake bound of the form \u03a6S(y) log ( n / \u03a6S(y) ) + \u03a6S(y).", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "As observed by [11], similar techniques have been developed to solve low-congestion routing problems.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "With this inductive bias in mind, [14] developed the Pounce algorithm, which can be seen as a combination of graph Perceptron and NN prediction.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "A further trick for the unweighted case proposed by [18] is to take advantage of both previous approaches (graph Perceptron and NN on line graphs) by building a binary tree on G.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "A similar logarithmic factor is achieved by the combined algorithm proposed in [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "An even more refined way of exploiting cluster structure and connectivity in graphs is contained in the paper of [15], where the authors provide a comprehensive study of the application of dualnorm techniques to the prediction of weighted graphs, again with the goal of obtaining logarithmic performance guarantees on large diameter graphs.", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "Further comments on the comparison between the results presented by [15] and the ones in our paper are postponed to the end of Section 5.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Departing from the online learning scenario, it is worth mentioning the significantly large literature on the general problem of learning the nodes of a graph in the train/test transductive setting: Many algorithms have been proposed, including the label-consistent mincut approach of [4, 5] and a number of other \u201cenergy minimization\u201d methods \u2014e.", "startOffset": 285, "endOffset": 291}, {"referenceID": 4, "context": "Departing from the online learning scenario, it is worth mentioning the significantly large literature on the general problem of learning the nodes of a graph in the train/test transductive setting: Many algorithms have been proposed, including the label-consistent mincut approach of [4, 5] and a number of other \u201cenergy minimization\u201d methods \u2014e.", "startOffset": 285, "endOffset": 291}, {"referenceID": 29, "context": ", the ones by [31, 2] of which label propagation is an instance.", "startOffset": 14, "endOffset": 21}, {"referenceID": 1, "context": ", the ones by [31, 2] of which label propagation is an instance.", "startOffset": 14, "endOffset": 21}, {"referenceID": 2, "context": "See the work of [3] for a relatively recent survey on this subject.", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": ", the recent monograph by [22].", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "In the unweighted case, a random spanning tree can be sampled with a random walk in expected time O(n lnn) for \u201cmost\u201d graphs, as shown by [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 28, "context": "Using the beautiful algorithm of [30], the expected time reduces to O(n) \u2014see also the work of [1].", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "Using the beautiful algorithm of [30], the expected time reduces to O(n) \u2014see also the work of [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "3 This is one of the examples considered in [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": ", [22].", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": ", the work of [26].", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Though this technique is similar to that one used by [18], the fact that the tree is weighted makes the analysis significantly more difficult, and the practical scope of our algorithm significantly wider.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "Another interesting comparison is to the covering ball bounds of [14, 15].", "startOffset": 65, "endOffset": 73}, {"referenceID": 14, "context": "Another interesting comparison is to the covering ball bounds of [14, 15].", "startOffset": 65, "endOffset": 73}, {"referenceID": 14, "context": "Whereas the dual norm approach of [15] gives a mistake bound of the form \u03a6G(y) logD, our approach, as well as the one by [18], yields \u03a6G(y) log n.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "Whereas the dual norm approach of [15] gives a mistake bound of the form \u03a6G(y) logD, our approach, as well as the one by [18], yields \u03a6G(y) log n.", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "A typical (and unsurprising) example where the dual-norm covering 6 As a matter of fact, a bound of the form \u03a6G(y) logD on unweighted trees is also achieved by the direct analysis of [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 14, "context": "One such example we already mentioned in Section 2: On the unweighted barbell graph made up of m-cliques connected by k m \u03c6-edges, the algorithm of [15] has a constant bound on the number of mistakes (i.", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "Yet, it is fair to point out that the bounds of [14, 15] refer to computationally heavier algorithms than WTA: Pounce has a deterministic initialization step that computes the inverse Laplacian matrix of the graph (this is cubic in n, or quadratic in the case of trees), the minimum (\u03a8, p)-seminorm interpolation algorithm of [15] has no initialization, but each step requires the solution of a constrained convex optimization problem (whose time complexity was not quantified by the authors).", "startOffset": 48, "endOffset": 56}, {"referenceID": 14, "context": "Yet, it is fair to point out that the bounds of [14, 15] refer to computationally heavier algorithms than WTA: Pounce has a deterministic initialization step that computes the inverse Laplacian matrix of the graph (this is cubic in n, or quadratic in the case of trees), the minimum (\u03a8, p)-seminorm interpolation algorithm of [15] has no initialization, but each step requires the solution of a constrained convex optimization problem (whose time complexity was not quantified by the authors).", "startOffset": 48, "endOffset": 56}, {"referenceID": 14, "context": "Yet, it is fair to point out that the bounds of [14, 15] refer to computationally heavier algorithms than WTA: Pounce has a deterministic initialization step that computes the inverse Laplacian matrix of the graph (this is cubic in n, or quadratic in the case of trees), the minimum (\u03a8, p)-seminorm interpolation algorithm of [15] has no initialization, but each step requires the solution of a constrained convex optimization problem (whose time complexity was not quantified by the authors).", "startOffset": 326, "endOffset": 330}, {"referenceID": 15, "context": "Introduced by [16] and here abbreviated as GPA (graph Perceptron algorithm).", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Following [19], we run GPA on a spanning tree T of the original graph.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "This is because a careful computation of the Laplacian pseudoinverse of a n-node tree takes time \u0398(n+m +mD) where m is the number of training examples plus the number of test examples (labels to predict), and D is the tree diameter \u2014see the work of [19] for a proof of this fact.", "startOffset": 249, "endOffset": 253}, {"referenceID": 29, "context": "Introduced by [31] and here abbreviated as LABPROP.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": ", the paper of [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "[19] use the shortest path tree because it has a small diameter (at most twice the diameter of G).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "This is essentially the algorithm analyzed by [18], and we denote it by NWWTA (non-weighted WTA).", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "The USPS dataset9 with features normalized into [0, 2].", "startOffset": 48, "endOffset": 54}, {"referenceID": 20, "context": "It has been used by [21] and [23].", "startOffset": 20, "endOffset": 24}, {"referenceID": 22, "context": "It has been used by [21] and [23].", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "A second dataset from the work of [23].", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "It is a combination of three datasets: [12]\u2019s, [20]\u2019s, and [28]\u2019s.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "It is a combination of three datasets: [12]\u2019s, [20]\u2019s, and [28]\u2019s.", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "It is a combination of three datasets: [12]\u2019s, [20]\u2019s, and [28]\u2019s.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Following previous experimental settings [31, 2], the graphs were constructed using k-NN based on the standard Euclidean distance \u2016xi \u2212 xj\u2016 between node i and node j.", "startOffset": 41, "endOffset": 48}, {"referenceID": 1, "context": "Following previous experimental settings [31, 2], the graphs were constructed using k-NN based on the standard Euclidean distance \u2016xi \u2212 xj\u2016 between node i and node j.", "startOffset": 41, "endOffset": 48}, {"referenceID": 23, "context": "We selected the set of functional labels at depth one in the FunCat classification scheme of the MIPS database [25], resulting in seventeen classes per dataset.", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "First, previous analyses [7] reveal that WTA\u2019s analysis is loose, at least when the input graph is an unweighted tree with small diameter.", "startOffset": 25, "endOffset": 28}, {"referenceID": 27, "context": "A partial answer to this question is provided by the recent work of [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "It would also be nice to tie this machinery with recent results in the active node classification setting on trees contained in [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 17, "context": "14 Item 2 in this lemma is essentially contained in the paper by [18].", "startOffset": 65, "endOffset": 69}], "year": 2012, "abstractText": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.", "creator": "LaTeX with hyperref package"}}}