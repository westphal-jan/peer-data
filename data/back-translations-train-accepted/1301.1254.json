{"id": "1301.1254", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2013", "title": "Dynamical Models and tracking regret in online convex programming", "abstract": "This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with the comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed Dynamic Mirror Descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.", "histories": [["v1", "Mon, 7 Jan 2013 16:39:09 GMT  (1149kb,D)", "http://arxiv.org/abs/1301.1254v1", "To appear in ICML 2013"]], "COMMENTS": "To appear in ICML 2013", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["eric c hall", "rebecca willett"], "accepted": true, "id": "1301.1254"}, "pdf": {"name": "1301.1254.pdf", "metadata": {"source": "META", "title": "Dynamical Models and Tracking Regret in Online Convex Programming", "authors": ["Eric C. Hall", "Rebecca M. Willett"], "emails": ["ech11@duke.edu", "willett@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Problem formulation", "text": "Let X denote the domain of our observations, and let \"repentance\" denote a convex practicable quantity. In the face of sequentially incoming observations, \"regret\" x \"X\" we want to construct a sequence of predictions. \"We present our problem as a dynamic game between a forecaster and the environment. In due course, the forecaster calculates a prediction,\" regret, \"and the environment generates the observation\" X. \"The forecaster then experiences the loss of\" t \"(.), as defined below. Let F and R denote families of convex functions, and let\" ft, \"f,\" xt, \"be a cost function that measures the accuracy of the prediction.\""}, {"heading": "3. Static, tracking, shifting, and adaptive regret", "text": "In this essay, we refer to regret in relation to a static comparator as static regret: definition 2 (static regret). The static regret of T isRT, T + T = 1 \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" \"t\" t \"t\" t \"t\" t \"t\" t \"\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t. \"t\" t \"t\" t \"t\" t \"t\" t \"t.\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t\" t \"t.\" t \"t\" t. \"t\" t \"t\" t. \"t.\" t \"t\" t \"t.\" t \"t\" t \"t\" t \"t\" \"\" t \"\" \"\" \"t\" t \"\" \"t\" \"\" \"\" \"\" \"\" \"\" \"t\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"t\" \"\" \"\" \"\" \"\" \"\" t \"\" \"\" \"\" \"\" \"\" \"\" \"\" t \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "4. Online convex optimization", "text": "A common approach to forecasting is to solve the following optimization problem:..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5. Dynamical models in online convex programming", "text": "In contrast to the bound in Theorem 3, Tracking or Shifting Repentance (Cesa-Bianchi & Lugosi, 2006; Cesa-Bianchi et al., 2012) boundaries are generally regarded as piecewise constant comparators, where there are sequences for much broader classes of dynamic comparators.In particular, we propose the following alternative to (2) and (4), which we call Dynamic Mirror Descent (DMD).Let us develop the required boundaries of repentance that sequences for much broader classes of dynamic comparators.In particular, we propose the following alternative to (2) and (4), which we call Dynamic Mirror Descent (DMD)."}, {"heading": "6. Prediction with a family of dynamical models", "text": "In practice, however, we do not know how to use the best dynamic model (1) (1) and describe a process that uses this collection to adapt to non-stationary environments. In particular, we adopt a finite series of dynamic models with different dynamic models at different time intervals. This class can be described as all predictors defined in m + 1 segments [ti + 1 \u2212 1] with periods of time. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "7. Experiments and results", "text": "To demonstrate the performance of the Dynamic Mirror Descent (DMD) in combination with the Fixed Share Algorithm (which we call Dynamic Fixed Share (DFS)), we consider two scenarios: reconstructing a dynamic scene (i.e. a video) from sequential compressed sensor observations and tracking connections in a dynamic social network."}, {"heading": "7.1. Compressive video reconstruction", "text": "To test DMD, we construct a video containing an object that moves in a 2-dimensional plane; the tth image is called a successt (a 150-150 image stored as a longitudinal 22500 vector), which values between 0 and 1. The corresponding observation is xt = At\u03b8t + nt, where At is a random 500-22500 matrix and nt corresponds to measurement noise. This model coincides with several compressed sensor architectures (Duarte et al., 2008). We used white Gaussian noise with variance 1. Our loss function uses ft (\u03b8) = 1 2: xt \u2212 At\u03b8 2: p \u00b7 p \u2212 p p p p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p p \u00b7 p p p \u00b7 p p p \u00b7 p \u00b7 p \u00b7 p p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p"}, {"heading": "7.2. Tracking dynamic social networks", "text": "However, we are not aware of its application in the context of online learning algorithms. (http: / / www.voteview.com / dwnl.htm) To show how DMD can bridge this gap, we trace the influence matrix of US Senate seats from 1795 to 2011 using voter data (http: / / www.voteview.com / dwnl.htm). (http: / / www.voteview.com / dwnl.htm) If a senator's vote is not available before a state joins the Union, we use a 0. (0) We form a length p = 100 vector of these votes indexed by the Senate, and denounce them xt.Following (Ravikumar et al., 2010), we form a loss function using a negative protocol issuing model associated with the dividing function of the issuing model."}, {"heading": "1859 1877 1887", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8. Conclusion and future directions", "text": "In this paper, we have proposed a novel online optimization method called Dynamic Mirror Descent (DMD), which includes dynamic model state updates. It is not believed that there is a \"true\" known underlying dynamic model or that the best dynamic model does not change over time. Furthermore, the proposed Dynamic Fixed Share (DFS) algorithm adaptively selects the most promising dynamic model from a range of candidates at each step. Recent work on shifting or tracking the boundaries of online convex optimization also suggests that the techniques developed in this paper may also be useful for limiting adaptive regret or developing methods for automatically adjusting step size parameters (Cesa-Bianchi et al., 2012). In experiments with real and simulated data, DMD exhibits strong tracking behavior even when underlying dynamic models change."}, {"heading": "9. Proofs", "text": "The evidence of Lemma 5: The optimum state of (5c) 4c (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (4c) (c) (c) (c) (c) (c) (c c) (c c c) (c c c) (c c c c c c) (c c c c c c c c c) (c c c c c) (c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}], "references": [{"title": "Compressed sensing of time-varying signals", "author": ["D. Angelosante", "G.B. Giannakis", "E. Grossi"], "venue": "In Intl Conf. on Dig. Sig. Proc.,", "citeRegEx": "Angelosante et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Angelosante et al\\.", "year": 2009}, {"title": "Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data", "author": ["O. Banerjee", "L. El Ghaoui", "A. d\u2019Aspremont"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Banerjee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2008}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex programming", "author": ["A. Beck", "M. Teboulle"], "venue": "Operations Research Letters,", "citeRegEx": "Beck and Teboulle,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2003}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput.,", "citeRegEx": "Belkin and Niyogi,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi", "year": 2003}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Prediction, Learning and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "A new look at shifting regret", "author": ["N. Cesa-Bianchi", "P. Gaillard", "G. Lugosi", "G. Stoltz"], "venue": null, "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Single pixel imaging via compressive sampling", "author": ["M.F. Duarte", "M.A. Davenport", "D. Takhar", "J.N. Laska", "T. Sun", "K.F. Kelly", "R.G. Baraniuk"], "venue": "IEEE Sig. Proc. Mag.,", "citeRegEx": "Duarte et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duarte et al\\.", "year": 2008}, {"title": "Composite objective mirror descent", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "In Conf. on Learning Theory (COLT),", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Efficient tracking of large classes of experts", "author": ["A. Gyorgy", "T. Linder", "G. Lugosi"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "Gyorgy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gyorgy et al\\.", "year": 2012}, {"title": "Efficient learning algorithms for changing environments", "author": ["E. Hazan", "C. Seshadhri"], "venue": "In Proc. Int. Conf on Machine Learning (ICML),", "citeRegEx": "Hazan and Seshadhri,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Seshadhri", "year": 2009}, {"title": "Tracking the best linear predictor", "author": ["M. Herbster", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Herbster and Warmuth,? \\Q2001\\E", "shortCiteRegEx": "Herbster and Warmuth", "year": 2001}, {"title": "Estimating time-varying networks", "author": ["M. Kolar", "L. Song", "A. Ahmed", "E.P. Xing"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Kolar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kolar et al\\.", "year": 2010}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Inf. Comput.,", "citeRegEx": "Littlestone and Warmuth,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1994}, {"title": "A unified view of regularized dual averaging and mirror descent with implicit updates", "author": ["B. McMahan"], "venue": null, "citeRegEx": "McMahan,? \\Q2011\\E", "shortCiteRegEx": "McMahan", "year": 2011}, {"title": "Universal prediction", "author": ["N. Merhav", "M. Feder"], "venue": "IEEE Trans. Info. Th.,", "citeRegEx": "Merhav and Feder,? \\Q1998\\E", "shortCiteRegEx": "Merhav and Feder", "year": 1998}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A.S. Nemirovsky", "D.B. Yudin"], "venue": null, "citeRegEx": "Nemirovsky and Yudin,? \\Q1983\\E", "shortCiteRegEx": "Nemirovsky and Yudin", "year": 1983}, {"title": "Online learning with predictable sequences", "author": ["A. Rakhlin", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin and Sridharan,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin and Sridharan", "year": 2012}, {"title": "High-dimenstional Ising model selection using `1regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Annals of Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "The statistical evaluation of social network dynamics", "author": ["T.A.B. Snijders"], "venue": "Sociological Methodology,", "citeRegEx": "Snijders,? \\Q2001\\E", "shortCiteRegEx": "Snijders", "year": 2001}, {"title": "Robust discrete-time minimum-variance filtering", "author": ["Y. Theodor", "U. Shaked"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "Theodor and Shaked,? \\Q1996\\E", "shortCiteRegEx": "Theodor and Shaked", "year": 1996}, {"title": "Modified-CS: Modifying compressive sensing for problems with partially known support", "author": ["N. Vaswani", "W. Lu"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "Vaswani and Lu,? \\Q2010\\E", "shortCiteRegEx": "Vaswani and Lu", "year": 2010}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Xiao,? \\Q2010\\E", "shortCiteRegEx": "Xiao", "year": 2010}, {"title": "Robust Kalman filtering for uncertain discrete-time systems", "author": ["L. Xie", "Y.C. Soh", "C.E. de Souza"], "venue": "IEEE Trans. Autom. Control,", "citeRegEx": "Xie et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Xie et al\\.", "year": 1994}, {"title": "Online convex programming and generalized infinitesimal gradient descent", "author": ["M. Zinkevich"], "venue": "In Proc. Int. Conf. on Machine Learning (ICML), pp", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 24, "context": "Some techniques have been proposed to learn the dynamics (Xie et al., 1994; Theodor & Shaked, 1996), but the underlying model still places heavy restrictions on the nature of the data.", "startOffset": 57, "endOffset": 99}, {"referenceID": 25, "context": "In particular, online convex programming methods (Nemirovsky & Yudin, 1983; Beck & Teboulle, 2003; Zinkevich, 2003; Cesa-Bianchi & Lugosi, 2006) rely on the gradient of the instantaneous loss of a predictor to update the prediction for the next data point.", "startOffset": 49, "endOffset": 144}, {"referenceID": 1, "context": "The role of regularization, particularly sparsity regularization, is increasingly well understood in batch settings and has resulted in significant gains in ill-posed and data-starved settings (Banerjee et al., 2008; Ravikumar et al., 2010; Cand\u00e8s et al., 2006; Belkin & Niyogi, 2003).", "startOffset": 193, "endOffset": 284}, {"referenceID": 19, "context": "The role of regularization, particularly sparsity regularization, is increasingly well understood in batch settings and has resulted in significant gains in ill-posed and data-starved settings (Banerjee et al., 2008; Ravikumar et al., 2010; Cand\u00e8s et al., 2006; Belkin & Niyogi, 2003).", "startOffset": 193, "endOffset": 284}, {"referenceID": 4, "context": "The role of regularization, particularly sparsity regularization, is increasingly well understood in batch settings and has resulted in significant gains in ill-posed and data-starved settings (Banerjee et al., 2008; Ravikumar et al., 2010; Cand\u00e8s et al., 2006; Belkin & Niyogi, 2003).", "startOffset": 193, "endOffset": 284}, {"referenceID": 0, "context": "There has been significant recent interest in using models of temporal structure to improve time series estimation from compressed sensing observations (Angelosante et al., 2009; Vaswani & Lu, 2010) or for time-varying networks (Snijders, 2001; Kolar et al.", "startOffset": 152, "endOffset": 198}, {"referenceID": 20, "context": ", 2009; Vaswani & Lu, 2010) or for time-varying networks (Snijders, 2001; Kolar et al., 2010); the associated algorithms, however, are typically batch methods poorly suited to large quantities of streaming data.", "startOffset": 57, "endOffset": 93}, {"referenceID": 12, "context": ", 2009; Vaswani & Lu, 2010) or for time-varying networks (Snijders, 2001; Kolar et al., 2010); the associated algorithms, however, are typically batch methods poorly suited to large quantities of streaming data.", "startOffset": 57, "endOffset": 93}, {"referenceID": 6, "context": "Performance relative to a temporally-varying or dynamic comparator sequence has been studied previously in the literature in the context of tracking regret, shifting regret (Herbster & Warmuth, 2001; Cesa-Bianchi et al., 2012), and the closely-related concept of adaptive regret (Littlestone & Warmuth, 1994; Hazan & Seshadhri, 2009).", "startOffset": 173, "endOffset": 226}, {"referenceID": 6, "context": "The relationship between the two has been formally shown (Cesa-Bianchi et al., 2012).", "startOffset": 57, "endOffset": 84}, {"referenceID": 25, "context": "The MD approach is a generalization of online learning algorithms such as online gradient descent (Zinkevich, 2003) and weighted majority (Littlestone & Warmuth, 1994).", "startOffset": 98, "endOffset": 115}, {"referenceID": 8, "context": "Several recently proposed methods consider the data-fit term separately from the regularization term (Duchi et al., 2010; Xiao, 2010; Langford et al., 2009).", "startOffset": 101, "endOffset": 156}, {"referenceID": 23, "context": "Several recently proposed methods consider the data-fit term separately from the regularization term (Duchi et al., 2010; Xiao, 2010; Langford et al., 2009).", "startOffset": 101, "endOffset": 156}, {"referenceID": 13, "context": "Several recently proposed methods consider the data-fit term separately from the regularization term (Duchi et al., 2010; Xiao, 2010; Langford et al., 2009).", "startOffset": 101, "endOffset": 156}, {"referenceID": 8, "context": "For instance, consider Composite Objective Mirror Descent (COMD) (Duchi et al., 2010):", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "Theorem 3 (Static regret for COMID (Duchi et al., 2010)).", "startOffset": 35, "endOffset": 55}, {"referenceID": 6, "context": "Unlike the bound in Theorem 3, tracking or shifting regret (Cesa-Bianchi & Lugosi, 2006; Cesa-Bianchi et al., 2012) bounds typically consider piecewise constant comparators, where \u03b8t \u2212 \u03b8t\u22121 = 0 for all but m values of t, where m is a constant, or yield regret bounds which scale with \u2211 t \u2016\u03b8t \u2212 \u03b8t\u22121\u2016.", "startOffset": 59, "endOffset": 115}, {"referenceID": 23, "context": "Rather than considering COMID, we might have used other online optimization algorithms, such as the Regularized Dual Averaging (RDA) method (Xiao, 2010), which has been shown to achieve similar performance with more regularized solutions.", "startOffset": 140, "endOffset": 152}, {"referenceID": 15, "context": "Recent results on the equivalence of COMID and RDA (McMahan, 2011) suggest that the bounds derived here might also hold for a variant of RDA, but proving this remains an open problem.", "startOffset": 51, "endOffset": 66}, {"referenceID": 6, "context": "Note that when \u03a6t corresponds to an identity operator, the bound in Theorem 4 corresponds to existing tracking or shifting regret bounds (Cesa-Bianchi & Lugosi, 2006; Cesa-Bianchi et al., 2012).", "startOffset": 137, "endOffset": 193}, {"referenceID": 25, "context": "This proof shares some ideas with the tracking regret bounds of (Zinkevich, 2003), but uses properties of the Bregman Divergence to eliminate some terms, while additionally incorporating dynamics.", "startOffset": 64, "endOffset": 81}, {"referenceID": 9, "context": "Other algorithms can accommodate larger classes of experts, or not assume knowledge of the number of switches, but come at the price of higher regret or complexity as explained in (Gyorgy et al., 2012).", "startOffset": 180, "endOffset": 201}, {"referenceID": 7, "context": "This model coincides with several compressed sensing architectures (Duarte et al., 2008).", "startOffset": 67, "endOffset": 88}, {"referenceID": 20, "context": "Dynamical models have a rich history in the context of social network analysis (Snijders, 2001), but we are unaware of their application in the context of online learning algorithms.", "startOffset": 79, "endOffset": 95}, {"referenceID": 19, "context": "Following (Ravikumar et al., 2010), we form a loss function using a negative log Ising model pseudolikelihood to sidestep challenging issues associated with the partition function of the Ising model likelihood.", "startOffset": 10, "endOffset": 34}, {"referenceID": 20, "context": "We set \u03c8(\u03b8) = 1 2\u2016\u03b8\u2016 2 2 and use a dynamical model inspired by (Snijders, 2001), where if |\u03b8ac\u2217\u03b8bc\u2217 | > |\u03b8ab|, with c\u2217 =arg maxc |\u03b8ac\u03b8bc|, then:", "startOffset": 63, "endOffset": 79}, {"referenceID": 13, "context": "As in (Langford et al., 2009), we find that regularizing (e.", "startOffset": 6, "endOffset": 29}], "year": 2013, "abstractText": "This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with the comparator\u2019s deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed Dynamic Mirror Descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.", "creator": "LaTeX with hyperref package"}}}