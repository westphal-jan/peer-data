{"id": "1301.1936", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2013", "title": "Risk-Aversion in Multi-armed Bandits", "abstract": "Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.", "histories": [["v1", "Wed, 9 Jan 2013 18:02:54 GMT  (429kb,D)", "http://arxiv.org/abs/1301.1936v1", "(2012)"]], "COMMENTS": "(2012)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amir sani", "alessandro lazaric", "r\u00e9mi munos"], "accepted": true, "id": "1301.1936"}, "pdf": {"name": "1301.1936.pdf", "metadata": {"source": "CRF", "title": "Risk\u2013Aversion in Multi\u2013armed Bandits", "authors": ["Amir Sani", "Alessandro Lazaric", "R\u00e9mi Munos"], "emails": ["amir.sani@inria.fr", "alessandro.lazaric@inria.fr", "remi.munos@inria.fr"], "sections": [{"heading": null, "text": "Stochastic, multi-armed bandits solve the dilemma between exploration and exploitation and ultimately maximize the expected reward. However, maximizing the expected reward is not the most desirable goal for many practical problems. In this paper, we present a novel setting based on the principle of risk aversion, with the goal of competing against the arm with the best risk-return balance. This setting proves to be inherently more difficult than the standard setting for multi-armed bandits, partly due to an exploration risk associated with the variability of an algorithm. Starting from variance as a measure of risk, we introduce two new algorithms, examine their theoretical guarantees and report preliminary empirical results."}, {"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Mean\u2013Variance Multi\u2013arm Bandit", "text": "In this section, we will present the main notation used throughout the work and define the mean variance of multi-arm bandit problems. We will look at the standard multi-arm bandit definition using the terms \"risk\" and \"risk.\" We will look at the standard multi-arm bandit definition using the terms \"risk\" and \"risk.\" We will define the bandit problem over a finite horizon of n rounds. We will specify the number of samples to be drawn from the distribution of arm i. All arms and samples are independent. In the multi-arm bandit protocols, an algorithm is defined in each round that selects arms. He and observes sample XIt, Ti, t, where Ti, t is the number of samples observed from arm i to time t (i.e., Ti, t = 1).While in the standard literature on multi-arm bandits, the goal is to select the highest reward in expectation (the biggest problem we expect)."}, {"heading": "3 The Mean\u2013Variance Lower Confidence Bound Algorithm", "text": "In this section, we present a novel risk-averse bandit algorithm, the aim of which is to identify the arm that best balances risk and return. It is a natural extension of UCB1 [6] and we report on a theoretical performance analysis that balances the exploration required to identify the best arm against the risk of pulling weapons by various means."}, {"heading": "3.1 The Algorithm", "text": "We propose an index-based bandit algorithm that estimates the mean variance of each arm and selects the optimal arm according to the optimistic confidence limits of the current estimates. A sketch of the algorithm is shown in Figure 1. For each arm, the algorithm tracks the empirical mean variance M-Vi, s calculated from s samples. We can calculate highly probable confidence limits on empirical mean variance by applying the Chernoff-Hoeffding inequality (see e.g. [1] for the bound mean variance M-Vi, s calculated from s samples)."}, {"heading": "3.2 Theoretical Analysis", "text": "In this section we report on the analysis of regret Rn (A) of the MV-LCB (Fig. 1). As emphasized in eq. 7, it is sufficient to analyze the number of interviews for each of the arms to avoid regret. The evidence (reported in the appendix) is largely based on similar arguments to prove UCB.We subtract the following regret, which is bound in high probability and expectation. Theorem 1. Leave the optimal arm i + 2 (5 + 2), the MV-LCB algorithm achieves a pseudo-regret. We subtract the following regret, which is bound in high probability and expectation. (A) \u2264 b2 log 1 / 2 arms (1 = i). The optimal arm i-LCB is unique and b = 2 (5 + 2), the MV-LCB algorithm achieves a pseudo-regret."}, {"heading": "4 The Exploration\u2013Exploitation Algorithm", "text": "The ExpExp algorithm divides the time horizon n into two different phases of length \u03c4 and n \u2212 \u03c4 = previous regret. During the first phase, all arms are uniformly examined, thereby collecting the sampling 7 points each. Once the exploration phase is completed, the mean variance of each arm is calculated and the arm with the least estimated mean variance MVi, \u03c4 / K is repeatedly drawn to the end. The MV-LCB is specifically designed to minimize the likelihood of pulling the wrong arms, so that whenever there are two equivalent arms (i.e. arms with the same mean variance), the algorithm tends to pull them the same number of times, at the expense of potentially introducing an additional variance that could lead to a permanent regret. On the other hand, ExpExp stops exploring the arms after the rounds and then triggers an optimal one and pulls them for the remaining n circles."}, {"heading": "5 Numerical Simulations", "text": "In this section, we report on numerical simulations aimed at confirming the most important theoretical results of the previous sections. In the following graphs, we examine the true regret Rn (A) on average over 500 runs. First, we consider the variance minimization problem (\u03c1 = 0) with K = 2 Gaussian 7In the definition and in the following analysis, we ignore the rounding effects. In Figure 2, we report the true regret Rn (as in the original definition in eq. 4) and its two components Rn = 0.5, \u03c3n = 0.05 and \u03c3 2 = 0.25 and perform MV-LCB 8. As expected (see e.g. Theorem 1), the regret Rn (as in the original definition in eq. 4) and its two components Rig."}, {"heading": "6 Discussion", "text": "Although this idea may be similar to other analyses of UCB-based algorithms (see e.g. the high probability analysis in [5]), it captures different features of the learning algorithm. When a bandit algorithm is executed over n rounds, its behavior, combined with the distributions of the arms, generates a probability distribution over n reward sequences. While the quality of this sequence is usually defined by its cumulative sum (or average), we say here that a sequence of rewards is good if it has a good compromise between its (empirical) mean and the variance. It is important to note that this notion of risk-return compromise does not coincide with the variance of the algorithm over several rounds (or the average). Let's consider a simple case of two arms intentionally generating a sequence and two different algorithms."}, {"heading": "7 Conclusions", "text": "The majority of the multi-armed bandit literature focuses on the problem of minimizing regret w.r.t. the arm with the highest return in expectation. We examine the concept of risk associated with variance over multiple runs and risks. Later, the case underscores an interesting effect on regret due to the need to estimate variability within a single sequence of finite random samples before making a risk-averse decision. Furthermore, controlling variance risk over multiple runs is not necessarily the control of variability risk over a single run. In this paper, we have introduced a novel multi-armed bandit setting with the goal of performing as good as the arm with the best return trade-off. In particular, we rely on the mean variance model introduced in [10] to measure the performance of weapons and define the regret of a learning algorithm. We have proposed two novel theoretical algorithms to solve the mean value problem and to report their respective mean value."}, {"heading": "A The Regret", "text": "In view of the definitions reported in the main paper, we will first deal with the two intermediate terms in the two intermediate terms (A) = 1nK = 1Ti, n = 1nK = 1Ti, n = 1nK (1Xi, n = 1Ti, n = 1Ti, n = 1Ti, n = 1Ti, n = 1Ti, t = 1nK (1Ti), t = 1Ti, n: 1Ti, n: 1Ti, n: 1Ti, n: 1Ti, n: 1Ti, n: 1Ti, n: 1Ti, n: 1K: 1Ti, n: 1K, n: 1Ti, n: 1K, Ti: 1K, 1K: n: 1K, 1Ti: 1i 1i, 1K: 1K, 1K: n: 1K, 1K: n: n, 1K: n: 1K, 1Ti: 1K, 1K: 1K, 1K: 1K: 1K, 1K: 1K: 1Ti 1i 1i 1Ti 1Ti 1Ti 1Ti, n: n: n: 1K, n: n: 1K, n: n: 1K, n: n: 1K, n: 1K: 1K, 1K: n: 1Ti: 1Ti, 1Ti: 1Ti: 1Ti 1Ti, n: 1Ti: 1Ti, n: 1Ti: 1Ti, n: 1Ti: 1Ti, n: 1Ti: 1Ti, n: n: 1Ti, n: n: 1K, n: 1K: 1K, n: 1K, n: 1K, n: 1K: 1K, n: 1K: 1K, n: 1K: 1K, 1K: 1K: 1K, 1K, 1K: 1K: 1K, 1Ti: 1Ti, 1Ti, 1Ti 1Ti, 1Ti, 1Ti, 1Ti, 1Ti, 1Ti: 1Ti, 1Ti, 1Ti,"}, {"heading": "C Exp\u2013Exp Theoretical Analysis", "text": "During the recovery phase, the algorithm pulls the arm i = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = K = p (K = 2 = p)."}, {"heading": "D Additional Simulations", "text": "D.1 Comparison between MV-LCB and ExpExp with K = 2We look at the variance minimization problem (\u03c1 = 0) with K = 2 Gaussian arms with different means and deviations. In particular, we look at a grid of values with 0,0; 1,1] and number of rounds n [50; 2,5 \u00d7 105]. Figures 3 and 4 report the mean regret for different values of n. Colors are renormalized in each diagram so that dark blue corresponds to the least regret and the greatest regret. Results confirm the theoretical results of theorem 1 and 2. Indeed, for simple problems (large maps) MV-LCB converts to zero regret."}], "references": [{"title": "Active learning in heteroscedastic noise", "author": ["Andr\u00e1s Antos", "Varun Grover", "Csaba Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Coherent measures of risk", "author": ["P Artzner", "F Delbaen", "JM Eber", "D Heath"], "venue": "Mathematical finance,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Best arm identification in multiarmed bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "R\u00e9mi Munos"], "venue": "In Proceedings of the Twenty-third Conference on Learning Theory (COLT\u201910),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Exploration-exploitation trade-off using variance estimates in multi-armed bandits", "author": ["Jean-Yves Audibert", "R\u00e9mi Munos", "Csaba Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1902}, {"title": "Finite-time analysis of the multi-armed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Large deviations bounds for estimating conditional value-at-risk", "author": ["David B. Brown"], "venue": "Operations Research Letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Risk-sensitive online learning", "author": ["Eyal Even-Dar", "Michael Kearns", "Jennifer Wortman"], "venue": "In Proceedings of the 17th international conference on Algorithmic Learning Theory (ALT\u201906),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "The Economics of Risk and Time", "author": ["Christian Gollier"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Portfolio selection", "author": ["Harry Markowitz"], "venue": "The Journal of Finance,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1952}, {"title": "The tight constant in the dvoretzky-kiefer-wolfowitz inequality", "author": ["Pascal Massart"], "venue": "The Annals of Probability,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Theory of games and economic behavior", "author": ["J Neumann", "O Morgenstern"], "venue": "Princeton University,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1947}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1952}, {"title": "Deviations of stochastic bandit regret", "author": ["Antoine Salomon", "Jean-Yves Audibert"], "venue": "In Proceedings of the 22nd international conference on Algorithmic learning theory (ALT\u201911),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction The multi\u2013armed bandit [13] elegantly formalizes the problem of on\u2013line learning with partial feedback, which encompasses a large number of real\u2013world applications, such as clinical trials, online advertisements, adaptive routing, and cognitive radio.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Two foundational risk modeling paradigms are Expected Utility theory [12] and the historically popular and accessible Mean-Variance paradigm [10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Two foundational risk modeling paradigms are Expected Utility theory [12] and the historically popular and accessible Mean-Variance paradigm [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": ", [9] for an introduction to risk from an expected utility theory perspective).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "In particular, [8] showed that in general, although it is possible to achieve a small regret w.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "In the multi\u2013arm bandit domain, the most interesting results are by [5] and [14].", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "In the multi\u2013arm bandit domain, the most interesting results are by [5] and [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "[5] introduced an analysis of the expected regret and its distribution, revealing that an anytime version of UCB [6] and UCB-V might have large regret with some nonnegligible probability.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5] introduced an analysis of the expected regret and its distribution, revealing that an anytime version of UCB [6] and UCB-V might have large regret with some nonnegligible probability.", "startOffset": 113, "endOffset": 116}, {"referenceID": 13, "context": "1 This analysis is further extended by [14] who derived negative results which show no anytime algorithm can achieve a regret with both a small expected regret and exponential tails.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "In particular, we refer to the first and most popular measure of risk\u2013return, the mean\u2013variance model introduce by [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "We consider the standard multi\u2013arm bandit setting withK arms, each characterized by a distribution \u03bdi bounded in the interval [0, 1].", "startOffset": 126, "endOffset": 132}, {"referenceID": 9, "context": "Although a large number of models for risk\u2013return trade\u2013off have been proposed, here we focus on the most historically popular and simple model: the mean\u2013variance model proposed by [10],2 where the return of an arm is measured by the expected reward and its risk by its variance.", "startOffset": 181, "endOffset": 185}, {"referenceID": 4, "context": "Although the analysis is mostly directed to the pseudo\u2013regret, as commented in Remark 2 at page 23 of [5], it can be extended to the true regret.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": ", [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "The algorithm is a natural extension of UCB1 [6] and we report a theoretical performance analysis on how well it balances the exploration needed to identify the best arm versus the risk of pulling arms with different means.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": ", [1] for the bound on the variance) on terms \u03bc\u0302 and \u03c3\u0302.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "random variables bounded in [0, 1] from the distribution \u03bdi with mean \u03bci and variance \u03c3 i , and the empirical mean \u03bc\u0302i,s and variance \u03c3\u0302 2 i,s computed as in Equation 1, then P [ \u2203i = 1, .", "startOffset": 28, "endOffset": 34}, {"referenceID": 2, "context": "In fact, in the latter case, UCB is known to have a worst\u2013case regret per round of \u03a9(1/ \u221a n) [3], while in the worst case, MV-LCB suffers a constant regret.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": ", [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", the high-probability analysis in [5]), it captures different features of the learning algorithm.", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "In particular, we relied on the mean\u2013variance model introduced in [10] to measure the performance of the arms and define the regret of a learning algorithm.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "On the other hand, we proved that ExpExp has a vanishing Notice that although in the paper we assumed the distributions to be bounded in [0, 1] all the results can be extended to sub-Gaussian distributions.", "startOffset": 137, "endOffset": 143}, {"referenceID": 1, "context": "It also violates the monotonocity condition due to the different orders of the mean and variance and is not a coherent measure of risk [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": ", [11]), estimating the quantile might be more difficult.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "In [2] axiomatic rules are listed to define coherent measures of risk.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "One can easily imagine a lower confidence bound algorithm based on [7] in the same composition as MVLCB which replaces the variance by the conditional value at risk.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "References [1] Andr\u00e1s Antos, Varun Grover, and Csaba Szepesv\u00e1ri.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] P Artzner, F Delbaen, JM Eber, and D Heath.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Jean-Yves Audibert and S\u00e9bastien Bubeck.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Jean-Yves Audibert, S\u00e9bastien Bubeck, and R\u00e9mi Munos.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jean-Yves Audibert, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter Auer, Nicol\u00f2 Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] David B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Eyal Even-Dar, Michael Kearns, and Jennifer Wortman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Christian Gollier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Harry Markowitz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Pascal Massart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] J Neumann and O Morgenstern.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Herbert Robbins.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Antoine Salomon and Jean-Yves Audibert.", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "Stochastic multi\u2013armed bandits solve the Exploration\u2013Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk\u2013aversion where the objective is to compete against the arm with the best risk\u2013return trade\u2013off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results.", "creator": "LaTeX with hyperref package"}}}