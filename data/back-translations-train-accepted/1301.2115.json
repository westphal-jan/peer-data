{"id": "1301.2115", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Domain Generalization via Invariant Feature Representation", "abstract": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.", "histories": [["v1", "Thu, 10 Jan 2013 13:29:17 GMT  (271kb)", "http://arxiv.org/abs/1301.2115v1", "The 30th International Conference on Machine Learning (ICML 2013)"]], "COMMENTS": "The 30th International Conference on Machine Learning (ICML 2013)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["krikamol muandet", "david balduzzi", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1301.2115"}, "pdf": {"name": "1301.2115.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["krikamol@tuebingen.mpg.de,", "david.balduzzi@inf.ethz.ch,", "bs@tuebingen.mpg.de."], "sections": [{"heading": null, "text": "ar Xiv: 130 1.21 15v1 [st at.M L] 10 Jan 2013Learning theoretical analysis shows that reducing inequality improves the expected generalization capability of classifiers on new domains and motivates the proposed algorithm. Experimental results on synthetic and real datasets show that DICA successfully learns invariant traits and improves classification performance in practice."}, {"heading": "1. Introduction", "text": "To illustrate the problem, an example from Blanchard et al. (2011), which investigated the automatic collection of flow cytometry data, must be used to diagnose the health of patients, a set of ni-cells is necessary to diagnose the health of patients. However, the manual collection is very time-consuming. To automate the collection, we need to perform a classification of previously invisible patients in which the distribution of cell types differs drastically from the training data. Unfortunately, we cannot apply standardized machine learning techniques because the data violate the basic assumption that the training data differs from the distribution of cell types."}, {"heading": "2. Domain-Invariant Component Analysis", "text": "X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-"}, {"heading": "3. Experiments", "text": "We illustrate the difference between the proposed algorithms and their singledomain counterparts."}, {"heading": "4. Conclusion and Discussion", "text": "Finally, we proposed a simple algorithm called Domain-Invariant Component Analysis (DICA), which has proven to be important both theoretically and empirically for domain generalization. Theorem 5 shows that the generalization error grows with the distribution variance on previously invisible domains. We also showed that DICA generalizes KPCA and COIR and is closely related to TCA. Finally, experimental results on both synthetic and real data sets show that DICA performs well in practice. Interestingly, the results also indicate that the distribution-driven SVM, which takes cross-domain variation into account, exceeds the pooling SVM, which ignores it. The motivating assumption in this work is that the functional relationship is stable or that the variations are smooth across domains."}, {"heading": "Appendix A. Domain Generalization and Related Frameworks", "text": "The most basic assumption in machine learning is that the observations are independent and equally distributed (i.i.d.), meaning that each observation comes from the same probability distribution as the others, and all are independent of each other. However, this assumption is often violated in practice, with standard machine learning algorithms not working well. In recent decades, many techniques have been proposed to address scenarios where there is a discrepancy between training and test distributions, including domain adaptation (Bickel et al. 2009a), multitask learning (Caruana 1997), transfer learning (Pan and Yang 2010b), covariate / dataset shifting (Quionero-Candela et al. 2009), and concept drift (Widmer et al."}, {"heading": "Appendix B. Proof of Theorem 1", "text": "Given is a series of distributions P = {P1, P2., PN} = < P2., PN}, the distribution variance of P is VH (P) = 1 N, N = 1 N, P = 2 H, where the probability distribution is as (1 / N), Ni = 1 P i = (1 / N), N i = (1 / N), Ni = 1 P i, i.e., P (x), Ni = (1 / N), Ni (1 / N), Ni (1 / N), Ni (1 / P), Pi."}, {"heading": "Appendix C. Proof of Theorem 2", "text": "The empirical estimator V-H (S) = 1 N tr (S) = tr (KQ) = 1 x (S) is a consistent estimator of VH (P).Proof.Proof.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph.Prophy.Prophy.Prophy.Prophy.Prophy.Prophy.Proph"}, {"heading": "Appendix F. Proof of Theorem 5", "text": "We will consider a scenario in which distributions Pi are drawn according to the number of distributions, with the probability that the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions in the number of distributions."}, {"heading": "Appendix G. Leave-one-out accuracy", "text": "Figure 4 shows the leave-one-out accuracy of different approaches evaluated for each topic in the dataset. Average leave-one-out accuracies are given in Table 2. Distribution SVM outperforms pooling SVM in this setting, possibly due to the relatively large number of training participants, i.e. 29 subjects. Using the invariant features learned from DICA also yields higher accuracies than other approaches."}], "references": [{"title": "Unifying divergence minimization and statistical inference via convex duality", "author": ["Y. Altun", "A. Smola"], "venue": "In Proc. of Conf. on Learning Theory (COLT),", "citeRegEx": "Altun and Smola.,? \\Q2006\\E", "shortCiteRegEx": "Altun and Smola.", "year": 2006}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ben.David et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "T.C. Agnan"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Berlinet and Agnan.,? \\Q2004\\E", "shortCiteRegEx": "Berlinet and Agnan.", "year": 2004}, {"title": "Discriminative learning under covariate shift", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "Journal of Machine Learning Research, 10:2137\u20132155,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Discriminative learning under covariate shift", "author": ["S. Bickel", "M. Br\u00fcckner", "T. Scheffer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Generalizing from several related classification tasks to a new unlabeled sample", "author": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Blanchard et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2011}, {"title": "High-content flow cytometry and temporal data analysis for defining a cellular signature of graft-versus-host disease", "author": ["R.R. Brinkman", "M. Gasparetto", "S.-J.J. Lee", "A.J. Ribickas", "J. Perkins", "W. Janssen", "R. Smiley", "C. Smith"], "venue": "Biol Blood Marrow Transplant,", "citeRegEx": "Brinkman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brinkman et al\\.", "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Universal kernels on Non-Standard input spaces", "author": ["A. Christmann", "I. Steinwart"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Christmann and Steinwart.,? \\Q2010\\E", "shortCiteRegEx": "Christmann and Steinwart.", "year": 2010}, {"title": "Kernel Dimensionality Reduction for Supervised Learning", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "andM.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Dataset Shift in Machine Learning, chapter Covariate Shift by Kernel Mean Matching, pages 131\u2013160", "author": ["A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "Learning the shared subspace for multi-task clustering and transductive transfer classification", "author": ["Q. Gu", "J. Zhou"], "venue": "In Proceedings of the 9th IEEE International Conference on Data Mining,", "citeRegEx": "Gu and Zhou.,? \\Q2009\\E", "shortCiteRegEx": "Gu and Zhou.", "year": 2009}, {"title": "Central subspace dimensionality reduction using covariance operators", "author": ["M. Kim", "V. Pavlovic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kim and Pavlovic.,? \\Q2011\\E", "shortCiteRegEx": "Kim and Pavlovic.", "year": 2011}, {"title": "Sliced inverse regression for dimension reduction", "author": ["K.-C. Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li.,? \\Q1991\\E", "shortCiteRegEx": "Li.", "year": 1991}, {"title": "Learning from distributions via support measure machines", "author": ["K. Muandet", "K. Fukumizu", "F. Dinuzzo", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Muandet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Muandet et al\\.", "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Flexible modeling of latent task structures in multitask learning", "author": ["A. Passos", "P. Rai", "J. Wainer", "H.D. III"], "venue": "In Proceedings of the 29th international conference on Machine learning,", "citeRegEx": "Passos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2012}, {"title": "Dataset Shift in Machine Learning", "author": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": null, "citeRegEx": "Quionero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quionero.Candela et al\\.", "year": 2009}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the 18th International Conference In Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B.K. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Sch\u00f6lkopf", "G.R.G. Lanckriet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Learning in the Presence of Concept Drift and Hidden Contexts", "author": ["G. Widmer", "M. Kurat"], "venue": "Machine Learning,", "citeRegEx": "Widmer and Kurat.,? \\Q1996\\E", "shortCiteRegEx": "Widmer and Kurat.", "year": 1996}, {"title": "Kernel sliced inverse regression with applications to classification", "author": ["H.-M. Wu"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Wu.,? \\Q2008\\E", "shortCiteRegEx": "Wu.", "year": 2008}, {"title": "2009a) and references therein). Domain adaptation, also known as covariate shift, deals primarily with a mismatch between training and test distributions. Domain generalization deals with a broader setting where training instances", "author": ["Bickel"], "venue": null, "citeRegEx": "Bickel,? \\Q2009\\E", "shortCiteRegEx": "Bickel", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "To illustrate the problem, consider an example taken from Blanchard et al. (2011) which studied automatic gating of flow cytometry data.", "startOffset": 58, "endOffset": 82}, {"referenceID": 16, "context": "A considerable effort has been made in domain adaptation and transfer learning to remedy this problem, see Pan and Yang (2010a), Ben-David et al.", "startOffset": 107, "endOffset": 128}, {"referenceID": 2, "context": "A considerable effort has been made in domain adaptation and transfer learning to remedy this problem, see Pan and Yang (2010a), Ben-David et al. (2010) and references therein.", "startOffset": 129, "endOffset": 153}, {"referenceID": 20, "context": "2004a), transfer component analysis (TCA) (Pan et al. 2011), and covariance operator inverse regression (COIR) (Kim and Pavlovic 2011), see \u00a72.", "startOffset": 42, "endOffset": 59}, {"referenceID": 14, "context": "Pan and Yang (2010a) and references therein).", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Recently, Blanchard et al. (2011) proposed an augmented SVM that incorporates empirical marginal distributions into the kernel.", "startOffset": 10, "endOffset": 34}, {"referenceID": 6, "context": "Recently, Blanchard et al. (2011) proposed an augmented SVM that incorporates empirical marginal distributions into the kernel. A detailed error analysis showed universal consistency of the approach. We apply methods from Blanchard et al. (2011) to derive theoretical guarantees on the finite sample performance of DICA.", "startOffset": 10, "endOffset": 246}, {"referenceID": 7, "context": "Using the samples S, our goal is to produce an estimate f : PX \u00d7 X \u2192 R that generalizes well to test samples S = {x (t) k } nt k=1 drawn according to some unknown distribution P \u2208 PX (Blanchard et al. 2011).", "startOffset": 183, "endOffset": 206}, {"referenceID": 25, "context": ", all the information about the distribution is preserved (Sriperumbudur et al. 2010).", "startOffset": 58, "endOffset": 85}, {"referenceID": 15, "context": "This nonlinear extension implies that E[X |Y ] lies on a function space spanned by {\u03a3xxck}k=1, which coincide with the eigenfunctions of the operator V(E[X |Y ]) (Wu 2008, Kim and Pavlovic 2011). Since we always work in H, we drop \u03c6 from the notation below. To avoid slicing the output space explicitly (Li 1991, Wu 2008), we exploit its kernel structure when estimating the covariance of the inverse regressor. The following result from Kim and Pavlovic (2011) states that, under a mild assumption, V(E[X |Y ]) can be expressed in terms of covariance operators: Theorem 4.", "startOffset": 172, "endOffset": 462}, {"referenceID": 23, "context": "If there is only a single distribution then unsupervised DICA reduces to KPCA since KQK = 0 and finding B requires solving the eigensystem KB = B\u0393 which recovers KPCA (Sch\u00f6lkopf et al. 1998).", "startOffset": 167, "endOffset": 190}, {"referenceID": 20, "context": "If there are two domains, source PS and target PT , then UDICA is closely related \u2013 though not identical to \u2013 Transfer Component Analysis (Pan et al. 2011).", "startOffset": 138, "endOffset": 155}, {"referenceID": 7, "context": "We adapt an approach to this problem developed in Blanchard et al. (2011) to prove a generalization bound that applies after transforming the empirical sample using B.", "startOffset": 50, "endOffset": 74}, {"referenceID": 8, "context": "The GvHD dataset (Brinkman et al. 2007) consists of weekly peripheral blood samples obtained from 31 patients following allogenic blood and marrow transplant.", "startOffset": 17, "endOffset": 39}, {"referenceID": 8, "context": "The goal of gating is to identify CD3CD4CD8\u03b2 cells, which were found to have a high correlation with the development of GvHD (Brinkman et al. 2007).", "startOffset": 125, "endOffset": 147}, {"referenceID": 7, "context": "The pooling SVM disregards the inter-patient variation by combining all datasets from different patients, whereas the distributional SVM also takes the inter-patient variation into account via the kernel function (Blanchard et al. 2011)", "startOffset": 213, "endOffset": 236}], "year": 2012, "abstractText": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.", "creator": "gnuplot 4.4 patchlevel 3"}}}