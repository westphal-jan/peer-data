{"id": "1301.2343", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Planning by Prioritized Sweeping with Small Backups", "abstract": "Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.", "histories": [["v1", "Thu, 10 Jan 2013 21:54:42 GMT  (36kb)", "http://arxiv.org/abs/1301.2343v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["harm van seijen", "richard s sutton"], "accepted": true, "id": "1301.2343"}, "pdf": {"name": "1301.2343.pdf", "metadata": {"source": "META", "title": "Planning by Prioritized Sweeping with Small Backups", "authors": ["Harm van Seijen"], "emails": ["harm.vanseijen@ualberta.ca", "sutton@cs.ualberta.ca"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.23 43v1 [cs.AI] 10 Jan 2013"}, {"heading": "1. Introduction", "text": "In Reinforcement Learning (RL) (Kaelbling et al., 1996; Sutton & Barto, 1998), an agent seeks an optimal tax policy for a sequential decision problem in an initially unknown environment.The environment provides feedback on the agent's behavior in the form of a reward signal.The agent's goal is to maximize the expected return, which is the discounted sum of rewards over future timeframeframes.An important performance metric in RL is sample efficiency, which refers to the number of environmental interactions required to obtain good policy.Many strategies improve policy by iteratively improving a state value or an action value function that provides estimates of the expected return for (environmental) states or pairs of state actions.Different approaches to updating these value functions exist. In terms of example efficiency, one of the most effective approaches is one of the environmental models that calculates the environmental model using the samples observed and the calculation of the sample."}, {"heading": "2. Reinforcement Learning Framework", "text": "RL problems are often formalized as Markov decision-making processes (MDPs), which can be described as tuples < S, A, P, R, \u03b3 > consisting of S, the set of all states; A, the set of all actions; Ps, sa = Pr (s, a), the transition probability of state s (S, S, State s) when an action A is taken; Rsa = E (r, a), the reward function that gives the expected reward r when an action is taken in state s; and \u03b3, the discount factor that controls the weight of future rewards relative to the immediate reward. Measures are selected at discrete intervals = 0, 2,... according to a political prediction: S \u00d7 A \u2192 [0, 1] which defines for each action an optimal probability tied to the state. Generally, the goal of RL policies is to improve policies to increase the cumulative return, which is G."}, {"heading": "3. Small Backup", "text": "This section introduces the small backup. We start with small state backups for the prediction task. Section 3.3 discusses small action value backups for the control task."}, {"heading": "3.1. Value Backups", "text": "In this section, we present a small backup version of the full backup prediction (Backup 2). In the introduction, we showed that a small backup requires the storage of the component values that make up the current value of a variable. In the case of a small backup, the component values correspond to the values of a single successor state. We specify these values using the function Us: S \u00b7 S \u2192 IR. The three backups shown in the theorem together form the small backup. Theorem 3.1 When the current ratio between V (s) and us is updated by V (s), V (s) = R (s) = R (s) = R (s) + P (s) s (s) s (s) s) s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s."}, {"heading": "3.2. Value Correction after Model Update", "text": "s S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S"}, {"heading": "12: Us\u0304(s\u0304", "text": "\u2032) \u2190 tmp 13: final loop 14: final loop"}, {"heading": "3.3. Action-value Backups", "text": "Before we can discuss backups with a small action value, we need to discuss a more efficient implementation of the full action value backup. Backup (4) has a computational complexity of O (| S | | A |). A more efficient implementation can be achieved by storing action values as well as statevalues, according to V (s) = maxa Q (s, a). Backup (4) can then be implemented by: Q (s, a) R (s) V (s) V (s) (14) V (s) p (s) \u2190 max a \u2032 Q (s, a). (15) The combined computation time of these backups is O (| S | | A |), a considerable reduction. Backup (14) is in its form similar to the forecast backup (s). Therefore, we can make a small backup version of it, similar to the backup version of Usa (s, Uxs)."}, {"heading": "3.4. Small Backups versus Sample Backups", "text": "A small backup has in common with an example backup that both update a state value based on the current value of only one of the succession states. Furthermore, they share the same computational complexity and their effect is generally smaller than that of a full backup.A disadvantage of an example backup compared to a small backup is that it introduces sampling variance caused by a stochastic environment.This requires the use of a step-by-step parameter to enable averaging of succession states (and rewards).A small backup does not introduce sampling variance because it is implicitly based on an expectation of succession states. Therefore, it does not require coordination of a step-by-step parameter for optimum performance.A second disadvantage of an example backup is that it affects the perceived distribution across the sample results, thereby imposing some limitations on the reuse of samples. For example, a model-free technique such as those repeated at a later point in time, as in 1992, may result in small experience effects that are too frequent to be repeated."}, {"heading": "4. Prioritized Sweeping with Small Backups", "text": "Prioritized sweep (PS) makes the model-based RL planning step more efficient by using heuristics (a \"priority\") to select backups that should cause a big change in value; it maintains a priority queue that determines what values are next for receiving backups; there are two main implementations: one by Moore & Atkeson (1993) and one by Peng & Williams (1993) 1. What all PS methods have in common is that they perform backups in so-called update cycles; by adjusting the number of update cycles performed per time step, the computation time per time step can be controlled; below we discuss in detail what happens in an update cycle for the two classic PS implementations.1 We refer to the version of queue dynasty for stochastic domains that differs from the version for deterministic domains."}, {"heading": "4.1. Classical Prioritized Sweeping Implementations", "text": "When implementing Moore & Atkeson, the elements in the queue are states, and the backups are full-fledged backups. In control, a full-fledged backup solution differs from Backup (2). Instead, it corresponds (in terms of effect and computational time) to performing Backup (14) for each action followed by Backup (15). Therefore, the associated computational time has complexity O (| S | | A | + | A |). An update cycle consists of the following steps: First, the top state is removed from the queue and receives a full-fledged backup; we bet that the top state and the change in value caused by the backup solution."}, {"heading": "4.2. Small Backup implementation", "text": "The pseudo-code of this implementation is represented in Algorithm 2. Below we will discuss some of the key features of the algorithm. However, the calculation time of a small backup is so short that it is comparable to the priority calculation in the classic PS implementations. So, instead of calculating a priority for each predecessor and making a backup for the highest priority element in the next update cycle, we can make a small backup for all predecessors, which raises the question of what should be included in the priority list and what kind of backup should be done for the top element. The natural answer is to put states in the priority queue and make backups (15) for the top status. The priority associated with a state is based on changing the action value and type of backup to be performed for the top element."}, {"heading": "5. Experimental Results", "text": "In this section we evaluate the performance of a minimum version of algorithm 1 and the performance of algorithm 2."}, {"heading": "5.1. Small backup versus Sample backup", "text": "We compare the performance of TD (0), which performs a sample per time step, with a version of Q (Q), which performs a small backup per time step. Specifically, the number of iterations (line 8) is 1, and the selected successor pair (line 9) is the pair that performs the most recent transition.Their performance is compared using two evaluation tasks, both consisting of 10 states arranged in a circle. Transition probabilities for both tasks are generated by a random process. Specifically, the transition probability to a neighboring state is generated by a random number between 0 and 1."}, {"heading": "5.2. Prioritized Sweeping", "text": "We compare the performance of prioritizing small backups (algorithm 2) with the two classic implementations of Moore & Atkeson and Peng & Williams on the labyrinth task shown at the top of Figure 2. The reward received in each step is -1 and the discount factor is 0.99. The agent can take four actions that move the agent in a different direction. At the bottom of Figure 2, the relative results of a \"North\" action are shown. In open space, one action can lead to 15 possible successor states, each with the same probability. If the agent is close to a wall, this number is reduced to the performance we have compared against a value rating."}, {"heading": "6. Discussion", "text": "Prioritized screening can be seen as a generalization of the idea of retrograde experience reproduction (Lin, 1992), which is inherently related to credentials (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994) What all these techniques have in common is that new information (which can be value changes, but essentially all value changes come from new data) is propagated backwards. While backward reproduction and credentials use the current path for the backward propagation of information, prioritized screening uses a model estimate to do so. Therefore, it propagates new information more broadly. What gives the small backup implementation the benefit is that it implements the principle of backward updating in a cleaner and more efficient way. An updating cycle of Algorithm 2 is, in a way, the ultimate backward backup: All predecessors are updated with the current value of a selected state."}, {"heading": "7. Conclusion", "text": "We have shown in this paper that the planning step in the model-based learning method of amplification can be done much more efficiently by using small backups. These backups are finer-grained versions of a full backup that allow more control over how the available computing time is used, enabling new, more efficient updating strategies. In addition, small backups can be useful in areas with very tight time constraints and provide a parameter-free alternative to sample backups that have often been the only viable option for such domains."}], "references": [{"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.P. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L.J. Lin"], "venue": "Machine learning,", "citeRegEx": "Lin,? \\Q1992\\E", "shortCiteRegEx": "Lin", "year": 1992}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less real time", "author": ["A. Moore", "C. Atkeson"], "venue": "Machine Learning,", "citeRegEx": "Moore and Atkeson,? \\Q1993\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1993}, {"title": "Efficient learning and planning within the dyna framework", "author": ["J. Peng", "R.J. Williams"], "venue": "Adaptive Behavior,", "citeRegEx": "Peng and Williams,? \\Q1993\\E", "shortCiteRegEx": "Peng and Williams", "year": 1993}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "On step-size and bias in temporal-difference learning", "author": ["R.S. Sutton", "S.P. Singh"], "venue": "In Proceedings of the Eight Yale Workshop on Adaptive and Learning Systems", "citeRegEx": "Sutton and Singh,? \\Q1994\\E", "shortCiteRegEx": "Sutton and Singh", "year": 1994}, {"title": "Learning from delayed rewards", "author": ["C. Watkins"], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "Watkins,? \\Q1989\\E", "shortCiteRegEx": "Watkins", "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "In reinforcement learning (RL) (Kaelbling et al., 1996; Sutton & Barto, 1998), an agent seeks an optimal control policy for a sequential decision problem in an initially unknown environment.", "startOffset": 31, "endOffset": 77}, {"referenceID": 4, "context": "A popular planning technique used for this is value iteration (VI) (Sutton, 1988; Watkins, 1989), which performs sweeps of backups through the state or state-action space, until the (action-)value function has converged.", "startOffset": 67, "endOffset": 96}, {"referenceID": 7, "context": "A popular planning technique used for this is value iteration (VI) (Sutton, 1988; Watkins, 1989), which performs sweeps of backups through the state or state-action space, until the (action-)value function has converged.", "startOffset": 67, "endOffset": 96}, {"referenceID": 1, "context": "For example, a model-free technique like experience replay (Lin, 1992), which stores experience samples in order to replay them at a later time, can introduce bias, which reduces performance, if some samples are replayed more often than others.", "startOffset": 59, "endOffset": 70}, {"referenceID": 1, "context": "Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility traces (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994).", "startOffset": 112, "endOffset": 123}, {"referenceID": 4, "context": "Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility traces (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994).", "startOffset": 174, "endOffset": 225}, {"referenceID": 7, "context": "Prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order (Lin, 1992), which by itself is related to eligibility traces (Sutton, 1988; Watkins, 1989; Sutton & Singh, 1994).", "startOffset": 174, "endOffset": 225}], "year": 2013, "abstractText": "Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.", "creator": "LaTeX with hyperref package"}}}