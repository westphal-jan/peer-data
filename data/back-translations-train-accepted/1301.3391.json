{"id": "1301.3391", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Feature grouping from spatially constrained multiplicative interaction", "abstract": "We present a feature learning model that learns to encode relationships between images. The model is defined as a Gated Boltzmann Machine, which is constrained such that hidden units that are nearby in space can gate each other's connections. We show how frequency/orientation \"columns\" as well as topographic filter maps follow naturally from training the model on image pairs. The model also helps explain why square-pooling models yield feature groups with similar grouping properties. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformation-learning model.", "histories": [["v1", "Tue, 15 Jan 2013 16:06:11 GMT  (769kb,D)", "https://arxiv.org/abs/1301.3391v1", "9 pages, 6 figures"], ["v2", "Wed, 16 Jan 2013 16:43:56 GMT  (1145kb,D)", "http://arxiv.org/abs/1301.3391v2", "9 pages, 6 figures"], ["v3", "Mon, 11 Mar 2013 15:38:05 GMT  (1209kb,D)", "http://arxiv.org/abs/1301.3391v3", "(new version:) added training formulae; added minor clarifications"]], "COMMENTS": "9 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["felix bauer", "roland memisevic"], "accepted": true, "id": "1301.3391"}, "pdf": {"name": "1301.3391.pdf", "metadata": {"source": "CRF", "title": "Feature grouping from spatially constrained multiplicative interaction", "authors": ["Felix Bauer", "Roland Memisevic"], "emails": ["fbauer@fias.uni-frankfurt.de", "memisevr@iro.umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2 Factored Gated Boltzmann Machine", "text": "There is an increasing interest in learning functions to encode relationships between multiple images, for example, to encode motion sequences (e.g.). In this work, we focus on the Gated Boltzmann Machine (GBM) [15, 22], which defines the relationship between two binary images x and y using the three-way energy functionE (x, y, h) = [1) The energy is exponentialized and normalized to define the probability of image pairs (we drop all bias terms to avoid ambiguity): p (x, y), h exp (x, y), z = x, y exp (E, y), h exp (E, y, h)))) (2) By adding appropriate penalty terms, one can expand the model to learn real-rated images."}, {"heading": "2.1 Phase usage in modeling transformations", "text": "Figure 2 (left, top) shows pairs of filters learned from translated random phases in which a Factored Gated Boltzmann Machine is used (for training details, see Section 4 below). Translation training turns into Fourier components, as originally observed by [16]. The bottom-left plot shows histograms of the occurrence of frequencies and orientations for input and output filters, which are generated by selecting the frequency and orientation of the strongest component for the filter. Histograms show that the learned filters cover the space of frequencies and orientations evenly."}, {"heading": "3 Group gating", "text": "The analysis in the previous section suggests using a richer connectivity that supports the reuse of filters. (In this case, the model can learn to compare all filters with multiple phase-shifted copies of itself rather than a single one.) All phase differences in Figure 2 (right) can then in principle be achieved with a much smaller number of filters. (One way to support the reuse of filters to represent multiple phase shifts is to loosen the diagonal factorization (Eq. 3) with a factorization that allows for richer connectivity: wijk = 1 def Cdefw x idw y y y y jew z kf (7), where we are Cdef the components of a (non-diagonal) core tensor C. Note: If C is diagonal, so that Cdef = 1 iff = e = f, we would restore PARAFAC factorization (Eq 3)."}, {"heading": "3.1 Significance for square-pooling models", "text": "Interestingly, the same analysis also applies to the reactions of complex cells in the energy model (e.g. [7, 19]) when images and filters are normalized. [14] In this case, the reaction of the energy model is the same as a factorized GBM mapping unit applied to a single image, i.e. x = y (see e.g. [19, 14]), which shows that the gating interpretation of frequency / orientation groups and topographic structure also applies to these models. Likewise, a square pooling model applied to a single image can be interpreted as encoding the relationship between, for example, the rows or columns within the patch. In natural images, the dominant transformation that takes one line to the next line is a local translation. Thus, the emergence of oriented gabor characteristics can also be considered as a result of modeling these local translations."}, {"heading": "4 Experiments", "text": "We compare the models in terms of transformation processes and transformation processes. We compare the models uniformly in terms of the relationship between four rotation and energy models. We compared the standard Boltzmann Gated Machine with a square pooling GBM [e.g. 19, 13] that is trained on the concatenation of images. We used the task to classify transformations from the mapping units by classifying transformations from image fields of size 13 x 13 pixels that are cut from larger images to ensure that no boundary artifacts are introduced. Training, validation and test data each contain 100,000 cases. We use logistical regression to classify transformations from mapping units, where we determine the optimal number of mapping units and factors, as well as the learning rates for the transformation models on the validation data. Random images ensure that individual images do not contain information about the transformation; in other words, a single image cannot be used to predict transformations."}, {"heading": "4.1 Topographic feature maps from local gating", "text": "A convenient approach is to arrange all characteristics (\"simple cells\") in a low-dimensional space and define groups over all those units that are located within some predefined neighborhoods, e.g. in a grid of n \u00d7 n units (e.g. [8]). Figure 6 shows the characteristics we learned from the van Hateren data (see Section 4), with a patch size of 16 \u00d7 16 and a variance of 99.9% that is maintained after swiping. We used 400 filters that we arranged in a 2-D grid (with wrap-around), with a group size of 5 \u00d7 5. We found that learning is simplified if we also initialize factor-to-mapping parameters so that each mapping unit has access to the filters of only one group."}, {"heading": "5 Conclusions", "text": "Energy mechanisms and \"square pooling\" are common approaches to modeling trait dependencies in sparse coding and to learning group-structured or invariant dictionaries. In this work, we have examined group-structured sparse codings from the perspective of image movements and differences from local multiplicative interactions. In addition, topographically structured representations (\"windmill\") can arise naturally as a result of binocular or spatio-temporal learning that uses spatially limited multiplicative interactions. Our work may provide some support for the claim that localized multiplicative interactions represent a biologically plausible alternative to quadratic poolings for implementing stereopsis and motion analysis."}], "references": [{"title": "Spatiotemporal energy models for the perception of motion", "author": ["E.H. Adelson", "J.R. Bergen"], "venue": "J. Opt. Soc. Am. A,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1985}, {"title": "Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition", "author": ["J. Douglas Carroll", "Jih-Jie Chang"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1970}, {"title": "Neural encoding of binocular disparity: Energy models, position shifts and phase shifts", "author": ["D. Fleet", "H. Wagner", "D. Heeger"], "venue": "Vision Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Digital Image Processing (3rd Edition)", "author": ["Rafael C. Gonzalez", "Richard E. Woods"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Emergence of phase- and shift-invariant features by decomposition of natural images into independent feature subspaces", "author": ["Aapo Hyv\u00e4rinen", "Patrik Hoyer"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Topographic ICA as a model of natural image statistics", "author": ["Aapo Hyv\u00e4rinen", "Patrik O. Hoyer", "Mika Inki"], "venue": "In Biologically Motivated Computer Vision. Springer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision", "author": ["Aapo Hyv\u00e4rinen", "J. Hurri", "Patrik O. Hoyer"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["Rodolphe Jenatton", "Julien Mairal", "Guillaume Obozinski", "Francis Bach"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Learning invariant features through topographic filter maps", "author": ["Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Rob Fergus", "Yann LeCun"], "venue": "In CVPR 2009. IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Toward a single-cell account for binocular disparity tuning: an energy model may be hiding in your dendrites", "author": ["Bartlett W. Mel", "Daniel L. Ruderman", "Kevin A. Archie"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Gradient-based learning of higher-order image features", "author": ["Roland Memisevic"], "venue": "In ICCV 2011,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "On multi-view feature learning", "author": ["Roland Memisevic"], "venue": "In ICML 2012,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Unsupervised learning of image transformations", "author": ["Roland Memisevic", "Geoffrey Hinton"], "venue": "In CVPR 2007,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order Boltzmann machines", "author": ["Roland Memisevic", "Geoffrey E Hinton"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Stereoscopic Depth Discrimination in the Visual Cortex: Neurons Ideally Suited as Disparity Detectors", "author": ["Izumi Ohzawa", "Gregory C. Deangelis", "Ralph D. Freeman"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B. Olshausen", "D. Field"], "venue": "Nature,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "Modeling Pixel Means and Covariances Using Factorized Third-Order Boltzmann Machines", "author": ["Marc\u2019Aurelio Ranzato", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A universal design principle for visual system pinwheels", "author": ["CF Stevens"], "venue": "Brain, behavior and evolution,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "On autoencoders and score matching for energy based models", "author": ["Kevin Swersky", "Marc\u2019Aurelio Ranzato", "David Buchman", "Benjamin Marlin", "Nando Freitas"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Convolutional learning of spatiotemporal features", "author": ["Graham", "W. Taylor", "Rob Fergus", "Yann LeCun", "Christoph Bregler"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex", "author": ["L. van Hateren", "J. Ruderman"], "venue": "Proc. Biological Sciences,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "Filters can be learned using a variety of criteria, including maximization of sparseness across filter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others.", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "Filters can be learned using a variety of criteria, including maximization of sparseness across filter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others.", "startOffset": 151, "endOffset": 155}, {"referenceID": 4, "context": "Filters can be learned using a variety of criteria, including maximization of sparseness across filter responses [18], minimizing reconstruction error [24], maximizing likelihood [5], and many others.", "startOffset": 179, "endOffset": 182}, {"referenceID": 5, "context": "The motivation for this is that group structure can explain several biological phenomena such as the presence of complex cells [6], it provides a simple way to model dependencies between features (eg.", "startOffset": 127, "endOffset": 130}, {"referenceID": 8, "context": ", [9] and references therein), and it can make learned representations more robust to small transformations which is useful for recognition [10].", "startOffset": 2, "endOffset": 5}, {"referenceID": 9, "context": ", [9] and references therein), and it can make learned representations more robust to small transformations which is useful for recognition [10].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Feature grouping can also be used as a way to obtain topographic feature maps [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "[7, 10, 8]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 9, "context": "[7, 10, 8]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 7, "context": "[7, 10, 8]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 5, "context": ", [6, 9, 10]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": ", [6, 9, 10]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 9, "context": ", [6, 9, 10]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 7, "context": "So they seem to capture a lot of what we are missing after performing a single layer of feature learning [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "Finally, it can be shown that in the presence of a upstream square-root non-linearity, using squared features generalizes standard feature learning, since it degenerates to a standard feature-learning model when using group size 1 [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 7, "context": "For a summary of these various heuristics, see [8] (page 215).", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 110, "endOffset": 117}, {"referenceID": 16, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 110, "endOffset": 117}, {"referenceID": 2, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 164, "endOffset": 171}, {"referenceID": 13, "context": "Our work is based on the close relationship between the well-known \u201cenergy models\u201d of motion and binocularity [1, 17] and the equivalent \u201ccross-correlation\u201d models [3, 14].", "startOffset": 164, "endOffset": 171}, {"referenceID": 19, "context": "That way, it may also help shed light onto the phenomenon that topographic filter maps do not seem to be present in rodents [20].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": ", [11, 22]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 21, "context": ", [11, 22]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "We focus in this work on the Gated Boltzmann Machine (GBM) [15, 22] which models the relationship between two binary images x and y using the three-way energy function", "startOffset": 59, "endOffset": 67}, {"referenceID": 21, "context": "We focus in this work on the Gated Boltzmann Machine (GBM) [15, 22] which models the relationship between two binary images x and y using the three-way energy function", "startOffset": 59, "endOffset": 67}, {"referenceID": 14, "context": "By adding appropriate penalty terms to the log-likelihood, one can extend the model to learn realvalued images [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "To reduce that number, [16] suggested factorizing the three-way parameter tensor W with entries wijk in Equation 1 into a three-way inner product: wijk = \u2211", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "This form of tensor factorization is also known as PARAFAC or \u201ccanonical decomposition\u201d in the literature, and it can be viewed as a three-way generalization of the SVD [2].", "startOffset": 169, "endOffset": 172}, {"referenceID": 15, "context": "Inferring the transformation h, given two images, x and y, is efficient, because the hidden variables are independent, given the data [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "In fact, it is not uncommon to use a number of factors that is larger than the dimensionality of the input data (for example, [16, 19]).", "startOffset": 126, "endOffset": 134}, {"referenceID": 18, "context": "In fact, it is not uncommon to use a number of factors that is larger than the dimensionality of the input data (for example, [16, 19]).", "startOffset": 126, "endOffset": 134}, {"referenceID": 15, "context": "To train the model, one can use contrastive divergence [16], score-matching or a variety of other approximations.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Recently, [21, 13] showed that one may equivalently add a decoder network, effectively turning the model into a \u201cgated\u201d version of a de-noising auto-encoder [24] and train it using back-prop.", "startOffset": 10, "endOffset": 18}, {"referenceID": 12, "context": "Recently, [21, 13] showed that one may equivalently add a decoder network, effectively turning the model into a \u201cgated\u201d version of a de-noising auto-encoder [24] and train it using back-prop.", "startOffset": 10, "endOffset": 18}, {"referenceID": 23, "context": "Recently, [21, 13] showed that one may equivalently add a decoder network, effectively turning the model into a \u201cgated\u201d version of a de-noising auto-encoder [24] and train it using back-prop.", "startOffset": 157, "endOffset": 161}, {"referenceID": 23, "context": "One may add noise to the data during training (but reconstruct the original, not noisy, data to compute the cost) [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Training on translations turns filters into Fourier components, as was initially observed by [16].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": ", [4]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "The phase-shift depends linearly on frequency [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": ", [14]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": ", [22]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": ", [7, 19]), too, if images and filters are contrast normalized [14].", "startOffset": 2, "endOffset": 9}, {"referenceID": 18, "context": ", [7, 19]), too, if images and filters are contrast normalized [14].", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": ", [7, 19]), too, if images and filters are contrast normalized [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "x = y (see, for example, [19, 14]).", "startOffset": 25, "endOffset": 33}, {"referenceID": 13, "context": "x = y (see, for example, [19, 14]).", "startOffset": 25, "endOffset": 33}, {"referenceID": 2, "context": "The experiment verifies the approximate equivalence of the two types of model derived, for example, in [3, 14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 13, "context": "The experiment verifies the approximate equivalence of the two types of model derived, for example, in [3, 14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 7, "context": "Energy models have been the standard approach to feature grouping in the past [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 22, "context": "Learning transformation features from natural videos: Figure 5 (right) shows a subset of filter groups learned from about one million patches of size 12 \u00d7 12 pixels that we crop from the van Hateren broadcast television database [23].", "startOffset": 229, "endOffset": 233}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "It also shows how low-frequency filters have the tendency of being grouped together [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "Localized gating may provide a somewhat more plausible explanation for the emergence of pinwheel-structures than squaring non-linearities, which are used, for example, in subspace models or topographic ICA (see [6, 7]).", "startOffset": 211, "endOffset": 217}, {"referenceID": 6, "context": "Localized gating may provide a somewhat more plausible explanation for the emergence of pinwheel-structures than squaring non-linearities, which are used, for example, in subspace models or topographic ICA (see [6, 7]).", "startOffset": 211, "endOffset": 217}, {"referenceID": 11, "context": "Our work may provide some support for the claim that localized multiplicative interactions are a biologically plausible alternative to square pooling for implementing stereopsis and motion analysis [12].", "startOffset": 198, "endOffset": 202}, {"referenceID": 19, "context": "It may also help explain why the development of pinwheels in V1 may be tied the presence of binocular vision and why topographic organization of features does not appear to occur, for example, in rodents [20].", "startOffset": 204, "endOffset": 208}], "year": 2013, "abstractText": "We present a feature learning model that learns to encode relationships between images. The model is defined as a Gated Boltzmann Machine, which is constrained such that hidden units that are nearby in space can gate each other\u2019s connections. We show how frequency/orientation \u201ccolumns\u201d as well as topographic filter maps follow naturally from training the model on image pairs. The model also offers a simple explanation why group sparse coding and topographic feature learning yields features that tend to by grouped according to frequency, orientation and position but not according to phase. Experimental results on synthetic image transformations show that spatially constrained gating is an effective way to reduce the number of parameters and thereby to regularize a transformationlearning model.", "creator": "LaTeX with hyperref package"}}}