{"id": "1301.3461", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Factorized Topic Models", "abstract": "In this paper we present a new type of latent topic model, which exploits supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for both image and text classification.", "histories": [["v1", "Tue, 15 Jan 2013 19:32:20 GMT  (1763kb)", "https://arxiv.org/abs/1301.3461v1", "11 pages include 2 appendix pages"], ["v2", "Wed, 16 Jan 2013 11:05:05 GMT  (1763kb)", "http://arxiv.org/abs/1301.3461v2", "11 pages include 2 appendix pages"], ["v3", "Thu, 24 Jan 2013 09:50:28 GMT  (1483kb)", "http://arxiv.org/abs/1301.3461v3", "11 pages include 2 appendix pages"], ["v4", "Thu, 7 Mar 2013 14:16:39 GMT  (1830kb)", "http://arxiv.org/abs/1301.3461v4", "11 pages include 2 appendix pages"], ["v5", "Fri, 15 Mar 2013 17:14:58 GMT  (2111kb)", "http://arxiv.org/abs/1301.3461v5", null], ["v6", "Wed, 10 Apr 2013 20:15:04 GMT  (1937kb)", "http://arxiv.org/abs/1301.3461v6", "ICLR 2013"], ["v7", "Tue, 23 Apr 2013 08:13:55 GMT  (1937kb)", "http://arxiv.org/abs/1301.3461v7", "ICLR 2013"]], "COMMENTS": "11 pages include 2 appendix pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.IR", "authors": ["cheng zhang", "carl henrik ek", "andreas damianou", "hedvig kjellstrom"], "accepted": true, "id": "1301.3461"}, "pdf": {"name": "1301.3461.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Andreas Damianou"], "emails": ["chengz@kth.se", "chek@kth.se", "andreas.damianou@sheffield.ac.uk", "hedvig@kth.se"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.34 61v7 [cs.LG] 2 3A pr2 01"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to play by the rules that they play by the rules, and they will have to play by the rules that they play by the rules."}, {"heading": "3 Model", "text": "As described in the introduction, we add factorization to a model that describes variations in data relating to a number of latent topics. We seek a structured representation that encodes topics that contain variations within the class or within the class, separate from those that contain variations that are shared between classes. We apply our factorization framework to an adjustment of the LDA that contains additional class information to restore such factorized latent space. In this section, we first review the traditional LDA model [4], followed by the description of our factorized topic model."}, {"heading": "3.1 LDA Revisited", "text": "Formally, a document w consists of a collection of words w = [w1,.., wN] of a vocabulary indexed by [1,.., V]. Within a subject model, each document of N words is described as a mixture of K topics, so that each word is associated with a specific subject: z = [z1,.., zN], where zn [1,.., K]. The mixture is described as asp (w | z, \u03b2) = N = 1K \u00b2 k = 1p (wn | zn, \u03b2k) (1), where \u03b2k is the distribution across the vocabulary for topic k. The novelty and reason for the success of the LDA model is how topics z and topic vocabulary \u03b2 are constructed within the framework. The underpinned intuition is that the topics represent a compact representation with K \u00b2 N and that the structure of the class should be economical."}, {"heading": "3.2 Factorized Topic Model", "text": "As motivated in Section 1, our idea is to divide the theme space into two parts, in which the class-dependent part explains the class-dependent information (signal) and the divided part of the class-independent information (structured noise). To achieve this, we will introduce an additional topic presented in [8]. This will promote a factorized structure so that the K topics can be \"softly\" divided into Kp-private topics and Ks topics in which Kp + Ks = K. The advantage of such a structured topic space is that it will be more compact than a regular model; all aspects of the data that correlate to the class will be pushed into the class-private part of the topic. Since the other, class-divided space then contains only noise, the class of a new document w is inferred, using only the class-private part."}, {"heading": "4 Experiments", "text": "The proposed model is evaluated on the basis of four different classification tasks and compared with two baselines, which consist of a regular LDA model with class marking [8] and a model with stronger class supervision in learning, SLDA [3]."}, {"heading": "4.1 Object Classification", "text": "We will first demonstrate how factorization works using a toy dataset. DA, shown in Figure 2, is effectively designed to have a very high level of structured noise. There are four object classes: bulb, car, duck, and cup. All 8 cases of a given class have the same shape and position. However, there is very high image variability within a class in the foreground and background texture. In addition, all four classes contain exactly the same texture combinations between front and background. Thus, the texture (which will dominate the variation between the characteristics of each visual extractor) can be regarded as structured noise, while the true signal relates to the shape. The characteristics of this dataset can also be found to some extent in natural images: most realistic image and object classes show large variations within a class, and different classes share appearance aspects. In addition, the backgrounds in natural scenes are often complex and varied, the introduction of even more training data under one class."}, {"heading": "4.2 Text Classification", "text": "We are now evaluating the proposed model in a realistic text classification scenario. We are using the standard R8 training and test set from the Reuters 21578 dataset [26], which contains 5485 training materials and 2189 test documents. We are using the full version of the data because we want to illustrate how our model handles noise. LDA, SLDA and factored LDA regular models are trained in 20 topics, and the parameter settings \u03b1 = 0.5 and \u03c0 = 0.1. Topic distributions are shown in Figures 4 (a), 4 (b) and 4 (c). Factored class-private topic distribution (to the left of the red line in Figure 4 (c)) is significantly cleaner than the regular distribution class (Figure 4 (a)). Factored LDA contributes only one percent to the classification, while the split topics LDA (to the right of the red line in Figure 4 (c) are effectively ignored."}, {"heading": "4.3 Scene Classification", "text": "We also evaluate the proposed model using a sophisticated natural scene data set used in [8]. There are four classes: forest, mountain, open country and coast, with 100 training frames and 50 test frames per class. SIFT features are densely extracted from each image on two different scales and labeled according to a 192-word vocabulary, as in [8]. The regular LDA, SLDA and factored LDA models are trained with 20 themes and parameter settings \u03b1 = 0.5 and \u03c0 = 0.1. Figures 5 (a), 5 (b) and 5 (c) show the respective theme distributions; in particular, the class-specific theme room used effectively for classification in our factored LDA contains only 8 themes, while 12 themes (s) are devoted to structured noise modeling. Thus, the factored LDA representation is significantly more economical than a regular LDA representation, which provides the possibility of saving both 4.DA and 4.DA during 8- computing time."}, {"heading": "4.4 Action Classification", "text": "The data set consists of three actions from the KTH Action dataset [15]: boxes, clapping hands and waving hands. There are 100 short video sequences of each action showing 25 different people performing the action, recorded in four shooting conditions (zooming and panning of the camera, different background).The recording state has a great influence on the movement in the video, as each zoom or panning motion adds global movement to the video and also contributes to the motion characteristics. However, the variation in the recording state does not correlate at all with the action class in the dataset. Just like in the toy experiment above (but now in a more realistic environment), a large proportion of the data variation is thus independent of the action class. Due to the low signal / noise ratio, a theme model without factorization is difficult to capture the aspects of the activity classes relevant to the discrimination. The experiment was carried out by separating the data from the DA models, which consists of all action boxes, the DA-filtered data with other 12DA-DA actions."}, {"heading": "5 Conclusions", "text": "We present a factorized latent topic model that explicitly represents aspects of the data that are not correlated with the model state. Specifically, we train an LDA class model with an additional factorization flow that distributes topics either very class-specific or evenly among the classes. Therefore, the topic space \u03b8 is divided into one part \u03b8p, whose topics are reserved for certain classes, and another part successs, with topics that are divided between classes. Only \u03b8p effectively contributes to the classification. Experiments show that the factorized LDA model consistently provides better classification performance and more economical topic representations than a regular LDA model [8] and SLDA [3]. Scanty representations are advantageous for large data sets as they save storage space and computing time during classification. Future work includes investigating the effects of this factorization before other topic models, such as HDP, and integrating the previous ones into models with multiple data views, such as in [12, 30]."}], "references": [{"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Technical report", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM, 55(4):77\u201384", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Supervised topic models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "arxiv:1003.0783", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, 3:993\u20131022", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold Relevance Determination", "author": ["A. Damianou", "C.H. Ek", "M. Titsias", "N.D. Lawrence"], "venue": "ICML", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Shared Gaussian Process Latent Variable Models", "author": ["C.H. Ek"], "venue": "PhD Thesis", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Ambiguity modeling in latent spaces", "author": ["C.H. Ek", "J. Rihan", "P.H.S. Torr", "G. Rogez", "N.D. Lawrence"], "venue": "Machine Learning for Multimodal Interaction", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Shared components topic models", "author": ["M.R. Gormley", "M. Dredze", "B. Van Durme", "J. Eisner"], "venue": "NAACL", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Technical report", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "UAI", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "ICCV", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Generative models that discover dependencies between data sets", "author": ["A. Klami", "S. Kaski"], "venue": "IEEE Workshop on Machine Learning for Signal Processing", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic approach to detecting dependencies between data sets", "author": ["A. Klami", "S. Kaski"], "venue": "Neurocomputing, 72(1-3):39\u201346", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "On space-time interest points", "author": ["I. Laptev"], "venue": "IJCV, 64(2/3):107\u2013123", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["N.D. Lawrence"], "venue": "Journal of Machine Learning Research, 6:1783\u20131816", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "A Gaussian process latent variable model formulation of canonical correlation analysis", "author": ["G. Leen", "C. Fyfe"], "venue": "European Symposium on Artificial Neural Networks", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning shared and separate features from two related data sets using GPLVM\u2019s", "author": ["G. Leen", "C. Fyfe"], "venue": "Learning from Multiple Sources Workshop, Neural Information Processing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust object detection with interleaved categorization and segmentation", "author": ["B. Leibe", "A. Leonardis", "B. Schiele"], "venue": "IJCV, 77(1):259\u2013289", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "IJCV, 60(2):91\u2013110", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "The joint manifold model for semi-supervised multivalued regression", "author": ["R. Navaratnam", "A.W. Fitzgibbon", "R. Cipolla"], "venue": "ICCV", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "The discrete infinite logistic normal distribution", "author": ["J. Paisley", "C. Wang", "D. Blei"], "venue": "arXiv preprint arXiv:1103.4789", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent semantic indexing: a probabilistic analysis", "author": ["C.H. Papadimitriou", "P. Raghavan", "H. Tamaki"], "venue": "Journal of Computer and System Sciences, (61):217\u2013235", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Interpolation artefacts in mutual information-based image registration", "author": ["J.P.W. Pluim", "J.B.A. Maintz", "M.A. Viergever"], "venue": "CVIU, 77(2):1077\u20133142", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Pereira", "E. Coviello", "G. Doyle", "G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "MMM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611\u2013622", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Factorized multi-modal topic model", "author": ["S. Virtanen", "Y. Jia", "A. Klami", "T. Darrell"], "venue": "UAI", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Decoupling sparsity and smoothness in the discrete hierarchical dirichlet process", "author": ["C. Wang", "D.M. Blei"], "venue": "NIPS", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Simultaneous image classification and annotation", "author": ["C. Wang", "D.M. Blei", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 19, "context": ", SIFT [20] features extracted from training images of horses, cows and cats with a variation of fur texture.", "startOffset": 7, "endOffset": 11}, {"referenceID": 22, "context": "Probabilistic topic models [23, 11, 4, 2] model a data example as a collection of words (in the case of images, visual words), each sampled from a latent distribution of topics.", "startOffset": 27, "endOffset": 41}, {"referenceID": 10, "context": "Probabilistic topic models [23, 11, 4, 2] model a data example as a collection of words (in the case of images, visual words), each sampled from a latent distribution of topics.", "startOffset": 27, "endOffset": 41}, {"referenceID": 3, "context": "Probabilistic topic models [23, 11, 4, 2] model a data example as a collection of words (in the case of images, visual words), each sampled from a latent distribution of topics.", "startOffset": 27, "endOffset": 41}, {"referenceID": 1, "context": "Probabilistic topic models [23, 11, 4, 2] model a data example as a collection of words (in the case of images, visual words), each sampled from a latent distribution of topics.", "startOffset": 27, "endOffset": 41}, {"referenceID": 1, "context": "We present a variant of a Latent Dirichlet Allocation (LDA) [2] model which is able to model the signal and structured noise separately from the data.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Experiments in Section 4 show that the proposed model outperforms both the standard LDA and a supervised variant, SLDA [3], on classification of images, text, and video.", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "The factorization method can be applied to other topic models as well, and the sparse factorized topic representation is beneficial not only for classification, as shown here, but also for synthesis [5], ambiguity modeling [7], and domain transfer [21].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "The factorization method can be applied to other topic models as well, and the sparse factorized topic representation is beneficial not only for classification, as shown here, but also for synthesis [5], ambiguity modeling [7], and domain transfer [21].", "startOffset": 223, "endOffset": 226}, {"referenceID": 20, "context": "The factorization method can be applied to other topic models as well, and the sparse factorized topic representation is beneficial not only for classification, as shown here, but also for synthesis [5], ambiguity modeling [7], and domain transfer [21].", "startOffset": 248, "endOffset": 252}, {"referenceID": 0, "context": "For continuous observations, several classic algorithms such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) can be interpreted as latent variable models [1, 16, 17, 27].", "startOffset": 185, "endOffset": 200}, {"referenceID": 15, "context": "For continuous observations, several classic algorithms such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) can be interpreted as latent variable models [1, 16, 17, 27].", "startOffset": 185, "endOffset": 200}, {"referenceID": 16, "context": "For continuous observations, several classic algorithms such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) can be interpreted as latent variable models [1, 16, 17, 27].", "startOffset": 185, "endOffset": 200}, {"referenceID": 25, "context": "For continuous observations, several classic algorithms such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) can be interpreted as latent variable models [1, 16, 17, 27].", "startOffset": 185, "endOffset": 200}, {"referenceID": 10, "context": "A first proposal of a generative topic model was Probabilistic Latent Semantic Indexing (pLSI) [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 3, "context": "This was done by the adaptation of a Dirichlet layer and referred to as Latent Dirichlet Allocation (LDA) [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "Central to the work presented in this paper is a specific latent structure simultaneously proposed by several authors [7, 13, 14, 18].", "startOffset": 118, "endOffset": 133}, {"referenceID": 12, "context": "Central to the work presented in this paper is a specific latent structure simultaneously proposed by several authors [7, 13, 14, 18].", "startOffset": 118, "endOffset": 133}, {"referenceID": 13, "context": "Central to the work presented in this paper is a specific latent structure simultaneously proposed by several authors [7, 13, 14, 18].", "startOffset": 118, "endOffset": 133}, {"referenceID": 17, "context": "Central to the work presented in this paper is a specific latent structure simultaneously proposed by several authors [7, 13, 14, 18].", "startOffset": 118, "endOffset": 133}, {"referenceID": 5, "context": "The latent representation is factorized such that the modality-independent and modality-dependent are encoded in separate subspaces [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "This factorization has an intuitive interpretation in that the private space encodes variations that exists in only one modality and does therefore encode variations representing the ambiguities between the modalities [7].", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "Results have been presented [12, 25, 28] for the case of two conditionally independent observation modalities, addressing the image and text cross-modal multimedia retrieval problem with topic representation.", "startOffset": 28, "endOffset": 40}, {"referenceID": 24, "context": "Results have been presented [12, 25, 28] for the case of two conditionally independent observation modalities, addressing the image and text cross-modal multimedia retrieval problem with topic representation.", "startOffset": 28, "endOffset": 40}, {"referenceID": 26, "context": "Results have been presented [12, 25, 28] for the case of two conditionally independent observation modalities, addressing the image and text cross-modal multimedia retrieval problem with topic representation.", "startOffset": 28, "endOffset": 40}, {"referenceID": 11, "context": "In [12] a model that can be seen as a Markov random field of LDA topic models is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Further, in [25] CCA is applied to the topic space of the text data, which in turn has been learned from LDA and the image feature space.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "(a) LDA with class label [8] \u03b1 \u03b8 c", "startOffset": 25, "endOffset": 28}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Differently, [28] instead use a Hierarchical Dirichlet Process (HDP) based method which has a complexity selection property.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "This is an extension of [22] to multi-modalities, hence it can not be generalized to other topic models, such as LDA or pLSI and it can not be used to model the private and shared information with only one modality.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "Differently from [12, 28, 25], which need to model the shared topics and private topics in the joint topic space across different observation modalities, our factorization takes place over one modality across different classes, where the structured noise is modeled in the class-shared topics and the signal is modeled in class-private topics.", "startOffset": 17, "endOffset": 29}, {"referenceID": 26, "context": "Differently from [12, 28, 25], which need to model the shared topics and private topics in the joint topic space across different observation modalities, our factorization takes place over one modality across different classes, where the structured noise is modeled in the class-shared topics and the signal is modeled in class-private topics.", "startOffset": 17, "endOffset": 29}, {"referenceID": 24, "context": "Differently from [12, 28, 25], which need to model the shared topics and private topics in the joint topic space across different observation modalities, our factorization takes place over one modality across different classes, where the structured noise is modeled in the class-shared topics and the signal is modeled in class-private topics.", "startOffset": 17, "endOffset": 29}, {"referenceID": 8, "context": "In [9] the topics are represented as combinations of a small number of latent components as such leading to a more compact model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 27, "context": "In [29] the each topic is constrained by the words in the vocabulary.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In this section, the traditional LDA model [4] is first revisited, followed by the description of our factorized topic model.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "One way of incorporating class information within the LDA framework was suggested in [8] where the use of a class dependent topic distribution was proposed.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Using this model the class can be inferred for a new document w\u2217 through a maximum likelihood procedure \u0109 = argmaxc p(w\u2217|\u03b1, \u03c0, c) [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "In this paper we take inspiration from the work presented in [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Thus, the model we will propose have a stronger class dependency compared to [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "To achieve this we introduce an additional prior p(\u03b8) to the model presented in [8].", "startOffset": 80, "endOffset": 83}, {"referenceID": 18, "context": "Information entropy, widely used in different fields [19, 24], provides a good measurement of this property.", "startOffset": 53, "endOffset": 61}, {"referenceID": 23, "context": "Information entropy, widely used in different fields [19, 24], provides a good measurement of this property.", "startOffset": 53, "endOffset": 61}, {"referenceID": 0, "context": "H(k) \u2208 [0, 1], 0 if all the probability in the topic k is concentrated to one class, 1 if all classes are equally probable to contain the topic k.", "startOffset": 7, "endOffset": 13}, {"referenceID": 9, "context": "We use Gibbs sampling for learning the parameters of the model, more specifically, collapsed Gibbs sampling [10] in the same manner as [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "We use Gibbs sampling for learning the parameters of the model, more specifically, collapsed Gibbs sampling [10] in the same manner as [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 7, "context": "The proposed model is evaluated on four different classification tasks, and compared to two baselines consisting of a regular LDA model with class label [8], and a model with stronger class-supervision in the topic learning, SLDA [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "The proposed model is evaluated on four different classification tasks, and compared to two baselines consisting of a regular LDA model with class label [8], and a model with stronger class-supervision in the topic learning, SLDA [3].", "startOffset": 230, "endOffset": 233}, {"referenceID": 7, "context": "In the following, we will by \u201cregular LDA\u201d mean the regular LDA with upstream supervision presented in [8], but trained using Gibbs sampling in the same way as our model, with the same value of \u03b1 for all documents.", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "With \u201dSLDA\u201d, we mean the more strongly supervised LDA variant with downstream supervision presented in [3], implemented by Blei et al.", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "3 Scene Classification We also evaluate the proposed model on a challenging natural scene dataset used in [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "From each image, SIFT features on two different scales are densely extracted, and labeled according to a 192-word vocabulary learned from the features, as in [8].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "All performances are slightly better than the original implementation of the regular LDA [8], which reaches 76.", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "The dataset consists of three actions from the KTH Action dataset [15]: boxing, handclapping and handwaving.", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "STIP features [15] were extracted from all sequences and clustered into a vocabulary of 128 spatiotemporal words.", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "Experiments show the factorized LDA model to give consistently better classification performance and sparser topic representations than both a regular LDA model [8] and SLDA [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "Experiments show the factorized LDA model to give consistently better classification performance and sparser topic representations than both a regular LDA model [8] and SLDA [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 11, "context": "Future work includes investigating the effect of this factorization prior on other topic models, such as HDP, and to integrate the prior into models with multiple data views, such as in [12, 28, 30].", "startOffset": 186, "endOffset": 198}, {"referenceID": 26, "context": "Future work includes investigating the effect of this factorization prior on other topic models, such as HDP, and to integrate the prior into models with multiple data views, such as in [12, 28, 30].", "startOffset": 186, "endOffset": 198}, {"referenceID": 28, "context": "Future work includes investigating the effect of this factorization prior on other topic models, such as HDP, and to integrate the prior into models with multiple data views, such as in [12, 28, 30].", "startOffset": 186, "endOffset": 198}], "year": 2013, "abstractText": "In this paper we present a modification to a latent topic model, which makes the model exploit supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior over the topic space. The approach allows for a more efficient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for image, text, and video classification.", "creator": "LaTeX with hyperref package"}}}