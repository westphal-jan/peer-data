{"id": "1301.3527", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Block Coordinate Descent for Sparse NMF", "abstract": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms, such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L$_1$ norm. However, present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow and other formulations for sparse NMF have been proposed such as those based on L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "histories": [["v1", "Tue, 15 Jan 2013 23:11:05 GMT  (1027kb,D)", "http://arxiv.org/abs/1301.3527v1", null], ["v2", "Mon, 18 Mar 2013 22:42:11 GMT  (822kb,D)", "http://arxiv.org/abs/1301.3527v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["vamsi k potluru", "sergey m plis", "jonathan le roux", "barak a pearlmutter", "vince d calhoun", "thomas p hayes"], "accepted": true, "id": "1301.3527"}, "pdf": {"name": "1301.3527.pdf", "metadata": {"source": "CRF", "title": "Block Coordinate Descent for Sparse NMF", "authors": ["Vamsi K. Potluru", "Sergey M. Plis"], "emails": ["ismav@cs.unm.edu", "splis@mrn.org", "leroux@merl.com", "barak@cs.nuim.ie", "vcalhoun@mrn.org", "hayes@cs.unm.edu"], "sections": [{"heading": "1 Introduction", "text": "Matrix factorization occurs in a wide range of applications and is useful for extracting latent features in the dataset. In particular, we are interested in matrix factorizations that require: \u2022 Non-negativity \u2022 short range \u2022 sparsityNonnegativity is a natural limitation in modeling data with physical limitations such as chemical concentrations in solutions, pixel intensities in images and radiation dosages for cancer treatment. Low range is useful to learn a representation of lower dimensionality. Thrift is useful to model the precision of representation or that of Thear Xiv: 130 1.35 27v1 [cs. Llatent features. All of these requirements of our matrix factorization leads to the sparse non-negative matrix factorization (SNMF) problem. SNMF enjoys some formulations [10, 9, 7, 20, 22, 23] with successful single-channel projection applications."}, {"heading": "2 Preliminaries and Previous Work", "text": "In this section, we will give an introduction to non-negative matrix factorization (NMF) and SNMF problems. We will also discuss some widely used algorithms from literature to solve them. Both problems share the following problem and solution structure. At a high level, we want to approximate it to a non-negative matrix X of size m \u00b7 n with a product of two non-negative matrices W, H of size m \u00b7 r or r \u00b7 n: X \u2248 WH. (1) The non-negative constraint of matrix H makes the representation a convex combination of features represented by the columns of matrix W. In particular, unlike other factoring techniques such as main component analysis (PCA) and vector quantization (VQ), NMF can lead to sparse representations or a partial representation. A common theme in the algorithms proposed to solve these problems is the use of alternating updates of the core factors, which are, of course, focused on the effector analysis, because the A is instead of the objective (W) and the component analysis in both (PCW)."}, {"heading": "2.1 Nonnegative Matrix Factorization", "text": "Factoring a matrix whose entries are all non-negative, as the product of two low-ranking non-negative factors, is a fundamental algorithmic challenge, which has occurred naturally in various fields such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometry [15], information gathering [8] and biology applications [2]. For further applications, see the references in the following papers [1, 4].We will consider the following version of the NMF problem, which measures reconstruction error using the Frobenius standard [17]: min W, H12, X \u2212 WH-2F s.t. W \u2265 0, H \u2265 0, Wj \u04322 = 1, \u04321, \u00b7, r} (2), where the reconstruction error is measured using the Frobenius standard [17]. We use subscript to mark column elements."}, {"heading": "2.2 Sparse Nonnegative Matrix Factorization", "text": "In general, non-negative decomposition is not unique [6]. Furthermore, the characteristics may not be part-based if the data is well within the positive orthants. [7] To solve these problems, we need to divide into two classes of formulations: explicit versions of SNMF [10, 7] can directly formulate the differences in matrix factors W, H. On the other hand, in implicit versions of SNMF [13, 22], thrift is controlled by a regulation parameter and is often difficult to adjust a priori to specified savings values. Algorithms for implicit versions of SNMF tend to be faster than explicit versions of SNMF. In this paper, we look at the explicit thrift of the NMF formulation."}, {"heading": "3 The Sequential Sparse NMF Algorithm", "text": "We present our algorithm, which we call Sequential Sparse NMF (SSNMF), to solve the SNMF problem as follows: Firstly, we look at a particular problem, which is the building block (algorithm 2) of our SSNMF algorithm, and specify an efficient and exact algorithm to solve it. Secondly, we describe our sequential approach (algorithm 3) to solving the SNMF partial problem, using the routine we developed in the previous step. Finally, we combine our routines developed in the previous two steps with standard solvers (such as algorithm 1) to complete the SSNMF algorithm (algorithm 4)."}, {"heading": "3.1 Sparse-opt", "text": "Sparse opt routine solves the following sub-problem that arises in solving the problem (3): max y \u2265 0b > y s.t. (2): max y \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (5) This problem has been considered so far [10], and a heuristic has been proposed to solve it, which we will henceforth call projection heuristic. Observation 1. For each i \u2212 j \u2212 s, we have this problem when bi \u2265 bj, then yi \u2265 yyy.Let us first consider the case when the vector b is sorted. Then, through the previous observation, we have a transition point p p p, which separates the zeros of the solution vector from the residual. Observation 2. Applying the Cauchy-Black inequality to y and all the one vector, we obtain p \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s of the problem (5) s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2."}, {"heading": "3.2 Sequential Approach \u2014Block Coordinate Descent", "text": "Previous approaches to solving SNMF [10, 7] use batch methods to solve column constraints = > Q = > Constant Convergence. This means that the entire matrix is updated and projected at once to meet the constraints. We take a different approach to updating a column vector at a given time, which gives us the advantage of being able to solve the subproblem (column) efficiently and accurately. Subsequent updates can benefit from the newly updated columns, which lead to faster convergence as seen in the experiments.In particular, we consider the optimization problem (3) for a column j of the matrix W, while we consider the rest of the elements of the matrix W, H: min Wj Wj \u2265 0f (Wj) = 12 g \u00b2 Wj \u00b2 22 + u > Wj s.t. Wj \u00b2 2 = 1... Wj \u00b2 1 = kwhere g = H > Hj and > ju = XHH = 6 (Wj \u00b2)."}, {"heading": "3.3 SSNMF Algorithm for Sparse NMF", "text": "By combining algorithms 1, 2 and 3 we get SSNMF (algorithm 4).Algorithm 4 ssnmf (X, W, H) 1: repeat 2: W = sequential-pass (X, W, H) 3: H = nnls-mult (X, W, H) 4: until convergence 5: output: matrices W, H."}, {"heading": "4 Implementation Issues", "text": "To illustrate the exposure, we presented the simple vanilla version of our SSNMF algorithm 4. We will now describe some of the actual implementation details. \u2022 Initialization: Generate a positive random vector v of size m and get z = sparse opt (v, k), where k = \u221a m \u2212 \u03b1 \u221a m \u2212 1 (from Equation (4). Use the solution z and its random permutations to initialize the matrix W. Initialize the matrix H to even random entries in [0, 1]. \u2022 Inclusion of faster solvers: We use multiplicative updates for a fair comparison with NMFSC and TPC. However, we can use other NLS solvers [18, 14, 3, 11] to solve the matrix H. Empirical results (which are not reported here) show that this further accelerates the SSNMF algorithm. \u2022 Updating: In our experiments, we will update the number of non-hazardous ones, or the number of them."}, {"heading": "5 Experiments and Discussion", "text": "In this section, we compare the performance of our algorithm with the most advanced NMFSC and TPC algorithms [10, 7]. Running times of the algorithms are displayed when applied to one synthetic and three real datasets. Experiments report reconstruction errors rather than objective values for better representation. In all experiments on the datasets, we ensure that our final reconstruction error is always better than that of the other two algorithms. Our algorithm was implemented in MATLAB (http: / / www. mathworks.com), similar to NMFSC and TPC. All of our experiments were performed on a 3.2Ghz Intel calculator with 24GB of RAM and the number of threads set to one."}, {"heading": "5.1 Datasets", "text": "To compare the performance of SSNMF with NMFSC and TPC, we look at the following synthetic and three real data sets: \u2022 Synthetic: 200 images of size 9 x 9, as provided by Healer and Sch\u00f6rr in their code implementation. \u2022 ORL: Face data set consisting of 400 images of size 112 x 92, available at http: / / www.cl.cam.ac.uk / research / dtg / attarchive / facedatabase.html. \u2022 Van Hateren: Natural data set consisting of 4000 images of size 1536 x 1024, available at http: / / www.kyb.tuebingen.mpg.de / bethge / vanhateren / iml /. We use 400 of these images in our experiments. \u2022 sMRI: Structural MRI scans of size 1536 x 1024 and scans of size 269 people taken at John Hopkins University."}, {"heading": "5.2 Comparing Performances of Core Updates", "text": "We compare our Sparse-opt (Algorithm 2) routine with the competing Projectionheuristic [10]. Specifically, we generate 40 random problems for each Sparsity constraint {0,2, 0,4, 0,6, 0,8} and a fixed problem size. The problems are of size 2i \u00d7 100, with integer values from 0 to 12. Input coefficients are generated by taking random samples evenly from [0, 1]. Means of runtimes for Sparse-opt and Projection-heuristic for each dimension and the corresponding Sparsity value are shown in Figure 1."}, {"heading": "5.3 Comparing Overall Performances", "text": "SSNMF versus NMFSC and TPC: We have shown the performance of SSNMF versus NMFSC and TPC on the synthetic dataset provided by Healer and Schnarr in Figure 2. We have used the default settings for both NMFSC and TPC using the software provided by the authors. Our experience with TPC has not been encouraging for larger datasets and therefore we only show their performance on the synthetic dataset. It is possible that TPC performance can be improved by changing the default settings, but we did not find it trivial to do so. SSNMF versus NMFSC: To ensure fairness, we have removed logging information from the NMFSC code [10] and only calculated the target for an appropriate number of matrix updates as SSNMF. We do not plot the objective values in the first iteration for the convenience of the display."}, {"heading": "5.4 Main Results", "text": "We compared the runtimes of our savings opt routine with the projection heuristic and found that we are on average faster on randomly generated problems. Furthermore, our savings opt routine is guaranteed to be monotonous and possibly converge to a lower target than when we compare the SSNMF and the SSNMF + Proj algorithms (Figure 1). Our results in switching the savings opt routine to the projection heuristic did not significantly slow down our SSNMF solver for the data sets we were looking at. Therefore, we conclude that the acceleration is mainly due to the sequential nature of the updates (Algorithm 3). Furthermore, we converge faster than NMFSC for fewer matrix updates. This is evident from the fact that the points applied in Figures 3 and 4 are such that the number of matrix updates for both SMF and NMSC we have found an order of magnitude in some of the data sets we are equal to."}, {"heading": "6 Connections to Related Work", "text": "Other SNMF formulations have been considered by Hoyer [9], M\u00f8rup et al. [20], Kim and Park [13], Pascual-Montano et al. [22] (nsNMF) and Peharz and Pernkopf [23]. We note that our measure of sparseness has all the desirable properties discussed in detail by Hurley and Rickard [12], except for one (\"cloning\"). Property cloning is satisfied if two vectors of the same sparseness, when concatenated, maintain their sparseness value. Dimensions in our optimization problem are fixed and therefore do not violate cloning property. Compare this with the L1 standard, which only meets one of these properties (namely \"rising flood\"). Property is the property where a constant is added to the elements of a vector, which reduces the sparseness of a vector and thus reduces the sparseness of the vector."}, {"heading": "7 Conclusions", "text": "We have proposed a new efficient algorithm to solve the sparse NMF problem. Experiments show the effectiveness of our approach to real data sets of practical interest. Our algorithm is faster over a range of splitting values and generally performs better when the splitting rate is higher. Acceleration is mainly due to the sequential nature of the updates as opposed to the previously used batch updates from Hoyer. We also presented an accurate and efficient algorithm to solve the problem of maximizing a linear object with a splitting constraint, which is an improvement over the heuristic approach in Hoyer. Our approach can be extended to other NMF variants [9]. Another possible application is the sparse version of non-negative tensors or factorization. Another research direction would be to scale our algorithm to have large data sets through chunking [19] and / or distributed computing settings."}, {"heading": "Acknowledgement", "text": "The first author is grateful for the support of NIBIB grants 1 R01 EB 000840 and 1 R01 EB 005846. The second author was supported by NIMH grants 1 R01 MH07628201, the latter two being funded by the NSF / NIH Collaborative Research Center Computational Neuroscience Program."}], "references": [{"title": "Computing a nonnegative matrix factorization \u2013 provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "In Proceedings of the 44th symposium on Theory of Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Color categories revealed by non-negative matrix factorization of munsell color spectra", "author": ["G. Buchsbaum", "O. Bloch"], "venue": "Vision research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Fast local algorithms for large scale nonnegative matrix and tensor factorizations", "author": ["A. Cichocki", "A.H. Phan"], "venue": "IEICE Transactions on Fundamentals of Electronics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Nonnegative ranks, decompositions, and factorizations of nonnegative matrices", "author": ["J. Cohen", "U. Rothblum"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1993}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Learning sparse representations by non-negative matrix factorization and sequential cone programming", "author": ["M. Heiler", "C. Schn\u00f6rr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "In Neural Networks for Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Non-negative matrix factorization with sparseness constraints", "author": ["P.O. Hoyer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Fast coordinate descent methods with variable selection for non-negative matrix factorization", "author": ["C.J. Hsieh", "I. Dhillon"], "venue": "ACM SIGKDD Internation Conference on Knowledge Discovery and Data Mining, page xx,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Comparing measures of sparsity", "author": ["N. Hurley", "S. Rickard"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis", "author": ["H. Kim", "H. Park"], "venue": "Bioinformatics, 23(12):1495\u20131502,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Toward faster nonnegative matrix factorization: A new algorithm and comparisons", "author": ["J. Kim", "H. Park"], "venue": "Data Mining, IEEE International Conference on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Self modeling curve resolution", "author": ["W. Lawton", "E. Sylvestre"], "venue": "Technometrics, pages 617\u2013633,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1971}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "S.H. Seung"], "venue": "In NIPS, pages 556\u2013562,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural Comp.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Approximate L0 constrained non-negative matrix and tensor factorization", "author": ["M. M\u00f8rup", "K.H. Madsen", "L.K. Hansen"], "venue": "In ISCAS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "CORE Discussion Papers,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Nonsmooth nonnegative matrix factorization (nsNMF)", "author": ["A. Pascual-Montano", "J.M. Carazo", "K. Kochi", "D. Lehmann", "R.D. Pascual-Marqui"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Sparse nonnegative matrix factorization with l-constraints", "author": ["R. Peharz", "F. Pernkopf"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Single-channel speech separation using sparse nonnegative matrix factorization", "author": ["M.N. Schmidt", "R.K. Olsson"], "venue": "In International Conference on Spoken Language Processing (INTERSPEECH),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 8, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 6, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 19, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 12, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 21, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 22, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 37, "endOffset": 63}, {"referenceID": 23, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 164, "endOffset": 172}, {"referenceID": 21, "context": "SNMF enjoys quite a few formulations [10, 9, 7, 20, 13, 22, 23] with successful applications to single-channel speech separation [24] and micro-array data analysis [13, 22].", "startOffset": 164, "endOffset": 172}, {"referenceID": 9, "context": "However, algorithms [10, 7] for solving SNMF which utilize the mixed norm of L1/L2 as their sparsity measure are slow and do not scale well to large datasets.", "startOffset": 20, "endOffset": 27}, {"referenceID": 6, "context": "However, algorithms [10, 7] for solving SNMF which utilize the mixed norm of L1/L2 as their sparsity measure are slow and do not scale well to large datasets.", "startOffset": 20, "endOffset": 27}, {"referenceID": 9, "context": "Thus, we develop an efficient algorithm to solve this problem and has the following ingredients: \u2022 An exact projection operator to enforce the user-defined sparsity as opposed to the previous heuristic approach [10].", "startOffset": 211, "endOffset": 215}, {"referenceID": 9, "context": "\u2022 Novel sequential updates which provide the bulk of our speedup compared to the previously employed batch methods [10, 7].", "startOffset": 115, "endOffset": 122}, {"referenceID": 6, "context": "\u2022 Novel sequential updates which provide the bulk of our speedup compared to the previously employed batch methods [10, 7].", "startOffset": 115, "endOffset": 122}, {"referenceID": 15, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "This has arisen naturally in diverse areas such as image analysis [16], micro-array data analysis [13], document clustering [25], chemometrics [15], information retrieval [8] and biology applications [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "For further applications, see the references in the following papers [1, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 3, "context": "For further applications, see the references in the following papers [1, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 16, "context": "We will consider the following version of the NMF problem, which measures the reconstruction error using the Frobenius norm [17]:", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "\u2022 The objective function converges to a limit point and the values are non-increasing across the updates, as shown by Lee and Seung [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "Algorithm 1 is an example of the kind of multiplicative update procedure used, for instance, by Lee and Seung [17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 142, "endOffset": 146}, {"referenceID": 2, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "Besides multiplicative updates, other algorithms have been proposed to solve the NMF problem based on projected gradient [18], block pivoting [14], sequential constrained optimization [3] and greedy coordinate-descent [11].", "startOffset": 218, "endOffset": 222}, {"referenceID": 5, "context": "The nonnegative decomposition is in general not unique [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "In explicit versions of SNMF [10, 7], one can set the sparsities of the matrix factors W ,H directly.", "startOffset": 29, "endOffset": 36}, {"referenceID": 6, "context": "In explicit versions of SNMF [10, 7], one can set the sparsities of the matrix factors W ,H directly.", "startOffset": 29, "endOffset": 36}, {"referenceID": 12, "context": "On the other hand, in implicit versions of SNMF [13, 22], the sparsity is controlled via a regularization parameter and is often hard to tune to specified sparsity values a priori.", "startOffset": 48, "endOffset": 56}, {"referenceID": 21, "context": "On the other hand, in implicit versions of SNMF [13, 22], the sparsity is controlled via a regularization parameter and is often hard to tune to specified sparsity values a priori.", "startOffset": 48, "endOffset": 56}, {"referenceID": 9, "context": "In this paper, we consider the explicit sparse NMF formulation proposed by Hoyer [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "The sparse NMF problem formulated by Hoyer [10] with sparsity on matrix W is as follows:", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "Desirable properties for sparsity measures have been previously explored [12] and it satisfies all of these properties for our problem formulation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "A gradient descent-based algorithm called Nonnegative Matrix Factorization with Sparseness Constraints (NMFSC) to solve SNMF was proposed [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "Heiler and Schn\u00f6rr[7] proposed two new algorithms which also solved this problem by sequential cone programming and utilized general purpose solvers like MOSEK (http://www.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "This problem has been previously considered [10], and a heuristic to solve it was proposed which we will henceforth refer to as the Projection-heuristic.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "Previous approaches for solving SNMF [10, 7] use batch methods to solve for sparsity constraints.", "startOffset": 37, "endOffset": 44}, {"referenceID": 6, "context": "Previous approaches for solving SNMF [10, 7] use batch methods to solve for sparsity constraints.", "startOffset": 37, "endOffset": 44}, {"referenceID": 20, "context": "Proof: To apply Theorem 5 from Nesterov\u2019s paper [21], we note the following transformations: Q = \u2297i=1Qi where Qi \u2208 R and Qi = {\u2016Wj\u20162 \u2264 1 \u2229 \u2016Wj\u20161 = k \u2229W \u2265 0}.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "Initialize the matrix H to uniform random entries in [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 17, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 13, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 2, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 10, "context": "However, we can use other NNLS solvers [18, 14, 3, 11] to solve for matrix H.", "startOffset": 39, "endOffset": 54}, {"referenceID": 9, "context": "\u2022 Sparsity constraints: We have primarly considered the sparse NMF model as formulated by Hoyer [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "This has been generalized by Heiler and Schn\u00f6rr [7] by relaxing the sparsity constraints to lie in user-defined intervals.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "Note that, we can handle this formulation [7] by making a trivial change to Algorithm 3.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "In this section, we compare the performance of our algorithm with the state-of-the-art NMFSC and TPC algorithms [10, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": "In this section, we compare the performance of our algorithm with the state-of-the-art NMFSC and TPC algorithms [10, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 9, "context": "We compare our Sparse-opt (Algorithm 2) routine with the competing Projectionheuristic [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Input coefficients are generated by drawing samples uniformly at random from [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 9, "context": "SSNMF versus NMFSC: To ensure fairness, we removed logging information from NMFSC code [10] and only computed the objective for equivalent number of matrix updates as SSNMF.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Other SNMF formulations have been considered by Hoyer [9], M\u00f8rup et al.", "startOffset": 54, "endOffset": 57}, {"referenceID": 19, "context": "[20] , Kim and Park [13], Pascual-Montano et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[20] , Kim and Park [13], Pascual-Montano et al.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "[22] (nsNMF) and Peharz and Pernkopf [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[22] (nsNMF) and Peharz and Pernkopf [23].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "We note that our sparsity measure has all the desirable properties, extensively discussed by Hurley and Rickard [12], except for one (\u201ccloning\u201d).", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "[22] claim that the SNMF formulation of Hoyer, as given by problem (3) does not capture the variance in the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "However, some transformation of the sparsity values is required to properly compare the two formulations [10, 22].", "startOffset": 105, "endOffset": 113}, {"referenceID": 21, "context": "However, some transformation of the sparsity values is required to properly compare the two formulations [10, 22].", "startOffset": 105, "endOffset": 113}, {"referenceID": 22, "context": "Peharz and Pernkopf [23] propose to tackle the L0 norm constrained NMF directly by projecting from intermediate unconstrained solutions to the required L0 constraint.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "Our approach can be extended to other NMF variants [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 18, "context": "A different research direction would be to scale our algorithm to handle large datasets by chunking [19] and/or take advantage of distributed computational settings.", "startOffset": 100, "endOffset": 104}], "year": 2017, "abstractText": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L0 norm, however its optimization is NP-hard. Mixed norms, such as L1/L2 measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L1 norm. However, present algorithms designed for optimizing the mixed norm L1/L2 are slow and other formulations for sparse NMF have been proposed such as those based on L1 and L0 norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "creator": "LaTeX with hyperref package"}}}