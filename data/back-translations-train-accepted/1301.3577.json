{"id": "1301.3577", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Saturating Auto-Encoders", "abstract": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "histories": [["v1", "Wed, 16 Jan 2013 04:07:46 GMT  (113kb,D)", "http://arxiv.org/abs/1301.3577v1", null], ["v2", "Wed, 13 Feb 2013 00:28:01 GMT  (235kb,D)", "http://arxiv.org/abs/1301.3577v2", null], ["v3", "Wed, 20 Mar 2013 15:37:33 GMT  (776kb,D)", "http://arxiv.org/abs/1301.3577v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rostislav goroshin", "yann lecun"], "accepted": true, "id": "1301.3577"}, "pdf": {"name": "1301.3577.pdf", "metadata": {"source": "CRF", "title": "Saturating Auto-Encoder", "authors": ["Rostislav Goroshin", "Yann LeCun"], "emails": ["goroshin@cs.nyu.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "It consists of an encoder that triggers a hidden (or latent) representation, and a decoder that attempts to reconstruct the input with the hidden representation. Encoders can implement many dimensionality techniques such as PCA and Sparse. [5] These costs are just a proxy for the true goal. Encoders can include many dimensionality techniques."}, {"heading": "2 Hidden Variable Regularization", "text": "Several auto-encoder variants that regulate their latent states have been proposed, including the sparse auto-encoder and the contractive auto-encoder [1] [2] [3]. The sparse auto-encoder contains an over-complete base in the encoder and imposes a sparse penalty (usually L1) for the hidden activations. This penalty prevents the auto-encoder from learning to reconstruct all possible points in the input space, and concentrates the expressive power of the auto-encoder on the representation of the data manifold. Likewise, the contractive auto-encoder avoids trivial solutions by introducing an auxiliary penalty that measures the square Frobenius standard of latent representation of the jacobic in relation to the input. This promotes a constant latent representation, with the exception of training samples, where it is contradicted by the reconstruction term."}, {"heading": "2.1 Saturating Auto-Encoder through Complementary Nonlinearities", "text": "Our goal is to introduce a simple new regulator that explicitly causes reconstruction errors for inputs that are not close to the data diversity. Let's consider activation functions with at least one flat region, such as shrinkage, linear equilibrium, and saturated linearity (Figure 1). Auto-encoders with such nonlinearities lose their ability to accurately reconstruct inputs that cause activations in the saturation regime of their activation functions (Figure 1). In this sense, we introduce a penalty for the form fc (Figure d = 1W e ijxj + bi), which encourages the argument to be in the saturation regime of the activation function (f). We call this the saturating auto-encoder (SATAE). For activation functions with zero-gradient regime (s), complementary nonlinearity (fc) can be defined as the distance from the next grounding region."}, {"heading": "3 Effect of the Saturation Regularizer", "text": "We will study the effect of the saturation regulator on auto-encoders with a variety of activation functions. It will be shown that the choice of activation function is an important factor in determining the type of basis that SATAE learns. First, we will present results on toy data in two dimensions, followed by results on higher-dimensional image data."}, {"heading": "3.1 Visualizing the Energy Landscape", "text": "For low-dimensional rooms (Rn, where n \u2264 3), we can evaluate the reconstruction error on a regular grid to visualize the portions of the room that are well represented by the auto encoder. Specifically, we can calculate E (x) = 12 x \u2212 xr \u00b2 2 for all x within a limited area of the entrance space. Ideally, the reconstruction energy will be low for all x that are in training and are high elsewhere. Figures 2 and 3 show the resulting reconstruction energy for inputs x, x, x, and \u2212 1 \u2264 xi, 1. Black corresponds to a low reconstruction energy. Training data consists of a one-dimensional manifold in yellow. Figure 2 shows a toy example for a SATAE system that uses ten base vectors and a shrink activation function."}, {"heading": "3.2 SATAE-shrink", "text": "Consider a SATAE with a shrink activation function and a shrink parameter \u03bb. The corresponding complementary nonlinearity, derived from Equation 1, is indicated elsewhere by: shrinkc (x) = {abs (x), | x | > \u03bb 0. Note that shrinkc (W ex + be) = abs (shrink (W ex + be))), which corresponds to an L1 penalty at activation, so this ambiguity can be avoided by normalizing the decoder weights to the standard. SATAE shrinkage is expected to learn characteristics similar to those that occur with a sparse L1 regulation. This ambiguity can be avoided by normalizing the decoder weights to the standard. SATAE shrinkage is expected to learn characteristics similar to those obtained with a sparse auto encoder."}, {"heading": "3.3 SATAE-saturated-linear", "text": "The SATAE with saturated linear activation function learns a completely different set of features. Empirically, increasing the regularization penalty leads to more localized feature detectors. To understand why these feature detectors arise, we look at a dataset in which the variables take binary values (e.g. MNIST).The scaled identity basis is a global minimizer of Equation 2 when a saturated linear activation function is used. Such a basis can perfectly reconstruct any binary input while working exclusively in the saturated regions of the activation function, which does not result in a saturation penalty. In contrast to SATAE shrinkage, the SATAE saturated linear function has a similar but less pronounced effect: more localized features are achieved, but of greater variety (Figure 4b)."}, {"heading": "4 Experimental Details", "text": "In all experiments, data samples were normalized by subtracting the mean and dividing them by the standard deviation of the dataset. Experiments with MNIST were performed with 200 base elements, while experiments with natural image fields used only 100 base elements. Decoder base elements of SATAEs with shrinkage and linear nonlinearity were projected onto the Unit Sphere after each 10 stochastic gradient updates. SATAEs using saturated linear activation function were trained with bound weights. All presented results were obtained by stochastic gradient descent."}, {"heading": "5 Discussion", "text": "We have shown that by using different activation functions, drastically different feature sets of SATAEs are learned. Hybrid SATAEs that use a mixture of activation functions are also possible."}, {"heading": "5.1 Relationship with the Contractive Auto-Encoder", "text": "The regulator imposed by the contractive auto encoder (CAE) can be expressed as follows: \u2211 ij (\u2202 hi \u2202 xj) 2 = dh \u2211 i f \u2032 (d \u2211 j = 1 W eijxj + bi) 2 \u0445 W ei \u0445 2, where x is a d-dimensional data vector, f \u2032 (\u00b7) is the derivative of f (\u00b7), bi is the bias of the ith coding unit, and W ei denotes the ith series of the coding weight matrix. Therefore, the first term in the above equation attempts to adjust the weights so that the activations are forced into the low gradient regime (saturation regime) of non-linearity, but is defined only for differentiable activation functions."}, {"heading": "5.2 Relationship with the Sparse Auto-Encoder", "text": "In Section 3.2 it was shown that SATAEs with shrink or rectified linear activation functions correspond to a sparse auto-encoder. Like saturation penalty, the saturation penalty can be applied at any point in a deep network at the same computing cost. In contrast to saturation penalty, the saturation penalty is adapted to the non-linearity of the layer to which it is applied."}], "references": [{"title": "Efficient Learning of Sparse Representations with an Energy- Based Model, in J", "author": ["Marc\u2019Aurelio Ranzato", "Christopher Poultney", "Sumit Chopra", "Yann LeCun"], "venue": "Platt et al. (Eds), Advances in Neural Information Processing Systems (NIPS 2006),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition", "author": ["Marc\u2019Aurelio Ranzato", "Fu-Jie Huang", "Y-Lan Boureau", "Yann LeCun"], "venue": "Proc. Computer Vision and Pattern Recognition Conference", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "Proceedings of the Twenty-eight International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "Proceedings of the 25th International Conference on Machine Learning (ICML\u20192008),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1", "author": ["Bruno A. Olhausen", "David J. Field"], "venue": "Vision Research", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Learning Fast Approximations of Sparse Coding", "author": ["Karol Gregor", "Yann LeCun"], "venue": "Proc. International Conference on Machine learning (ICML\u201910),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Auto-encoders can implement many dimensionality reduction techniques such as PCA and Sparse Coding (SC) [5] [6] [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "Auto-encoders can implement many dimensionality reduction techniques such as PCA and Sparse Coding (SC) [5] [6] [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "In recent years, renewed interest in auto-encoders networks has mainly been due to their empirical success in unsupervised feature learning [1] [2] [3] [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "In recent years, renewed interest in auto-encoders networks has mainly been due to their empirical success in unsupervised feature learning [1] [2] [3] [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "In recent years, renewed interest in auto-encoders networks has mainly been due to their empirical success in unsupervised feature learning [1] [2] [3] [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "In recent years, renewed interest in auto-encoders networks has mainly been due to their empirical success in unsupervised feature learning [1] [2] [3] [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "This approach is exemplified by the Contractive and Sparse Auto-Encoders [3] [1] [2].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "This approach is exemplified by the Contractive and Sparse Auto-Encoders [3] [1] [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "This approach is exemplified by the Contractive and Sparse Auto-Encoders [3] [1] [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "Several auto-encoder variants which regularize their latent states have been proposed, they include the sparse auto-encoder and the contractive auto-encoder [1] [2] [3].", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "Several auto-encoder variants which regularize their latent states have been proposed, they include the sparse auto-encoder and the contractive auto-encoder [1] [2] [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "Several auto-encoder variants which regularize their latent states have been proposed, they include the sparse auto-encoder and the contractive auto-encoder [1] [2] [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "It has been noted in [3] that these two approaches are strongly related.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "Furthermore, the complexity of computing the Jacobian is O(d\u00d7 dh) [3], compared to the O(dh) for the saturation penalty.", "startOffset": 66, "endOffset": 69}], "year": 2017, "abstractText": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE\u2019s ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "creator": "LaTeX with hyperref package"}}}