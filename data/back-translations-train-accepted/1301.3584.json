{"id": "1301.3584", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Revisiting Natural Gradient for Deep Networks", "abstract": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "histories": [["v1", "Wed, 16 Jan 2013 04:47:02 GMT  (16kb)", "http://arxiv.org/abs/1301.3584v1", null], ["v2", "Tue, 22 Jan 2013 03:57:04 GMT  (374kb,D)", "http://arxiv.org/abs/1301.3584v2", null], ["v3", "Wed, 23 Jan 2013 14:59:04 GMT  (375kb,D)", "http://arxiv.org/abs/1301.3584v3", null], ["v4", "Wed, 13 Mar 2013 19:13:08 GMT  (1006kb,D)", "http://arxiv.org/abs/1301.3584v4", "The title, wording and structure of the paper was slightly changed to better reflect the final conclusions. We improved notation, providing more details where they were missing"], ["v5", "Fri, 20 Dec 2013 19:29:42 GMT  (234kb,D)", "http://arxiv.org/abs/1301.3584v5", null], ["v6", "Tue, 7 Jan 2014 18:11:36 GMT  (224kb,D)", "http://arxiv.org/abs/1301.3584v6", null], ["v7", "Mon, 17 Feb 2014 16:29:27 GMT  (228kb,D)", "http://arxiv.org/abs/1301.3584v7", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["razvan pascanu", "yoshua bengio"], "accepted": true, "id": "1301.3584"}, "pdf": {"name": "1301.3584.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 1.35 84v1 [cs.LG] 1 6Ja n"}, {"heading": "1 Introduction", "text": "Several recent papers have attempted to address the question of the use of better optimization techniques for machine learning, especially for the formation of deep architectures or neural networks of various kinds. Hessian-free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient Descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They can usually be divided into two different categories depending on what their theoretical standpoint looks like: those using second-order information and those using the geometry of the underlying parameters are diverse, and within each group they differentiate further by the approximations they use to scale up."}, {"heading": "2 Natural Gradient", "text": "The natural gradient can be traced back to Amari's work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), although a deeper idea can be found in Amari (1998); Park et al. (2000). Le Roux et al. (2007) introduces a different formulation of the algorithm, which, although it is called the same, is differently motivated, and it is not equivalent to Amari's version. We will discuss this discussion in Section 4.The main intuition of the natural gradient is that the model describes a family of density functions parameterized by the parameterization of every single value of that family. This family of density functions defines a low-dimensional Riemannian multiplicity in RP (where P is the dimensionality of the disorder), which can be traced by changing the magnitude."}, {"heading": "3 Natural Gradient for Neural Networks", "text": "In order to define the natural gradient for neural networks, we must rely on their probability interpretation (q = q function), in which a similarity measurement is induced between different parameterizations of the model. The family of probability density functions corresponding to any model (as if by changing the parameters \u03b8) defines a Riemannian Multiplicity (whose metric is the Fisher information matrix). Below, we will consider the most typical three output functions and the probable interpretation induced by them. Note that similar derivatives were performed in Park et al. (2000), but we choose to repeat them in order to clarify some misconceptions regarding the shape of the natural gradient metric, which is often an uncentered covariance of the gradient. We will use Equation (5) to define the loss, whereby z becomes our random variable z = (x, t), where x is the input of the model and the target."}, {"heading": "3.1 Linear activation function", "text": "In the case of linear outputs, we assume that each entry of the vector t, ti results from a Gaussian distribution centered around yi (x) with a certain standard deviation \u03b2. It follows that: p (t | x, \u03b8) = o \u00b2 i = 1N (ti | y (x, \u03b8) i, \u03b2 2) (8) G = Ex \u00b2 n (t | y (x, \u03b8), \u03b22I) [Et \u00b2 n (t | y (x, \u03b8) i, \u03b2 2 \u0432) T (v = y (ti | y) i, \u03b2 2 \u0432)]]] = Ex \u00b2 n (v = 1 [t | y (x, TB), \u03b22I) [Et \u00b2 n (t \u2212 yi) T (ti \u2212 yi) T (xi \u2212 yi) T (xi yi) T (ti \u2212 yi) T (ti \u2212 yi) T (ti) T (ti \u2212 yi) T I (ti \u2212 yi) T (ti) T (ti \u2212 yi) T (xi Y (xi) Y (xi Y) T (xi Y (xi yi) T (xi) T (xi yi T (xi yi) T (xi T \u2212 yi) T (xi T (xi T \u2212 yi) T (xi T \u2212 yi) T (xi T \u2212 yi) T (xi Y (xi T \u2212 yi) T (xi Y (xi T \u2212 yi) T (xi Y (xi T \u2212 yi) T (xi Y (xi Y) T (xi Y (xi T \u2212 yi) T (xi Y (xi Y) T \u2212 yi) T (xi Y (xi Y (xi Y) T (xi Y (xi T \u2212 yi) T \u2212 yi) T (xi Y (xi Y (xi Y) T (xi Y (xi Y) T (xi Y (xi Y), T \u2212 yi), T (xi Y (xi Y (xi Y), T (xi Y (xi), T (xi Y (xi), T (xi Y (xi Y), T (xi Y (xi), T (xi Y (xi), T (xi Y), T (xi Y (xi), T (xi Y (xi Y), T ("}, {"heading": "3.2 Sigmoid activation function", "text": "In the case of sigmoid units, i, e, y = sigmoid (r), we assume a binomial distribution which gives us the following result: p (t | x) = 1 \u2212 yi (1 \u2212 yi) 1 \u2212 ti (10) log p gives us the usual cross entropy error used in sigmoid units. We can calculate the Fisher information matrix as follows: G = Ex-q [Et-p (t | x) [\u2211 o i = 1 (ti \u2212 yi) 2y2i (1 \u2212 yi) 2 (\u00a5yi) 2 (\u00a5yi \u043d) T-yi \u043d] = Ex-q-q] = 1-yi (1 \u2212 yi) (\u0445 yi-xi) T-yi \u043d] = Ex-q [JTydiag (1 y (1 \u2212 y) Jy] (11)"}, {"heading": "3.3 Softmax activation function", "text": "In the case of the Softmax activation function, y = Softmax (r), p (t | x) takes the form of a multinomial: p (t | x) = o-iytii (12) G = Ex-q [o-i1yi (\u2202 yi \u2202 \u03b8) T-yi \u2202 \u03b8] (13)"}, {"heading": "4 A different Natural Gradient", "text": "In Le Roux et al. (2007), a different approach is used to derive the natural gradient. Specifically, it is assumed that the gradients calculated using different mini-lots are distributed around the true gradient with a certain covariance matrix according to a Gaussian approach. By using the uncertainty provided by C, we can correct the step we take to maximize the probability of a downward movement in the generalization error (expected negative log probability), resulting in a formula similar to that of the natural gradient (Equation (14). G = X = X = X = X = 1 n x x (x)) x (Gradient p (t (i)) x) x leads."}, {"heading": "5 The extended Gauss-Newton approximation of the Hessian", "text": "Hessisch-Freie as well as Krylov Subspace Descent rely on the extended Gauss-Newton approximation of the Hessian instead of the actual Hessian. The reason is not mathematical, since the calculation can be done at the same speed, but rather better behavior during learning. This is usually assumed to be caused by the fact that the Gauss-Newton method is positive, so one does not have to worry about negative curves. In this section we show that the extended Gauss-Newton approximation corresponds perfectly to the natural gradient metric, and therefore one can choose this specific approximation by considering both algorithms as implementations of natural gradients rather than typical methods. To do this, we follow Schraudolph (2002) and rewrite the Hessian as the sum of the matrices described in the equation."}, {"heading": "6 Natural Gradient Descent as a learning algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Natural Gradient versus Natural Gradient", "text": "The two definitions of the algorithm (using the uncentered covariance of gradients) versus the manifold metric have different practical implications. While there is no detailed published comparison between the two variants, there are reasons to expect one method to work better than the other. One such reason has to do with numerical stability. G can be expressed as the sum of n \u00d7 o outer products (where n is the size of the minibatch over which we estimate the matrix), while U is a sum of only n outer products. Since the number of terms in these sums represents an upper limit on the rank of each matrix, it follows that one might expect U to rank lower than G. This observation is also made by Schraudolph (2002) to motivate the extended Gauss-Newton matrix as a middle layer between the natural gradient and the true Hessen."}, {"heading": "6.2 Natural Gradient and Second Order Methods", "text": "Although it is usually assumed that gradients are a second method, it is more useful and probably more correct to consider them as a first method of how we minimize the curvature of the manifold rather than the error function that we are trying to minimize. To make this distinction clear, we can try to see what information each of the matrices we carry (as used in Roux and Fitzgibbon (2010) for Newton's and Le Roux's methods), how the matrix is used to decide how the gradients change when moving in the parameter spaces. The centered or uncentered covariance of the gradients answers the question: If I change my input x by a bit, how the gradients change, the matrix provides information about how the gradients change, how the gradients move in the input space."}, {"heading": "7 Conclusion and Future Work", "text": "Natural Gradient is a powerful tool for learning complex nonlinear functions, because while it retains characteristics of a second-order method (such as fast convergence), it is easily suited for the classical learning framework. For example, it can easily be used either as an online learning algorithm or as a batch algorithm."}], "references": [{"title": "Differential geometrical methods in statistics", "author": ["S. Amari"], "venue": "Lecture notes in statistics, 28.", "citeRegEx": "Amari,? 1985", "shortCiteRegEx": "Amari", "year": 1985}, {"title": "Information geometry of Boltzmann machines", "author": ["S. Amari", "K. Kurata", "H. Nagaoka"], "venue": "IEEE Trans. on Neural Networks, 3, 260\u2013271.", "citeRegEx": "Amari et al\\.,? 1992", "shortCiteRegEx": "Amari et al\\.", "year": 1992}, {"title": "Natural gradient works efficiently in learning", "author": ["Amari", "S.-I."], "venue": "Neural Comput., 10(2), 251\u2013276.", "citeRegEx": "Amari and S..I.,? 1998", "shortCiteRegEx": "Amari and S..I.", "year": 1998}, {"title": "Improved Preconditioner for Hessian Free Optimization", "author": ["O. Chapelle", "D. Erhan"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning.", "citeRegEx": "Chapelle and Erhan,? 2011", "shortCiteRegEx": "Chapelle and Erhan", "year": 2011}, {"title": "Neural learning in structured parameter spaces - natural Riemannian gradient", "author": ["S. ichi Amari"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Amari,? \\Q1997\\E", "shortCiteRegEx": "Amari", "year": 1997}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N. Le Roux", "Manzagol", "P.-A.", "Y. Bengio"], "venue": "Technical Report 1299, D\u00e9partement d\u2019informatique et recherche op\u00e9rationnelle, Universit\u00e9 de Montr\u00e9al.", "citeRegEx": "Roux et al\\.,? 2007", "shortCiteRegEx": "Roux et al\\.", "year": 2007}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N. Le Roux", "Manzagol", "P.-A.", "Y. Bengio"], "venue": "NIPS\u201907.", "citeRegEx": "Roux et al\\.,? 2008", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Improving first and second-order methods by modeling uncertainty", "author": ["N. Le Roux", "Y. Bengio", "A. Fitzgibbon"], "venue": "Optimization for Machine Learning. MIT Press.", "citeRegEx": "Roux et al\\.,? 2011", "shortCiteRegEx": "Roux et al\\.", "year": 2011}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "ICML, pages 735\u2013742.", "citeRegEx": "Martens,? 2010", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S.J. Wright"], "venue": "Springer.", "citeRegEx": "Nocedal and Wright,? 2000", "shortCiteRegEx": "Nocedal and Wright", "year": 2000}, {"title": "Adaptive natural gradient learning algorithms for various stochastic models", "author": ["H. Park", "Amari", "S.-I.", "K. Fukumizu"], "venue": "Neural Networks, 13(7), 755 \u2013 764.", "citeRegEx": "Park et al\\.,? 2000", "shortCiteRegEx": "Park et al\\.", "year": 2000}, {"title": "Fast exact multiplication by the hessian", "author": ["B.A. Pearlmutter"], "venue": "Neural Computation, 6, 147\u2013160.", "citeRegEx": "Pearlmutter,? 1994", "shortCiteRegEx": "Pearlmutter", "year": 1994}, {"title": "A fast natural newton method", "author": ["N.L. Roux", "A.W. Fitzgibbon"], "venue": "J. F\u00fcrnkranz and T. Joachims, editors, ICML, pages 623\u2013630. Omnipress.", "citeRegEx": "Roux and Fitzgibbon,? 2010", "shortCiteRegEx": "Roux and Fitzgibbon", "year": 2010}, {"title": "Fast curvature matrix-vector products", "author": ["N.N. Schraudolph"], "venue": "ICANN, pages 19\u201326.", "citeRegEx": "Schraudolph,? 2001", "shortCiteRegEx": "Schraudolph", "year": 2001}, {"title": "Fast curvature matrix-vector products for second-order gradient descent", "author": ["N.N. Schraudolph"], "venue": "Neural Computation, 14(7), 1723\u20131738.", "citeRegEx": "Schraudolph,? 2002", "shortCiteRegEx": "Schraudolph", "year": 2002}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "ICML, pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Krylov Subspace Descent for Deep Learning", "author": ["O. Vinyals", "D. Povey"], "venue": "AISTATS.", "citeRegEx": "Vinyals and Povey,? 2012", "shortCiteRegEx": "Vinyals and Povey", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian.", "startOffset": 55, "endOffset": 70}, {"referenceID": 16, "context": "First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian.", "startOffset": 99, "endOffset": 124}, {"referenceID": 8, "context": "Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.", "startOffset": 26, "endOffset": 91}, {"referenceID": 15, "context": "Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.", "startOffset": 26, "endOffset": 91}, {"referenceID": 3, "context": "Hessian-Free optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.", "startOffset": 26, "endOffset": 91}, {"referenceID": 16, "context": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al.", "startOffset": 59, "endOffset": 84}, {"referenceID": 10, "context": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms.", "startOffset": 111, "endOffset": 192}, {"referenceID": 8, "context": "One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).", "startOffset": 210, "endOffset": 225}, {"referenceID": 16, "context": "One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).", "startOffset": 254, "endOffset": 279}, {"referenceID": 9, "context": "Section 6 We use the wording \u201cHessian Free\u201c to refer to the particular algorithm proposed by Martens (2010), and this should not be confused with the more generic truncated gradient method (Nocedal and Wright, 2000).", "startOffset": 189, "endOffset": 215}, {"referenceID": 0, "context": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).", "startOffset": 117, "endOffset": 700}, {"referenceID": 0, "context": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012).", "startOffset": 117, "endOffset": 733}, {"referenceID": 0, "context": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficiently products between Jacobian or Hessian matrices and vectors.", "startOffset": 117, "endOffset": 969}, {"referenceID": 0, "context": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficiently products between Jacobian or Hessian matrices and vectors. The advantage of this pipeline is that it considers the exact Hessian and only inverts it approximately without explicitly storing the matrix in memory, instead of other approaches that do a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal). Our contribution is two-fold. We first show that the Hessian-Free approach of Martens (2010) 1 (and implicitly Krylov Subspace Descent (KSD) algorithm) can be casted into into the framework of natural gradient, showing how these methods could be seen as doing natural gradient rather then second-order optimization.", "startOffset": 117, "endOffset": 1479}, {"referenceID": 0, "context": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), Natural Gradient descent (ichi Amari, 1997; Park et al., 2000; Le Roux et al., 2008; Le Roux et al., 2011) are just a few of such recently proposed algorithms. They usually can be split in two different categories according to their theoretical standpoint: those that make use of second-order information and those that use the geometry of the underlying parameter manifold (natural gradient). Within each group they further differentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), finetuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficiently products between Jacobian or Hessian matrices and vectors. The advantage of this pipeline is that it considers the exact Hessian and only inverts it approximately without explicitly storing the matrix in memory, instead of other approaches that do a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal). Our contribution is two-fold. We first show that the Hessian-Free approach of Martens (2010) 1 (and implicitly Krylov Subspace Descent (KSD) algorithm) can be casted into into the framework of natural gradient, showing how these methods could be seen as doing natural gradient rather then second-order optimization. The second contribution of the paper is to look at the Natural Gradient Descent as a learning algorithm and suggest how it contrasts to a typical second order optimization technique. We also provide a detailed derivation of Natural Gradient that aims at eliminating possible confusion regarding the different formulation of the algorithm that are found in the literature. The organization of the paper is as follows. In section 2 we describe the basic concepts behind Natural Gradient Descent. Section 4 talks about the different variants of the algorithm, while section 5 describes the relationship between Hessian-Free optimization and Natural Gradient. Section 6 We use the wording \u201cHessian Free\u201c to refer to the particular algorithm proposed by Martens (2010), and this should not be confused with the more generic truncated gradient method (Nocedal and Wright, 2000).", "startOffset": 117, "endOffset": 2466}, {"referenceID": 0, "context": "2 Natural Gradient Natural Gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al.", "startOffset": 95, "endOffset": 108}, {"referenceID": 1, "context": "2 Natural Gradient Natural Gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al.", "startOffset": 156, "endOffset": 194}, {"referenceID": 0, "context": "2 Natural Gradient Natural Gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al.", "startOffset": 58, "endOffset": 261}, {"referenceID": 0, "context": "2 Natural Gradient Natural Gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000). Le Roux et al.", "startOffset": 58, "endOffset": 281}, {"referenceID": 0, "context": "2 Natural Gradient Natural Gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; ichi Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000). Le Roux et al. (2007) introduces a different formulation of the algorithm, which even though is called the same, is motivated differently and it is not equivalent with Amari\u2019s version.", "startOffset": 58, "endOffset": 304}, {"referenceID": 10, "context": "Note that similar derivations have been done in Park et al. (2000), but we choose to redo them in order to clarify some misconceptions regarding the form Natural Gradient metric which is often assumed to be the uncentered covariance of the gradients.", "startOffset": 48, "endOffset": 67}, {"referenceID": 5, "context": "4 A different Natural Gradient In Le Roux et al. (2007) a different approach is taken to derive Natural Gradient.", "startOffset": 37, "endOffset": 56}, {"referenceID": 3, "context": "i ( \u2202 log p(t|x) \u2202\u03b8 \u2212 \u3008 \u2202 log p(t|x) \u2202\u03b8 T ( \u2202 log p(t|x) \u2202\u03b8 \u2212 \u3008 \u2202 log p(t|x) \u2202\u03b8 \u3009) (14) While the probabilistic derivation requires the use of the centered covariance (equation (14), in Le Roux et al. (2007) it is argued that using the uncentered covariance U is equivalent up to a constant resulting in a simplified formula (15) which is usually confused with the metric derived by Amari.", "startOffset": 189, "endOffset": 208}, {"referenceID": 13, "context": "To do so, we follow Schraudolph (2002) and re-write the Hessian as the sum of matrices described in equation (16), where i ranges over the o outputs of the network, and ri denotes the subnetwork that produces the i-th element of r.", "startOffset": 20, "endOffset": 39}, {"referenceID": 13, "context": "To do so, we follow Schraudolph (2002) and re-write the Hessian as the sum of matrices described in equation (16), where i ranges over the o outputs of the network, and ri denotes the subnetwork that produces the i-th element of r. The extended Gauss-Newton approximation of the Hessian GN is represented by the first term of the equation. One justification for picking this term is that \u2202 log p(t|x) \u2202ri approaches 0 as L approaches 0 (for the typical pairings of activation functions and error measures), making the approximation exact close to the solution. The first term is also well behaved, being positive semi-definite by construction. For more details see Schraudolph (2002).", "startOffset": 20, "endOffset": 684}, {"referenceID": 13, "context": "This observation is also made by Schraudolph (2002) to motivate the extended Gauss-Newton matrix as middle ground between Natural Gradient and the true Hessian.", "startOffset": 33, "endOffset": 52}, {"referenceID": 13, "context": "This observation is also made by Schraudolph (2002) to motivate the extended Gauss-Newton matrix as middle ground between Natural Gradient and the true Hessian. Note that Schraudolph (2002) 7", "startOffset": 33, "endOffset": 190}], "year": 2017, "abstractText": "The aim of this paper is two-folded. First we intend to show that Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of Natural Gradient Descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive Natural Gradient from basic principles, contrasting the difference between the two version of the algorithm that are in the literature.", "creator": "LaTeX with hyperref package"}}}