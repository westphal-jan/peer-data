{"id": "1301.3666", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Zero-Shot Learning Through Cross-Modal Transfer", "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "histories": [["v1", "Wed, 16 Jan 2013 12:01:34 GMT  (450kb,D)", "https://arxiv.org/abs/1301.3666v1", null], ["v2", "Wed, 20 Mar 2013 00:44:08 GMT  (455kb,D)", "http://arxiv.org/abs/1301.3666v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["richard socher", "milind ganjoo", "christopher d manning", "andrew y ng"], "accepted": true, "id": "1301.3666"}, "pdf": {"name": "1301.3666.pdf", "metadata": {"source": "CRF", "title": "Zero-Shot Learning Through Cross-Modal Transfer", "authors": ["Richard Socher", "Milind Ganjoo", "Hamsa Sridhar", "Osbert Bastani", "Christopher D. Manning", "Andrew Y. Ng"], "emails": ["richard@socher.org,", "ang}@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The ability to classify instances of an invisible visual class, called zero-shot learning, is useful in many situations. There are many types, products or activities without any described data, and new visual categories, such as the latest gadgets or car models that are frequently introduced. In this work, we show how to use the enormous amount of knowledge about the visual world in natural language to classify invisible objects. We try to identify people's ability to identify invisible objects, even if the only knowledge about that object comes from reading them. For example, after reading the description of a two-wheeled electric vehicle that is controlled by a stick that can move while it is on top, many would be able to identify a new object that is briefly confused because the new object represents a previously observed object class. We are introducing a zero-shot model that includes both seen and invisible classes."}, {"heading": "2 Related Work", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3 Word and Image Representations", "text": "Distribution approaches are widely used to capture semantic similarities between words. In these approaches, words are presented as vectors of distribution characteristics - mostly in their coexistence with words in context [25, 9, 1, 32]. These representations have proven very effective in tasks of natural language processing such as meaning processing [29], thesaurus extraction [23, 8] and cognitive modelling [20]. We initialize all word vectors with pre-trained 50-dimensional word vectors from the unattended model by Huang et al. [15]. Using the free Wikipedia text, your model learns word vectors by predicting how likely each word is to occur in its context. Your model uses both local contexts in the window around each word as well as global document contexts. Similar to other local occurrences based on vector space models."}, {"heading": "4 Projecting Images into Semantic Word Spaces", "text": "In order to learn semantic relationships and class affiliation of images, we project the image functions of vectors into the 50-dimensional word space. During training and testing, we look at a number of classes Y. Some of the classes y in this sentence have available training data, others are zero-shot classes without training data. We respectfully define the former as the seen classes Ys and the latter as the unseen classes Yu. Let us consider W = Ws \"Wu\" as word vectors that capture distribution information for seen and unseen visual classes. All training images x (i) \"Xy\" are assigned to a seen class y \"Ys are assigned to the word vector that corresponds to the class name. To train this assignment, we minimize the following objective function with respect to the matrix\" R50 \"x x\" x \"F: J (\" s) \"Xy\" Ys, \"images of seen classes y\" and \"Xy\" of classes, \"the\" seen classes Xy. \""}, {"heading": "5 Zero-Shot Learning Model", "text": "In this section we will first give an overview of our model and then describe each of its components. In general, we want to predict p (y | x), the conditional probability for both seen and unseen classes y-Ys-Yu given an image x. since standard classifiers will never predict a class that has no educational examples, we introduce a binary visibility variable that indicates whether animations in a seen or unseen class V-Ys, u-Ys, u-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu-Yu"}, {"heading": "6 Experiments", "text": "We perform most of our experiments with the CIFAR10 dataset, which consists of 10 classes of 5000 32 x 32 x 3 RGB images each. We use the method of unattended feature extraction of Coates and Ng [6] to obtain a 12,800-dimensional feature vector for each image. In the following experiments, we omit the training images of 2 classes for zero-shot analysis."}, {"heading": "6.1 Zero-Shot Classes Only", "text": "In this section, we compare the classification between only two zero-shot classes. We note that if there is no seen class that is even remotely similar to the zero-shot classes, the performance is almost random. In other words, if the two zero-shot classes are the most similar classes and the seen classes do not include the sub-range of the zero-shot classes correctly, the performance is poor. If, for example, cat and dog are taken out of training, the resulting zero-shot classification does not work well because none of the other 8 categories is similar enough to learn good feature mapping. On the other hand, if cat and truck are removed, the cat vectors can be mapped to the word space thanks to the transfer of dogs and trucks, so the performance is very high. Figure 3 shows the performance at different sections for outlier detection. The cutoff is formed on the negative log probability of the limit accuracy of each point in the classification, which can only be seen in the upper part of the classification."}, {"heading": "6.2 Zero-Shot and Seen Classes", "text": "In Fig. 3, we can observe that depending on the threshold that divides images into seen or unseen classes at test time, we can obtain accuracies of trained classes of about 80%. At an accuracy of 70%, unseen classes can be classified with accuracies between 30% and 15%, with a chance probability of 10%."}, {"heading": "7 Conclusion", "text": "We introduced a novel model for common standard and zero-shot classification, based on deeply learned word and image representations; the two key ideas are that (i) the use of semantic word vector representations can help transmit knowledge between categories, even if these representations are learned unsupervised; and (ii) that our Bayesian framework, which initially distinguishes outliers from dots on projected semantic diversity, can help combine both zero-shot and face classification into one framework."}], "references": [{"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Cross-generalization: learning novel classes from a single example by feature replacement", "author": ["E. Bart", "S. Ullman"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N. Tran"], "venue": "In ACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization", "author": ["A. Coates", "A. Ng"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "From Distributional to Semantic Similarity", "author": ["J. Curran"], "venue": "PhD thesis, University of Edinburgh,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "A structured vector space model for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3"], "venue": "In EMNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Visual information in semantic representation", "author": ["Y. Feng", "M. Lapata"], "venue": "In HLT-NAACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Object classification from a single example utilizing class relevance pseudo-metrics", "author": ["M. Fink"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Domain adaptation for Large-Scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Geometric context from a single image", "author": ["D. Hoiem", "A.A. Efros", "M. Herbert"], "venue": "In ICCV,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "LoOP: local Outlier Probabilities", "author": ["H. Kriegel", "P. Kr\u00f6ger", "E. Schubert", "A. Zimek"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "One-shot learning of object", "author": ["Perona L. Fei-Fei", "Fergus"], "venue": "categories. TPAMI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Learning to Detect Unseen Object Classes by Between- Class Attribute Transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "A solution to Plato\u2019s problem: the Latent Semantic Analysis theory of acquisition, induction and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Zero-data learning of new tasks", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "In AAAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Going beyond text: A hybrid image-text approach for measuring word relatedness", "author": ["C.W. Leong", "R. Mihalcea"], "venue": "In IJCNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Automatic retrieval and clustering of similar words", "author": ["D. Lin"], "venue": "In Proceedings of COLING-ACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Dependency-based construction of semantic space models", "author": ["S. Pado", "M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G. Hinton", "T. Mitchell"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Towards cross-category knowledge propagation for learning visual concepts", "author": ["Guo-Jun Qi", "C. Aggarwal", "Y. Rui", "Q. Tian", "S. Chang", "T. Huang"], "venue": "In CVPR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Learning to learn with compound hierarchical-deep models", "author": ["A. Torralba R. Salakhutdinov", "J. Tenenbaum"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Automatic word sense discrimination", "author": ["H. Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "First, images are mapped into a semantic space of words that is learned by a neural network model [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "Unlike previous work on zero-shot learning which can only predict intermediate features or differentiate between various zero-shot classes [26], our joint model can achieve both state of the art accuracy on known classes as well as reasonable performance on unseen classes.", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "pared to related work in knowledge transfer [19, 27] we do not require manually defined semantic or visual attributes for the zero-shot classes.", "startOffset": 44, "endOffset": 52}, {"referenceID": 25, "context": "pared to related work in knowledge transfer [19, 27] we do not require manually defined semantic or visual attributes for the zero-shot classes.", "startOffset": 44, "endOffset": 52}, {"referenceID": 24, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] describe the unseen zero-shot classes by a \u201ccanonical\u201d example or use ground truth human labeling of attributes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "One-Shot Learning One-shot learning [17, 18] seeks to learn a visual object class by using very few training examples.", "startOffset": 36, "endOffset": 44}, {"referenceID": 1, "context": "This is usually achieved by either sharing of feature representations [2], model parameters [12] or via similar context [14].", "startOffset": 70, "endOffset": 73}, {"referenceID": 11, "context": "This is usually achieved by either sharing of feature representations [2], model parameters [12] or via similar context [14].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "This is usually achieved by either sharing of feature representations [2], model parameters [12] or via similar context [14].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19, 10] were two of the first to use well-designed visual attributes of unseen classes to classify them.", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "[19, 10] were two of the first to use well-designed visual attributes of unseen classes to classify them.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "[27] learn when to transfer knowledge from one category to another for each instance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "For instance, in sentiment analysis one could train a classifier for movie reviews and then adapt from that domain to book reviews [4, 13].", "startOffset": 131, "endOffset": 138}, {"referenceID": 12, "context": "For instance, in sentiment analysis one could train a classifier for movie reviews and then adapt from that domain to book reviews [4, 13].", "startOffset": 131, "endOffset": 138}, {"referenceID": 22, "context": "Multimodal embeddings relate information from multiple sources such as sound and video [24] or images and text.", "startOffset": 87, "endOffset": 91}, {"referenceID": 28, "context": "[30] project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Among other recent work is that by Srivastava and Salakhutdinov [31] who developed multimodal Deep Boltzmann Machines.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Some work has been done on multimodal distributional methods [11, 22].", "startOffset": 61, "endOffset": 69}, {"referenceID": 20, "context": "Some work has been done on multimodal distributional methods [11, 22].", "startOffset": 61, "endOffset": 69}, {"referenceID": 4, "context": "[5] worked on perceptually grounding word meaning and showed that joint models are better able to predict the color of concrete objects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "In these approaches, words are represented as vectors of distributional characteristics \u2013 most often their co-occurrences with words in context [25, 9, 1, 32].", "startOffset": 144, "endOffset": 158}, {"referenceID": 8, "context": "In these approaches, words are represented as vectors of distributional characteristics \u2013 most often their co-occurrences with words in context [25, 9, 1, 32].", "startOffset": 144, "endOffset": 158}, {"referenceID": 0, "context": "In these approaches, words are represented as vectors of distributional characteristics \u2013 most often their co-occurrences with words in context [25, 9, 1, 32].", "startOffset": 144, "endOffset": 158}, {"referenceID": 30, "context": "In these approaches, words are represented as vectors of distributional characteristics \u2013 most often their co-occurrences with words in context [25, 9, 1, 32].", "startOffset": 144, "endOffset": 158}, {"referenceID": 27, "context": "These representations have proven very effective in natural language processing tasks such as sense disambiguation [29], thesaurus extraction [23, 8] and cognitive modeling [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "These representations have proven very effective in natural language processing tasks such as sense disambiguation [29], thesaurus extraction [23, 8] and cognitive modeling [20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 7, "context": "These representations have proven very effective in natural language processing tasks such as sense disambiguation [29], thesaurus extraction [23, 8] and cognitive modeling [20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 18, "context": "These representations have proven very effective in natural language processing tasks such as sense disambiguation [29], thesaurus extraction [23, 8] and cognitive modeling [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "For further details and evaluations of these embeddings, see [3, 7].", "startOffset": 61, "endOffset": 67}, {"referenceID": 6, "context": "For further details and evaluations of these embeddings, see [3, 7].", "startOffset": 61, "endOffset": 67}, {"referenceID": 5, "context": "[6] to extract F image features from raw pixels in an unsupervised fashion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "The mapping from 50 to 2 dimensions was done with t-SNE [33].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "[16] to obtain an outlier probability for each testing point and then use the weighted combination of classifiers for both seen and unseen classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We use the unsupervised feature extraction method of Coates and Ng [6] to obtain a 12,800-dimensional feature vector for each image.", "startOffset": 67, "endOffset": 70}], "year": 2013, "abstractText": "This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.", "creator": "LaTeX with hyperref package"}}}