{"id": "1301.3775", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Discriminative Recurrent Sparse Auto-Encoders", "abstract": "We present the discriminative recurrent sparse auto-encoder model, which consists of an encoder whose hidden layer is recurrent, and two linear decoders, one to reconstruct the input, and one to predict the output. The hidden layer is composed of rectified linear units (ReLU) and is subject to a sparsity penalty. The network is first trained in unsupervised mode to reconstruct the input, and subsequently trained discriminatively to also produce the desired output. The recurrent network is time-unfolded with a given number of iterations, and trained using back-propagation through time. In its time-unfolded form, the network can be seen as a very deep multi-layer network in which the weights are shared between the hidden layers. The depth allows the system to exhibit all the power of deep network while substantially reducing the number of trainable parameters.", "histories": [["v1", "Wed, 16 Jan 2013 18:07:01 GMT  (355kb,D)", "https://arxiv.org/abs/1301.3775v1", null], ["v2", "Fri, 1 Feb 2013 18:51:59 GMT  (364kb,D)", "http://arxiv.org/abs/1301.3775v2", "Improved readability of the text, particularly the introduction. Added one figure. 14 pages, 10 figures"], ["v3", "Wed, 13 Mar 2013 21:17:19 GMT  (366kb,D)", "http://arxiv.org/abs/1301.3775v3", "Added clarifications suggested by reviewers, including additional references. 15 pages, 10 figures"], ["v4", "Tue, 19 Mar 2013 18:43:29 GMT  (432kb,D)", "http://arxiv.org/abs/1301.3775v4", "Added clarifications suggested by reviewers. 15 pages, 10 figures"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["jason tyler rolfe", "yann lecun"], "accepted": true, "id": "1301.3775"}, "pdf": {"name": "1301.3775.pdf", "metadata": {"source": "CRF", "title": "Discriminative Recurrent Sparse Auto-Encoders", "authors": ["Jason Tyler Rolfe"], "emails": ["yann}@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able to determine for themselves what they want and what they want must take things into their own hands. In fact, it is the case that most people are able to take things into their own hands. In fact, it is the case that most people are able to take things into their own hands. In fact, it is the case that people are able to take things into their own hands. In fact, it is the case that people are able to take things into their own hands, which they have to do. In fact, it is the case that people are able to take things into their own hands."}, {"heading": "1.1 Prior work", "text": "The encoder architecture of DrSAE is modelled according to the Iterative Shrinkage and Threshold Algorithm (ISTA) of 1998, a proximal method of sparse encoding (Chambolle, et al., 1998; Daubechies, Defrise, & De Mol, 2004).Gregor & LeCun (2010) showed that the sparse representations calculated by ISTA can be efficiently approximated by a structurally similar encoder with a less restrictive learned parameterization. Instead of learning to find a precalculated, optimal, sparse code, the LISTA autoencoders were trained by Sprechmann, Bronstein, & Sapiro (2012a, b) to directly minimize the sparse reconstruction loss function. DrSAE expands the LISTA autoencoders with a non-negative restriction on the shrinkage non-linearity of LISTA, which transforms the non-linearity of ISTA into a non-cursive operating LISTA."}, {"heading": "2 Network architecture", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3 Analysis of the hidden unit representation", "text": "In this context, it should be noted that this is not a purely formal matter, but a purely formal matter, which is a purely formal matter, but a purely formal matter."}, {"heading": "3.1 Part-units", "text": "The study of the relationship between the elements of S \u2212 I and D > \u00b7 D confirms that sub-units with an encoder decoder angle of less than 0.5 radians are ISTA compliant and therefore perform sparse encoding at the residual input according to the prototypes of categorical units. Figure 4 \"s prominent diagonals with corresponding slopes, which represent the value of Si, j \u2212 \u03b4i, j versus Di \u00b7 Dj for connections between sub-units or from categorical units to sub-units, show that sub-units of all units receive ISTA consistent connections. Faithfulness of these links to the ISTA ideal is not strongly dependent on whether the afferent units are ISTA-compliant sub-units or ISTA-ignoring categorical units. As a result, the sub-units treat the categorical units as if they were also involved in the reconstruction of the input."}, {"heading": "3.2 Categorical-units", "text": "In fact, it is the case that most of them will be able to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live."}, {"heading": "4 Performance", "text": "In fact, the fact is that most of them are able to survive themselves, and that they feel able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "5 Discussion", "text": "In fact, the fact is that most of them are able to survive themselves without there being a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, \"in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process in which there is a process, in which there is a process, in which there is a process, in which is a process, in which is a process, in which is a process in which there is a"}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2012}, {"title": "Recurrent neural networks for missing or asynchronous data", "author": ["Y. Bengio", "F. Gingras"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Bengio and Gingras,? \\Q1996\\E", "shortCiteRegEx": "Bengio and Gingras", "year": 1996}, {"title": "Differentiable sparse coding", "author": ["D.M. Bradley", "J.A. Bagnell"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Bradley and Bagnell,? \\Q2008\\E", "shortCiteRegEx": "Bradley and Bagnell", "year": 2008}, {"title": "Learning mid-level features for recognition", "author": ["Y. Boureau", "F. Bach", "L. LeCun", "J. Ponce"], "venue": "In Proceedings of the 23rd IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Boureau et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2010}, {"title": "Nonlinear wavelet image processing: Variational problems, compression, and noise removal through wavelet shrinkage", "author": ["A. Chambolle", "R.A. De Vore", "N.Y. Lee", "B.J. Lucier"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Chambolle et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chambolle et al\\.", "year": 1998}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In Proceedings of the 25th IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "The importance of encoding versus training with sparse coding and vector quantization L", "author": ["A. Coates", "A.Y. Ng"], "venue": "Getoor & T. Scheffer (Eds.) Proceedings of the 28th International Conference on Machine Learning (ICML", "citeRegEx": "Coates and Ng,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng", "year": 2011}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Recovery of sparse translation-invariant signals with continuous basis pursuit", "author": ["C. Ekanadham", "D. Tranchina", "E.P. Simoncelli"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Ekanadham et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ekanadham et al\\.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "JMLR W&CP: Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Maxout networks arXiv:1302.4389v3 [stat.ML", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "In J. Fu\u0308rnkranz & T. Joachims (Eds.) Proceedings of the 27th International Conference on Machine Learning (ICML", "citeRegEx": "Gregor and LeCun,? \\Q2010\\E", "shortCiteRegEx": "Gregor and LeCun", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "A practical guide to training restricted Boltzmann machines (UTML TR 2010003", "author": ["G. Hinton"], "venue": null, "citeRegEx": "Hinton,? \\Q2010\\E", "shortCiteRegEx": "Hinton", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors arXiv:1207.0580v1 [cs.NE", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "Hubel and Wiesel,? \\Q1962\\E", "shortCiteRegEx": "Hubel and Wiesel", "year": 1962}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "M.A. Ranzato", "Y. LeCun"], "venue": "In Proceedings of the 12th International Conference on Computer Vision (ICCV", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "The nonlinear statistics of high-contrast patches in natural images", "author": ["A.B. Lee", "K.S. Pedersen", "D. Mumford"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lee et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2003}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Mairal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In J. Fu\u0308rnkranz & T. Joachims (Eds.) Proceedings of the 27th International Conference on Machine Learning (ICML", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Sample complexity of testing the manifold hypothesis", "author": ["H. Narayanan", "S. MItter"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Narayanan and MItter,? \\Q2010\\E", "shortCiteRegEx": "Narayanan and MItter", "year": 2010}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "images. Nature,", "citeRegEx": "Olshausen and Field,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1996}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by VI", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "Sparse coding of sensory inputs", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Current opinion in neurobiology,", "citeRegEx": "Olshausen and Field,? \\Q2004\\E", "shortCiteRegEx": "Olshausen and Field", "year": 2004}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Ranzato M", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "M. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "M. et al\\.", "year": 2006}, {"title": "Semi-supervised learning of compact document representations with deep networks", "author": ["M. Ranzato", "M. Szummer"], "venue": "Proceedings of the 25th Annual International Conference on Machine Learning (ICML", "citeRegEx": "Ranzato and Szummer,? \\Q2008\\E", "shortCiteRegEx": "Ranzato and Szummer", "year": 2008}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "and the PDP Research Group (Eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Vol. 1. Foundations (pp. 318\u2013362)", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "A model of multiplicative neural responses in parietal cortex", "author": ["E. Salinas", "L.F. Abbott"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Salinas and Abbott,? \\Q1996\\E", "shortCiteRegEx": "Salinas and Abbott", "year": 1996}, {"title": "Learning continuous attractors in recurrent networks", "author": ["H.S. Seung"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Seung,? \\Q1998\\E", "shortCiteRegEx": "Seung", "year": 1998}, {"title": "Efficient pattern recognition using a new transformation distance", "author": ["P. Simard", "Y. LeCun", "J.S. Denker"], "venue": "Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Simard et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1993}, {"title": "Transformation invariance in pattern recognition: Tangent distance and tangent propagation", "author": ["P. Simard", "Y. LeCun", "J. Denker", "B. Victorri"], "venue": null, "citeRegEx": "Simard et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1998}, {"title": "Learning efficient structured sparse models", "author": ["P. Sprechmann", "A. Bronstein", "G. Sapiro"], "venue": "In J. Langford & J. Pineau (Eds.) Proceedings of the 29th International Conference on Machine Learning (ICML", "citeRegEx": "Sprechmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2012}, {"title": "Learning efficient sparse and low rank models", "author": ["P. Sprechmann", "A. Bronstein", "G. Sapiro"], "venue": null, "citeRegEx": "Sprechmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sprechmann et al\\.", "year": 2012}, {"title": "Deep convex net: A scalable architecture for speech pattern classification", "author": ["L. Deng", "D. Yu"], "venue": "In Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH", "citeRegEx": "Deng and Yu,? \\Q2011\\E", "shortCiteRegEx": "Deng and Yu", "year": 2011}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "In Proceedings of the 13th International Conference on Computer Vision (ICCV", "citeRegEx": "Zeiler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Deep networks complement the hierarchical structure in natural data (Bengio, 2009).", "startOffset": 68, "endOffset": 82}, {"referenceID": 0, "context": "DrSAEs resemble the structure of deep sparse rectifier neural networks (Glorot, Bordes, & Bengio, 2011), but differ in that the parameter matrices at each layer are tied (Bengio, BoulangerLewandowski, & Pascanu, 2012), the input projects to all layers, and the outputs are normalized. DrSAEs are also reminiscent of the recurrent neural networks investigated by Bengio & Gingras (1996), but use a different nonlinearity and a heavily regularized loss function.", "startOffset": 90, "endOffset": 386}, {"referenceID": 0, "context": "DrSAEs resemble the structure of deep sparse rectifier neural networks (Glorot, Bordes, & Bengio, 2011), but differ in that the parameter matrices at each layer are tied (Bengio, BoulangerLewandowski, & Pascanu, 2012), the input projects to all layers, and the outputs are normalized. DrSAEs are also reminiscent of the recurrent neural networks investigated by Bengio & Gingras (1996), but use a different nonlinearity and a heavily regularized loss function. Finally, they are similar to the recurrent networks described by Seung (1998), but have recurrent connections amongst the hidden units, rather than between the hidden units and the input units, and introduce classification and sparsification losses.", "startOffset": 90, "endOffset": 539}, {"referenceID": 16, "context": "This bound serves the same function as `2 weight regularization (Hinton, 2010).", "startOffset": 64, "endOffset": 78}, {"referenceID": 16, "context": "As in the case of the encoder, this serves the same function as `2 weight regularization (Hinton, 2010).", "startOffset": 89, "endOffset": 103}], "year": 2013, "abstractText": "We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit far more representational power, while keeping the number of trainable parameters fixed. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.", "creator": "LaTeX with hyperref package"}}}