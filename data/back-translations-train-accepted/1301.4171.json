{"id": "1301.4171", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2013", "title": "Affinity Weighted Embedding", "abstract": "Supervised (linear) embedding models like Wsabie and PSI have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration's features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results.", "histories": [["v1", "Thu, 17 Jan 2013 17:46:27 GMT  (19kb)", "http://arxiv.org/abs/1301.4171v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG stat.ML", "authors": ["jason weston", "ron j weiss", "hector yee"], "accepted": true, "id": "1301.4171"}, "pdf": {"name": "1301.4171.pdf", "metadata": {"source": "CRF", "title": "Affinity Weighted Embedding", "authors": ["Jason Weston"], "emails": ["jweston@google.com", "ronw@google.com", "hyee@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.41 71v1 [cs.IR]"}, {"heading": "1 (Supervised) Linear Embedding Models", "text": "Standard linear embedding models are in the form of: f (x, y) = x U V y = \u2211 ijxiU i Vjyy.where x are the input characteristics and y is a possible label (in the case of the remark), document (in the case of retrieval of information) or item (in the case of recommendation).These models are used in both supervised and unsupervised environments. In the case of supervised ranking, they have proven successful in many of the tasks described above, e.g. in the Wsabie algorithm [5, 4, 6] which approximately optimizes precision at the top of the ranking, has proven useful for remark and recommendation.These methods scale well to large data and are easy to implement and use. However, as they do not contain non-linearity (except in the representation of the characteristics in x and y), their ability to insert large complex data sets can be limited and fit, and typically do not fit according to our experience."}, {"heading": "2 Affinity Weighted Embedding Models", "text": "In this work, we propose the following generalized embedding model: f (x, y) = 2 (x) x, y) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x x (x) x x (x) x x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x) x x x (x) x x) x x (x) x (x) x x) x x (x) x (x) x (x) x (x) x (x) x (x) x) x (x) x (x) x (x) x x (x) x) x x x (x) x (x) x) x x x x (x) x (x) x (x) x (x) x) x x x x x (x) x (x) x x x (x) x x (x) x x (x) x x (x) x x (x) x (x) x x (x) x x x x (x) x x x (x) x (x x) x (x x x) x (x x x) x x (x x x) x x (x x x x) x (x x x x x (x x) x (x x x x) x (x x x x x) x x x (x x x x x x (x) x (x x x x x x x x x x x x x x x x x x (x) x x x x x x x x x x x x x (x) x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 Experiments", "text": "yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy"}, {"heading": "4 Conclusions", "text": "In summary, by including a learned reweighting function G in the supervised linear embedding, we can increase the capacity of the model, which leads to improved results. However, one problem is that the cost of reducing underequipment by using G is that it increases both the storage and calculation requirements of the model."}], "references": [{"title": "Polynomial semantic indexing", "author": ["B. Bai", "J. Weston", "D. Grangier", "R. Collobert", "K. Sadamasa", "Y. Qi", "C. Cortes", "M. Mohri"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "A. Senior", "P. Tucker", "K. Yang"], "venue": "Large scale distributed deep networks. In Advances in Neural Information Processing Systems 25, pages 1232\u20131240", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1106\u20131114", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale music annotation and retrieval: Learning to rank in joint semantic spaces", "author": ["J. Weston", "S. Bengio", "P. Hamel"], "venue": "Journal of New Music Research", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Intl. Joint Conf. Artificial Intelligence, (IJCAI), pages 2764\u20132770", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent collaborative retrieval", "author": ["J. Weston", "C. Wang", "R. Weiss", "A. Berenzeig"], "venue": "ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Supervised (linear) embedding models like Wsabie [5] and PSI [1] have proven successful at ranking, recommendation and annotation tasks.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Supervised (linear) embedding models like Wsabie [5] and PSI [1] have proven successful at ranking, recommendation and annotation tasks.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "the Wsabie algorithm [5, 4, 6] which approximately optimizes precision at the top of the ranked list has proven useful for annotation and recommendation.", "startOffset": 21, "endOffset": 30}, {"referenceID": 3, "context": "the Wsabie algorithm [5, 4, 6] which approximately optimizes precision at the top of the ranked list has proven useful for annotation and recommendation.", "startOffset": 21, "endOffset": 30}, {"referenceID": 5, "context": "the Wsabie algorithm [5, 4, 6] which approximately optimizes precision at the top of the ranked list has proven useful for annotation and recommendation.", "startOffset": 21, "endOffset": 30}, {"referenceID": 3, "context": "Wsabie has been applied to both tasks previously [4, 5].", "startOffset": 49, "endOffset": 55}, {"referenceID": 4, "context": "Wsabie has been applied to both tasks previously [4, 5].", "startOffset": 49, "endOffset": 55}, {"referenceID": 3, "context": "On Magnatagatune we used MFCC features for both Wsabie and our method, similar to those used in [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "4% Convolutional Net [2] 15.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "We used similar KPCA features as in [5] for both Wsabie and our method.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Our method is competitive with the convolutional neural network model of [2] (note, this is on a different train/test split).", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "However, we believe the method of [3] would likely perform better again if applied in the same setting.", "startOffset": 34, "endOffset": 37}], "year": 2013, "abstractText": "Supervised (linear) embedding models like Wsabie [5] and PSI [1] have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration\u2019s features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results. 1 (Supervised) Linear Embedding Models Standard linear embedding models are of the form: f(x, y) = xUV y = \u2211", "creator": "LaTeX with hyperref package"}}}