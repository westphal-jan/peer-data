{"id": "1302.2176", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2013", "title": "Minimax Optimal Algorithms for Unconstrained Linear Optimization", "abstract": "We design and analyze minimax-optimal algorithms for online linear optimization games where the player's choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. The standard benchmark is the loss of the best strategy chosen from a bounded comparator set. When the the comparison set and the adversary's gradients satisfy L_infinity bounds, we give the value of the game in closed form and prove it approaches sqrt(2T/pi) as T -&gt; infinity.", "histories": [["v1", "Fri, 8 Feb 2013 23:16:04 GMT  (17kb)", "http://arxiv.org/abs/1302.2176v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["h brendan mcmahan"], "accepted": true, "id": "1302.2176"}, "pdf": {"name": "1302.2176.pdf", "metadata": {"source": "CRF", "title": "Minimax Optimal Algorithms for Unconstrained Linear Optimization", "authors": ["H. Brendan McMahan"], "emails": ["mcmahan@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 2.21 76v1 [cs.LG] Feb 8 2 \u221a 2T / \u03c0 as T \u2192 \u221e. Interesting algorithms arise when we look at the soft limitations of the comparator, instead of limiting it to a limited sentence. As a warm-up, we analyze the game with a square penalty. The value of this game is exactly T / 2, and this value is achieved by perhaps the simplest online algorithm of all: unforeseen gratification drop with a constant learning rate. We derive from this a minimax-optimal algorithm for a much gentler punishment function. This algorithm reaches good limits under the common notion of regretting for a comparison point without having to set the comparator set in advance. The value of this game converges with E as T \u2192 \u221e; we give a closed form for the exact value as a function of T. The resulting algorithm is in unlimited investment or reward scenarios, of course, as a simple loss is guaranteed at worst, while allowing for a constant loss. \""}, {"heading": "1 Introduction", "text": "However, the results of the learner study are very different in this case, and then an opponent is chosen who decides on an exact minimaxstrategy, and the learner suffers from the loss of the learner. The goal of the learner is to minimize exceeding this mark by means of a benchmark strategy. We define Regret = Loss \u2212 (Benchmark Loss) = 1gt \u00b7 L (g1, gT) as a benchmark strategy. We define the learner strategy = 1gt \u00b7 L (g1, gT) as a benchmark strategy. We define the learner strategy = 1gt \u00b7 L (g1, g1) as a benchmark strategy."}, {"heading": "2 Alternative Notions of Regret", "text": "One of our contributions shows that interesting results can be achieved by selecting L differently than in Eq. (2); in particular, we get minimax optimal algorithms for the problem considered by Streeter and McMahan [2012], by choosing an appropriate choice of L. So you could choose L (G) = 0, but this leads to an uninteresting game: the opponent has no long-term limitations, and so we can simply select a bet to maximize the best strategy for what the player has chosen. Thus, the player can do nothing better than always select xt = 0, but this is precisely the reason for studying the default preconception of regret: we do not demand that we do well in absolute terms, but rather relative to the best strategy of a fixed set.This is interesting when the player accepts the fact that it is impossible to do well in terms of absolute loss."}, {"heading": "3 General Unconstrained Linear Optimization", "text": "In this section we prove a theory that greatly simplifies the task of calculating minimax values and derives algorithms from it for the games we are looking at (1,1). We prove this result in the onedimensional case; corollary 2 then \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p [2], where the result is limited to n-dimension.Theorem 1. Consider the one-dimensional unrestricted game in which the player selects the player who selects the player (1, 1), and L is concave in each of its arguments and limited to GT below. Then V is T = E gt \u00b2, where the player selects the player (g1,., gT)].where the expectation is above each selected and uniformally from {\u2212 1, 1}. That is, the gt are random variables wheel makers). Moreover, the conditional value of the game is isVt (g1,.,., gt) = E gt + 1,."}, {"heading": "4 Deriving Minimax Optimal Algorithms", "text": "In this section we will examine three possible uses of the tools from the previous section. We will start with a relatively simple but interesting example that illustrates the technique."}, {"heading": "4.1 Constant step-size gradient descent can be minimax optimal", "text": "Suppose we use a \"soft\" practicable set for the benchmark, L (G) = min xGx + \u03c3 2 x2 = \u2212 1 2\u03c3 G2, (9) for a constant \u03c3 > 0. Is there a no regrets algorithm against this comparison class? Unfortunately, the general answer is no, as shown in the next theorem: Theorem 3. The value of this game is V T = EG BT [1 2\u03c3G2] = T2\u03c3. Proof. Starting from Eq. (7), E G \u00b2 BT [G2] = 12TT \u2211 i = 0 (Ti \u2212 T) 2 Eq. (7) = 12T (4T \u00b2 G2] = 0 (Ti \u2212 T \u00b2) i2 \u2212 4T \u2211 i = 0 (Ti) i + T 2 T \u00b2 (Ti) = 12Ti \u00b2) = 12Ti \u00b2 (T \u00b2) = 4T = 2T = 2T = 2T = 2T = 2T = 2T = 2T = 2T = T (T)."}, {"heading": "4.2 Optimal regret against hypercube adversaries", "text": "Abernethy et al. [2008] specifies a minimax optimal algorithm, if the player's xt and the comparator's x-qs = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = qm = = qm = qm = = qm = = qm = = qm = = qm = = = qm = = qm = = qm = = = qm = = qm = = qm = = qm = = qm = = qm = = qm = = qm = = qm = = qm = = qm = = qm = = qm = = = qm = = = qm = = = qm = = = qm = = = = qm = = = = qm = = = ="}, {"heading": "4.3 Non-stochastic betting and No-regret for all feasible sets simultaneously", "text": "We derive a minimax optimal approach to the betting problem presented in Section 2, which is also in line with the attitude introduced by Streeter and McMahan (2012). Again, it is sufficient to consider the one-dimensional case. \u2212 \u2212 \u2212 In this work, the goal was to simultaneously prove limits such as remorse \u2264 O (R \u221a T) (((((1 + R) T)) for each benchmark x (1). \u2212 In this work, the goal was achieved by an algorithm that guarantees that Loss = T = 1gtxt \u2264 \u2212 exp (| g1: T) + O (1)."}, {"heading": "A A Symmetric Betting Algorithm", "text": "The one-sided algorithm of Theorem 6 has \u2212 gt \u2212 gt = V T + L (G) \u2264 \u2212 exp (G \u221a T) + \u221a e.To be successful when g1: T is large and negative, we can copy the algorithm to \u2212 g1,..., \u2212 gT by suggesting the signs of each of them. Then, the combined algorithm satisfies Loss \u2264 \u2212 exp (G 270) \u2212 exp (\u2212 G 270 T) + 2 \u221a e \u2264 (| G | T) + 2 \u221a e, and so after Equation (13) and Theorem 1 of Streeter and McMahan [2012], we get the desired remorse limits. The following theorem actually implies the symmetric algorithm minimax optimally in relation to the combined benchmark LC (G) = \u2212 exp (G 270)."}], "references": [{"title": "Repeated games against budgeted adversaries", "author": ["Jacob Abernethy", "Manfred K. Warmuth"], "venue": "In NIPS,", "citeRegEx": "Abernethy and Warmuth.,? \\Q2010\\E", "shortCiteRegEx": "Abernethy and Warmuth.", "year": 2010}, {"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "A stochastic view of optimal regret through minimax duality", "author": ["Jacob Abernethy", "Alekh Agarwal", "Peter Bartlett", "Alexander Rakhlin"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "Algorithms for portfolio management based on the Newton method", "author": ["Amit Agarwal", "Elad Hazan", "Satyen Kale", "Robert E. Schapire"], "venue": "In ICML,", "citeRegEx": "Agarwal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2006}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["Ofer Dekel", "Ambuj Tewari", "Raman Arora"], "venue": "In ICML,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Closed form summation for classical distributions: Variations on a theme of de Moivre", "author": ["Persi Diaconis", "Sandy Zabell"], "venue": "Statistical Science,", "citeRegEx": "Diaconis and Zabell.,? \\Q1991\\E", "shortCiteRegEx": "Diaconis and Zabell.", "year": 1991}, {"title": "On stochastic and worst-case models for investing", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In NIPS", "citeRegEx": "Hazan and Kale.,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2009}, {"title": "A new interpretation of information rate", "author": ["J.L. Kelly Jr."], "venue": "Bell System Technical Journal,", "citeRegEx": "Jr.,? \\Q1956\\E", "shortCiteRegEx": "Jr.", "year": 1956}, {"title": "Putting bayes to sleep", "author": ["Wouter Koolen", "Dmitry Adamskiy", "Manfred Warmuth"], "venue": "In NIPS", "citeRegEx": "Koolen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2012}, {"title": "On sequential strategies for loss functions with memory", "author": ["N. Merhav", "E. Ordentlich", "G. Seroussi", "M.J. Weinberger"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Merhav et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Merhav et al\\.", "year": 2006}, {"title": "Relax and randomize: From value to algorithms", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In NIPS,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "No-regret algorithms for unconstrained online convex optimization", "author": ["Matthew Streeter", "H. Brendan McMahan"], "venue": "In NIPS,", "citeRegEx": "Streeter and McMahan.,? \\Q2012\\E", "shortCiteRegEx": "Streeter and McMahan.", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}, {"title": "Theorem 1 of Streeter and McMahan [2012], we obtain the desired regret bounds. The following theorem implies the symmetric algorithm is in fact minimax optimal with respect to the combined benchmark", "author": ["\u221a e"], "venue": null, "citeRegEx": "e,? \\Q2012\\E", "shortCiteRegEx": "e", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "1 Introduction Minimax analysis has recently been shown to be a powerful tool for the construction of online learning algorithms [Rakhlin et al., 2012].", "startOffset": 129, "endOffset": 151}, {"referenceID": 1, "context": "2 provides results analogous to those of Abernethy et al. [2008]; we consider regret compared to the best x\u030a where \u2016x\u030a\u2016\u221e \u2264 1 against an adversary constrained to play \u2016gt\u2016\u221e \u2264 1, while Abernethy et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 1, "context": "2 provides results analogous to those of Abernethy et al. [2008]; we consider regret compared to the best x\u030a where \u2016x\u030a\u2016\u221e \u2264 1 against an adversary constrained to play \u2016gt\u2016\u221e \u2264 1, while Abernethy et al. considered \u2016gt\u20162 \u2264 1 and \u2016x\u030a\u20162 \u2264 1 for n \u2265 3 dimensions. Interestingly, while we prove results for the unconstrained player, we show the optimal strategy in fact always plays points from X = {x | \u2016x\u2016\u221e \u2264 1}, and so applies to the constrained case as well. Our results hold for the n = 1 case (where L2 and L\u221e coincide), showing that the value of the game approaches \u221a 2T/\u03c0 as T \u2192 \u221e, as opposed to \u221a T as one might extrapolate from the results of Abernethy. This indicates an interesting change in the geometry of the L2 game between n = 1 and n = 3. Finally, Section 4.3 gives a minimax optimal algorithm for the setting introduced by Streeter and McMahan [2012]. Following their work, our algorithm obtains standard regret at most O(R \u221a T log ((1 +R)T )) simultaneously for any comparator x\u030a with |\u030ax| = R, without needing to choose R in advance.", "startOffset": 41, "endOffset": 862}, {"referenceID": 11, "context": "However, following the approach of Rakhlin et al. [2012], we omit the terms \u2211t s=1 xs \u00b7 gs from Eq.", "startOffset": 35, "endOffset": 57}, {"referenceID": 6, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction.", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction.", "startOffset": 115, "endOffset": 146}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein.", "startOffset": 115, "endOffset": 382}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online convex optimization framework.", "startOffset": 115, "endOffset": 423}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online convex optimization framework. This notion can be expressed by an appropriate L(g1, . . . , gT ), but now the order of the gradients matters, unlike the benchmarks L considered in this work. Merhav et al. [2006] and Dekel et al.", "startOffset": 115, "endOffset": 679}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online convex optimization framework. This notion can be expressed by an appropriate L(g1, . . . , gT ), but now the order of the gradients matters, unlike the benchmarks L considered in this work. Merhav et al. [2006] and Dekel et al. [2012] consider the stronger notion of policy regret in the online experts and bandit settings, respectively.", "startOffset": 115, "endOffset": 703}, {"referenceID": 3, "context": "For investing scenarios, Agarwal et al. [2006] and Hazan and Kale [2009] consider regret with respect to the best constant-rebalanced portoflio.", "startOffset": 25, "endOffset": 47}, {"referenceID": 3, "context": "For investing scenarios, Agarwal et al. [2006] and Hazan and Kale [2009] consider regret with respect to the best constant-rebalanced portoflio.", "startOffset": 25, "endOffset": 73}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary.", "startOffset": 0, "endOffset": 29}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al.", "startOffset": 0, "endOffset": 180}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al. [2009]. Rakhlin et al.", "startOffset": 0, "endOffset": 267}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al. [2009]. Rakhlin et al. [2012] takes powerful tools for non-constructive analysis of online learning problems and shows they can be used to design algorithms; our work differs in that we focus on cases where the exact minimax strategy can be computed.", "startOffset": 0, "endOffset": 290}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al. [2009]. Rakhlin et al. [2012] takes powerful tools for non-constructive analysis of online learning problems and shows they can be used to design algorithms; our work differs in that we focus on cases where the exact minimax strategy can be computed. 2 Alternative Notions of Regret One of our contributions is showing that that interesting results can be obtained by choosing L differently than in Eq. (2); in particular, we obtain minimax optimal algorithms for the problem considered by Streeter and McMahan [2012] by analyzing an appropriate choice of L.", "startOffset": 0, "endOffset": 778}, {"referenceID": 14, "context": "gradient descent [Zinkevich, 2003] with a constant step size.", "startOffset": 17, "endOffset": 34}, {"referenceID": 6, "context": "Despite the theoretical guarantees, the player certainly might feel regret at having won only \u221a T in this situation! One might also hope to use online algorithms for portfolio management, for example those of Hazan and Kale [2009] and Agarwal et al.", "startOffset": 209, "endOffset": 231}, {"referenceID": 3, "context": "Despite the theoretical guarantees, the player certainly might feel regret at having won only \u221a T in this situation! One might also hope to use online algorithms for portfolio management, for example those of Hazan and Kale [2009] and Agarwal et al. [2006]. However, these algorithms require the assumption that you always retain at least an \u03b1 > 0 fraction of your bet, which is directly violated in our game.", "startOffset": 235, "endOffset": 257}, {"referenceID": 2, "context": "1 from Cesa-Bianchi and Lugosi [2006]), which gives Vt\u22121(g1, .", "startOffset": 7, "endOffset": 38}, {"referenceID": 1, "context": "The use of randomization to allow the application of the minimax theorem is similar to the technique used by Abernethy et al. [2009]. A key insight from the proof is that an optimal adversary can always select from {\u22121, 1}.", "startOffset": 109, "endOffset": 133}, {"referenceID": 1, "context": "2 Optimal regret against hypercube adversaries Abernethy et al. [2008] gives a minimax optimal algorithm when the player\u2019s xt and the comparator x\u030a are constrained to an L2 ball, and the adversary must also select gt from an L2 ball, for n \u2265 3 dimensions.", "startOffset": 47, "endOffset": 71}, {"referenceID": 14, "context": "where we have applied a classic formula of de Moivre [1718] for the mean absolute deviation of the binomial distribution (see also Diaconis and Zabell [1991]).", "startOffset": 2, "endOffset": 60}, {"referenceID": 6, "context": "where we have applied a classic formula of de Moivre [1718] for the mean absolute deviation of the binomial distribution (see also Diaconis and Zabell [1991]).", "startOffset": 131, "endOffset": 158}, {"referenceID": 1, "context": "Abernethy et al. [2008] shows that for the linear game with n \u2265 3 where both the learner and adversary select vectors from the unit sphere, the minimax value is exactly \u221a T .", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Abernethy et al. [2008] shows that for the linear game with n \u2265 3 where both the learner and adversary select vectors from the unit sphere, the minimax value is exactly \u221a T . Interestingly, in the n = 1 case (where L2 and L\u221e coincide), the value of the game is lower, about 0.8 \u221a T rather than \u221a T . This indicates a fundamental difference in the geometry of the n = 1 space and n \u2265 3. We conjecture the minimax value for the L2 game with n = 2 lies somewhere in between. 4.3 Non-stochastic betting and No-regret for all feasible sets simultaneously We derive a minimax optimal approach to the betting problem presented in Section 2, which also corresponds to the setting introduced by Streeter and McMahan [2012]. Again, it is sufficient to consider the one-dimensional case.", "startOffset": 0, "endOffset": 714}], "year": 2013, "abstractText": "We design and analyze minimax-optimal algorithms for online linear optimization games where the player\u2019s choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. The standard benchmark is the loss of the best strategy chosen from a bounded comparator set. When the the comparison set and the adversary\u2019s gradients satisfy L\u221e bounds, we give the value of the game in closed form and prove it approaches \u221a 2T/\u03c0 as T \u2192 \u221e. Interesting algorithms result when we consider soft constraints on the comparator, rather than restricting it to a bounded set. As a warmup, we analyze the game with a quadratic penalty. The value of this game is exactly T/2, and this value is achieved by perhaps the simplest online algorithm of all: unprojected gradient descent with a constant learning rate. We then derive a minimax-optimal algorithm for a much softer penalty function. This algorithm achieves good bounds under the standard notion of regret for any comparator point, without needing to specify the comparator set in advance. The value of this game converges to \u221a e as T \u2192 \u221e; we give a closed-form for the exact value as a function of T . The resulting algorithm is natural in unconstrained investment or betting scenarios, since it guarantees at worst constant loss, while allowing for exponential reward against an \u201ceasy\u201d adversary.", "creator": "LaTeX with hyperref package"}}}