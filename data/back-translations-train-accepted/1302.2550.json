{"id": "1302.2550", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2013", "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning", "abstract": "We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Holder continuity of rewards and transition probabilities.", "histories": [["v1", "Mon, 11 Feb 2013 17:44:10 GMT  (19kb)", "http://arxiv.org/abs/1302.2550v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ronald ortner", "daniil ryabko"], "accepted": true, "id": "1302.2550"}, "pdf": {"name": "1302.2550.pdf", "metadata": {"source": "CRF", "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning", "authors": ["Ronald Ortner", "Daniil Ryabko"], "emails": ["rortner@unileoben.ac.at", "daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar Xiv: 130 2.25 50v1 [cs.LG] 1 1Fe b"}, {"heading": "1 Introduction", "text": "In many problems, there is a natural metric for the state space, so that close states exhibit similar behavior. Frequently, such similarities can be formalized as Lipschitz or more generally as Holder continuity of reward and transition functions. Limits of regret with respect to optimal policies on the assumption that the reward function Holder is a continuous learning process. [15, 4] The proposed algorithms apply the UCB algorithm [2] to discrediting the problem. In this way, there is regret about the loss through aggregation (which can be limited by Holder continuity)."}, {"heading": "2 Preliminaries", "text": "We consider the following attitude. In view of a Markov decision-making process (MDP) M with state space S = [0, 1] d and finite scope A. For the sake of simplicity, we assume below that d = 1. \"However, we assume that bias and results lead directly to the arbitrary dimension, cf. note 5 below. We will make the natural assumption that rewards and transitional probabilities are similar in narrow states. More precisely, we assume that rewards and transitional probabilities are continuous. Assumption 1\" There is such an assumption that for two states, s \"and all actions are optimal.\" We assume that rewards and transitional probabilities are similar in narrow states."}, {"heading": "3 Algorithm", "text": "Our UCCRL algorithms, which are presented in detail in Figure 1, set the \"optimism in the face of uncertainty maxim\" exactly like UCRL2 [11] or REGAL [5]. It maintains a series of plausible MDPs M and algorithm 1 [7]. The UCCRL algorithms input: [0, 1], action space A, constellation A, constellation A, constellation A, constellation A, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B, constellation B"}, {"heading": "4 Regret Bounds", "text": "Theorem 4. Let M be an MDP with permanent state space (0, 1), A actions, rewards and transition probabilities ensuring the satisfaction of assumptions 1 and 2, and bias above the limit of regret. (6) Then, with probability 1 \u2212 \u03b4, the regret of the UCCRL (executed with input parameters n and H) after T steps upper limits. (2 + 2\u03b1) Therefore, setting n = T gives 1 / (2 + 2\u03b1) upper limits of regret upper limits of regret. (2 + 2\u03b1) gives regret upper limits of regret. (2 + 2\u03b1)"}, {"heading": "5 Proof of Theorem 4", "text": "For the proof of the main theorem, we adapt the proof of the limits of regret for finite MDPs in [11] and [5]. Although the state space is now continuous due to the finite horizon T, we can repeat some arguments so that we maintain the structure of the original proof of theorem 2 in [11]. Some of the necessary adjustments are similar to techniques used to show the limits of regret for other modifications of the original UCRL2 algorithm [21, 22] but which only took finite MDPs into account."}, {"heading": "5.1 Splitting into Episodes", "text": "Let vk (s, a) be the number of times action a was selected in episode k, if it is in state s, and denote the total number of episodes by m. Then the regret of the UCCRL is set after T steps above (cf. section 4.1 of [11]), \u221a 5 8T log (8T \u03b4) + \u2211 mk = 1 \u2206 k. (7)."}, {"heading": "5.2 Failing Confidence Intervals", "text": "Next, we consider regret when the true MDP M is not included in the series of plausible MDPs Mk. (Together, we consider a pair of state actions (s, a), and consider that in step t in the states in I (s) and in the i-th sample a transition from state si (s) to state s (s) was observed (i = 1,..., N). First, we assume that in step t in the states in I (s) and in the i-th sample a transition from state si (s) to state s (s) was observed (i = 1,.,.)."}, {"heading": "5.3 Regret in Episodes with M \u2208 Mk", "text": "Now we can for episodes with M-Mk after the optimistic choice of M-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-k-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "5.4 The True Transition Functions", "text": "Now the term in (16) can be limited by (3) and (12), because we assume that the M-k, M-Mk (s) (s-k) (s-k) (s-k) (p-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-k) (s-s) (s-k) (s-s) (s-k) (s-s) (s-k) (s-s) (s-k) (s) (s-k) (s-k) (s-k) (s-k (s) (s-k) (s-k (s) (s) (s) (s) (s) (s) (s) (s) (s) (s) (s-k) (s) (s) (s) (s) (s) (s) (s) (s-k) (s) (s) (s) (s) (s) (s) (s) (s-k) (s) (s) (s) (s) (s-k) (s) (s) (s) (s-k) (s-k (s) (s) (s-k (s) (s) (s) (s-k (s) (s-k) (s-k (s) (s-k (s) (s-k (s) (s) (s-k-k-k (s) (s-k) (s (s-k) (s (s) (s-k (s) (s-k-k (s) (s-k (s (s) (s) (s-k-k (s) (s-k (s) (s-k (s) (s) (s-k (s) (s-k (s) (s), k (s), k (s-k (s-k (s-k (s-k (s) (s) (s-k (s-k (s)"}, {"heading": "5.5 Summing over Episodes with M \u2208 Mk", "text": "Finally, we sum up (14) all episodes with M-Mk using (15), (17) and (18), which results in the probability of at least 1 \u2212 \u03b412T 5 / 4m \u00b2 k = 1 \u2206 k1M \u00b2 Mk \u2264 2HLn \u2212 \u03b1T + 4H \u221a 14n log (2AT \u00b2) \u00b7 m \u00b2 k = 1n \u00b2 j = 1 \u00b2 a \u00b2 Avk (Ij, a) \u00b2 + H \u00b2 2T log (8T \u00b2) + HnA log (8T \u00b2) + 2Ln \u00b2 (8T \u00b2) + 2Ln \u00b2 14 log (2nAT \u00b2) m \u00b2 k = 1n \u00b2 j = 1 \u00b2 a \u00b2 a \u00b2 Avk (Ij, a) + H \u00b2 (Nk (Ij, a) + HnA \u00b2 (19) analogous to Section 4.3.3 \u2212 Annex C.3 of [11], it is possible to prove that n \u00b2 j = 1 \u00b2 a \u00b2 Avk (Ij, a) + H \u00b2 (Nk (8K, a) + HnA \u00b2 (4T), 8K (1T), 8K (1T), 1K (8T), 1K (1T), 1K (8K), 1K (1T), 1K (1K), 1K (1K), 1K (1K), 1K (1K), 2K (1K), 2K (1K), 2K (2K), 2K (2K (2K), 2K (2K), 2K (2K (2K), 2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K (2K), 2K (2K), (2K (2K), (2K), (2K (2K)"}, {"heading": "6 Outlook", "text": "We believe that generalizing our results to a continuous space of action should not pose major problems. To improve beyond the given boundaries, it may be promising to examine more complex discretization patterns. Assuming greater continuity is an obvious, but not the only possible assumption regarding transition probabilities and reward functions. A more general problem is to assume a set of F functions, to find a way to measure the \"size\" of F and to derive remorse limits depending on that size of F."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the three anonymous reviewers for their helpful suggestions and Re'mi Munos for the useful discussion that contributed to the improvement of the framework conditions, financed by the Ministry of Higher Education and Research, the Nord-Pas-de-Calais Regional Council and FEDER (Contrat de Projets Etat Region CPER 2007-2013), the ANR projects EXPLO-RA (ANR-08-COSI004), Lampada (ANR-09-EMER-007) and CoAdapt, as well as the FP7 programme of the European Community under Funding Agreements No. 216886 (PASCAL2) and No. 270327 (CompLACS)."}], "references": [{"title": "Regret bounds for the adaptive control of linear quadratic systems", "author": ["Yasin Abbasi-Yadkori", "Csaba Szepesv\u00e1ri"], "venue": "COLT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Finite-time analysis of the multi-armed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Improved rates for the stochastic continuum-armed bandit problem", "author": ["Peter Auer", "Ronald Ortner", "Csaba Szepesv\u00e1ri"], "venue": "COLT", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["Peter L. Bartlett", "Ambuj Tewari"], "venue": "In Proc. UAI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Adaptive-resolution reinforcement learning with polynomial exploration in deterministic domains", "author": ["Andrey Bernstein", "Nahum Shimkin"], "venue": "Mach. Learn.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Provably efficient learning with typed parametric models", "author": ["Emma Brunskill", "Bethany R. Leffler", "Lihong Li", "Michael L. Littman", "Nicholas Roy"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Online optimization of \u03c7-armed bandits", "author": ["S\u00e9bastien Bubeck", "R\u00e9mi Munos", "Gilles Stoltz", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Discrete-time Markov control processes, volume 30 of Applications of mathematics", "author": ["On\u00e9simo Hern\u00e1ndez-Lerma", "Jean Bernard Lasserre"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Further topics on discrete-time Markov control processes, volume 42 of Applications of mathematics", "author": ["On\u00e9simo Hern\u00e1ndez-Lerma", "Jean Bernard Lasserre"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Model-based exploration in continuous state spaces", "author": ["Nicholas K. Jong", "Peter Stone"], "venue": "SARA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Exploration in metric state spaces", "author": ["Sham Kakade", "Michael J. Kearns", "John Langford"], "venue": "In Machine Learning, Proc. 20th International Conference,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael J. Kearns", "Satinder P. Singh"], "venue": "Mach. Learn.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Nearly tight bounds for the continuum-armed bandit problem", "author": ["Robert Kleinberg"], "venue": "In Advances Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proc. 40th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Selecting the state-representation in reinforcement learning", "author": ["Odalric-Ambrym Maillard", "R\u00e9mi Munos", "Daniil Ryabko"], "venue": "In Advances Neural Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Optimal regret bounds for selecting the state representation in reinforcement learning", "author": ["Odalric-Ambrym Maillard", "Phuong Nguyen", "Ronald Ortner", "Daniil Ryabko"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Efficient continuous-time reinforcement learning with adaptive state graphs", "author": ["Gerhard Neumann", "Michael Pfeiffer", "Wolfgang Maass"], "venue": "ECML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Multi-resolution exploration in continuous spaces", "author": ["Ali Nouri", "Michael L. Littman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Adaptive aggregation for reinforcement learning in average reward Markov decision processes", "author": ["Ronald Ortner"], "venue": "Ann. Oper. Res.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Regret bounds for restless Markov bandits", "author": ["Ronald Ortner", "Daniil Ryabko", "Peter Auer", "R\u00e9mi Munos"], "venue": "In Proc. 23rd Conference on Algorithmic Learning Theory, ALT", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Online linear regression and its application to model-based reinforcement learning", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "NIPS", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Tree based discretization for continuous state space reinforcement learning", "author": ["William T.B. Uther", "Manuela M. Veloso"], "venue": "In Proc. AAAI 98, IAAI", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}], "referenceMentions": [{"referenceID": 14, "context": "Bounds on the regret with respect to an optimal policy under the assumption that the reward function is H\u00f6lder continuous have been given in [15, 4].", "startOffset": 141, "endOffset": 148}, {"referenceID": 3, "context": "Bounds on the regret with respect to an optimal policy under the assumption that the reward function is H\u00f6lder continuous have been given in [15, 4].", "startOffset": 141, "endOffset": 148}, {"referenceID": 1, "context": "The proposed algorithms apply the UCB algorithm [2] to a discretization of the problem.", "startOffset": 48, "endOffset": 51}, {"referenceID": 15, "context": "More recently, algorithms that adapt the used discretization (making it finer in more promising regions) have been proposed and analyzed [16, 8].", "startOffset": 137, "endOffset": 144}, {"referenceID": 7, "context": "More recently, algorithms that adapt the used discretization (making it finer in more promising regions) have been proposed and analyzed [16, 8].", "startOffset": 137, "endOffset": 144}, {"referenceID": 18, "context": "In the simplest case, the transition function is considered to be deterministic as in [19], and mistake bounds for the respective discounted setting have been derived in [6].", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "In the simplest case, the transition function is considered to be deterministic as in [19], and mistake bounds for the respective discounted setting have been derived in [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 22, "context": "For such settings sample complexity bounds have been given in [23, 7], while \u00d5( \u221a T ) bounds for the regret after T steps are shown in [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 6, "context": "For such settings sample complexity bounds have been given in [23, 7], while \u00d5( \u221a T ) bounds for the regret after T steps are shown in [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 0, "context": "For such settings sample complexity bounds have been given in [23, 7], while \u00d5( \u221a T ) bounds for the regret after T steps are shown in [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 11, "context": "While most of this work is purely experimental [12, 24], there are also some contributions with theoretical guarantees.", "startOffset": 47, "endOffset": 55}, {"referenceID": 23, "context": "While most of this work is purely experimental [12, 24], there are also some contributions with theoretical guarantees.", "startOffset": 47, "endOffset": 55}, {"referenceID": 12, "context": "Thus, [13] considers PAC-learning for continuous reinforcement learning in metric state spaces, when generative sampling is possible.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "The proposed algorithm is a generalization of the E algorithm [14] to continuous domains.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "A respective adaptive discretization approach is suggested in [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "The proposed algorithm is in the tradition of algorithms like UCRL2 [11] in that it implements 1", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "Given is a Markov decision process (MDP) M with state space S = [0, 1] and finite action space A.", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": "The random rewards in state s under action a are assumed to be bounded in [0, 1] with mean r(s, a).", "startOffset": 74, "endOffset": 80}, {"referenceID": 9, "context": "Chapter 10 of [10]) that the Poisson equation holds for the optimal policy.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The following result follows from the bias definition and Assumptions 1 and 2 (together with results from Chapter 10 of [10]).", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Chapter 10 of [10]) can obtain higher accumulated reward than T\u03c1 +H(M).", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "3 Algorithm Our algorithm UCCRL, shown in detail in Figure 1, implements the \u201coptimism in the face of uncertainty maxim\u201d just like UCRL2 [11] or REGAL [5].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "3 Algorithm Our algorithm UCCRL, shown in detail in Figure 1, implements the \u201coptimism in the face of uncertainty maxim\u201d just like UCRL2 [11] or REGAL [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "Algorithm 1 The UCCRL algorithm Input: State space S = [0, 1], action space A, confidence parameter \u03b4 > 0, aggregation parameter n \u2208 N, upper bound H on the bias span, Lipschitz parameters L, \u03b1.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "C algorithm [5] selects optimistic MDP and optimal policy in the same way as UCCRL.", "startOffset": 12, "endOffset": 15}, {"referenceID": 20, "context": "While the algorithm presented here is the first modification of UCRL2 to continuous reinforcement learning problems, there are similar adaptations to online aggregation [21] and learning in finite state MDPs with some additional similarity structure known to the learner [22].", "startOffset": 169, "endOffset": 173}, {"referenceID": 21, "context": "While the algorithm presented here is the first modification of UCRL2 to continuous reinforcement learning problems, there are similar adaptations to online aggregation [21] and learning in finite state MDPs with some additional similarity structure known to the learner [22].", "startOffset": 271, "endOffset": 275}, {"referenceID": 0, "context": "Let M be an MDP with continuous state space [0, 1], A actions, rewards and transition probabilities satisfying Assumptions 1 and 2, and bias span upper bounded by H .", "startOffset": 44, "endOffset": 50}, {"referenceID": 16, "context": "Specifically, we can use the modelselection technique introduced in [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Optimizing over the parameters \u03c4i and \u03c4 \u2032 i as done in [17], and increasing the number J of considered parameter values, one can obtain regret bounds of \u00d5(T ), or \u00d5(T ) in the Lipschitz case.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "For details see [17].", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "Recently, the results of [17] have been improved [18], and it seems that similar analysis gives improved regret bounds for the case of unknown H\u00f6lder parameters as well.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "Recently, the results of [17] have been improved [18], and it seems that similar analysis gives improved regret bounds for the case of unknown H\u00f6lder parameters as well.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Consider the following reinforcement learning problem with state space [0, 1].", "startOffset": 71, "endOffset": 77}, {"referenceID": 2, "context": "The rewards on each interval Ij are also constant for each a and are chosen as in the lower bounds for a multi-armed bandit problem [3] with nA arms.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "That is, giving only one arm slightly higher reward, it is known [3] that regret of \u03a9( \u221a nAT ) can be forced upon any algorithm on the respective bandit problem.", "startOffset": 65, "endOffset": 68}, {"referenceID": 14, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 3, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 15, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 7, "context": "The bounds of Theorems 4 and 8 cannot be directly compared to bounds for the continuous-armed bandit problem [15, 4, 16, 8], because the latter is no special case of learning MDPs with continuous state space (and rather corresponds to a continuous action space).", "startOffset": 109, "endOffset": 123}, {"referenceID": 10, "context": "5 Proof of Theorem 4 For the proof of the main theorem we adapt the proof of the regret bounds for finite MDPs in [11] and [5].", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "5 Proof of Theorem 4 For the proof of the main theorem we adapt the proof of the regret bounds for finite MDPs in [11] and [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 10, "context": "Although the state space is now continuous, due to the finite horizon T , we can reuse some arguments, so that we keep the structure of the original proof of Theorem 2 in [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 20, "context": "Some of the necessary adaptations made are similar to techniques used for showing regret bounds for other modifications of the original UCRL2 algorithm [21, 22], which however only considered finite-state MDPs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 21, "context": "Some of the necessary adaptations made are similar to techniques used for showing regret bounds for other modifications of the original UCRL2 algorithm [21, 22], which however only considered finite-state MDPs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 10, "context": "1 of [11]),", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "1 of [11] \u2014 but now using Hoeffding for independent and not necessarily identically distributed random variables \u2014 that Pr { \u2223 r\u0302(s, a)\u2212 E[r\u0302(s, a)] \u2223", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": ", Lemma 10 in [11]), Pr{ \u2211N i=1 Xi \u2265 \u03b8} \u2264 exp(\u2212\u03b82/8N) and in particular Pr { \u2211N i=1 Xi \u2265 \u221a", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "(12) This shows that the true MDP is contained in the set of plausible MDPs M(t) at step t with probability at least 1\u2212 \u03b4 15t6 , just as in Lemma 17 of [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "2 of [11].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "(18) in [11]) that after summing over all episodes we have m \u2211", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "2 of [11].", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "3 of [11], one can show that n \u2211", "startOffset": 5, "endOffset": 9}], "year": 2013, "abstractText": "We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are H\u00f6lder continuity of rewards and transition probabilities.", "creator": "LaTeX with hyperref package"}}}