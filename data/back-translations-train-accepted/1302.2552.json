{"id": "1302.2552", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2013", "title": "Selecting the State-Representation in Reinforcement Learning", "abstract": "The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T^{2/3} where T is the horizon time.", "histories": [["v1", "Mon, 11 Feb 2013 17:49:38 GMT  (24kb)", "http://arxiv.org/abs/1302.2552v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["odalric-ambrym maillard", "r\u00e9mi munos", "daniil ryabko"], "accepted": true, "id": "1302.2552"}, "pdf": {"name": "1302.2552.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["odalricambrym.maillard@gmail.com", "remi.munos@inria.fr", "daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar Xiv: 130 2.25 52v1 [cs.LG]"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Notation and definitions", "text": "We consider a space of observation O, a space of action A, and a space of rewards R > function (everything is assumed to be Polish). Furthermore, we assume that A is finite cardinality. < t def = | A | and that 0-R [0, 1]. The set of history up to the time t for all t-N environments is denoted by H < t def = O < t def = (A \u00b7 R \u00b7 O) t \u2212 1, and we define the set of all possible stories by H def = 1 H < t for all environments. For a Polish X, we are denoted by P (X) the set of probability distributions via X \u2212 1. Let us define an environment that is an illustration from the set of history H for the set of functions that depicts each action. < t for an action that represents a probability distribution. & ltP (R \u00b7 O) via the product space of rewards and observations.We consider the problem of learning interacting with an unknown environment when amplifying."}, {"heading": "3 Main results", "text": "Considering a set \u03a6 = {\u03c6j; j 6 J} of J state representation functions (models), one of which is a Markov model of the unknown environment, we want to construct a strategy that works almost as well as the best algorithm that knows which \u03c6j Markov is, and knows all probable characteristics (transition probabilities and rewards) of the MDP corresponding to this model. To this end, we define regret a strategy at the time T, as in [6, 3], as \u0445 (T) def = T\u03c1 - T = 1rt, where rt are the rewards obtained by following the proposed strategy, and \u0438 is the average optimal value in the best Markov model, i.e. \u03c1 = limT 1T E (\u0445 T = 1 rt (\u03c0)), where rt (zipated) are the rewards obtained by following the optimal policy for the best Markov model. Note that this definition is useful when the MDP communicates weakly."}, {"heading": "3.1 Best Lower Bound (BLB) algorithm", "text": "In this section, we introduce the Best-Lower-Bound (BLB) algorithm described in Figure 1.J. The algorithm operates in stages of double length. Each stage consists of 2 phases: an exploration phase and an exploitation phase. In the exploration phase, BLB first plays the model with the highest lower limit, according to the empirical rewards obtained in the previous exploration phase. This model is first selected for the same time as in the exploration phase, and then a test decides whether this model will continue to be played (if its performance during exploitation is still above the corresponding lower limit, i.e. if the rewards are at least as good as if it plays the best model)."}, {"heading": "3.2 Regret analysis", "text": "Theorem 1 (main result) We assume that a finite series of J-state-representation functions is given in the same way as F-state-representation functions, and there is at least one function that the environment is a Markov-decision process. If there are several such models, the one with the highest average reward for the optimal policy of the corresponding MDP will be. Then the regret (in relation to the optimal policy, the one to?) of the BLB-algorithm, the one with the parameter????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4 Discussion and outlook", "text": "In fact, it is so that most of us are able to abide by the rules that they have imposed on themselves. (...) Indeed, it is so that they are able to determine themselves. (...) It is not so that they are able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were not able to determine themselves. (...) It is as if they are not able to determine themselves. (...) \"(...)\" (...) \"(...)\" (...) \"((...)\" ((...) \"((...)\" () (...) \"() (...) () () () () () () () () () () () () () () () () () () () ()) (()) () () () () () () () () () () () () () () ()) () () () () () () ()) () () () () () () ()) () () () () () () () ()) () () () () ()) () () ()) () () () () () ()) () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () (() () () () () (() () () () () () ((() () () () (() () () (() () () (() (() () (() () () () ("}, {"heading": "5 Proof of Theorem 1", "text": "In this section, we will now explain the proof for theorem 1. The proof consists of several parts. First, we recall a general trust reserved for the UCRL2 algorithm in the true model. Then, we divide the regret into the sum of regret in each stage i. After analyzing the contribution to regret in stage i, we then collect all stages and match the length of each stage and episode to tie the final regret."}, {"heading": "5.1 Upper and Lower confidence bounds", "text": "Analysis of UCRL2 in [6] shows that the regret of UCRL2, when performed in several successive steps from time to time t1 in the true model, is with a probability higher than 1 \u2212 \u03b4 than the upper limit of the MDP. Interestingly, this diameter need not be known by the algorithm. Also, by careful consideration of the evidence of UCRL2, it can be shown that the following limit is also valid with a probability greater than 1 \u2212 \u043c: 1\u03c4t1 + \u03c4 \u2212 1 \u0445 t = t1rt \u2212 \u043c, 6 34D | S\u03c6 | A log (\u0421\u043c)."}, {"heading": "5.2 Regret of stage i", "text": "In this section, we analyze the regret of stage i, which we refer to as \"I.\" Note that, since each stage i is 6 I of length \u03c4i = 2i, except for the last stage I, which could be in front of it, we have stage I (T) = I (T) \u2211 i = 1 \u2206 i, (8), in which I (T) = xlog2 (T + 1) y. We also break down \"i = 1, i + \u0445 i, 2 into the regret that corresponds to the exploration stage\" II, i \"and\" II, \"and the regret that corresponds to the exploitation stage\" i \", 2.II\" Remember that \"i,\" 1 \"is the total length of the exploration stage\" i \"and\" i, \"2 is the total length of the exploitation stage\" II, \"(II,\" (II, \") II,\" II, \"(II,\" II, \"II,\" II, \"II,\" II, \"II,\" (II, \"),\" (II, \"(II,\") II, \"(II,\"), \"(II,\" II, \"(II,\"), \"II,\" (II, \"),\" (II, \"II,\" II, \"(II,\"), \"(II,\") II, \"(II,\"),"}, {"heading": "5.2.1 Regret in the exploration phase", "text": "Since in the exploration stage i each model \u03c6 for \u03c4i, 1, J is performed many steps, the regret for each model \u03c6 6 = \u03c6 \u03c6 is by \u03c4i, 1, J\u03c1. Now the regret for the true model \u03c4i, 1, J (\u03c1, \u00b5, 1 (\u03c6)), therefore the total contribution to regret in the exploration stage i is limited upwards by \u0432 i, 1 6 \u03c4i, 1, J (\u03c1, \u00b5, 1 (\u03c6)) + (J \u2212 1) \u03c4i, 1, J\u0438 (9)."}, {"heading": "5.2.2 Regret in the exploitation phase", "text": "By definition, all models in Gi-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i"}, {"heading": "5.3 Tuning the parameters of each stage.", "text": "We conclude that we can define the parameters of each stage, i.e. the probabilities \u03b4i (\u03b4) and the length \u03c4i, \u03c4i, 1 and \u03c4i, 2. The total length of stage i is definable. < The total length of stage i is definable. < The total length of stage i is definable. < The total length of stage i is definable. < The total length of stage i is definable. < The total length of stage i is definable. < The total length of stage i is definable. < The total length is definable."}, {"heading": "Acknowledgements", "text": "This research was partially supported by the French Ministry of Higher Education and Research, the Nord-Pas-de-Calais Regional Council and FEDER through CPER 2007-2013, the ANR projects EXPLO-RA (ANR-08-COSI-004) and Lampada (ANR-09-EMER-007), the Seventh Framework Programme of the European Communities (FP7 / 2007-2013) under Funding Agreement 231495 (CompLACS project) and Pascal-2."}], "references": [{"title": "Finite time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "REGAl: a regularization based algorithm for reinforcement learning in weakly communicating mdps", "author": ["Peter L. Bartlett", "Ambuj Tewari"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "R-max - a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I. Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Feature reinforcement learning: Part I: Unstructured MDPs", "author": ["Marcus Hutter"], "venue": "Journal of Artificial General Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1985}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "Bulletin of the American Mathematics Society,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1952}, {"title": "On the possibility of learning in reactive environments with arbitrary dependence", "author": ["Daniil Ryabko", "Marcus Hutter"], "venue": "Theoretical Compututer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "PAC model-free reinforcement learning", "author": ["Alexander L. Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L. Littman"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Optimistic linear programming gives logarithmic regret for irreducible mdps", "author": ["Ambuj Tewari", "Peter L. Bartlett"], "venue": "In Proceedings of Neural Information Processing Systems Conference (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "Namely, we use in our algorithm as a subroutine the algorithm UCRL2 of [6] that is designed to provide finite time bounds for undiscounted MDPs.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.", "startOffset": 119, "endOffset": 136}, {"referenceID": 10, "context": "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.", "startOffset": 119, "endOffset": 136}, {"referenceID": 11, "context": "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.", "startOffset": 119, "endOffset": 136}, {"referenceID": 5, "context": "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.", "startOffset": 119, "endOffset": 136}, {"referenceID": 2, "context": "Such a problem has been pioneered in the reinforcement learning literature by [7] and then improved in various ways by [4, 11, 12, 6, 3]; UCRL2 achieves a regret of the order DT 1/2 in any weakly-communicating MDP with diameter D, with respect to the best policy for this MDP.", "startOffset": 119, "endOffset": 136}, {"referenceID": 5, "context": "The diameter D of a MDP is defined in [6] as the expected minimum time required to reach any state starting from any other state.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "A related result is reported in [3], which improves on constants related to the characteristics of the MDP.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "A similar approach has been considered in [10]; the difference is that in that work the probabilistic characteristics of each model are completely known, but the models are not assumed to be Markovian, and belong to a countably infinite (rather than finite) set.", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "[9, 8, 1]): there are finitely many \u201carms\u201d, corresponding to the policies used in each model, and one of the arms is the best, in the sense that the corresponding model is the \u201ctrue\u201d one.", "startOffset": 0, "endOffset": 9}, {"referenceID": 7, "context": "[9, 8, 1]): there are finitely many \u201carms\u201d, corresponding to the policies used in each model, and one of the arms is the best, in the sense that the corresponding model is the \u201ctrue\u201d one.", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[9, 8, 1]): there are finitely many \u201carms\u201d, corresponding to the policies used in each model, and one of the arms is the best, in the sense that the corresponding model is the \u201ctrue\u201d one.", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "Therefore, the optimal rate of rewards can be obtained by a clever exploration/exploitation strategy, such as UCRL2 algorithm [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Then we present the proposed algorithm in Section 3; it uses UCRL2 of [6] as a subroutine and selects the models \u03c6 according to a penalized empirical criterion.", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Moreover, we assume that A is of finite cardinality A def = |A| and that 0 \u2208 R \u2282 [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 5, "context": "For that purpose we define the regret of any strategy at time T , like in [6, 3], as", "startOffset": 74, "endOffset": 80}, {"referenceID": 2, "context": "For that purpose we define the regret of any strategy at time T , like in [6, 3], as", "startOffset": 74, "endOffset": 80}, {"referenceID": 4, "context": "Another approach not discussed here would be to try to build a good state representation function, as what is suggested for instance in [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "The reader familiar with adversarial bandit literature will notice that our bound of order T 2/3 is worse than T 1/2 that usually appears in this context (see, for example [2]).", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "First, we notice that, as reported in [6], when we compute an optimistic model based on the empirical rewards and transitions of the true model, the span of the corresponding optimistic value function sp(V\u0302 ) is always smaller than the diameter D.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "In [3], the authors derive a regret bound that scales with the span of the true value function sp(V ), which is also less than D, and can be significantly smaller in some cases.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "1 Upper and Lower confidence bounds From the analysis of UCRL2 in [6], we have the property that with probability higher than 1 \u2212 \u03b4, the regret of UCRL2 when run for \u03c4 consecutive many steps from time t1 in the true model \u03c6 is upper bounded by \u03c1 \u2212 1 \u03c4 t1+\u03c4\u22121 \u2211", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Now from the analysis provided in [6] we know that when we run the UCRL2 with the true model \u03c6 with parameter \u03b4i(\u03b4), then there exists an event \u03a91,i of probability at least 1\u2212 \u03b4i(\u03b4) such that on this event \u03c1 \u2212 \u03bc\u0302i,1(\u03c6) 6 BD(\u03c4i,1,J , \u03c6, \u03b4i(\u03b4)) , and similarly there exists an event \u03a92,i of probability at least 1\u2212 \u03b4i(\u03b4), such that on this event \u03c1 \u2212 \u03bc\u0302i,2,t(\u03c6) 6 BD(\u03c4i,2(\u03c6), \u03c6, \u03b41(\u03b4)) .", "startOffset": 34, "endOffset": 37}], "year": 2013, "abstractText": "The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T 2/3 where T is the horizon time.", "creator": "LaTeX with hyperref package"}}}