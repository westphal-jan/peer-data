{"id": "1302.2553", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2013", "title": "Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning", "abstract": "We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order $O(T^{2/3})$ with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after $T$ time steps is $O(\\sqrt{T})$, with all constants reasonably small. This is optimal in $T$ since $O(\\sqrt{T})$ is the optimal regret in the setting of learning in a (single discrete) MDP.", "histories": [["v1", "Mon, 11 Feb 2013 17:55:49 GMT  (43kb)", "https://arxiv.org/abs/1302.2553v1", "in proceedings of ICML 2013"], ["v2", "Mon, 18 Mar 2013 09:11:15 GMT  (43kb)", "http://arxiv.org/abs/1302.2553v2", null]], "COMMENTS": "in proceedings of ICML 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["odalric-ambrym maillard", "phuong nguyen", "ronald ortner", "daniil ryabko"], "accepted": true, "id": "1302.2553"}, "pdf": {"name": "1302.2553.pdf", "metadata": {"source": "META", "title": "Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning", "authors": ["Odalric-Ambrym Maillard", "Phuong Nguyen"], "emails": ["odalricambrym.maillard@gmail.com", "nmphuong@cecs.anu.edu.au", "rortner@unileoben.ac.at", "daniil@ryabko.net"], "sections": [{"heading": null, "text": "ar Xiv: 130 2.25 53v2 [cs.LG] \u221a T), with all constants being relatively small. This is optimal in T, since O (\u221a T) is the optimal regret in the learning environment of a (single discrete) MDP."}, {"heading": "1. Introduction", "text": "In Reinforcement Learning (RL), an agent has a task to learn by interacting with the environment that is very costly. JMLR: W & CP Volume 28. Copyright 2013 by Author (s).Decision process (MDP) MarkovProceedings of the 30th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. Unfortunately, the real world is not (always) a finite state MDP, and the learner often has to find a suitable state representation model: a function that provides action sequences, observations, and rewards by the environment in such a way that the resulting process in the state space is Markovian, which reduces the problem of learning in a finite state. However, finding such a model is highly non-trivial."}, {"heading": "2. Setting", "text": "For each time step t = 1, 2,., let Ht = = O \u00b7 (A \u00b7 R \u00b7 O) t \u2212 1 be the set of histories up to time t, where O is the set of observations, A \u2192 is a finite set of actions and R = [0, 1] is the set of possible rewards. We look at the problem of reinforcing learning when the learner interacts sequentially with any unknown environment: first some initial observation h1 = o1% H1 = O is made available to the learner, then at any time step t > 0, the learner selects an action at the BA based on current history. He then receives the immediate reward rt and the next observation ot + 1 from the environment. Thus ht + 1 is the concatenation of ht with (at, rt, ot + 1).State representation models. Let \u2012 be a set of state-representation models."}, {"heading": "3. Algorithm", "text": "The OMS algorithm (represented in detail as algorithm 1 = false) runs in episodes k = 1, 2,., each consisting of several runs j = 1, 2,... In each run j of an episode k, starting with time t = tk, j, OMS selects a policy \u03c0k, j applying optimism in the face of uncertainty principle twice. First, in line 6, OMS considers for each model a set of allowable MDPs Mt, \u03c6 (defined by confidence intervals for previous estimates), and calculates a so-called optimistic MDP M + t (solution). First, in line 6, OMS considers for each model a set of allowable MDPs Mt, so that the average reward (M + t) is achieved (defined by confidence intervals for previous estimates), and calculates a so-called optimistic MDP M + t (solution)."}, {"heading": "4. Main result", "text": "We present now the main result of this work, an upper limit for regretting our OMS strategy. The limit concerns the diameter of a Markov model \u03c6, D (\u03c6), which is defined as the expected minimum time required to achieve any state starting from any other state in the MDP M (\u03c6) (Jaksch et al., 2010). Theorem 1 Let it act as the optimal model, i.e., it becomes an upper limit defined by (8D, S, E, E, E, Markovian). Then the regret thereof (B, T) of OMS (with parameter level) may be r.t. \u03c6 after any T > SA step by (8D, S, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E"}, {"heading": "In particular, if for all \u03c6 \u2208 \u03a6, S\u03c6 6 B, then S 6 B|\u03a6| and hence with high probability", "text": "(D).D.D (A).D (D).A).D (D).D (D).A).D (D).D (D).D (D).A).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D (D).D).D (D).D.D.D.D.D (D).D.D.D (D).D (D).D (D).D (D).D.D (D).D.D (D).D.D (D).D.D (D).D.D (D).D.D (D).D (D).D.D (D).D (D).D.D (D).D (D).D (D).D.D (D).D (D).D (D).D.D (D).D (D).D (D).D.D (D).D.D (D).D (D).D.D (D).D.D (D).D (D).D (D).D (D).D.D (D).D.D (D).D (D).D (D).D (D).D (D).D.D (D).D (D).D (D).D.D (D).D (D).D (D).D (D).D (D).D (D).D (D).D.D.D.D (D (D).D (D).D (D).D (D).D (D).D.D (D).D (D).D (D).D (D).D.D (D).D.D (D (D).D).D (D (D).D.D (D).D (D).D"}, {"heading": "5.2. Regret analysis", "text": "Next, we consider a model (k), which does not necessarily have to be k), k), k), k), k), k), k), k), k (k), k), k (k), k), k), k), k), k), k), k), k), k), k), k), k (k), k (k), k (k), k), k), k), k), k), k), k), k), k), k), k), k), k), k), k (k), k (k), k (k), k (k), k (k), k (k), k), k (k), k (k), k), k (k), k (k), k), k (k), k (k), k (k), k), k (k), k (k), k), k (k), k), k (k), k), k), k (k), k), k), k), k), k), k (k), k), k), k (k), k), k), k (k), k), k), k (k), k), k), k (k), k), k (k), k), k), k), k), k (k), k), k), k), k), k (k), k), k (k), k), k), k), k), k), k (k), k), k), k), k (k), k), k), k), k (k), k), k), k (k), k), k), k), k), k), k (k), k), k), k), k), k (k), k), k), k), k (k), k), k), k), k), k), k), k), k), k), k"}, {"heading": "6. Outlook", "text": "The first natural question about the performance guarantees achieved is whether they are optimal. From the corresponding lower limits for learning MDPs (Jaksch et al., 2010), we know that the dependence on T that we obtain for OMS is actually optimal. Among other parameters, perhaps the most important is the number of models | \u03a6 |; here we suspect that the dependence that we obtain is optimal, but this has yet to be proven. Other parameters are the size of the action and state spaces for each model; here we lose in relation to the precursor BLB algorithm (see the note according to Theorem 1) and therefore have room for improvement. It may be possible to achieve a better dependence for OMS at the expense of a more differentiated analysis. Note, however, that there are no known algorithms for learning even a single MDP that would know an optimal dependence on all of these parameters. Another important direction for future research is an infinite number of models; perhaps a more infinite step, a more natural one, where the problem is a more obvious one, a certain measure."}, {"heading": "Acknowledgments", "text": "This work was supported by the French National Research Agency (ANR-08-COSI-004 project EXPLORA), the Seventh Framework Programme of the European Community (FP7 / 2007-2013) under funding agreements 270327 (CompLACS) and 216886 (PASCAL2), the Nord-Pas-de-Calais Regional Council and FEDER through CPER 2007-2013, the Austrian Science Fund (FWF): J 3259-N13 and the Australian Discovery Project DP120100950, NICTA."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order O(T ) with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after T time steps is O( \u221a T ), with all constants reasonably small. This is optimal in T since O( \u221a T ) is the optimal regret in the setting of learning in a (single discrete) MDP.", "creator": "LaTeX with hyperref package"}}}