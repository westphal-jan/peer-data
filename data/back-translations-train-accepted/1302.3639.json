{"id": "1302.3639", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2013", "title": "A Latent Source Model for Nonparametric Time Series Classification", "abstract": "We study a binary classification problem whereby an infinite time series having one of two labels (\"event\" or \"non-event\") streams in, and we want to predict the label of the time series. Intuitively, the longer we wait, the more of the time series we see and so the more accurate our prediction could be. Conversely, making a prediction too early could result in a grossly inaccurate prediction. In numerous applications, such as predicting an imminent market crash or revealing which topics will go viral in a social network, making an accurate prediction as early as possible is highly valuable. Motivated by these applications, we propose a generative model for time series which we call a latent source model and which we use for non-parametric online time series classification. Our main assumption is that there are only a few ways in which a time series corresponds to an \"event\", such as a market crashing or a Twitter topic going viral, and that we have access to training data that are noisy versions of these few distinct modes. Our model naturally leads to weighted majority voting as a classification rule, which operates without knowing nor learning what the few latent sources are. We establish theoretical performance guarantees of weighted majority voting under the latent source model and then use the voting to predict which news topics on Twitter will go viral to become trends.", "histories": [["v1", "Thu, 14 Feb 2013 22:12:40 GMT  (1095kb,D)", "https://arxiv.org/abs/1302.3639v1", null], ["v2", "Mon, 25 Mar 2013 01:37:27 GMT  (1095kb,D)", "http://arxiv.org/abs/1302.3639v2", null], ["v3", "Tue, 26 Mar 2013 15:28:08 GMT  (1095kb,D)", "http://arxiv.org/abs/1302.3639v3", null], ["v4", "Sat, 9 Nov 2013 00:21:07 GMT  (1110kb,D)", "http://arxiv.org/abs/1302.3639v4", "Advances in Neural Information Processing Systems (NIPS 2013)"], ["v5", "Fri, 13 Dec 2013 04:20:34 GMT  (1112kb,D)", "http://arxiv.org/abs/1302.3639v5", "Advances in Neural Information Processing Systems (NIPS 2013)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.SI", "authors": ["george h chen", "stanislav nikolov", "devavrat shah"], "accepted": true, "id": "1302.3639"}, "pdf": {"name": "1302.3639.pdf", "metadata": {"source": "CRF", "title": "A Latent Source Model for Nonparametric Time Series Classification", "authors": ["George H. Chen", "Stanislav Nikolov", "Devavrat Shah"], "emails": ["georgehc@mit.edu", "snikolov@twitter.com", "devavrat@mit.edu"], "sections": [{"heading": null, "text": "In order to classify time series, in practice, the approach of next-door neighbor is often used, where the performance often competes with that of next-door neighbor or is better than with more complex methods such as neural networks, decision trees and vector machines. We develop a theoretical justification for the effectiveness of next-door neighbor classification of time series. Our leading hypothesis is that in many applications, such as predicting which topics on Twitter will become trends, there are not really as many prototypical time series, relative to the number of time series to which we have access, e.g. topics on Twitter will become trends only in a few distinct manners, while we can collect enormous amounts of Twitter data. In order to operationalize this hypothesis, we propose a latent source model for time series, which of course leads to a classification rule of the \"weighted majority,\" which can be fully incorporated by a next-door neighbor classifier, not even in the classification of 79%."}, {"heading": "1 Introduction", "text": "This year it has come to the point where it is only a matter of time before it will happen, until it happens, until it happens."}, {"heading": "2 Weighted Majority Voting and Nearest-Neighbor Classification", "text": "Faced with a time series2 s: Z \u2192 R, we want to classify them as either q = bor q (\"trend\") or \u2212 1 (\"not trend\"). To do this, we have access to labeled training data R + and R \u2212, which denotes the sets of all training time series with the denominations + 1 and \u2212 1, respectively. Weighted majority votes. (Each positively labeled example r \u2212 R + throws a weighted vote e \u2212 g (r, s) for the question of whether the time series s denotes + 1, where d (T) (r, s) is a measure of the similarity between the two time series r and s. (T) Superscript (T) indicates that we may only consider the first T time steps (i.e., the time steps 1, 2,., T) of s (but we may look outside of these training time series r), and the constant duration of 0 is a scaling parameter that determines the \"sphere of influence\" of each example."}, {"heading": "3 A Latent Source Model and Theoretical Guarantees", "text": "We assume that there are m unknown latent sources (time series) that generate observed time series."}, {"heading": "4 Experimental Results", "text": "In fact, we are in a position to go in search of a solution that enables us, puts us in a position to put ourselves in the position we are in, \"he said.\" We have to be able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position to put ourselves in the position we are in, \"he said."}, {"heading": "A Proof of Theorem 1", "text": "-, - Q +, - V +, - V - M +, - D +, - D +, - D +, - D +, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -,"}, {"heading": "B Proof of Theorem 2", "text": "The proof uses steps similar to the weighted majority choice. As before, we consider the case when our training data see each latent source at least once (this event occurs with a probability of at least 1 \u2212 \u2212 p \u2212 p). We break down the error probability into terms depending on which latent source has generated V: P (L) NN (T) NN (S) 6 = L) = V (V = v) P (T) NN (S) 6 = L | V = v) = V 1 m P (L) (T) NN (T) NN (S) 6 = L | V (T) NN (T) 6 = V = V) = v) = v) V + V (T) 1 m P (T) 1 m (T) NN (T)."}, {"heading": "C Handling Non-uniformly Sampled Latent Sources", "text": "If each time series generated from the latent source model is sampled uniformly randomly, then our training data see each latent source at least once. If the latent sources are not sampled uniformly at random, we show that we can simply replace the condition n > m log 2m3 with n \u2265 8 \u03c0minlog 2m3 to achieve a similar (actually stronger) guarantee, whereby \u03c0min is the least probability of a particular latent source. Lemma 3. Suppose that the i-te latent source in the latent source model occurs with the probability \u03c0i. Name \u03c0min, mini \u043fmin,..., m \u00b2 \u03c0i the number of times in which the i-te latent source appears in the training data."}, {"heading": "D Sample Complexity for the Gaussian Setting Without Time Shifts", "text": "The existing results of the learning mixtures of the Gaussians of Dasgupta and Schulman (7) and Vempala and Wang (19) each refer to a different conception of gap than we do. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 In our notation, their gap can be written in such a way that it refers to gap (T) -V s.t.v 6 = v \u00b2 v \u00b2 v \u00b2 -V \u00b2 2T, (37) which measures the minimal separation between the true latent sources, without taking into account their designations. We now translate our main theoretical guarantees that it refers to gap (T) -V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 V \u00b2 T \u00b2 V \u00b2 V \u00b2 V \u00b2 T \u00b2 V \u00b2 V \u00b2 T \u00b2 V \u00b2 T \u00b2 V \u00b2 T \u00b2 s and that there are no time shifts. Theorem 4. Under the latent source model, we assume that noise is zero-mean Gaussian with variance 2, that there are no time shifts (i.e.), that we max = 0), and that we have m3 and that we have 4 series 4."}, {"heading": "E Forecasting Trending Topics on Twitter", "text": "It is a network whose users post messages called tweets, which are then sent to a user's followers who do not know which phrases are considered a topic. Frequently, emerging topics of interest are discussed on Twitter in real-time. We evaluated 500 random examples of trends from a list of trends in June 2012 and recorded the earliest time of each topic within the month, while we filtered out trends that never ranked 3 or better on Twitter Trends5 as well as trends that took less than 30 minutes to keep our trend examples reasonably up to date. We also randomly selected 500 examples of non-trends from a list of n-grams (sizes 1, 2 and 3) that appear in tweets in which we filter out n-gram-containing words that appeared in one of our 500 selected trend examples, as we randomly select non-trends."}], "references": [{"title": "Trends in social media: Persistence and decay", "author": ["Sitaram Asur", "Bernardo A. Huberman", "G\u00e1bor Szab\u00f3", "Chunyan Wang"], "venue": "In Proceedings of the Fifth International Conference on Weblogs and Social Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Transformation based ensembles for time series classification", "author": ["Anthony Bagnall", "Luke Davis", "Jon Hills", "Jason Lines"], "venue": "In Proceedings of the 12th SIAM International Conference on Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A complexity-invariant distance measure for time series", "author": ["Gustavo E.A.P.A. Batista", "Xiaoyue Wang", "Eamonn J. Keogh"], "venue": "In Proceedings of the 11th SIAM International Conference on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Beyond trending topics: Real-world event identification on Twitter", "author": ["Hila Becker", "Mor Naaman", "Luis Gravano"], "venue": "In Proceedings of the Fifth International Conference on Weblogs and Social Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Emerging topic detection on twitter based on temporal and social terms evaluation", "author": ["Mario Cataldi", "Luigi Di Caro", "Claudio Schifanella"], "venue": "In Proceedings of the 10th International Workshop on Multimedia Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Nearest neighbor pattern classification", "author": ["Thomas M. Cover", "Peter E. Hart"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1967}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["Hui Ding", "Goce Trajcevski", "Peter Scheuermann", "Xiaoyue Wang", "Eamonn Keogh"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Scalable training of mixture models via coresets", "author": ["Dan Feldman", "Matthew Faulkner", "Andreas Krause"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Introduction to statistical pattern recognition (2nd ed.)", "author": ["Keinosuke Fukunaga"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Learning mixtures of spherical gaussians: Moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M. Kakade"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Emerging topic detection using dictionary learning", "author": ["Shiva Prasad Kasiviswanathan", "Prem Melville", "Arindam Banerjee", "Vikas Sindhwani"], "venue": "In Proceedings of the 20th ACM Conference on Information and Knowledge Management,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Online l1dictionary learning with application to novel document detection", "author": ["Shiva Prasad Kasiviswanathan", "Huahua Wang", "Arindam Banerjee", "Prem Melville"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["Beatrice Laurent", "Pascal Massart"], "venue": "Annals of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Twittermonitor: trend detection over the Twitter stream", "author": ["Michael Mathioudakis", "Nick Koudas"], "venue": "In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Feature-based classification of time-series data", "author": ["Alex Nanopoulos", "Rob Alcock", "Yannis Manolopoulos"], "venue": "International Journal of Computer Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Interval and dynamic time warping-based decision trees", "author": ["Juan J. Rodr\u0131\u0301guez", "Carlos J. Alonso"], "venue": "In Proceedings of the 2004 ACM Symposium on Applied Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Lawrence K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Distance-function design and fusion for sequence data", "author": ["Yi Wu", "Edward Y. Chang"], "venue": "In Proceedings of the 2004 ACM International Conference on Information and Knowledge Management,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Numerous standard classification methods have been tailored to classify time series, yet a simple nearest-neighbor approach is hard to beat in terms of classification performance on a variety of datasets [22], with results competitive to or better than various other more elaborate methods such as neural networks [17], decision trees [18], and support vector machines [21].", "startOffset": 314, "endOffset": 318}, {"referenceID": 17, "context": "Numerous standard classification methods have been tailored to classify time series, yet a simple nearest-neighbor approach is hard to beat in terms of classification performance on a variety of datasets [22], with results competitive to or better than various other more elaborate methods such as neural networks [17], decision trees [18], and support vector machines [21].", "startOffset": 335, "endOffset": 339}, {"referenceID": 20, "context": "Numerous standard classification methods have been tailored to classify time series, yet a simple nearest-neighbor approach is hard to beat in terms of classification performance on a variety of datasets [22], with results competitive to or better than various other more elaborate methods such as neural networks [17], decision trees [18], and support vector machines [21].", "startOffset": 369, "endOffset": 373}, {"referenceID": 2, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 100, "endOffset": 110}, {"referenceID": 7, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 100, "endOffset": 110}, {"referenceID": 19, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 100, "endOffset": 110}, {"referenceID": 1, "context": "More recently, researchers have examined which distance to use with nearest-neighbor classification [3, 8, 20] or how to boost classification performance by applying different transformations to the time series before using nearest-neighbor classification [2].", "startOffset": 256, "endOffset": 259}, {"referenceID": 5, "context": "If we don\u2019t confine ourselves to classifying time series, then as the amount of data tends to infinity, nearest-neighbor classification has been shown to achieve a probability of error that is at worst twice the Bayes error rate, and when considering the nearest k neighbors with k allowed to grow with the amount of data, then the error rate approaches the Bayes error rate [6].", "startOffset": 375, "endOffset": 378}, {"referenceID": 6, "context": "We show that existing performance guarantees on learning spherical Gaussian mixture models [7, 11, 19] require more stringent conditions than what our results need, suggesting that learning the latent sources is overkill if the goal is classification.", "startOffset": 91, "endOffset": 102}, {"referenceID": 10, "context": "We show that existing performance guarantees on learning spherical Gaussian mixture models [7, 11, 19] require more stringent conditions than what our results need, suggesting that learning the latent sources is overkill if the goal is classification.", "startOffset": 91, "endOffset": 102}, {"referenceID": 18, "context": "We show that existing performance guarantees on learning spherical Gaussian mixture models [7, 11, 19] require more stringent conditions than what our results need, suggesting that learning the latent sources is overkill if the goal is classification.", "startOffset": 91, "endOffset": 102}, {"referenceID": 3, "context": "Existing work that identify trends on Twitter [4, 5, 15] instead, as part of their trend detection, define models for what trends are, which we do not do, nor do we assume we have access to such definitions.", "startOffset": 46, "endOffset": 56}, {"referenceID": 4, "context": "Existing work that identify trends on Twitter [4, 5, 15] instead, as part of their trend detection, define models for what trends are, which we do not do, nor do we assume we have access to such definitions.", "startOffset": 46, "endOffset": 56}, {"referenceID": 14, "context": "Existing work that identify trends on Twitter [4, 5, 15] instead, as part of their trend detection, define models for what trends are, which we do not do, nor do we assume we have access to such definitions.", "startOffset": 46, "endOffset": 56}, {"referenceID": 11, "context": "(The same could be said of previous work on novel document detection on Twitter [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 12, "context": "(The same could be said of previous work on novel document detection on Twitter [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 15, "context": "In general, the number of samples we need from a Gaussian mixture mixture model to estimate the mixture component means is exponential in the number of mixture components [16].", "startOffset": 171, "endOffset": 175}, {"referenceID": 6, "context": "We could learn such a model using Dasgupta and Schulman\u2019s modified EM algorithm [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 18, "context": "Vempala and Wang [19] have a spectral method for learning Gaussian mixture models that can handle smallerG )\u2217 than Dasgupta and Schulman\u2019s approach but requires n = \u03a9\u0303(T m) training data, where we\u2019ve hidden the dependence on \u03c3 and other variables of interest for clarity of presentation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Hsu and Kakade [11] have a moment-based estimator that doesn\u2019t have a gap condition but, under a different non-degeneracy condition, requires substantially more samples for our problem setup, i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "In such scenarios, one could instead non-uniformly subsample O(Tm/\u03b5) time series from the training data using the procedure given in [9] and then feed the resulting smaller dataset, referred to as an (m, \u03b5)-coreset, to the EM algorithm for learning the latent sources.", "startOffset": 133, "endOffset": 136}], "year": 2013, "abstractText": "For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren\u2019t actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a \u201cweighted majority voting\u201d classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such \u201ctrending topics\u201d in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%.", "creator": "LaTeX with hyperref package"}}}