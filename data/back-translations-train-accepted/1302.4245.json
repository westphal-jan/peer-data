{"id": "1302.4245", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation", "abstract": "Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.", "histories": [["v1", "Mon, 18 Feb 2013 12:41:50 GMT  (448kb,D)", "http://arxiv.org/abs/1302.4245v1", "15 pages, 5 figures, 1 table. Submitted for publication"], ["v2", "Tue, 19 Feb 2013 12:52:04 GMT  (448kb,D)", "http://arxiv.org/abs/1302.4245v2", "15 pages, 5 figures, 1 table. Submitted for publication"], ["v3", "Tue, 31 Dec 2013 16:41:30 GMT  (456kb,D)", "http://arxiv.org/abs/1302.4245v3", "10 pages, 5 figures, 1 table. Minor edits and titled changed from \"Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation\" to \"Gaussian Process Kernels for Pattern Discovery and Extrapolation\". Appears at the International Conference on Machine Learning (ICML), JMLR W&amp;CP 28(3):1067-1075, 2013"]], "COMMENTS": "15 pages, 5 figures, 1 table. Submitted for publication", "reviews": [], "SUBJECTS": "stat.ML cs.AI stat.ME", "authors": ["andrew gordon wilson", "ryan prescott adams"], "accepted": true, "id": "1302.4245"}, "pdf": {"name": "1302.4245.pdf", "metadata": {"source": "CRF", "title": "Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation", "authors": ["Andrew Gordon Wilson", "Ryan Prescott Adams"], "emails": ["agw38@cam.ac.uk", "rpa@seas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "The first models of machine learning, such as the Perceptron functions (Rosenblatt, 1962), are based on a simple model of a neuron (McCulloch and Pitts, 1943). Indeed, machine learning aims not only to equip people with tools to analyze data, but also to fully automate learning and decision-making. Research on Gaussian processes (GPs) within the machine learning community, which has evolved from neural networks, is triggered by Neal (1996), which has observed that Bayesian neural networks have become hidden entities approaching infinity. Neal (1996) predicts that \"there are easier ways to draw conclusions in this case.\""}, {"heading": "2 Gaussian Processes", "text": "A Gaussian process is a collection of random variables, each finite number having a common Gaussian distribution. By means of a Gaussian process, we can define a distribution using the functions f (x), f (x) \u0445 GP (x), k (x), (1), where x-RP is an arbitrary input variable and the mean function m (x) and the covariant core k (x, x) as asasm (x) = E [f (x)]], (2) k (x) = cov (f), f (x)))). (3) Each collection of function values has a common Gaussian distribution [f (x1), f (x2)],.., f (xsie)] > Williams' N (\u00b5, K) = cov (f), f (x), (4), with the N-N coance matrix K simply having the entries Kij (xi, xj) and the mean."}, {"heading": "3 Kernels for Pattern Discovery", "text": "In this section we present a class of nuclei that can represent patterns, extrapolates, and negative covariances, containing a large number of stationary nuclei. Roughly speaking, a kernel measures the similarity between data points. However, as in Equation (3), the covariance nucleus of a GP determines how the associated random functions can tend to vary with input factors (predictors). A stationary kernel is a function of the profile (Bochner, 1959; Stein, 1999; Gikhman and Skorokhod, 2004): Theorem 3.1 (Bochner) A complex weighted function k on RP is the covariance function of a poorly measured procedure on RP."}, {"heading": "4 Experiments", "text": "We show how the SM kernel in Eq. (12) can be used to discover patterns, extrapolated, and model negative covariances. We contrast the SM kernel with popular cores in, e.g., Rasmussen and Williams (1997), which usually only provides smooth interpolation. Although the SM kernel generally improves the prediction probability over popular alternatives, we focus on the clear visualization of learned cores and spectral densities to discover patterns and predictions in the data, our goal is to identify the fundamental differences between the proposed SM kernel and the alternative users. In all experiments, it is assumed that the marginalization via the unknown function can be performed in a closed form. Kernel hyperparameters are trained using non-linear conjugate gradiments to optimize the marginal gradiments in order to optimize the marginal probabilities."}, {"heading": "4.2 Recovering Popular Kernels", "text": "The SM class of the kernel contains many stationary kernels, because mixtures of Gaussians are only integrated into components Q = 4. The SM class of the kernel contains many stationary kernels, because mixtures of Gaussians are only used in education. Q = > Q can also be used to construct a wide range of spectral densities. Even with a small number of mixture components, e.g. Q \u2264 10, the SM kernel can closely restore the most popular stationary kernels catalogued in Rasmussen and Williams (2006). As an example, we begin to capture 100 points from a one-dimensional GP with a mate-rn kernel with degrees of freedom."}, {"heading": "4.3 Negative Covariances", "text": "All stationary covariance functions in the standard machine learning Gaussian process reference Rasmussen and Williams (2006) are positive everywhere, including the periodic nucleus, k (\u03c4) = exp (\u2212 2 sin2 (\u03c0 \u03c4) / '2. While positive covariances are often suitable for interpolation, capturing negative covariances can be crucial for the extrapolation of patterns: linear trends exhibit wide-ranging negative covariances. We test the ability of the SM nucleus to learn a negative covariance function by scanning 400 points from a simple AR (1) discrete TimeGaussian process: y (x + 1) = \u2212 e \u2212 0.01y (x) + \u03c3 (x), (16) (x) \u0445 N (0, 1), (17), the kernelk (x, x \u2032) shows that the process has a positive covariance function (\u2212 e \u2212 0.01y (+ 0x) + 0.1y (x)."}, {"heading": "4.4 Discovering the Sinc Pattern", "text": "The sink function is defined as sinc (x) = sin (\u03c0x) / (\u03c0x) pattern with an almost perfect pattern Q. In addition, we have created a pattern by combining three sink functions: y (x) = sinc (x + 10) + sinc (x \u2212 10) + sinc (x \u2212 10). (19) This is a complex oscillating pattern. Given only the points shown in Figure 4a, we would like to complete the pattern for x (\u2212 4,5, 4,5]. Unlike the CO2 example in Section 4.1, it may even be difficult for a human to extrapolate the missing pattern from the training data. It is an interesting exercise to focus on this figure, identify features and fill in the missing part."}, {"heading": "4.5 Airline Passenger Data", "text": "In recent years, it has been shown that the number of unemployed who are able to do their jobs has increased significantly. (...) The number of unemployed who are able to do their jobs is increasing. (...) The number of unemployed who are able to do their jobs is increasing. (...) The number of unemployed who are able to do their jobs is increasing. (...) The number of unemployed who are able to do their jobs is increasing. (...) The number of unemployed who are able to do their jobs is increasing. (...) The number of unemployed who are able to do their jobs is increasing. (...) The number of workers who are able to do their jobs is increasing. (...) The number of workers who are employed in the world of work is increasing. (...) The number of workers who are employed in the world of work is increasing. (...) The number of workers who are employed in the world of work is increasing. (...)"}, {"heading": "5 Discussion", "text": "The simplicity of these cores is one of their strongest characteristics: they can be used as drop-in substitutes for popular cores such as the square exponential nucleus, with great benefits in terms of expressiveness and performance while maintaining simple training and inference procedures. Gaussian processes have proven to be powerful smoothing interpolators. We believe that pattern recognition and extrapolation are an exciting new direction for nonparametric approaches that can capture rich variations in data. At this point, we have shown how non-parametric Bayesian models can be used naturally to generalize a pattern from a small number of examples. We have only begun to explore what could be done with such pattern-finding methods. In future work, one could incorporate spectral density by applying recently developed efficient MCMC models for significant MGP parameters (applied models for Murray-Adams hypertractions) and more recent saddles."}, {"heading": "Acknowledgements", "text": "We thank Richard E. Turner, Carl Edward Rasmussen and Neil D. Lawrence for interesting discussions."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Derivations for Spectral Mixture Kernels", "text": "A stationary kernel k (x, x) is the inverse Fourier transformation of its spectral density (q) (f), k (f) = p (s), e2\u03c0is > \u03c4ds, (20), where \u03c4 = x \u2212 x \u2032. Replace (21) in (20), k (x, x) = \"exp\" (2), (x \u2212 x) 2, (21), where s, \u00b5 \u2212 \u03c32 (s \u2212 \u00b5) 2) ds (22) we leave both in (20), k (x, x \u2032) = \"exp\" (2), (s2 \u2212 x), 1 \u00b2 \"exp\" (\u2212 2), \u2212 p \"exp\" (\u2212 2), p \"exp\" (22), p \"x \u2212 x \u2032 = 1\" imp, \"p\" imp, \"p.\""}, {"heading": "6.2 Comment on Training Hyperparameters", "text": "In general, we have succeeded in naively training kernel hyperparameters using conjugated gradients (we are using Carl Rasmussen's 2010 version of minimize.m) to maximize the limit probability p (y | \u03b8) of the data y taking into account the hyper-factors \u03b8, analytically removing a Gaussian process of zero mean. We have found that subtracting an empirical mean from the data before training hyperparameters (with conjugated gradients) is undesirable, which sometimes leads to local optimizations with lower limit probability, especially for small data sets with increasing trend."}], "references": [{"title": "A review of Gaussian random fields and correlation functions", "author": ["P. Abrahamsen"], "venue": "Norweigan Computing Center Technical report.", "citeRegEx": "Abrahamsen,? 1997", "shortCiteRegEx": "Abrahamsen", "year": 1997}, {"title": "Multiple Gaussian process models", "author": ["C. Archambeau", "F. Bach"], "venue": "arXiv preprint arXiv:1110.5238.", "citeRegEx": "Archambeau and Bach,? 2011", "shortCiteRegEx": "Archambeau and Bach", "year": 2011}, {"title": "Time Series Analysis: An Introduction", "author": ["C. Chatfield"], "venue": "London: Chapman and Hall.", "citeRegEx": "Chatfield,? 1989", "shortCiteRegEx": "Chatfield", "year": 1989}, {"title": "Statistics for Spatial Data (Wiley Series in Probability and Statistics)", "author": ["N. Cressie"], "venue": "WileyInterscience.", "citeRegEx": "Cressie,? 1993", "shortCiteRegEx": "Cressie", "year": 1993}, {"title": "Deep Gaussian processes", "author": ["A. Damianou", "N. Lawrence"], "venue": "arXiv preprint arXiv:1211.0358.", "citeRegEx": "Damianou and Lawrence,? 2012", "shortCiteRegEx": "Damianou and Lawrence", "year": 2012}, {"title": "Additive kernels for Gaussian process modeling", "author": ["N. Durrande", "D. Ginsbourger", "O. Roustant"], "venue": "arXiv preprint arXiv:1103.4023.", "citeRegEx": "Durrande et al\\.,? 2011", "shortCiteRegEx": "Durrande et al\\.", "year": 2011}, {"title": "The Theory of Stochastic Processes, volume 2", "author": ["I. Gikhman", "A. Skorokhod"], "venue": "Springer Verlag.", "citeRegEx": "Gikhman and Skorokhod,? 2004", "shortCiteRegEx": "Gikhman and Skorokhod", "year": 2004}, {"title": "Multiple kernel learning algorithms", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "Journal of Machine Learning Research, 12:2211\u20132268.", "citeRegEx": "G\u00f6nen and Alpayd\u0131n,? 2011", "shortCiteRegEx": "G\u00f6nen and Alpayd\u0131n", "year": 2011}, {"title": "The Analysis of Linear Partial Differential Operators I, Distribution Theory and Fourier Analysis", "author": ["L. H\u00f6rmander"], "venue": "Springer-Verlag.", "citeRegEx": "H\u00f6rmander,? 1990", "shortCiteRegEx": "H\u00f6rmander", "year": 1990}, {"title": "Time series data library", "author": ["R. Hyndman"], "venue": "http://www-personal.buseco.monash.edu.au/ ~hyndman/TSDL/. Keeling, C. D. and Whorf, T. P. (2004). Atmospheric CO2 records from sites in the SIO air sampling network. Trends: A Compendium of Data on Global Change. Carbon Dioxide Information Analysis Center.", "citeRegEx": "Hyndman,? 2005", "shortCiteRegEx": "Hyndman", "year": 2005}, {"title": "Gaussian mixtures and their applications to signal processing", "author": ["N. Kostantinos"], "venue": "Advanced Signal Processing Handbook: Theory and Implementation for Radar, Sonar, and Medical Imaging Real Time Systems.", "citeRegEx": "Kostantinos,? 2000", "shortCiteRegEx": "Kostantinos", "year": 2000}, {"title": "Bayesian nonlinear modeling for the prediction competition", "author": ["D MacKay"], "venue": "Ashrae Transactions, 100(2):1053\u20131062.", "citeRegEx": "MacKay,? 1994", "shortCiteRegEx": "MacKay", "year": 1994}, {"title": "Introduction to Gaussian processes", "author": ["D.J. MacKay"], "venue": "Christopher M. Bishop, e., editor, Neural Networks and Machine Learning, chapter 11, pages 133\u2013165. Springer-Verlag.", "citeRegEx": "MacKay,? 1998", "shortCiteRegEx": "MacKay", "year": 1998}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W. McCulloch", "W. Pitts"], "venue": "Bulletin of mathematical biology, 5(4):115\u2013133.", "citeRegEx": "McCulloch and Pitts,? 1943", "shortCiteRegEx": "McCulloch and Pitts", "year": 1943}, {"title": "Slice sampling covariance hyperparameters in latent Gaussian models", "author": ["I. Murray", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems 23.", "citeRegEx": "Murray and Adams,? 2010", "shortCiteRegEx": "Murray and Adams", "year": 2010}, {"title": "Bayesian Learning for Neural Networks", "author": ["R. Neal"], "venue": "Springer Verlag.", "citeRegEx": "Neal,? 1996", "shortCiteRegEx": "Neal", "year": 1996}, {"title": "Evaluation of Gaussian Processes and Other Methods for Non-linear Regression", "author": ["C.E. Rasmussen"], "venue": "PhD thesis, University of Toronto.", "citeRegEx": "Rasmussen,? 1996", "shortCiteRegEx": "Rasmussen", "year": 1996}, {"title": "Gaussian processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K. Williams"], "venue": "The MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2006", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Principles of Neurodynamics", "author": ["F. Rosenblatt"], "venue": "Spartan Book.", "citeRegEx": "Rosenblatt,? 1962", "shortCiteRegEx": "Rosenblatt", "year": 1962}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": "Nature, 323(6088):533\u2013536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Scalable Inference for Structured Gaussian Process Models", "author": ["Y. Saatchi"], "venue": "PhD thesis, University of Cambridge.", "citeRegEx": "Saatchi,? 2011", "shortCiteRegEx": "Saatchi", "year": 2011}, {"title": "Using deep belief nets to learn covariance kernels for Gaussian processes", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 20:1249\u20131256.", "citeRegEx": "Salakhutdinov and Hinton,? 2008", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2008}, {"title": "Interpolation of Spatial Data: Some Theory for Kriging", "author": ["M. Stein"], "venue": "Springer Verlag.", "citeRegEx": "Stein,? 1999", "shortCiteRegEx": "Stein", "year": 1999}, {"title": "Probabilistic inference in human semantic memory", "author": ["M. Steyvers", "T. Griffiths", "S. Dennis"], "venue": "Trends in Cognitive Sciences, 10(7):327\u2013334.", "citeRegEx": "Steyvers et al\\.,? 2006", "shortCiteRegEx": "Steyvers et al\\.", "year": 2006}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["J. Tenenbaum", "C. Kemp", "T. Griffiths", "N. Goodman"], "venue": "Science, 331(6022):1279\u20131285.", "citeRegEx": "Tenenbaum et al\\.,? 2011", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2011}, {"title": "Bayesian inference: An introduction to principles and practice in machine learning", "author": ["M. Tipping"], "venue": "Advanced Lectures on Machine Learning, pages 41\u201362.", "citeRegEx": "Tipping,? 2004", "shortCiteRegEx": "Tipping", "year": 2004}, {"title": "Gaussian process regression networks", "author": ["A.G. Wilson", "D.A. Knowles", "Z. Ghahramani"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML).", "citeRegEx": "Wilson et al\\.,? 2012", "shortCiteRegEx": "Wilson et al\\.", "year": 2012}, {"title": "Vision as Bayesian inference: analysis by synthesis", "author": ["A. Yuille", "D. Kersten"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Yuille and Kersten,? \\Q2006\\E", "shortCiteRegEx": "Yuille and Kersten", "year": 2006}], "referenceMentions": [{"referenceID": 18, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943).", "startOffset": 58, "endOffset": 76}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943).", "startOffset": 119, "endOffset": 146}, {"referenceID": 17, "context": "These simple inference techniques became the cornerstone of subsequent Gaussian process models for machine learning (Rasmussen and Williams, 2006).", "startOffset": 116, "endOffset": 146}, {"referenceID": 17, "context": "Gaussian process models have become popular for non-linear regression and classification (Rasmussen and Williams, 2006), and often have impressive empirical performance (Rasmussen, 1996).", "startOffset": 89, "endOffset": 119}, {"referenceID": 16, "context": "Gaussian process models have become popular for non-linear regression and classification (Rasmussen and Williams, 2006), and often have impressive empirical performance (Rasmussen, 1996).", "startOffset": 169, "endOffset": 186}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943). Papers such as Rumelhart et al. (1986) inspired hope that it would be possible to develop intelligent agents with models like neural networks, which could automatically discover hidden representations in data.", "startOffset": 120, "endOffset": 187}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943). Papers such as Rumelhart et al. (1986) inspired hope that it would be possible to develop intelligent agents with models like neural networks, which could automatically discover hidden representations in data. Indeed, machine learning aims not only to equip humans with tools to analyze data, but to fully automate the learning and decision making process. Research on Gaussian processes (GPs) within the machine learning community developed out of neural networks research, triggered by Neal (1996), who observed that Bayesian neural networks became Gaussian processes as the number of hidden units approached infinity.", "startOffset": 120, "endOffset": 648}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943). Papers such as Rumelhart et al. (1986) inspired hope that it would be possible to develop intelligent agents with models like neural networks, which could automatically discover hidden representations in data. Indeed, machine learning aims not only to equip humans with tools to analyze data, but to fully automate the learning and decision making process. Research on Gaussian processes (GPs) within the machine learning community developed out of neural networks research, triggered by Neal (1996), who observed that Bayesian neural networks became Gaussian processes as the number of hidden units approached infinity. Neal (1996) conjectured that \u201cthere may be simpler ways to do inference in this case\u201d.", "startOffset": 120, "endOffset": 781}, {"referenceID": 12, "context": "Such simple smoothing devices are not a realistic replacement for neural networks, which were envisaged as intelligent agents that could discover hidden features in data via adaptive basis functions (MacKay, 1998).", "startOffset": 199, "endOffset": 213}, {"referenceID": 27, "context": "It has been suggested that the human ability for inductive reasoning \u2013 concept generalization with remarkably few examples \u2013 could derive from a prior combined with Bayesian inference (Yuille and Kersten, 2006; Tenenbaum et al., 2011; Steyvers et al., 2006).", "startOffset": 184, "endOffset": 257}, {"referenceID": 24, "context": "It has been suggested that the human ability for inductive reasoning \u2013 concept generalization with remarkably few examples \u2013 could derive from a prior combined with Bayesian inference (Yuille and Kersten, 2006; Tenenbaum et al., 2011; Steyvers et al., 2006).", "startOffset": 184, "endOffset": 257}, {"referenceID": 23, "context": "It has been suggested that the human ability for inductive reasoning \u2013 concept generalization with remarkably few examples \u2013 could derive from a prior combined with Bayesian inference (Yuille and Kersten, 2006; Tenenbaum et al., 2011; Steyvers et al., 2006).", "startOffset": 184, "endOffset": 257}, {"referenceID": 15, "context": "Bayesian nonparametric models, and Gaussian processes in particular, are an expressive way to encode prior knowledge, and also reflect the belief that the real world is infinitely complex (Neal, 1996).", "startOffset": 188, "endOffset": 200}, {"referenceID": 21, "context": "Expressive kernels have been developed by combining Gaussian processes in a type of Bayesian neural network structure (Salakhutdinov and Hinton, 2008; Wilson et al., 2012; Damianou and Lawrence, 2012).", "startOffset": 118, "endOffset": 200}, {"referenceID": 26, "context": "Expressive kernels have been developed by combining Gaussian processes in a type of Bayesian neural network structure (Salakhutdinov and Hinton, 2008; Wilson et al., 2012; Damianou and Lawrence, 2012).", "startOffset": 118, "endOffset": 200}, {"referenceID": 4, "context": "Expressive kernels have been developed by combining Gaussian processes in a type of Bayesian neural network structure (Salakhutdinov and Hinton, 2008; Wilson et al., 2012; Damianou and Lawrence, 2012).", "startOffset": 118, "endOffset": 200}, {"referenceID": 1, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 5, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 7, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 17, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 16, "context": "the fundamental differences between the proposed kernels and the popular alternatives in Rasmussen and Williams (2006). In particular, we show how the proposed kernels can automatically discover patterns and extrapolate on the CO2 dataset in Rasmussen and Williams (2006), on a synthetic dataset with strong negative covariances, on a difficult synthetic sinc pattern, and on airline passenger data.", "startOffset": 89, "endOffset": 119}, {"referenceID": 16, "context": "the fundamental differences between the proposed kernels and the popular alternatives in Rasmussen and Williams (2006). In particular, we show how the proposed kernels can automatically discover patterns and extrapolate on the CO2 dataset in Rasmussen and Williams (2006), on a synthetic dataset with strong negative covariances, on a difficult synthetic sinc pattern, and on airline passenger data.", "startOffset": 89, "endOffset": 272}, {"referenceID": 14, "context": "This marginal likelihood can be optimised to estimate hyperparameters such as `, or used to integrate out the hyperparameters via Markov chain Monte Carlo (Murray and Adams, 2010).", "startOffset": 155, "endOffset": 179}, {"referenceID": 13, "context": "This marginal likelihood can be optimised to estimate hyperparameters such as `, or used to integrate out the hyperparameters via Markov chain Monte Carlo (Murray and Adams, 2010). Detailed Gaussian process references include Rasmussen and Williams (2006), Stein (1999), and Cressie (1993).", "startOffset": 156, "endOffset": 256}, {"referenceID": 13, "context": "This marginal likelihood can be optimised to estimate hyperparameters such as `, or used to integrate out the hyperparameters via Markov chain Monte Carlo (Murray and Adams, 2010). Detailed Gaussian process references include Rasmussen and Williams (2006), Stein (1999), and Cressie (1993).", "startOffset": 156, "endOffset": 270}, {"referenceID": 3, "context": "Detailed Gaussian process references include Rasmussen and Williams (2006), Stein (1999), and Cressie (1993).", "startOffset": 94, "endOffset": 109}, {"referenceID": 22, "context": "Any stationary kernel (aka covariance function) can be expressed as an integral using Bochner\u2019s theorem (Bochner, 1959; Stein, 1999; Gikhman and Skorokhod, 2004):", "startOffset": 104, "endOffset": 161}, {"referenceID": 6, "context": "Any stationary kernel (aka covariance function) can be expressed as an integral using Bochner\u2019s theorem (Bochner, 1959; Stein, 1999; Gikhman and Skorokhod, 2004):", "startOffset": 104, "endOffset": 161}, {"referenceID": 2, "context": "If \u03c8 has a density S(s), then S is called the spectral density or power spectrum of k, and k and S are Fourier duals (Chatfield, 1989):", "startOffset": 117, "endOffset": 134}, {"referenceID": 10, "context": "Indeed, mixtures of Gaussians are dense in the set of all distribution functions (Kostantinos, 2000).", "startOffset": 81, "endOffset": 100}, {"referenceID": 8, "context": "H\u00f6rmander (1990)).", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "These kernels are easy to interpret, and provide drop-in replacements for kernels in Rasmussen and Williams (2006). The weights wq specify the relative contribution of each mixture component.", "startOffset": 85, "endOffset": 115}, {"referenceID": 15, "context": ", Rasmussen and Williams (2006) and Abrahamsen (1997), which typically only provide smooth interpolation.", "startOffset": 2, "endOffset": 32}, {"referenceID": 0, "context": ", Rasmussen and Williams (2006) and Abrahamsen (1997), which typically only provide smooth interpolation.", "startOffset": 36, "endOffset": 54}, {"referenceID": 25, "context": "Automatic relevance determination (MacKay et al., 1994; Tipping, 2004) takes place during training, removing extraneous components from the proposed model, through the complexity penalty in the marginal likelihood (Rasmussen and Williams, 2006).", "startOffset": 34, "endOffset": 70}, {"referenceID": 17, "context": ", 1994; Tipping, 2004) takes place during training, removing extraneous components from the proposed model, through the complexity penalty in the marginal likelihood (Rasmussen and Williams, 2006).", "startOffset": 166, "endOffset": 196}, {"referenceID": 14, "context": "For a fully Bayesian treatment, the spectral density could alternatively be integrated out using Markov chain Monte Carlo (Murray and Adams, 2010), rather than choosing a point estimate.", "startOffset": 122, "endOffset": 146}, {"referenceID": 16, "context": "This dataset was used in Rasmussen and Williams (2006), and is frequently used in Gaussian process tutorials, to show how GPs are flexible statistical tools: a human can look at the data, recognize patterns, and then hard code those patterns into covariance kernels.", "startOffset": 25, "endOffset": 55}, {"referenceID": 16, "context": "This dataset was used in Rasmussen and Williams (2006), and is frequently used in Gaussian process tutorials, to show how GPs are flexible statistical tools: a human can look at the data, recognize patterns, and then hard code those patterns into covariance kernels. Rasmussen and Williams (2006) identify, by looking at the blue and green curves in Figure 1a, a long term rising trend, seasonal variation with possible decay away from periodicity, medium term irregularities, and noise, and hard code a stationary covariance kernel to represent each of these features.", "startOffset": 25, "endOffset": 297}, {"referenceID": 16, "context": ",Q \u2264 10, the SM kernel can closely recover the most popular stationary kernels catalogued in Rasmussen and Williams (2006).", "startOffset": 93, "endOffset": 123}, {"referenceID": 3, "context": "Although often used in geostatistics to guide choices of Gaussian process kernel functions (and parameters) (Cressie, 1993), the empirical autocorrelation function tends to be unreliable, particularly with a small amount of data (e.", "startOffset": 108, "endOffset": 123}, {"referenceID": 3, "context": "Although often used in geostatistics to guide choices of Gaussian process kernel functions (and parameters) (Cressie, 1993), the empirical autocorrelation function tends to be unreliable, particularly with a small amount of data (e.g., N < 1000), and at high lags (for \u03c4 > 10). In Figure 2a, the empirical autocorrelation function is erratic and does not resemble the Mat\u00e9rn kernel for \u03c4 > 10. Moreover, the squared exponential kernel cannot capture the heavy tails of the Mat\u00e9rn kernel, no matter what length-scale it has. The learned SM kernel more closely matches the Mat\u00e9rn kernel. Next, we reconstruct a mixture of the rational quadratic (RQ) and periodic kernels (PE) in Rasmussen and Williams (2006):", "startOffset": 109, "endOffset": 707}, {"referenceID": 15, "context": "Derivations for both the RQ and PE kernels in (14) and (15) are in Rasmussen and Williams (2006). Rational quadratic and Mat\u00e9rn kernels are also discussed in Abrahamsen (1997).", "startOffset": 67, "endOffset": 97}, {"referenceID": 0, "context": "Rational quadratic and Mat\u00e9rn kernels are also discussed in Abrahamsen (1997). We sample 100 points from a Gaussian process with kernel 10kRQ + 4kPE, with \u03b1 = 2, \u03c9 = 1/20, `RQ = 40, `PE = 1.", "startOffset": 60, "endOffset": 78}, {"referenceID": 16, "context": "All of the stationary covariance functions in the standard machine learning Gaussian process reference Rasmussen and Williams (2006) are everywhere positive, including the periodic kernel, k(\u03c4) = exp(\u22122 sin(\u03c0 \u03c4 \u03c9)/`).", "startOffset": 103, "endOffset": 133}, {"referenceID": 9, "context": "Figure 5a shows airline passenger numbers, recorded monthly, from 1949 to 1961 (Hyndman, 2005).", "startOffset": 79, "endOffset": 94}, {"referenceID": 14, "context": "In future work, one could integrate away a spectral density, using recently developed efficient MCMC for GP hyperparameters (Murray and Adams, 2010).", "startOffset": 124, "endOffset": 148}, {"referenceID": 20, "context": "Moreover, spectral representations of the proposed models suggest recent Toeplitz methods (Saatchi, 2011) could be applied for significant speedup in inference and predictions.", "startOffset": 90, "endOffset": 105}], "year": 2017, "abstractText": "Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density \u2013 the Fourier transform of a kernel \u2013 with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.", "creator": "LaTeX with hyperref package"}}}