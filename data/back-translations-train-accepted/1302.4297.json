{"id": "1302.4297", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Feature Multi-Selection among Subjective Features", "abstract": "When dealing with subjective, noisy, or otherwise nebulous features, the \"wisdom of crowds\" suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated `feature multi-selection' algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people's height and weight from photos, using features such as 'gender' and 'estimated weight' as well as culturally fraught ones such as 'attractive'.", "histories": [["v1", "Mon, 18 Feb 2013 15:00:47 GMT  (145kb)", "https://arxiv.org/abs/1302.4297v1", null], ["v2", "Wed, 17 Apr 2013 17:03:56 GMT  (148kb)", "http://arxiv.org/abs/1302.4297v2", "Published in Proceedings of the 30th International Conference on Machine Learning (ICML), 2013"], ["v3", "Tue, 14 May 2013 21:35:25 GMT  (136kb,D)", "http://arxiv.org/abs/1302.4297v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sivan sabato", "adam kalai"], "accepted": true, "id": "1302.4297"}, "pdf": {"name": "1302.4297.pdf", "metadata": {"source": "META", "title": "Feature Multi-Selection among Subjective Features", "authors": ["Sivan Sabato", "Adam Kalai"], "emails": ["SIVAN.SABATO@MICROSOFT.COM", "ADAM.KALAI@MICROSOFT.COM"], "sections": [{"heading": "1. Introduction", "text": "This year, the number of working women living in the United States has skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, skyrocketed, and skyrocketed."}, {"heading": "2. Preliminary assumptions and definitions", "text": "We assume that for each object o and each attribute a there is a distribution of judgments in which a new random number of people is selected for each attribute-object judgement (in our experiments we limit the total amount of work each individual worker can do). We assume that a distribution D takes place over labeled objects and objects in which labels are real numbers. We rename the marginal distribution after D. We let P [a] = PO attributes DO [a] | O] | O = Labels y are assumed to be real values. As is standard, we assume that a \"true\" label yi is for each object oi. For notoriously ease, we assume that multiplier judgments are collected in the attribute selection."}, {"heading": "3. Feature Multi-Selection Algorithms", "text": "The input to a multiple selection algorithm is an example given as B and m, in which each attribute has been evaluated k times, and the output is a repeat vector r-RB. Our ultimate goal is to find r and a predictor w-Rd so that \"(w, Dr) is minimal. We now give intuition about the derivation of the algorithms, but their formal definition is in Alg. \u2212 1. Define the loss of a repeat vector so that it is\" (r) minw-Rd \"(w, Dr). The goal is to minimize\" (r) over r-RB. We give two forward selection algorithms that both begin with r = (0,., 0) and greedily r [a] for one that reduces an estimate of \"(r, Dr) the most. The key question is how to simplify this predicted loss\" (r), which is positive after Y, since we can briefly exceed the number after Y]."}, {"heading": "3.1. A Scoring Algorithm", "text": "The first algorithm we propose is derived from the zero correlation assumption that E [X] X [a] X [a] X [a]] = 0 for a 6 = a, \"or equivalent to the fact that the covariance matrix is\" diagonal. \"Perhaps, however, the simplest approach of standard feature selection is to evaluate each feature independently, based on its normalized empirical correlation with the label, and to select the features of the B-top scoring characteristics \u2212 r.\" If features are uncorrelated and the training sample is sufficiently large, then this efficient approach finds an optimal set of characteristics. The feature multi-selection scoring algorithm we propose is the singular feature correlation r, which replaces the pseudo-inverse feature correlation \u2212 1r. \"Henceforth, it is optimal under similar assumptions, but it is complicated by the fact that we can include multiple repetitions of each feature below zero."}, {"heading": "3.2. The Full Multi-Selection Algorithm", "text": "The scoring algorithm is motivated by the assumption of a zero correlation between the characteristics. However, this assumption algorithm 1 feature multi-selection algorithms 1: Input: Budget B; (x1, y1),..., (xm, ym))......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.3. Guarantees", "text": "Under our distribution assumptions, we show that the estimated objective functions used by our algorithms converge to E [Y 2] \u2212 '(r), approximately minimizing the estimated target' (r). Formally, the targets used in Alg. 1 for the complete algorithm and the scoring algorithm should be the implicit functions of the training sample S. For a symmetrical matrix S, care should be taken that \u03bbmin (S) are the smallest eigenvalues of S. We define: \u03bb = minr-RB-min (sub-r) and B = min (B, d).Theorem 3.1. Suppose that all judgments and labels are contained in [\u2212 1, 1] and then for each of these algorithms the smallest eigenvalues of S (0, 1), with a probability of at least 1 \u2212 \u03b4 above m i.d. Training samples of Dk, for all r-judgments and labels, are in [1] \u2212 1."}, {"heading": "If the external covariance matrix \u03a3 is diagonal, then for", "text": "The proof of this convergence rate is given in Appendix A. However, the convergence rate for the complete algorithm is based on two limits: (1) If the standard of minimization w is at most \u03b1, then the convergence rate is for most B-2 / \u221a m; (2) the standard of minimization w is for most B-2 models. An additional factor of minimization w for most h models is uniform convergence over r-RB. The components of this result are in the same order as the equivalents of the standard convergence for the least squadratic regressions. An improved rate of B-2 / m can be achieved for the least squadratic regressions. The components of this result are of the same magnitude as the equivalents of the standard convergence for the least squadratic regressions."}, {"heading": "3.4. Implementation", "text": "If our estimate is not PSD, we use the method \"MakePSD,\" which uses a symmetrical matrix A as input and returns the PSD closest to A in the Frobenius norm. This can be done by calculating the eigenvalue replacement A = UDUT, where U is orthogonal and D is diagonal, and returning the UD-UT, where D is with zero negative entries (Higham, 1988). If we assume a diagonal external covariance, then this method is equivalent to rounding up the estimate from \u03c32 (a) to zero, as in the scoring algorithm. On a budget of B, the complete algorithm Bd performs SVD to calculate pseudo-inverses. Note, however, that the largest matrix that may be broken down here is rounded up from size (d, B) to zero."}, {"heading": "4. Experiments", "text": "\"In the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade."}, {"heading": "5. Conclusions", "text": "We present the problem of trait multitude selection and provide two algorithms in case of regression with averaging of assessments. Future research directions include other learning tasks such as classification and other types of trait aggregation, such as mean averaging (equivalent to majority formation for binary traits), and another important question for future work is how to perform trait multitude selection in an environment of changing crowding."}, {"heading": "Acknowledgments", "text": "We would like to thank Edith Law and Haoqi Zhang for some helpful conversations."}, {"heading": "A. Analysis", "text": "Similarly, for some constants C, C, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0."}], "references": [{"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson", "year": 2002}, {"title": "Generalized inverses: theory and applications, volume 15", "author": ["A. Ben-Israel", "T.N.E. Greville"], "venue": null, "citeRegEx": "Ben.Israel and Greville,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville", "year": 2003}, {"title": "Efficient learning with partially observed attributes", "author": ["N. Cesa-Bianchi", "S. Shalev-Shwartz", "O. Shamir"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Statistical regression with measurement error, volume 6", "author": ["C. Cheng", "J.W. Van Ness"], "venue": "Arnold London,", "citeRegEx": "Cheng and Ness,? \\Q1999\\E", "shortCiteRegEx": "Cheng and Ness", "year": 1999}, {"title": "The photographic height and weight chart", "author": ["Cockerham", "Rob"], "venue": "http://www.cockeyed.com/photos/ bodies/heightweight.html,", "citeRegEx": "Cockerham and Rob.,? \\Q2013\\E", "shortCiteRegEx": "Cockerham and Rob.", "year": 2013}, {"title": "Coefficient alpha and the internal structure of tests", "author": ["L. Cronbach"], "venue": "Psychometrika, 16(3):297\u2013334,", "citeRegEx": "Cronbach,? \\Q1951\\E", "shortCiteRegEx": "Cronbach", "year": 1951}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "Dawid and Skene,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene", "year": 1979}, {"title": "Computing a nearest symmetric positive semidefinite matrix", "author": ["N.J. Higham"], "venue": "Linear algebra and its applications,", "citeRegEx": "Higham,? \\Q1988\\E", "shortCiteRegEx": "Higham", "year": 1988}, {"title": "Understanding the intrinsic memorability of images", "author": ["P. Isola", "D. Parikh", "A. Torralba", "A. Oliva"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Isola et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Isola et al\\.", "year": 2011}, {"title": "Content analysis: An introduction to its methodology", "author": ["K. Krippendorff"], "venue": "Sage Publications,", "citeRegEx": "Krippendorff,? \\Q2012\\E", "shortCiteRegEx": "Krippendorff", "year": 2012}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "Surveys in Combinatorics, pp", "citeRegEx": "McDiarmid,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid", "year": 1989}, {"title": "Subset selection in regression", "author": ["A. Miller"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Miller,? \\Q2002\\E", "shortCiteRegEx": "Miller", "year": 2002}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM journal on computing,", "citeRegEx": "Natarajan,? \\Q1995\\E", "shortCiteRegEx": "Natarajan", "year": 1995}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["Patterson", "Genevieve", "Hays", "James"], "venue": "In Proceeding of the 25th Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Patterson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Patterson et al\\.", "year": 2012}, {"title": "Partial Information and DistributionDependence in Supervised Learning Models", "author": ["S. Sabato"], "venue": "PhD thesis,", "citeRegEx": "Sabato,? \\Q2012\\E", "shortCiteRegEx": "Sabato", "year": 2012}, {"title": "Feature multi-selection among subjective features", "author": ["S. Sabato", "A. Kalai"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "Sabato and Kalai,? \\Q2013\\E", "shortCiteRegEx": "Sabato and Kalai", "year": 2013}, {"title": "Inferring ground truth from subjective labelling of venus images", "author": ["P. Smyth", "U.M. Fayyad", "M.C. Burl", "P. Perona", "P. Baldi"], "venue": "In NIPS,", "citeRegEx": "Smyth et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Smyth et al\\.", "year": 1994}, {"title": "Smoothness, lownoise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "The multidimensional wisdom of crowds", "author": ["Welinder", "Peter", "Branson", "Steve", "Belongie", "Serge", "Perona", "Pietro"], "venue": "In Neural Information Processing Systems Conference (NIPS),", "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "By Rademacher complexity bounds (Bartlett & Mendelson, 2002), with probability", "author": ["X Ek[`r(w"], "venue": null, "citeRegEx": "Ek.`r.w and ...,? \\Q2002\\E", "shortCiteRegEx": "Ek.`r.w and ...", "year": 2002}], "referenceMentions": [{"referenceID": 14, "context": "This work has been published in Sabato & Kalai (2013).", "startOffset": 32, "endOffset": 54}, {"referenceID": 12, "context": "Since the feature selection problem is well known to be NP-hard (Natarajan, 1995), our problem is also NP-hard in the general case.", "startOffset": 64, "endOffset": 81}, {"referenceID": 16, "context": "A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.", "startOffset": 125, "endOffset": 199}, {"referenceID": 18, "context": "A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses.", "startOffset": 125, "endOffset": 199}, {"referenceID": 2, "context": "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein).", "startOffset": 255, "endOffset": 282}, {"referenceID": 2, "context": "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the \u2018true\u2019 regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements. A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses. Two recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments.", "startOffset": 255, "endOffset": 987}, {"referenceID": 2, "context": "A number of researchers have studied attribute-efficient prediction (also called budgeted learning) assuming, as we do, that there is a cost to evaluating attributes and one would like to evaluate as few as possible (see, for instance, the recent work by Cesa-Bianchi et al. (2011) and references therein). In that line of work, each attribute is judged at most once. The errors-in-variables approach (e.g., Cheng & Van Ness, 1999) in statistics estimates the \u2018true\u2019 regression coefficients using noisy feature measurements. This approach is less suitable in our setting, since our final goal is to predict from noisy measurements. A wide variety of techniques have been studied to combine estimates of experts or the crowd of a single quantity of interest (see, e.g. Dawid & Skene, 1979; Smyth et al., 1994; Welinder et al., 2010), like estimating the number of jelly beans in a jar from a number of guesses. Two recent works on crowdsourcing are very relevant. Patterson & Hays (2012) crowdsourced the mean of 3 judgments of each of 102 binary attributes on over 14,000 images, yielding over 4 million judgments. Some of their attributes are subjective, e.g., soothing. We employ their crowdsourcing protocol to label our binary attributes. Isola et al. (2011) study subjective and objective features for the task of estimating how memorable an image is, by taking the mean of 10 judgments per attribute for each im-", "startOffset": 255, "endOffset": 1263}, {"referenceID": 5, "context": "Finally, in the social sciences, a wide array of techniques have been developed for assessing inter-rater reliability of attributes, with the most popular perhaps being the \u03b1 coefficient (Cronbach, 1951).", "startOffset": 187, "endOffset": 203}, {"referenceID": 9, "context": "For an overview of reliability theory, see (Krippendorff, 2012).", "startOffset": 43, "endOffset": 63}, {"referenceID": 17, "context": "An improved rate of \u221a B\u0304\u03b12/m can be achieved for least-squares regression, if the algorithm exactly minimizes the sample squared loss (Srebro et al., 2010).", "startOffset": 134, "endOffset": 155}, {"referenceID": 7, "context": "This can be done by calculating the eigenvalue decomposition A = UDU where U is orthogonal and D is diagonal, and returning UD\u0303U , where D\u0303 is D with zeroed negative entries (Higham, 1988).", "startOffset": 174, "endOffset": 188}, {"referenceID": 8, "context": "The first baseline, denoted \u2018Averages\u2019 in the plots, is based on the \u201cpredictive\u201d feature selection algorithm of Isola et al. (2011): We first average the 2 judgments per attribute to create a standard data set with one value for each objectattribute pair, and then greedily add attributes, one at a time, so as to minimize the least-squares error.", "startOffset": 113, "endOffset": 133}], "year": 2013, "abstractText": "When dealing with subjective, noisy, or otherwise nebulous features, the \u201cwisdom of crowds\u201d suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated feature multi-selection algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people\u2019s height and weight from photos, using features such as gender and estimated weight as well as culturally fraught ones such as attractive. This work has been published in Sabato & Kalai (2013).", "creator": "LaTeX with hyperref package"}}}