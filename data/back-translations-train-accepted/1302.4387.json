{"id": "1302.4387", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Online Learning with Switching Costs and Other Adaptive Adversaries", "abstract": "We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full information and bandit feedback. We measure the player's performance using a new notion of regret, also known as policy regret, which better captures the adversary's adaptiveness to the player's behavior. In a setting where losses are allowed to drift, we characterize ---in a nearly complete manner--- the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switching costs, the attainable rate with bandit feedback is $\\Theta(T^{2/3})$. Interestingly, this rate is significantly worse than the $\\Theta(\\sqrt{T})$ rate attainable with switching costs in the full information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force $\\Theta(T^{2/3})$ regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.", "histories": [["v1", "Mon, 18 Feb 2013 18:46:37 GMT  (154kb)", "https://arxiv.org/abs/1302.4387v1", null], ["v2", "Sat, 1 Jun 2013 09:35:15 GMT  (25kb)", "http://arxiv.org/abs/1302.4387v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nicol\u00f2 cesa-bianchi", "ofer dekel", "ohad shamir"], "accepted": true, "id": "1302.4387"}, "pdf": {"name": "1302.4387.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Switching Costs and Other Adaptive Adversaries", "authors": ["Nicol\u00f2 Cesa-Bianchi"], "emails": ["nicolo.cesa-bianchi@unimi.it", "oferd@microsoft.com", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 130 2,43 87v2 [cs.LG] \u221a T) with switching costs in case of full information. By a novel reduction of experts to bandits, we also show that an opponent with limited memory can force remorse even in case of full information, which proves that switching costs are easier to control than opponents with limited memory. Our lower limits are based on a new stochastic strategy of the opponent, which generates loss processes with strong dependencies."}, {"heading": "1 Introduction", "text": "An important example of the framework of prediction with expert advice - see, for example, Cesa-Bianchi and Lugosi [2006] - is the following repeated game, between a randomized player with a finite and fixed set of available actions and an opponent. The goal of the player is to accumulate losses at the smallest possible rate as the game progresses. Two versions of this game are typically considered: in the complete feedback of the game, at the end of each round, the player observes the allocation of the loss values to each action. In the bandit version, the player observes only the loss with his chosen action, but not the loss values of other activities. We assume that the opponent is adapted, that the opponent is adapted."}, {"heading": "1.1 The Current State of the Art", "text": "Most of these earlier results are based on the additional assumption that the range of loss functions is limited at a fixed interval, say [0, C]. We explicitly note this because our new results require us to generalize these assumptions somewhat. (As mentioned above, the oblivious adversary has been extensively studied by us and is the best understanding of all the adversaries discussed in this paper.) Both the hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the sequence of the Perturbed Leader (FPL) of the Kalai and Vempala algorithms [2005] guarantee a regret of the O (instead of T), with a suitably lower limit of communication (see e.g. Cesa-Bianchi and Lugosi [2006]."}, {"heading": "1.2 Our Contribution", "text": "In this paper, we use the following contributions (see Table 1): \"Our most important technical contribution is essentially a new, deeper regret that covers the existing upper limits in several of the areas mentioned above.\" \"Our most important technical contribution is a new, deeper regret that builds up the existing upper limits in relation to the limited memory areas.\" \"We have shown a different, deeper regret in relation to the limited memory areas that we use for switching costs and bandit feedback, although we also use stochastic i.d.\" We confirm that the existing upper limits of regret in our setting and game coincide with the lower limits up to logarithmic factors. \"\" Despite the lower limits, we show that we use the switching costs and bandit feedback if we assume that stochastic i.d. losses, then you can get a distributional regret that is bound to the limits of regret. \"\" Log T logs \"actions for limited T actions only."}, {"heading": "2 Lower Bounds", "text": "In this section, we demonstrate lower limits on the expected regrets of players in various settings. (T 2 / 3) It is sufficient to consider a very simple setting, with only two actions based on any number of games. (T 2 / 3) There is only a random sequence of losses that the opponent selects before adding switching costs. (T 2 / 3) There are no random actions based on any number of rounds. (T 2 / 3) There are loss functions f1, fT, the forgotten sequence of losses that the opponent selects before adding switching costs. (T 2 / 3) There are loss functions f1, fT that are neglected, with a range of C = 2, and a drift that logs from Dt = 3. (t) + 16, so that E [RT]."}, {"heading": "3 Upper Bounds", "text": "In this section, we show that the known upper limits of regret, which were originally proven to be limited losses, can be extended to the case of losses with limited range and limited drift. (The remaining upper limits in Table 1 are either trivial or follow from the principle that an upper limit is still held if we weaken the opponent or provide a more informative feedback. 3.1 O) with limited storage capacity. (The remaining upper limits in Table 1 are either trivial or follow from the principle that an upper limit is still held. (The remaining upper limit in Table 1 is either trivial or follow from the principle of having another informative feedback. 3.1 O) with switching costs and full-information feedback. In this setting, ft (x1: t) = (xt)."}, {"heading": "4 Discussion", "text": "In this paper, we examined the problem of predicting with expert opinions against various types of adversaries, ranging from the opponent's regret to the general adaptive adversary. We demonstrated upper and lower limits of regret against each of these enemy concepts, both in the full information and in the bandit's feedback models. Our lower limits essentially corresponded to our upper limits in all but one case: the adaptive adversary with a unit memory in the full information, where we only know that the regret is greater than the regret (T 2 / 3). Our new limits have two important consequences. First, we characterize the regret achievable with switching costs, and show an environment in which predicting with the bandit feedback is strictly more difficult than predicting with the fulfillment feedback - even in terms of dependence on T, and even on small, infinite approaches to action."}, {"heading": "A Distribution-free regret bound for bandits with switching costs", "text": "In this appendix we adjust the results of Cesa-Bianchi et al. [2013] to show a strategy that achieves O (\u221a T log logT) repentance against any i.i.d. oblivious opponent in the bandit setting with switching costs. [2012] The strategy used by this stochastic opponent is specified by a probability distribution via oblivious loss functions. The oblivious loss function for each step t = 1, 2,. is the realization of an independent draw Lt from this distribution. The regret of a player who chooses actions X0 = X1, X2,. is defined by RT = 1Et. [Xt) + I {Xt 6 = Xt \u2212 1}. \u2212 minx."}, {"heading": "B Proof of Thm. 1", "text": "\"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\""}, {"heading": "C Proof of Thm. 2", "text": "However, it is to be understood as part of the proof that there is some random strategy of the opponent \u2212 \u2212 \u2212"}, {"heading": "D Proofs of Upper Bounds", "text": "The proof for Thm. 3. Each loss function corresponds to ft (x1: t) = 1 (x1) = 1 (x1) (x2) (x2) (x2) (x2) (x2) (x3) (x3) (x3) (x3) (x3) (x3) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4 (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x4) (x"}], "references": [{"title": "Asymptotically efficient adaptive allocation rules for the multiarmed bandit problem with switching cost", "author": ["R. Agrawal", "M.V. Hedge", "D. Teneketzis"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Agrawal et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1988}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["R. Arora", "O. Dekel", "A. Tewari"], "venue": "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Online computation and competitive analysis", "author": ["A. Borodin", "R. El-Yaniv"], "venue": null, "citeRegEx": "Borodin and El.Yaniv.,? \\Q1998\\E", "shortCiteRegEx": "Borodin and El.Yaniv.", "year": 1998}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Regret minimization for reserve prices in second-price auctions", "author": ["N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2013}, {"title": "Robbing the bandit: Less regret in online geometric optimization against an adaptive adversary", "author": ["V. Dani", "T.P. Hayes"], "venue": "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Dani and Hayes.,? \\Q2006\\E", "shortCiteRegEx": "Dani and Hayes.", "year": 2006}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "A survey on the bandit problem with switching costs", "author": ["T. Jun"], "venue": "De Economist,", "citeRegEx": "Jun.,? \\Q2004\\E", "shortCiteRegEx": "Jun.", "year": 2004}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Adaptive bandits: Towards the best history-dependent strategy", "author": ["O. Maillard", "R. Munos"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Maillard and Munos.,? \\Q2010\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2010}, {"title": "Online geometric optimization in the bandit setting against an adaptive adversary", "author": ["H.B. McMahan", "A. Blum"], "venue": "In Proceedings of the Seventeenth Annual Conference on Learning Theory,", "citeRegEx": "McMahan and Blum.,? \\Q2004\\E", "shortCiteRegEx": "McMahan and Blum.", "year": 2004}, {"title": "Sequential strategies for loss functions with memory", "author": ["N. Merhav", "E. Ordentlich", "G. Seroussi", "M.J. Weinberger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Merhav et al\\.,? \\Q1947\\E", "shortCiteRegEx": "Merhav et al\\.", "year": 1947}, {"title": "Online learning with delayed label feedback", "author": ["C. Mesterharm"], "venue": "In Proceedings of the Sixteenth International Conference on Algorithmic Learning Theory,", "citeRegEx": "Mesterharm.,? \\Q2005\\E", "shortCiteRegEx": "Mesterharm.", "year": 2005}, {"title": "Online regret bounds for Markov decision processes with deterministic transitions", "author": ["R. Ortner"], "venue": "Theoretical Computer Science,", "citeRegEx": "Ortner.,? \\Q2010\\E", "shortCiteRegEx": "Ortner.", "year": 2010}, {"title": "On the complexity of bandit and derivative-free stochastic convex optimization", "author": ["O. Shamir"], "venue": "CoRR, abs/1209.2388,", "citeRegEx": "Shamir.,? \\Q2012\\E", "shortCiteRegEx": "Shamir.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": ", Cesa-Bianchi and Lugosi [2006]\u2014 is defined as the following repeated game, between a randomized player with a finite and fixed set of available actions and an adversary.", "startOffset": 2, "endOffset": 33}, {"referenceID": 4, "context": ", Cesa-Bianchi and Lugosi [2006]\u2014 is defined as the following repeated game, between a randomized player with a finite and fixed set of available actions and an adversary. At the beginning of each round of the game, the adversary assigns a loss to each action. Next, the player defines a probability distribution over the actions, draws an action from this distribution, and suffers the loss associated with that action. The player\u2019s goal is to accumulate loss at the smallest possible rate, as the game progresses. Two versions of this game are typically considered: in the full-information feedback version of the game, at the end of each round, the player observes the adversary\u2019s assignment of loss values to each action. In the bandit feedback version, the player only observes the loss associated with his chosen action, but not the loss values of other actions. We assume that the adversary is adaptive (also called nonoblivious by Cesa-Bianchi and Lugosi [2006] or reactive by Maillard and Munos [2010]), which means that the adversary chooses the loss values on round t based on the player\u2019s actions on rounds 1 .", "startOffset": 2, "endOffset": 970}, {"referenceID": 4, "context": ", Cesa-Bianchi and Lugosi [2006]\u2014 is defined as the following repeated game, between a randomized player with a finite and fixed set of available actions and an adversary. At the beginning of each round of the game, the adversary assigns a loss to each action. Next, the player defines a probability distribution over the actions, draws an action from this distribution, and suffers the loss associated with that action. The player\u2019s goal is to accumulate loss at the smallest possible rate, as the game progresses. Two versions of this game are typically considered: in the full-information feedback version of the game, at the end of each round, the player observes the adversary\u2019s assignment of loss values to each action. In the bandit feedback version, the player only observes the loss associated with his chosen action, but not the loss values of other actions. We assume that the adversary is adaptive (also called nonoblivious by Cesa-Bianchi and Lugosi [2006] or reactive by Maillard and Munos [2010]), which means that the adversary chooses the loss values on round t based on the player\u2019s actions on rounds 1 .", "startOffset": 2, "endOffset": 1011}, {"referenceID": 13, "context": "This definition is the same as the one used in Merhav et al. [2002] and Arora et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 1, "context": "[2002] and Arora et al. [2012] (in the latter, it is called policy regret), but differs from the more common definition of expected regret", "startOffset": 11, "endOffset": 31}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 21}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 46}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 69}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 96}, {"referenceID": 1, "context": "Indeed, if the adversary is adaptive, the quantity ft(X1:t\u22121, x)is hardly interpretable \u2014see Arora et al. [2012] for a more detailed discussion.", "startOffset": 93, "endOffset": 113}, {"referenceID": 1, "context": "Indeed, if the adversary is adaptive, the quantity ft(X1:t\u22121, x)is hardly interpretable \u2014see Arora et al. [2012] for a more detailed discussion. In general, we seek algorithms for which E[RT ] can be bounded by a sublinear function of T , implying that the per-round expected regret, E[RT ]/T , tends to zero. Unfortunately, Arora et al. [2012] shows that arbitrary adaptive adversaries can easily force the regret to grow linearly.", "startOffset": 93, "endOffset": 345}, {"referenceID": 14, "context": "In the information theory literature, this setting is called individual sequence prediction against loss functions with memory Merhav et al. [2002]. In addition to the adversary types described above, the bounded memory adaptive adversary has additional interesting special cases.", "startOffset": 127, "endOffset": 148}, {"referenceID": 14, "context": "In the information theory literature, this setting is called individual sequence prediction against loss functions with memory Merhav et al. [2002]. In addition to the adversary types described above, the bounded memory adaptive adversary has additional interesting special cases. One of them is the delayed feedback oblivious adversary of Mesterharm [2005], which defines an oblivious loss sequence, but reveals each loss value with a delay of m rounds.", "startOffset": 127, "endOffset": 358}, {"referenceID": 2, "context": "With full-information feedback, both the Hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the follow the perturbed leader (FPL) algorithm Kalai and Vempala [2005] guarantee a regret of O( \u221a T ), with a matching lower bound of \u03a9( \u221a T ) \u2014see, e.", "startOffset": 57, "endOffset": 88}, {"referenceID": 2, "context": "With full-information feedback, both the Hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the follow the perturbed leader (FPL) algorithm Kalai and Vempala [2005] guarantee a regret of O( \u221a T ), with a matching lower bound of \u03a9( \u221a T ) \u2014see, e.", "startOffset": 89, "endOffset": 116}, {"referenceID": 2, "context": "With full-information feedback, both the Hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the follow the perturbed leader (FPL) algorithm Kalai and Vempala [2005] guarantee a regret of O( \u221a T ), with a matching lower bound of \u03a9( \u221a T ) \u2014see, e.", "startOffset": 89, "endOffset": 193}, {"referenceID": 1, "context": ", Cesa-Bianchi and Lugosi [2006]. Analyses of Hedge in settings where the loss range may vary over time have also been considered \u2014see, e.", "startOffset": 2, "endOffset": 33}, {"referenceID": 1, "context": ", Cesa-Bianchi and Lugosi [2006]. Analyses of Hedge in settings where the loss range may vary over time have also been considered \u2014see, e.g., Cesa-Bianchi et al. [2007]. The oblivious setting with bandit feedback, where the player only observes the incurred loss ft(X1:t), is called the nonstochastic (or adversarial) multi-armed bandit problem.", "startOffset": 2, "endOffset": 169}, {"referenceID": 0, "context": "In this setting, the Exp3 algorithm of Auer et al. [2002] guarantees the same regret O( \u221a T ) as the full-information setting, and clearly the full-information lower bound \u03a9( \u221a T ) still applies.", "startOffset": 39, "endOffset": 58}, {"referenceID": 0, "context": "In this setting, the Exp3 algorithm of Auer et al. [2002] guarantees the same regret O( \u221a T ) as the full-information setting, and clearly the full-information lower bound \u03a9( \u221a T ) still applies. The follow the lazy leader (FLL) algorithm of Kalai and Vempala [2005] is designed for the switching costs setting with full-information feedback.", "startOffset": 39, "endOffset": 267}, {"referenceID": 0, "context": "In this setting, the Exp3 algorithm of Auer et al. [2002] guarantees the same regret O( \u221a T ) as the full-information setting, and clearly the full-information lower bound \u03a9( \u221a T ) still applies. The follow the lazy leader (FLL) algorithm of Kalai and Vempala [2005] is designed for the switching costs setting with full-information feedback. The analysis of FLL guarantees that the oblivious component of the player\u2019s expected regret (without counting the switching costs), as well as the expected number of switches, is upper bounded byO( \u221a T ), implying an expected regret of at most O( \u221a T ). The algorithm of Merhav et al. [2002] focuses on the bounded memory adversary with full-information feedback, referring to this problem as loss functions with memory, and guaranteeing a regret ofO(T ).", "startOffset": 39, "endOffset": 635}, {"referenceID": 0, "context": "The work of Arora et al. [2012] extends this result to the bandit feedback case, maintaining the same regret bound.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "The work of Arora et al. [2012] extends this result to the bandit feedback case, maintaining the same regret bound. Learning with bandit feedback and switching costs has mostly been considered in the economics literature, using a different setting than ours and with prior knowledge assumptions (see Jun [2004] for an overview).", "startOffset": 12, "endOffset": 311}, {"referenceID": 0, "context": "from a fixed distribution) was first studied by Agrawal et al. [1988], where they show that O(log T ) switches are sufficient to asymptotically guarantee logarithmic regret.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "from a fixed distribution) was first studied by Agrawal et al. [1988], where they show that O(log T ) switches are sufficient to asymptotically guarantee logarithmic regret. The paper Ortner [2010] achieves logarithmic regret nonasymptotically with O(logT ) switches.", "startOffset": 48, "endOffset": 198}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 95}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 118}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 145}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 170}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood. For example, even the Exp3 algorithm of Auer et al. [2002] has extensions to the \u201cadaptive\u201d adversary case, with a regret upper bound of O( \u221a T ).", "startOffset": 76, "endOffset": 310}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood. For example, even the Exp3 algorithm of Auer et al. [2002] has extensions to the \u201cadaptive\u201d adversary case, with a regret upper bound of O( \u221a T ). This bound doesn\u2019t contradict the \u03a9(T ) lower bound for general adaptive adversaries mentioned earlier, since these papers use the regret defined in Eq. (2) rather than the regret used in our work, defined in Eq. (1). Another related body of work lies in the field of competitive analysis \u2014see Borodin and El-Yaniv [1998], which also deals with loss functions that depend on the player\u2019s past actions, and the adversary\u2019s memory may even be unbounded.", "startOffset": 76, "endOffset": 720}, {"referenceID": 4, "context": "This result uses ideas from Cesa-Bianchi et al. [2013], and is deferred to Appendix A.", "startOffset": 28, "endOffset": 55}, {"referenceID": 2, "context": "To the best of our knowledge, this is the first theoretical confirmation that learning with bandit feedback is strictly harder than learning with full-information, even on a small finite action set and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the number of actions Auer et al. [2002], or required large or continuous action spaces \u2014see, e.", "startOffset": 316, "endOffset": 335}, {"referenceID": 2, "context": "To the best of our knowledge, this is the first theoretical confirmation that learning with bandit feedback is strictly harder than learning with full-information, even on a small finite action set and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the number of actions Auer et al. [2002], or required large or continuous action spaces \u2014see, e.g., Bubeck et al. [2011], Shamir [2012]).", "startOffset": 316, "endOffset": 415}, {"referenceID": 2, "context": "To the best of our knowledge, this is the first theoretical confirmation that learning with bandit feedback is strictly harder than learning with full-information, even on a small finite action set and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the number of actions Auer et al. [2002], or required large or continuous action spaces \u2014see, e.g., Bubeck et al. [2011], Shamir [2012]).", "startOffset": 316, "endOffset": 430}, {"referenceID": 13, "context": "somewhat surprising given the ideas presented in Merhav et al. [2002] and later extended in Arora et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 1, "context": "[2002] and later extended in Arora et al. [2012]: The main technique used in these papers is to take an algorithm originally designed for oblivious adversaries, forcefully prevent it from switching actions very often, and obtain a new algorithm that guarantees a regret of O(T ) against bounded memory adversaries.", "startOffset": 29, "endOffset": 49}, {"referenceID": 9, "context": "lT (without the additional switching costs) were all bounded in [0, 1], the Follow the Lazy Leader (FLL) algorithm of Kalai and Vempala [2005] would guarantee a regret of O( \u221a T ) with respect to these losses (again, without the additional switching costs).", "startOffset": 118, "endOffset": 143}, {"referenceID": 4, "context": "The player sets a fixed horizon T and focuses on controlling his regret at time T ; he can then use a standard doubling trick Cesa-Bianchi and Lugosi [2006] to handle an infinite horizon.", "startOffset": 126, "endOffset": 157}, {"referenceID": 2, "context": "P algorithm, due to Auer et al. [2002], is such an algorithm.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "The player runs the Hedge algorithm Freund and Schapire [1997] in the background, invoking it only at the beginning of each epoch and using it to choose one exploitation action that will be played consistently on all of the exploitation rounds in the epoch.", "startOffset": 36, "endOffset": 63}, {"referenceID": 1, "context": "Can we characterize the dependence of the regret on the size of the action set A? Can we strengthen any of our expected regret bounds to bounds that hold with high probability? Can any of our results be generalized to more sophisticated notions of regret, such as shifting regret and swap regret, as in Arora et al. [2012]? In addition to the adversary types discussed in this paper, there are other interesting classes of adversaries that lie between the oblivious and the adaptive.", "startOffset": 303, "endOffset": 323}, {"referenceID": 1, "context": "Can we characterize the dependence of the regret on the size of the action set A? Can we strengthen any of our expected regret bounds to bounds that hold with high probability? Can any of our results be generalized to more sophisticated notions of regret, such as shifting regret and swap regret, as in Arora et al. [2012]? In addition to the adversary types discussed in this paper, there are other interesting classes of adversaries that lie between the oblivious and the adaptive. One of these is the oblivious adversary with delayed feedback, briefly mentioned in the introduction. While some results for this adversary exist (see Mesterharm [2005]), the attainable regret, especially in the bandit feedback case, is not clear.", "startOffset": 303, "endOffset": 653}, {"referenceID": 5, "context": "A Distribution-free regret bound for bandits with switching costs In this appendix we adapt results of Cesa-Bianchi et al. [2013] to show a strategy that achieves O (\u221a T log log logT )", "startOffset": 103, "endOffset": 130}, {"referenceID": 8, "context": "We run a mini-batched version of the Hedge algorithm Freund and Schapire [1997] over the epochs: at the beginning of each epoch j, Hedge draws an action Xj \u2208 A which is played consistently throughout the epoch.", "startOffset": 53, "endOffset": 80}], "year": 2013, "abstractText": "We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player\u2019s performance using a new notion of regret, also known as policy regret, which better captures the adversary\u2019s adaptiveness to the player\u2019s behavior. In a setting where losses are allowed to drift, we characterize \u2014in a nearly complete manner\u2014 the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switching costs, the attainable rate with bandit feedback is \u0398\u0303(T ). Interestingly, this rate is significantly worse than the \u0398( \u221a T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force \u0398\u0303(T ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.", "creator": "LaTeX with hyperref package"}}}