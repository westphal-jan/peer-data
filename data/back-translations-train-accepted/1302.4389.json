{"id": "1302.4389", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Maxout Networks", "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its {\\em out}put is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.", "histories": [["v1", "Mon, 18 Feb 2013 18:59:07 GMT  (517kb,D)", "http://arxiv.org/abs/1302.4389v1", null], ["v2", "Tue, 19 Feb 2013 04:39:48 GMT  (484kb,D)", "http://arxiv.org/abs/1302.4389v2", "Revision; fixed a broken reference and removed a latex command from the abstract on the metadata page"], ["v3", "Wed, 20 Feb 2013 22:33:13 GMT  (485kb,D)", "http://arxiv.org/abs/1302.4389v3", "Corrected a mistake in the \"Review of dropout\" section"], ["v4", "Fri, 20 Sep 2013 08:54:35 GMT  (1433kb,D)", "http://arxiv.org/abs/1302.4389v4", "This is the version of the paper that appears in ICML 2013"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian j goodfellow", "david warde-farley", "mehdi mirza", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1302.4389"}, "pdf": {"name": "1302.4389.pdf", "metadata": {"source": "META", "title": "Maxout Networks", "authors": ["Ian J. Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville"], "emails": ["goodfeli@iro.umontreal.ca", "wardefar@iro.umontreal.ca", "mirzamom@iro.umontreal.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": "1. Introduction", "text": "A recently introduced technique known as dropout (Hinton et al., 2012) provides an inexpensive and simple means of forming a large ensemble of models that share parameters, as well as an inexpensive and simple means of approximating these models to make a prediction. Dropout has been used to improve the performance of multilayer perceptrons and deep revolutionary networks, with state-of-the-art technology ranging from audio classification to very large object recognition (Hinton et al., 2012; Krizhevsky et al.). While dropout works well in practice, it has not yet been shown that there are models for averaging deep architectures. Furthermore, dropout is associated with this paper."}, {"heading": "2. Review of dropout", "text": "Dropout is a technique that can be applied to deterministic predictive architectures that predict an output y given input vector. These architectures contain Xiv: 130 2.43 89v1 [st at.M L] February 18, 20a series of hidden layers h = {h (1),.., h (L)}. Dropout builds a model consisting of the set of all models that contains a subset of variables in both v and h. The same set of parameters is used to parameterize a family of distributions p (y | v). (Dropout builds a set of models that form a binary mask that determines which variables should be included in the model. If the model N contains complete input and hidden variables, and M denotes the set of all possible 2N masks that are then assumed to be randomly scanned, dropout maximizes the probability of the model by defining it."}, {"heading": "3. Description of maxout", "text": "The maxout model is simply a forward-looking architecture, such as a multi-layered perceptron or a deep Convolutionary Neural Network, which uses a new type of activation function: the maxout unit. In the face of an input x-Rd, a maxout hidden layer implements the functionhi (x) = max j [1, k] zijwherezij = x TW \u00b7 \u00b7 ij + bijfor learned parameters W, Rd \u00b7 m \u00b7 k and b \u00b2 Rm \u00b7 k. In the context of revolutionary networks, a maxout function card can be constructed by taking the maximum from caffeine function cards (i.e. a pool across channels rather than across spatial locations). A single maxout unit can be interpreted to represent a piecemeal linear approach to any convex function (in other words, the training algorithm optimizes the main parameters)."}, {"heading": "4. Maxout is a universal approximator", "text": "A standard MLP with enough hidden units is a universal approximator. Surprisingly, the maxout network requires only two maxout hidden units to be a universal approximator. The key is that each hidden unit may require any number of affine components. In particular, we show below that a maxout model with only two hidden units can approximate any continuous function of x-Rd. A diagram illustrating the basic idea of the proof is shown in Fig. 3. Consider the continuous piecewise linear (PWL) function g (x) consisting of k locally linear (affine) regions on Rd. Proposition 4.1 (from Theorem 2.1 in (Wang, 2004))) For all positive integers m and d, there are always two groups of d + 1-dimensional real parameters vectors [W1j, b1j], j [1] k, and Wj [b2j]."}, {"heading": "5. Benchmark results", "text": "We evaluated the Maxout model using four benchmark data sets and determined the state of the art for all. Maxout not only shows a marked improvement over previous methods, but also a marked improvement in the skills of dropouts."}, {"heading": "5.1. MNIST", "text": "The MNIST (LeCun et al., 1998) dataset consists of 28 x 28 pixel grayscale images of the handwritten digitalis0-9, with 60,000 training examples and 10,000 test examples. Traditionally, methods are evaluated in separate categories, depending on whether or not they take advantage of the fact that the examples have an image structure. For the permutation invariant version of the MNIST task, only methods that are not aware of the 2D structure of the data are allowed. In this case, we trained a model consisting of two tightly connected maxout layers, followed by a softmax layer. Besides using suspenders, we further regulate the model by imposing a restriction on the standard of each weight vector. All limitation values, learning rate and dynamics schedule parameters, layer sizes, etc. were selected by specifying the error on a validation quantity consisting of the last 10,000 training examples."}, {"heading": "5.2. CIFAR-10", "text": "The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) consists of 32 x 32 color images drawn from 10 classes; the training set contains 50,000 images and the test set contains 10,000; we pre-processed the data using global contrast normalization and ZCA whitening; this is the same pre-processing used by (Coates et al., 2011) on individual patches of the dataset in the context of unattended modeling of patches.We follow a similar procedure to the MNIST dataset, but with a modification. On MNIST, we find the best number of training periods in terms of validation errors, then probably record the training protocol and continue the entire training until the validation log probability has reached this value. On CIFAR-10, training like this model is impracticable because the final value of the learning rate is very small and the validation error rate is very high."}, {"heading": "5.3. CIFAR-100", "text": "The CIFAR-100 dataset (Krizhevsky & Hinton, 2009) has the same size and format as the CIFAR-10 dataset, but contains 100 classes, using only one-tenth of the labeled examples per class. Due to lack of time, we did not cross-validate hyperparameters on CIFAR-100, but simply applied the hyperparameters that provided the best validation set performance on CIFAR-10. We got a state-of-the-art test set error of 38.57% (if we do not use the entire training set, we get a test set error of 41.48%, which also exceeds the current state of the art). A summary of the best methods on CIFAR-100 can be found in Table 4."}, {"heading": "5.4. Street View House Numbers", "text": "The SVHN (Netzer et al., 2011) dataset consists of color images of house numbers collected by Google Street View. The dataset comes in two formats. We consider the second format, in which each image is 32 x 32 in size and the task is to classify the digit in the center of the image. Additional digits may appear next to it but must be ignored. This is a difficult, unsolved task in the real world with potential commercial applications for systems that achieve less than 1% error. There are 73,257 digits in the training set, 26,032 digits in the test set and 531,131 additional, slightly less difficult examples that can be used as an additional training set. Following Sermanet al al al (2012b) to build a validation set, we select 400 samples per class from the training set and 200 samples per class from the additional set. The remaining digits of the train and additional training numbers from the training are used for maxal."}, {"heading": "6. Model Averaging", "text": "After showing that maxout networks are effective learning algorithms, we turn to analyzing the reasons for their success. We first identify reasons that maxout networks are highly compatible with the approximate model of averaging techniques. The intuitive justification for averaging dropout models by dividing weights by 2 given by (Hinton et al., 2012) is that this is an exact model of averaging for a single layer model, softmax regression. To this characterization, we add the observation that the averaging of the model is extended to multiple linear layers. While this has the same representational power as a single layer, the expression of weights as a product of multiple matrices may have a different inductive bias. More importantly, it shows that dropouts have exact modeling averages in deeper architectures, provided they are locally linear under the space of the inputs to each layer being visited."}, {"heading": "7. Optimization", "text": "The second main reason maxout performs well is that it improves the learning phase of the dropout algorithm in bagging style. Note that the arguments in Section 6 that motivate the use of maxout also apply to linear units (???). On the surface, Maxout appears to be similar to max pooling via a set of linear corrected units, which is equivalent to the inclusion of a constant 0 in the sentence from which maxout selects the maximum. However, we note that including this constant 0 is very harmful to optimization in the context of dropout. For example, in MNIST, our best validation error rate is 1.04%. If we include a 0 at maximum, this value rises to more than 1.2%. In the context of dropout, we argue that maxout has better optimization properties compared to maximum pooling versus linear units."}, {"heading": "7.1. Optimization experiments", "text": "To verify that maxout delivers better optimization performance than max pooled rectified linear units when exercising with dropout, we conducted two experiments. First, we emphasized the optimization capability of the training algorithm by training a small (two hidden wavy layers with k = 2 and sixteen cores) model on the large (600,000 example) SVHN dataset. Training with rectifier units had a 7.3% training error. If we train with maxout units instead, we get a training error of 5.1%. As another optimization stress test, we tried very deep and narrow models on MNIST and found that maxout networks cope better than rectifiers with increasing depth. See Fig. 9 for details."}, {"heading": "7.2. Saturation", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "8. Conclusion", "text": "In this paper, we have proposed a new family of functions called maxout, which is particularly well suited for dropout training and for which we have demonstrated a universal approximation theorem. We have shown empirical evidence that dropouts achieve a good approximation to the model mean in deep models. We have shown that maxout takes advantage of this model averaging behavior because the approximation for maxout units is more accurate than for tanh units. We have shown that optimization in the context of dropouts behaves very differently than in the pure SGD case. By designing the maximum gradient so as to avoid pitfalls such as not using many filters of a model, we are able to train maxout networks on much larger training sets and with much deeper networks than is possible with rectifier units. We have also shown that maxout variations in the gradient due to different choices of the network of dropouts produce parameters in our model, rather than generating the benefit of each of the five dropout masses in our model."}, {"heading": "9. Acknowledgements", "text": "The authors would like to thank the developers of Theano, in particular Fre \u0301 de \u0301 ric Bastien and Pascal Lamblin for their support in developing the infrastructure and optimising performance, as well as Yann Dauphin for the helpful discussions."}], "references": [{"title": "Deep big simple neural nets for handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In Proc. International Conference on Computer Vision (ICCV\u201909),", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Jia", "Yangqing", "Huang", "Chang"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2011}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "The manifold tangent classifier", "author": ["Rifai", "Salah", "Dauphin", "Yann", "Vincent", "Pascal", "Bengio", "Yoshua", "Muller", "Xavier"], "venue": "In NIPS\u20192011,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009),", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Sermanet", "Pierre", "Chintala", "Soumith", "LeCun", "Yann"], "venue": "CoRR, abs/1204.3968,", "citeRegEx": "Sermanet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2012}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Sermanet", "Pierre", "Chintala", "Soumith", "LeCun", "Yann"], "venue": "In International Conference on Pattern Recognition (ICPR", "citeRegEx": "Sermanet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2012}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan Prescott"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "General constructive representations for continuous piecewise-linear functions", "author": ["Wang", "Shuning"], "venue": "IEEE Trans. Circuits Systems,", "citeRegEx": "Wang and Shuning.,? \\Q2004\\E", "shortCiteRegEx": "Wang and Shuning.", "year": 2004}, {"title": "Deep convex net: A scalable architecture for speech pattern classification", "author": ["Yu", "Dong", "Deng", "Li"], "venue": "In INTERSPEECH,", "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "CoRR, abs/1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "A recently introduced technique known as dropout (Hinton et al., 2012) provides an inexpensive and simple means of training a large ensemble of models that share parameters, as well as an inexpensive and simple means of approximately averaging together these models to make a prediction.", "startOffset": 49, "endOffset": 70}, {"referenceID": 2, "context": "Dropout has been used to improve the performance of multilayer perceptrons and deep convolutional networks, redefining the state of the art on tasks ranging from audio classification to very large scale object recognition (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 222, "endOffset": 268}, {"referenceID": 6, "context": "Dropout has been used to improve the performance of multilayer perceptrons and deep convolutional networks, redefining the state of the art on tasks ranging from audio classification to very large scale object recognition (Hinton et al., 2012; Krizhevsky et al., 2012).", "startOffset": 222, "endOffset": 268}, {"referenceID": 7, "context": "The MNIST (LeCun et al., 1998) dataset consists of 28 \u00d7 28 pixel greyscale images of handwritten digits", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "Rectifier MLP + dropout (Hinton et al., 2012) 1.", "startOffset": 24, "endOffset": 45}, {"referenceID": 9, "context": "Manifold Tangent Classifier (Rifai et al., 2011) 0.", "startOffset": 28, "endOffset": 48}, {"referenceID": 2, "context": "DBM + dropout (Hinton et al., 2012) 0.", "startOffset": 14, "endOffset": 35}, {"referenceID": 5, "context": "We were able to rapidly explore parameter space thanks to the extremely fast GPU convolution library developed by Krizhevsky et al. (2012). We obtained a test set error rate of 0.", "startOffset": 114, "endOffset": 139}, {"referenceID": 3, "context": "2-layer CNN+2-layer NN (Jarrett et al., 2009) 0.", "startOffset": 23, "endOffset": 45}, {"referenceID": 0, "context": "Note that it is possible to get better results on MNIST by augmenting the dataset with transformations of the standard set of images (Ciresan et al., 2010) .", "startOffset": 133, "endOffset": 155}, {"referenceID": 1, "context": "This is the same preprocessing applied by (Coates et al., 2011) to individual patches of the dataset in the context of unsupervised modeling of patches.", "startOffset": 42, "endOffset": 63}, {"referenceID": 13, "context": "CNN + Spearmint (Snoek et al., 2012) 14.", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "This is similar to the architecture used by (Hinton et al., 2012) except that our penultimate layer is fully connected instead of locally connected.", "startOffset": 44, "endOffset": 65}, {"referenceID": 8, "context": "The SVHN (Netzer et al., 2011) dataset consists of color images of house numbers collected by Google Street View.", "startOffset": 9, "endOffset": 30}, {"referenceID": 11, "context": "Following Sermanet et al. (2012b) to build a validation set, we select 400 samples per class from the training set and 200 samples per class from the extra set.", "startOffset": 10, "endOffset": 34}, {"referenceID": 2, "context": "The intuitive justification for averaging together dropout models by dividing the weights by 2 given by (Hinton et al., 2012) is that this does exact model averaging for a single layer model, softmax regression.", "startOffset": 104, "endOffset": 125}], "year": 2017, "abstractText": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout\u2019s fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR100, and SVHN.", "creator": "LaTeX with hyperref package"}}}