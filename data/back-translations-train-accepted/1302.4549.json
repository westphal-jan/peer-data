{"id": "1302.4549", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2013", "title": "Breaking the Small Cluster Barrier of Graph Clustering", "abstract": "This paper investigates graph clustering in the planted cluster model in the presence of {\\em small clusters}. Traditional results dictate that for an algorithm to provably correctly recover the clusters, {\\em all} clusters must be sufficiently large (in particular, $\\tilde{\\Omega}(\\sqrt{n})$ where $n$ is the number of nodes of the graph). We show that this is not really a restriction: by a more refined analysis of the trace-norm based recovery approach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones.", "histories": [["v1", "Tue, 19 Feb 2013 09:21:09 GMT  (57kb,D)", "https://arxiv.org/abs/1302.4549v1", null], ["v2", "Wed, 20 Feb 2013 08:35:39 GMT  (56kb,D)", "http://arxiv.org/abs/1302.4549v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nir ailon", "yudong chen", "huan xu"], "accepted": true, "id": "1302.4549"}, "pdf": {"name": "1302.4549.pdf", "metadata": {"source": "CRF", "title": "Breaking the Small Cluster Barrier of Graph Clustering", "authors": ["Nir Ailon", "Yudong Chen", "Xu Huan"], "emails": ["nailon@cs.technion.ac.il", "ydchen@utexas.edu", "mpexuh@nus.edu.sg"], "sections": [{"heading": null, "text": "We show that this is not really a limitation: by refining the approach to the restoration of trace-norm matrices proposed in Jalali et al. [2011] and Chen et al. [2012], we demonstrate that small clusters, under certain mild assumptions, do not impede the restoration of large clusters. On the basis of this result, we develop an iterative algorithm for restoring almost all clusters by means of a \"peeling strategy,\" i.e., first restoring large clusters, which leads to a reduced problem, and repeat this procedure. These results are extended to the partial observation situation in which only a (selected) part of the graph is observed. The peeling strategy leads to an active learning algorithm in which edges adjacent to smaller clusters are queried more frequently as large clusters are learned (and removed)."}, {"heading": "1 Introduction", "text": "This paper considers a classic problem in machine learning and theoretical computer science, namely graph clustering, i.e. it is not the only problem we identify in many areas of science and engineering. Some prominent examples include the capture of communities in the social network Mishra et al. [2007], the sub-market identification of graphics in e-commerce and sponsored search is indeed higher than those in other areas. [1995], among others. From a purely binary classification of views, the edges of graphics are (noisy) labels of similarity or affinity between object pairs and the concept class consists of objects (encoded graphically by the identification of clusters with cliques).Many theoretical results in graphical clusters."}, {"heading": "1.1 Previous work", "text": "The literature of graph cluster size is too large for a detailed overview here; we focus on most of the work associated with it, and in certain cases, these offer theoretical guarantees on cluster restoration. [2011] While the arrangement we are examining is the classic planted partition models Boppana [1987], also known as the stochastic block model Rohe et al. [2011] Here, n nodes are divided into subsets called \"true clusters,\" and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to an equal subset, an edge is generated that connects them with a probability of p or q. The goal is to correctly restore the clusters given the random graphics. The planted partition model was examined as early as 1980 by Boppana [1987]. Previous work focused on the 2-partition or general l-partition case, i.e."}, {"heading": "2 Notation and Setup", "text": "Throughout, V denotes a series of elements that we identify with the set [n] q = 1,..., n}. We assume a true accumulation of truth of V given by a pair of entanglements of V1..., Vk, where k is the number of clusters. We say i \u0445 j, if i, j \"Va for some a\" [k], otherwise i 6 \u0445 y. we leave ni = | Vi | for all i. \"For all i\" n \"is < i > the unique index that satisfies i\" V < i >. For a matrix X \"Rn\" and a subset S \"[n] of size m,\" the matrix X \"Rm\" m \"m\" m \"is the main minority number of X corresponding to the set of indices S. For a matrix\" M \"(M) means the support of M\" mmm, \"namely the set of index pairs (i, j) etrix.\""}, {"heading": "3 Results", "text": "We remind the reader that the Trace-Norm of a Matrix is the sum of its individual values, and we define the \"1 Norm of a Matrix B.\" (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\" (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\"). (\"1 Norm of another Matrix B.\" (\"). (\" 1 Norm of another Matrix B. \"). (\" (\"1 Norm of another Matrix B.\"). (\"(\"). (\"1 Norm of another Matrix B.\" (\"). (\" (\"1 Norm of another Matrix B.\"). (\"(\"). (\"). (\"). (\"(\" 1 Norm of another Matrix B. (\"). (\" 1 Norm of another Matrix B. (\"). (\"). (\"1 Norm of another Matrix B. (\"). (\"). (\" 1 Norm of another Matrix B. (\"1). (\"). (\"). (\" 1 Norm of another Matrix B. (\"). (\"). (\"). (\"). (\"(\" 1). (\"1 Norm of another Matrix B. (\" 1). (\"1). (\" (\"1). (\"). (\"). (\" (\"). (\" 1 Norm of another Matrix B. (\"). (\"). (. (\"1). (\"). (\"1). (. (\"). ("}, {"heading": "3.1 Partial Observations", "text": "We consider the case where the input matrix A is not given to us in its entirety, but rather that we have oracular access to A (i, j) for (i, j) n of our choice. \"Unused\" values become formally independent of each other with A (i, j) = q q q (f) = q q (f) for (i, j) n (for i) n (for i) n (for i) n (for each i, j). More precisely, for i \"j\" we have A (i, j) = 1 with probability (1) p), 0 with probability (1 \u2212 p) and? with probability remaining (1 \u2212 p). \"For i\" j \"with probability we have A (i, j) = 1 with probability (0)."}, {"heading": "4 Experiments", "text": "We experimented with simplified versions of our algorithms. Here, we did not try to calculate the various constants that define the algorithms in this work, creating a difficulty in the exact implementation. Instead, it is obvious that our experiments support our theoretical results. A more practical \"user guide\" for this method with actual constants is the subject of future work. In all the experiment reports below, we use a variant of the augmented Lagrangian Multiplier (ALM) method Lin et al. [2009] to solve the semi-defined program (CP1). Whenever we say that \"the clusters {Vi1, Vi2,.} have been restored,\" we think that a corresponding instantiation (CP1) has led to an optimal solution."}, {"heading": "5 Discussion", "text": "Our current results do not say anything about clusters that are neither large nor small and fall at intervals (','). Our numerical experiments confirm that the medium-size phenomenon is real: they are neither fully recovered nor completely ignored by the optimal K cluster. Therefore, the part of K cluster confined to these clusters does not seem to have an obvious pattern. It would be interesting to apply the presented methodology to real-world applications, in particular to large datasets merged from web applications and social networks. Therefore, our experiments focused on confirming the theoretical results with data generated exactly according to the distribution for which we could provide verifiable guarantees. It would be interesting to apply the presented methodology to real-world applications, especially to large datasets merged from web applications and social networks. Another interesting direction is the extension of the \"peeling strategy\" to other high-dimensional learning problems, which requires understanding if such a strategy works delightfully."}, {"heading": "A Notation and Conventions", "text": "For a real n \u00b7 n matrix M, we use the unadorned standard (< M) to mark its spectral norm; the notation amp # 160; M & # 160; F formally refers to the Frobenius norm; in order to distinguish it from the matrices studied in this paper, we will simply call these objects \"operators\" and mark them with a calligraphic font, e.g. P & # 160; P & # 160; n. M & # 160; M & # 160; M & # 160; n. & # 160; n. & # 160; M & # 160; n. & # 160; n. & # 160; n."}, {"heading": "B Proof of Theorem 1", "text": "The evidence is based on the results of the study. (2012) We point out that the adjustment for Q (Q) (Q) (Q) (Q) (Q) (Q) (Q) (Q) (P) (Q) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P)) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P)) (P) (P) (P) (P)) (P) (P) (P) (P) (P) () (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P) (P (P) (P) (P (P) (P) (P) (P) (P) (P) (P) (P (P) (P (P) (P) (P) (P (P) (P (P) (P) (P) (P) (P) (P (P) (P) (P) (P (P) (P) (P (P) (P) (P (P) (P) (P) (P) (P) (P) (P) (P) (P (P) (P (P) (P) (P) (P) (P) (P) (P) (P) (P (P) (P) (P"}, {"heading": "C Proof of Theorem 3", "text": "We assume that K is a partial clustering caused by U1. First, we must note that any value of t in the permitted range [14p + 3 4q, 3 4p + 1 4q] for satisfaction q + 14 (p \u2212 q) for satisfaction q + 14 (p \u2212 q) for satisfaction q + 14 (p \u2212 q) \u2264 t \u2264 4 (p \u2212 q). Also, we must note that from the definition of t, c1, c2 + 1 4 (p \u2212 q) for satisfaction c2 c2 + c2 = t \u2264 p \u2212 4 (p \u2212 q). We say that a pair of the sentences Y V, Z V is separated if there is no pair."}, {"heading": "D Proof of Theorem 9", "text": "Evidence of theorem 3 in the previous section repeatedly used Hoeffding tail inequalities to limit the size of the intersection of sound support \"with various submatrices, which is narrow for p, q delimited by 0 and 1. However, if p = \u03c1p, q = \u03c1q\" the sound probabilities p, \"q\" are fixed and tend towards 0, a sharper boundary is obtained with Amber's tail boundary (see Appendix F.2, Lemma 17). Using Amber's inequality instead of Chernoff inequality, the expression (p \u2212 q) 2 in (9), (11), (13), (14), (15), (16), (17), (20), (21), (22), (23) can be substituted, which clearly gives the desired result."}, {"heading": "E Proof of Lemma 5", "text": "Proof. Let us remind the user that g = b3b4 log 2 n, the multiplicative value of the interval '[,'. Let us consider the set of intervals (n / gk0, n / k0), (n / g 2k0, n / gk0),... (n / g k0 + 1k0, n / g k0k0). According to the push-hole principle, one of these intervals must not intersect the set of cluster sizes. Suppose this interval is (n / gi0 + 1k0, n / g i0k0), for some 0 \u2264 i0 k0. Let us leave \u03b1 = n / gi + 1k0. Let us set C3 (p, q) small enough and C4 (p, q) large enough, it is easy to verify that the requirements of Korollary 4 apply at this value of \u03b1 and s = n / k0."}, {"heading": "F Technical Lemmas", "text": "F. 1 The spectral norm of random matrices is generally known that the spectral norm \u03bb1 (A) of a zero-mean random matrix A is higher than that of the constant C, where C is a constant dependent on the variance and magnitude of the entries of A. Let Aij, 1 \u2264 i, j \u2264 n be independent random variables, each of which has a mean 0 and a variance of no more than 2 and in absolute values of B. Then, with probability, at least 1 \u2212 2n \u2212 2\u04321 (A) \u2264 6 max {n \u00b2 n Log n n n n n n Yum n, B log2 n} Proof. Let it be the i-th standard basis in Rn. Let Zij = Aijeie > j's be zero-average variables in Zij."}], "references": [{"title": "Active learning using smooth relative regret approximations with", "author": ["N. Ailon", "R. Begleiter", "E. Ezra"], "venue": null, "citeRegEx": "Ailon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2008}, {"title": "Correlation clustering", "author": ["N. Bansal", "A. Blum", "S. Chawla"], "venue": "Machine Learning,", "citeRegEx": "Bansal et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2004}, {"title": "Robust principal component analysis", "author": ["E. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Hill-climbing finds random planted bisections", "author": ["T. Carson", "R. Impagliazzo"], "venue": "In SODA,", "citeRegEx": "Carson and Impagliazzo.,? \\Q2001\\E", "shortCiteRegEx": "Carson and Impagliazzo.", "year": 2001}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "S. Parrilo", "A. Willsky"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Clustering with qualitative information", "author": ["Moses Charikar", "Venkatesan Guruswami", "Anthony Wirth"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Charikar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charikar et al\\.", "year": 2005}, {"title": "Spectral clustering of graphs with general degrees in the extended planted partition model", "author": ["K. Chaudhuri", "F. Chung", "A. Tsiatas"], "venue": null, "citeRegEx": "Chaudhuri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "Clustering sparse graphs", "author": ["Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "In NIPS. Available on arXiv:1210.3335,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Random Structures and Algorithms,", "citeRegEx": "Condon and Karp.,? \\Q2001\\E", "shortCiteRegEx": "Condon and Karp.", "year": 2001}, {"title": "Correlation clustering in general weighted graphs", "author": ["E. Demaine", "D. Emanuel", "A. Fiat", "N. Immorlica"], "venue": "Theoretical Comp. Sci.,", "citeRegEx": "Demaine et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Demaine et al\\.", "year": 2006}, {"title": "Active clustering: Robust and efficient hierarchical clustering using adaptively selected similarities", "author": ["B. Eriksson", "G. Dasarathy", "A. Singh", "R. Nowak"], "venue": null, "citeRegEx": "Eriksson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 2011}, {"title": "A database interface for clustering in large spatial databases", "author": ["M. Ester", "H. Kriegel", "X. Xu"], "venue": "In KDD,", "citeRegEx": "Ester et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ester et al\\.", "year": 1995}, {"title": "Reconstructing many partitions using spectral techniques", "author": ["J. Giesen", "D. Mitsche"], "venue": "In Fundamentals of Computation Theory, pages 433\u2013444,", "citeRegEx": "Giesen and Mitsche.,? \\Q2005\\E", "shortCiteRegEx": "Giesen and Mitsche.", "year": 2005}, {"title": "Correlation clustering with a fixed number of clusters", "author": ["Ioannis Giotis", "Venkatesan Guruswami"], "venue": "Theory of Computing,", "citeRegEx": "Giotis and Guruswami.,? \\Q2006\\E", "shortCiteRegEx": "Giotis and Guruswami.", "year": 2006}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "In ICML. Available on arXiv:1104.4803,", "citeRegEx": "Jalali et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2011}, {"title": "Efficient active algorithms for hierarchical clustering", "author": ["A. Krishnamurthy", "S. Balakrishnan", "M. Xu", "A. Singh"], "venue": null, "citeRegEx": "Krishnamurthy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2012}, {"title": "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "UIUC Technical Report UILU-ENG-09-2215,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": "In FOCS, pages 529\u2013537,", "citeRegEx": "McSherry.,? \\Q2001\\E", "shortCiteRegEx": "McSherry.", "year": 2001}, {"title": "Clustering social networks. Algorithms and Models for Web-Graph", "author": ["N. Mishra", "I. Stanton R. Schreiber", "R.E. Tarjan"], "venue": null, "citeRegEx": "Mishra et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mishra et al\\.", "year": 2007}, {"title": "Finding dense clusters via low rank + sparse decomposition", "author": ["S. Oymak", "B. Hassibi"], "venue": null, "citeRegEx": "Oymak and Hassibi.,? \\Q2011\\E", "shortCiteRegEx": "Oymak and Hassibi.", "year": 2011}, {"title": "Spectral clustering and the high-dimensional stochastic block model", "author": ["K. Rohe", "S. Chatterjee", "B. Yu"], "venue": "Ann. of Stat.,", "citeRegEx": "Rohe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohe et al\\.", "year": 2011}, {"title": "Spectral Clustering on a Budget", "author": ["O. Shamir", "N. Tishby"], "venue": "In AISTATS,", "citeRegEx": "Shamir and Tishby.,? \\Q2011\\E", "shortCiteRegEx": "Shamir and Tishby.", "year": 2011}, {"title": "Improved algorithms for the random cluster graph model", "author": ["R. Shamir", "D. Tsur"], "venue": "Random Struct. & Alg.,", "citeRegEx": "Shamir and Tsur.,? \\Q2007\\E", "shortCiteRegEx": "Shamir and Tsur.", "year": 2007}, {"title": "Active clustering of biological sequences", "author": ["K. Voevodski", "M. Balcan", "H. R\u00f6glin", "S. Teng", "Y. Xia"], "venue": "JMLR, 13:203\u2013225,", "citeRegEx": "Voevodski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Voevodski et al\\.", "year": 2012}, {"title": "Proof of Theorem 1 The proof is based on Chen et al. [2012]. We prove it for \u03ba = 1. The adjustment for \u03ba > 1 is done using a padding", "author": [], "venue": null, "citeRegEx": "B,? \\Q2012\\E", "shortCiteRegEx": "B", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "We show that this is not really a restriction: by a more refined analysis of the trace-norm based matrix recovery approach proposed in Jalali et al. [2011] and Chen et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones.", "startOffset": 11, "endOffset": 30}, {"referenceID": 17, "context": "Some prominent examples include community detection in social network Mishra et al. [2007], submarket identification in E-commerce and sponsored search Yahoo!-Inc [2009], and co-authorship analysis in analyzing document database Ester et al.", "startOffset": 70, "endOffset": 91}, {"referenceID": 17, "context": "Some prominent examples include community detection in social network Mishra et al. [2007], submarket identification in E-commerce and sponsored search Yahoo!-Inc [2009], and co-authorship analysis in analyzing document database Ester et al.", "startOffset": 70, "endOffset": 170}, {"referenceID": 11, "context": "[2007], submarket identification in E-commerce and sponsored search Yahoo!-Inc [2009], and co-authorship analysis in analyzing document database Ester et al. [1995], among others.", "startOffset": 145, "endOffset": 165}, {"referenceID": 6, "context": ", Chaudhuri et al., 2012, Bollob\u00e1s and Scott, 2004, Chen et al., 2012, McSherry, 2001] is not really a restriction, but rather an artifact of the attempt to solve the problem in a single shot using convex relaxation techniques. Using a more careful analysis, we prove that the mixed trace-norm and `1 based convex formulation, initially proposed in Jalali et al. [2011], can recover clusters of size \u03a9\u0303( \u221a n) even in the presence of smaller clusters.", "startOffset": 2, "endOffset": 370}, {"referenceID": 13, "context": "(1) We provide a refined analysis of the mixed trace norm and `1 convex relaxation approach for exact recovery of clusters proposed in Jalali et al. [2011] and Chen et al.", "startOffset": 135, "endOffset": 156}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], focusing on the case where small clusters exist.", "startOffset": 11, "endOffset": 30}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], focusing on the case where small clusters exist. We show that in the classical planted partition settings Boppana [1987], if each cluster is either large (more precisely, of size at least x \u2248 \u221a n log n) or small (of size at most x/ log n), then with high probability, this convex relaxation approach correctly identifies all big clusters while \u201cignoring\u201d the small ones.", "startOffset": 11, "endOffset": 152}, {"referenceID": 14, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al.", "startOffset": 85, "endOffset": 100}, {"referenceID": 12, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al. [2011]. Here, n nodes are partitioned into subsets, referred as the \u201ctrue clusters\u201d, and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probability p or q respectively.", "startOffset": 142, "endOffset": 161}, {"referenceID": 12, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al. [2011]. Here, n nodes are partitioned into subsets, referred as the \u201ctrue clusters\u201d, and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probability p or q respectively. The goal is to correctly recover the clusters given the random graph. The planted partition model has been studied as early as 1980\u2019s Boppana [1987]. Earlier work focused on the 2-partition or more generally l-partition case, i.", "startOffset": 142, "endOffset": 586}, {"referenceID": 12, "context": "Planted partition model: The setup we study is the classical planted partition model Boppana [1987], also known as the stochastic block model Rohe et al. [2011]. Here, n nodes are partitioned into subsets, referred as the \u201ctrue clusters\u201d, and a graph is randomly generated as follows: for each pair of nodes, depending on whether they belong to a same subset, an edge connecting them is generated with a probability p or q respectively. The goal is to correctly recover the clusters given the random graph. The planted partition model has been studied as early as 1980\u2019s Boppana [1987]. Earlier work focused on the 2-partition or more generally l-partition case, i.e., the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004].", "startOffset": 142, "endOffset": 717}, {"referenceID": 5, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004].", "startOffset": 51, "endOffset": 74}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004].", "startOffset": 75, "endOffset": 105}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes.", "startOffset": 75, "endOffset": 132}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al.", "startOffset": 75, "endOffset": 497}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al. [2012], Ames and Vavasis [2011], Oymak and Hassibi [2011].", "startOffset": 75, "endOffset": 517}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al. [2012], Ames and Vavasis [2011], Oymak and Hassibi [2011].", "startOffset": 75, "endOffset": 542}, {"referenceID": 3, "context": ", the minimal cluster size is \u0398(n) Boppana [1987], Condon and Karp [2001], Carson and Impagliazzo [2001], Bollob\u00e1s and Scott [2004]. Recently, several works have proposed methods to handle sublinear cluster sizes. These works can be roughly classified into three approaches: randomized algorithms [e.g., Shamir and Tsur, 2007], spectral clustering [e.g., McSherry, 2001, Giesen and Mitsche, 2005, Chaudhuri et al., 2012, Rohe et al., 2011]), and low-rank matrix decomposition Jalali et al. [2011], Chen et al. [2012], Ames and Vavasis [2011], Oymak and Hassibi [2011]. While these work differs in the methodology, they all impose constraints on the size of the minimum true cluster \u2013 the best result up-to-date requires it to be \u03a9\u0303( \u221a n).", "startOffset": 75, "endOffset": 568}, {"referenceID": 0, "context": "Correlation Clustering This problem, originally defined by Bansal, Blum and Chawla Bansal et al. [2004], also considers graph clustering but in an adversarial noise setting.", "startOffset": 83, "endOffset": 104}, {"referenceID": 0, "context": "Correlation Clustering This problem, originally defined by Bansal, Blum and Chawla Bansal et al. [2004], also considers graph clustering but in an adversarial noise setting. The goal there is to find the clustering minimizing the total disagreement (intercluster edges plus intracluster nonedges), without there being necessarily a notion of true clustering (and hence no \u201cexact recovery\u201d). This problem is usually studied in the combinatorial optimization framework and is known to be NP-Hard to approximate to within some constant factor. Prominent work includes Demaine et al. [2006], Ailon et al.", "startOffset": 83, "endOffset": 587}, {"referenceID": 0, "context": "[2006], Ailon et al. [2008], Charikar et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "[2006], Ailon et al. [2008], Charikar et al. [2005]. A PTAS is known in case the number of clusters is fixed Giotis and Guruswami [2006].", "startOffset": 8, "endOffset": 52}, {"referenceID": 0, "context": "[2006], Ailon et al. [2008], Charikar et al. [2005]. A PTAS is known in case the number of clusters is fixed Giotis and Guruswami [2006].", "startOffset": 8, "endOffset": 137}, {"referenceID": 3, "context": "Low rank matrix decomposition via trace norm: Motivated from robust PCA, it has recently been shown Chandrasekaran et al. [2011], Cand\u00e8s et al.", "startOffset": 100, "endOffset": 129}, {"referenceID": 2, "context": "[2011], Cand\u00e8s et al. [2011], that it is possible to recover a low-rank matrix from sparse errors of arbitrary magnitude, where the key ingredient is using trace norm (aka nuclear norm) as a convex surrogate of the rank.", "startOffset": 8, "endOffset": 29}, {"referenceID": 2, "context": "[2011], Cand\u00e8s et al. [2011], that it is possible to recover a low-rank matrix from sparse errors of arbitrary magnitude, where the key ingredient is using trace norm (aka nuclear norm) as a convex surrogate of the rank. A similar result is also obtained when the low rank matrix is corrupted by other types of noise Xu et al. [2012].", "startOffset": 8, "endOffset": 334}, {"referenceID": 13, "context": "Of particular relevance to this paper is Jalali et al. [2011] and Chen et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], where the authors apply this approach to graph clustering, and specifically to the planted partition model.", "startOffset": 11, "endOffset": 30}, {"referenceID": 7, "context": "[2011] and Chen et al. [2012], where the authors apply this approach to graph clustering, and specifically to the planted partition model. Indeed, Chen et al. [2012] achieve state-of-art performance guarantees for the planted partition problem.", "startOffset": 11, "endOffset": 166}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering.", "startOffset": 25, "endOffset": 45}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do.", "startOffset": 25, "endOffset": 404}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al.", "startOffset": 25, "endOffset": 832}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al.", "startOffset": 25, "endOffset": 858}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al. [2011], Krishnamurthy et al.", "startOffset": 25, "endOffset": 910}, {"referenceID": 0, "context": "The most related work is Ailon et al. [2012], who investigated active learning for correlation clustering. The authors obtain a (1 + \u03b5)-approximate solution with respect to the optimal, while (actively) querying no more than O(npoly(log n, k, \u03b5\u22121)) edges. The result imposed no restriction on cluster sizes and hence inspired this work, but differs in at least two major ways. First, Ailon et al. [2012] did not consider exact recovery as we do. Second, their guarantees fall in the ERM (Empirical Risk Minimization) framework, with no running time guarantees. Our work recovers true cluster exactly using a convex relaxation algorithm, and is hence computationally efficient. The problem of active learning has also been investigated in other clustering setups including clustering based on distance matrix Voevodski et al. [2012], Shamir and Tishby [2011], and hierarchical clustering Eriksson et al. [2011], Krishnamurthy et al. [2012]. These setups differ from ours and cannot be easily compared.", "startOffset": 25, "endOffset": 939}, {"referenceID": 7, "context": ") The proof is based on Chen et al. [2012] and is deferred to the supplemental material due to lack of space.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": "3In comparison, Ailon et al. [2012] require k0 to be constant for their guarantees, as do the Correlation Clustering PTAS Giotis and Guruswami [2006].", "startOffset": 16, "endOffset": 36}, {"referenceID": 0, "context": "3In comparison, Ailon et al. [2012] require k0 to be constant for their guarantees, as do the Correlation Clustering PTAS Giotis and Guruswami [2006].", "startOffset": 16, "endOffset": 150}, {"referenceID": 16, "context": "In all experiment reports below, we use a variant of the Augmented Lagrangian Multiplier (ALM) method Lin et al. [2009] to solve the semi-definite program (CP1).", "startOffset": 102, "endOffset": 120}], "year": 2013, "abstractText": "This paper investigates graph clustering in the planted cluster model in the presence of small clusters. Traditional results dictate that for an algorithm to provably correctly recover the clusters, all clusters must be sufficiently large (in particular, \u03a9\u0303( \u221a n) where n is the number of nodes of the graph). We show that this is not really a restriction: by a more refined analysis of the trace-norm based matrix recovery approach proposed in Jalali et al. [2011] and Chen et al. [2012], we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to recover almost all clusters via a \u201cpeeling strategy\u201d, i.e., recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often as large clusters are learned (and removed). From a high level, this paper sheds novel insights on high-dimensional statistics and learning structured data, by presenting a structured matrix learning problem for which a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations does the job.", "creator": "LaTeX with hyperref package"}}}