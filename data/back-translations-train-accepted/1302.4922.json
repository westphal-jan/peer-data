{"id": "1302.4922", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Structure Discovery in Nonparametric Regression through Compositional Kernel Search", "abstract": "Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.", "histories": [["v1", "Wed, 20 Feb 2013 14:53:13 GMT  (2806kb,D)", "http://arxiv.org/abs/1302.4922v1", "9 pages, 7 figures"], ["v2", "Tue, 5 Mar 2013 11:48:12 GMT  (2810kb,D)", "http://arxiv.org/abs/1302.4922v2", "9 pages, 7 figures, Submitted for review"], ["v3", "Fri, 5 Apr 2013 16:53:30 GMT  (2358kb,D)", "http://arxiv.org/abs/1302.4922v3", "9 pages, 7 figures, Submitted for review"], ["v4", "Mon, 13 May 2013 13:10:31 GMT  (2372kb,D)", "http://arxiv.org/abs/1302.4922v4", "9 pages, 7 figures, To appear in proceedings of the 2013 International Conference on Machine Learning"]], "COMMENTS": "9 pages, 7 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["david k duvenaud", "james robert lloyd", "roger b grosse", "joshua b tenenbaum", "zoubin ghahramani"], "accepted": true, "id": "1302.4922"}, "pdf": {"name": "1302.4922.pdf", "metadata": {"source": "META", "title": "Structure Discovery in Nonparametric Regression through Compositional Kernel Search", "authors": ["David Duvenaud", "James Robert Lloyd", "Roger Grosse", "Joshua B. Tenenbaum", "Zoubin Ghahramani"], "emails": ["dkd23@cam.ac.uk", "jrl44@cam.ac.uk", "rgrosse@mit.edu", "jbt@mit.edu", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "2. Expressing structure through kernels", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "3. Searching over structures", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves in the world, and that we are able to assert ourselves in the world, that we are able to stay in the world, \"he said."}, {"heading": "4. Related Work", "text": "It is possible to achieve greater flexibility by putting into the world more than half of the people who are able to move in the world. (...) It is very difficult to generalize the basic versions of these methods. (...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety. \"(...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety. (...) It is very difficult to grasp them in their entirety."}, {"heading": "5. Structure discovery in time series", "text": "In order to gain the ability to discover the structure, we performed a search for several data sets. Our method opens up in these data sets and produces plausible extrapolations. As discussed in Section 2, a gp, whose core is a sum of functions drawn by the components, can provide another method of visualizing the acquired structures. In particular, all the nuclei in our search space can be described as the equivalent sum of products of the distribution effects."}, {"heading": "6. Validation on synthetic data", "text": "We validated the ability of our method to restore known structures using a set of synthetic datasets. We constructed synthetic data for multiple compound kernel expressions by first randomly sampling 300 points and then sampling function values at these points from a previous GPS test. We then added Gaussian noise to the function values, selecting the variance so that the standard deviation of noise \u03c3n in relation to the function sample variance was 0.1. Table 1 lists the composite cores used to generate data (subscriptions indicate the dimension to which each core was applied), the dimensionality D of the input space, the core selected by our search, and the estimated noise variance. Our method finds all relevant structures in all but one test. In fact, it also detects unintentionally introduced linear structures: functions sampled from SE cores with long length scales, occasionally leading to near-linear trends in the data."}, {"heading": "7. Quantitative evaluation", "text": "In addition to the qualitative evaluation in Section 5, we have quantitatively examined how our method performs in both extrapolation and interpolation tasks."}, {"heading": "7.1. Extrapolation", "text": "We compared the extrapolation capabilities of our model with standard baselines. Divided into contiguous training and test sets, we calculated the predictive mean square error (MSE) of each method. We varied the size of the training set from the first 10% to the first 90% of the data range. Figure 7 shows the learning curves of linear regression, a variety of Gp models for fixed cores, and our methods. gp models with only SE and Pro cores did not capture the long-term trends, as the best parameter values in terms of the marginal Gp probability capture only the short-term structure. Linear regression roughly captured the long-term trend, but quickly placed itself in the forecast performance. The richer Gp models (SE + Per and SE \u00d7 Per) ultimately captured more structure and performed better, but the complete structures discovered by our search outperformed the other approaches in terms of predicting all data sets."}, {"heading": "7.2. High-dimensional prediction", "text": "In order to evaluate the predictive accuracy of our method in a high-dimensional environment, we extended the comparison of (Duvenaud et al., 2011) to our method. We performed 10-fold cross-validation on 5 datasets2 and compared 5 methods with respect to MSE and predictive probability. Our structure search was performed using SE and RQ as the base core families down to the depth of 10. The comparison included three methods with fixed core families: Additives gps, Generalized Additive Models (GAM) and a GPS with a standard SE kernel using automatic relevance determination (gp SEARD). Also included was the associated kernel search method of Hierarichical Kernel Learning (HKL). The results are shown in Table 2; our method outperformed all other methods in all tests.2The datasets showed dimensions from 4 to 13 and the number of data points ranged from 150 to 450."}, {"heading": "8. Discussion", "text": "\"It would be very nice to have a formal apparatus that gives us an\" optimal \"way to detect unusual phenomena and invent new classes of hypotheses that contain the true one; but this remains an art for the creative human mind.\" E. T. Jaynes, 1985The ability to automatically learn kernel parameters and combination weights was an important factor in the widespread use of kernel methods. Essentially, however, it was up to the user to choose the right shape for the kernel, a task that required considerable expertise. To automate this process, we introduced a space of composite cores, defined compositively as sums and products of a small number of base cores. We proposed a search method for this space of cores that resembled the process of scientific discoveries. We found that the learned structures are often able to extrapolate precisely in complex time series data sets, mirroring the combinations commonly used by kernel, mirroring the diversity of combinations."}, {"heading": "Acknowledgements", "text": "The authors thank Carl Rasmussen for the helpful discussions."}, {"heading": "A. Appendix", "text": "Kernel definitions For scalar inputs, the squared exponential (SE), periodic (Per), linear (Lin) and rational square (RQ) nuclei are defined as follows: kSE (x, x \u2032) = \u03c32 exp (\u2212 (x \u2212 x \u2032) 22 '2) kPer (x, x \u2032) = \u03c32 exp (\u2212 2 sin 2 (\u03c0 | x \u2212 x \u2032 | / p)' 2) kLin (x, x \u2032) = \u03c32b + \u03c3 2 v (x \u2032 \u2212) (x \u2032 \u2212) kRQ (x \u2032) = (1 + (x \u2212 x \u2032) 22\u03b1 '2) \u2212 \u03b1 posterior decomposition We can analytically decompose a gp posterior distribution by additive components using the following identity: The conditional distribution of a Gauss vector f1 conditioned by another Gauss vector f = f1 + f2, where f1 posterior decomposition is deposited by additive components over K\u00b5T (K1), K1 (1) (1), K\u00b5T (1) (1)."}], "references": [{"title": "Exploring large feature spaces with hierarchical multiple kernel learning", "author": ["F. Bach"], "venue": "In Advances in Neural Information Processing Systems, pp", "citeRegEx": "Bach,? \\Q2009\\E", "shortCiteRegEx": "Bach", "year": 2009}, {"title": "A GP-based kernel construction and optimization method for RVM", "author": ["W. Bing", "Z. Wen-qiong", "C. Ling", "L. Jia-hong"], "venue": "In International Conference on Computer and Automation Engineering (ICCAE),", "citeRegEx": "Bing et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bing et al\\.", "year": 2010}, {"title": "Time series analysis: forecasting and control", "author": ["G.E.P. Box", "G.M. Jenkins", "G.C. Reinsel"], "venue": null, "citeRegEx": "Box et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Box et al\\.", "year": 1976}, {"title": "Bayesian localized multiple kernel learning", "author": ["M. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "Technical report, EECS Department,", "citeRegEx": "Christoudias et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Christoudias et al\\.", "year": 2009}, {"title": "Evolving kernel functions for SVMs by genetic programming", "author": ["L. Diosan", "A. Rogozan", "J.P. Pecuchet"], "venue": "In Machine Learning and Applications,", "citeRegEx": "Diosan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Diosan et al\\.", "year": 2007}, {"title": "Additive Gaussian processes", "author": ["D. Duvenaud", "H. Nickisch", "C.E. Rasmussen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Duvenaud et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2011}, {"title": "Exploiting compositionality to explore a large space of model structures", "author": ["R.B. Grosse", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Grosse et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2012}, {"title": "Smoothing spline ANOVA models", "author": ["C. Gu"], "venue": null, "citeRegEx": "Gu,? \\Q2002\\E", "shortCiteRegEx": "Gu", "year": 2002}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "Highly informative priors", "author": ["E.T. Jaynes"], "venue": "In Proceedings of the Second International Meeting on Bayesian Statistics,", "citeRegEx": "Jaynes,? \\Q1985\\E", "shortCiteRegEx": "Jaynes", "year": 1985}, {"title": "The discovery of structural form", "author": ["C. Kemp", "J.B. Tenenbaum"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kemp and Tenenbaum,? \\Q2008\\E", "shortCiteRegEx": "Kemp and Tenenbaum", "year": 2008}, {"title": "Probabilistic non-linear principal component analysis with gaussian process latent variable models", "author": ["N. Lawrence"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lawrence,? \\Q2005\\E", "shortCiteRegEx": "Lawrence", "year": 2005}, {"title": "Reconstruction of solar irradiance since 1610: Implications for climate change", "author": ["J. Lean", "J. Beer", "R. Bradley"], "venue": "Geophysical Research Letters,", "citeRegEx": "Lean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Lean et al\\.", "year": 1995}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Accuracy versus interpretability in flexible modeling: Implementing a tradeoff using Gaussian process models", "author": ["T.A. Plate"], "venue": "Behaviormetrika,", "citeRegEx": "Plate,? \\Q1999\\E", "shortCiteRegEx": "Plate", "year": 1999}, {"title": "Sum-product networks: a new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Conference on Uncertainty in AI,", "citeRegEx": "Poon and Domingos,? \\Q2011\\E", "shortCiteRegEx": "Poon and Domingos", "year": 2011}, {"title": "Occam\u2019s razor", "author": ["C.E. Rasmussen", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmussen and Ghahramani,? \\Q2001\\E", "shortCiteRegEx": "Rasmussen and Ghahramani", "year": 2001}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Semiparametric regression, volume 12", "author": ["D. Ruppert", "M.P. Wand", "R.J. Carroll"], "venue": null, "citeRegEx": "Ruppert et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ruppert et al\\.", "year": 2003}, {"title": "Using deep belief nets to learn covariance kernels for Gaussian processes", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Advances in Neural information processing systems,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2008}, {"title": "Distilling free-form natural laws from experimental data", "author": ["M. Schmidt", "H. Lipson"], "venue": null, "citeRegEx": "Schmidt and Lipson,? \\Q2009\\E", "shortCiteRegEx": "Schmidt and Lipson", "year": 2009}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics,", "citeRegEx": "Schwarz,? \\Q1978\\E", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "Declarative bias in equation discovery", "author": ["L. Todorovski", "S. Dzeroski"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Todorovski and Dzeroski,? \\Q1997\\E", "shortCiteRegEx": "Todorovski and Dzeroski", "year": 1997}, {"title": "Spline models for observational data", "author": ["G. Wahba"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "Wahba,? \\Q1990\\E", "shortCiteRegEx": "Wahba", "year": 1990}, {"title": "Discovering admissible model equations from observed data based on scale-types and identity constraints", "author": ["T. Washio", "H. Motoda", "Y Niwa"], "venue": "In International Joint Conference On Artifical Intelligence,", "citeRegEx": "Washio et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Washio et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Examples include learning hyperparameters (Rasmussen & Williams, 2006), linear combination weights (Bach, 2009), and mappings from the input space to an embedding space (Salakhutdinov & Hinton, 2008).", "startOffset": 99, "endOffset": 111}, {"referenceID": 6, "context": "Borrowing discrete search techniques which have proved successful in equation discovery (Todorovski & Dzeroski, 1997) and unsupervised learning (Grosse et al., 2012), we automatically search over this space of kernel structures using marginal likelihood as the search criterion.", "startOffset": 144, "endOffset": 165}, {"referenceID": 13, "context": "Many architectures for learning complex functions, such as convolutional networks (LeCun et al., 1989) and sum-product networks (Poon & Domingos, 2011), include units which compute AND-like and OR-like operations.", "startOffset": 82, "endOffset": 102}, {"referenceID": 21, "context": "To avoid an expensive integration over kernel parameters, we used the Bayesian information criterion (Schwarz, 1978) as an approximation.", "startOffset": 101, "endOffset": 116}, {"referenceID": 5, "context": "Additive Gaussian processes (Duvenaud et al., 2011) are a gp model whose kernel implicitly sums over all possible products of onedimensional base kernels.", "startOffset": 28, "endOffset": 51}, {"referenceID": 5, "context": "Additive Gaussian processes (Duvenaud et al., 2011) are a gp model whose kernel implicitly sums over all possible products of onedimensional base kernels. Plate (1999) constructs a gp with a composite kernel, summing an SE kernel along each dimension, with an SE-ARD kernel (i.", "startOffset": 29, "endOffset": 168}, {"referenceID": 23, "context": "A closely related procedure is smoothing-splines ANOVA (Wahba, 1990; Gu, 2002).", "startOffset": 55, "endOffset": 78}, {"referenceID": 7, "context": "A closely related procedure is smoothing-splines ANOVA (Wahba, 1990; Gu, 2002).", "startOffset": 55, "endOffset": 78}, {"referenceID": 0, "context": "Kernel learning There is a large body of work attempting to construct a rich kernel through a weighted sum of base kernels (e.g. Christoudias et al., 2009; Bach, 2009).", "startOffset": 123, "endOffset": 167}, {"referenceID": 11, "context": "Lawrence (2005) learns an embedding of the data points into a low-dimensional space, and constructs a fixed kernel structure over that space.", "startOffset": 0, "endOffset": 16}, {"referenceID": 11, "context": "Lawrence (2005) learns an embedding of the data points into a low-dimensional space, and constructs a fixed kernel structure over that space. This model is typically used in unsupervised tasks and requires an expensive integration or optimisation over potential embeddings when generaliing to new data points. Salakhutdinov & Hinton (2008) use a deep neural network to learn an embedding; this is a flexible approach to kernel learning but potentially less interpretable.", "startOffset": 0, "endOffset": 340}, {"referenceID": 1, "context": "(2007) and Bing et al. (2010) learn composite kernels for support vector machines and relevance vector machines, using genetic search algorithms.", "startOffset": 11, "endOffset": 30}, {"referenceID": 24, "context": "For example, (Schmidt & Lipson, 2009), (Todorovski & Dzeroski, 1997) and (Washio et al., 1999) attempt to learn parametric forms of equations to describe time series, or relations between quantities.", "startOffset": 73, "endOffset": 94}, {"referenceID": 2, "context": "Airline passenger data Figure 6 shows the decomposition produced by applying our method to monthly totals of international airline passengers (Box et al., 1976).", "startOffset": 142, "endOffset": 160}, {"referenceID": 12, "context": "Solar irradiance Data Finally, we analyzed annual solar irradiation data from 1610 to 2011 (Lean et al., 1995).", "startOffset": 91, "endOffset": 110}, {"referenceID": 5, "context": "To evaluate the predictive accuracy of our method in a high-dimensional setting, we extended the comparison of (Duvenaud et al., 2011) to include our method.", "startOffset": 111, "endOffset": 134}], "year": 2017, "abstractText": "Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.", "creator": "LaTeX with hyperref package"}}}