{"id": "1302.6677", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization", "abstract": "Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.", "histories": [["v1", "Wed, 27 Feb 2013 06:45:28 GMT  (257kb,D)", "http://arxiv.org/abs/1302.6677v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["stefano ermon", "carla p gomes", "ashish sabharwal", "bart selman"], "accepted": true, "id": "1302.6677"}, "pdf": {"name": "1302.6677.pdf", "metadata": {"source": "CRF", "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization", "authors": ["Stefano Ermon", "Carla P. Gomes", "Ashish Sabharwal"], "emails": ["ermonste@cs.cornell.edu", "gomes@cs.cornell.edu", "ashish.sabharwal@us.ibm.com", "selman@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "The question of whether this is a \"real\" problem, which is in the manner in which it is a \"pure\" problem, is not new: The question is whether it is a \"real\" problem, but whether it is a \"pure,\" \"pure,\" \"pure,\" \"pure,\" \"pure,\" \"pure,\" \"pure,\" \"\" \"pure,\" \"\" \"pure,\" \"\" \"\" pure, \"\" \"pure,\" \"\" \"pure,\" \"\" pure, \"\" \"\" pure, \"\" \"\" pure, \"\" \"\" pure, \"\" \"\" \"pure,\" \"\" \"\" \"\" \"pure,\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "2 Problem Statement and Assumptions", "text": "Let us calculate the total weight of the set (approximately), defined as the following discrete integral or \"partitioning function\" W = distribution-related probability table. Note, however, that our results are of a more general nature and not based on factored representation. Assumption: We assume to have access to an optimization oracle that can solve the following limited optimization problem: w (\u03c3) 1 {C} (\u03c3) (2), where 1 {C}: \u03a3 \u2192 {0, 1} is an indicator function for a compactly represented subset C \u0440, i.e. 1 {C} (\u03c3) 1 {C} (\u03c3) (2), specifically discussing our probability models that present our applicability in the context of motivating assumptions."}, {"heading": "2.1 Inference in Graphical Models", "text": "We consider a graphical model specified as a factor diagram with N = | V | discrete random variables xi, i \u0445 V, where xi-Xi. The global random vector x = {xs, s-V} assumes a value in the cartesian product X = X1 \u00b7 X2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 XN. We consider a probability distribution via x-X (so-called configurations) p (x) = 1Z-\u03b1-I-\u03b1 ({x} \u03b1) that factors are included in potentials or factors. In the face of a graphical model, we leave \u03a3 = X the set of all possible configurations (variable assignments) and {x-\u03b1-V a subset of variables on which the factor-\u03b1 depends, and Z is a normalization constant known as a partition problem."}, {"heading": "2.2 Quadratures of Integrals", "text": "Suppose we get squared for a continuous (multidimensional) integral of a function f: Rn \u2192 R + via a high-dimensional set S'Rn-S f (x) dx-x-x-x w (x) = Win X is a discretization of S (e.g. grid-based), and w (x) approaches the integral of f (x) via the corresponding volume element. In this case, we need a compact representation for w and access to an oracle that is able to optimize the discarded function, subject to arbitrary constraints. See, for example, Figure 1. For simplicity, we will limit ourselves to the binary case below, i.e. we will limit ourselves to the general multinomic case in which the sum can be transformed via X1 \u00d7 X2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 XN into the previous case by using a binary representation, i.e., 2, 1} (Bits)."}, {"heading": "3 Preliminaries", "text": "(1) A reader already familiar with these results may limit himself to the next section. (2) A family of functions H = {0, 1} m, however, is pairs independent if the following two conditions.2) The following two conditions.2) The following two conditions.2) We are both independent of each other if the following two conditions.2) The random variables H (x1) and H (x2) are independent to construct such a function.2) We think of the family H of all possible functions {0, 1} n \u2192 n. This is a family of functions that are not only clever but completely independent."}, {"heading": "5 Analysis", "text": "Since many configurations may have identical weight, it will be helpful for the purposes of analysis, e.g. to fix a weight-based arrangement of configurations and a natural division of configurations into n + 1 containers provoked by the order.Definition 2. Repair an order \u03c3i, 1 \u2264 i \u2264 2n of configurations in such a way that for 1 \u2264 j < 2n, w (\u03c3j) \u2265 w (\u03c3j + 1). For i, 1, \u00b7 \u00b7 \u00b7, n} define bi, w (\u03c32i). Define a special container B, {\u03c31} and for i, 0, 1 \u00b7 \u00b7 \u00b7, n \u2212 1} define bin Bi, {\u03c32i + 1, \u03c32i + 2, \u00b7 \u00b7 \u00b7 \u00b7, \u03c32i + 1}. Note that bin Bi has exactly 2 i configurations. Furthermore, for all these Bi configurations, the definition of 1 and 1 is derived."}, {"heading": "5.1 Estimating the Total Weight", "text": "Our main theory is that algorithm 1 provides a constant factor approximation to the partition function = = Let's let i = Let's let i = Let's let i = Let's let i = Let's let i = Let's let's let 1 = Let's let's let's let's have a 16-fold approximation to W = + Let's define 1 as algorithm 1 and bi as in definition 2. The proof is based on two intermediate results, the proof of which is in the appendix. Lemma 1. Let's define Mi = median (w 1 i, \u00b7, wTi) as in algorithm 1 and bi as in definition 2. Then, for all c 2, there is an approximation (c) > 0 of such values that for 0 < we have one that for 0 < L = Let's have an equation (c), Pr [bmin {i + c, n}, bmax {i \u2212 c, 0}]."}, {"heading": "5.2 Estimating the Tail Distribution", "text": "Theorem 2. Let Mi be defined as in algorithm 1, u, R + and q (u) so that the maximum i is that it can be queried from 0, \u00b7 \u00b7 \u00b7, i}, Mj \u2265 u. Then 2q (u) for each \u03b4 > 0 with the probability \u2265 (1 \u2212 \u03b4) is an 8 approximation to G (u) calculated using O (n lnn ln 1 / 3) MAP. Although this in itself is an interesting result, the scheme in section 5.1, which requires a total of only 1 / 3 MAP queries, is more efficient than the first estimate of the tail distribution for several values of and more."}, {"heading": "5.3 Improving the Approximation Factor", "text": "If you adopt the following construction, you can design an (1 +) approximation algorithm like algorithm 1 and each > 0: Let \"= log1 +. Define a new set of configurations \u03a3\" = \u03a3 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u03a3, and a new weight function w \": \u03a3\" \u2192 R as w \"(\u03c31, \u00b7 \u00b7, \u03c3\") = w (\u03c31) \u00b7 \u00b7 \u00b7 w (\u03c3 \"). Proposition 2. Let W be an approximation of what we call\" w. \"Then W\" 1 / \"is an approximation of what we are.\" To understand why this is, note that W \"=\" w \"(cycle) = (cycle) = (cycle)\" = (cycle). \"Since W\" \u2264 W. \""}, {"heading": "5.4 Further Approximations", "text": "Theorem 3. Let us find suboptimal solutions to the optimization problems in algorithm 1, i.e., W-ti \u2264 wti. Let us be the output of algorithm 1 with these suboptimal solutions. Then, W-ti is for each \u03b4 > 0, with a probability of at least 1 \u2212 \u03b4, W-16 \u2264 W. Further, if w-ti \u2265 1Lw t i for some L > 0, then with a probability of at least 1 \u2212 \u043c, W-xi is a 16L approximation to W. The output is always an approximate lower limit, even if the optimization is stopped early. The lower limit is not decreasing monotonously over time and is guaranteed sometime within a constant factor of W. So we have an algorithm valid at all times."}, {"heading": "6 Experimental Evaluation", "text": "We implemented WISH with the open source solver ToulBar2 [1] to solve the MAP inference problem. ToulBar2 is a complete solver (i.e., if sufficient time is available, it will find an optimal solution and provide an optimality certificate), and it was one of the winning algorithms in the UAI-2010 inference competition. We enhanced ToulBar2 with the IBM ILOG CPLEX CP Optimizer 12.3 based techniques borrowed from Gomes et al. [13] to efficiently handle the random parity constraints. Specifically, the equation sets Ax = b mod 2 are linear equations across the field F (2), thus enabling efficient propagation and domain filtering using Gaussian elimination. For our experiments, we run WISH in parallel with a compute cluster with 642 cores. We refer to each optimization instances when an inner loop or an optimization input the results into an inner loop has been achieved."}, {"heading": "6.1 Provably Accurate Approximations", "text": "For our first experiment, we consider the problem of calculating the partition function Z (cf. Eqn. (3)) of random clique-structured issuing models on n binary variables xi-wij (0, 1) for i-wij (1, \u00b7 \u00b7, n). The interaction between xi and xj is defined as error (xi, xj) = exp (\u2212 wij) if xi 6 = xj, and 1 otherwise if wij is uniformly sampled from [0, w | i \u2212 j |] and w and a parameter is set to 0.2. We also inject a structure by introducing a closed chain of strong repellent interactions uniformly sampled from [\u2212 10w, 0]. We consider models with n between 10 and 60. These models have a tree width n and can be solved accurately (by raw force), with isolation only up to about n = 25 variable.4 (a shows the problem with the different methods used for the results)."}, {"heading": "6.2 Anytime Usage with Suboptimal Solutions", "text": "Next, we examine the quality of our results if not all optimization cases can be optimally solved due to timeouts, so that the strong theoretical guarantees of Theorem 1 do not apply (although Theorem 3 still applies).We look at 10 \u00d7 10 binary grid-issuing models, for which the truth of the soil can be calculated using the junction tree method [32].We use the same experimental setup as Hazan and Jaakkola [14], which also use random MAP queries to derive boundaries (without density guarantee) for the partition function. Specifically, we have n = 100 binary variables xi, 1} with interaction, xi, xj) = exp (wijxixj).In the attractive case, we draw wij from [0, w]; in the mixed case, from [\u2212 w, w]."}, {"heading": "6.3 Hard Combinatorial Structures", "text": "An interesting and combinatorically challenging graphical model results from sudoku, which is a popular number placement problem where the goal is to fill a 9 \u00b7 9 grid (see Figure 4 \u00b7 b) with numbers from {1, \u00b7 \u00b7 \u00b7 \u00b7, 9}, so that the entries in each line, column, and 3 \u00b7 3 block that composes the grid are all different; the puzzle can be encoded as a graphical model with 81 discrete variables with domain {1, \u00b7 \u00b7 \u00b7, 9}, with potentials of \u03b1 ({x} \u03b1) = 1, if and only if all variables in {x} \u03b1 are different, where I is an index set containing the subsets of variables in each row, column, and block; this defines a uniform probability distribution over all valid complete sudoku grids (an invalid complete grid has a zero probability), and the normalized number corresponds to the total number of valid tzes."}, {"heading": "6.4 Model Selection", "text": "Many conclusions and learning tasks require the calculation of the normalization constant of graphical models. Although, for example, it is necessary to evaluate the probability of the observed data for a given model, this is necessary for model selection, i.e. to classify candidate models or to trigger an early stop during training when the probability of a validation set begins to decrease to avoid a match [6]. We train the Boltzmann Restricted Machinery (RBM) [15] using contrasting divergence (CD) [5, 33] on handwritten digits dataset. In an RBM there is a layer of nh hidden binary variables h = h1, \u00b7 hnh and a layer of nv binary visible units v = v1, \u00b7 \u00b7 \u00b7, vnv. The common probability distribution is given by P (h, v) = 1Z exp (b \u2032 v \u2032 v \u2032 h \u2032 Wv)."}, {"heading": "7 Conclusion", "text": "We have introduced WISH, a randomized algorithm that most likely represents a constant factor approximation of a general discrete integral defined by an exponentially large amount. WISH reduces the insoluble counting problem to a small number of cases of a combinatorial optimization problem that is subject to parity limitations used as a hash function. Within the framework of graphical models, we demonstrated how to approximate the normalization constant or partition function by using a small number of MAP queries. Finally, using state-of-the-art combinatorial optimization tools, we are able to offer discrete integral or partition function estimates with approximation guarantees on a scale that could previously only be handled heuristically. Finally, our method is a massively parallelizable and always applicable algorithm that can also be stopped early to obtain empirically accurate estimates that offer lower probabilities."}, {"heading": "Acknowledgments", "text": "Supported by NSF Expeditions in Computing grant on Computational Sustainability # 0832782 and NSF Computing Research Infrastructure grant # 1059284."}, {"heading": "A Appendix: Proofs", "text": "The proof of Proposition 1 follows directly from Lemma 3.Lemma 3 (pairs independent of each other) = 1 (pairs independent of each other) = 1 (pairs independent of each other) = 1 (pairs independent of each other) = 1 (1) = 1 (1) = 1 (1) = 1 (1) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 2) = 1 (2) = 2) = 2 (2) = 2 (2) = 2 (2) = 2 (2) = 2 (2) = 1 (2) = 2) = 1 (2) = 2) = 2)."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>Integration is affected by the curse of dimensionality and quickly becomes intractable as<lb>the dimensionality of the problem grows. We propose a randomized algorithm that, with high<lb>probability, gives a constant-factor approximation of a general discrete integral defined over an<lb>exponentially large set. This algorithm relies on solving only a small number of instances of a<lb>discrete combinatorial optimization problem subject to randomly generated parity constraints<lb>used as a hash function. As an application, we demonstrate that with a small number of MAP<lb>queries we can efficiently approximate the partition function of discrete graphical models, which<lb>can in turn be used, for instance, for marginal computation or model selection.", "creator": "LaTeX with hyperref package"}}}