{"id": "1303.0339", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2013", "title": "Learning Hash Functions Using Column Generation", "abstract": "Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning data-dependent hash functions have been developed. In this work, we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.", "histories": [["v1", "Sat, 2 Mar 2013 03:01:46 GMT  (1602kb,D)", "http://arxiv.org/abs/1303.0339v1", "9 pages, published in International Conf. Machine Learning, 2013"]], "COMMENTS": "9 pages, published in International Conf. Machine Learning, 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xi li", "guosheng lin", "chunhua shen", "anton van den hengel", "anthony r dick"], "accepted": true, "id": "1303.0339"}, "pdf": {"name": "1303.0339.pdf", "metadata": {"source": "META", "title": "Learning Hash Functions Using Column Generation", "authors": ["Xi Li", "Guosheng Lin", "Chunhua Shen", "Anton van den Hengel"], "emails": ["xi.li03@adelaide.edu.au", "guosheng.lin@adelaide.edu.au", "chunhua.shen@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au", "anthony.dick@adelaide.edu.au"], "sections": [{"heading": null, "text": "In this paper, we propose a method based on column generation for learning data-pendent hash functions based on proximity comparison information. Considering a set of triplets that encode the paired proximity comparison information, our method learns hash functions that maintain the relative comparison relationships in the data as best as possible within the extensive learning framework. The learning method is implemented by column generation and is therefore called CGHash. Each iteration of the column generation method selects the best hash function. Unlike most other hash methods, our method naturally generalizes new data points and has a training goal that is convex, ensuring that the global optimum can be identified. Experiments show that the proposed method learns compact binary codes and that its retrieval performance is comparable to the state of the art."}, {"heading": "1. Introduction", "text": "In fact, it is the case that most of them are able to survive themselves by taking on the role of the loner. (...) In fact, it is the case that they are able to put themselves in the role of the loner. (...) It is not as if they are taking on the role of the loner. (...) It is not as if they are taking on the role of the loner. (...) It is as if they are taking on the role of the loner. (...) It is as if they are taking on the role of the loner. (...) It is as if they are taking on the role of the loner. (...) It is as if they are taking on the role of the loner. (...) It is as if they are taking on the role of the loner."}, {"heading": "2. The proposed algorithm", "text": "Considering a series of training samples xm, x \u2212 xi, (m = 1, 2,.), we aim to learn a series of hash functions hj (x) xi, j = 1, 2,.. to map these training samples to a low-dimensional binary space, which is described by a set of binary code words bm (m = 1, 2,..). Here, each bm is a \"-dimensional binary vector.\" In low-dimensional binary binary space, the code words bm's are to obtain the underlying proximity information of the underlying xi's in original high-dimensional space. Next, we learn such hash functions {hj (x \u2212 xi) \"j = 1 within the large-scale learning framework. Formally, we assume that we obtain a set of triplets i, which we compare with x (xi, x \u2212 i)."}, {"heading": "2.1. Learning hashing functions with the hinge loss", "text": "Hashing with l1 norm regularization using the hinge loss, we define the following large-margin optimization problem: min w, long-margin optimization problem: min w, long-q | i = 1 + 0, 0,. < 0; dH (xi, x-i) \u2212 dH (xi, x + i) \u2265 1 \u2212 i, (2), where 0-0 is the 1 norm, w = (w1, w2,., w ') > is the weight vector, is the slip variable; C is a parameter that controls the trade-off between the training error and the model capacity, and the symbol \"<\" indicates elementary inequalities. The optimization problem (2) can be rewritten as: min w, long-up-up-up."}, {"heading": "2.2. Hashing with a general convex loss function", "text": "We assume that the general convex loss function f () is smooth (exponential, logistical, quared hinge loss, etc.), although our algorithm can easily be extended to non-smooth loss functions. We assume that we want to find a number of hash functions so that the number of constraints dH (xi, x \u2212 i) \u2212 dH (xi, x + i) = w > ai > 0, i = 1, 2. Let's keep as good as possible. These constraints do not all need to be strictly met. Now, we need to define the margin deur i = w > ai, and we want to maximize the margin with regulation. With l1norm regulation, we control the capacity, we can define the primary optimization problem as: min w."}, {"heading": "3. Experimental results", "text": "This year it has come to the point that it will be able to retaliate, \"he said.\" We have to be able, \"he said.\" We have to be able to hide, \"he said.\" We have to be able, \"he said.\" We have to be able to retaliate, \"he said."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "In Proc. IEEE Symp. Foundations of Computer Science,", "citeRegEx": "Andoni and Indyk,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk", "year": 2006}, {"title": "Learning to hash: forgiving hash functions and applications", "author": ["S. Baluja", "M. Covell"], "venue": "Data Mining & Knowledge Discovery,", "citeRegEx": "Baluja and Covell,? \\Q2008\\E", "shortCiteRegEx": "Baluja and Covell", "year": 2008}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe", "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "J. American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Linear programming boosting via column generation", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": "Machine Learning,", "citeRegEx": "Demiriz et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Demiriz et al\\.", "year": 2002}, {"title": "Iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Trans. Pattern Analysis & Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Coherency sensitive hashing", "author": ["S. Korman", "S. Avidan"], "venue": "In Proc. Int. Conf. Computer Vision,", "citeRegEx": "Korman and Avidan,? \\Q2011\\E", "shortCiteRegEx": "Korman and Avidan", "year": 2011}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "In Proc. Adv. Neural Information Process. Systems,", "citeRegEx": "Kulis and Darrell,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell", "year": 2009}, {"title": "Kernelized localitysensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "In Proc. Int. Conf. Computer Vision, pp", "citeRegEx": "Kulis and Grauman,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Grauman", "year": 2009}, {"title": "Supplementary document: Effectively learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A. van den Hengel", "A. Dick"], "venue": "paper.html,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.F. Chang"], "venue": "In Proc. Int. Conf. Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.G. Jiang", "S.F. Chang"], "venue": "In Proc. IEEE Conf. Computer Vision & Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "In Proc. Int. Conf. Machine Learning,", "citeRegEx": "Norouzi and Fleet,? \\Q2011\\E", "shortCiteRegEx": "Norouzi and Fleet", "year": 2011}, {"title": "Learning a distance metric from relative comparisons", "author": ["M. Schultz", "T. Joachims"], "venue": "In Proc. Adv. Neural Information Processing Systems,", "citeRegEx": "Schultz and Joachims,? \\Q2004\\E", "shortCiteRegEx": "Schultz and Joachims", "year": 2004}, {"title": "Fast pose estimation with parameter-sensitive hashing", "author": ["G. Shakhnarovich", "P. Viola", "T. Darrell"], "venue": "In Proc. Int. Conf. Computer Vision, pp", "citeRegEx": "Shakhnarovich et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Shakhnarovich et al\\.", "year": 2003}, {"title": "Positive semidefinite metric learning using boostinglike algorithms", "author": ["C. Shen", "J. Kim", "L. Wang", "A. van den Hengel"], "venue": "J. Machine Learning Research,", "citeRegEx": "Shen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2012}, {"title": "Ldahash: Improved matching with smaller descriptors", "author": ["C. Strecha", "A.M. Bronstein", "M.M. Bronstein", "P. Fua"], "venue": "IEEE Trans. Pattern Analysis & Machine Intelligence,", "citeRegEx": "Strecha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Strecha et al\\.", "year": 2011}, {"title": "Small codes and large image databases for recognition", "author": ["A. Torralba", "R. Fergus", "Y. Weiss"], "venue": "In Proc. IEEE Conf. Computer Vision & Pattern Recognition,", "citeRegEx": "Torralba et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2008}, {"title": "Semisupervised hashing for large scale search", "author": ["J. Wang", "S. Kumar", "S.F. Chang"], "venue": "IEEE Trans. Pattern Analysis & Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "In Proc. Adv. Neural Information Processing Systems,", "citeRegEx": "Weinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2006}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In Proc. Adv. Neural Information Process. Systems,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}, {"title": "Laplacian co-hashing of terms and documents", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. Eur. Conf. Information Retrieval,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "In Proc. ACM SIGIR Conf., pp", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Zhu et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 20, "context": "Considerable effort has been spent on designing hashing methods which address both the issues of fast similarity search and efficient data storage (for example, (Andoni & Indyk, 2006; Weiss et al., 2008; Zhang et al., 2010b; Norouzi & Fleet, 2011; Kulis & Darrell, 2009; Gong et al., 2012)).", "startOffset": 161, "endOffset": 289}, {"referenceID": 5, "context": "Considerable effort has been spent on designing hashing methods which address both the issues of fast similarity search and efficient data storage (for example, (Andoni & Indyk, 2006; Weiss et al., 2008; Zhang et al., 2010b; Norouzi & Fleet, 2011; Kulis & Darrell, 2009; Gong et al., 2012)).", "startOffset": 161, "endOffset": 289}, {"referenceID": 17, "context": "Hash-based approaches have thus found a wide range of applications, including object recognition (Torralba et al., 2008), information retrieval (Zhang et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 16, "context": ", 2010b), local descriptor compression (Strecha et al., 2011), image matching (Korman & Avidan, 2011), and many more.", "startOffset": 39, "endOffset": 61}, {"referenceID": 14, "context": "Recently a number of effective hashing methods have been developed which construct a variety of hash functions, mainly on the assumption that semantically similar data samples should have similar binary codes, such as random projection-based locality sensitive hashing (LSH) (Andoni & Indyk, 2006), boosting learning-based similarity sensitive coding (SSC) (Shakhnarovich et al., 2003), and spectral hashing of Weiss et al.", "startOffset": 357, "endOffset": 385}, {"referenceID": 5, "context": ", 2010b; Norouzi & Fleet, 2011; Kulis & Darrell, 2009; Gong et al., 2012)). A hashing-based approach constructs a set of hash functions that map highdimensional data samples to low-dimensional binary codes. These binary codes can be easily loaded into the memory in order to allow rapid retrieval of data samples. Moreover, the pairwise Hamming distance between these binary codes can be efficiently computed by using bit operations, which are well supported by modern processors, thus enabling efficient similarity calculation on large-scale datasets. Hash-based approaches have thus found a wide range of applications, including object recognition (Torralba et al., 2008), information retrieval (Zhang et al., 2010b), local descriptor compression (Strecha et al., 2011), image matching (Korman & Avidan, 2011), and many more. Recently a number of effective hashing methods have been developed which construct a variety of hash functions, mainly on the assumption that semantically similar data samples should have similar binary codes, such as random projection-based locality sensitive hashing (LSH) (Andoni & Indyk, 2006), boosting learning-based similarity sensitive coding (SSC) (Shakhnarovich et al., 2003), and spectral hashing of Weiss et al. (2008) which is inspired by Laplacian eigenmap.", "startOffset": 55, "endOffset": 1259}, {"referenceID": 20, "context": "In more detail, spectral hashing (Weiss et al., 2008) optimizes a graph Laplacian based objective function such that in the learned low-dimensional binary ar X iv :1 30 3.", "startOffset": 33, "endOffset": 53}, {"referenceID": 14, "context": "SSC (Shakhnarovich et al., 2003) makes use of boosting to adaptively learn an embedding of the original space, represented by a set of weak learners or hash functions.", "startOffset": 4, "endOffset": 32}, {"referenceID": 15, "context": "This type of relative proximity comparisons have been successfully applied to learn quadratic distance metrics (Schultz & Joachims, 2004; Shen et al., 2012).", "startOffset": 111, "endOffset": 156}, {"referenceID": 20, "context": ", spectral hashing (Weiss et al., 2008), self-taught hashing (Zhang et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 20, "context": ", spectral hashing (Weiss et al., 2008), self-taught hashing (Zhang et al., 2010b)). The spectral hashing (SPH) method directly optimizes a graph Laplacian objective function in the Hamming space. Inspired by SPH, Zhang et al. (2010b) developed the self-taught hashing (STH)", "startOffset": 20, "endOffset": 235}, {"referenceID": 10, "context": "Liu et al. (2011) proposed a scalable graph-based hashing method which uses a small-size anchor graph to approximate the original neighborhood graph and alleviates the computational limitation of spectral hashing.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "Boosting methods have also been employed to develop hashing methods such as SSC (Shakhnarovich et al., 2003) and Forgiving Hash (Baluja & Covell, 2008), both of which learn a set of weak learners as hash functions in the boosting framework.", "startOffset": 80, "endOffset": 108}, {"referenceID": 17, "context": "It is demonstrated in (Torralba et al., 2008) that some datadependent hashing methods like stacked RBM and boosting SSC perform much better than LSH on largescale databases of millions of images.", "startOffset": 22, "endOffset": 45}, {"referenceID": 13, "context": "Strecha et al. (2011) use Fisher linear discriminant analysis (LDA) to embed the original data samples into a lower-dimensional space, where the embedded data samples are binarized using thresholding.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Boosting methods have also been employed to develop hashing methods such as SSC (Shakhnarovich et al., 2003) and Forgiving Hash (Baluja & Covell, 2008), both of which learn a set of weak learners as hash functions in the boosting framework. It is demonstrated in (Torralba et al., 2008) that some datadependent hashing methods like stacked RBM and boosting SSC perform much better than LSH on largescale databases of millions of images. Wang et al. (2012) proposed a semi-supervised hashing method, which aims to ensure the smoothness of similar data samples and the separability of dissimilar data samples.", "startOffset": 81, "endOffset": 456}, {"referenceID": 10, "context": "More recently, Liu et al. (2012) introduced a kernel-based supervised hashing method, where the hashing functions are nonlinear kernel functions.", "startOffset": 15, "endOffset": 33}, {"referenceID": 14, "context": "The closest work to ours might be boosting based SSC hashing (Shakhnarovich et al., 2003), which also learns a set of weighted hash functions through boosting learning.", "startOffset": 61, "endOffset": 89}, {"referenceID": 9, "context": "More details of our hashing algorithm can be found in Algorithm 1 and the supplementary file (Li et al., 2013).", "startOffset": 93, "endOffset": 110}, {"referenceID": 23, "context": "Solve the primal problem for w (using LBFGS-B (Zhu et al., 1997)) and obtain the dual variable u using KKT condition (10).", "startOffset": 46, "endOffset": 64}, {"referenceID": 4, "context": "Demiriz et al. (2002) used this method to design boosting algorithms.", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "The above optimization problem can be efficiently solved by using LBFGS (Zhu et al., 1997) after feature normalization.", "startOffset": 72, "endOffset": 90}, {"referenceID": 23, "context": "Note that both the primal problems (7) and (15) can be efficiently solved using quasi-Newton methods such as L-BFGS-B (Zhu et al., 1997) by eliminating the auxiliary variable \u03c1.", "startOffset": 118, "endOffset": 136}, {"referenceID": 19, "context": "Moreover, the triplets used for learning hash functions are generated in the same way as (Weinberger et al., 2006).", "startOffset": 89, "endOffset": 114}, {"referenceID": 17, "context": "For simplicity, they are respectively referred to as LSH (Locality Sensitive Hashing (Andoni & Indyk, 2006)), SSC (Supervised Similarity Sensitive Coding (Torralba et al., 2008) as a modified version of (Shakhnarovich et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 14, "context": ", 2008) as a modified version of (Shakhnarovich et al., 2003)), LSI (Latent Semantic Indexing (Deerwester et al.", "startOffset": 33, "endOffset": 61}, {"referenceID": 3, "context": ", 2003)), LSI (Latent Semantic Indexing (Deerwester et al., 1990)), LCH (Laplacian Co-Hashing (Zhang et al.", "startOffset": 40, "endOffset": 65}, {"referenceID": 20, "context": ", 2010a)), SPH (Spectral Hashing (Weiss et al., 2008)), STH", "startOffset": 33, "endOffset": 53}, {"referenceID": 10, "context": ", 2010b)), AGH (Anchor Graph Hashing (Liu et al., 2011)), BREs (Supervised Binary Reconstructive Embedding (Kulis & Darrell, 2009)), SPLH (Semi-Supervised Learning Hashing (Wang et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 18, "context": ", 2011)), BREs (Supervised Binary Reconstructive Embedding (Kulis & Darrell, 2009)), SPLH (Semi-Supervised Learning Hashing (Wang et al., 2012)), and ITQ (Iterative Quantization (Gong et al.", "startOffset": 124, "endOffset": 143}, {"referenceID": 5, "context": ", 2012)), and ITQ (Iterative Quantization (Gong et al., 2012)).", "startOffset": 42, "endOffset": 61}], "year": 2013, "abstractText": "Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning datadependent hash functions have been developed. In this work, we propose a column generation based method for learning datadependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets. * indicates equal contributions. Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).", "creator": "LaTeX with hyperref package"}}}