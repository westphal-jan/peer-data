{"id": "1303.0551", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2013", "title": "Sparse PCA through Low-rank Approximations", "abstract": "We introduce a novel algorithm that computes the $k$-sparse principal component of a positive semidefinite matrix $A$. Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of $A$. We obtain provable approximation guarantees that depend on the spectral profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation. For example, if the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-time approximation algorithm for any desired accuracy. We implement our algorithm and test it on multiple artificial and real data sets. Due to a feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets.", "histories": [["v1", "Sun, 3 Mar 2013 19:08:55 GMT  (1269kb,D)", "https://arxiv.org/abs/1303.0551v1", "23 pages, 6 figures, 4 tables, submitted for publication"], ["v2", "Thu, 8 May 2014 00:30:12 GMT  (1262kb,D)", "http://arxiv.org/abs/1303.0551v2", "Long version of the ICML 2013 paper:this http URL"]], "COMMENTS": "23 pages, 6 figures, 4 tables, submitted for publication", "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["dimitris s papailiopoulos", "alexandros g dimakis", "stavros korokythakis"], "accepted": true, "id": "1303.0551"}, "pdf": {"name": "1303.0551.pdf", "metadata": {"source": "CRF", "title": "Sparse PCA through Low-rank Approximations", "authors": ["Dimitris S. Papailiopoulos", "Alexandros G. Dimakis", "Stavros Korokythakis"], "emails": ["dimitris@utexas.edu", "dimakis@austin.utexas.edu", "stavros@stochastictechnologies.com"], "sections": [{"heading": null, "text": "An important algorithmic component of our scheme is a combinatorial feature elimination step that is proven to be safe and in practice significantly reduces the ongoing complexity of our algorithm. We implement our algorithm and test it on multiple artificial and real data sets. By taking the step of eliminating features, it is possible to perform sparse PCAs on data sets consisting of millions of entries in minutes. Our experimental evaluation shows that our scheme is nearly optimal while we find very sparse vectors. We compare it with the state of the art and show that our scheme matches or exceeds previous algorithms in all data sets tested."}, {"heading": "1 Introduction", "text": "The statistical significance of PCA is partly due to the fact that the main components capture the widest possible variance of the data. PCA is the first major component (i.e. the first eigenvector) of an n \u00d7 n matrix A. PCA can be efficiently calculated using the singular value substitution (SVD), where A = SST and S is the n x m dataset matrix consisting of m datasets or inputs, each evaluated on n characteristics. PCA's statistical properties and computational properties make it one of the most widely used tools in data analysis and cluster applications.A disadvantage of PCA is that the generated parsimony factors are generally not major components."}, {"heading": "1.1 Sparse PCA", "text": "(1) The limitation of cardinality \"0\" limits optimization compared to vectors with k-unequal entries. As expected, frugality comes at a price, since optimization in (1) NP-hard (Moghaddam et al., 2006a) and is therefore generally mathematically insoluble."}, {"heading": "1.2 Overview of main results", "text": "We introduce a novel algorithm for sparse PCA, which has a verifiable approximation warranty. Our algorithm generates a k-sparse unit length vector xd, which verifiably yields an objective within a 1 \u2212 d factor of the optimum: xTdAxd \u2265 (1 \u2212 d) xT \u0445 Ax \u0445 withd \u2264 min {nk \u00b7 \u03bbd + 1 \u03bb1, \u03bbd + 1\u03bb (1) 1}, (2) where \u03bbi is the largest eigenvalue of A and \u03bb (1) 1 is the maximum diagonal element of A. For each desired value of parameter d, our algorithm runs in the time O (nd + 1 logn + SVD (A, d)), where SVD (A, d) is the time to calculate the d most important eigenvectors of A. Our approximation warranty is directly related to the spectrum of A: the greater the eigenvalue decay matt, the better the approximation can be determined from the general equation (2), each of which contains a eigenvalue, and one of the two boundaries of A."}, {"heading": "1.2.1 Constant-factor approximation", "text": "If we only assume that there is an arbitrary decay of the eigenvalues of A, i.e. that there is a constant d = O (1), so that \u03bb1 > \u03bbd + 1, then we can obtain a constant factor approximation guarantee for the linear sparsity regime. Specifically, we find a constant \u03b40, so that for all sparsity levels k > \u03b40 n we obtain a constant approximation ratio for sparse PCA, which partially solves the open problem discussed in (Zhang et al., 2012; d'Aspremont et al., 2012). This result is easily derived from our main theorem."}, {"heading": "1.2.2 PTAS under a power-law decay", "text": "If the spectrum of the data matrix exhibits a performance decay, we can obtain a much stronger performance guarantee: we can solve the sparse PCA for any desired accuracy of the time polynomial in n, k (but not in 1), sometimes referred to as the polynomial approximation scheme (PTAS). Moreover, performance decay is not necessary: the spectrum does not have to follow exactly this decay, but exhibits a significant spectral decline only after some eigenvalues."}, {"heading": "1.2.3 Algorithmic details", "text": "Our algorithm works by scanning a low-dimensional subspace of A. It first calculates the leading eigenvectors of the covariance input matrix and then scans this subspace for k sparse vectors with a large explained variance. Constant dimensional search is possible for a hyperspherical transformation of the n-dimensional problem space into one with a constant d-dimension.This framework was introduced by the basic work of (Karystinos and Liavas, 2010) in connection with the solution of quadratic form maximization problems over \u00b1 1 vectors.This framework was logically used in (Asteris et al., 2011) to develop a constant rangefinder that calculates the sparse major component of a constant rank matrix in polynomic time O (nd + 1).As a subroutine, we use a modified version of the solution set of (Asteris et al., 2011) to demonstrate a polymimic number of main vectors in polynomial time (nd + 1)."}, {"heading": "1.2.4 Experimental Evaluation", "text": "Our real dataset consists of a large Twitter collection of more than 10 million tweets over a period of about six months. We conducted several experiments with different subsets of our dataset: tweets over a specific time frame, tweets containing a specific word, and so on. Our implementation performs in less than a second for 50k-100k documents, and in a few minutes for millions of documents on a PC. Our scheme typically delivers more than 90% of optimal performance, even for d < 3, and empirically exceeds the sparse PCA algorithms previously proposed."}, {"heading": "1.3 Related Work", "text": "There was a considerable volume of previous work on sparse PCA. Initial heuristic approaches used factor rotation techniques and thresholds of eigenvectors to achieve low validity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995), then a modified PCA technique based on LASSO (SCoTLASS) was introduced to produce sparse PCs (Jolliffe et al., 2003). In (Zou et al., 2006), a non-convex regression-type approach, a minimal LASSO was used to produce sparse PCs. A non-convex technique was presented in (Sriperumbudur et al, 2007). In (Moghaddam et al, 2006b), the authors used spectral arguments to motivate a greedy branch-and-bound approach, further researched in Mogdam (2007)."}, {"heading": "2 Sparse PCA through Low-rank Approximations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Proposed Algorithm", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2.2 Approximation Guarantees", "text": "The desired sparse PC is x x x x x x 2 = 1, x x x x x x x x x = 11, x x x x x x x x = 11, x x x x x x x x = 11, x x x x x x x x x = 11, x x x x x x x = 11, x x x x x x x x x x = 11, x x x x x x x x x = 11, x x x x x x x x = 11, x x x x x x x x x x x x = 11, x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 The Spannogram Algorithm", "text": "In this case, we are only able to limit ourselves to the question of whether we shall be able to see the infinite number of the infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinity, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, infinite, in"}, {"heading": "3.2.1 Spherical variables and the spannogram", "text": "Here we use a transformation of our problem space into a 2-dimensional space, as it took place in (Karystinos and Liavas, 2010). The transformation is carried out by spherical variables that allow us to visualize the 2-dimensional range of V2. For the case of rank 2 we have a single-phase variable \u03c6 = (\u2212 \u03c02, \u03c02] and use it to rewrite c without loss of generality, asc = [Sininiveau cos\u03c6], which in turn is unit norm and for all cases scans only one unit vectors. Under this characterization we can express vc in relation to Vc (Siniff) = Siniff = Siniff points [Siniff points], the absolute property norm and for all sinivectors scans all4 \u00b7 1 vectors. From this point of view we can express vc points in relation to v points."}, {"heading": "3.2.2 Building S2", "text": "In order to build S2, we must i) determine all c intersection vectors defined at intersections on the \u03c6 axis i and ii). To determine an intersection vector, we must solve all 2 (n 2) equations [v (\u03c6)] i = \u00b1 [v (\u03c6)] j for all pairs i, j [n]. This results in [v (\u03c6)] i = \u00b1 [v (\u03c6)] j \u21d2 eTi V c, that is (eTi \u00b1 eTj) Vc = 0 \u21d2 c = nullspace (((eTi \u00b1 eTj) V). (9) Since c must be standard unit, we simply must find the solution c = \u00b1 eTj V c, that is (eTi \u00b1 eTj V c) n, that is (eTi \u00b1 eTj) Vc = 0 \u21d2 c = nullspace (eTj) V)."}, {"heading": "4 Experimental Evaluation", "text": "We are now empirically evaluating the performance of our algorithm and comparing it with the full regularization pathway (FullPath) of (d'Aspremont et al., 2007b), the generalized power method (GPower) of (Journe \u0301 e et al., 2010) and the truncated power method (TPower) of (Yuan et al., 2011). We are starting with a synthetic experiment: we are trying to estimate the support for the first two sparse eigenvectors of a covariance matrix of sample vectors. We are continuing to test our algorithm using gene expression data. Finally, we are conducting experiments with an extensive document dataset consisting of millions of Twitter messages."}, {"heading": "4.1 Spiked Covariance Recovery", "text": "We will first test our approximation algorithm using an interesting dataset generated in the same way as in (Shen and Huang, 2008; Yuan and Zhang, 2011).We will consider a covariance matrix \u03a3, which has two sparse eigenvectors with very large eigenvectors and the rest of the eigenvectors correspond to small eigenvects.Here we will consider \u03a3 = n = 1 \u03bbiviv T i with \u03bb1 = 400, \u03bb2 = 300, \u03bb3 = 1, \u03bb500 = 1. where the first two eigenvectors are sparse and each has 10 non-zero entries and non-overlapping supports.The remaining eigenvectors are selected as n \u2212 2 orthogonal vectors in zero space of [v1 v2].We will have two sets of experiments, one for a few samples and one for an extremely few. We will generate m = 50 samples of length n = 500 distributed Gaussian with coance matrix."}, {"heading": "4.2 Gene Expression Data Set", "text": "In the same way as in the relevant sparse PCA literature, we evaluate our approximation using two sets of gene expression data used in d'Aspremont et al., 2007b, 2008; Yuan et al., 2011). We record the ratio of the variance from the first sparse PC to the declared variance of the first eigenvector (which is equal to the first eigenvalue). We also record the ratio of the outer power limit derived in d'Aspremont et al., 2008. We observe that our approximation follows the same pattern of optimism as most previous methods for many Sparsity k values. In these experiments, we did not test the GPower method as the output savings cannot be explicitly predicted. However, earlier literature indicates that GPower is almost optimal in this scenario as well."}, {"heading": "4.3 Large-scale Twitter data-set", "text": "This year, it is so far that it will only take a few days for it to come to a conclusion."}, {"heading": "5 Conclusions", "text": "We come to the conclusion that our algorithm can provide efficiently interpretable sparse PCs while matching or exceeding the accuracy of previous methods. Parallel implementation within the framework of MapReduce and larger data studies are very interesting future directions."}, {"heading": "B Nonnegative matrix speed-up", "text": "In this section, we show that if A has nonnegative entries, we can speed up the calculations by a factor of 2d \u2212 1. The main idea behind this acceleration is that if A has only nonnegative entries, we then have to check in our intersection equations in Equation. (13) We do not have to check all possible signed combinations of the d curves. In the following, we will explain this point. First, we note that the Perron-Frobenius theorem (Horn and Johnson, 1990) grants us the fact that the optimal solution x \u00b2 -Sk will have nonnegative entries. That is, if A has nonnegative entries, then there will also be nonnegative entries. This allows us to apply a redundant nonnegativity restriction to our optimization max x \u00b2 -Sk xTAx = max x \u00b2 -Sk, x \u00b2 elements, x \u00b2 elements, which we use only for the optimal form of Sd \u00b2 entries, to reduce the factor of 2d points \u2212 Sk = \u00b2 (let us consider the Lx \u00b2 elements here: 1 x \u00b2)."}, {"heading": "C Feature Elimination", "text": "In this section we present our feature Elimination Algorithm. This step reduces the dimension of the problem and this reduction in practice is empirically presented as significant and allows us to maintain our algorithm for very large matrices. [...] This step is based on the basic idea of our elimination techniques, which are based on the value of their norm. [...] This step, in turn, is based on the locally optimal support framework used in our approach algorithm for the uppermost elements of vc. [...] As we have already mentioned, all elements of vc correspond to hyper surfaces. [v] i | these are functions of the d \u2212 1 spherical variables in relation."}, {"heading": "D Approximation Guarantees", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "E Resolving singularities", "text": "In our algorithmic developments, we have made an assumption about the curves studied, i.e., about the rows of the Vd matrix. (This assumption has been made so that disordered cases where more than d-curves intersect in a single point in d-dimensional space are circumvented. Ed Ed Ed + 1 allows such a singularity even for full-fledged matrices Vd and can generate enumerable problems in the generation of locally optimal candidate vectors achieved by the intersection equations: e T i1 \u2212 b1eTi2... eTi1 \u2212 bd \u2212 1eTid d \u2212 1 \u00d7 n Vdc = 0d \u2212 1 \u00d7 1. (26) The above requirement can be formalized as: no system of equations of the following form has a non-trivial (i.e., nonzero) solution eTi1 \u2212 b1eTi2 \u2212 bd \u2212 bd \u2212 1eTi1 \u2212 bd \u2212 bd \u2212 1."}, {"heading": "F Twitter data-set description", "text": "In Table 4, we provide an overview of our Twitter datasets.F.1 Performance laws In this subsection, we provide empirical evidence that our tested datasets show a decay of the performance law in their spectrum. We report on these observations as proof of the concept of our approximation guarantees. On the basis of the spectrum of some subsets of our dataset, we provide the exact approximation guarantees derived from our limitations.In Figure 6, we present the most appropriate power law for the spectrum and the degrees with the data set parameters specified on the numbers. An interesting observation is that a very similar power law is followed by the degrees of terms in the dataset, and subsets of our dataset based on a specific query. We observe that for all of these subsets of our dataset, the spectrum actually follows a power law. An interesting observation is that a very similar power law is followed by the degrees of terms in the dataset. This finding is compatible with the models, miscellaneous models, and large data hairs (based on the modelling algorithms and large hairs)."}], "references": [{"title": "High-dimensional analysis of semidefinite relaxations for sparse principal components", "author": ["A.A. Amini", "M.J. Wainwright"], "venue": "In Information Theory,", "citeRegEx": "Amini and Wainwright.,? \\Q2008\\E", "shortCiteRegEx": "Amini and Wainwright.", "year": 2008}, {"title": "Sparse principal component of a rank-deficient matrix", "author": ["M. Asteris", "D.S. Papailiopoulos", "G.N. Karystinos"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Asteris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Asteris et al\\.", "year": 2011}, {"title": "Optimal detection of sparse principal components in high dimension", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "arXiv preprint arXiv:1202.5070,", "citeRegEx": "Berthet and Rigollet.,? \\Q2012\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2012}, {"title": "Complexity theoretic lower bounds for sparse principal component detection, 2013a", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": null, "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Computational lower bounds for sparse pca", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "arXiv preprint arXiv:1304.0828,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "Loading and correlations in the interpretation of principle compenents", "author": ["J. Cadima", "I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "Cadima and Jolliffe.,? \\Q1995\\E", "shortCiteRegEx": "Cadima and Jolliffe.", "year": 1995}, {"title": "Sparse pca: Optimal rates and adaptive estimation", "author": ["T Tony Cai", "Zongming Ma", "Yihong Wu"], "venue": "arXiv preprint arXiv:1211.1309,", "citeRegEx": "Cai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2012}, {"title": "Optimal estimation and rank detection for sparse spiked covariance matrices", "author": ["Tony Cai", "Zongming Ma", "Yihong Wu"], "venue": "arXiv preprint arXiv:1305.3235,", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Eigenvalues of random power law graphs", "author": ["F. Chung", "L. Lu", "V. Vu"], "venue": "Annals of Combinatorics,", "citeRegEx": "Chung et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2003}, {"title": "A direct formulation for sparse pca using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L.E. Ghaoui"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Approximation bounds for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L.E. Ghaoui"], "venue": "arXiv preprint arXiv:1205.0121,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2012\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2012}, {"title": "Full regularization path for sparse principal component analysis", "author": ["Alexandre d\u2019Aspremont", "Francis R. Bach", "Laurent El Ghaoui"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Concept decompositions for large sparse text data using clustering", "author": ["I.S. Dhillon", "D.S. Modha"], "venue": "Machine learning,", "citeRegEx": "Dhillon and Modha.,? \\Q2001\\E", "shortCiteRegEx": "Dhillon and Modha.", "year": 2001}, {"title": "Sparse pca for text corpus summarization and exploration", "author": ["B. Gawalt", "Y. Zhang", "L. El Ghaoui"], "venue": "NIPS 2010 Workshop on Low-Rank Matrix Approximation,", "citeRegEx": "Gawalt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gawalt et al\\.", "year": 2010}, {"title": "Matrix analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge university press,", "citeRegEx": "Horn and Johnson.,? \\Q1990\\E", "shortCiteRegEx": "Horn and Johnson.", "year": 1990}, {"title": "Rotation of principal components: choice of normalization constraints", "author": ["I.T. Jolliffe"], "venue": "Journal of Applied Statistics,", "citeRegEx": "Jolliffe.,? \\Q1995\\E", "shortCiteRegEx": "Jolliffe.", "year": 1995}, {"title": "A modified principal component technique based on the lasso", "author": ["I.T. Jolliffe", "N.T. Trendafilov", "M. Uddin"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Jolliffe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jolliffe et al\\.", "year": 2003}, {"title": "Generalized power method for sparse principal component analysis", "author": ["M. Journ\u00e9e", "Y. Nesterov", "P. Richt\u00e1rik", "R. Sepulchre"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "The varimax criterion for analytic rotation in factor analysis", "author": ["H.F. Kaiser"], "venue": null, "citeRegEx": "Kaiser.,? \\Q1958\\E", "shortCiteRegEx": "Kaiser.", "year": 1958}, {"title": "Efficient computation of the binary vector that maximizes a rank-deficient quadratic form", "author": ["G.N. Karystinos", "A.P. Liavas"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Karystinos and Liavas.,? \\Q2010\\E", "shortCiteRegEx": "Karystinos and Liavas.", "year": 2010}, {"title": "Fast algorithms for sparse principal component analysis based on rayleigh quotient iteration", "author": ["Volodymyr Kuleshov"], "venue": null, "citeRegEx": "Kuleshov.,? \\Q2013\\E", "shortCiteRegEx": "Kuleshov.", "year": 2013}, {"title": "Deflation methods for sparse pca", "author": ["L. Mackey"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Mackey.,? \\Q2009\\E", "shortCiteRegEx": "Mackey.", "year": 2009}, {"title": "On the eigenvalue power law. Randomization and approximation techniques in computer science, pages", "author": ["M. Mihail", "C. Papadimitriou"], "venue": null, "citeRegEx": "Mihail and Papadimitriou.,? \\Q2002\\E", "shortCiteRegEx": "Mihail and Papadimitriou.", "year": 2002}, {"title": "Generalized spectral bounds for sparse lda", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "Spectral bounds for sparse pca: Exact and greedy algorithms", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Moghaddam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2006}, {"title": "Fast pixel/part selection with sparse eigenvectors", "author": ["B. Moghaddam", "Y. Weiss", "S. Avidan"], "venue": "In Computer Vision,", "citeRegEx": "Moghaddam et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moghaddam et al\\.", "year": 2007}, {"title": "Sparse principal component analysis via regularized low rank matrix approximation", "author": ["H. Shen", "J.Z. Huang"], "venue": "Journal of multivariate analysis,", "citeRegEx": "Shen and Huang.,? \\Q2008\\E", "shortCiteRegEx": "Shen and Huang.", "year": 2008}, {"title": "Sparse eigen methods by dc programming", "author": ["B.K. Sriperumbudur", "D.A. Torres", "G.R.G. Lanckriet"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2007}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["X.T. Yuan", "T. Zhang"], "venue": "arXiv preprint arXiv:1112.2679,", "citeRegEx": "Yuan and Zhang.,? \\Q2011\\E", "shortCiteRegEx": "Yuan and Zhang.", "year": 2011}, {"title": "Large-scale sparse principal component analysis with application to text data", "author": ["Y. Zhang", "L. El Ghaoui"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang and Ghaoui.,? \\Q2011\\E", "shortCiteRegEx": "Zhang and Ghaoui.", "year": 2011}, {"title": "Sparse pca: Convex relaxations, algorithms and applications", "author": ["Y. Zhang", "A. d\u2019Aspremont", "L.E. Ghaoui"], "venue": "Handbook on Semidefinite, Conic and Polynomial Optimization,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Sparse principal component analysis", "author": ["H. Zou", "T. Hastie", "R. Tibshirani"], "venue": "Journal of computational and graphical statistics,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 14, "context": ") using the few keywords in their support (Gawalt et al., 2010; Zhang and El Ghaoui, 2011).", "startOffset": 42, "endOffset": 90}, {"referenceID": 31, "context": "Specifically, we find a constant \u03b40 such that for all sparsity levels k > \u03b40 n we obtain a constant approximation ratio for sparse PCA, partially solving the open problem discussed in (Zhang et al., 2012; d\u2019Aspremont et al., 2012).", "startOffset": 184, "endOffset": 230}, {"referenceID": 11, "context": "Specifically, we find a constant \u03b40 such that for all sparsity levels k > \u03b40 n we obtain a constant approximation ratio for sparse PCA, partially solving the open problem discussed in (Zhang et al., 2012; d\u2019Aspremont et al., 2012).", "startOffset": 184, "endOffset": 230}, {"referenceID": 20, "context": "This framework was introduced by the foundational work of (Karystinos and Liavas, 2010) in the context of solving quadratic form maximization problems over\u00b11 vectors.", "startOffset": 58, "endOffset": 87}, {"referenceID": 1, "context": "This framework was consequently used in (Asteris et al., 2011) to develop a constant rank solver that computes", "startOffset": 40, "endOffset": 62}, {"referenceID": 1, "context": "We use as a subroutine a modified version of the solver of (Asteris et al., 2011), to examine a polynomial number of special vectors that lead to a sparse principal component which admits provable performance.", "startOffset": 59, "endOffset": 81}, {"referenceID": 19, "context": "Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995).", "startOffset": 113, "endOffset": 170}, {"referenceID": 16, "context": "Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995).", "startOffset": 113, "endOffset": 170}, {"referenceID": 5, "context": "Initial heuristic approaches used factor rotation techniques and thresholding of eigenvectors to obtain sparsity (Kaiser, 1958; Jolliffe, 1995; Cadima and Jolliffe, 1995).", "startOffset": 113, "endOffset": 170}, {"referenceID": 17, "context": "Then, a modified PCA technique based on the LASSO (SCoTLASS) was introduced in (Jolliffe et al., 2003).", "startOffset": 79, "endOffset": 102}, {"referenceID": 32, "context": "In (Zou et al., 2006), a nonconvex regression-type approximation, penalized \u00e0 la LASSO was used to produce sparse PCs.", "startOffset": 3, "endOffset": 21}, {"referenceID": 28, "context": "A nonconvex technique was presented in (Sriperumbudur et al., 2007).", "startOffset": 39, "endOffset": 67}, {"referenceID": 26, "context": ", 2006b), the authors used spectral arguments to motivate a greedy branch-and-bound approach, further explored in (Moghaddam et al., 2007).", "startOffset": 114, "endOffset": 138}, {"referenceID": 27, "context": "In (Shen and Huang, 2008), a similar technique to SVD was used employing sparsity penalties on each round of projections.", "startOffset": 3, "endOffset": 25}, {"referenceID": 31, "context": "A significant body of work based on semidefinite programming (SDP) approaches was established in (d\u2019Aspremont et al., 2007a; Zhang et al., 2012; d\u2019Aspremont et al., 2008).", "startOffset": 97, "endOffset": 170}, {"referenceID": 10, "context": "A significant body of work based on semidefinite programming (SDP) approaches was established in (d\u2019Aspremont et al., 2007a; Zhang et al., 2012; d\u2019Aspremont et al., 2008).", "startOffset": 97, "endOffset": 170}, {"referenceID": 18, "context": "A variation of the power method was used in (Journ\u00e9e et al., 2010).", "startOffset": 44, "endOffset": 66}, {"referenceID": 22, "context": "When computing multiple PCs, the issue of deflation arises as discussed in (Mackey, 2009).", "startOffset": 75, "endOffset": 89}, {"referenceID": 29, "context": "In (Yuan and Zhang, 2011), the authors introduced a very efficient sparse PCA approximation based on truncating the well-known power method to obtain the exact level of sparsity desired.", "startOffset": 3, "endOffset": 25}, {"referenceID": 21, "context": "A fast algorithm based on Rayleigh quotient iteration was developed in (Kuleshov, 2013).", "startOffset": 71, "endOffset": 87}, {"referenceID": 0, "context": "In (Amini and Wainwright, 2008), the first theoretical optimality guarantees were established under the spiked covariance for diagonal thresholding and the SDP relaxation of (d\u2019Aspremont et al.", "startOffset": 3, "endOffset": 31}, {"referenceID": 29, "context": "In (Yuan and Zhang, 2011), the authors provide peformance guarantees for the truncated power method under specific assumptions of data model, similar to the restricted isometry property.", "startOffset": 3, "endOffset": 25}, {"referenceID": 11, "context": "In (d\u2019Aspremont et al., 2012) the authors provide detection guarantees under the single spike covariance model.", "startOffset": 3, "endOffset": 29}, {"referenceID": 6, "context": "Then, in (Cai et al., 2012) and (Cai et al.", "startOffset": 9, "endOffset": 27}, {"referenceID": 7, "context": ", 2012) and (Cai et al., 2013) the authors provide guarantees under the assumption of multiple spikes in the covariance.", "startOffset": 12, "endOffset": 30}, {"referenceID": 2, "context": "It is also suspected that it is computationally challenging to recover the sparse spikes of a spiked covariance model, under optimal sample complexity as was shown in (Berthet and Rigollet, 2013b), (Berthet and Rigollet, 2013a), and (Berthet and Rigollet, 2012).", "startOffset": 233, "endOffset": 261}, {"referenceID": 0, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 29, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 11, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 7, "context": "Despite this extensive literature, to the best of our knowledge, there are very few provable approximation guarantees for the optimization version of the sparse PCA problem, and usually under restricted statistical data models (Amini and Wainwright, 2008; Yuan and Zhang, 2011; d\u2019Aspremont et al., 2012; Cai et al., 2013).", "startOffset": 227, "endOffset": 321}, {"referenceID": 1, "context": "However, we show how sparse PCA can be efficiently solved on Ad if the rank d is constant with respect to n, using the machinery of (Asteris et al., 2011).", "startOffset": 132, "endOffset": 154}, {"referenceID": 1, "context": "2 Rank-2 case Now we describe how to compute S2 using the constant rank solver of (Asteris et al., 2011).", "startOffset": 82, "endOffset": 104}, {"referenceID": 20, "context": "1 Spherical variables and the spannogram Here we use a transformation of our problem space into a 2-dimensional space as was done in (Karystinos and Liavas, 2010).", "startOffset": 133, "endOffset": 162}, {"referenceID": 1, "context": "3This is a special case of the general d dimensional lemma of (Asteris et al., 2011) (found in the Appendix), but we prove the special case to simplify the presentation.", "startOffset": 62, "endOffset": 84}, {"referenceID": 18, "context": ", 2007b), the generalized power method (GPower) of (Journ\u00e9e et al., 2010), and the truncated power method (TPower) of (Yuan and Zhang, 2011).", "startOffset": 51, "endOffset": 73}, {"referenceID": 29, "context": ", 2010), and the truncated power method (TPower) of (Yuan and Zhang, 2011).", "startOffset": 52, "endOffset": 74}, {"referenceID": 10, "context": ", 2007a), since the FullPath algorithm is experimentally shown to have similar or better performance (d\u2019Aspremont et al., 2008).", "startOffset": 101, "endOffset": 127}, {"referenceID": 27, "context": "1 Spiked Covariance Recovery We first test our approximation algorithm on an artificial data set generated in the same manner as in (Shen and Huang, 2008; Yuan and Zhang, 2011).", "startOffset": 132, "endOffset": 176}, {"referenceID": 29, "context": "1 Spiked Covariance Recovery We first test our approximation algorithm on an artificial data set generated in the same manner as in (Shen and Huang, 2008; Yuan and Zhang, 2011).", "startOffset": 132, "endOffset": 176}, {"referenceID": 22, "context": "We use the projection deflation method (Mackey, 2009) to obtain A\u2032 = (I \u2212 \u1e7d1\u1e7d 1 )A(I \u2212 \u1e7d1\u1e7d 1 ) and work on it to obtain \u1e7d2, the second estimated eigenvector of \u03a3.", "startOffset": 39, "endOffset": 53}, {"referenceID": 0, "context": "Interesting tradeoffs of sample complexity and probability of recovery where derived in (Amini and Wainwright, 2008).", "startOffset": 88, "endOffset": 116}, {"referenceID": 29, "context": "In the same manner as in the relevant sparse PCA literature, we evaluate our approximation on two gene expression data-sets used in (d\u2019Aspremont et al., 2007b, 2008; Yuan and Zhang, 2011).", "startOffset": 132, "endOffset": 187}, {"referenceID": 10, "context": "We also plot the performance outer bound derived in (d\u2019Aspremont et al., 2008).", "startOffset": 52, "endOffset": 78}, {"referenceID": 13, "context": "Empirical results have been reported that indicate power-law like decays for eigenvalues where no cutoff is observed (Dhillon and Modha, 2001) and some derived power-law generative models for 0/1 matrices (Mihail and Papadimitriou, 2002; Chung et al.", "startOffset": 117, "endOffset": 142}, {"referenceID": 23, "context": "Empirical results have been reported that indicate power-law like decays for eigenvalues where no cutoff is observed (Dhillon and Modha, 2001) and some derived power-law generative models for 0/1 matrices (Mihail and Papadimitriou, 2002; Chung et al., 2003).", "startOffset": 205, "endOffset": 257}, {"referenceID": 8, "context": "Empirical results have been reported that indicate power-law like decays for eigenvalues where no cutoff is observed (Dhillon and Modha, 2001) and some derived power-law generative models for 0/1 matrices (Mihail and Papadimitriou, 2002; Chung et al., 2003).", "startOffset": 205, "endOffset": 257}], "year": 2014, "abstractText": "We introduce a novel algorithm that computes the k-sparse principal component of a positive semidefinite matrix A. Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of A. We obtain provable approximation guarantees that depend on the spectral decay profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation. For example, if the eigenvalues of A follow a power-law decay, we obtain a polynomial-time approximation algorithm for any desired accuracy. A key algorithmic component of our scheme is a combinatorial feature elimination step that is provably safe and in practice significantly reduces the running complexity of our algorithm. We implement our algorithm and test it on multiple artificial and real data sets. Due to the feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes. Our experimental evaluation shows that our scheme is nearly optimal while finding very sparse vectors. We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms in all tested data sets.", "creator": "LaTeX with hyperref package"}}}