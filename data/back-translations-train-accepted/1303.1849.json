{"id": "1303.1849", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2013", "title": "Revisiting the Nystrom method for improved large-scale machine learning", "abstract": "We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projections methods. These bounds are qualitatively superior to existing bounds---e.g., improved additive-error bounds for the spectral and Frobenius norm errors and relative-error bounds for the trace norm error.", "histories": [["v1", "Thu, 7 Mar 2013 23:16:16 GMT  (1195kb,D)", "http://arxiv.org/abs/1303.1849v1", "45 pages, 14 color figures"], ["v2", "Mon, 3 Jun 2013 20:07:19 GMT  (2649kb,D)", "http://arxiv.org/abs/1303.1849v2", "60 pages, 15 color figures; updated proof of Frobenius norm bounds, added comparison to projection-based low-rank approximations, and an analysis of the power method applied to SPSD sketches"]], "COMMENTS": "45 pages, 14 color figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.NA", "authors": ["alex gittens", "michael w mahoney"], "accepted": true, "id": "1303.1849"}, "pdf": {"name": "1303.1849.pdf", "metadata": {"source": "CRF", "title": "REVISITING THE NYSTRO\u0308M METHOD FOR IMPROVED LARGE-SCALE MACHINE LEARNING", "authors": ["ALEX GITTENS", "MICHAEL W. MAHONEY"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}], "references": [{"title": "Nystr\u00f6m approximation of Wishart matrices", "author": ["N. Arcolano", "P.J. Wolfe"], "venue": "In Proceedings of the 2010 IEEE International Conference on Acoustics Speech and Signal Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Blendenpik: Supercharging LAPACK\u2019s least-squares solver", "author": ["H. Avron", "P. Maymounkov", "S. Toledo"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Sharp analysis of low-rank kernel matrix approximations", "author": ["F. Bach"], "venue": "Technical report. Preprint:", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Efficient Gaussian process regression for large data sets", "author": ["A. Banerjee", "D. Dunson", "S. Tokdar"], "venue": "Technical report. Preprint:", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Fast low-rank approximation for covariance matrices", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "In Second IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "On sparse representations of linear operators and the approximation of matrix products", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "In Proceedings of the 42nd Annual Conference on Information Sciences and Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "On landmark selection and sampling in high-dimensional data analysis", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Philosophical Transactions of the Royal Society, Series A,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Proc. Natl. Acad. Sci. USA,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "On the approximation of matrix products and positive definite matrices", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Technical report. Preprint:", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "In Proceedings of the 7th Annual ACM SIGKDD Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Improved matrix algorithms via the subsampled randomized Hadamard transform", "author": ["C. Boutsidis", "A. Gittens"], "venue": "Technical report. Preprint:", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "An improved approximation algorithm for the column subset selection problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "In Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sublinear randomized algorithms for skeleton decompositions", "author": ["J. Chiu", "L. Demanet"], "venue": "Technical report. Preprint:", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "A Robotics Toolbox for MATLAB", "author": ["P.I. Corke"], "venue": "IEEE Robotics and Automation Magazine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["C. Cortes", "M. Mohri", "A. Talwalkar"], "venue": "In Proceedings of the 13th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Localization on low-order eigenvectors of data matrices", "author": ["M. Cucuringu", "M.W. Mahoney"], "venue": "Technical report. Preprint:", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "D.P. Woodruff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Relative-error CUR matrix decompositions", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Faster least squares approximation", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan", "T. Sarl\u00f3s"], "venue": "Numerische Mathematik,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A novel greedy algorithm for Nystr\u00f6m approximation", "author": ["A.K. Farahat", "A. Ghodsi", "M.S. Kamel"], "venue": "In Proceedings of the 14th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Experiments with random projections for machine learning", "author": ["D. Fradkin", "D. Madigan"], "venue": "In Proceedings of the 9th Annual ACM SIGKDD Conference,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Classes of Kernels for Machine Learning: A Statistics Perspective", "author": ["M. Genton"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "The spectral norm error of the naive Nystrom extension", "author": ["A. Gittens"], "venue": "Technical report. Preprint:", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Towards the identification of essential genes using targeted genome sequencing and comparative analysis", "author": ["A.M. Gustafson", "E.S. Snitkin", "S.C.J. Parker", "C. DeLisi", "S. Kasif"], "venue": "BMC Genomics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Result analysis of the NIPS 2003 feature selection challenge", "author": ["I. Guyon", "S.R. Gunn", "A. Ben-Hur", "G. Dror"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J.A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Spectral approximations in machine learning", "author": ["D. Homrighausen", "D.J. McDonald"], "venue": "Technical report. Preprint:", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Improved bound for the Nystr\u00f6m\u2019s method and its application to kernel classification", "author": ["R. Jin", "T. Yang", "M. Mahdavi", "Y.-F. Li", "Z.-H. Zhou"], "venue": "Technical report. Preprint:", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "The Enron corpus: A new dataset for email classification research", "author": ["B. Klimt", "Y. Yang"], "venue": "In Proceedings of the 15th European Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Ensemble Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "In Annual Advances in Neural Information Processing Systems 22: Proceedings of the 2009 Conference,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "On sampling-based approximate spectral decomposition", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Sampling techniques for the Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "In Proceedings of the 12th Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Sampling methods for the Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Graph Evolution: Densification and Shrinking Diameters", "author": ["J. Leskovec", "J. Kleinberg", "C. Faloutsos"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Making large-scale Nystr\u00f6m approximation possible", "author": ["M. Li", "J.T. Kwok", "B.-L. Lu"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Learning low-rank kernel matrices with column-based methods", "author": ["S. Liu", "J. Zhang", "K. Sun"], "venue": "Communications in Statistics\u2014Simulation and Computation,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Stochastic low-rank kernel learning for regression", "author": ["P. Machart", "T. Peel", "S. Anthoine", "L. Ralaivola", "H. Glotin"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Divide-and-conquer matrix factorization", "author": ["L. Mackey", "A. Talwalkar", "M.I. Jordan"], "venue": "Technical report. Preprint:", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Divide-and-conquer matrix factorization", "author": ["L. Mackey", "A. Talwalkar", "M.I. Jordan"], "venue": "In Annual Advances in Neural Information Processing Systems 24: Proceedings of the 2011 Conference,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning", "author": ["M.W. Mahoney"], "venue": "NOW Publishers,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Algorithmic and statistical perspectives on large-scale data analysis", "author": ["M.W. Mahoney"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2012}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proc. Natl. Acad. Sci. USA,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "A randomized algorithm for the decomposition of matrices", "author": ["P.-G. Martinsson", "V. Rokhlin", "M. Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "LSRN: A parallel iterative solver for strongly over- or under-determined systems", "author": ["X. Meng", "M.A. Saunders", "M.W. Mahoney"], "venue": "Technical report. Preprint:", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "Can matrix coherence be efficiently and accurately estimated", "author": ["M. Mohri", "A. Talwalkar"], "venue": "In Proceedings of the 14th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2011}, {"title": "Molecular characterisation of soft tissue tumours: a gene expression study", "author": ["T.O. Nielsen", "R.B. West", "S.C. Linn", "O. Alter", "M.A. Knowling", "J.X. O\u2019Connell", "S. Zhu", "M. Fero", "G. Sherlock", "J.R. Pollack", "P.O. Brown", "D. Botstein", "M. van de Rijn"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2002}, {"title": "A signal processing application of randomized low-rank approximations", "author": ["P. Parker", "P.J. Wolfe", "V. Tarok"], "venue": "In Proceedings of the 13th IEEE Workshop on Statistical Signal Processing,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2005}, {"title": "PCA-correlated SNPs for structure identification in worldwide human populations", "author": ["P. Paschou", "E. Ziv", "E.G. Burchard", "S. Choudhry", "W. Rodriguez-Cintron", "M.W. Mahoney", "P. Drineas"], "venue": "PLoS Genetics,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2007}, {"title": "A randomized algorithm for principal component analysis", "author": ["V. Rokhlin", "A. Szlam", "M. Tygert"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2009}, {"title": "Adaptive beamforming using fast low-rank covariance matrix approximations", "author": ["D.N. Spendley", "P.J. Wolfe"], "venue": "In Proceedsings of the IEEE Radar Conference,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "Large-scale manifold learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2008}, {"title": "Matrix coherence and the Nystr\u00f6m method", "author": ["A. Talwalkar", "A. Rostamizadeh"], "venue": "In Proceedings of the 26th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2010}, {"title": "Improved analysis of the subsampled randomized Hadamard transform", "author": ["J.A. Tropp"], "venue": "Adv. Adapt. Data Anal.,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2011}, {"title": "The Johnson-Lindenstrauss transform: An empirical study", "author": ["S. Venkatasubramanian", "Q. Wang"], "venue": "In ALENEX11: Workshop on Algorithms Engineering and Experimentation,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2011}, {"title": "Observations on the Nystr\u00f6m method for Gaussian process prediction", "author": ["C.K.I. Williams", "C.E. Rasmussen", "A. Schwaighofer", "V. Tresp"], "venue": "Technical report, University of Edinburgh,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2002}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In Annual Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2001}, {"title": "A fast randomized algorithm for the approximation of matrices", "author": ["F. Woolfe", "E. Liberty", "V. Rokhlin", "M. Tygert"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2008}, {"title": "Objective identification of informative wavelength regions in galaxy spectra", "author": ["C.-W. Yip", "M.W. Mahoney", "A.S. Szalay", "I. Csabai", "T. Budav\u00e1ri", "R.F.G. Wyse", "L. Dobos"], "venue": "Manuscript submitted for publication.,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2013}, {"title": "Density-weighted Nystr\u00f6m method for computing large kernel eigensystems", "author": ["K. Zhang", "J.T. Kwok"], "venue": "Neural Computation,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2009}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["K. Zhang", "J.T. Kwok"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2010}, {"title": "Improved Nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2008}], "referenceMentions": [{"referenceID": 19, "context": "For example, existing worst-case bounds such as those of [21] are very weak, especially compared with existing bounds for least-squares regression and general low-rank matrix approximation problems [22, 23, 45].", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "For example, existing worst-case bounds such as those of [21] are very weak, especially compared with existing bounds for least-squares regression and general low-rank matrix approximation problems [22, 23, 45].", "startOffset": 198, "endOffset": 210}, {"referenceID": 21, "context": "For example, existing worst-case bounds such as those of [21] are very weak, especially compared with existing bounds for least-squares regression and general low-rank matrix approximation problems [22, 23, 45].", "startOffset": 198, "endOffset": 210}, {"referenceID": 43, "context": "For example, existing worst-case bounds such as those of [21] are very weak, especially compared with existing bounds for least-squares regression and general low-rank matrix approximation problems [22, 23, 45].", "startOffset": 198, "endOffset": 210}, {"referenceID": 36, "context": "Moreover, many other worst-case bounds make strong assumptions about the coherence properties of the input data [38, 28].", "startOffset": 112, "endOffset": 120}, {"referenceID": 26, "context": "Moreover, many other worst-case bounds make strong assumptions about the coherence properties of the input data [38, 28].", "startOffset": 112, "endOffset": 120}, {"referenceID": 59, "context": "For example, some work has concluded that the statistical leverage scores of realistic data matrices are fairly uniform, meaning that the coherence is small and thus uniform sampling is appropriate [61, 38], while other work has demonstrated that leverage scores are often very nonuniform in ways that render uniform sampling inappropriate and that can be essential to highlight properties of downstream interest [53, 47].", "startOffset": 198, "endOffset": 206}, {"referenceID": 36, "context": "For example, some work has concluded that the statistical leverage scores of realistic data matrices are fairly uniform, meaning that the coherence is small and thus uniform sampling is appropriate [61, 38], while other work has demonstrated that leverage scores are often very nonuniform in ways that render uniform sampling inappropriate and that can be essential to highlight properties of downstream interest [53, 47].", "startOffset": 198, "endOffset": 206}, {"referenceID": 51, "context": "For example, some work has concluded that the statistical leverage scores of realistic data matrices are fairly uniform, meaning that the coherence is small and thus uniform sampling is appropriate [61, 38], while other work has demonstrated that leverage scores are often very nonuniform in ways that render uniform sampling inappropriate and that can be essential to highlight properties of downstream interest [53, 47].", "startOffset": 413, "endOffset": 421}, {"referenceID": 45, "context": "For example, some work has concluded that the statistical leverage scores of realistic data matrices are fairly uniform, meaning that the coherence is small and thus uniform sampling is appropriate [61, 38], while other work has demonstrated that leverage scores are often very nonuniform in ways that render uniform sampling inappropriate and that can be essential to highlight properties of downstream interest [53, 47].", "startOffset": 413, "endOffset": 421}, {"referenceID": 1, "context": "Third, in recent years several high-quality numerical implementations of randomized matrix algorithms for least-squares and low-rank approximation problems have been developed [3, 49, 62, 54, 48].", "startOffset": 176, "endOffset": 195}, {"referenceID": 47, "context": "Third, in recent years several high-quality numerical implementations of randomized matrix algorithms for least-squares and low-rank approximation problems have been developed [3, 49, 62, 54, 48].", "startOffset": 176, "endOffset": 195}, {"referenceID": 60, "context": "Third, in recent years several high-quality numerical implementations of randomized matrix algorithms for least-squares and low-rank approximation problems have been developed [3, 49, 62, 54, 48].", "startOffset": 176, "endOffset": 195}, {"referenceID": 52, "context": "Third, in recent years several high-quality numerical implementations of randomized matrix algorithms for least-squares and low-rank approximation problems have been developed [3, 49, 62, 54, 48].", "startOffset": 176, "endOffset": 195}, {"referenceID": 46, "context": "Third, in recent years several high-quality numerical implementations of randomized matrix algorithms for least-squares and low-rank approximation problems have been developed [3, 49, 62, 54, 48].", "startOffset": 176, "endOffset": 195}, {"referenceID": 44, "context": "are of greater interest [46], and where relatively strong homogeneity assumptions can be made about the input data.", "startOffset": 24, "endOffset": 28}, {"referenceID": 43, "context": "Less obviously, the statistical leverage scores play a crucial role in recent work on randomized matrix algorithms: they define the key structural nonuniformity that must be dealt with in order to obtain high-quality low-rank and least-squares approximation of general matrices via random sampling and random projection methods [45].", "startOffset": 328, "endOffset": 332}, {"referenceID": 18, "context": "Moreover, they can be approximated more quickly than the time required to compute that basis with a truncated SVD or a QR decomposition [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 59, "context": ", kernel matrices and Laplacian matrices, where this low-rank approximation approach (in particular, the sampling-based approach) is often called the Nystr\u00f6m method [61, 21, 38].", "startOffset": 165, "endOffset": 177}, {"referenceID": 19, "context": ", kernel matrices and Laplacian matrices, where this low-rank approximation approach (in particular, the sampling-based approach) is often called the Nystr\u00f6m method [61, 21, 38].", "startOffset": 165, "endOffset": 177}, {"referenceID": 36, "context": ", kernel matrices and Laplacian matrices, where this low-rank approximation approach (in particular, the sampling-based approach) is often called the Nystr\u00f6m method [61, 21, 38].", "startOffset": 165, "endOffset": 177}, {"referenceID": 59, "context": "The Nystr\u00f6m method\u2014both randomized and deterministic variants\u2014has proven useful in applications where the kernel matrices are reasonably well-approximated by low-rank matrices; and it has been applied to Gaussian process regression, spectral clustering and image segmentation, manifold learning, and a range of other common machine learning tasks [61, 60, 25, 56, 65, 38].", "startOffset": 347, "endOffset": 371}, {"referenceID": 58, "context": "The Nystr\u00f6m method\u2014both randomized and deterministic variants\u2014has proven useful in applications where the kernel matrices are reasonably well-approximated by low-rank matrices; and it has been applied to Gaussian process regression, spectral clustering and image segmentation, manifold learning, and a range of other common machine learning tasks [61, 60, 25, 56, 65, 38].", "startOffset": 347, "endOffset": 371}, {"referenceID": 23, "context": "The Nystr\u00f6m method\u2014both randomized and deterministic variants\u2014has proven useful in applications where the kernel matrices are reasonably well-approximated by low-rank matrices; and it has been applied to Gaussian process regression, spectral clustering and image segmentation, manifold learning, and a range of other common machine learning tasks [61, 60, 25, 56, 65, 38].", "startOffset": 347, "endOffset": 371}, {"referenceID": 54, "context": "The Nystr\u00f6m method\u2014both randomized and deterministic variants\u2014has proven useful in applications where the kernel matrices are reasonably well-approximated by low-rank matrices; and it has been applied to Gaussian process regression, spectral clustering and image segmentation, manifold learning, and a range of other common machine learning tasks [61, 60, 25, 56, 65, 38].", "startOffset": 347, "endOffset": 371}, {"referenceID": 63, "context": "The Nystr\u00f6m method\u2014both randomized and deterministic variants\u2014has proven useful in applications where the kernel matrices are reasonably well-approximated by low-rank matrices; and it has been applied to Gaussian process regression, spectral clustering and image segmentation, manifold learning, and a range of other common machine learning tasks [61, 60, 25, 56, 65, 38].", "startOffset": 347, "endOffset": 371}, {"referenceID": 36, "context": "The Nystr\u00f6m method\u2014both randomized and deterministic variants\u2014has proven useful in applications where the kernel matrices are reasonably well-approximated by low-rank matrices; and it has been applied to Gaussian process regression, spectral clustering and image segmentation, manifold learning, and a range of other common machine learning tasks [61, 60, 25, 56, 65, 38].", "startOffset": 347, "endOffset": 371}, {"referenceID": 20, "context": "Often, \u201cfiltering\u201d a low-rank approximation in this way through a (lower) rank-k space has a regularization effect: for example, relative-error CUR matrix decompositions are implicitly regularized by letting the \u201cmiddle matrix\u201d have rank no greater than k [22, 47]; and [15] considers a regularization of the uniform column sampling Nystr\u00f6m extension where, before forming the extension, all singular values of W smaller than a threshold are truncated to zero.", "startOffset": 256, "endOffset": 264}, {"referenceID": 45, "context": "Often, \u201cfiltering\u201d a low-rank approximation in this way through a (lower) rank-k space has a regularization effect: for example, relative-error CUR matrix decompositions are implicitly regularized by letting the \u201cmiddle matrix\u201d have rank no greater than k [22, 47]; and [15] considers a regularization of the uniform column sampling Nystr\u00f6m extension where, before forming the extension, all singular values of W smaller than a threshold are truncated to zero.", "startOffset": 256, "endOffset": 264}, {"referenceID": 13, "context": "Often, \u201cfiltering\u201d a low-rank approximation in this way through a (lower) rank-k space has a regularization effect: for example, relative-error CUR matrix decompositions are implicitly regularized by letting the \u201cmiddle matrix\u201d have rank no greater than k [22, 47]; and [15] considers a regularization of the uniform column sampling Nystr\u00f6m extension where, before forming the extension, all singular values of W smaller than a threshold are truncated to zero.", "startOffset": 270, "endOffset": 274}, {"referenceID": 8, "context": "While these methods often perform well in practice [10, 9, 24, 38], rigorous analyses of them are hard to come by\u2014interested readers are referred to the discussion in [24, 38].", "startOffset": 51, "endOffset": 66}, {"referenceID": 7, "context": "While these methods often perform well in practice [10, 9, 24, 38], rigorous analyses of them are hard to come by\u2014interested readers are referred to the discussion in [24, 38].", "startOffset": 51, "endOffset": 66}, {"referenceID": 22, "context": "While these methods often perform well in practice [10, 9, 24, 38], rigorous analyses of them are hard to come by\u2014interested readers are referred to the discussion in [24, 38].", "startOffset": 51, "endOffset": 66}, {"referenceID": 36, "context": "While these methods often perform well in practice [10, 9, 24, 38], rigorous analyses of them are hard to come by\u2014interested readers are referred to the discussion in [24, 38].", "startOffset": 51, "endOffset": 66}, {"referenceID": 22, "context": "While these methods often perform well in practice [10, 9, 24, 38], rigorous analyses of them are hard to come by\u2014interested readers are referred to the discussion in [24, 38].", "startOffset": 167, "endOffset": 175}, {"referenceID": 36, "context": "While these methods often perform well in practice [10, 9, 24, 38], rigorous analyses of them are hard to come by\u2014interested readers are referred to the discussion in [24, 38].", "startOffset": 167, "endOffset": 175}, {"referenceID": 43, "context": "Motivated by large-scale data analysis and machine learning applications, recent theoretical and empirical work has focused on \u201csketching\u201d methods such as random sampling and random projection algorithms; a large part of the recent body of this work on randomized matrix algorithms has been summarized in the recent monograph of Mahoney [45] and the recent review article of Halko, Martinsson, and Tropp [31].", "startOffset": 337, "endOffset": 341}, {"referenceID": 29, "context": "Motivated by large-scale data analysis and machine learning applications, recent theoretical and empirical work has focused on \u201csketching\u201d methods such as random sampling and random projection algorithms; a large part of the recent body of this work on randomized matrix algorithms has been summarized in the recent monograph of Mahoney [45] and the recent review article of Halko, Martinsson, and Tropp [31].", "startOffset": 404, "endOffset": 408}, {"referenceID": 10, "context": ", [12, 26, 59] and [6]) and random sampling methods (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 24, "context": ", [12, 26, 59] and [6]) and random sampling methods (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 57, "context": ", [12, 26, 59] and [6]) and random sampling methods (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 4, "context": ", [12, 26, 59] and [6]) and random sampling methods (e.", "startOffset": 19, "endOffset": 22}, {"referenceID": 51, "context": ", [53, 47]) have been used in applications for clustering and classification of general data matrices; and that some of this work has highlighted the importance of the statistical leverage scores that we use in this paper [53, 47, 45, 63].", "startOffset": 2, "endOffset": 10}, {"referenceID": 45, "context": ", [53, 47]) have been used in applications for clustering and classification of general data matrices; and that some of this work has highlighted the importance of the statistical leverage scores that we use in this paper [53, 47, 45, 63].", "startOffset": 2, "endOffset": 10}, {"referenceID": 51, "context": ", [53, 47]) have been used in applications for clustering and classification of general data matrices; and that some of this work has highlighted the importance of the statistical leverage scores that we use in this paper [53, 47, 45, 63].", "startOffset": 222, "endOffset": 238}, {"referenceID": 45, "context": ", [53, 47]) have been used in applications for clustering and classification of general data matrices; and that some of this work has highlighted the importance of the statistical leverage scores that we use in this paper [53, 47, 45, 63].", "startOffset": 222, "endOffset": 238}, {"referenceID": 43, "context": ", [53, 47]) have been used in applications for clustering and classification of general data matrices; and that some of this work has highlighted the importance of the statistical leverage scores that we use in this paper [53, 47, 45, 63].", "startOffset": 222, "endOffset": 238}, {"referenceID": 61, "context": ", [53, 47]) have been used in applications for clustering and classification of general data matrices; and that some of this work has highlighted the importance of the statistical leverage scores that we use in this paper [53, 47, 45, 63].", "startOffset": 222, "endOffset": 238}, {"referenceID": 59, "context": "Originally used by Williams and Seeger to solve regression and classification problems involving Gaussian processes when the SPSD matrix A is well-approximated by a low-rank matrix [61, 60], the Nystr\u00f6m extension has been used in a large body of subsequent work.", "startOffset": 181, "endOffset": 189}, {"referenceID": 58, "context": "Originally used by Williams and Seeger to solve regression and classification problems involving Gaussian processes when the SPSD matrix A is well-approximated by a low-rank matrix [61, 60], the Nystr\u00f6m extension has been used in a large body of subsequent work.", "startOffset": 181, "endOffset": 189}, {"referenceID": 54, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 97, "endOffset": 113}, {"referenceID": 34, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 97, "endOffset": 113}, {"referenceID": 35, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 97, "endOffset": 113}, {"referenceID": 41, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 97, "endOffset": 113}, {"referenceID": 64, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 118, "endOffset": 130}, {"referenceID": 38, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 118, "endOffset": 130}, {"referenceID": 63, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 118, "endOffset": 130}, {"referenceID": 50, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 193, "endOffset": 218}, {"referenceID": 5, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 193, "endOffset": 218}, {"referenceID": 9, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 193, "endOffset": 218}, {"referenceID": 53, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 193, "endOffset": 218}, {"referenceID": 6, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 193, "endOffset": 218}, {"referenceID": 8, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 193, "endOffset": 218}, {"referenceID": 7, "context": "For example, applications of the Nystr\u00f6m method to large-scale machine learning problems include [56, 36, 37, 43] and [66, 40, 65], and applications in statistics and signal processing include [52, 7, 11, 55, 8, 10, 9].", "startOffset": 193, "endOffset": 218}, {"referenceID": 64, "context": ", [66, 64, 41, 1, 40]) and/or coupling the method with downstream applications (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 62, "context": ", [66, 64, 41, 1, 40]) and/or coupling the method with downstream applications (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 39, "context": ", [66, 64, 41, 1, 40]) and/or coupling the method with downstream applications (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": ", [66, 64, 41, 1, 40]) and/or coupling the method with downstream applications (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 38, "context": ", [66, 64, 41, 1, 40]) and/or coupling the method with downstream applications (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 3, "context": ", [5, 17, 33, 32, 42, 4]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 15, "context": ", [5, 17, 33, 32, 42, 4]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 31, "context": ", [5, 17, 33, 32, 42, 4]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 30, "context": ", [5, 17, 33, 32, 42, 4]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 40, "context": ", [5, 17, 33, 32, 42, 4]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 2, "context": ", [5, 17, 33, 32, 42, 4]).", "startOffset": 2, "endOffset": 24}, {"referenceID": 36, "context": "The most detailed results are provided by [38] (as well as the conference papers on which it is based [36, 35, 37]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 34, "context": "The most detailed results are provided by [38] (as well as the conference papers on which it is based [36, 35, 37]).", "startOffset": 102, "endOffset": 114}, {"referenceID": 33, "context": "The most detailed results are provided by [38] (as well as the conference papers on which it is based [36, 35, 37]).", "startOffset": 102, "endOffset": 114}, {"referenceID": 35, "context": "The most detailed results are provided by [38] (as well as the conference papers on which it is based [36, 35, 37]).", "startOffset": 102, "endOffset": 114}, {"referenceID": 55, "context": "Interestingly, they observe that uniform sampling performs quite well, suggesting that in the data they considered the leverage scores are quite uniform, which also motivated the related work [57, 50].", "startOffset": 192, "endOffset": 200}, {"referenceID": 48, "context": "Interestingly, they observe that uniform sampling performs quite well, suggesting that in the data they considered the leverage scores are quite uniform, which also motivated the related work [57, 50].", "startOffset": 192, "endOffset": 200}, {"referenceID": 51, "context": "This is in contrast with applications in genetics [53], term-document analysis [47], and astronomy [63], where the statistical leverage scores were seen to be very nonuniform in ways of interest to the downstream scientist; we return to this issue in Section 3.", "startOffset": 50, "endOffset": 54}, {"referenceID": 45, "context": "This is in contrast with applications in genetics [53], term-document analysis [47], and astronomy [63], where the statistical leverage scores were seen to be very nonuniform in ways of interest to the downstream scientist; we return to this issue in Section 3.", "startOffset": 79, "endOffset": 83}, {"referenceID": 61, "context": "This is in contrast with applications in genetics [53], term-document analysis [47], and astronomy [63], where the statistical leverage scores were seen to be very nonuniform in ways of interest to the downstream scientist; we return to this issue in Section 3.", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "On the theoretical side, much of the work has followed that of Drineas and Mahoney [21], who provided the first rigorous bounds for the Nystr\u00f6m extension of a general SPSD matrix.", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "(Actually, they prove a stronger result of the form given in Equation (4), except with W\u2020 replaced with W k, where Wk represents the best rank-k approximation to W [21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 34, "context": ", A = CW\u2020CT , with high probability [36].", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "Gittens extends this to the case where A is only approximately low-rank [28].", "startOffset": 72, "endOffset": 76}, {"referenceID": 34, "context": "For example, Equation (4) provides an additiveerror approximation with a very large scale; the bounds of Kumar, Mohri, and Talwalkar require a sampling complexity that depends on the coherence of the input matrix [36], which means that unless the coherence is very low one needs to sample essentially all the rows and columns in order to reconstruct the matrix; Equation (5) provides a bound where the additive scale depends on n; and Equation (6) provides a spectral norm bound where the scale of the additional error is the (much larger) trace norm.", "startOffset": 213, "endOffset": 217}, {"referenceID": 19, "context": "sketch type ` \u2016A\u2212CW\u2020CT \u20162 \u2016A\u2212CW\u2020CT \u2016F \u2016A\u2212CW\u2020CT \u2016? Prior works [21], Nystr\u00f6m \u03a9(\u03b5\u22124k) opt2 + \u03b5 \u2211n i=A 2 ii optF + \u03b5 \u2211n i=1A 2 ii \u2013 [10], Nystr\u00f6m \u03a9(1) \u2013 \u2013 O ( n\u2212` n ) \u2016A\u2016? [57], Nystr\u00f6m \u03a9(\u03c4r ln r) 0 0 0 [38], Nystr\u00f6m \u03a9(1) opt2 + O( 2n \u221a ` ) \u2016A\u20162 optF + O(n( k ` ) ) \u2016A\u20162 \u2013 This work Nystr\u00f6m \u03a9((1\u2212 \u03b5)\u22121\u03bck ln k) opt2(1 + n \u03b5` ) optF (1 + \u221a \u03b5\u22121) + \u03b5opt? opt?(1 + \u03b5 \u22121) leverage-based \u03a9((\u03b2\u03b5)\u22121k ln(k/\u03b2)) opt2 + \u03b5opt? (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Fourier-based \u03a9(\u03b5\u22121k lnn ln k) \u03b5 (1\u2212 \u221a \u03b5) ln k ( opt2 + 1 lnnopt? ) (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Gaussian-based \u03a9(k\u03b5\u22121) \u03b5 ln k k opt2 + \u03b5 kopt? (1 + \u221a \u03b5)optF + \u221a \u03b5opt? (1 + \u03b5)opt? Table 1.", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "sketch type ` \u2016A\u2212CW\u2020CT \u20162 \u2016A\u2212CW\u2020CT \u2016F \u2016A\u2212CW\u2020CT \u2016? Prior works [21], Nystr\u00f6m \u03a9(\u03b5\u22124k) opt2 + \u03b5 \u2211n i=A 2 ii optF + \u03b5 \u2211n i=1A 2 ii \u2013 [10], Nystr\u00f6m \u03a9(1) \u2013 \u2013 O ( n\u2212` n ) \u2016A\u2016? [57], Nystr\u00f6m \u03a9(\u03c4r ln r) 0 0 0 [38], Nystr\u00f6m \u03a9(1) opt2 + O( 2n \u221a ` ) \u2016A\u20162 optF + O(n( k ` ) ) \u2016A\u20162 \u2013 This work Nystr\u00f6m \u03a9((1\u2212 \u03b5)\u22121\u03bck ln k) opt2(1 + n \u03b5` ) optF (1 + \u221a \u03b5\u22121) + \u03b5opt? opt?(1 + \u03b5 \u22121) leverage-based \u03a9((\u03b2\u03b5)\u22121k ln(k/\u03b2)) opt2 + \u03b5opt? (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Fourier-based \u03a9(\u03b5\u22121k lnn ln k) \u03b5 (1\u2212 \u221a \u03b5) ln k ( opt2 + 1 lnnopt? ) (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Gaussian-based \u03a9(k\u03b5\u22121) \u03b5 ln k k opt2 + \u03b5 kopt? (1 + \u221a \u03b5)optF + \u221a \u03b5opt? (1 + \u03b5)opt? Table 1.", "startOffset": 129, "endOffset": 133}, {"referenceID": 55, "context": "sketch type ` \u2016A\u2212CW\u2020CT \u20162 \u2016A\u2212CW\u2020CT \u2016F \u2016A\u2212CW\u2020CT \u2016? Prior works [21], Nystr\u00f6m \u03a9(\u03b5\u22124k) opt2 + \u03b5 \u2211n i=A 2 ii optF + \u03b5 \u2211n i=1A 2 ii \u2013 [10], Nystr\u00f6m \u03a9(1) \u2013 \u2013 O ( n\u2212` n ) \u2016A\u2016? [57], Nystr\u00f6m \u03a9(\u03c4r ln r) 0 0 0 [38], Nystr\u00f6m \u03a9(1) opt2 + O( 2n \u221a ` ) \u2016A\u20162 optF + O(n( k ` ) ) \u2016A\u20162 \u2013 This work Nystr\u00f6m \u03a9((1\u2212 \u03b5)\u22121\u03bck ln k) opt2(1 + n \u03b5` ) optF (1 + \u221a \u03b5\u22121) + \u03b5opt? opt?(1 + \u03b5 \u22121) leverage-based \u03a9((\u03b2\u03b5)\u22121k ln(k/\u03b2)) opt2 + \u03b5opt? (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Fourier-based \u03a9(\u03b5\u22121k lnn ln k) \u03b5 (1\u2212 \u221a \u03b5) ln k ( opt2 + 1 lnnopt? ) (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Gaussian-based \u03a9(k\u03b5\u22121) \u03b5 ln k k opt2 + \u03b5 kopt? (1 + \u221a \u03b5)optF + \u221a \u03b5opt? (1 + \u03b5)opt? Table 1.", "startOffset": 169, "endOffset": 173}, {"referenceID": 36, "context": "sketch type ` \u2016A\u2212CW\u2020CT \u20162 \u2016A\u2212CW\u2020CT \u2016F \u2016A\u2212CW\u2020CT \u2016? Prior works [21], Nystr\u00f6m \u03a9(\u03b5\u22124k) opt2 + \u03b5 \u2211n i=A 2 ii optF + \u03b5 \u2211n i=1A 2 ii \u2013 [10], Nystr\u00f6m \u03a9(1) \u2013 \u2013 O ( n\u2212` n ) \u2016A\u2016? [57], Nystr\u00f6m \u03a9(\u03c4r ln r) 0 0 0 [38], Nystr\u00f6m \u03a9(1) opt2 + O( 2n \u221a ` ) \u2016A\u20162 optF + O(n( k ` ) ) \u2016A\u20162 \u2013 This work Nystr\u00f6m \u03a9((1\u2212 \u03b5)\u22121\u03bck ln k) opt2(1 + n \u03b5` ) optF (1 + \u221a \u03b5\u22121) + \u03b5opt? opt?(1 + \u03b5 \u22121) leverage-based \u03a9((\u03b2\u03b5)\u22121k ln(k/\u03b2)) opt2 + \u03b5opt? (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Fourier-based \u03a9(\u03b5\u22121k lnn ln k) \u03b5 (1\u2212 \u221a \u03b5) ln k ( opt2 + 1 lnnopt? ) (1 + \u221a \u03b5)optF + \u03b5opt? (1 + \u03b5)opt? Gaussian-based \u03a9(k\u03b5\u22121) \u03b5 ln k k opt2 + \u03b5 kopt? (1 + \u221a \u03b5)optF + \u221a \u03b5opt? (1 + \u03b5)opt? Table 1.", "startOffset": 200, "endOffset": 204}, {"referenceID": 19, "context": "With the exception of [21], which samples columns with probability proportional to their Euclidean norms, and our novel leverage-based Nystr\u00f6m bound, these bounds are for sampling columns or linear combinations of columns uniformly at random.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "The large additional error in the spectral norm error bound is necessary in the worse case [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 55, "context": "Table 2 illustrates the gap between the theoretical results currently available in the literature and what is observed in practice: it depicts the ratio between the error bounds in Table 1 and the average errors observed over 10 runs of the SPSD approximation algorithms (the error bound from [57] is not considered in the table, as it does not apply at the number of samples ` used in the experiments).", "startOffset": 293, "endOffset": 297}, {"referenceID": 19, "context": "trace error Enron, k = 60 [21], Nystr\u00f6m 3041 66.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "2 \u2013 [10], Nystr\u00f6m \u2013 \u2013 2.", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "0 [38], Nystr\u00f6m 331.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "4 Protein, k = 10 [21], Nystr\u00f6m 119.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "6 \u2013 [10], Nystr\u00f6m \u2013 \u2013 3.", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "6 [38], Nystr\u00f6m 33.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "15, k = 20 [21], Nystr\u00f6m 349.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "5 \u2013 [10], Nystr\u00f6m \u2013 \u2013 2.", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "0 [38], Nystr\u00f6m 62.", "startOffset": 2, "endOffset": 6}, {"referenceID": 19, "context": "1 WineS, \u03c3 = 1, k = 20 [21], Nystr\u00f6m 422.", "startOffset": 23, "endOffset": 27}, {"referenceID": 8, "context": "0 \u2013 [10], Nystr\u00f6m \u2013 \u2013 2.", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "1 [38], Nystr\u00f6m 72.", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": "For instance, prima facie, algorithms based on leverage-based column sampling might be expected to be more expensive than those based on uniform column sampling or random projections, but (based on previous work for general matrices [22, 23, 45]) they might also be expected to deliver lower approximation errors.", "startOffset": 233, "endOffset": 245}, {"referenceID": 21, "context": "For instance, prima facie, algorithms based on leverage-based column sampling might be expected to be more expensive than those based on uniform column sampling or random projections, but (based on previous work for general matrices [22, 23, 45]) they might also be expected to deliver lower approximation errors.", "startOffset": 233, "endOffset": 245}, {"referenceID": 43, "context": "For instance, prima facie, algorithms based on leverage-based column sampling might be expected to be more expensive than those based on uniform column sampling or random projections, but (based on previous work for general matrices [22, 23, 45]) they might also be expected to deliver lower approximation errors.", "startOffset": 233, "endOffset": 245}, {"referenceID": 37, "context": "The data sets used in our empirical evaluation ([39], [34], [30], [29], [51], [16], [2]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 32, "context": "The data sets used in our empirical evaluation ([39], [34], [30], [29], [51], [16], [2]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 28, "context": "The data sets used in our empirical evaluation ([39], [34], [30], [29], [51], [16], [2]).", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "The data sets used in our empirical evaluation ([39], [34], [30], [29], [51], [16], [2]).", "startOffset": 66, "endOffset": 70}, {"referenceID": 49, "context": "The data sets used in our empirical evaluation ([39], [34], [30], [29], [51], [16], [2]).", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "The data sets used in our empirical evaluation ([39], [34], [30], [29], [51], [16], [2]).", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "When \u03bd is larger than (d + 1)/2, this matrix is positive semidefinite; and as the cutoff point C decreases this matrix becomes more sparse [27].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "(We should note, however, that there are exceptions to this, where one observes very strong localization on low-order eigenvectors of data matrices [18].", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "rank of the low-rank approximation to be no greater than k by projecting onto the best rank-k approximation to the original matrix [22].", "startOffset": 131, "endOffset": 135}, {"referenceID": 45, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 172, "endOffset": 180}, {"referenceID": 43, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 172, "endOffset": 180}, {"referenceID": 20, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 417, "endOffset": 429}, {"referenceID": 21, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 417, "endOffset": 429}, {"referenceID": 43, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 417, "endOffset": 429}, {"referenceID": 51, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 517, "endOffset": 533}, {"referenceID": 45, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 517, "endOffset": 533}, {"referenceID": 43, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 517, "endOffset": 533}, {"referenceID": 61, "context": "Finally, we note that previous work has shown that the statistical leverage scores reflect an important nonuniformity structure in the columns of the general data matrices [47, 45]; that randomly sampling columns according to this distribution results in lower worst-case error (for problems such as leastsquares approximation and low-rank approximation of general matrices) than sampling columns uniformly at random [22, 23, 45]; and that leverage scores have proven useful in a wide range of practical applications [53, 47, 45, 63].", "startOffset": 517, "endOffset": 533}, {"referenceID": 54, "context": "These linear kernels (and also to some extent the dense RBF kernels below that have larger \u03c3 parameter) are examples of relatively \u201cnice\u201d machine learning data sets that are similar to matrices where uniform sampling has been shown to perform well previously [56, 36, 37, 38]; and for these matrices our empirical results agree with these prior works.", "startOffset": 259, "endOffset": 275}, {"referenceID": 34, "context": "These linear kernels (and also to some extent the dense RBF kernels below that have larger \u03c3 parameter) are examples of relatively \u201cnice\u201d machine learning data sets that are similar to matrices where uniform sampling has been shown to perform well previously [56, 36, 37, 38]; and for these matrices our empirical results agree with these prior works.", "startOffset": 259, "endOffset": 275}, {"referenceID": 35, "context": "These linear kernels (and also to some extent the dense RBF kernels below that have larger \u03c3 parameter) are examples of relatively \u201cnice\u201d machine learning data sets that are similar to matrices where uniform sampling has been shown to perform well previously [56, 36, 37, 38]; and for these matrices our empirical results agree with these prior works.", "startOffset": 259, "endOffset": 275}, {"referenceID": 36, "context": "These linear kernels (and also to some extent the dense RBF kernels below that have larger \u03c3 parameter) are examples of relatively \u201cnice\u201d machine learning data sets that are similar to matrices where uniform sampling has been shown to perform well previously [56, 36, 37, 38]; and for these matrices our empirical results agree with these prior works.", "startOffset": 259, "endOffset": 275}, {"referenceID": 19, "context": ", [21, 38, 28]) would suggest.", "startOffset": 2, "endOffset": 14}, {"referenceID": 36, "context": ", [21, 38, 28]) would suggest.", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": ", [21, 38, 28]) would suggest.", "startOffset": 2, "endOffset": 14}, {"referenceID": 43, "context": ") This observation is intriguing, because the motivation of leverage score sampling (and, recall, that in this context random projections should be viewed as performing uniform random sampling in a randomly-rotated basis where the leverage scores have been approximately uniformized [45]) is very much tied to the Frobenius norm, and so there is no a priori reason to expect its good performance to extend to the spectral or trace norms.", "startOffset": 283, "endOffset": 287}, {"referenceID": 18, "context": "As shown below, by using the recently developed algorithm of [20], not only does this approximation algorithm run in time comparable with random projections (for certain parameter settings), but it leads to approximations that soften the strong bias that the exact leverage scores provide toward the best rank-k approximation to the matrix, thereby leading to improved reconstruction results in many cases.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "Recent work, however, has shown that relative-error approximations to all the statistical leverage scores can be computed more quickly than this exact algorithm [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 18, "context": "Algorithm 1: Algorithm (originally Algorithm 1 in [20]) for approximating the leverage scores `i of an n\u00d7 d matrix A, where n d, to within a multiplicative factor of 1\u00b1 .", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "Algorithm 2: Algorithm (originally Algorithm 4 in [20]) for approximating the leverage scores (relative to the best rank-k approximation to A) of a general n\u00d7 d matrix A with those of a matrix that is close by in the spectral norm.", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "of the leverage score approximation algorithms of [20], illustrating empirically the tradeoffs between cost and efficiency in a practical setting.", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "Algorithm 1 (which originally appeared as Algorithm 1 in [20]) takes as input an arbitrary n\u00d7dmatrix A, where n d, and it returns as output a 1\u00b1 approximation to all of the statistical leverage scores of the input matrix.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "The original algorithm of [20] uses a subsampled Hadamard transform and requires r1 to be somewhat larger.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "That an SRFT with a smaller value of r1 can be used instead is a consequence of the fact that Lemma 3 in [20] is also satisfied by an SRFT matrix with the given r1; this is established in [58, 13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 56, "context": "That an SRFT with a smaller value of r1 can be used instead is a consequence of the fact that Lemma 3 in [20] is also satisfied by an SRFT matrix with the given r1; this is established in [58, 13].", "startOffset": 188, "endOffset": 196}, {"referenceID": 11, "context": "That an SRFT with a smaller value of r1 can be used instead is a consequence of the fact that Lemma 3 in [20] is also satisfied by an SRFT matrix with the given r1; this is established in [58, 13].", "startOffset": 188, "endOffset": 196}, {"referenceID": 18, "context": "SRFT, premultiplying by it takes roughly O(nd ln d) time, and \u03a01A needs to be post multiplied by a second random projection in order to compute all of the leverage scores in the allotted time; see [20] for details.", "startOffset": 197, "endOffset": 201}, {"referenceID": 18, "context": "Consider, next, Algorithm 2 (which originally appeared as Algorithm 4 in [20]), which takes as input an arbitrary n\u00d7 d matrix A and a rank parameter k, and returns as output a 1\u00b1 approximation to all of the statistical leverage scores (relative to the best rank-k approximation) of the input.", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "See [20] for details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "Thus, one could use this algorithm to compute approximations to the leverage scores to obtain relative-error approximations to a least-squares problem involving A [22, 23, 45], or one could use the sketch thereby obtained as a preconditioner to an iterative method to solve the least-squares problem, in a manner analogous to how Blendenpik or LSRN does so with a random projection [3, 49].", "startOffset": 163, "endOffset": 175}, {"referenceID": 21, "context": "Thus, one could use this algorithm to compute approximations to the leverage scores to obtain relative-error approximations to a least-squares problem involving A [22, 23, 45], or one could use the sketch thereby obtained as a preconditioner to an iterative method to solve the least-squares problem, in a manner analogous to how Blendenpik or LSRN does so with a random projection [3, 49].", "startOffset": 163, "endOffset": 175}, {"referenceID": 43, "context": "Thus, one could use this algorithm to compute approximations to the leverage scores to obtain relative-error approximations to a least-squares problem involving A [22, 23, 45], or one could use the sketch thereby obtained as a preconditioner to an iterative method to solve the least-squares problem, in a manner analogous to how Blendenpik or LSRN does so with a random projection [3, 49].", "startOffset": 163, "endOffset": 175}, {"referenceID": 1, "context": "Thus, one could use this algorithm to compute approximations to the leverage scores to obtain relative-error approximations to a least-squares problem involving A [22, 23, 45], or one could use the sketch thereby obtained as a preconditioner to an iterative method to solve the least-squares problem, in a manner analogous to how Blendenpik or LSRN does so with a random projection [3, 49].", "startOffset": 382, "endOffset": 389}, {"referenceID": 47, "context": "Thus, one could use this algorithm to compute approximations to the leverage scores to obtain relative-error approximations to a least-squares problem involving A [22, 23, 45], or one could use the sketch thereby obtained as a preconditioner to an iterative method to solve the least-squares problem, in a manner analogous to how Blendenpik or LSRN does so with a random projection [3, 49].", "startOffset": 382, "endOffset": 389}, {"referenceID": 19, "context": ", [21, 38, 28]) would suggest.", "startOffset": 2, "endOffset": 14}, {"referenceID": 36, "context": ", [21, 38, 28]) would suggest.", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": ", [21, 38, 28]) would suggest.", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": "In [28], it is shown that", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "for any arbitrary matrix X [14, 31].", "startOffset": 27, "endOffset": 35}, {"referenceID": 29, "context": "for any arbitrary matrix X [14, 31].", "startOffset": 27, "endOffset": 35}, {"referenceID": 19, "context": "A much weaker form of this was used in [21], but the stronger form that we use here in Equation (9) was first proved in [28].", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": "A much weaker form of this was used in [21], but the stronger form that we use here in Equation (9) was first proved in [28].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "A bound of the form of Equation (10) was originally proven in [14] for solving the Column Subset Selection Problem, and it was improved in [31], where it was applied to a random projection algorithm.", "startOffset": 62, "endOffset": 66}, {"referenceID": 29, "context": "A bound of the form of Equation (10) was originally proven in [14] for solving the Column Subset Selection Problem, and it was improved in [31], where it was applied to a random projection algorithm.", "startOffset": 139, "endOffset": 143}, {"referenceID": 26, "context": "In [28], it is shown that CW\u2020CT = APA1/2SA , and from this it follows that \u2225\u2225A\u2212CW\u2020CT\u2225\u2225 F = \u2225\u2225\u2225A1/2(I\u2212PA1/2S)A1/2\u2225\u2225\u2225 F .", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [31], it is shown that", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Likewise, the fact that I\u2212 (I + FTF)\u22121 FF (which is shown in [31]) implies that we can bound T1 as T1 \u2264 \u2225\u2225\u2225\u03a31/2 1 FTF\u03a3 1 \u2225\u2225\u22252 F = \u2225\u2225\u2225(\u03a31/2 2 \u03a92\u03a9\u20201)T\u03a31/2 2 \u03a92\u03a9\u20201\u2225\u2225\u22252 F \u2264 \u2225\u2225\u2225\u03a31/2 2 \u03a92\u03a9\u20201\u2225\u2225\u22254 F .", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "Recall the estimate I\u2212 (I+FTF)\u22121 FF (shown in [31]) and the basic estimate I\u2212F(I+FTF)\u22121FT I.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Work on approximating the product of matrices by random sampling shows that to obtain non-trivial bounds one must sample with respect to the norm of the rank-1 components [19], which here (since we are approximating the product of two orthogonal matrices) equal the statistical leverage scores.", "startOffset": 171, "endOffset": 175}, {"referenceID": 21, "context": "From this perspective, random projections satisfy this condition since (informally) they rotate to a random basis where the leverage scores of the rotated matrix are approximately uniform and thus where uniform sampling is appropriate [23, 45].", "startOffset": 235, "endOffset": 243}, {"referenceID": 43, "context": "From this perspective, random projections satisfy this condition since (informally) they rotate to a random basis where the leverage scores of the rotated matrix are approximately uniform and thus where uniform sampling is appropriate [23, 45].", "startOffset": 235, "endOffset": 243}, {"referenceID": 2, "context": "As observed recently [4], methods that use knowledge of a matrix square root \u03a6 (i.", "startOffset": 21, "endOffset": 24}, {"referenceID": 43, "context": "These running times depend sensitively on the size of the data and the model of data access; see [45, 31] for detailed discussions of these issues.", "startOffset": 97, "endOffset": 105}, {"referenceID": 29, "context": "These running times depend sensitively on the size of the data and the model of data access; see [45, 31] for detailed discussions of these issues.", "startOffset": 97, "endOffset": 105}, {"referenceID": 18, "context": ") However, the randomized algorithm of [20] that computes relative-error approximations to all of the statistical leverage in a time that is qualitatively faster\u2014in worst-case theory and, by using existing highquality randomized numerical code [3, 49, 31], in practice\u2014gets around this bottleneck, as was shown in Section 3.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": ") However, the randomized algorithm of [20] that computes relative-error approximations to all of the statistical leverage in a time that is qualitatively faster\u2014in worst-case theory and, by using existing highquality randomized numerical code [3, 49, 31], in practice\u2014gets around this bottleneck, as was shown in Section 3.", "startOffset": 244, "endOffset": 255}, {"referenceID": 47, "context": ") However, the randomized algorithm of [20] that computes relative-error approximations to all of the statistical leverage in a time that is qualitatively faster\u2014in worst-case theory and, by using existing highquality randomized numerical code [3, 49, 31], in practice\u2014gets around this bottleneck, as was shown in Section 3.", "startOffset": 244, "endOffset": 255}, {"referenceID": 29, "context": ") However, the randomized algorithm of [20] that computes relative-error approximations to all of the statistical leverage in a time that is qualitatively faster\u2014in worst-case theory and, by using existing highquality randomized numerical code [3, 49, 31], in practice\u2014gets around this bottleneck, as was shown in Section 3.", "startOffset": 244, "endOffset": 255}, {"referenceID": 18, "context": "The computational bottleneck for the algorithms of [20] is that of applying a random projection, and thus the running time for leverage-based Nystr\u00f6m extension is that of applying a (\u201cfast\u201d Fourier-based or \u201cslow\u201d Gaussian-based, as appropriate) random projection to A [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "The computational bottleneck for the algorithms of [20] is that of applying a random projection, and thus the running time for leverage-based Nystr\u00f6m extension is that of applying a (\u201cfast\u201d Fourier-based or \u201cslow\u201d Gaussian-based, as appropriate) random projection to A [20].", "startOffset": 269, "endOffset": 273}, {"referenceID": 1, "context": "See Section 3 or [3, 49, 31] for additional details.", "startOffset": 17, "endOffset": 28}, {"referenceID": 47, "context": "See Section 3 or [3, 49, 31] for additional details.", "startOffset": 17, "endOffset": 28}, {"referenceID": 29, "context": "See Section 3 or [3, 49, 31] for additional details.", "startOffset": 17, "endOffset": 28}, {"referenceID": 18, "context": ", in case the scores are computed only approximately with the fast algorithm of [20]), we formulate the following lemma in terms of any probability distribution that is \u03b2-close to the leverage score distribution.", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "The additive scale factors for the spectral and Frobenius norm bounds are much improved relative to the prior results of [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": ", how to satisfy the condition in Theorems 1, 2, and 3 that \u03a91 has full row rank) in a more refined way than the importance sampling probabilities of [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "These improvements come at additional computational expense, but we remind the reader that leverage-based sampling probabilities of the form used by Lemma 1 can be computed faster than the time needed to compute the basis U1 [20].", "startOffset": 225, "endOffset": 229}, {"referenceID": 18, "context": "The computational bottleneck of the algorithm of [20] is the time required to perform a random projection on the input matrix.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "This has been observed previously [22, 47].", "startOffset": 34, "endOffset": 42}, {"referenceID": 45, "context": "This has been observed previously [22, 47].", "startOffset": 34, "endOffset": 42}, {"referenceID": 29, "context": "The difference between the number of samples necessary for Fourier-based sketches and Gaussian-based sketches is reflective of the differing natures of the random projections: the geometry of any k-dimensional subspace is preserved under projection onto the span of ` = O(k) Gaussian random vectors [31], but the sharpest analysis available suggests that to preserve the geometry of such a subspace under projection onto the span of ` SRFT vectors, ` must satisfy ` = \u03a9(max{k, lnn} ln k) [58].", "startOffset": 299, "endOffset": 303}, {"referenceID": 56, "context": "The difference between the number of samples necessary for Fourier-based sketches and Gaussian-based sketches is reflective of the differing natures of the random projections: the geometry of any k-dimensional subspace is preserved under projection onto the span of ` = O(k) Gaussian random vectors [31], but the sharpest analysis available suggests that to preserve the geometry of such a subspace under projection onto the span of ` SRFT vectors, ` must satisfy ` = \u03a9(max{k, lnn} ln k) [58].", "startOffset": 488, "endOffset": 492}, {"referenceID": 43, "context": "(informally) Fourier-based (and other) random projections rotate to a random basis where the leverage scores are approximately uniform and thus where uniform sampling is appropriate [45].", "startOffset": 182, "endOffset": 186}, {"referenceID": 18, "context": "The running times of the Fourier-based and the leverage-based algorithms are the same, to leading order, if the algorithm of [20] (which uses the same transform S = \u221a n `DHR) is used to approximate the leverage scores.", "startOffset": 125, "endOffset": 129}, {"referenceID": 43, "context": "The former case has proven to be a useful parameterization for certain numerical implementations [45, 31]; we already see that the guarantees here are as good as those for the leverage-based sampling algorithm with \u03a9(k ln k) samples, and better than those for Fourier-based sampling.", "startOffset": 97, "endOffset": 105}, {"referenceID": 29, "context": "The former case has proven to be a useful parameterization for certain numerical implementations [45, 31]; we already see that the guarantees here are as good as those for the leverage-based sampling algorithm with \u03a9(k ln k) samples, and better than those for Fourier-based sampling.", "startOffset": 97, "endOffset": 105}, {"referenceID": 26, "context": "In [28], it is shown that \u2225\u2225\u2225\u03a9\u20201\u2225\u2225\u22252 2 \u2264 n (1\u2212 \u03b5)` with probability at least 1\u2212 \u03b4 when ` satisfies the stated bound.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": ", [38, 28], these results for uniform sampling are much weaker than our bounds from the previous subsections, since the sampling complexity depends on the coherence of the input matrix.", "startOffset": 2, "endOffset": 10}, {"referenceID": 26, "context": ", [38, 28], these results for uniform sampling are much weaker than our bounds from the previous subsections, since the sampling complexity depends on the coherence of the input matrix.", "startOffset": 2, "endOffset": 10}, {"referenceID": 18, "context": "Recall that, by the algorithm of [20], the coherence of an arbitrary input matrix can be computed in roughly the time it takes to perform a random projection on the input matrix.", "startOffset": 33, "endOffset": 37}], "year": 2017, "abstractText": "We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds\u2014e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.", "creator": "LaTeX with hyperref package"}}}