{"id": "1303.2314", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2013", "title": "Mini-Batch Primal and Dual Methods for SVMs", "abstract": "We address the issue of using mini-batches in stochastic optimization of SVMs. We show that the same quantity, the spectral norm of the data, controls the parallelization speedup obtained for both primal stochastic subgradient descent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.", "histories": [["v1", "Sun, 10 Mar 2013 12:00:59 GMT  (53kb)", "http://arxiv.org/abs/1303.2314v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["martin tak\u00e1c", "avleen singh bijral", "peter richt\u00e1rik", "nati srebro"], "accepted": true, "id": "1303.2314"}, "pdf": {"name": "1303.2314.pdf", "metadata": {"source": "META", "title": "Mini-Batch Primal and Dual Methods for SVMs", "authors": [], "emails": ["martin.taki@gmail.com", "abijral@ttic.edu", "peter.richtarik@ed.ac.uk", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.23 14v1 [cs.LG] 1 0"}, {"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Support Vector Machines", "text": "We consider the optimization problem of training a Linear1 support vector machine (SVM) on the basis of n designated training examples (xi, yi) n i = 1, where xi, xi, xi and yi, \u00b1 1. We use X = [x1,..., xn] n to mark the matrix of training examples. We assume that the data will be standardized in such a way that maxi, xi and yi, xi, and thus the dependence on maxi, xi and xi is suppressed in all outcomes. Training an SVM corresponds to the determination of a linear predictor w that the matrix with low, 2-norm and small (empirical) average hinge losses L (w): = 1 (yi < w, xi >), where we use a linear predictor w Rd with low, 2-norm, w and small (empirical)."}, {"heading": "3. Mini-Batches in Primal Stochastic Gradient Descent Methods", "text": "(USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) () (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) (USA) ("}, {"heading": "4. Mini-Batches in Dual Stochastic Coordinate Ascent Methods", "text": "An alternative stochastic method to Pegasos is Stochastic Dual Coordinate Ascent (SDCA, Hsieh et al. 2008), which aims to solve the dual problem (2). In each iteration, we select a single training example (xi, yi) that is uniformly random, according to a single dual variable (coordinate) \u03b1i = e i \u03b1. Afterwards, \u03b1i is updated to maximize the (dual) target, leaving all other \u03b1 coordinates unchanged and maintaining the box constraints. At2Except that we avoid the logarithmic factor by relying on the averaging of the tail and a more modern SGD analysis. Iteration, updating from quantitative (t) i to quantitative (t) i is calculated (t) i is calculated (t) i: = argmax0 \u2264 \u03b1 (t) i + quantitative (t)."}, {"heading": "4.1. Naive Mini-Batching", "text": "A naive approach to parallelizing SDCA with the help of minibatches is to calculate \u03b4 (t) i in parallel, according to (14), for all i-At, all based on the current iteration \u03b1 (t + 1), and then \u03b1 (t + 1) i = \u03b1 (t) i + \u03b4 (t) i for i-At to update and maintain \u03b1 (t + 1) j = \u03b1 (t) j for j 6-At. However, not only could this approach not reduce the number of required iterations, it could even increase the number of required iterations, because the dual target does not need to be improved monotonously (as with \"pure\" SDCA) and not even converted. To see this, let's consider an extreme situation with only two identical training examples: Q = [1 11 1], \u03bb = 1 n = 1 2 and mini-batch size b = 2 (i.e. in each iteration we use both examples)."}, {"heading": "4.2. Safe Mini-Batching", "text": "Correct consideration of the interactions between the coordinates in the mini-batch would require a common optimization (across all \u03b1i, i). This would be a very effective update and would undoubtedly reduce the number of required iterations, but would essentially require the solution of a square program, with a square term of form. (This would be a very effective update and no doubt about the number of iterations needed. This square program cannot be distributed among different machines, each handling requires only a single data point. Instead, we propose a \"safe\" variant where the term \"AQA\u03b4A\" is roughly limited by the divisible environment. (This applies to some \u03b2 > 0, which we will discuss later.) The update is done by: (t) i: = argmax0 \u2264 (t) i + i \u2264 (Qei) i. \""}, {"heading": "4.3. Aggressive Mini-Batching", "text": "In particular, we have used the spectral standard to base ourselves on a worst-case approach, which could mean that we are taking much smaller steps than we could actually be. Moreover, the approach we have presented so far is based on knowing the spectral standard of the data, or at least knowing a limit for the relevant vectors (recall (10)), to determine the step size. Although it is possible to estimate this quantity by sampling, this can certainly be uncomfortable. Instead, we propose a more aggressive variant of the minimized SDCA, which is gradually based on the actual values of the SDCA (t)."}, {"heading": "5. Experiments", "text": "Figure 1 shows the number of iterations (corresponding to the parallel runtime) required to achieve a primary sub-optimality of 0.001 with Pegasos, naive SDCA, safe SDCA and aggressive SDCA, using four benchmark datasets described in Table 1, using different mini-batch sizes. Also shown (on an independent scale; right axis) is the leading term \u03b2bb in our complexity results. The results confirm the advantage of SDCA over Pegasos, at least for b = 1, and that both Pegasos and SDCA exhibit near-linear speeddups, at least for small batch sizes. Once the mini-batch size is large enough that \u03b2bb begins to flatten (corresponding to b-1\u03c32, and thus significant correlations within each mini-batch), the safe variant of SDCA exhibits similar behavior and does not allow large parallelism acceleration as a cause."}, {"heading": "6. Conclusion", "text": "Our contribution to this work is twofold: (i) we identify the spectral norm of the data (and thus the quantity \u03b2b, since the important quantitative control guarantees minibatched / parallelized Pegasos (primal method) and SDCA (dual method); we provide the first analysis of mini-batched Pagasos, using the non-smooth hingeloss method, which shows that we have novel variants of mini-batched SDCA with guarantees for the primary problem (hence, our mini-batched SDCA is a primal-method dual); (ii) we present novel variants of mini-batched SDCA, which are necessary to achieve speedups similar to those of Pegasos, and thus open the door to effective mini-batching using the often-empirically-better SDCA.work related."}, {"heading": "A. Proof of Theorem 2", "text": "The detection of Theorem 2 largely follows the path of Shalev-Shwartz & Zhang (2012), with Lemma 3 (and a few other necessary modifications) following in detail.We will prove the theorems for a general L-Lipschitz loss function (\u00b7). (For consistency with Shalev-Shwartz & Zhang, we will also allow example-specific loss functions: + 1, 2,. (n, and only for each individual loss I am an individual Lipschitz, and therefore refer to the primary and dual problems (expressed slightly differently, but equivalent): min w-specific loss functions (P (w): 1nn-specific loss functions i = 1, i (< w, xi >) + 1-Lipschitz function, and thus to the primary and dual problems (expressed slightly differently, but equivalent) (slightly different, but equivalent): min w-specific loss functions i = 1, i + w-specific loss functions (< w), max-xi-xi-i (P), x-specific loss functions (P)."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J. Duchi"], "venue": "In NIPS,", "citeRegEx": "Agarwal and Duchi,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi", "year": 2011}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["J.K. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin"], "venue": "In ICML,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In NIPS,", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "Optimal distributed online prediction using minibatches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "A dual coordinate descent method for large-scale linear svm", "author": ["Hsieh", "C-J", "Chang", "K-W", "Lin", "S.S. Keerthi", "S. Sundarajan"], "venue": "In ICML,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Nesterov", "Yu"], "venue": "SIAM J. Optimization,", "citeRegEx": "Nesterov and Yu.,? \\Q2012\\E", "shortCiteRegEx": "Nesterov and Yu.", "year": 2012}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S. Wright"], "venue": "K.Q. (eds.),", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d", "year": 2012}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d,? \\Q2013\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d", "year": 2013}, {"title": "Stochastic Methods for l1-regularized", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Loss Minimization. JMLR,", "citeRegEx": "Shalev.Shwartz and Tewari,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Tewari", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": null, "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2012}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming: Series A and B- Special Issue on Optimization and Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Solving large scale linear prediction using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In ICML,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008).", "startOffset": 63, "endOffset": 83}, {"referenceID": 4, "context": "SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.", "startOffset": 67, "endOffset": 116}, {"referenceID": 12, "context": "SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.", "startOffset": 67, "endOffset": 116}, {"referenceID": 4, "context": "2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008). However, the inherent sequential nature of such approaches becomes a problematic limitation for parallel and distributed computations as the predictor must be updated after each training point is processed, providing very little opportunity for parallelization. A popular remedy is to use mini-batches. That is, to use several training points at each iteration, instead of just one, calculating the update based on each point separately and aggregating the updates. The question is then whether basing each iteration on several points can indeed reduce the number of required iterations, and thus yield parallelization speedups. In this paper, we consider using mini-batches with Pegasos (SGD on the primal objective) and with Stochastic Dual Coordinate Ascent (SDCA). We show that for both methods, the quantity that controls the speedup obtained using mini-batching/parallelization is the spectral norm of the data. In Section 3 we provide the first analysis of minibatched Pegasos (with the original, non-smooth, SVM objective) that provably leads to parallelization speedups (Theorem 1). The idea of using mini-batches with Pegasos is not new, and is discussed already by Shalev-Shwartz et al. (2011), albeit without a theoretical justification.", "startOffset": 64, "endOffset": 1290}, {"referenceID": 13, "context": "Similar to a recent analysis of non-mini-batched SDCA by Shalev-Shwartz & Zhang (2012), we establish a guarantee on the duality gap, and thus also on the suboptimality of the primal SVM objective, when using mini-batched SDCA (Theorem 2).", "startOffset": 74, "endOffset": 87}, {"referenceID": 3, "context": "Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011).", "startOffset": 194, "endOffset": 258}, {"referenceID": 2, "context": "Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011).", "startOffset": 194, "endOffset": 258}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.", "startOffset": 0, "endOffset": 362}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.1). More directly related is recent work of Richt\u00e1rik & Tak\u00e1\u010d (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.", "startOffset": 0, "endOffset": 823}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.1). More directly related is recent work of Richt\u00e1rik & Tak\u00e1\u010d (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richt\u00e1rik & Tak\u00e1\u010d (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for \u201cstandard\u201d (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality.", "startOffset": 0, "endOffset": 1074}, {"referenceID": 1, "context": "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a \u201csafe\u201d variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as \u201cnaive\u201d mini-batching (Section 4.1). More directly related is recent work of Richt\u00e1rik & Tak\u00e1\u010d (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richt\u00e1rik & Tak\u00e1\u010d (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for \u201cstandard\u201d (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality.", "startOffset": 0, "endOffset": 1135}, {"referenceID": 12, "context": "When b = 1 we have \u03b2b = 1 (see (11)) and Theorem 1 agrees with the standard (serial) Pegasos analysis (Shalev-Shwartz et al., 2011).", "startOffset": 102, "endOffset": 131}, {"referenceID": 4, "context": "SDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed.", "startOffset": 46, "endOffset": 66}, {"referenceID": 4, "context": "SDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed. In terms of a theoretical analysis, by considering the dual problem (2) as an l1-regularized, box-constrained quadratic problem, it is possible to obtain guarantees on the dual suboptimality, D(\u03b1) \u2212 D(\u03b1), after a finite number of SDCA iterations (Shalev-Shwartz & Tewari, 2011; Nesterov, 2012; Richt\u00e1rik & Tak\u00e1\u010d, 2013). However, such guarantees do not directly imply guarantees on the primal suboptimality of w(\u03b1). Recently, Shalev-Shwartz & Zhang (2012) bridged this gap, and provided guarantees on P(w(\u03b1)) \u2212 P(w) after a finite number of SDCA iterations.", "startOffset": 46, "endOffset": 583}, {"referenceID": 13, "context": "Based on the above lemma, we can modify the analysis of Shalev-Shwartz & Zhang (2012) to obtain (see complete proof in the appendix): Theorem 2.", "startOffset": 73, "endOffset": 86}, {"referenceID": 12, "context": "cov is the forest covertype dataset of Shalev-Shwartz et al. (2011), astro-ph consists of abstracts of papers from physics also of Shalev-Shwartz et al.", "startOffset": 39, "endOffset": 68}, {"referenceID": 12, "context": "cov is the forest covertype dataset of Shalev-Shwartz et al. (2011), astro-ph consists of abstracts of papers from physics also of Shalev-Shwartz et al. (2011), rcv1 is from the Reuters collection and news20 is from the 20 news groups both obtained from libsvm collection (Libsvm).", "startOffset": 39, "endOffset": 160}, {"referenceID": 1, "context": "Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richt\u00e1rik & Tak\u00e1\u010d (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest.", "startOffset": 94, "endOffset": 116}, {"referenceID": 1, "context": "Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richt\u00e1rik & Tak\u00e1\u010d (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest.", "startOffset": 94, "endOffset": 145}], "year": 2013, "abstractText": "We address the issue of using mini-batches in stochastic optimization of SVMs. We show that the same quantity, the spectral norm of the data, controls the parallelization speedup obtained for both primal stochastic subgradient descent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.", "creator": "LaTeX with hyperref package"}}}