{"id": "1303.3055", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2013", "title": "Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions", "abstract": "We study the problem of learning Markov decision processes with finite state and action spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. We introduce an algorithm whose regret with respect to any policy in a comparison class grows as the square root of the number of rounds of the game, provided the transition probabilities satisfy a uniform mixing condition. Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy. Designing an efficient algorithm with small regret for the general case remains an open problem.", "histories": [["v1", "Tue, 12 Mar 2013 23:25:37 GMT  (13kb)", "http://arxiv.org/abs/1303.3055v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yasin abbasi-yadkori", "peter l bartlett", "varun kanade", "yevgeny seldin", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1303.3055"}, "pdf": {"name": "1303.3055.pdf", "metadata": {"source": "CRF", "title": "Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions", "authors": ["Yasin Abbasi-Yadkori", "Peter L. Bartlett", "Csaba Szepesv\u00e1ri"], "emails": ["yasin.abbasiyadkori@qut.edu.au", "bartlett@eecs.berkeley.edu", "szepesva@cs.ualberta.ca"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.30 55v1 [cs.LG] 1 2M ar2 01"}, {"heading": "1 Notation", "text": "Let X be a finite state space and A a finite space of action. Let \u2206 S be the space of the probability distributions over sentence S. Define a policy \u03c0 as a mapping of the state space on \u2206 A, \u03c0: X \u2192 \u2206 A. We use \u03c0 (a | x) to denote the probability of choosing action a in the state x under politics \u03c0. A random action under politics \u03c0 is characterized by \u03c0 (x). A transition probability matrix (or transition model) m is a mapping of the immediate product of the state and spaces of action on X: m: X \u00d7 A \u2192 X. Let P (\u03c0, m) define the transition probability matrix of politics \u03c0 under transition model m. A loss function is a limited real function over state and spaces of action,: X \u00d7 A \u2192 R. For a vector v define the vector v = 1 = 1 = 2 | vi | For a real evaluated function f, defined via X \u00d7 A < define a rev function between two (mav = xx)."}, {"heading": "2 Introduction", "text": "Consider the following game between a learner and an opponent: in round t, the learner selects a policy of regret from a political class. In the answer, the opponent selects a transition model from a series of models M and a loss function. In the discussion, we assume that the opponent is forgetful, i.e. that his decisions do not depend on the previous decisions of the learner. We assume that he is able to study the complete information version of the game, in which the learner observes the transition model and the loss function at the end of the round does not depend. Figure 1 shows the learner's goal of suffering low losses."}, {"heading": "3 Online MDP Problems", "text": "Let A be an online learning algorithm that generates a political learning problem (Cesa-Bianchi and the problems of DP 2006) Let x A not be the state if we have followed the policy generated by algorithm A. Similarly, x\u03c0t denotes the state if we have chosen the same policy. Let BT (x, \u03c0) = (x, \u03c0 (x). The regret of algorithm A to round T in relation to any policy action is defined by RT (A, \u03c0) = T \u2211 t (x). (x) The regret of algorithm A to round T in relation to any policy action is defined by the order of states. (A, \u03c0) = T \u2211 t = 1\u0445 t (x). Our goal is to design an algorithm that causes little regret in relation to any policy action. In the absence of state variables, the problem is reduced to a complete online learning problem (Cesa-DP 2006 and the Bianchi problems)."}, {"heading": "3.1 Full Information Algorithms", "text": "The first candidate we are looking at is the well-known exponentially weighted average (EWA) algorithm (Vovk, 1990, Littlestone and Warmuth, 1994) shown in Figure 2. In our MDP problem, the EWA algorithm chooses a policy that is most likely to differ in successive rounds, and therefore the EWA algorithm may change its policy frequently. However, a variant of EWA, called Shrinking Dartboard (SD) (Geulen et al., 2010) and the policy that this EWA algorithm produces is most likely to differ in successive rounds, and thus the EWA algorithm may change its policy frequently. A variant of EWA, called Shrinking Dartboard (SD) (Geulen et al., 2010) and shown in Figure 3, fulfils the A1 assumption. Our algorithm, called SD-MDP, is based on the SD algorithm and is shown in Figure 4."}, {"heading": "3.2 Analysis of the SD-MDP Algorithm", "text": "The main result of this section is the following regret for the SD-MDP algorithm. Theorem 2 (Leave the loss functions selected by the opponent in [0, 1], and the transition models selected by the opponent in [0, 2], [1], [2], [3], [4], [4], [4], [4], [5], [5], [6], [6], [6], [6], [6], [6], [6], [6], [6], [6], [6], [7], [7], [7], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8]. \""}], "references": [{"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Experts in a Markov decision process", "author": ["Eyal Even-Dar", "Sham M. Kakade", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "Even.Dar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2004}, {"title": "Online Markov decision processes", "author": ["Eyal Even-Dar", "Sham M. Kakade", "Yishay Mansour"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2009}, {"title": "Regret minimization for online buffering problems using the weighted majority algorithm", "author": ["Sascha Geulen", "Berthold V\u00f6cking", "Melanie Winkler"], "venue": "In COLT,", "citeRegEx": "Geulen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Geulen et al\\.", "year": 2010}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Inf. Comput.,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Online Markov decision processes under bandit feedback", "author": ["Gergely Neu", "Andr\u00e1s Gy\u00f6rgy", "Andr\u00e1s Antos Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Aggregating strategies", "author": ["Vladimir Vovk"], "venue": "In COLT, pages 372\u2013383,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "Arbitrarily modulated Markov decision processes", "author": ["Jia Yuan Yu", "Shie Mannor"], "venue": "In IEEE Conference on Decision and Control,", "citeRegEx": "Yu and Mannor.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Mannor.", "year": 2009}, {"title": "Online learning in Markov decision processes with arbitrarily changing rewards and transitions", "author": ["Jia Yuan Yu", "Shie Mannor"], "venue": "In GameNets,", "citeRegEx": "Yu and Mannor.,? \\Q2009\\E", "shortCiteRegEx": "Yu and Mannor.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Even-Dar et al. (2004) prove a hardness result for MDP problems with adversarially chosen transition models.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "|xt, at) Learner observes mt and lt end for Figure 1: Online Markov Decision Processes Even-Dar et al. (2009) prove regret bounds for MDP problems with a fixed and known transition model and adversarially chosen loss functions.", "startOffset": 87, "endOffset": 110}, {"referenceID": 0, "context": "In the absence of state variables, the problem reduces to a full information online learning problem (Cesa-Bianchi and Lugosi, 2006).", "startOffset": 101, "endOffset": 132}, {"referenceID": 4, "context": "As discussed by Neu et al. (2010), if Assumption A2 holds for deterministic policies, then it holds for all policies.", "startOffset": 16, "endOffset": 34}, {"referenceID": 3, "context": "However, a variant of EWA, called Shrinking Dartboard (SD) (Geulen et al., 2010) and shown in Figure 3, satisfies Assumption A1.", "startOffset": 59, "endOffset": 80}, {"referenceID": 3, "context": "The proof of the bound on the probability of switch is similar to the proof of Lemma 2 in (Geulen et al., 2010) and is as follows: As shown in (Geulen et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 2, "context": "Proof is easy and can be found in (Even-Dar et al., 2009), Lemma 5.", "startOffset": 34, "endOffset": 57}], "year": 2013, "abstractText": "We study the problem of learning Markov decision processes with finite state and action spaces when the transition probability distributions and loss functions are chosen adversarially and are allowed to change with time. We introduce an algorithm whose regret with respect to any policy in a comparison class grows as the square root of the number of rounds of the game, provided the transition probabilities satisfy a uniform mixing condition. Our approach is efficient as long as the comparison class is polynomial and we can compute expectations over sample paths for each policy. Designing an efficient algorithm with small regret for the general case remains an open problem.", "creator": "LaTeX with hyperref package"}}}