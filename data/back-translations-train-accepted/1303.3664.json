{"id": "1303.3664", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2013", "title": "Topic Discovery through Data Dependent and Random Projections", "abstract": "We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics. We will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme.", "histories": [["v1", "Fri, 15 Mar 2013 02:37:19 GMT  (416kb)", "https://arxiv.org/abs/1303.3664v1", "This paper was submitted to the 30th International Conference on Machine Learning (ICML 2013) on February 15, 2013"], ["v2", "Mon, 18 Mar 2013 13:11:02 GMT  (416kb)", "http://arxiv.org/abs/1303.3664v2", null]], "COMMENTS": "This paper was submitted to the 30th International Conference on Machine Learning (ICML 2013) on February 15, 2013", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["weicong ding", "mohammad hossein rohban", "prakash ishwar", "venkatesh saligrama"], "accepted": true, "id": "1303.3664"}, "pdf": {"name": "1303.3664.pdf", "metadata": {"source": "META", "title": "Topic Discovery through Data Dependent and Random Projections", "authors": ["Weicong Ding", "Mohammad H. Rohban"], "emails": ["dingwc@bu.edu", "mhrohban@bu.edu", "pi@bu.edu", "srv@bu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.36 64v2 [st at.M L] 18 Mar 2"}, {"heading": "1. Introduction", "text": "We consider a body of M-documents consisting of words, which consist of a vocabulary of W-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A"}, {"heading": "2. Related Work", "text": "In this approach, the different concepts, which differ in the different concepts of the different concepts, in the individual concepts of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, of the individual concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different concepts, the different, the different concepts, the different, the different concepts, the different, the different, the different, the different concepts of the different, the different, the different, the different, the different concepts of the different, the different, the different, the different concepts of the different, the different, the different concepts of the different, the different, the"}, {"heading": "3. Topic Geometry", "text": "Remember that X and A are each the W \u00b7 M empirical and actual document word distribution = \u03b2 \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 \u03b2 matrices and A = \u03b2\u03b8, where \u03b2 is the latent subject word distribution matrix and \u03b8 is the underlying weight matrix. However, let us designate the i \u2212 th line of X and A representing the cross-document pattern of word i. We assume that \u03b2 = diag (Def. 1) \u2212 1\u03b2 diag (\u03b81), so that A + 0 = \u03b2. Topic k and C0 are the set of non-novel words. The geometric intuition underlying our approach is the transverse document pattern of word i. We assume that \u03b2 is divisible (Def. 1). Let Ck be the set of new words of topic k and let C0 be the set of non-novel words.The geometric intuition underlying our approach is formulated in the following formula:"}, {"heading": "4. Proposed Algorithm", "text": "The geometric intuition mentioned in Propositions 1 and 2 motivates the following three-step approach to topic identification: (1) Novel Word Detection: Considering the empirical word-by-document matrix X, we extract the sentence of all new words I. We present variants of projection-based algorithms in paragraph 4.1. (2) Novel Word Clustering: Considering a set of new words I with | I | \u2265 K, we group them into K-groups corresponding to K-themes. Choose a representative for each group. We use a distance-based cluster algorithm. (paragraph 4.2). (3) Topic Estimation: Estimate the topic matrix as proposed in Proposition 2 by means of restricted linear regression. (Section 4.3)"}, {"heading": "4.1. Novel Word Detection", "text": "When we project each point of a convex body in a direction d, the maximum and the minimum correspond to the extreme points of the convex object. Our proposed approaches, data-dependent and random projections, both take advantage of this fact; they differ only in the choice of projected directions."}, {"heading": "A. Data Dependent Projections (DDP)", "text": "To simplify our analysis, we randomly divide each document into two subsets and obtain two statistically independent sets of documents X and X, both distributed as A, and then normalize the series as X and X. For a certain threshold, d, to be determined later, and for each word i, we consider the set of Ji of all other words sufficiently different from word i in the following sense: Ji = {j | M (X) (X) i \u2212 X \"i\" j \"j\" j \"j\" j \"j\" j \"d / 2\" i. \"(1) We then declare word i as a new word when all words j\" Ji \"are uniformly uncorrelated with word i\" i. \""}, {"heading": "B. Random Projections (RP)", "text": "The Random Projection Algorithm (RP) roughly uses P = O (K) random directions, which are uniformly drawn iid over the unit sphere. For each direction d, we project all X-i's onto it and select the maximum and minimum. Note that X-id will converge to A-id due to the dAlgorithm 2 Novel Word Detection - RP1: Input X, P 2: Output: The indexes of the novel words I 3: I-4: for all 1 \u2264 j \u2264 P do 5: Generate d-Uniform (Uniform in RM) 6: imax = argmax X-id, imin = argmax X-id, imin = argmax X-id 7: I \u2190 I {imax, imin} 8: forend and cuit as M. In addition, we can only expect the minimum number of K-consistency for the extreme points i-injection."}, {"heading": "C. Random Projections with Binning", "text": "Another alternative to RP is a binning algorithm that is mathematically more efficient. In this case, the corpus is divided into words of equal size in M. For each bin j, a random direction d (j) is chosen and the word with the maximum projection along d (j) is selected as the winner. Then, we can calculate the number of wins for each word i. We divide these win rates by M like an estimate for pi, Pr (6 = i: A) id A (j) jd). pi can be shown that zero is for all non-novel words. For non-degenerated previous words, these probabilities converge to strictly positive values for novel words. Therefore, the estimation of pi's helps in the identification of new words. We then choose the indices of O (K) largest Pi values as novel words. The binning algorithm is outlined in category I: Algorithm 3.Algorithm 3 Novel Word Detection - Input: X, X."}, {"heading": "4.2. Novel Word Clustering", "text": "Since there may be several novel words for a single topic, our DDP or RP algorithm can extract several novel words for each topic, which requires clustering to group the copies. We can show that our cluster scheme is consistent if we assume that R = 1M E (\u03b8\u03b8) is positively defined: Proposition 6. Leaving Ci, j, MX, iX, and Di, j, Ci, i \u2212 2Ci, j + Cj, j. If R is positively defined, then Di, j converges to zero in probability whenever i and j are novel words of the same topic as M. Furthermore, if i and j are novel words of different type, it is likely to converge to a strictly positive value greater than any constant. The proof is presented in the supplementary section: As suggested by Proposition 6, we construct a bi-algorithm of 4 novel word cluster input \u00d7 1, J: X, J: X: K, J: X: K, X: I: K, X: K, X: K, X: K: I x: K."}, {"heading": "4.3. Topic Matrix Estimation", "text": "Given the new K-words of various topics (J), we could estimate (\u03b2) directly as in sentence 2, which is described in algorithm 5. We note that this part of the algorithm is similar to some other subject modeling approaches that exploit separability, and consistency of this step is also validated in (Arora et al., 2012b). Indeed, one can use the convergence of extreme estimators (Amemiya, 1985) to show the consistency of this step. Algorithm 5 Topic Matrix Estimation1: Input: J = {j1,.., jK}, X, X \u2032 2: Output: \u03b2, which is the estimate of \u03b2-matrix 3: Y = (X-matrix j1,..., X-matrix jK), Y \u2032 = (X-matrix j1,."}, {"heading": "5. Statistical Complexity Analysis", "text": "In this section, we describe the sample complexity that is limited for each step of our algorithm. In addition, we provide guarantees for the DDP algorithm under some mild assumptions about the distribution over \u03b8. The analysis of the random projection algorithm is much more complicated and requires complex arguments. We will omit them in this paper. We need the following technical assumptions about the correlation matrix R and the mean vector a of \u03b8: (P1) R is positively defined with its minimal eigenvalue limited by a lower value. (P2) In addition, there is a positive value, so that for i 6 = j, Ri, i / (aiai) \u2212 Ri, j / (aiaj). The second condition implies the following intuition: If two new words originate from different topics, they must appear in a considerable number of different documents."}, {"heading": "5.1. Novel Word Detection Consistency", "text": "In this section, we provide analyses only for the DDP algorithm. However, sample complexity analysis of the randomized projection algorithms is more involved and is the subject of ongoing research. Suppose P1 and P2 hold. Denotes \u03b2-1 and \u03bb-2 are positive lower limits for non-zero elements of \u03b2 and minimum eigenvalue of R. We have: Theorem 1. For parameter decisions d = \u03bb-2 value and g = complete convergence, the DDP algorithm is consistent than M value. Specifically, true new and non-novel words are asymptotically declared as novel and non-novel. Furthermore, forM \u2265 C1 (logW + log (1) \u03b22-2 value as convergence is novel, which is 2a-2a-2 value), where C1 is a constant, algorithm 1 finds all new words without outliers with probability."}, {"heading": "5.2. Novel Word Clustering Consistency", "text": "Similarly, we demonstrate the consistency and sample complexity of the novel word cluster algorithm: Theorem 2. For d = \u03bb \u0445 \u03b22 \u0445, since all real new words are given as input, the cluster algorithm, algorithm 4 (ClusterNovelWords) differs asymptotically (since M \u2192 \u221e restores novel word indexes of different types, namely the support of corresponding \u03b2 lines for two indexes retrieved at will. Furthermore, a more detailed analysis is provided in the supplement section. We can show that Ci, i \u2212 2Ci, j + Cj, j groups all novel words correctly into algorithm 4 with a probability of at least 1 \u2212 insp2.Proof Sketch. We can show that Ci, i \u2212 2Ci, j + Cj, j converge to a strictly positive value d if i and j are novel words."}, {"heading": "5.3. Topic Estimation Consistency", "text": "Finally, we show that the subject of estimation by regression is also consistent. Theorem \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 (algorithm 5 prints \u03b2 in view of the indices of K. Then \u03b2 \u2212 p \u2212 \u2192 \u03b2 becomes. \"Specifically, ifM \u2265 C3W 4 (log (W) + log (K) + log (1 / 3))) + + + + + + + + + + + + + + + + + + + + + + [[[[+]]] 8a 8] then becomes for all i and j, \u03b2 \u2212 i, j with a probability of at least 1 \u2212 3, where < 1, C3 is a constant, [[[+]] a minimum constant. We will provide a detailed analysis in the Supplementary Section, j with a probability of at least 1 \u2212 3, where < 1, C3 is a constant, a minimum constant, [[[[]]]."}, {"heading": "6. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Practical Considerations", "text": "The DDP algorithm requires two parameters \u03b3 and d. In practice, we can apply DDP without knowing it adaptively and agnostically. Note that d is intended for the construction of Ji. Otherwise, we can construct Ji by finding r < W words that are maximally distant from i in the sense of Equation 1. To bypass \u03b3, we can place the values of minj-Ji M < X-i, X-i < X < i, X-j > in all i and declare the uppermost s values as novel words. The clustering algorithm also requires the parameter d. Note that d applies only to the threshold of a weighted chart of 0 \u2212 1. In practice, we could avoid a hard threshold by using exp (\u2212 Ci, i \u2212 2Ci, j + Cj, j) as new words."}, {"heading": "6.2. Synthetic Dataset", "text": "In this section, we validate our algorithms on synthetic examples. We generate a W \u00b7 K separable topic matrix \u03b2 with W1 / K > 1 novel words per topic as follows: First, iid 1 \u00b7 K row vectors that do not correspond to novel words are uniformly generated on the probability simplex. Then, W1 iid uniform [0, 1] values are generated for the non-zero entries in the lines of new words. The resulting matrix is then column normalized to obtain a realization of \u03b2: = W1 / W. Further, M iid K \u00d7 1 column vectors are generated for the non-zero entries in the lines of new words. The resulting matrix is then presented as a topic to obtain a realization of \u03b2."}, {"heading": "6.4. Real World Text Corpora", "text": "In this section, we apply our algorithm to two different real text corpora (Frank & Asuncion, 2010); the smaller body is the NIPS Procedure Dataset with M = 1, 700 documents, a vocabulary of W = 14, 036 words and an average of N \u2248 900 words in each document; another is a large corpus of New York (NY) Times article dataset with M = 300, 000, W = 102, 660 and N \u2248 300. Vocabulary is gained by deleting a standard \"stop\" word list in computational linguistics, including numbers, single characters and some common English words such as \"the.\" To compare with the practical algorithm in (Arora et al., 2012a), we followed the same procedure to shrink the vocabulary to W = 2, 500 for NIPS and W = 100, 000 for NY Times."}, {"heading": "7. Conclusion and Discussion", "text": "We summarize our proposed approaches (DDP, binning, and RP) and compare them with other existing methods in terms of assumptions, computational complexity, and sample complexity (see Table 3). Among the list of algorithms, DDP and RecL2 are the best and most competitive methods. Although the DDP algorithm has polynomial sample complexity, its runtime2das zzz prefix would comment on the named entirety, resulting in greater time complexity in RecL2, which depends on 1 / 2. Although the complexity of DDP appears to be independent of W, increasing W would reduce the elements of \u03b2 and decrease the precision required for recovering \u03b2, resulting in greater time complexity in RecL2. In contrast, the temporal complexity of DDP does not scale with the entirety of W. On the other hand, the sample complexity of both DDP and RecL2 would be reduced during polyscaling."}, {"heading": "Supplementary Materials", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Proofs", "text": "Since \u03b2 is separable, we can order the rows of \u03b2 in such a way that \u03b2 = [D \u03b2 \u2032], where D is diagonal."}, {"heading": "A.1. Proof of Proposition 3", "text": "Sentence 3 is a direct result of sentence 1. Further details can be found in section A.7."}, {"heading": "A.2. Proof of Proposition 4", "text": "Proposition 4 (in Section 4.1) summarizes the computational complexity of the DDP algorithm 1. Proof: We can show that due to the rarity of X, C = MXX \u00b2 can be calculated in O (MN2 + W) time. First, note that C is a scaled word-word-occurrence matrix that can be calculated by adding the occurrence matrices of each document. This runtime can be achieved if all W words in the vocabulary are first indexed by a hash table (which O (W) needs. Since each document consists of no more than N words, O (N2) time is needed to calculate the co-occurrence matrix of each document."}, {"heading": "A.3. Proof of Proposition 5", "text": "Remember that proposal 5 summarizes the computational complexity of RP (algorithm 2) and binning (and see section B in the appendix for more details). Here, we offer a more detailed proof. Proposal 5 (in section 4.1) Runtime of RP (algorithm 2) and binning algorithm (in the appendix section B) are O (MNK + WK) and O (MN + \u221a MW), respectively. Proof: Note that the number of operations required to find the predictions is O (MN + W) in binning and O (MNK + W) in RP. This can be accomplished by first indexing the words through a hash table and then projecting each document along the corresponding component of the random directions.Of course, the O (N) takes time for each document. In addition, the search for the word with the maximum projection value (in RP) and the winner in each paper bin (random directions) of the corresponding component."}, {"heading": "A.4. Proof of Proposition 6", "text": "Proposal 6 (in Section 4.2) is a direct result of Theory 2. Please read Section A.8 for detailed proof. A.5. Validating the assumptions in Section 5 for the Dirichelet DistributionIn this section we will check the validity of the assumptions P1 and P2 made in Section 5. For x x x x x x x x x x Ki = 1 xi = 1, xi x 0, x x x x x Dir (\u03b11,.., \u03b1K) has pdf P (x) = c x x K i = 1 x \u03b1i \u2212 1 i. Allow it, x \u03b1 = min 1 \u2264 i \u2264 K \u03b1i and \u03b10 = p \u2212 \u03b1i. Proposal A.1 For a Dirichlet in front of you (\u03b11,.,., \u03b1K): 1. The correlation matrix R is positively defined with minimum eigenvalue."}, {"heading": "A.6. Convergence Property of the co-occurrence Matrix", "text": "In this section, we show a set of lemmas as ingredients to show the main theorems 1, 2 and 3 in section 5. These lemmas in order: \u2022 Convergence of C = MX-X-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y"}, {"heading": "A.7. Proof of Theorem 1", "text": "Now we can prove theory 1 in section 5. To summarize the notations, we allow \u03b2-J to be a strictly positive lower limit for non-zero elements of \u03b2, \u03bb-j is the minimum eigenvalue of R, and a-j is the minimum component of the mean vector a. Furthermore, for parameter decisions we define d = min1 \u2264 i-W \u03b2iaand, min 1 \u2264 i6 = j \u2264 K Ri, i aiai \u2212 Ri, jaiaj > 0.Theorem 1 (in section 5.1) For parameter decisions d = novel \u03b22 and \u03b3 = a-a-ha the DDP algorithm is consistent as M \u2192. Specifically, true novel and non-novel words are asymptotically declared novel and non-novel. Furthermore, forM-words C1 (logW + log (1) and 3-a-ha-ha-ha-ha) are defined as novel."}, {"heading": "A.8. Proof of Theorem 2", "text": "Theorem 2 (in Section 5.2) Algorithm 4 (ClusterNovelWords) asymptotically (since M \u2192 \u221e restores novel K-word indexes of different types, i.e. support for the corresponding \u03b2-lines is different for any two retrieved indexes."}, {"heading": "Furthermore, if", "text": "M \u2265 C2 (logW + log (1 \u03b42) \u03b78\u03bb2 \u0445 \u03b2 4 \u0445 then algorithm 4 clusters all new words correctly with a probability of at least 1 \u2212 \u03b42.Proof of theorem 2. The statement follows on the basis of the (| I | 2) number of composite boundaries about the probability that Ci, i \u2212 2Ci, j + Cj, j lies outside an interval of length d / 2 centered around the value to which it converges. Convergence rate of random variables is in Lemma 1. Therefore, the probability that the cluster algorithm fails in clustering all new words is really limited by e1W2 exp (\u2212 Me2throu8d2), where e1 and e2 are constants and d is defined in the theorem."}, {"heading": "A.9. Proof of Theorem 3", "text": "Theorem 3 (in Section 5.3) Suppose that algorithm"}, {"heading": "5 outputs \u03b2\u0302 given the indices of K distinct novel words. Then, \u03b2\u0302", "text": "p \u2212 \u03b2 \u2212 bY is the optimal solution as b \u2212 b). \u2212 b) We will rearrange the rows so that Y and Y are the first K-rows of X and X. \u2212 b) For the optimization target in algorithm 5, if i < K, b = ei, where all components of egg are zero, which is one. \u2212 b) We assign the objective function as QM (b) = M (X-bY)."}, {"heading": "B. Experiment results", "text": "B.1. Sample topics extracted on NIPS dataset Tables 4, 5, 6 and 7 show the most common words in topics extracted by different algorithms on NIPS dataset. The words are listed in descending order. There are M = 1,700 documents. Average words per document is N \u2248 900. Vocabulary size is W = 2,500. It is difficult and confusing to group four groups of topics. We simply show topics that are extracted individually by each algorithm. B.2. Sample topics extracted on New York Times dataset Tables 8 to 11 show the most common words in topics extracted by algorithms on NY Times dataset. There are M = 300,000 documents. Average words per document is N \u2248 300. Vocabulary size is W = 15,000."}], "references": [{"title": "Advanced econometrics", "author": ["T. Amemiya"], "venue": null, "citeRegEx": "Amemiya,? \\Q1985\\E", "shortCiteRegEx": "Amemiya", "year": 1985}, {"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A Practical Algorithm for Topic Modeling with Provable Guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "Zhu", "Michael"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Learning topic models \u2013 going beyond", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "SVD. arXiv:1204.1956v2 [cs.LG],", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A correlated topic model of science. annals of applied statistics", "author": ["D. Blei", "J. Lafferty"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Blei and Lafferty,? \\Q2007\\E", "shortCiteRegEx": "Blei and Lafferty", "year": 2007}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Commun. ACM,", "citeRegEx": "Blei,? \\Q2012\\E", "shortCiteRegEx": "Blei", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S. Amari"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts", "author": ["D. Donoho", "V. Stodden"], "venue": "In Advances in Neural Information Processing Systems 16,", "citeRegEx": "Donoho and Stodden,? \\Q2004\\E", "shortCiteRegEx": "Donoho and Stodden", "year": 2004}, {"title": "Finding scientific topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "In Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers", "year": 2004}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791,", "citeRegEx": "Lee and Seung,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Pachinko allocation: Dagstructured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li and McCallum,? \\Q2007\\E", "shortCiteRegEx": "Li and McCallum", "year": 2007}, {"title": "Factoring nonnegative matrices with linear programs", "author": ["B. Recht", "C. Re", "J. Tropp", "V. Bittorf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Recht et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "We adopt the classic \u201cbags of words\u201d modeling paradigm widely-used in probabilistic topic modeling (Blei, 2012).", "startOffset": 99, "endOffset": 111}, {"referenceID": 7, "context": "To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function.", "startOffset": 88, "endOffset": 174}, {"referenceID": 12, "context": "To address the scenario where only the realization X is known and not A, several papers (Lee & Seung, 1999; Donoho & Stodden, 2004; Cichocki et al., 2009; Recht et al., 2012) attempt to minimize a regularized cost function.", "startOffset": 88, "endOffset": 174}, {"referenceID": 6, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling.", "startOffset": 34, "endOffset": 65}, {"referenceID": 5, "context": "Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012) is a statistical approach to topic modeling.", "startOffset": 34, "endOffset": 65}, {"referenceID": 1, "context": "(Anandkumar et al., 2012) describe a novel method of moments approach.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "In fact, one may use the convergence of extremum estimators (Amemiya, 1985) to show the consistency of this step.", "startOffset": 60, "endOffset": 75}, {"referenceID": 0, "context": "\u03b2\u0302 p \u2212\u2192 \u03b2\u2217 (Amemiya, 1985).", "startOffset": 11, "endOffset": 26}, {"referenceID": 5, "context": "Following typical settings in (Blei, 2012) and (Arora et al.", "startOffset": 30, "endOffset": 42}, {"referenceID": 1, "context": ", 2012a); ECA from (Anandkumar et al., 2012); Gibbs from (Griffiths & Steyvers, 2004); NMF from (Lee & Seung, 1999).", "startOffset": 19, "endOffset": 44}, {"referenceID": 1, "context": "R) max { C1aK 3 log(W ) \u01eb\u03b36p6 , C2a K log(W ) \u01eb3\u03b34p4 } Separable \u03b2; Robust Simplicial Property of R Pr(Error) \u2192 0; Requires Novel words to be linearly independent; ECA (Anandkumar et al., 2012) O(W 3 +MN) N/A : For the provided basic algorithm, the probability of error is at most 1/4 but does not converge to zero LDA model; The concentration parameter of the Dirichlet distribution \u03b10 is known Requires solving SVD for large matrix, which makes it impractical; Pr(Error) 9 0 for the basic algorithm Gibbs (Griffiths & Steyvers, 2004) N/A N/A LDA model No convergence guarantee NMF (Tan & F\u00e9votte, in press) N/A N/A General model Non-convex optimization; No convergence guarantee", "startOffset": 168, "endOffset": 193}], "year": 2013, "abstractText": "We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics. We will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme.", "creator": "LaTeX with hyperref package"}}}