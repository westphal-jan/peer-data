{"id": "1303.4172", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "Margins, Shrinkage, and Boosting", "abstract": "This manuscript shows that AdaBoost and its immediate variants can produce approximate maximum margin classifiers simply by scaling step size choices with a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman's empirically successful \"shrinkage\" procedure for gradient boosting (Friedman, 2000). Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.", "histories": [["v1", "Mon, 18 Mar 2013 07:33:29 GMT  (383kb,D)", "http://arxiv.org/abs/1303.4172v1", "To appear, ICML 2013"]], "COMMENTS": "To appear, ICML 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matus telgarsky"], "accepted": true, "id": "1303.4172"}, "pdf": {"name": "1303.4172.pdf", "metadata": {"source": "META", "title": "Margins, Shrinkage, and Boosting", "authors": ["Matus Telgarsky"], "emails": ["mtelgars@cs.ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "One explanation for the effectiveness of the increase is that it not only seeks aggregates with low empirical risk, but also favors good margins, leading to improved generalization (Schapire et al., 1997).Since AdaBoost does not achieve maximum margins in general instances, a push has been made to develop methods that carry such a warranty (Ra \ufffd tsch & Warmuth, 2005; Shalev-Shwartz & Singer, 2008; Rudin et al., 2007).This work shows that margin maximization can be achieved by scaling back the step size.The intuition for this result is simple (see Figure 1): When (equivalently) the steps in a coordinated descent process are considered, the iterates that are presented as a path."}, {"heading": "1.1. Outline", "text": "After summarizing the main content, this introduction concludes with links to related work; then, Section 2 recalls the core algorithm, defines the class of loss functions and provides the four step variables. Xiv: 130 3.41 72v1 [cs.LG] 1 8M ar2 013Since the increase is generally studied under the weak learning assumption (a condition of separability), the predominant study in this manuscript is also under the condition of separability and appears in Section 3. The first step is to show that shrinkage does not drastically alter the convergence rate of empirical risk among these methods; the more comprehensive study deals with the issue of margins, and the last subsection compares these boundaries with those of other methods. General (potentially inseparable) instances are discussed in Section 4."}, {"heading": "1.2. Related Work", "text": "Three obvious studies on increasing vulnerability (2000) gave the same pattern as here (albeit only with optimal line search); the follow-up work was mainly empirical, and the issues of convergence rates and marginal guarantees do not appear in the literature. Second, Zhang & Yu (2005) was also considered a regulatory line search, but with the aim of proving consistency; maximization is proven to be a by-product, and the analog results are less pronounced, and the rates for the stricter steps are higher."}, {"heading": "2. Algorithms and Notation", "text": "First, some basic notations: Let (xi, yi) mi = 1 X \u00b7 {\u2212 1, + 1} take an M-dot sample. Take H0 to denote the collection of weak learners; it is assumed that h-H0 h (xm) is fulfilled; this is true, for example, if there is a fixed finite series of results from H0, i.e., each h is binary. Consequently, H = {hj} nj = 1 denotes the effective finite set of hypotheses and collect the answers to the sample into a matrix A-1, + 1] m \u00b7 n with Aij = \u2212 yihj (xi).Boosting finds a weighting of H that corresponds to a minor number."}, {"heading": "2.1. The Family of Loss Functions", "text": "Class L is effectively \"functions similar to exponential loss.\" Some of these functions are for analytical convenience, but some of them seem to be essential, and therefore a bit of motivation is needed. Optimization problems usually take advantage of curvature (e.g. strong convexity) to establish a convergence rate. Instead, the analysis here uses a relative form of curvature: It is sufficient, for example, to keep the Hessian curvature \u2212 \u2212 not too small in relation to the current primary objective value and the primary optimum. In this sense, exponential loss is ideal as it is a fixed point of the differentiator. Definition 2.1: Faced with a loss \": R + + (where R + stands for positive realities), let C '(z) 1 (with potential C' (z) =\" be the narrowest positive constant, \"so that for each x \u2264 z: z = derivation: C '."}, {"heading": "2.2. Algorithm", "text": "The algorithm appears in algorithm 1 (e > i). Before the different step sizes are defined, two further definitions are in order (definition 2.5.). For each definition the following definition is required (definition 4). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 2). (definition 2). (definition 2). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 1). (definition 2). (definition 2). (definition 2). (definition 2). (definition. (definition 1). (definition 1). (definition 1). (definition 1). (definition. (definition 1). (definition 1). (definition. (definition 1). (definition. (definition.). (.......). (..... (....). (....). (....). (..... (....). (....). (..... (....). (..... (....). (.........................................................."}, {"heading": "3. The Separable Case", "text": "This section is about defining separability, which means that the weak learning assumption is fulfilled (\u03b3 > 0).The three subsections each offer convergence rates in terms of empirical risks, basic margin guarantees, and conclude with a discussion."}, {"heading": "3.1. Convergence of Empirical Risk", "text": "The basic guarantee is that all these line search methods, for each loss in L and with any shrinkage factor, actually have the same basic convergence rate as AdaBoost.Theorem 3.1. Let's consider matrix A with appropriate \u03b3 > 0 and shrinkage parameter \u03bd (0, 1] if necessary. Given that \"L, any > 0, and iterations are sufficient to ensure that L (A\u03bb1), where the O () suppresses the terms according to C1 and state, is in the appendix, but a basic discussion will appear here for each step size. The evidence is simple as it should be: convergence analyses usually for one step, and then the bound steps."}, {"heading": "3.2. Margin Maximization", "text": "The margin rates here follow a simple pattern: the more regulates the step size, the faster the convergence to a good margin. While no lower limits are displayed, this is an interesting and intuitive correspondence (in particular, in accordance with Figure 1). Unfortunately, the unlimited step sizes have only asymptotic convergence (no rates), so the Umbrella theorem for this subsection also asymptotic.Theorem 3.5. Let the increase of matrix A be given with corresponding margins > 0 and shrinkage parameters (0, 1]. Given the fact that any \"L,\" any > 0, and iterates {0, and iterates} t = 0 in accordance with Qt (sound), \u03b1 W t (sound) with \"= Lemp, or \u03b1Ot (sound) with binary A {\u2212 1} m, then there is T, so that for all M (sound)."}, {"heading": "3.3. Discussion", "text": "To get a sense of these margin limits, let's first remember Freund's lower limit for increasing the methods in the separable case, which states that additional iterations are necessary to achieve a classification error. (Freund, 1995, section 2) However, if we use a comparison, it follows that these iterations are indeed sufficient to achieve a non-negative margin value. (ln (m) / \u03b32) Iterations are necessary to achieve a non-negative margin value (Lemma 3.7). More generally, \"Wt\" achieves margin value (1 \u2212 \u03bd) and \"exp\" only 12 ln (m) / \u03b32 iterations with options. (1 / 2) Iterations with options are sufficient to achieve margin value."}, {"heading": "4. The General Case", "text": "The final technical contribution of this manuscript is to briefly consider the general case (which is potentially inseparable). Similar to the separable case, this section will set convergence rates for empirical risks, margin guarantees, and briefly discuss the link to existing margin-maximizing methods. However, first, it is necessary to discuss the structure of the general case and in particular to develop what margins without separability mean. This section depends on the following decomposition of a strengthening instance. This decomposition will in particular include their examples {(xi, yi)} mi = 1, into a hard subset H (A), and a simple subset H (A) is separable alone, and therefore the margins will be measured there. Although the analysis will be heavily based on characteristics of this decomposition (2012), the decomposition itself has appeared with various guarantees (Goldliazzo, 1995)."}, {"heading": "5. Discussion", "text": "This manuscript immediately raises a number of questions: in the first place, perhaps, is the general question of the influence of margins on the effectiveness of increase. Although margins certainly offer an intuitive theory, it is still unclear how strongly they directly correlate with good algorithms (Reyzin & Schapire, 2006). Next, the limits for logistic loss are not narrow. As there does not seem to be any more conciliatory analyses of logistic loss, the natural question is whether there are new techniques that allow better characterization. Finally, Figure 2 shows a threshold effect: shrinkage 1 does not lead to the right margin, but 1 / 2 and smaller is sufficient to reach the maximum margin. (In fact, experimentation shows that the threshold is about 0.92.) This behavior should be illustrated from the perspective of dynamic systems: smaller steps evade bad attractors (Rudin et al., 2004; 2007)."}, {"heading": "Acknowledgements", "text": "The author would like to thank Daniel Hsu and the ICML reviewers for their helpful comments and discussions. Robert Schapire also deserves great gratitude for numerous discussions, insights, and for proposing an investigation into the unrestricted step size (at the time there were guarantees only for the other decisions!), which was kindly supported by NSF under grant number IIS0713540."}, {"heading": "A. Deferred Material from Section 2", "text": "The lower limit can be verified in two steps. First, if the lower limit can be verified in two steps. (First, if the lower limit is exceeded in two steps.) First, if the lower limit is exceeded in two steps. (First, if the lower limit is exceeded in two steps.) First, if the lower limit is exceeded in two steps. (First, if the lower limit is exceeded.) First, if the lower limit is exceeded. (First, if the lower limit is exceeded.) First, if the lower limit is exceeded. (Achieved). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved. (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (Achieved.). (.....). (..... (..... (.....). (...... (........ (.....). (..... (....... (......). (..... (..... (.....). (.....). (..... (..... (.....). (.....). (..... (....... (.....). (.....). (..... (.....). (.....). (..... (..... (.....). (.....). (.....). (..... (..... (.....). (.....).. (.....). (..... (......). (.....)... (..... (.....). (.....). (..... (.......)..). (..... (....."}, {"heading": "B. Deferred Material from Section 3", "text": "B. 1. Suspended material from Section 3.1Proof of Lemma 3.2. Of Lemma A.1, for each t, L (A (A (A) + 1)), \u2264 L (A), and use the fact that Ct + 1 \u2264 Ct.Proof of of Lemma 3.3. Let us not specify fixed iteration. Substitute c1 = 1 \u2212 \u03bd / 2, c2 = 1 \u2212 \u03bd / 4, and use the fact that Ct + 1 \u2264 Ct.Proof Lemma 3.3. Let us not specify fixed iteration. Substitute c1 = 1 \u2212 \u03bd / 2, c2 = 1 \u2212 \u03bd / 4, and \u03b7 = C2t + 1 in an almost identical warranty for wolf line search (Telgarsky, 2012, Proposition D.6) (where Lemgarsky is simply the largest ratio between \"and\" \u2032 Prop. \""}, {"heading": "C. Deferred Material from Section 4", "text": "All convergence rates developed by Telgarsky (2012, Section 6) stem from an inequality (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A (A) (A) (A (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A) (A) (A (A) (A) (A) (A) (A (A) (A) (A) (A (A) (A (A) (A) (A) (A)"}], "references": [{"title": "Boosting a weak learning algorithm by majority", "author": ["Freund", "Yoav"], "venue": "Information and Computation,", "citeRegEx": "Freund and Yoav.,? \\Q1995\\E", "shortCiteRegEx": "Freund and Yoav.", "year": 1995}, {"title": "A decisiontheoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Friedman", "Jerome H"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman and H.,? \\Q2000\\E", "shortCiteRegEx": "Friedman and H.", "year": 2000}, {"title": "A hard-core predicate for all one-way functions", "author": ["Goldreich", "Oded", "Levin", "Leonid"], "venue": "STOC, pp", "citeRegEx": "Goldreich et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Goldreich et al\\.", "year": 1989}, {"title": "Hard-core distributions for somewhat hard problems", "author": ["Impagliazzo", "Russell"], "venue": "In FOCS, pp", "citeRegEx": "Impagliazzo and Russell.,? \\Q1995\\E", "shortCiteRegEx": "Impagliazzo and Russell.", "year": 1995}, {"title": "Cryptographic limitations on learning finite automata and boolean formulae", "author": ["Kearns", "Michael", "Valiant", "Leslie"], "venue": "STOC, pp", "citeRegEx": "Kearns et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1989}, {"title": "The convergence rate of AdaBoost", "author": ["Mukherjee", "Indraneel", "Rudin", "Cynthia", "Schapire", "Robert"], "venue": "In COLT,", "citeRegEx": "Mukherjee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2011}, {"title": "Soft margins for adaboost", "author": ["G. R\u00e4tsch", "T. Onoda", "M\u00fcller", "K.-R"], "venue": "Machine Learning,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2001\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2001}, {"title": "Efficient margin maximizing with boosting", "author": ["R\u00e4tsch", "Gunnar", "Warmuth", "Manfred"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "R\u00e4tsch et al\\.,? \\Q2005\\E", "shortCiteRegEx": "R\u00e4tsch et al\\.", "year": 2005}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["Reyzin", "Lev", "Schapire", "Robert E"], "venue": "Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Reyzin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Reyzin et al\\.", "year": 2006}, {"title": "The dynamics of AdaBoost: cyclic behavior and convergence of margins", "author": ["Rudin", "Cynthia", "Daubechies", "Ingrid", "Schapire", "Robert E"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 2004}, {"title": "Analysis of boosting algorithms using the smooth margin function", "author": ["Rudin", "Cynthia", "Schapire", "Robert E", "Daubechies", "Ingrid"], "venue": "Annals of Statistics,", "citeRegEx": "Rudin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rudin et al\\.", "year": 2007}, {"title": "Boosting: Foundations and Algorithms", "author": ["Schapire", "Robert E", "Freund", "Yoav"], "venue": null, "citeRegEx": "Schapire et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 2012}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["Schapire", "Robert E", "Singer", "Yoram"], "venue": "Machine Learning,", "citeRegEx": "Schapire et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1999}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["Schapire", "Robert E", "Freund", "Yoav", "Barlett", "Peter", "Lee", "Wee Sun"], "venue": "In ICML, pp", "citeRegEx": "Schapire et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1997}, {"title": "On the equivalence of weak learnability and linear separability: New relaxations and efficient boosting algorithms", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "In COLT, pp", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2008}, {"title": "The Cauchy-Schwarz Master Class", "author": ["Steele", "J. Michael"], "venue": null, "citeRegEx": "Steele and Michael.,? \\Q2004\\E", "shortCiteRegEx": "Steele and Michael.", "year": 2004}, {"title": "A primal-dual convergence analysis of boosting", "author": ["Telgarsky", "Matus"], "venue": null, "citeRegEx": "Telgarsky and Matus.,? \\Q2012\\E", "shortCiteRegEx": "Telgarsky and Matus.", "year": 2012}, {"title": "Totally corrective boosting algorithms that maximize the margin", "author": ["Warmuth", "Manfred K", "Liao", "Jun", "R\u00e4tsch", "Gunnar"], "venue": "In ICML, pp", "citeRegEx": "Warmuth et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Warmuth et al\\.", "year": 2006}, {"title": "Boosting with early stopping: Convergence and consistency", "author": ["Zhang", "Tong", "Yu", "Bin"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}, {"title": "The only other thing to check is that ` \u2208 G, the class of losses considered by Telgarsky (2012", "author": [], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Proof sketch. As discussed in the proof of Theorem 4.3, the results of Telgarsky (2012), which are superficially specialized to the Wolfe line search, carry over for the other line searches here with only a change", "author": ["\u2265 \u03b3"], "venue": null, "citeRegEx": "\u03b3\u0302.,? \\Q2012\\E", "shortCiteRegEx": "\u03b3\u0302.", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "One explanation for the efficacy of boosting is that it not only seeks aggregates with low empirical risk, but moreover that it prefers good margins, which leads to improved generalization (Schapire et al., 1997).", "startOffset": 189, "endOffset": 212}, {"referenceID": 11, "context": "Since AdaBoost does not attain maximum margins on general instances, a push was made to develop methods which carry such a guarantee (R\u00e4tsch & Warmuth, 2005; Shalev-Shwartz & Singer, 2008; Rudin et al., 2007).", "startOffset": 133, "endOffset": 208}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates.", "startOffset": 21, "endOffset": 42}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.", "startOffset": 21, "endOffset": 424}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al.", "startOffset": 21, "endOffset": 850}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al.", "startOffset": 21, "endOffset": 881}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary).", "startOffset": 21, "endOffset": 906}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme.", "startOffset": 21, "endOffset": 1101}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)).", "startOffset": 21, "endOffset": 1560}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf.", "startOffset": 21, "endOffset": 1773}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature.", "startOffset": 21, "endOffset": 2024}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature.", "startOffset": 21, "endOffset": 2149}, {"referenceID": 7, "context": "A third work, due to R\u00e4tsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of R\u00e4tsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature. It is worth mentioning that these methods produce bad constants when applied to the logistic loss; unfortunately, previous work also suffers in this case (for instance, the work of Collins et al. (2002) provided only convergence of empirical risk, and not rates).", "startOffset": 21, "endOffset": 2508}, {"referenceID": 18, "context": "6). The explicit margin-maximizing method of ShalevShwartz & Singer (2008) requires t \u2265 32 ln(m)/ 2 iterations to achieve margin \u03b3 \u2212 , where \u2208 (0, \u03b3).", "startOffset": 0, "endOffset": 75}, {"referenceID": 6, "context": "position due to Telgarsky (2012), the decomposition itself has appeared, with various guarantees, in numerous places (Goldreich & Levin, 1989; Impagliazzo, 1995; Mukherjee et al., 2011).", "startOffset": 117, "endOffset": 185}, {"referenceID": 18, "context": "Indeed, consider the \u201csoft-margin\u201d boosting method (Shalev-Shwartz & Singer, 2008), originally due to Warmuth et al. (2006), which, roughly speaking, has a parameter controlling how many examples to give up on.", "startOffset": 102, "endOffset": 124}, {"referenceID": 10, "context": ") It should be possible to clarify this behavior from the perspective of dynamical systems: smaller steps dodge bad attractors (Rudin et al., 2004; 2007).", "startOffset": 127, "endOffset": 153}, {"referenceID": 10, "context": "in the case that \u03bd = 1, this quantity has been extensively studied in the context of AdaBoost\u2019s margins (R\u00e4tsch & Warmuth, 2005; Rudin et al., 2004; Schapire & Freund, 2012) \u2666", "startOffset": 104, "endOffset": 173}], "year": 2013, "abstractText": "This manuscript shows that AdaBoost and its immediate variants can produce approximate maximum margin classifiers simply by scaling step size choices with a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman\u2019s empirically successful \u201cshrinkage\u201d procedure for gradient boosting (Friedman, 2000). Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.", "creator": "LaTeX with hyperref package"}}}