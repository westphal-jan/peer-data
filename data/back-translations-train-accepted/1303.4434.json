{"id": "1303.4434", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems", "abstract": "Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.", "histories": [["v1", "Mon, 18 Mar 2013 21:41:53 GMT  (98kb)", "http://arxiv.org/abs/1303.4434v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA stat.CO stat.ML", "authors": ["pinghua gong", "changshui zhang", "zhaosong lu", "jianhua huang", "jieping ye"], "accepted": true, "id": "1303.4434"}, "pdf": {"name": "1303.4434.pdf", "metadata": {"source": "META", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems", "authors": ["Pinghua Gong", "Jianhua Z. Huang"], "emails": ["gph08@mails.tsinghua.edu.cn", "zcs@mail.tsinghua.edu.cn", "zhaosong@sfu.ca", "jianhua@stat.tamu.edu", "jieping.ye@asu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.44 34v1 [cs.LProceedings of the 30th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W & CP Volume 28. Copyright 2013 by the author (s).Borwein (BB) rule, which allows to quickly find a suitable step size. Also, the paper presents a detailed convergence analysis of the GIST algorithm, demonstrating the efficiency of the proposed algorithm through extensive experiments on large-scale datasets."}, {"heading": "1. Introduction", "text": "The aforementioned brainVo nvo nlrVo nvo rf\u00fc nde nlrVo nvo rf\u00fc nde nlrVo, eerdw the braintee\u00fccnlrVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo nvo rf\u00fc the hirnVo, nlteeeeeew the hirnVo nvo rf\u00fc the hirnVo zu.ndne"}, {"heading": "2. The Proposed Algorithm: GIST", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. General Problems", "text": "We consider the solution to the following general problem: min w = Rd {f (w) = l (w) + r (w)}. (1) Throughout the essay, we assume that A1 l (w) is continuously differentiable with Lipschitz, i.e., there is a positive constant \u03b2 (l), so that the difference between two convex functions, i.e. r (w) = r1 (w) \u2212 r2 (w), where r1 (w) and r2 (w) are convex functions that may not be smooth and non-convex, and can be rewritten as the difference between two convex functions, i.e. (w) = r1 (w) \u2212 r2 (w), where r1 (w) and r2 (w) are convex functions that may not be uniform and non-convex."}, {"heading": "2.2. Some Examples", "text": "Many formulations in machine learning fulfil the above assumptions. The following least quadratic and logistical loss functions are two commonly used ones that satisfy the assumption A1: l (w) = 12n \u0445 Xw \u2212 y \u04412 or1nn \u2211 i = 1log (1 + exp (\u2212 yix T i w)), where X = [xT1; \u00b7 \u00b7; x T n] \u0441 R n \u00b7 d is a data matrix and y = [y1, \u00b7 \u00b7, yn] T-Rn is a target vector. Regulators (penalties) that satisfy the assumption A2 are in Table 1. They are not convex (with the exception of the \u03c61 standard) and are widely used in sparse learning. The above mentioned functions l (w) and r (w) are not negative. Therefore, f is limited from below and fulfils the assumption A3."}, {"heading": "2.3. Algorithm", "text": "Our proposed general iterative shrinkage and threshold (GIST) is a solution (1) to all listed problems by generating a sequence {w (k)} via: w (k + 1) = argmin w (k) + < w (k), w \u2212 w (k), w \u2212 w (k), w \u2212 w (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k), p (k)."}, {"heading": "2.3.2. Line Search Criterion", "text": "Specifically, we propose to accept the step size 1 / t (k) on the outer iteration k if the following monotone line search criterion is fulfilled: f (w (k + 1)) \u2264 f (w (k)) \u2212 \u03c32 t (k) \u0394w (k + 1) \u2212 w (k) \u0445 2, (3) where \u03c3 is a constant in the interval (0, 1). It may accept the step size 1 / t (k), even if w (k + 1) is a larger objective function value than w (k) \u2212 \u2212 In particular, we propose to accept the step size 1 / t (k) p (p), if w \u00b7 t (k) is smaller than the step size 1 / t (k), even if w (k + 1) is a larger function value than w (k) \u2212 \u2212 we propose to accept the step size 1 / t (p) p (p), p, p, k, k (k) k, k + k k k (k) k (k) k (k) less than k p p, p, p, p, k p, k (k) p, k, k (k), k k + k k k k (k)."}, {"heading": "2.3.3. Convergence Analysis", "text": "Dec c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "2.3.4. Discussions", "text": "Note that l (w (k)) + < p (w (k), w \u2212 w (k) > + t (k) 2, w \u2212 w (k) 2 can be considered an approximation of l (w) at w = w (k) for each outer iteration. Instead of the approximate replacement function in problem (1), the GIST algorithm minimizes each outer iteration. We also observe that if t (k) \u2265 \u03b2 (l) / (1 \u2212 \u03c3) > \u03b2 (l) [the sufficient state of equation (3)], we have reached t (w) \u2264 l (w (k) + < p (w (k)), w \u2212 w (k) > + t (k) > w \u2212 w (k) 2, w \u2212 w (k)."}, {"heading": "3. Related Work", "text": "In this section we will discuss some related algorithms (w = k). A commonly used approach to solving the problem (1) is the problem (1) of multi-stage (MS) (MS). (3) It is the problem (1). (3) MS algorithms solve the problem (1). (3) MS algorithms solve the problem (1). (3) MS algorithms solve the problem (1). (4) MS algorithms solve the problem (1). (4) MS algorithms solve the problem (1) by creating a sequence (3). (4) MS algorithms solve the problem (1). (4) MS algorithms solve the problem (1). (4) MS algorithms solve the problem (1). (4) MS algorithms solve the problem (1)."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setup", "text": "We evaluate our GIST algorithm by taking into account the Capped-1 problem of regulated logistic regression, i.e. l (w) = 1n \u2211 n i = 1 log (1 + exp (\u2212 yix T i w)) and r (w) = \u03bb \u2211 di = 1 min (| wi |, \u03b8). We compare our GIST algorithm with the multi-level (MS) algorithm and the SCP algorithm in different settings using twelve datasets grouped in Table 2. These datasets are high-dimensional and sparse. Two of them (news20, realsim) 1 were pre-processed as two-class datasets (Lin et al., 2008). The other Ten2 are multi-class datasets. We transform the multi-class datasets into two-class datasets by labeling the first half of all classes as a positive class and labeling the remaining classes as a negative class. All algorithms are implemented in Matlab function CR and all data sets (4GB) on an Intel (GHR = 4GB)."}, {"heading": "4.2. Experimental Evaluation and Analysis", "text": "We report on the objective function value vs. CPU time diagrams with different parameter settings in Figure 1. From these figures, the following observations emerge: (1) Both GISTbb monotones and GISTbbNonmonotones rapidly decrease the objective function value, and they always have the fastest convergence speed, showing that adopting the BB rule to initialize t (k) actually greatly accelerates the convergence speed. Furthermore, both GISTbb monotons and GISTbb nonmonotone algorithms achieve the smallest objective function values. (2) GISTbbNonmonotone can lead to an increasing objective function value, but eventually converges and has a faster overall convergence speed than GISTbb monotons in most cases, suggesting that the non-monotonous time criterion can further accelerate these values (see: http: / / www.GISTb.data.ws / http: / www.GIST.data.csiew.w / multiple)."}, {"heading": "5. Conclusions", "text": "We propose an efficient iterative shrinkage and threshold algorithm to solve a general class of non-convex optimization problems arising from sparse learning. A critical step of the proposed algorithm is the calculation of a proximal operator, which has a closed solution for many commonly used formulations. We propose to initialize the step size with each iteration using the BB rule and apply both monotonous and non-monotonous criteria as line search conditions, which considerably accelerate the convergence speed. In addition, we offer a detailed convergence analysis of the proposed algorithm, which shows that the algorithm converges between both monotonous and non-monotonous search criteria. The results of experiments on large-scale datasets show the rapid convergence of the proposed algorithm. In our future work we will focus on the analysis of theoretical performance (e.g. error-based prediction, error-bound solution achieved by the IST algorithm, etc.)."}, {"heading": "Acknowledgements", "text": "This work is partially supported by 973 Program (2013CB329503), NSFC (Grant No. 91120301, 61075004, 61021063), NIH (R01 LM010730) and NSF (IIS-0953662, CCF-1025177, DMS1208952)."}, {"heading": "Appendix: Solutions to Problem (2)", "text": "Note that r (w) = \u03b8 di = 1 ri (wi) and problem (2) can be equally divided into d-independent univariate optimization problems: w (k + 1) i = argminwihi (wi) = 12 (wi \u2212 u (k) i) 2 + 1t (k) ri (wi), where i = 1, \u00b7 \u00b7, d and u (k) i is the i-th entry of u (k) = w (k) \u2212 l (w (k)) / t (k). To simplify the notation, we decipher the above equation by decoding the characters and supscripts as follows: w (k + 1) = argmin whi (w) = 12 (w \u2212 u) 2 + 1 t ri (w)."}], "references": [{"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J.M. Borwein"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Barzilai and Borwein,? \\Q1988\\E", "shortCiteRegEx": "Barzilai and Borwein", "year": 1988}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Candes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2008}, {"title": "Signal recovery by proximal forward-backward splitting", "author": ["P.L. Combettes", "V.R. Wajs"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "Combettes and Wajs,? \\Q2005\\E", "shortCiteRegEx": "Combettes and Wajs", "year": 2005}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Fan and Li,? \\Q2001\\E", "shortCiteRegEx": "Fan and Li", "year": 2001}, {"title": "Sparsest solutions of underdetermined linear systems via lq-minimization for 0 < q \u2264 1", "author": ["S. Foucart", "M.J. Lai"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Foucart and Lai,? \\Q2009\\E", "shortCiteRegEx": "Foucart and Lai", "year": 2009}, {"title": "Recovering sparse signals with a certain family of nonconvex penalties and dc programming", "author": ["G. Gasso", "A. Rakotomamonjy", "S. Canu"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Gasso et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gasso et al\\.", "year": 2009}, {"title": "Nonlinear image recovery with half-quadratic regularization", "author": ["D. Geman", "C. Yang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Geman and Yang,? \\Q1995\\E", "shortCiteRegEx": "Geman and Yang", "year": 1995}, {"title": "Multi-stage multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In NIPS, pp. 1997\u20132005,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Robust multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "In SIGKDD,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Nonmonotone globalization techniques for the barzilai-borwein gradient method", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Grippo and Sciandrone,? \\Q2002\\E", "shortCiteRegEx": "Grippo and Sciandrone", "year": 2002}, {"title": "A nonmonotone line search technique for newton\u2019s method", "author": ["L. Grippo", "F. Lampariello", "S. Lucidi"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Grippo et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Grippo et al\\.", "year": 1986}, {"title": "A fixed-point continuation method for l1-regularized minimization with applications to compressed sensing", "author": ["E.T. Hale", "W. Yin", "Y. Zhang"], "venue": "CAAM TR07-07,", "citeRegEx": "Hale et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hale et al\\.", "year": 2007}, {"title": "Quantile regression via an mm algorithm", "author": ["D.R. Hunter", "K. Lange"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Hunter and Lange,? \\Q2000\\E", "shortCiteRegEx": "Hunter and Lange", "year": 2000}, {"title": "Trust region newton method for logistic regression", "author": ["C.J. Lin", "R.C. Weng", "S.S. Keerthi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Iterative reweighted minimization methods for lp regularized unconstrained nonlinear programming", "author": ["Z. Lu"], "venue": "arXiv preprint arXiv:1210.0066,", "citeRegEx": "Lu,? \\Q2012\\E", "shortCiteRegEx": "Lu", "year": 2012}, {"title": "Sequential convex programming methods for a class of structured nonlinear programming", "author": ["Z. Lu"], "venue": "arXiv preprint arXiv:1210.3039,", "citeRegEx": "Lu,? \\Q2012\\E", "shortCiteRegEx": "Lu", "year": 2012}, {"title": "A simple and efficient algorithm for gene selection using sparse logistic regression", "author": ["S.K. Shevade", "S.S. Keerthi"], "venue": null, "citeRegEx": "Shevade and Keerthi,? \\Q2003\\E", "shortCiteRegEx": "Shevade and Keerthi", "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "A duality principle for non-convex optimisation and the calculus of variations", "author": ["Toland", "JF"], "venue": "Archive for Rational Mechanics and Analysis,", "citeRegEx": "Toland and JF.,? \\Q1979\\E", "shortCiteRegEx": "Toland and JF.", "year": 1979}, {"title": "Relaxed conditions for sparse signal recovery with general concave priors", "author": ["J. Trzasko", "A. Manduca"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Trzasko and Manduca,? \\Q2009\\E", "shortCiteRegEx": "Trzasko and Manduca", "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wright et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2008}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R. Nowak", "M. Figueiredo"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}, {"title": "Sparse methods for biomedical data", "author": ["J. Ye", "J. Liu"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Ye and Liu,? \\Q2012\\E", "shortCiteRegEx": "Ye and Liu", "year": 2012}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation,", "citeRegEx": "Yuille and Rangarajan,? \\Q2003\\E", "shortCiteRegEx": "Yuille and Rangarajan", "year": 2003}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["C.H. Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang,? \\Q2010\\E", "shortCiteRegEx": "Zhang", "year": 2010}, {"title": "Analysis of multi-stage convex relaxation for sparse regularization", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang,? \\Q2010\\E", "shortCiteRegEx": "Zhang", "year": 2010}, {"title": "Multi-stage convex relaxation for feature selection", "author": ["T. Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2012\\E", "shortCiteRegEx": "Zhang", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "The l1-norm regularizer, a continuous and convex surrogate, has been studied extensively in the literature (Tibshirani, 1996; Efron et al., 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al.", "startOffset": 107, "endOffset": 145}, {"referenceID": 5, "context": "The l1-norm regularizer, a continuous and convex surrogate, has been studied extensively in the literature (Tibshirani, 1996; Efron et al., 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al.", "startOffset": 107, "endOffset": 145}, {"referenceID": 24, "context": ", 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al., 2008; Beck & Teboulle, 2009; Wright et al., 2009; Ye & Liu, 2012).", "startOffset": 141, "endOffset": 247}, {"referenceID": 25, "context": ", 2004) and has been applied successfully to many applications including signal/image processing, biomedical informatics and computer vision (Shevade & Keerthi, 2003; Wright et al., 2008; Beck & Teboulle, 2009; Wright et al., 2009; Ye & Liu, 2012).", "startOffset": 141, "endOffset": 247}, {"referenceID": 2, "context": "Although the l1norm based sparse learning formulations have achieved great success, they have been shown to be suboptimal in many cases (Candes et al., 2008; Zhang, 2010b; 2012), since the l1-norm is a loose approximation of the l0-norm and often leads to an over-penalized problem.", "startOffset": 136, "endOffset": 177}, {"referenceID": 2, "context": "They include lq-norm (0 < q < 1) (Foucart & Lai, 2009), Smoothly Clipped Absolute Deviation (SCAD) (Fan & Li, 2001), Log-Sum Penalty (LSP) (Candes et al., 2008), Minimax Concave Penalty (MCP) (Zhang, 2010a), Geman Penalty (GP) (Geman & Yang, 1995; Trzasko & Manduca, 2009) and Capped-l1 penalty (Zhang, 2010b; 2012; Gong et al.", "startOffset": 139, "endOffset": 160}, {"referenceID": 25, "context": "Remark 1 We say that w is a critical point of problem (1), if the following holds (Toland, 1979; Wright et al., 2009): 0 \u2208 \u2207l(w) + \u2202r1(w )\u2212 \u2202r2(w ), where \u2202r1(w ) is the sub-differential of the function r1(w) at w = w , that is, \u2202r1(w ) = { s : r1(w) \u2265 r1(w ) + \u3008s,w\u2212w\u3009, \u2200w \u2208 R }", "startOffset": 82, "endOffset": 117}, {"referenceID": 13, "context": "(3) is a nonmonotone line search criterion (Grippo et al., 1986; Grippo & Sciandrone, 2002; Wright et al., 2009).", "startOffset": 43, "endOffset": 112}, {"referenceID": 25, "context": "(3) is a nonmonotone line search criterion (Grippo et al., 1986; Grippo & Sciandrone, 2002; Wright et al., 2009).", "startOffset": 43, "endOffset": 112}, {"referenceID": 22, "context": "Inspired by Wright et al. (2009); Lu (2012a), we present detailed convergence analysis under both monotone and non-monotone line search criteria.", "startOffset": 12, "endOffset": 33}, {"referenceID": 18, "context": "(2009); Lu (2012a), we present detailed convergence analysis under both monotone and non-monotone line search criteria.", "startOffset": 8, "endOffset": 19}, {"referenceID": 8, "context": "One commonly used approach to solve problem (1) is the Multi-Stage (MS) convex relaxation (or CCCP, or DC programming) (Zhang, 2010b; Yuille & Rangarajan, 2003; Gasso et al., 2009).", "startOffset": 119, "endOffset": 180}, {"referenceID": 4, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 14, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 25, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 17, "context": "A class of related algorithms called iterative shrinkage and thresholding (IST), which are also known as different names such as fixed point iteration and forward-backward splitting (Daubechies et al., 2004; Combettes & Wajs, 2005; Hale et al., 2007; Beck & Teboulle, 2009; Wright et al., 2009; Liu et al., 2009), have been extensively applied to solve problem (1).", "startOffset": 182, "endOffset": 312}, {"referenceID": 18, "context": "The most related algorithm to our propose GIST is the Sequential Convex Programming (SCP) proposed by Lu (2012b). SCP solves problem (1) by generating a sequence {w} as w = argmin w\u2208R l(w) + \u3008\u2207l(w),w \u2212w\u3009", "startOffset": 102, "endOffset": 113}, {"referenceID": 16, "context": "Two of them (news20, realsim) have been preprocessed as two-class data sets (Lin et al., 2008).", "startOffset": 76, "endOffset": 94}], "year": 2013, "abstractText": "Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the BarzilaiProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s). Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.", "creator": "LaTeX with hyperref package"}}}