{"id": "1303.4664", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2013", "title": "Large-Scale Learning with Less RAM via Randomization", "abstract": "We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.", "histories": [["v1", "Tue, 19 Mar 2013 17:00:22 GMT  (59kb,D)", "http://arxiv.org/abs/1303.4664v1", "Extended version of ICML 2013 paper"]], "COMMENTS": "Extended version of ICML 2013 paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel golovin", "d sculley", "h brendan mcmahan", "michael young"], "accepted": true, "id": "1303.4664"}, "pdf": {"name": "1303.4664.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Learning with Less RAM via Randomization", "authors": ["Daniel Golovin", "Brendan McMahan"], "emails": ["dgg@google.com", "dsculley@google.com", "mcmahan@google.com", "mwyoung@google.com"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Related Work", "text": "A classic approach to reducing memory usage is to promote economy, for example through the lasso variant (Tibshirani, 1996) of the smallest squares regression and the more general application of L1 regulators (Duchi et al., 2008; Langford et al., 2009; Xiao, 2009; McMahan, 2011); a more recent trend has been to reduce memory usage costs through the use of feature hashing (Weinberger et al., 2009); both approaches are effective, and the rough coding schemes described here can be used in conjunction with these methods to further reduce memory usage. Randomized rounds Randomized rounding schemes have been widely used in numerical processing and algorithm design (Raghavan & Tompson, 1987)."}, {"heading": "3. Learning with Randomized Rounding and Probabilistic Counting", "text": "For concreteness, we focus on logistic regression with binary feature vectors \u03b2 \u03b2 = \u03b2 = Fixed Effects \u03b2 = \u03b2 = Fixed Effects (0, 1) d and labels y (0, 1). The model has coefficients \u03b2-Rd and gives predictions p\u03b2 (x) increasing (\u03b2, 1 + e \u2212 z) is the logistic function. Logistic regression finds the model that minimizes the logistic loss L. In the face of a marked example (x, y) the logistic loss isL (x, y) \u2261 \u2212 y log (p\u03b2 (x) \u2212 y) log (1 \u2212 y) log (1 \u2212 p\u03b2 (x))), in which we take 0 log 0 = 0 to be the natural logarithm. We define x-p as the \"p norm of a vector x; if the subscript p is ignored, the\" 2 norm is implied."}, {"heading": "3.1. Regret Bounds for Randomized Rounding", "text": "We are considering theoretical guarantees (in the form of regret limits) for a variant of OGD that uses randomized rounding on an adaptive grid, as well as pro-coordinate learning rates. (These limits can also be applied to a fixed grid.) We are using the standard definition variables, and since we allow the adversary to adjust based on the previously observed \u03b2 t, the ft- and post-hoc optimal \u03b2 variables are also random variables. We are proving that the expectation of randomization used by our algorithms is also possible.) We are considering regret in relation to the best model in the comparison class F = \u2212 R, i.e. We are pursuing the usual reduction of functions to which we are using."}, {"heading": "3.2. Approximate Feature Counts", "text": "Online methods of convex optimization typically use a learning rate that decreases over time, for example, by being throut proportional to 1 / \u221a t. Pro-coordinate learning rates require the storage of a unique counting \u03c4i for each coordinate, where \u03c4i is the number of times the coordinate i has previously appeared with an unequal gradient. Significant space is saved by using an 8-bit randomized counting scheme instead of a 32-bit (or 64-bit) integer to store the d total number. We use a variant of Morris \"probability counting algorithm (1978), analyzed by Flajolet (1985). Specifically, we initialize a counting scheme C = 1, and for each increment operation we increase C with the probability number p (C) = b \u2212 C, where base b \u2212 C is a parameter. We estimate the counting steps as C = unassumed, which is an estimate b \u2212 b \u2212 er \u2212 1."}, {"heading": "4. Encoding During Prediction Time", "text": "Many problems in the real world require large-scale predictions. The achievement of scales can require a trained model to be replicated on several machines (\u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 = \u03b2 \u03b2 = \u03b2 \u03b2 = \u03b2 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5. Experimental Results", "text": "In fact, it is a very complex situation where most people are able to survive themselves, as if they were able to survive themselves, \"he said.\" But it's not as if they are able to survive themselves. \"He added,\" It's not as if they are able to survive themselves, as if they are able to survive themselves. \""}, {"heading": "6. Conclusions", "text": "While we focus on OGD in this paper, similar randomized rounding schemes can be applied to other learning algorithms, and extending to algorithms that handle L1 regularization efficiently, such as RDA (Xiao, 2009) and FTRL-Proximal (McMahan, 2011), is relatively straightforward. [4] Large kernel machines, matrix decompositions, theme models, and other large-scale learning methods can all be modified to take advantage of RAM savings through precise randomized rounding methods."}, {"heading": "Acknowledgments", "text": "We would like to thank Matthew Streeter, Gary Holt, Todd Phillips and Mark Rose for their help in this work."}, {"heading": "A. Appendix: Proofs", "text": "The proof of theory 3.\u03b2 2 becomes the additional regret due to the random answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the answer to the question after the answer to the answer to the question after the answer to the answer to the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the question after the answer to the question after the question after the answer to the question after the question after the answer to the question after the question after the answer to the question after the answer to the question after the answer to the question after the question after the answer to the question after the question after the answer to the question after the question after the answer to the question after the question after the answer to the question after the answer to the question after the answer to the question after the answer to the question after the question after the answer to the question after the answer to the answer to the question after the question after the answer to the question after the answer to the question after the question after the question after the question after the question after the question after the answer after the answer after the question after the answer after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the answer after the answer after the answer after the question after the question after the question after the question after the question after the answer after the answer after the question after the question after the question after the question after the question after the answer after the answer after the answer after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the question after the"}], "references": [{"title": "Predictive client-side profiles for personalized advertising", "author": ["Bilenko", "Mikhail", "Richardson", "Matthew"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Bilenko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bilenko et al\\.", "year": 2011}, {"title": "Compact dictionaries for variable-length keys and data with applications", "author": ["Blandford", "Daniel K", "Blelloch", "Guy E"], "venue": "ACM Trans. Algorithms,", "citeRegEx": "Blandford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blandford et al\\.", "year": 2008}, {"title": "Beating the hold-out: bounds for k-fold and progressive crossvalidation", "author": ["Blum", "Avrim", "Kalai", "Adam", "Langford", "John"], "venue": "In Proceedings of the twelfth annual conference on Computational learning theory,", "citeRegEx": "Blum et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1999}, {"title": "The tradeoffs of large scale learning", "author": ["Bottou", "L\u00e9on", "Bousquet", "Olivier"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2008}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "An experimental comparison of click positionbias models", "author": ["Craswell", "Nick", "Zoeter", "Onno", "Taylor", "Michael", "Ramsey", "Bill"], "venue": "In Proceedings of the international conference on Web search and web data mining,", "citeRegEx": "Craswell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Craswell et al\\.", "year": 2008}, {"title": "Efficient projections onto the l1ball for learning in high dimensions", "author": ["Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "In COLT,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Approximate counting: A detailed analysis", "author": ["Flajolet", "Philippe"], "venue": "BIT, 25(1):113\u2013134,", "citeRegEx": "Flajolet and Philippe.,? \\Q1985\\E", "shortCiteRegEx": "Flajolet and Philippe.", "year": 1985}, {"title": "Spam and the ongoing battle for the inbox", "author": ["Goodman", "Joshua", "Cormack", "Gordon V", "Heckerman", "David"], "venue": "Commun. ACM, 50(2),", "citeRegEx": "Goodman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2007}, {"title": "Sparse online learning via truncated gradient", "author": ["Langford", "John", "Li", "Lihong", "Zhang", "Tong"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization", "author": ["McMahan", "H. Brendan"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "McMahan and Brendan.,? \\Q2011\\E", "shortCiteRegEx": "McMahan and Brendan.", "year": 2011}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["McMahan", "H. Brendan", "Streeter", "Matthew"], "venue": "In COLT,", "citeRegEx": "McMahan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2010}, {"title": "Counting large numbers of events in small registers", "author": ["Morris", "Robert"], "venue": "Communications of the ACM,", "citeRegEx": "Morris and Robert.,? \\Q1978\\E", "shortCiteRegEx": "Morris and Robert.", "year": 1978}, {"title": "Randomized rounding: a technique for provably good algorithms and algorithmic proofs", "author": ["Raghavan", "Prabhakar", "Tompson", "Clark D"], "venue": "Combinatorica, 7(4),", "citeRegEx": "Raghavan et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Raghavan et al\\.", "year": 1987}, {"title": "Predicting clicks: estimating the click-through rate for new ads", "author": ["Richardson", "Matthew", "Dominowska", "Ewa", "Ragno", "Robert"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2012}, {"title": "Less regret via online conditioning", "author": ["Streeter", "Matthew J", "McMahan", "H. Brendan"], "venue": "CoRR, abs/1002.4862,", "citeRegEx": "Streeter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2010}, {"title": "String hashing for linear probing", "author": ["Thorup", "Mikkel"], "venue": "In Proceedings of the 20th ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Thorup and Mikkel.,? \\Q2009\\E", "shortCiteRegEx": "Thorup and Mikkel.", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Probabilistic counting with randomized storage", "author": ["Van Durme", "Benjamin", "Lall", "Ashwin"], "venue": "In Proceedings of the 21st international jont conference on Artifical intelligence,", "citeRegEx": "Durme et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Durme et al\\.", "year": 2009}, {"title": "Feature hashing for large scale multitask learning", "author": ["Weinberger", "Kilian", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alex", "Attenberg", "Josh"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Weinberger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["Xiao", "Lin"], "venue": "In NIPS,", "citeRegEx": "Xiao and Lin.,? \\Q2009\\E", "shortCiteRegEx": "Xiao and Lin.", "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 15, "context": "This is true for training massive-scale distributed learning systems, such as those used for predicting ad click through rates (CTR) for sponsored search (Richardson et al., 2007; Craswell et al., 2008; Bilenko & Richardson, 2011; Streeter & McMahan, 2010) or for filtering email spam at scale (Goodman et al.", "startOffset": 154, "endOffset": 256}, {"referenceID": 5, "context": "This is true for training massive-scale distributed learning systems, such as those used for predicting ad click through rates (CTR) for sponsored search (Richardson et al., 2007; Craswell et al., 2008; Bilenko & Richardson, 2011; Streeter & McMahan, 2010) or for filtering email spam at scale (Goodman et al.", "startOffset": 154, "endOffset": 256}, {"referenceID": 9, "context": ", 2008; Bilenko & Richardson, 2011; Streeter & McMahan, 2010) or for filtering email spam at scale (Goodman et al., 2007).", "startOffset": 99, "endOffset": 121}, {"referenceID": 6, "context": "Smaller Models A classic approach to reducing memory usage is to encourage sparsity, for example via the Lasso (Tibshirani, 1996) variant of least-squares regression, and the more general application of L1 regularizers (Duchi et al., 2008; Langford et al., 2009; Xiao, 2009; McMahan, 2011).", "startOffset": 219, "endOffset": 289}, {"referenceID": 10, "context": "Smaller Models A classic approach to reducing memory usage is to encourage sparsity, for example via the Lasso (Tibshirani, 1996) variant of least-squares regression, and the more general application of L1 regularizers (Duchi et al., 2008; Langford et al., 2009; Xiao, 2009; McMahan, 2011).", "startOffset": 219, "endOffset": 289}, {"referenceID": 21, "context": "A more recent trend has been to reduce memory cost via the use of feature hashing (Weinberger et al., 2009).", "startOffset": 82, "endOffset": 107}, {"referenceID": 6, "context": "Per-Coordinate Learning Rates Duchi et al. (2010) and McMahan & Streeter (2010) demonstrated that per-coordinate adaptive regularization (i.", "startOffset": 30, "endOffset": 50}, {"referenceID": 6, "context": "Per-Coordinate Learning Rates Duchi et al. (2010) and McMahan & Streeter (2010) demonstrated that per-coordinate adaptive regularization (i.", "startOffset": 30, "endOffset": 80}, {"referenceID": 2, "context": "Metrics are computed using progressive validation (Blum et al., 1999) as is standard for online learning: on each round a prediction is made for a given example and record for evaluation, and only after that is the model allowed to train on the example.", "startOffset": 50, "endOffset": 69}], "year": 2013, "abstractText": "We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement percoordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.", "creator": "TeX"}}}