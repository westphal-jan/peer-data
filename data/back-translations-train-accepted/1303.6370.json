{"id": "1303.6370", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2013", "title": "Convex Tensor Decomposition via Structured Schatten Norm Regularization", "abstract": "We discuss structured Schatten norms for tensor decomposition that includes two recently proposed norms (\"overlapped\" and \"latent\") for convex-optimization-based tensor decomposition, and connect tensor decomposition with wider literature on structured sparsity. Based on the properties of the structured Schatten norms, we mathematically analyze the performance of \"latent\" approach for tensor decomposition, which was empirically found to perform better than the \"overlapped\" approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific mode, this approach performs as good as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structures Schatten norms, establish the consistency, and discuss the identifiability of this approach. We confirm through numerical simulations that our theoretical prediction can precisely predict the scaling behavior of the mean squared error.", "histories": [["v1", "Tue, 26 Mar 2013 02:36:49 GMT  (458kb)", "http://arxiv.org/abs/1303.6370v1", "12 pages, 3 figures"]], "COMMENTS": "12 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NA", "authors": ["ryota tomioka", "taiji suzuki"], "accepted": true, "id": "1303.6370"}, "pdf": {"name": "1303.6370.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 3,63 70v1 [st at.M L] 26 Mar 201 3 000 001 002 003 004 005 006 007 008 009 010 0 1 012 013 014 015 017 018 019 021 022 023 024 025 026 027 028 029 030 031 032 033 0 4 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 077 078 079 081 082 083 084 085 086 087 088 089 090 092 093 094 095 096 097 098 099 100 101 102 103 106 107 108 9"}, {"heading": "1. Introduction", "text": "In fact, it is a matter of a way in which people are able to survive themselves. (...) It is not as if people are able to survive themselves. (...) It is not as if people are able to survive themselves. (...) It is not as if people are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. \""}, {"heading": "2. Structured Schatten norms for tensors", "text": "In this section we define the overlapping shadow norm and the latent shadow norm and discuss their basic properties. First, we need some basic definitions.Let W-Rn1 \u00b7 \u00b7 \u00b7 nK be a K-way tensor. We denote the total number of entries in W by N = Kk = 1 nk. The point product between two tensors W and X is defined as < W, X > = vec (W) vec (X); i.e. the point product as vectors in RN. The Frobenius norm of a tensor is defined as a matrix achieved by concatenating the mode-k fibers along the columns."}, {"heading": "2.1. Overlapped Schatten norms", "text": "The overlapped Sp / q-norm is described as follows: the overlapped Sp / q-norm, which includes the overlapping Sp / q-norm, which includes the overlapping Sp / q-norm, which includes the overlapping Sp / q-norm. (The overlapped Sp / q-norm is described as follows: the overlapped Sp / q-norm, which includes the overlapped Sp / q-norm, which includes the overlapped Sp / q-norm), the overlapped Sp / q-norm, the overlapped Sp / q-norm, the overlapped Sp / qm-norm, the overlapped Sp-norm. (The overlapped Sp / q-norm), the overlapped Sp / q-norm."}, {"heading": "2.2. Latent Schatten norms", "text": "The latent approach to tensor decomposition was developed by Tomioka et al. (2011a) solves the following minimization difficulties: W (1), W (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K), K (K, K, K, K (K), K (K), K (K, K, K, K, K (K), K (K), K (K), K (K), K (K), K (K), K (K)."}, {"heading": "3. Main theoretical results", "text": "In this section, we will examine the consistency, generalization power, and identifiability of the latent approach to tensor decomposition associated with restoring an unknown tensor W \u0445 from noisy measurements. This is the framework of the experiment in Figure 2. First, we show that the latent approach is consistent, that is, the error goes back to zero when the noise drops to zero, which is the situation when the entries are repeatedly observed. Second, by combining the duality presented in the previous section with the techniques of Agarwal et al. (2011), we will analyze the denoizing power of the latent approach in connection with restoring an unknown tensor W \u0445 from noisy measurements. This is the framework of the experiment in Figure 2. First, we will prove a deterministic inequality that maintains the regularization constant under certain conditions. Next, we will assume that Gaussian noise exhibits an inequality, the likelihood of a high scalability."}, {"heading": "3.1. Consistency", "text": "Let us consider W & W as the underlying true tensor, and the noisy version Y results as follows: Y = W + 442 + E, where E + Rn1 \u00b7 \u00b7 \u00b7 \u00b7 nK is the noise tensor. \u2212 First, let us determine the consistency of the latent approach. \u2212 S1 / 1) That is, if the noise goes to zero (e.g. if the entries are observed repeatedly), W \u2212 W \u00b2 for each sequence. \u2212 S1 \u2212 S1 S1 S1 / 1 S0 S1 / 1 S1 S1 / 1 S1 S1 / 1 S1 S1 / 2 S0 S1 / 2 S1 / 2 S1 S1 / 2 S0 S1 / 2 S1 \u2212 S0 S1 S0 S1 S1 S1 / 2 S0 S1 S1 / 2 S1 S0 S1 / 2 S0 S1 / 2 S1 S0 S1 / 2 S0 446 S1 446 S1 446 S1 446 S1 446 S0 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 / 1 446 S1 446 S1 446 S1 446 S1 446 / 1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 446 S1 / 1 / 1 446 S1 446 S1 446 S1 / 1 446 S1 446 S1 / 1 / 1 446 S1 / 1 446 S1 446 S1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 0 / 0 / 1 / 0 S1 446 S1 446 S1 446 S1 / 1 446 S1 / 1 446 S1 / 1 / 1 / 1 / 0 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 / 1 /"}, {"heading": "3.2. Deterministic bound", "text": "The consistency of the statement in the previous section deals with all-in-one K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-"}, {"heading": "3.3. Gaussian noise", "text": "If the elements of the noise tensor E are Gaussian random variables with variance \u03c32, we obtain the following theory.Theorem 3. Let us assume that the elements of the noise tensor E are independent Gaussian random variables with variance \u03c32. Furthermore, we assume without loss of generality that the dimensionalities of W * are sorted in descending order, i.e., n1 \u00b2 \u00b7 \u00b7 nK. Then there are universal constants c0, c1, so that with a high probability any solution of the minimization problem (11) with the regulation constant \u03bb = c0\u03c3 (\u221a N / nK + \u221a n1 + \u221a logK) + \u03b1 (K \u2212 1) meets the following limit: 1NK \u00b2 k = 1. Appendix."}, {"heading": "3.4. Comparison with the overlapped approach", "text": "The imbalance achieved in (Tomioka et al., 2011b) for the overlapping approach, the overlapping shadow 1-Norm (1) can be expressed as follows: 1N-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm, 1K-Norm-Norm, 1K-Norm, 1K-Norm-1K-Norm, 1K-1K-1K-Norm, 1K-1K-1K-Norm, 1K-1K-1K-1K-Norm, 1K-1K-1K-1K-Norm, 1K-1K-Norm, 1K-1K-Norm-1K-Norm, 1K-1K-Norm, 1K-Norm-1K-Norm, 1K-1K-Norm-Norm, 1K-Norm-Norm, 1K-1K-Norm-Norm, 1K-Norm, 1K-1K-Norm-Norm, 1K-1K-Norm, 1K-Norm-Norm, 1K-1K-Norm, 1K-Norm, 1K-1K-Norm, 1K-1K-Norm, 1K-1K-Norm, 1K-Norm, 1K-1K-Norm, 1K-1K-1K Norm, 1K-1K Norm, 1K Norm, 1K-1K Norm, 1K Norm, 1K Norm, 1K Norm, 1K Norm, 1K Norm, 1K"}, {"heading": "3.5. Discussion on the identifiability", "text": "(17) We say that a decomposition (17) is locally identifiable if there is no other decomposition. (17) Theorem 4. Decomposition (17) is locally identifiable if and only if W (k) = W (k) = W (k) = K (k) for k = K and W (k) = 0 otherwise for some k). Proof. The proof for this is in Appendix D. The above theorem partially explains the difficulty of assessing individual components W (k) without additional incoherence assumptions as in (10)."}, {"heading": "4. Numerical results", "text": "The aim of this experiment is to determine the actual number of 664 666 693 695 696 698 701 772 673 678 679 681 682 684 685 686 688 688 688 691 692 695 696 696 696 701 704 707 708 710 711 712 713 716 717 718 720 720 722 724 725 727 728 729 732 734 735 738 744 744 744 744 744 744 744 744 744 744 744 744 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 20 720 720 20 20 720 20 720 20 20 20 720 20 20 720 20 20 720 20 20 20 20 720 720 20 20 20 720 720 720 20 720 720 20 20 20 720 720 20 720 20 20 720 720 720 720 20 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 20 20 20 20 20 20 720 720 20 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 20 20 20 20 20 20 20 20 20 720 20 20 720 20 720 720 20 720 720 720 720 20 20 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720 720"}, {"heading": "5. Conclusion", "text": "In this paper, we have outlined a framework for structured shadow standards, which includes both the overlapping shadow-1 approach and the latent shadow-1 standard recently proposed in the context of convex optimization-based tensor decomposition (Signoretto et al., 2010; Gandy et al., 2011; Liu et al., 2009; Tomioka et al., 2011a), linking these studies with the more comprehensive studies on structured sparseness (Bach et al., 2011; Jenatton et al., 2011; Obozinski et al., 2011; Maurer & Pontil et al, 2011). In addition, we have demonstrated a duality that holds between the two types of standards. Furthermore, we have rigorously examined the performance of the latent approach to tensor decomposition. We have shown the consistency of latent shadow-1 standard decomposition that is not standardized."}, {"heading": "A. Proof of Lemma 1", "text": "Evidence. From the definition it emerges that the double norm, the double norm, the double norm, the double norm, the double norm, the double norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm, the norm and the norm and the norm, the norm and the norm, the norm and the norm and the norm, the norm and the norm, the norm and the norm and the norm, the norm and the norm and the norm, the norm and the norm and the norm, the norm and the norm and the norm and the norm, the norm and the norm and the norm and the norm, the norm and the norm and the norm and the norm and the norm and the norm, the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm, the norm and the norm and the norm and the norm and the norm and the norm, the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm, the norm and the norm and the norm and the norm and the norm and the norm and the norm and the norm and the"}, {"heading": "B. Proof of Theorem 2", "text": "(k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (n) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k) (k)"}, {"heading": "C. Proof of Theorem 3", "text": "Since each entry of E is an independent zero-man-Gaussian random variable with variance \u03c32, for each mode k we have the following tail limit (corollary limit 5.35 in (Vershynin, 2010)) P (EE (k) E (S) E (N / nk + n k) + t) \u2264 exp (\u2212 t2 / (2\u03c32)).Next we take a compound limit P (max k) E (k) E (n / nk + n k k) + t) \u2264 K exp (\u2212 t2 / (2\u03c32)).If we replace t (t + n).logK, we have P (n / nk) E (n / nk + n k).maxk (n / nk + n k) \u2264 K + p (n / k).exp (\u2212 t) T (n / k). \u2212 Em (n / k) E (n / nk) E (n / nk) + k)."}, {"heading": "D. Proof of Theorem 4", "text": "The proof that we have the \"if.\" K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K (K) -K -K --K --K --K --K -K -K (K) -K (K) -K (K) -K (K) -K) -K -K (K) -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K (K) -K (K) -K) -K (K) -K -K (K) -K) -K (K) -K -K -K -K (K) -K (K) -K) -K (K) -K -K (K) -K (K) -K) -K (K) -K (K) -K -K -K -K (K) -K -K -K -K (K) -K (K) -K) -K -K -K (K) -K (K) -K) -K -K (K) -K (K) -K) -K -K -K -K (K) -K (K) -K) -K (K) -K) -K -K -K -K -K (K) -K (K) -K (K) -K) -K -K (K) -K) -K -K -K -K -K -K -K -K -K (K) -K -K -K -K -K -K -K (K) -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K (K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K -K"}], "references": [{"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "author": ["A. Agarwal", "S. Negahban", "M.J. Wainwright"], "venue": "Technical report,", "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "Convex optimization with sparsity-inducing norms. In Optimization for Machine Learning", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": null, "citeRegEx": "Bach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2011}, {"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "Technical report,", "citeRegEx": "Candes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2009}, {"title": "On the best rank-1 and rank-(R1", "author": ["L. De Lathauwer", "B. De Moor", "J. Vandewalle"], "venue": "RN ) approximation of higher-order tensors. SIAM J. Matrix Anal. Appl.,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "A Rank Minimization Heuristic with Application to Minimum Order System Approximation", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "In Proc. of the American Control Conference,", "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "Concentration-based guarantees for low-rank matrix reconstruction", "author": ["R. Foygel", "N. Srebro"], "venue": "Technical report,", "citeRegEx": "Foygel and Srebro,? \\Q2011\\E", "shortCiteRegEx": "Foygel and Srebro", "year": 2011}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Comput. Math. Appl.,", "citeRegEx": "Gabay and Mercier,? \\Q1976\\E", "shortCiteRegEx": "Gabay and Mercier", "year": 1976}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Problems,", "citeRegEx": "Gandy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gandy et al\\.", "year": 2011}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Hsu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2011}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan"], "venue": "Advances in NIPS", "citeRegEx": "Jalali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jalali et al\\.", "year": 2010}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["R. Jenatton", "J.Y. Audibert", "F. Bach"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Jenatton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2011}, {"title": "Lpnorm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Kloft et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2011}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review,", "citeRegEx": "Kolda and Bader,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader", "year": 2009}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "In Prof. ICCV,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Convex and network flow optimization for structured sparsity", "author": ["J. Mairal", "R. Jenatton", "G. Obozinski", "F. Bach"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Mairal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2011}, {"title": "Structured sparsity and generalization", "author": ["A. Maurer", "M. Pontil"], "venue": "Technical report,", "citeRegEx": "Maurer and Pontil,? \\Q2011\\E", "shortCiteRegEx": "Maurer and Pontil", "year": 2011}, {"title": "Learning the kernel function via regularization", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Micchelli and Pontil,? \\Q2005\\E", "shortCiteRegEx": "Micchelli and Pontil", "year": 2005}, {"title": "Applications of tensor (multiway array) factorizations and decompositions in data mining", "author": ["M. M\u00f8rup"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,", "citeRegEx": "M\u00f8rup,? \\Q2011\\E", "shortCiteRegEx": "M\u00f8rup", "year": 2011}, {"title": "Group lasso with overlaps: the latent group lasso approach", "author": ["G. Obozinski", "L. Jacob", "J.P. Vert"], "venue": "Technical report,", "citeRegEx": "Obozinski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Obozinski et al\\.", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "Nuclear norms for tensors and their use for convex multilinear estimation", "author": ["M. Signoretto", "L. De Lathauwer", "J.A.K. Suykens"], "venue": "Technical Report 10-186,", "citeRegEx": "Signoretto et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2010}, {"title": "Low-dimensional procedure for the characterization of human faces", "author": ["L. Sirovich", "M. Kirby"], "venue": "J. Opt. Soc. Am. A,", "citeRegEx": "Sirovich and Kirby,? \\Q1987\\E", "shortCiteRegEx": "Sirovich and Kirby", "year": 1987}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "Advances in NIPS", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Estimation of low-rank tensors via convex optimization", "author": ["R. Tomioka", "K. Hayashi", "H. Kashima"], "venue": "Technical report,", "citeRegEx": "Tomioka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tomioka et al\\.", "year": 2011}, {"title": "Statistical performance of convex tensor decomposition", "author": ["R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima"], "venue": "Advances in NIPS", "citeRegEx": "Tomioka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tomioka et al\\.", "year": 2011}, {"title": "Multilinear analysis of image ensembles: Tensorfaces", "author": ["M. Vasilescu", "D. Terzopoulos"], "venue": "Computer Vision\u2014 ECCV", "citeRegEx": "Vasilescu and Terzopoulos,? \\Q2002\\E", "shortCiteRegEx": "Vasilescu and Terzopoulos", "year": 2002}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Technical report,", "citeRegEx": "Vershynin,? \\Q2010\\E", "shortCiteRegEx": "Vershynin", "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "For example, in neuroimaging, we are often interested in finding spatio-temporal patterns of neural activities that are related to certain experimental conditions or subjects; one way to do this is to compute the decomposition of the data tensor, which can be of size channels \u00d7 time-points \u00d7 subjects \u00d7 conditions (M\u00f8rup, 2011).", "startOffset": 315, "endOffset": 328}, {"referenceID": 20, "context": "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors (Signoretto et al., 2010; Gandy et al., 2011; Liu et al., 2009; Tomioka et al., 2011a), and its performance has been analyzed in (Tomioka et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 7, "context": "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors (Signoretto et al., 2010; Gandy et al., 2011; Liu et al., 2009; Tomioka et al., 2011a), and its performance has been analyzed in (Tomioka et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 13, "context": "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors (Signoretto et al., 2010; Gandy et al., 2011; Liu et al., 2009; Tomioka et al., 2011a), and its performance has been analyzed in (Tomioka et al.", "startOffset": 108, "endOffset": 194}, {"referenceID": 4, "context": "folded matrices to be simultaneously low-rank based on the Schatten 1-norm, which is also known as the trace norm and nuclear norm (Fazel et al., 2001; Srebro et al., 2005; Recht et al., 2010); see the left panel of Figure 1.", "startOffset": 131, "endOffset": 192}, {"referenceID": 22, "context": "folded matrices to be simultaneously low-rank based on the Schatten 1-norm, which is also known as the trace norm and nuclear norm (Fazel et al., 2001; Srebro et al., 2005; Recht et al., 2010); see the left panel of Figure 1.", "startOffset": 131, "endOffset": 192}, {"referenceID": 19, "context": "folded matrices to be simultaneously low-rank based on the Schatten 1-norm, which is also known as the trace norm and nuclear norm (Fazel et al., 2001; Srebro et al., 2005; Recht et al., 2010); see the left panel of Figure 1.", "startOffset": 131, "endOffset": 192}, {"referenceID": 1, "context": "This result is closely related and generalize the results in structured sparsity literature (Bach et al., 2011; Jenatton et al., 2011; Obozinski et al., 2011; Maurer & Pontil, 2011).", "startOffset": 92, "endOffset": 181}, {"referenceID": 10, "context": "This result is closely related and generalize the results in structured sparsity literature (Bach et al., 2011; Jenatton et al., 2011; Obozinski et al., 2011; Maurer & Pontil, 2011).", "startOffset": 92, "endOffset": 181}, {"referenceID": 18, "context": "This result is closely related and generalize the results in structured sparsity literature (Bach et al., 2011; Jenatton et al., 2011; Obozinski et al., 2011; Maurer & Pontil, 2011).", "startOffset": 92, "endOffset": 181}, {"referenceID": 9, "context": "The latent group lasso predicts with a mixture of group sparse weights (see also Wright et al., 2010; Jalali et al., 2010; Agarwal et al., 2011).", "startOffset": 71, "endOffset": 144}, {"referenceID": 0, "context": "The latent group lasso predicts with a mixture of group sparse weights (see also Wright et al., 2010; Jalali et al., 2010; Agarwal et al., 2011).", "startOffset": 71, "endOffset": 144}, {"referenceID": 20, "context": "The low-rank inducing norm studied in (Signoretto et al., 2010; Gandy et al., 2011; Liu et al., 2009; Tomioka et al., 2011a), which we call overlapped Schatten 1-norm, can be written as follows:", "startOffset": 38, "endOffset": 124}, {"referenceID": 7, "context": "The low-rank inducing norm studied in (Signoretto et al., 2010; Gandy et al., 2011; Liu et al., 2009; Tomioka et al., 2011a), which we call overlapped Schatten 1-norm, can be written as follows:", "startOffset": 38, "endOffset": 124}, {"referenceID": 13, "context": "The low-rank inducing norm studied in (Signoretto et al., 2010; Gandy et al., 2011; Liu et al., 2009; Tomioka et al., 2011a), which we call overlapped Schatten 1-norm, can be written as follows:", "startOffset": 38, "endOffset": 124}, {"referenceID": 14, "context": "It is related to the overlapped group regularization (see Jenatton et al., 2011; Mairal et al., 2011) in a sense that the same object W appears repeatedly in the norm.", "startOffset": 53, "endOffset": 101}, {"referenceID": 10, "context": "It is related to the overlapped group regularization (see Jenatton et al., 2011; Mairal et al., 2011) in a sense that the same object W appears repeatedly in the norm. The following inequality relates the overlapped Schatten 1-norm with the Frobenius norm, which was a key step in the analysis of Tomioka et al. (2011b):", "startOffset": 58, "endOffset": 320}, {"referenceID": 9, "context": "Although being recognized in special instances (Jalali et al., 2010; Obozinski et al., 2011; Maurer & Pontil, 2011; Agarwal et al., 2011), to the best of our knowledge, this duality has not been presented in the generality of Lemma 1.", "startOffset": 47, "endOffset": 137}, {"referenceID": 18, "context": "Although being recognized in special instances (Jalali et al., 2010; Obozinski et al., 2011; Maurer & Pontil, 2011; Agarwal et al., 2011), to the best of our knowledge, this duality has not been presented in the generality of Lemma 1.", "startOffset": 47, "endOffset": 137}, {"referenceID": 0, "context": "Although being recognized in special instances (Jalali et al., 2010; Obozinski et al., 2011; Maurer & Pontil, 2011; Agarwal et al., 2011), to the best of our knowledge, this duality has not been presented in the generality of Lemma 1.", "startOffset": 47, "endOffset": 137}, {"referenceID": 23, "context": "The latent approach for tensor decomposition proposed by Tomioka et al. (2011a) solves the following minimization problem", "startOffset": 57, "endOffset": 80}, {"referenceID": 0, "context": "Second, combining the duality we presented in the previous section with the techniques from Agarwal et al. (2011), we analyze the denoising performance of the latent approach in the context of recovering an unknown tensor W\u2217 from noisy measurements.", "startOffset": 92, "endOffset": 114}, {"referenceID": 2, "context": "Note that such an additional incoherence assumption has also been used in (Candes et al., 2009; Wright et al., 2010; Agarwal et al., 2011; Hsu et al., 2011).", "startOffset": 74, "endOffset": 156}, {"referenceID": 0, "context": "Note that such an additional incoherence assumption has also been used in (Candes et al., 2009; Wright et al., 2010; Agarwal et al., 2011; Hsu et al., 2011).", "startOffset": 74, "endOffset": 156}, {"referenceID": 8, "context": "Note that such an additional incoherence assumption has also been used in (Candes et al., 2009; Wright et al., 2010; Agarwal et al., 2011; Hsu et al., 2011).", "startOffset": 74, "endOffset": 156}, {"referenceID": 23, "context": "For an observation Y, we computed tensor decompositions using the overlapped approach and the latent approach (11) using the solver available from the webpage of one of the authors of Tomioka et al. (2011a). The solver uses the alternating direction method of multipliers (Gabay &Mercier, 1976) and the algorithm is described in the above paper.", "startOffset": 184, "endOffset": 207}], "year": 2013, "abstractText": "We discuss structured Schatten norms for tensor decomposition that includes two recently proposed norms (\u201coverlapped\u201d and \u201clatent\u201d) for convex-optimization-based tensor decomposition, and connect tensor decomposition with wider literature on structured sparsity. Based on the properties of the structured Schatten norms, we mathematically analyze the performance of \u201clatent\u201d approach for tensor decomposition, which was empirically found to perform better than the \u201coverlapped\u201d approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific mode, this approach performs as good as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structures Schatten norms, establish the consistency, and discuss the identifiability of this approach. We confirm through numerical simulations that our theoretical prediction can precisely predict the scaling behaviour of the mean squared error.", "creator": null}}}