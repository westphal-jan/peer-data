{"id": "1303.6977", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2013", "title": "ABC Reinforcement Learning", "abstract": "This paper introduces a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The main advantage is that we only require a prior distribution on a class of simulators (generative models). This is useful in domains where an analytical probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique, even in this case. In addition, it can be seen as an extension of rollout algorithms to the case where we do not know what the correct model to draw rollouts from is. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is a sound methodology in principle, even when non-sufficient statistics are used.", "histories": [["v1", "Wed, 27 Mar 2013 20:51:33 GMT  (26kb)", "https://arxiv.org/abs/1303.6977v1", "16 pages, 4 figures"], ["v2", "Wed, 8 May 2013 12:54:53 GMT  (58kb)", "http://arxiv.org/abs/1303.6977v2", "10 pages, 4 figures"], ["v3", "Tue, 18 Jun 2013 09:42:59 GMT  (58kb)", "http://arxiv.org/abs/1303.6977v3", "10 pages, 4 figures"], ["v4", "Fri, 28 Jun 2013 11:18:26 GMT  (58kb)", "http://arxiv.org/abs/1303.6977v4", "Corrected version of paper appearing in ICML 2013"]], "COMMENTS": "16 pages, 4 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["christos dimitrakakis", "nikolaos tziortziotis"], "accepted": true, "id": "1303.6977"}, "pdf": {"name": "1303.6977.pdf", "metadata": {"source": "CRF", "title": "ABC Reinforcement Learning", "authors": ["Christos Dimitrakakis"], "emails": ["christos.dimitrakakis@gmail.com", "ntziorzi@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.69 77v4 [st at.M L] 28 Jun 20"}, {"heading": "1. Introduction", "text": "Bayesian reinforcement learning (Strens, 2000; Vlassis et al., 2012) is the decision theory approach (DeGroot, 1970) to solving the problem of enhanced learning. Apart from the fact that the calculation of posterior distributions and Bayesian optimal decision are often insoluble (Duff, 2002; Ross et al., 2008), there is another major difficulty in specifying the previous and model classes. While there are a number of non-parametric Bayesian model classes that can be used to estimate the dynamics of an unknown process, it may not be a trivial matter to select the right class and previous ones. On the other hand, it is often known that the process can be well approximated by a complex parametrized simulator. The question is how to use this knowledge when the best simulator is Baximry, Proceedings of the 30th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013."}, {"heading": "1.1. The setting", "text": "The policy of the agent is a procedure for selecting a sequence of actions, whereby the action at the time of the action is not at the point where it actually takes its turn. The environment responds to this sequence with a corresponding sequence of observations xt-X and rewards rt-R. This interaction may depend on the complete story1 h-H, in which H, (X-A-R) is the sequence of all state action sequences, since neither the actor nor the environment are necessarily finite order Markov. For example, the actor can learn, or the environment can be partially observed. In this work we use a number of abbreviations to simplify the notation. First, we designate the (random) prob-1A story can encompass several trajectories in the episodic environment."}, {"heading": "1.2. Related work and our contribution", "text": "The first difficulty in adopting a sequential decision-making approach is that the search for a solution to this problem is not always clear (Duff, 2002), even if it is a complex system, a system, a system, a way, a way, a way, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system, a system,"}, {"heading": "2. Approximate Bayesian Computation", "text": "Approximate Bayesian Computation encompasses a series of probability-free techniques in which only an approximate trailing value is calculated using simulation. We will first discuss how the standard Bayesian conclusion differs from the ABC conclusion in amplification learning, and then present a theorem on the quality of the ABC approach."}, {"heading": "2.1. Bayesian inference for reinforcement learning", "text": "Imagine that the story h-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p"}, {"heading": "2.2. ABC inference for reinforcement learning", "text": "The main idea of ABC is to approximate the samples from the posterior distribution via simulation. We need to generate a sequence of sample models (k) from the previous investigation and then generate the data h (k) from each. If the data generated is \"sufficiently close\" to the story, then the k-th model is accepted as a sample from the back investigation (k). Specifically, ABC requires that we define a roughly adequate statistical method for amplification theory (W, E, E). If we (h) \u2212 f (k))), then it is accepted as a sample from the back investigation (k). Algorithm 1 gives the method for amplification theory in detail. An important difference from the standard ABC posterior approximations is, as well as the exact conclusion, the dependence on the results. Note that even Remark 2.1 declares that the posterior distribution applied independently of the applied politics is not true."}, {"heading": "2.3. A Hoeffding-based utility statistic", "text": "In particular, given a history h including Ndat trajectories in the environment, where i-th trajectory gains the benefit U (i), we obtain an average estimate E-NdatU, 1 Ndat U (i). To test whether these are close enough, we use the Hoeffding inequality (Hoeffding, 1963). In fact, it is easy to see that probability is limited to at least 1 \u2212 \u043c, | E\u03c0\u00b5 U-E \u03c0 (k) U | lower by: | E-N datU-E-Ntrj k U-Firmax-ln (2 / 3) (Ndat + Ntrj) 2NdatNtrj, (2,4) 4For discrete observations, this is simply the measurement of the ball. In more general cases, it can be extended to an appropriate measurement, which is very large if the benefit of the Utrj function (which we observe) is very high."}, {"heading": "3. ABC reinforcement learning", "text": "We now present a simple algorithm for ABC amplification learning, based on the ideas explained in the previous section. For each given set of observations and strategies, we draw a number of sample environments from the previous distribution. For each environment, we execute the relevant strategies and calculate the corresponding statistics. If these are close enough to the observed statistics, the sample will be accepted. The next step is to find a good policy for the simulation simulator. As we can draw any number of rollouts in the simulator, any kind of approximate dynamic programming algorithms can be used. In our experiments, we used LSPI (Lagoudakis & Parr, 2003b), which is easy to program and effective. It is hoped that if the approximate posterior sampling is reasonable, then we can use our prior knowledge of the environmental class to learn a good policy with less data, at the expense of additional calculations."}, {"heading": "3.1. ABC-LSPI", "text": "Consider the class of continuous, discrete Markov decision-making processes (MDPs), and then a number of sample-based ADP algorithms can be used to find good strategies, such as the Customized Q-Iteration (FQI) (Ernst et al., 2005) and the Least Square Policy Iteration (LSPI) (Lagoudakis & Parr, 2003b) that we use here. Since we take an arbitrary number of trajectories from the sampled MDP, an important algorithmic parameter is the number of rollouts Nrol to draw. Higher values result in better approximation values at the expense of additional computation. Since LSPI uses a linear value function5 approximation, it is necessary to choose an appropriate basis for suitability for good. The computational complexity of ABC-LSPI depends on the quality of the approximation we want to achieve, and on the number of samples we have to get close to when a sample is required to perform a statistical model."}, {"heading": "4. Experiments", "text": "We conducted some experiments to test the viability of ABC-RL, with all algorithms implemented using (?). In these we compared ABC-LSPI with LSPI. The intuition is that if ABC can find a good simulator, we can do a much better estimation of the value function by using a large number of samples5 The value function V (s) is simply the expected benefit that depends on the system state s. We omit details as this is not necessary to understand the proposed framework from the simulator, rather than evaluating the value function directly from the observations."}, {"heading": "4.1. Domains", "text": "We will consider two areas to illustrate ABC-RL. In both areas, we have access to a series of parameterized simulators M = {\u00b5\u03b8 | \u03b8 \u0432} for domains. However, we do not know the true parameters of domains. For ABC, the recorded parameters are taken from a uniform distribution unit Unif (2001), with the goal of bringing a car to the top of a hill. The problem has 7 parameters: upper and lower limits on the horizontal position of the car, upper and lower limits on the speed of the car, upper and lower limits on the speed of the car, maximum acceleration, gravity and finally the height of uniform noise. The real goal of the pendulum motion is the pendulum motion, upper and lower limits on the horizontal position of the car, upper and lower limits on the speed of the car, maximum acceleration, gravity and finally the height of uniform noise."}, {"heading": "4.2. Results", "text": "We compared the offline performance of LSPI and ABC-LSPI in the two areas, although ABC trajectories were evaluated in the real environment with a uniform random policy. These trajectories are used by both ABC-LSPI and LSPI to estimate a policy, and this policy is then evaluated via 103 trajectories. The experiment was repeated for 102 runs. As LSPI requires a basis, in both cases we have a uniform 4-4 grid of RBFs as well as an additional unit for estimation. The results of the experiment are shown in Fig, where we have found the expected benefit (with a discount factor = 0.99) of the policy as the number of trajectories increases, as well as an improved basis for the value function. Both LSPI and ABC-LSPI manage to find an improved policy with more data."}, {"heading": "5. Conclusion", "text": "This method is particularly interesting in areas where it is difficult to determine a suitable probabilistic model, and where the calculation is significantly cheaper than data acquisition. However, in principle, it is generally applicable to any type of affirmation problem, including continuous, partially observable and multiagent areas. We also introduce a general theorem for the quality of approximate ABC posterior distribution, which can be used for further analysis of ABC methods. We then applied ABC inference to affirmation problems. This includes the use of ABC inference on affirmation problems. This includes the use of simulation both to estimate approximate posterior distributions and to find good strategies. Thus, ABC-RL can be considered simultaneously as an extension of ABC inference for controlling problems and an extension of approximate dynamic programming methods to approximate lamp-free approximate ramp values. The main advantage if we do not have a probabilistic model is a reasonable one."}, {"heading": "A. Collected proofs", "text": "The probability of drawing a model in B-M is: The probability of drawing a model in B-M is: the probability that the probability of drawing a model in B-Z is: the probability of drawing a model in B-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}], "references": [{"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Agrawal", "Shipra", "Goyal", "Navi"], "venue": "COLT", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Near-optimal BRL using optimistic local transitions", "author": ["M. Araya", "V. Thomas", "O Buffet"], "venue": "In ICML,", "citeRegEx": "Araya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Araya et al\\.", "year": 2012}, {"title": "Rollout algorithms for constrained dynamic programming", "author": ["Bertsekas", "Dimitri P"], "venue": "Technical Report LIDS 2646, Dept. of Electrical Engineering and Computer Science,", "citeRegEx": "Bertsekas and P.,? \\Q2006\\E", "shortCiteRegEx": "Bertsekas and P.", "year": 2006}, {"title": "Using linear programming for Bayesian exploration in Markov decision processes", "author": ["Castro", "Pablo Samuel", "Precup", "Doina"], "venue": null, "citeRegEx": "Castro et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Castro et al\\.", "year": 2007}, {"title": "Approximate Bayesian computation (ABC) in practice", "author": ["K. Csill\u00e9ry", "M.G.B. Blum", "O.E. Gaggiotti", "O Fran\u00e7ois"], "venue": "Trends in ecology & evolution,", "citeRegEx": "Csill\u00e9ry et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Csill\u00e9ry et al\\.", "year": 2010}, {"title": "Asymptotic behaviour of approximate bayesian estimators", "author": ["Dean", "Thomas A", "Singh", "Sumeetpal S"], "venue": "arXiv preprint arXiv:1105.3655,", "citeRegEx": "Dean et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2011}, {"title": "Optimal Statistical Decisions", "author": ["DeGroot", "Morris H"], "venue": null, "citeRegEx": "DeGroot and H.,? \\Q1970\\E", "shortCiteRegEx": "DeGroot and H.", "year": 1970}, {"title": "Robust bayesian reinforcement learning through tight lower bounds", "author": ["Dimitrakakis", "Christos"], "venue": "In European Workshop on Reinforcement Learning (EWRL 2011),", "citeRegEx": "Dimitrakakis and Christos.,? \\Q2011\\E", "shortCiteRegEx": "Dimitrakakis and Christos.", "year": 2011}, {"title": "Rollout sampling approximate policy iteration", "author": ["Dimitrakakis", "Christos", "Lagoudakis", "Michail G"], "venue": "Machine Learning,", "citeRegEx": "Dimitrakakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2008}, {"title": "Optimal Learning Computational Procedures for Bayes-adaptive Markov Decision Processes", "author": ["Duff", "Michael O\u2019Gordon"], "venue": "PhD thesis, University of Massachusetts at Amherst,", "citeRegEx": "Duff and O.Gordon.,? \\Q2002\\E", "shortCiteRegEx": "Duff and O.Gordon.", "year": 2002}, {"title": "Differential privacy and robust statistics", "author": ["Dwork", "Cynthia", "Lei", "Jing"], "venue": "In Proceedings of the 41st annual ACM symposium on Theory of computing,", "citeRegEx": "Dwork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2009}, {"title": "Tree-based batch mode reinforcement learning", "author": ["Ernst", "Damien", "Geurts", "Pierre", "Wehenkel", "Louis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Classificationbased policy iteration with a critic", "author": ["Gabillon", "Victor", "Lazaric", "Alessandro", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno"], "venue": "ICML", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Using simulation methods for Bayesian econometric models: inference, development, and communication", "author": ["J. Geweke"], "venue": "Econometric Reviews,", "citeRegEx": "Geweke,? \\Q1999\\E", "shortCiteRegEx": "Geweke", "year": 1999}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Filtering via approximate bayesian computation", "author": ["Jasra", "Ajay", "Singh", "Sumeetpal S", "Martin", "James S", "McCoy", "Emma"], "venue": "Stat. Comput,", "citeRegEx": "Jasra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jasra et al\\.", "year": 2010}, {"title": "Thompson sampling: An optimal finite time analysis", "author": ["Kaufmanna", "Emilie", "Korda", "Nathaniel", "Munos", "R\u00e9mi"], "venue": "In ALT-2012,", "citeRegEx": "Kaufmanna et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmanna et al\\.", "year": 2012}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["Kolter", "J. Zico", "Ng", "Andrew Y"], "venue": "ICML", "citeRegEx": "Kolter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolter et al\\.", "year": 2009}, {"title": "Reinforcement learning as classification: Leveraging modern classifiers", "author": ["M. Lagoudakis", "R. Parr"], "venue": "In ICML, pp", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Approximate Bayesian computational methods", "author": ["J.M. Marin", "P. Pudlo", "C.P. Robert", "R.J. Ryder"], "venue": "Statistics and Computing,", "citeRegEx": "Marin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Marin et al\\.", "year": 2011}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "ICML", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Model-based Bayesian reinforcement learning in partially observable domains", "author": ["Poupart", "Pascal", "Vlassis", "Nikos"], "venue": "In International Symposium on Artificial Intelligence and Mathematics (ISAIM),", "citeRegEx": "Poupart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2008}, {"title": "Bayes-adaptive POMDPs", "author": ["Ross", "Stephane", "Chaib-draa", "Brahim", "Pineau", "Joelle"], "venue": "Advances in Neural Information Processing Systems 20,", "citeRegEx": "Ross et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Strens", "Malcolm"], "venue": "ICML", "citeRegEx": "Strens and Malcolm.,? \\Q2000\\E", "shortCiteRegEx": "Strens and Malcolm.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of two Samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems", "author": ["T. Toni", "D. Welch", "N. Strelkowa", "A. Ipsen", "M.P.H. Stumpf"], "venue": "Journal of the Royal Society Interface,", "citeRegEx": "Toni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Toni et al\\.", "year": 2009}, {"title": "Reinforcement Learning, chapter Bayesian Reinforcement Learning, pp. 359\u2013386", "author": ["N. Vlassis", "M. Ghavamzadeh", "S. Mannor", "P. Poupart"], "venue": null, "citeRegEx": "Vlassis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}, {"title": "Rollout sampling policy iteration for decentralized POMDPs", "author": ["Wu", "Feng", "Zilberstein", "Shlomo", "Chen", "Xiaoping"], "venue": "In The 26th conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 28, "context": "Bayesian reinforcement learning (Strens, 2000; Vlassis et al., 2012) is the decision-theoretic approach (DeGroot, 1970) to solving the reinforcement learning problem.", "startOffset": 32, "endOffset": 68}, {"referenceID": 23, "context": "However, apart from the fact that calculating posterior distributions and the Bayesoptimal decision is frequently intractable (Duff, 2002; Ross et al., 2008), another major difficulty is the specification of the prior and model class.", "startOffset": 126, "endOffset": 157}, {"referenceID": 4, "context": "We propose a simple, general, reinforcement learning framework employing the principles of Approximate Bayesian Computation (ABC, see (Csill\u00e9ry et al., 2010) for an overview) for performing Bayesian inference using simulation.", "startOffset": 134, "endOffset": 157}, {"referenceID": 26, "context": "On the other hand, simple heuristics such as Thompson sampling (Strens, 2000; Thompson, 1933) provide an efficient trade-off (Agrawal & Goyal, 2012; Kaufmanna et al.", "startOffset": 63, "endOffset": 93}, {"referenceID": 16, "context": "On the other hand, simple heuristics such as Thompson sampling (Strens, 2000; Thompson, 1933) provide an efficient trade-off (Agrawal & Goyal, 2012; Kaufmanna et al., 2012) between exploration and exploitation.", "startOffset": 125, "endOffset": 172}, {"referenceID": 1, "context": "Alghough other heuristics exist (Araya et al., 2012; Castro & Precup, 2007; Kolter & Ng, 2009; Poupart et al., 2006; Strens, 2000), in this paper we focus on an approximate version of Thompson sampling for reasons of simplicity.", "startOffset": 32, "endOffset": 130}, {"referenceID": 21, "context": "Alghough other heuristics exist (Araya et al., 2012; Castro & Precup, 2007; Kolter & Ng, 2009; Poupart et al., 2006; Strens, 2000), in this paper we focus on an approximate version of Thompson sampling for reasons of simplicity.", "startOffset": 32, "endOffset": 130}, {"referenceID": 23, "context": "The second difficulty is that in many interesting problems, the exact posterior calculation may be intractable, mainly due to partial observability (Poupart & Vlassis, 2008; Ross et al., 2008).", "startOffset": 148, "endOffset": 192}, {"referenceID": 12, "context": "Methods for finding good policies using simulation have been extensively studied before (Bertsekas, 2006; Bertsekas & Tsitsiklis, 1996; Dimitrakakis & Lagoudakis, 2008; Gabillon et al., 2011; Wu et al., 2010).", "startOffset": 88, "endOffset": 208}, {"referenceID": 29, "context": "Methods for finding good policies using simulation have been extensively studied before (Bertsekas, 2006; Bertsekas & Tsitsiklis, 1996; Dimitrakakis & Lagoudakis, 2008; Gabillon et al., 2011; Wu et al., 2010).", "startOffset": 88, "endOffset": 208}, {"referenceID": 15, "context": "As far as we know, this is a new and widely applicable result, although some other theoretical results using similar assumptions appear in (Jasra et al., 2010) and in (Dean & Singh, 2011) for hidden Markov models.", "startOffset": 139, "endOffset": 159}, {"referenceID": 26, "context": "This particular version of the algorithm can be seen as an ABC variant of Thompson sampling (Strens, 2000; Thompson, 1933).", "startOffset": 92, "endOffset": 122}, {"referenceID": 11, "context": "Then, a number of sample-based ADP algorithms can be used to find good policies, such as fitted Q-iteration (FQI) (Ernst et al., 2005) and least-square policy iteration (LSPI) (Lagoudakis & Parr, 2003b), which we use herein.", "startOffset": 114, "endOffset": 134}], "year": 2013, "abstractText": "We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound.", "creator": "LaTeX with hyperref package"}}}