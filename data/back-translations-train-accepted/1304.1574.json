{"id": "1304.1574", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2013", "title": "Generalization Bounds for Domain Adaptation", "abstract": "In this paper, we provide a new framework to obtain the generalization bounds of the learning process for domain adaptation, and then apply the derived bounds to analyze the asymptotical convergence of the learning process. Without loss of generality, we consider two kinds of representative domain adaptation: one is with multiple sources and the other is combining source and target data.", "histories": [["v1", "Thu, 4 Apr 2013 22:34:55 GMT  (421kb)", "http://arxiv.org/abs/1304.1574v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.PR", "authors": ["chao zhang", "lei zhang", "jieping ye"], "accepted": true, "id": "1304.1574"}, "pdf": {"name": "1304.1574.pdf", "metadata": {"source": "CRF", "title": "Generalization Bounds for Domain Adaptation", "authors": ["Chao Zhang", "Lei Zhang", "Jieping Ye"], "emails": ["zhangchao1015@gmail.com;", "jieping.ye@asu.edu", "zhanglei.njust@yahoo.com.cn"], "sections": [{"heading": null, "text": "ar Xiv: 130 4.15 74v1 [cs.LSpecifically, we use the integral probability metric to measure the difference between two domains. For each type of domain fit, we develop a related deviation from the Hoeffding type and symmetrical inequality to achieve the corresponding generalization limit based on the uniform entropy number. We also generalize the classic McDiarmid inequality to a more general environment in which independent random variables can take values from different domains. By using this inequality, we then obtain generalization limits based on the radiator complexity. We then analyze the asymptotic convergence and convergence of the learning process for such a type of domain fit. Meanwhile, we discuss the factors influencing the asymptotic behavior of the learning process, and the numerical experiments also support our theoretical results."}, {"heading": "1 Introduction", "text": "The generalization bound measures the likelihood that a function, selected from a function class by an algorithm, has a sufficiently small error and plays an important role in statistical learning theory (see Vapnik, 1999; Bousquet et al., 2004).The generalization boundaries have been widely used to study the consistency of the ERM-based learning process (Vapnik, 1999), the asymptotic convergence of the empirical process (Van der Vaart and Wellner, 1996) and the learning ability of learning models (Blumer et al., 1989).In general, there are three essential aspects to the generalization boundaries of a specific learning process: complexity levels of functional classes, deviation (or concentration) inequalities and symmetrization inequalities related to the learning process. For example, Van der Vaart and Wellner (1996) presents the generalization boundaries of the learning process that are based on the Rademacher complexity."}, {"heading": "1.1 Overview of Main Results", "text": "In this essay, we present a new framework to obtain the generalization limits of the learning process for the two types of representative domain fitting. On the basis of the resulting limits, we then analyze the asymptotic characteristics of the learning processes for the two types of domain fitting. There are four main aspects within the framework: \u2022 the quantity of difference between two domains; \u2022 the degree of complexity of the functional class; \u2022 the deviation inequalities of the learning process for the domain fitting; \u2022 the symmetry inequality of the learning process for the domain fitting. Generally, in order to obtain the generalization limits of a learning process, it is necessary to distinguish the related deviations (or concentration inequalities) of the learning process. For each type of domain fitting, we use a marginal method to develop the related Hoeffding-type deviation inequality. Furthermore, in the situation of the desired domain fitting, we must adapt the"}, {"heading": "1.2 Organization of the Paper", "text": "The rest of this paper is structured as follows: Section 2 presents the problems examined in this paper; Section 3 introduces the integral probability metric for measuring the difference between two domains; Section 4 presents two types of complexity measurements of function classes, including the uniform entropy number and the Rademacher complexity; Section 5 (or Section 6) presents the generalization limits of the multi-source domain matching learning process (or the combination of source and target data); and then analyzes the asymptotic behavior of the learning process in addition to the corresponding numerical experiment that supports our results; Section 7 compares the existing work on the theoretical analysis of domain matching; and the last section concludes the paper. In the appendices, we document the main results of this paper. To illustrate the presentation, we also postpone the discussion of deviation and symmetry inequalities in the appendices."}, {"heading": "2 Problem Setup", "text": "In this section, the main themes of this essay are formalized by introducing some necessary notations."}, {"heading": "2.1 Domain Adaptation with Multiple Sources", "text": "We call Z (S): = X (Sk) \u00b7 Y (Sk) \u00b7 K (S) \u00b7 K (S) \u00b7 K (S) \u00b7 K (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 S) \u00b7 W (S) \u00b7 W (S) \u00b7 S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 S) \u00b7 W (S) \u00b7 W (S) \u00b7 S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 S \u00b7 W (S) \u00b7 S \u00b7 S \u00b7 S \u00b7 W (S) \u00b7 S \u00b7 W (S) \u00b7 S \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 S \u00b7 W (S) \u00b7 W (S) \u00b7 W \u00b7 S \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S) \u00b7 W (S \u00b7 W (S) \u00b7 W (S) \u00b7 W (S \u00b7 W (S) \u00b7 W (S \u00b7 S \u00b7 S \u00b7 W (S) \u00b7 W (S) \u00b7 W (S \u00b7 S \u00b7 W (S) \u00b7 S \u00b7 W (S) \u00b7 S \u00b7 S \u00b7 W (S \u00b7 W (S) \u00b7 S \u00b7 W (S) \u00b7 W (S) \u00b7 W (S \u00b7 W (S) \u00b7 S \u00b7 W (S) \u00b7 W (S) \u00b7 S (S) \u00b7 W (S (S) \u00b7 W (S) \u00b7 W (S (S) \u00b7 W (S) \u00b7 S (S) \u00b7 W (S (S) \u00b7 W (S (S) \u00b7 S (S) \u00b7 W (S (S) \u00b7 W (S (S) \u00b7 W (S) \u00b7 S (S) \u00b7 S (S (S) \u00b7 W (S (S) \u00b7 W (S (S) \u00b7 W (S (S) \u00b7 W (S (S) \u00b7 W (S) \u00b7"}, {"heading": "2.2 Domain Adaptation Combining Source and Target Data", "text": "Denote Z (S): = X (S) \u00b7 Y (S) \u00b7 RI \u00b7 RJ and Z (T): = X (T) \u00b7 Y (T) \u00b7 RI \u00b7 RJ as source and target domains, respectively. Let D (S) and D (T) stand for the distributions of the input spaces X (S) and X (T), respectively. Denote g (S) \u2212 RJ as source and target data (S) \u2192 Y (T): X (T) \u2192 Y (T) as the labeling functions of Z (S) and Z (T), respectively. In the situation of domain matching, which combines source and target data (see Blitzer et al., 2008; Ben-David et al., 2010), the input space distributions D (S) and D (T), which differ from each other, or the labeling functions g (T), differ from each other."}, {"heading": "3 Integral Probability Metric", "text": "As shown in some existing papers (cf. Mansour et al., 2008, 2009a; Ben-David et al., 2010, 2006), one of the greatest challenges in the theoretical analysis of domain matching is to find a quantity to measure the difference between the source domain Z (S) and the target domain Z (T), and then use the quantity to reach generalizing limits for domain matching. In this section, we use the integral probability metric to measure the difference between the distributions of Z (S) and Z (T), and then discuss the relationship between the integral probability metric and other quantities proposed in existing work, e.g. H divergence and discrepancy difference (cf. Ben-David et al., 2010; Mansour et al., 2009b). Furthermore, we will show that there is a specific situation of domain matching where the integral probability metric performs better than other sities (cf. 3.1)."}, {"heading": "3.1 Integral Probability Metric", "text": "In Ben-David et al. (2010, 2006), H divergence was introduced to derive the boundaries of generalization based on the VC dimension (1997) (Rachric). (Mansour et al. (2009b) obtained the boundaries of generalization based on the Rademacher complexity by using the divergence. (Both quantities are aimed at measuring the difference between two input-space distributions T (S) and D (T). Furthermore, Mansour et al. (2009a) used the Re-Nyi divergence to measure the distance between two distributions. (In this paper, we use the following quantity to measure the difference between the distributions of the source and the target domains: Definition 3.1 Given two domains Z (S), Z (T), Z (T), and z (T), divergence (S) and z (T) can take the random variables, the values of Z (T) and Z (T)."}, {"heading": "3.2 Relationship with Other Quantities", "text": "Before the formal discussion, we briefly present the related quantities proposed in existing work (see Ben-David et al., 2010; Mansour et al., 2009b).3.2.1 H divergence and discrepancy in classification by using the absolute value loss function (x, y) = difference (x, y) = difference (x, y) between two (x), Ben-David et al. (2010) a variant of H divergence: dH (D), D (T) = sup g1, g2, H (S), E (S), E (g1), g2 (x), D (g1), x (T), g2 (x), g2 (T), D (T), D (14) to achieve VC sizing."}, {"heading": "4 Complexity Measures of Function Classes", "text": "Generally, the generalization of a certain learning process is achieved by the inclusion of a certain degree of complexity of the functional class, e.g. the coverage number, the VC dimension and the Rademacher complexity. In this essay, we mainly deal with the uniform entropy number and the Rademacher complexity."}, {"heading": "4.1 Uniform Entropy Number", "text": "The uniform entropy number is derived from the concept of domain adaptation and we refer to Mendelson (2003) for details. The coverage number of a function class F is defined as follows: Definition 4.1 F is a function class and d is a metric to F. In some classic results of statistical learning theory, the coverage number of F is applied in the radius of metric d by letting d be the distributional metric. For example, as shown in Theorem 2.3 by Mendelson (2003), d can be set as the norm 1 (Z N 1) and derive the generalization limit of the i.i.d. learning process by including the expectation of the coverage number, i.e., the domain of the free domain (F, 0, 1 (ZN1)."}, {"heading": "4.1.1 Domain Adaptation with Multiple Sources", "text": "To illustrate the representation we give a useful notation for the following discussion. Leave {ZNk1} Kk = 1: = {z (k) n} Nkn = 1 Kk = 1 the collection of samples from several sources {Z (Sk)} Kk = 1, respectively. Name {Z (Nk1) Kk = 1: = {z (k) n Nkn = 1 Kk = 1 like the collection of ghost samples from {Z (Sk)} Kk = 1 so that the ghost sample z (k) n has the same distribution as z (k) n for each 1 \u2264 k \u2264 K and each 1 \u2264 Nk. Note Z2Nk1: = {ZNk1, Z \u2032 Nk1} for each 1 \u2264 k \u2264 K \u2264 K. In addition, the f = (w1, \u00b7 \u00b7 \u00b7 \u00b7, wK) Nkk with each additional K = 1wk = 1 k = 1 k = 1 (1) that we use the variant (1wk) = 1k = 1 (1) (1)."}, {"heading": "4.1.2 Domain Adaptation Combining Source and Target Data", "text": "In the situation of domain adaptation, in which source and target data are combined, we must apply another variant of the \"1\" standard to F. If we allow ZNS1 = {z (S) n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 NSn = 1 and Z NT 1 = {z (T) n} NTn = 1 two sets of samples from the domains Z (S) and Z (T), we define \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (Z) n) n). (21) Note that the variant \"1\" NS1 \"(0, 1), Z NT 1\" is still a standard on the functional space that can be easily verified by the definition of the norm, so that we hereby leave it. \"Z,\" Z \"Z\" and T \"(1)."}, {"heading": "4.2 Rademacher Complexity", "text": "The Rademacher complexity is one of the most frequently used yardsticks for the complexity of function classes, and we refer to Van der Vaart and Wellner (1996); Mendelson (2003) for details. Definition 4.2 F is a function class and {zn} Nn = 1 is an example theorem taken from Z. Designation {\u03c3n} Nn = 1 is a set of random variables independently of each other, with each value taken from {\u2212 1, 1} with equal probability. Rademacher complexity of F is defined as R (F): = E sup f \u00b2 F {1N \u00b2 N = 1\u03c3nf (zn), where E stands for expectation taken in relation to all random variables {zn} Nn = 1 and {chsn} Nn = 1, which stands only for expectation."}, {"heading": "5 Learning Processes of Domain Adaptation with Multi-", "text": "In this section we present two generalization limits of the learning process for domain adaptation with multiple sources: They are based on the uniform entropy number or the Rademacher complexity. By using the derived limits on the basis of the uniform entropy number, we then analyze the asymptotic convergence and the convergence rate of the learning process. The numerical experiment also supports our theoretical analysis."}, {"heading": "5.1 Generalization Bounds", "text": "Based on the uniform entropy number expressed in (20), a generalization that is defined for domain adaptation with multiple sources, we will assume in the following Theorie.Theorem 5.1 that F is a functional category consisting of a number of different functions with the range [a, b).Let w = (w1, \u00b7 \u00b7 \u00b7 \u00b7, wK).Let w = (wK).Let w = (wK).Let w = (wS).Let w = (wS).Let w = (w1, \u00b7, wK).Let w (wK).Let (w1).K (wS).K (wS).K (wS).K (wS).K (wS).K (wS).K w.K w.K w.K w.K w.K w.K (wS).K w.K w.K w.K w.K w.K (wS).K w.K w.K (wS).K w.K w.K (wS).K (w.K).K w.K w.K (wS).K (w.K (wS).K w.K w.K (wS).K w.K (w.K).K (w.K (wS).K (w.K).K (w.K).K (w.K (wS).K (.K).K w.K (.K).K (.K (.K).K (.K).K (.K).K w.K (.K (.K).K (.K (.K).K (.K).K (.K).K (.K (.K).K (.K).K (.K (.K).K (.K).K (.K).K (.K (.K).K (.K).K (.K (.K).K (.K (.K).K (.K).K (.K).K (.K).K (.K).K (.K).K (.K).K (.K"}, {"heading": "5.2 Asymptotic Convergence", "text": "In statistical learning theory it is known that the complexity of the functional class is one of the main factors for the asymptotic convergence of the learning process under the assumption of equal distribution (Vapnik, 1999; Van der Vaart and Wellner, 1996; Mendelson, 2003). From Theorem 5.1 we can directly come to the following conclusion: The asymptotic convergence of the learning process for the domain adaptation with multiple sources is influenced by three aspects: the choice of w, the discrepancy D (w) F (S, T) and the uniform entropy number lnNw1 (F, E / 8, 2) Kk = 1Nk."}, {"heading": "5.3 Rate of Convergence", "text": "Referring to (25), we can find that the convergence rate is influenced by the choice of w. According to the Cauchy-Black inequality, which sets wk = Nk / \u2211 K = 1Nk (1 \u2264 k), we have a minimum of convergence {(Kk = 1Nk) 32 (b \u2212 a) 2 (K = 1w 2 k (i 6 = k Ni))))} = N1 + N2 + \u00b7 \u00b7 + NK 32 (b \u2212 a) 2, (32) which minimizes the second part of the learning process to the right of (25). Consequently, we find the fastest convergence rate of the learning process to O (1 / \u221a N), which corresponds to the classical result (27) of the learning process assuming the same distribution if the discrepancy D (w) F (S, T) is ignored."}, {"heading": "5.4 Numerical Experiment", "text": "We have performed the numerical experiments to verify the theoretical analysis of the asymptotic convergence of learning processes (\u03b2 = > T) subsequently (\u03b2 = > Convergence of learning processes for domain adaptation with multiple sources. Without loss of generality, we only consider the case of K = 2, i.e. there are two source domains and one target domain. Experimental data are generated in the following way: For the target domain Z (T) = X (T) \u00b7 Y (T) \u00b7 R100 \u00b7 R, we consider X (T) as 0.5% distribution N (0, 1) and draw {x (T) n} NTn = 1 (NT = 4000) of X (T) randomly and independently. Let us leave \u03b2 (R100) as a random vector of a Gaussian distribution N (1, 5), and let us leave the random vector R (R100) as a noise date with R-( 0, 0.5) and 4000."}, {"heading": "6 Learning Process of Domain Adaptation Combining", "text": "Source and Target DataIn this section, we present two generalization limits of the domain adaptation learning process based on the uniform entropy number and the 1SLEP package: http: / / www.public.asu.edu / \u0445 jye02 / Software / SLEP / index.htm of the Rademacher complexity. Then, in addition to the numerical experiments that support our theoretical analysis, we analyze the asymptotic convergence and convergence rate of the learning process."}, {"heading": "6.1 Generalization Bounds", "text": "The following theory provides a generalization based on the uniform entropy number in terms of metric inequality and symmetry (defined in (22).Similar to the situation of domain adaptation with multiple sources, the proof of this theorem is provided by the use of a specific Hoeffding type deviation inequality and a symmetrical inequality for domain adaptation (see Appendix B).Theorem 6.1 assumes that F is a functional class derived from the limited functions with bandwidth [a, b].Let ZNS1 = {z} n NSn = 1 and Z NT 1 = {z (T) n} NTn = 1 will have two sets of i.d. samples drawn from the domains Z (S) and Z (T), respectively. Then we will find for each individual domain [0, 1) and any other arbitrary solution (1)."}, {"heading": "6.2 Asymptotic Convergence", "text": "Following Theorem 6.1, we can directly obtain the relevant result, which indicates that the asymptotic convergence of the learning process for domain fitting is influenced by three factors: the uniform entropy number lnN \u03c41 (F, N / 8, 2 (NS + NT)), the holistic probability metric DF (S, T), and the choice of system requirements. Theorem 6.3 Let us assume that F is a function class consisting of limited functions with bandwidth [a, b]. If the following condition applies: lim NS \u2192 + \u221e lnN \u03c41 (F, N / 8, 2 (NS + NT) NSNT (((((1 \u2212 NT) 2NT + \u03c42NS) < + GOP (39) with the same condition for learning ability (1 \u2212 Z), then we have no condition for learning ability (S, T)."}, {"heading": "6.3 Rate of Convergence", "text": "We consider the choice of the threshold, which is an essential factor for the convergence rate in the learning process and is associated with the trade-off between the NS and NT sample numbers. If we (37) set the value of lnN \u03c41 (F, E, E, 8, 2 (NS + NT), we minimize the second term on the right side of (37) and then arrive at ln (F, E, E, T) f, E (T) f, E, NSDF (S, T) NS + NT + (((lnN \u03c41 (F, E, 8, 2 (NS + NT))) \u2212 ln (E / 8)) NS + NT 32 (b \u2212 a) 2) 1 2, (41) which implies that setting this threshold = NT NT + NST can lead to the fastest convergence rate, while also leading to the relatively greater discrepancy between the empirical risk and the expected NS (T, N, NT, NT, NT, NT, NT, N, N, NT, NT."}, {"heading": "6.4 Numerical Experiments", "text": "In the situation of domain adaptation, which combines source and target data, the samples {(x (T) n, y (T) n)} NTn = 1 (NT = 4000) of the target domain Z (T) are generated in the above manner (see (33)). We randomly select N \u2032 T = 100 samples from them to form the objective function, and the rest N \u2032 T = 3900 is used to test. Similarly, the samples {(x (S) n, y (S) n) n, (S) n, (S) n) n NSn = 1 (NS = 4000) of the source domain Z (S) are generated as follows: for each 1 \u2264 n NS, y (S) n = < x (S) n NS, (42), where x (S) n (NS) n (N), \u03b2 (S) n (N), \u03b2 (S) n), N (1), 5), and R (S) n) in order to resonate the square method."}, {"heading": "7 Prior Works", "text": "There has been some previous work on the theoretical analysis of multiple source domain adaptation (see Ben-David et al., 2010; Crammer et al., 2006, 2008; Mansour et al., 2008, 2009a) and domain adaptation, which combines source and target data (see Blitzer et al., 2008; Ben-David et al., 2010). Crammer et al. (2006, 2008) assumes that the functional class and the loss function meet the conditions of \"\u03b1 triangle inequality\" and \"uniform convergence bound.\" In addition, some prior information on the disparity between each source domain and the target domain must be obtained. Under these conditions, some generalization limits have been reached using the classical techniques that assume the same distribution.Mansour et al al al al al al al al al al al al al al al al al al al al. (2008) suggested another framework to investigate the problem of multiple source domain adaptation."}, {"heading": "8 Conclusion", "text": "In this book, we propose a new framework to obtain generalization boundaries of the learning process for two representative types of domain adaptation (2008), each in the form of multi-source domain adaptation and domain adaptation, the quantity of learning tasks, including classification and regression. Based on the derived boundaries, we theoretically analyze the asymptotic convergence and the rate of convergence of the learning process for domain adaptation. There are four important aspects of this framework: the quantity of the difference between two domains; the complexity of the functional class; the deviation from inequality; and the symmetric inequality for domain adaptation. \u2022 We use integral probability metrics DF (S, T) to measure the difference between two domains (S) and Z (T). We show that integrative probability is well defined and a (semi) metric one based on the space of probability distributions."}, {"heading": "A Proof of Theorem 5.1", "text": "To prove this, we need to develop the specific Hoeffding type deviation (or concentration), which plays an essential role in achieving the generalization limits for a particular learning process. Generally, specific deviation qualities need to be developed for different learning processes. There are many popular deviations and concentration qualities, e.g. Hoeffding's inequality, McDiarmid's inequality, Bennett's inequality, Bernstein's inequality and Talagrand's inequality. These results are all constructed on the assumption of the same distribution, and therefore they are not applicable (or at least not directly applicable) to the setting of multiple sources."}, {"heading": "B Proof of Theorem 6.1", "text": "We need to find the related Hoeffding type deviation inequality and symmetry inequality for the combination of the source and target domains. (B.1 Hoeffding type deviation InequalityBased on Hoeffding's inequality (Hoeffding, 1963), we derive a deviation inequality for the combination of the source and target domains. (Theorem B.1 Assumption that F is a function class consisting of limited functions with the range [a, b]. Let ZNS1: = {z (S) n} NSn = 1 and Z NT 1: = {z) n} NTn = 1, sets of i.e. samples are taken from the source domain Z (S)."}, {"heading": "C Proofs of Theorems 5.2 & 6.2", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "In this paper, we provide a new framework to obtain the generalization bounds of the<lb>learning process for domain adaptation, and then apply the derived bounds to analyze the<lb>asymptotical convergence of the learning process. Without loss of generality, we consider<lb>two kinds of representative domain adaptation: one is with multiple sources and the other<lb>is combining source and target data.<lb>In particular, we use the integral probability metric to measure the difference between<lb>two domains. For either kind of domain adaptation, we develop a related Hoeffding-type<lb>deviation inequality and a symmetrization inequality to achieve the corresponding gener-<lb>alization bound based on the uniform entropy number. We also generalized the classical<lb>McDiarmid\u2019s inequality to a more general setting where independent random variables can<lb>take values from different domains. By using this inequality, we then obtain generaliza-<lb>tion bounds based on the Rademacher complexity. Afterwards, we analyze the asymptotic<lb>convergence and the rate of convergence of the learning process for such kind of domain<lb>adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the<lb>learning process and the numerical experiments support our theoretical findings as well.", "creator": "LaTeX with hyperref package"}}}