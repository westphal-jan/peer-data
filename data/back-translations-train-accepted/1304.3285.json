{"id": "1304.3285", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2013", "title": "Scaling the Indian Buffet Process via Submodular Maximization", "abstract": "Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara &amp; Welling (2008)'s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a one-third approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model.", "histories": [["v1", "Thu, 11 Apr 2013 13:20:51 GMT  (707kb,D)", "https://arxiv.org/abs/1304.3285v1", "currently under review; 13 pages; 8 figures"], ["v2", "Wed, 8 May 2013 20:15:08 GMT  (536kb,D)", "http://arxiv.org/abs/1304.3285v2", "To appear ICML 2013; 13 pages; 8 figures"], ["v3", "Tue, 18 Jun 2013 14:24:58 GMT  (537kb,D)", "http://arxiv.org/abs/1304.3285v3", "To appear ICML 2013; 13 pages; 8 figures"], ["v4", "Wed, 24 Jul 2013 19:20:15 GMT  (536kb,D)", "http://arxiv.org/abs/1304.3285v4", "13 pages, 8 figures"]], "COMMENTS": "currently under review; 13 pages; 8 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["colorado reed", "zoubin ghahramani"], "accepted": true, "id": "1304.3285"}, "pdf": {"name": "1304.3285.pdf", "metadata": {"source": "META", "title": "Scaling the Indian Buffet Process via Submodular Maximization", "authors": ["Zoubin Ghahramani"], "emails": ["cr478@cam.ac.uk", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. The Indian Buffet Process", "text": "Griffiths & Ghahramani (2006) previously derived the IBP by placing independent beta-priors on Bernoulli-generated entries of a N \u00b7 K binary matrix Z, which marginalized K over the beta-priors into infinity. However, in this infinite limit P (Z) is zero for a specific Z. Griffiths & Ghahramani (2006) therefore take the limit of an equivalence class of binary matrices, [Z], defined by the \"left-ordered form\" (lof) of the columns, and show that P ([Z] lof) has a probability that is not zero because K goes into infinity. The lof sequence arranges the columns of Z so that the binary values of the columns do not rise, with the first row being the most significant bit! Ding and al. (2010) investigate various \"shifted\" equivalence classes by not equalling all zero columns to the K while the column is equal!"}, {"heading": "2.2. Maximization-Expectation", "text": "Kurihara & Welling (2008) introduced the ME algorithm: an inference algorithm that exchanges the expectation and maximization variables in the EM algorithm. Consider a general probability model p (X, Z, A) in which X are the observed random variables (RVs), Z are the local latent RVs and A are the global latent RVs. RVs are qualified as \"local\" if there is an RV for each observation, and RVs are \"global\" if their multiplicity is constant or derived from the data. ME can be considered a special case of a Mean-Field Variational Bayes (MFVB), which is an approximation to a posterior that cannot be analytically calculated, and RVs are \"global\" if their multiplicity is constant or derived from data.ME."}, {"heading": "2.3. Submodularity", "text": "Submodularity is a fixed function property that makes a function optimization tractable or approximate feasible. Given base set V and specified function f: 2V \u2192 R, f is submodular if for all A B V and e-V\\ B: f (A) \u2265 f (B- {e}) \u2212 f (B), (4) which expresses a property of \"decreasing yields\" in which the incremental utility of the element e decreases when we include it in larger solution sets. Submodularity is desirable for discrete optimization, since submodular functions are discrete analogies of convex functions and can be minimized globally in polynomial time (Lova'sz, 1983). However, global submodular maximization NP-hard, but submodularity often allows approximation boundaries over greedy algorithms. In the next section, we show that determining a MAP estimation of Z-algorithm is a subalgorithm problem."}, {"heading": "3. Maximization-Expectation IBP", "text": "Here we present the ME algorithm for non-negative linear-Gaussian IBP models and show that an approxi-mate MAP inference occurs as a submodular maximization problem. Bold face variables are matrices with (row, column) drawings; a dot shows all elements of the dimension, and lowercase variables are scalars."}, {"heading": "3.1. Nonnegative Linear-Gaussian IBP Model", "text": "We consider the following probability model: p (X, Z, A | \u03b8) = p (X | Z, A, \u03c32X) p (A | \u03c32A) p (Z | \u03b1) (5) p (X | Z, A, \u03c32A) = N \u00b2 n = 1 N (Xn \u00b7; Zi \u00b7 A, \u03c32AI) (6) p (A | 0, \u03c32A) = K \u00b2 k = 1 D \u00b2 d = 1 TN (akd; 0, \u03c32A) (7) with p ([Z] | \u03b1) specified in Equation 1. This is a non-negative linear-Gaussian IBP model in which the precedence over the latent factors p (A | 0, \u03c32A) is a zero mean i.e. truncated Gauss with non-negative support, denoted TN. As we show below, this non-negative predecessor model results in a submodular maximization problem in the optimization of Z. We use a truncated Gaussian, non-Gaussian, non-Gaussian support."}, {"heading": "3.2. MEIBP Evidence", "text": "In the ME framework, we approach the true posterior distribution via an MFVB assumption: p (Z, A | X, \u03b8) \u2248 q (A) \u03b4 (Z \u2212 Z \u0445). (8) That is, we maintain a varying distribution over the latent factors A and optimize the latent characteristics Z. Taking into account the MFVB constraint, we determine the variational distributions by minimizing the KL divergence between the varying distributions and the true posterior factor A, which is equivalent to maximizing a lower limit in the evidence (Attias, 2000): ln p (X | \u03b8) = E q [ln p (X, A, Z \u2212 \u03b8)] + H [ln p) \u2265 E [ln p (X, A, Z | permanence)] + H [q] = negative (F (9), where H [q] is causal to the entropy of q."}, {"heading": "3.3. Variational Factor Updates", "text": "Maximizing equation 10 with respect to q (A) results in q (A) = K-k = 1 D-d = 1 TN (akd; \u00b5-kd, \u03c3-kd), (13) with the parameters updates\u00b5-kd = \u03c1k N-n = 1 znk (xnd-k-k = k-k znk-E [ak-d]) (14) - 2kd = 2 X, (15) where \"k\" = (mk + 2 X-2A) \u2212 1. These updates absorb O (NK2D), and the relevant moments are: E [akd] = \u00b5-kd + 1 / 2-kd (16) E [a2kd] = \u00b5-kd + 2 kd + 2 kd-kd = 2 / 2-kd (kd)."}, {"heading": "3.4. Evidence Lower Bound as K \u2192\u221e", "text": "Here we show that the lower limit of evidence [Equation 10] is well defined in limit K \u2192 \u221e; in fact, all instances of K are simply replaced by K +. Therefore, a user must specify a maximum model complexity K + similar to the IBP variational methods. However, an advantage over the IBP variational methods is that the q (Z) updates are not affected by inactive features - see \u00a7 4. We take this limit by breaking the evidence into components 1,.., K + and K + + 1,.. and note that if mk = 0: \u00b5 features kd = 0, \u03c3 \u00b2 2 kd = \u03c3 2 A, and H (akd) = 1 2 ln occurrences 2A 2. After some algebra, the evidence becomes: K + 12 K occurrences k = K + + 1 D features kd = 1 [\u2212 ln incidents 2 A + incidents 2 A.18]."}, {"heading": "3.5. Z Objective Function", "text": "(A), we calculate MAP estimates of Z by maximizing probative force [Eq. 10] for each individual function {1,.., N}, while we calculate the constant of n \u00b2 {1,.., N}\\ n. The decomposition of Eq. 10 into terms that depend on Zn \u00b7 n, and those that do not lead to a result (see the supplementary material): F (Zn \u00b7) = \u2212 2X ZTn \u00b7 Zn \u00b7 T n (K + K + n) that do not lead to a result (see the supplementary material): F (Zn = 0} znk)))). (19). (E [akD]), the probative force nk = 1X (K \u00b7 n), is probative force 2X (K + 12 D), d = 1 [akd] 2 \u2212 E [a2kd])."}, {"heading": "3.6. Z Optimization", "text": "(A): A (A): A: A): A (A): A): A (A): A): A): A (A): A (A): A): A (A): A): A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A): A (A): A (A): A (A): A (A): A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A): A (A): A (A): A (A): A (A): A): A (A): A (A): A (A): A (A): A (A): A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A): A (A): A (A (A): A (A): A (A): A (A): A (A): A (A): A): A): A (A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A (A): A): A): A (A (A): A): A (A): A (A): A (A): A): A (A): A (A): A): A (A): A (A"}, {"heading": "4. Related Work", "text": "In the following section, we compare these methods to two synthetic and three real datasets. Doshi-Velez et al. (2009) formulated a coordinate ascent variational inference technique for IBP models (VIBP), using the IBP \"stick breaking\" formulation, which maintains coupled beta registrations on the entries of Z - marginalization of these priors does not allow closed-form variational inference technique for IBP models. Unlike MEIBP inference, maintaining beta privileges has the undesirable consequence that inactive features contribute to the evidence that must be ignored when updating variant distributions. This was not a problem for Doshi-Velez et al. (2009) s finite variational IBP models, which provide variable distributions for linear-Gaussian similarity to Berni updates."}, {"heading": "5. Experiments", "text": "We evaluated the quality and efficiency of this model \"BNMF\"; it has a per capita completion of the data sets (we used the runtime and predictive probability of the observations held as our performance criteria and compared MEIBP inference with the methods listed in Table 1 (the finite and infinite VIBP are differentiated by a \"f\" and \"i\" prefix \"method, however); we used an abbreviated Gaussian before the latent fac-tors for UGibbs and INMF, and Gaussian priors for AIBP and variable methods. In our evaluations we also included Schmidt et al. (2009) s iterated modes algorithm, which compiles an MAP estimate of a parametric non-negative matrix factorization model: X = BA + E, where B and A have exponential priorities and E is zero-Gaussian noise."}, {"heading": "6. Summary and Future Work", "text": "Our key findings were to exploit the submodularity inherent in the evidence boundaries formulated in \u00a7 3, resulting from the quadratic pseudo-Boolean component of the linear Gaussian model. MEIBP inference converged faster than competing IBP methods and achieved comparable solutions for different datasets. There are many discrete non-parametric Bayesian priors, such as the Dirichlet process, and an interesting area for future research will be to generalize our results to formulate the inference with these priors as submodular optimization problems. In addition, we used a simple local search algorithm to obtain a 13 approximation boundary, but at the same time IBbinder et al. (2012) IBP has a simpler stochastic algorithm for the uncontrolled execution of this algorithm."}, {"heading": "Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S.1. Truncated Gaussian Properties", "text": "In the main text we examined an abbreviated Gaussian form: TN (\u00b5-kd, \u03c3-2kd) = 2 erfc (\u2212 \u00b5-kd \u03c3-kd \u221a 2) N (\u00b5-kd, \u03c3-kd) (23), where N represents a Gaussian distribution.The first two moments of TN (\u00b5-kd, \u03c3-kd) are: E [akd] = \u00b5-kd-kd-2 / \u03c0erfcx-kd-2 / \u03c0erfcx-kd-2 (24) E [a2kd] = \u00b5-2kd + \u03c3-2 kd-kd\u00b5-kd-2 / \u03c0erfcx-kd-25), where entropy is kd-kd-2 and erfd-kd-kd-kd-kd-2 (-kd-kd-kd)."}, {"heading": "S.2. Shifted Equivalence Classes", "text": "Here we discuss the \"shifted\" equivalence class of binary matrices first proposed by Ding et al. (2010) For a given N \u00b7 K binary matrix Z, the equivalence class for this binary matrix [Z] is achieved by shifting all zero columns to the right of the non-zero columns while maintaining the non-zero column orders, see Figure 5. Khahramani (2005): P (Z) = K = 1 column of Z and integration over these priors gives the following probability for Z, see Equation 27 in Griffiths & Ghahramani (2005): P (Z) = K = 1 column of Z + 1)."}, {"heading": "S.3. Hyperparameter Inference", "text": "In the main text, we assumed that the hyperparameters \u03b8 = {\u03c3X, \u03c3A, \u03b1} were known (i.e. estimated from the data) and that the placement of conjugated gamma hyperpriors on these parameters enables a simple extension in which we derive their values. Formally, letp (\u03c4X) = Gamma (\u03c4X; aX, bX) (33) p (\u03c4A) = Gamma (\u03c4A; aA, bA) (34) p (\u03b1) = Gamma (\u03b1; a\u03b1, b\u03b1) (35), with Progress representing the precision corresponding to the inverse variance 1\u03c32 for the variance parameter specified in the subscript. Updated equations for the variation distributions result from the standard actualization equations for conclusions of variations in exponential families, cf. Attias (2000) and yield: q (\u03c4X) = Gamma = 38 kamma = A (zamma = 44) (zamma = (A) (zamma = 44)."}, {"heading": "S.4. Evidence as a function of Zn\u00b7", "text": "As shown in the main text, we obtain a submodular objective function for each Zn = > k = = dependence on the components of the function. (., N) by examining the evidence as a function of Zn while keeping constant terms. (.) The evidence is 1\u03c32X n = 1 [-1 2 Zn \u00b7 6 TZTn \u00b7 + Zn \u00b7 n \u00b7 n \u00b7 K + + K + + K + K + K + K + K + K = 1 [ln (N \u2212 mk)! (mk \u2212 1) N = 1! N! + n constant (46). (X T n \u00b7 12 D = 1 [akd] 2 \u2212 E [a2kd]]] (47). (D = D = 1 [\u2212 ln]."}, {"heading": "S.5. Additional MEIBP Characterization", "text": "In this section, we will keep a growing list of additional MEIBP characterization experiments. See http: / / arxiv.org / abs / 1304.3285 for the current version."}, {"heading": "S.5.1. Learning K+", "text": "Clever sampling techniques such as slice sampling and retrospective sampling allow samples to be extracted from these non-parametric priors, c.f. Teh et al. (2007) and Papaspiliopoulos & Roberts (2008). However, variational methods are not directly applicable to Bayesian non-parametric priors, as variational optimization cannot be performed over an unlimited previous space. Instead, variational methods must specify maximum model complexity (Parameter Multiplicity).Several heuristic methods have been proposed to address this limitation (2012) sampled from the variational distribution for the local parameters - which include sampling from the unlimited prior - and used empirical distributions of local samples to update the global parameters."}], "references": [{"title": "A variational bayesian framework for graphical models", "author": ["H. Attias"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Attias,? \\Q2000\\E", "shortCiteRegEx": "Attias", "year": 2000}, {"title": "Clusters and features from combinatorial stochastic processes", "author": ["T. Broderick", "M.I. Jordan", "J. Pitman"], "venue": null, "citeRegEx": "Broderick et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2012}, {"title": "A tight linear time (1/2)-approximation for unconstrained submodular maximization", "author": ["N. Buchbinder", "M. Feldman", "J. Naor", "R. Schwartz"], "venue": "In 53rd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Buchbinder et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Buchbinder et al\\.", "year": 2012}, {"title": "Nonparametric Bayesian matrix factorization by Power-EP", "author": ["N. Ding", "Y.A. Qi", "R. Xiang", "I. Molloy", "N. Li"], "venue": "In 14th Int\u2019l Conf. on AISTATS,", "citeRegEx": "Ding et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2010}, {"title": "Accelerated sampling for the Indian buffet process", "author": ["F. Doshi-Velez", "Z. Ghahramani"], "venue": "In Proceedings of the 26th Annual Int\u2019l Conference on Machine Learning,", "citeRegEx": "Doshi.Velez and Ghahramani,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez and Ghahramani", "year": 2009}, {"title": "Variational inference for the Indian buffet process", "author": ["F. Doshi-Velez", "K.T. Miller", "J. Van Gael", "Y.W. Teh"], "venue": "In 13th Int\u2019l Conf. on AISTATS,", "citeRegEx": "Doshi.Velez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez et al\\.", "year": 2009}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V.S. Mirrokni", "J. Vondrak"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Feige et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feige et al\\.", "year": 2011}, {"title": "Submodular functions and optimization, volume 58", "author": ["S. Fujishige"], "venue": "Elsevier Science Limited,", "citeRegEx": "Fujishige,? \\Q2005\\E", "shortCiteRegEx": "Fujishige", "year": 2005}, {"title": "Propagation algorithms for variational bayesian learning", "author": ["Z. Ghahramani", "M.J. Beal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ghahramani and Beal,? \\Q2001\\E", "shortCiteRegEx": "Ghahramani and Beal", "year": 2001}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T. Griffiths", "Z. Ghahramani"], "venue": "Technical report, Gatsby Unit,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2005}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2006\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2006}, {"title": "Infinite sparse factor analysis and infinite independent components analysis", "author": ["D. Knowles", "Z. Ghahramani"], "venue": "Independent Component Analysis and Signal Separation,", "citeRegEx": "Knowles and Ghahramani,? \\Q2007\\E", "shortCiteRegEx": "Knowles and Ghahramani", "year": 2007}, {"title": "Utilizing object-object and object-scene context when planning to find things", "author": ["T. Kollar", "N. Roy"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "Kollar and Roy,? \\Q2009\\E", "shortCiteRegEx": "Kollar and Roy", "year": 2009}, {"title": "Bayesian k-means as a maximization-expectation algorithm", "author": ["K. Kurihara", "M. Welling"], "venue": "Neural Computation,", "citeRegEx": "Kurihara and Welling,? \\Q2008\\E", "shortCiteRegEx": "Kurihara and Welling", "year": 2008}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.C. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "Lee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2005}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical programming: the state of the art,", "citeRegEx": "Lov\u00e1sz,? \\Q1983\\E", "shortCiteRegEx": "Lov\u00e1sz", "year": 1983}, {"title": "Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models", "author": ["O. Papaspiliopoulos", "G.O. Roberts"], "venue": null, "citeRegEx": "Papaspiliopoulos and Roberts,? \\Q2008\\E", "shortCiteRegEx": "Papaspiliopoulos and Roberts", "year": 2008}, {"title": "A discriminative model for polyphonic piano transcription", "author": ["G.E. Poliner", "D.P.W. Ellis"], "venue": "EURASIP Journal on Advances in Signal Processing,", "citeRegEx": "Poliner and Ellis,? \\Q2006\\E", "shortCiteRegEx": "Poliner and Ellis", "year": 2006}, {"title": "Beam search based map estimates for the Indian buffet process", "author": ["P. Rai", "H. Daume III"], "venue": "In Proceedings of the 28th Annual Int\u2019l Conference on Machine Learning,", "citeRegEx": "Rai and III,? \\Q2011\\E", "shortCiteRegEx": "Rai and III", "year": 2011}, {"title": "Stickbreaking construction for the indian buffet process", "author": ["Y.W. Teh", "D. Gorur", "Z. Ghahramani"], "venue": "In Int\u2019l Conference on AISTATS,", "citeRegEx": "Teh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2007}, {"title": "Truncation-free online variational inference for bayesian nonparametric models", "author": ["C. Wang", "D. Blei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang and Blei,? \\Q2012\\E", "shortCiteRegEx": "Wang and Blei", "year": 2012}, {"title": "Kh is the number of columns of Z with binary value h \u2208", "author": ["Ding"], "venue": null, "citeRegEx": "Ding,? \\Q2010\\E", "shortCiteRegEx": "Ding", "year": 2010}, {"title": "However variational methods are not directly amenable to Bayesian nonparametric priors as the variational optimization cannot be performed over an unbounded prior space", "author": ["nonparametric priors", "c.f. Teh"], "venue": null, "citeRegEx": "priors and Teh,? \\Q2008\\E", "shortCiteRegEx": "priors and Teh", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "More generally, feature models can be viewed as a generalization of unsupervised clustering, see Broderick et al. (2012).", "startOffset": 97, "endOffset": 121}, {"referenceID": 3, "context": "Ding et al. (2010) examine different \u201cshifted\u201d equivalence classes formed by shifting all-zero columns to the right of non-zero columns while maintaining the non-zero column ordering.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "MFVB operates by approximating the posterior distribution of a given probabilistic model by assuming independent variational distributions, p(Z,A|X) \u2248 q(Z)q(A) (Attias, 2000; Ghahramani & Beal, 2001).", "startOffset": 160, "endOffset": 199}, {"referenceID": 15, "context": "Submodularity is desirable in discrete optimization because submodular functions are discrete analogs of convex functions and can be globally minimized in polynomial time (Lov\u00e1sz, 1983).", "startOffset": 171, "endOffset": 185}, {"referenceID": 0, "context": "Given the MFVB constraint, we determine the variational distributions by minimizing the KL-divergence between the variational distributions and the true posterior, which is equivalent to maximizing a lower bound on the evidence (Attias, 2000): ln p(X|\u03b8) = E q [ln p(X,A,Z|\u03b8)] +H[q] +D(q\u2016p) \u2265 E q [ln p(X,A,Z|\u03b8)] +H[q] \u2261 F (9) where H[q] is the entropy of q and D(q\u2016p) represents the KL-divergence between the variational distribution and the true posterior.", "startOffset": 228, "endOffset": 242}, {"referenceID": 7, "context": "We can prove F(Zn\u00b7) is submodular given the following two well-known propositions, see Fujishige (2005): Proposition 1.", "startOffset": 87, "endOffset": 104}, {"referenceID": 6, "context": "Feige et al. (2011) prove that an approximibility guarantee is NP-hard for this class of functions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Feige et al. (2011) prove that an approximibility guarantee is NP-hard for this class of functions. However, Feige et al. (2011) also show that a local-search (ls) algorithm obtains a constant-factor approximation to the optimal solution, provided the submodular objective function is nonnegative.", "startOffset": 0, "endOffset": 129}, {"referenceID": 3, "context": "Ding et al. (2010) used mixed expectation-propagation style updates with MFVB inference in order to perform variational inference for a nonnegative linearGaussian IBP model (INMF).", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Ding et al. (2010) used mixed expectation-propagation style updates with MFVB inference in order to perform variational inference for a nonnegative linearGaussian IBP model (INMF). The expectationpropagation style updates are more complicated than MFVB updates and have per-iteration complexity O(N(KD+KD)). Ding et al. (2010) motivated this framework by stating that the evidence lower bound of a linear-Gaussian likelihood with a truncated Gaussian prior on the latent factors is negative infinity.", "startOffset": 0, "endOffset": 327}, {"referenceID": 5, "context": "Algorithm Iteration Complexity MEIBP O(N(K +D+K 3 + lnK+)) VIBP (Doshi-Velez et al., 2009) O(NK +D) AIBP (Doshi-Velez & Ghahramani, 2009) O(N(K + +K+D)) UGibbs (Doshi-Velez & Ghahramani, 2009) O(NK +D) BS-IBP (Rai & Daume III, 2011) O(N(K+ +D)2 +) INMF (Ding et al.", "startOffset": 64, "endOffset": 90}, {"referenceID": 3, "context": ", 2009) O(NK +D) AIBP (Doshi-Velez & Ghahramani, 2009) O(N(K + +K+D)) UGibbs (Doshi-Velez & Ghahramani, 2009) O(NK +D) BS-IBP (Rai & Daume III, 2011) O(N(K+ +D)2 +) INMF (Ding et al., 2010) O(N(K +D +K+D ))", "startOffset": 170, "endOffset": 189}, {"referenceID": 14, "context": "Dataset Size (N \u00d7D) Details Piano (Poliner & Ellis, 2006) 16000\u00d7 161 DFT of piano recordings Flickr (Kollar & Roy, 2009) 25000\u00d7 1500 binary image-tag indicators Yale-BC (Lee et al., 2005) 2414\u00d7 32256 face images with various lightings", "startOffset": 169, "endOffset": 187}, {"referenceID": 2, "context": "Furthermore, we used a simple local-search algorithm to obtain a 13 -approximation bound, but concurrently with this work, Buchbinder et al. (2012) proposed a simpler stochastic algorithm for unconstrained submodular maximization that obtains an expected 12 approximation bound.", "startOffset": 123, "endOffset": 148}], "year": 2013, "abstractText": "Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Welling (2008)\u2019s maximization-expectation framework to perform approximate MAP inference for linearGaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 13 approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model.", "creator": "LaTeX with hyperref package"}}}