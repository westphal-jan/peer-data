{"id": "1304.5575", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2013", "title": "Inverse Density as an Inverse Problem: the Fredholm Equation Approach", "abstract": "In this paper we address the problem of estimating the ratio $\\frac{q}{p}$ where $p$ is a density function and $q$ is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another. It is often referred as {\\it importance sampling} in statistical inference and is also closely related to the problem of {\\it covariate shift} in transfer learning as well as to various MCMC methods. It may also be useful for separating the underlying geometry of a space, say a manifold, from the density function defined on it.", "histories": [["v1", "Sat, 20 Apr 2013 00:57:35 GMT  (3438kb,D)", "https://arxiv.org/abs/1304.5575v1", null], ["v2", "Thu, 25 Apr 2013 11:46:51 GMT  (3438kb,D)", "http://arxiv.org/abs/1304.5575v2", "Fixing a few typos in last version"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["qichao que", "mikhail belkin"], "accepted": true, "id": "1304.5575"}, "pdf": {"name": "1304.5575.pdf", "metadata": {"source": "CRF", "title": "Inverse Density as an Inverse Problem: the Fredholm Equation Approach", "authors": ["Qichao Que", "Mikhail Belkin"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we address the problem of estimating the ratio q pOur approach is based on reformulating the problem of estimating q pas of an inverse problem in relation to an integral operator corresponding to a nucleus, thus reducing it to an integral equation, the so-called Fredholm problem of the first kind. This formulation, combined with the techniques of regulation and core methods, leads to a principle-based nuclear framework for the construction and theoretical analysis of algorithms. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, easy and easy to implement. We offer detailed theoretical analyses, including concentration limits and convergence rates for the Gaussian nucleus in the case of densities defined on Rd, compact domains in Rd and smooth d-dimensional partial manifolds of euclidean space. We also show experimental results chosen within the framework, including supercoded learning and some superparametric applications."}, {"heading": "1 Introduction", "text": "In fact, it is the case that it is able to be in a position, in a position, in a position, in a position to move, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position, in a position."}, {"heading": "2 Related work", "text": "The problem of density estimation has a long history in classical statistical literature and a rich variety of methods are available [11]. However, as far as we know the problem of estimating the reverse density or density ratio from a sample, the problem of density estimation has not been fully investigated until recently. However, some of the related older work includes the density estimate for inverse problems [7] and the literature on deconvolution, e.g. [4].In recent years, the problem of density estimation is partly due to the increased interest in transfer learning and in particular the form of transfer learning known as covariate shift [24]. To give a brief summary, considering the feature space X and the label space Y, two probability distributions p and q on X \u00d7 Y satisfy the covariate assumption if for all x, p (y | x) = q (y | x)."}, {"heading": "3 Settings and Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Some preliminaries", "text": "We start by introducing some objects and function spaces that are important for our development. As usual, the space of quadratically integrable functions in relation to a measurement variable \u03c1 is defined as follows: L2, \u03c1 = {f: area content (x) | 2d\u03c1 < \u043c}. This is a Hilbert space with the inner product, which is usually defined by < f, g > 2. We will use the notation Kt to explicitly refer to the parameters of the core function kt (x, y) (a kernel) if it is a kuttle family.If the function k (x, y) f (x) d\u03c1 (x).We will use the notation Kt to explicitly refer to the parameters of the core qualities kt (x, y) if it is a core family.If the function k (x, y) is symmetrical and positively defined, then there is a corresponding rendering of the core qualities (x)."}, {"heading": "3.2 The FIRE Algorithms", "text": "As discussed in the introduction, the starting point for our development is the integral equality [type I]: Kp qp (x) = surface techniques q (x, y) q (y) p (y) dy = Kq1 (x). (6) Note that in type I the kernel q (x, y) is not necessary to be in category II. For example, it could be linear kernel. Therefore, we leave the t in the kernel for type I (x, y) is a Gaussian kernel kt (x, y) which we will analyze in detail, or another cipher family and for f sufficiently smooth Kt, qf (x) p (x) p (x) p (x) p (1) and hence [type II) is a Gaussian kernel kt (x, y) q (x)."}, {"heading": "3.3 Algorithms for the Type I setting.", "text": "Given an iid sample from p, zp = {x1, x2,. =, xn} and an iid sample from q, zq = {x \u2032 1, x \u2032 2,.., x \u2212 m} (we will denote the combined sample by z) we can approach the integral operators Kp and Kq after Kzpf (x) = 1n \u2211 xi zp k (xi, x) f (xi) and Kzqf (x) after Kzq k (x) f (x \u2032 i) f (x \u2032 i). (10) The empirical version of Eq xi Kp. 8 becomesf Ixi, z = arg min f. H1n."}, {"heading": "3.3.1 Algorithms for \u03b3L2,p + (1\u2212 \u03b3)L2,q norm.", "text": "Depending on the setting, we can minimize the error in estimating the probability distribution p, q or a linear combination of these factors. A significant potential advantage of using a linear combination is that both samples can be used in the loss function at the same time. First, we specify the continuous version of the problem: f * \u03bb = arg min f \u00b2 H \u00b2 Kpf \u2212 Kq1 \u00b2 22, p + (1 \u2212 3) Kpf \u2212 Kq1 \u00b2 22, q + \u00b2 f \u00b2 2H (13) Taking into account a sample of p, zp = {x1, x2,., xn} and a sample of q, zq \u2212 1, x \u00b2 2,., x \u00b2 m}, we obtain an empirical version of the Eq \u00b2 2H (13: f \u00b2), z (x) = arg min f \u00b2 H \u00b2 n \u00b2 xi \u2212 Kxi \u2212 kxi \u2212 kxi \u00b2 2,."}, {"heading": "3.3.2 Algorithms for the RKHS norm.", "text": "In addition to using the RKHS standard for the regularization standard, we can also use it as a loss function: f * \u03bb = arg min f \u0394H-Kpf \u2212 Kq1-2H-2H-2H-2H (14) Here, the Hilbert room H \u00b2 must correspond to the kernel K and may potentially differ from the room H used for regularization. Note that this formulation is only applicable in the Type I setting, since the function q must belong to the RKHS H. For two samples zp, zq, it is easy to note the empirical version of this problem, which results in the following formula: f *, z (x) = \u2211 xi-zp vikH (xi, x) v = (Kp, pKH + n\u03bbI) \u2212 1 Kp, q1zq. (15) The result is somewhat similar to our Type I formulation with the L2, p standard."}, {"heading": "3.4 Algorithms for the Type II and 1.5 settings.", "text": "In the Type II setting, we assume that we have a sample z = {x1, x2,.., xn}, which is drawn from p, and that we know the function values q (xi) at the points of the sample. If we replace the standard and the integral operator with their empirical versions, we get the following optimization problem: f II\u03bb, z = arg min f, H1n, xi-z (Kt, zf (xi) \u2212 q (xi) 2 + \u03bb, f, 2H (16) Let us remember that Kt, z is the empirical version of Kt, p, defined by Kt, zf (x) = 1n, xi-z (xi, x) f (xi). As before, we get an analytical formula for the solution: f II\u03bb, z (x) = xi-z (xi, x) vi, where v = (K2KnH + cipI) f (xi) f (xi), j I = (xi) (xi), \u2212 Kj = (xi)."}, {"heading": "3.4.1 Type 1.5: The setting and the algorithm.", "text": "This case (see Eq.4) lies between type I and type II. The setting is the same as for type I, because we get two samples zp from p and zq from q. However, similar to type II, we use the fact that if Kp qp \u2248 K \u2032 q1 and K \u2032 q differ in their function (e.g. two Gaussian nuclei of different bandwidth), the algorithm is similar to type I. The difference is that the kernel matrix K \u2032 q, q is calculated using the kernel k \u2032 (x, y): (K \u2032 q, q) ij = 1 mk \u2032 (xi, x \u2032 j).f1.5\u03bb, z (x) = xi-zp kH (xi, x) vi and v = (K2p, pKH + ncI) \u2212 1 Kp, pK \u2032 q, q1zq."}, {"heading": "3.5 Spectral Cutoff Regularization", "text": "In this section we briefly discuss an alternative form of regularization based on the threshold of the spectrum of the kernel matrix. It also leads to simple algorithms comparable to those of the Tikhonov regularization and which may have certain computational advantages. Since Kp is a compact self-adjugated operator on L2, p, its eigenfunctions {u0, u1,.} form a complete orthogonal basis for L2, p. \u2212 An alternative method of regularization is the so-called spectral cutoff, where the problem is limited to the subspace, which is covered by the few eigenfunctions of Kp. \u2212 The regularization problems becomef I, spektrum Kpf \u2212 Kq1, spektralg II, spektralg = arg min f Qt, k \u00b2 Qt, pf \u2212 q \u00b2 the eigenfunctions of Kp are covered."}, {"heading": "3.6 Comparison of type I and type II settings.", "text": "While at first glance type II, setting may seem more restrictive than type I, there are a number of important differences in its applicability. 1. In type II, setting q does not have to be a density function (i.e., it does not integrate negatively and into one). 2. Equation 11 of the type I setting cannot be solved so easily without example zq of q, since estimating Kq requires either scanning q (if it is a density) or estimating the integral in another way, which may be difficult in high dimensions, but perhaps of interest in certain low-dimensional applications.3. There are a number of problems (e.g. many problems with MCMC) where q (x) is explicitly known (possibly up to a multiplicative constant), while scanning q is expensive or even impossible computational [17].4. In contrast to Equation 8, equation 9 indicates an error concept that depends on the kernel, which is essentially the difference between the available kernel and the type II."}, {"heading": "4 Theoretical analysis: bounds and convergence rates for Gaus-", "text": "In this section we present our most important results on limits and convergence rates of our algorithm based on Tikhonov regularization with a Gaussian kernel. We will consider both type I settings and type II settings for Euclidean and manifold cases and make a comment on Euclidean domains with borders. To simplify the theoretical development, the integral operator and the RKHS H will correspond to the same Gaussian kernel kt (x, y)."}, {"heading": "4.1 Assumptions", "text": "Before coming to the main results, we will explain the assumptions about the density functions p and q and the basic setting for our theorems: 1. The set in which the density function p is defined could be one of the following: (1) the whole Rd; (2) a compact, smooth belt-like subset M of the d dimension in Rn. In both cases, we need 0 < p (x) < p (x) for each x dimension. The function q should fulfill q-L2, p and must be limited from above. We will also make some remarks about a compact domain in Rd with limit. 2. We also need q (x) p (x) p (x) p (x) p (x) p-W 2 (B) and q-W 22 (B), where W 22 (B) is the Sobolev space of the functions on the map (e.g. [30]). Certain properties of W 22 (K) will be discussed later in the proof that it is important for us that H is the value."}, {"heading": "4.2 Main Theorems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Type I setting", "text": "We will provide theoretical results for our setting, where both the operator and the regulatory kernel will be two density functions based on p and q that fulfill the assumption in sec. \u2212 p \u2212 p \u2212 n with the same bandwidth parameter. \u2212 p \u2212 p = {x1, x2,.., xn}, i.i.d. from p and m points, zq = {x \u2032 1, x \u00b2 2,.., x \u00b2 p \u2212 p \u00b2, i.i.d. sampled from q, xn \u00b2, for solving the optimization problem in (11), with confidence at least 1 \u2212 2e \u2212 p \u00b2, we have (1) if domain is 2. \u2212 p \u2212 p \u00b2, i.i.d. sampled from q, and for solving the optimization problem in (11), with confidence at least 1 \u2212 2e \u2212 p \u00b2 if domain is 2."}, {"heading": "4.2.2 Type II setting", "text": "In this section we provide an analysis for the type II setting and also make a comment on the error analysis for the compact ranges in R. It should be remembered that in the type II environment we have a number of points that come from p \u2212 \u2212 and assume that the values of q are known at these points. Note: q does not have to be a density function. Theorem 3. Let p be a density function on p \u2212 and q be a function that meets the assumptions in Sec \u2212 4.1. Given n points z = {x1, x2,.., xn} scanned i.i.d. from p, and for sufficiently small t, for solving the optimization problem in (16), with confidence at least 1 \u2212 2e \u2212 5, we have (1) If the domain is qm = Rd, we have qm-R.f IIII.d., z \u2212 qp 2, p \u00b2, p \u00b2, p \u00b2, p \u00b2."}, {"heading": "5 Proofs of Theorems", "text": "In this section we provide a proof of our main law 1 for setting I. The proof of set 3 for setting type II goes in a similar direction and can be found in the appendix."}, {"heading": "5.1 Basics about RKHS", "text": "Since Kt, \u03c1 is a self-adjoining operator, its eigenfunctions {u0, t, u1, t,...} form a complete orthogonal basis for L2, \u03c1. Denote the eigenvalues of Kt, \u03c1 by {\u03c30, t, \u03c30, t,...}. The norm of Kt, \u03c1 Kt, \u0438 L2, \u03c1 \u2192 L2, \u03c1 \u2264 maxi \u03c3i, t < c for a constant c. We know that Ht is isometric to L2 for a constant c and this is the definition we use for the norm of K1 / 2t."}, {"heading": "5.2 Proof of Theorem 1", "text": "Proof. Remember the definition of f I\u03bb and f I \u03bb, z in Eq.8 and Eq.11. Due to the triangular inequality, we have that the approximation error is a measure of the distance between f I\u03bb \u2212 f I\u03bb, z \u00b2 2, p = (Approximation Error) + (Sampling Error) (27) The approximation error is a measure of the distance between f I\u0445 \u2212 qp \u00b2 2, p and the optimal approximation given by the algorithm (8). The sample error concept is, depending on the type of estimates, composed of two parts: the approximate error, z \u00b2, p the difference between f I\u03bb and f I \u00b2, z \u00b2, z, z, depending on the data points."}, {"heading": "5.2.1 Bound for Approximation Error", "text": "Let us first introduce two lemmas that are useful for limiting the approximation problems.Lemma 5. Let us use the function f + > W 22 (Rd) and p + p (x) > 0 for each x-dimensional formula. (28) for constants D1, D2.Proof. See Appendix A.Lemma 6. Let us know that the function f + W 22 (M) is defined on a compact K1-dimensional formula. (28) for constants D1, D2.Proof. See Appendix A.Lemma 6. Let us know that the function f + W 22 (M) is defined on a compact K1-dimensional formula of the d-dimensional formula."}, {"heading": "5.2.2 Bound for Sampling Error", "text": "In the next sequence we will give a concentration of the sampling error, namely in the order of magnitude in which we have the assumptions in the order of (8) and (11), certainly in the order of (8) and (11), certainly in the order of (8) and (11), certainly in the order of (1) and (2), which we have in the order of (2) and (2) Kp (2) Kp (2) Kp (2). (3) Kp (2) and (2) Kp (2) Kp (2), in the order of (2) Kp (2) and (2) Kp (2), in the order of (2) Kp (2) and (3) Kp (2), in the order of (2) Kp (2) and (2) Kp (2)."}, {"heading": "6 Experiments", "text": "In this section, we will examine the empirical performance of our methods under different settings. We will focus primarily on our Type II setting and use the same Gaussian core for the integral operator and regularization term to simplify model selection. In Section 6.1, we will describe a completely unattended method for parameter selection that is used throughout the entire test section. In Section 6.2, we will briefly describe the data sets and the re-sampling procedures we use. In Section 6.3, we will offer a comparison between our methods that use different standards and other methods based on the expected performance under our evaluation criteria. In Section 6.4, we will provide a number of experiments that compare our method with different methods on several different data sets for classification and regression tasks. Finally, in Section 6.5, we will examine the performance of different cores in both Type I and Type II settings using two simulated data sets."}, {"heading": "6.1 Experimental Setting and Model Selection", "text": "For a given equality, we have the following function: We have a set of functions Xp = {xpi, i = 1, 2,. (1, 2,.) In our experiments, we have a set of data sets Xp = {xpi functions = 1, 2,.) The goal is to estimate q p on the assumption that Xp is sampled from p and Xq and another set of instances Xp is used, while our algorithms typically have two parameters that need to be selected, the kernel width t and the regulation parameters \u03bb. In the general selection of parameters in an unattended or semi-monitored setting, it is a hard problem as it can be difficult to validate the resulting classification / estimator. Certain properties of our setting allow us to construct an adequate unattended performance of the algorithms. Now, we construct a performance measure for the quality of the estimator."}, {"heading": "6.2 Data sets and Resampling", "text": "In our experiments, several data sets are considered: Bank8FM, CPUsmall, and Kin8nq for regression; and USPS and 20 message groups for classification. For each data set, we assume that they are i.e. sampled from a distribution denoted by p. We draw the first 500 or 1000 points from the original data set as Xp. To get Xq, we apply a sampling scheme to the remaining points of the original data set. Two types of sampling are used, using the characteristics of the data and the label information (along the lines suggested in [8]. Specifically, a set of data points with labels {x1, y1), (x2, y2), will be used."}, {"heading": "6.3 Testing the FIRE algorithm", "text": "In the first experiment, we test our method of selecting the parameters described in Section 6.1 by focusing on the error J (f; Xp, Xq, U) in Eq. 36 for different function classes U. We use different function families to fine-tune the parameters and validate. This measurement is important because in practice, the functions of interest to us may not be included in the collection selected for validation. To avoid confusion, we designate the function for cross-validation by f cv and the function for error measurement by f err.We use the handwritten number records CPUsmall and USPS. For each of them, we create two data sets Xp and Xq using the resampling method PCA (a, \u03c3v), described in Section 6.2. We compare FIRE with several methods, including TIKDE, LSIF. Figure 1 gives an illustration of the procedure and use of the data for the experiments."}, {"heading": "6.4 Supervised Learning: Regression and Classification", "text": "In our experiments, we compare our FIRE method with several methods within the framework of supervised learning, i.e. regression and classification. Specifically, we consider the situation as part or the entire training set Xp labeled and all Xq unlabeled. In the following experiments, we will estimate the density ratio function based on 1000 points in Xp and use the labeled data from Xp to build a regression function or classification on Xq."}, {"heading": "6.4.1 Regression", "text": "In view of datasets (Xp, Y p) where Xp stands for independent variable and Y p for dependent variable, and a test dataset Xq with a different distribution, the regression problem is to obtain a function that represents a predictor for Xq. To compare unweighted regression method with different weighting schemes, we use the simplest regression method, the least square linear regression. In this method, the regression function is of the form (x, \u03b2) = \u03b2tx, where \u03b2 = (XWXT) + XWY and (\u00b7) + denotes the pseudo-inverse of a matrix. Here, W is a diagonal matrix with the estimated density ratio on the diagonal, which is estimated using FIRE and other methods for estimating the density ratio. The results of 3 regression datasets are shown in Tables 5, 3 and 4."}, {"heading": "6.4.2 Classification", "text": "Similar to the case of regression, the density ratio can also be used to build a classifier such as SVM. Given a series of labeled data, {(x1, y1), (x2, y2),.., (xn, yn) and xi-q, we build a linear classifier f by means of the weighted linear SVM algorithm as follows: f = arg min \u03b2-RdCn n n \u2211 i = 1 wi (\u03b2 Txi \u2212 yi) + + + 0\u03b2 22The weights wi's are determined by various density quotation algorithms using two sets of data X p and Xq. Note that the estimate of density ratios using Xp and Xq is completely independent of the label information. We also examine the performance of these weighted SVM as the number of labeled points used for training classification changes. In the experiments, we first estimate the density ratios on the entire Xp and Xq messages with the parameters we then use in the XS to validate the results of the X."}, {"heading": "6.5 Simulated Examples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.5.1 Simulated Dataset 1.", "text": "We use a simple example where the two densities are known to demonstrate the properties of our methods and to show how the number of data points affects performance. For this experiment, we assume p = 0.5N (\u2212 2, 12) + 0.5N (2, 0.52) and q = N (0, 0.52) and fix | Xq | = 2000 and vary | Xp | from 50 to 1000. We compare our method with the other two methods for the same problem: TIKDE and KMM. For all the methods we look at in this experiment, we select the optimal parameter based on the empirical L2 standard of the difference between the estimated ratio and the true ratio to be known in this toy example. Figure 2 gives the reader an intuition about how the estimated ratios behave for different methods, and Figure 3 shows how different methods work when | Xp | varies from 50 to 1000 and | Xq | is fixed to 2000."}, {"heading": "6.5.2 Simulated Dataset 2.", "text": "In the second simulated example, we will test our method for different cores and different standards as a cost function. Specifically, we will assume that p = N (0, 0.52) and q = Unif ([\u2212 1, 1]). We will use this example to explore the power of our methods with different cores. In this experiment, three settings are taken into account: (1) Different cores kh for the RKHS. We will use polynomial cores of degrees 1, 5 and 20, exponential kernels and Gausskernels; (2) Type I setting and Type II setting; (3) Different standard for the cost function in the algorithm, i.e., 2, p and 2, q. In this example, p focuses on the region near 0, but still has a penalty outside the interval [\u2212 1, 1]."}, {"heading": "Acknowledgements", "text": "We thank Yusu Wang for many valuable discussions and suggestions. We also thank Lorenzo Rosasco for very useful discussions and Christoph Lampert for pointing out important related papers. We thank the Austrian Institute of Technology (ISTA) and Herbert Edelsbrunner's research group for their hospitality in writing the paper. The work was partially supported by NSF grants IIS 0643916 and IIS 1117707."}, {"heading": "A Proof for Lemma 5", "text": "The proof that RKHS is unique for a specific domain and kernel is thus independent of the measure used to define L2. (So) for each g-L2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2, p-2."}, {"heading": "B Proof for Lemma 6", "text": "However, the definition of Fourier transformation is obscure, so we must consider alternative ways to achieve the same limit. < < p > p > p > p < p > p < p > p < p > p < p < p > p < p > p < p > p < p > p < p > p > p > p < p > p > p > p > p > p > p > p < p < p < p > p > p > p > p > p > p > p < p > p > p > p < p > p > p & p > p > p > p > p > p > p > p > p < p > p > p > p > p > p < p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p < p > p > p > p > p > p < p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p > p < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "C Proof for Theorems in 4.2.2", "text": "In the second case, since we have no samples of q, we replace Kt, q, q, q, q, q, q, q, q, x, x, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}], "references": [{"title": "On regularization algorithms in learning theory", "author": ["Frank Bauer", "Sergei Pereverzev", "Lorenzo Rosasco"], "venue": "Journal of complexity,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Discriminative learning for differing training and test distributions", "author": ["Steffen Bickel", "Michael Br\u00fcckner", "Tobias Scheffer"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Optimal rates of convergence for deconvolving a density", "author": ["Raymond J Carroll", "Peter Hall"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Learning from examples as an inverse problem", "author": ["Ernesto De Vito", "Lorenzo Rosasco", "Andrea Caponnetto", "Umberto De Giovannini", "Francesca Odone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Spectral regularization for support estimation", "author": ["Ernesto De Vito", "Lorenzo Rosasco", "Alessandro Toigo"], "venue": "Advances in Neural Information Processing Systems, NIPS Foundation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Maximum smoothed likelihood density estimation for inverse problems", "author": ["P. Eggermont", "V. LaRicca"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Covariate shift by kernel mean matching", "author": ["Arthur Gretton", "Alex Smola", "Jiayuan Huang", "Marcel Schmittfull", "Karsten Borgwardt", "Bernhard Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Conditional mean embeddings as regressors", "author": ["S Gr\u00fcnew\u00e4lder", "G Lever", "L Baldassarre", "S Patterson", "A Gretton", "M Pontil"], "venue": "In Proceedings of the 29th International Conference on Machine Learning, ICML 2012,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Jiayuan Huang", "Alexander J. Smola", "Arthur Gretton", "Karsten M. Borgwardt", "Bernhard Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Review papers: Recent developments in nonparametric density estimation", "author": ["Alan Julian Izenman"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "A least-squares approach to direct importance estimation", "author": ["Takafumi Kanamori", "Shohei Hido", "Masashi Sugiyama"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Robust kernel density estimation", "author": ["Joo Seuk Kim", "Clayton Scott"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Linear integral equations, volume 82", "author": ["Rainer Kress"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Metropolized independent sampling with comparisons to rejection sampling and importance sampling", "author": ["Jun Liu"], "venue": "Statistics and Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Annealed importance sampling", "author": ["Radford M Neal"], "venue": "Statistics and Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization", "author": ["XuanLong Nguyen", "Martin J Wainwright", "Michael I Jordan"], "venue": "Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1962}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Kernel methods for pattern analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": "Cambridge university press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Data spectroscopy: Eigenspaces of convolution operators and clustering", "author": ["Tao Shi", "Mikhail Belkin", "Bin Yu"], "venue": "The Annals of Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Improving predictive inference under covariate shift by weighting the loglikelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "On a kernel-based method for pattern recognition, regression, approximation, and operator inversion", "author": ["Alex J Smola", "Bernhard Sch\u00f6lkopf"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Support vector machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Analysis of the laplacian on the complete riemannian manifold", "author": ["R.S. Strichartz"], "venue": "Journal of Functional Analysis,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1983}, {"title": "Covariate shift adaptation by importance weighted cross validation", "author": ["Masashi Sugiyama", "Matthias Krauledat", "Klaus-Robert M\u00fcller"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["Masashi Sugiyama", "Shinichi Nakajima", "Hisashi Kashima", "Paul Von Buenau", "Motoaki Kawanabe"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Partial Differential Equation", "author": ["M.E. Taylor"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1997}, {"title": "Support vector method for multivariate density estimation", "author": ["Vladimir Vapnik", "Sayan Mukherjee"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Practical approximate solutions to linear operator equations when the data are noisy", "author": ["Grace Wahba"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1977}, {"title": "The effect of the input density distribution on kernel-based classifiers", "author": ["Christopher Williams", "Matthias Seeger"], "venue": "In Proceedings of the 17th International Conference on Machine Learning. Citeseer,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2000}, {"title": "On early stopping in gradient descent learning", "author": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto"], "venue": "Constructive Approximation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Analysis of kernel mean matching under covariate shift", "author": ["Yaoliang Yu", "Csaba Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": ", see the review [11]), particularly, dealing with a class of non-parametric kernel estimators going back to the work of Parzen [20].", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": ", see the review [11]), particularly, dealing with a class of non-parametric kernel estimators going back to the work of Parzen [20].", "startOffset": 128, "endOffset": 132}, {"referenceID": 14, "context": ", [16]) deals with this problem by using a ratio of two densities (which is typically assumed to be known in that literature).", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 11, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 9, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 26, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 2, "context": ", [8, 13, 10, 28, 3].", "startOffset": 2, "endOffset": 20}, {"referenceID": 22, "context": "Many of these papers consider this problem in the context of covariate shift assumption [24] or the so-called selection bias [36].", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "We will provide a more detailed discussion of these and other related papers and connections to our work in Section 2, where we also discuss how the Kernel Mean Matching algorithm [8, 10] can be viewed within our framework.", "startOffset": 180, "endOffset": 187}, {"referenceID": 9, "context": "We will provide a more detailed discussion of these and other related papers and connections to our work in Section 2, where we also discuss how the Kernel Mean Matching algorithm [8, 10] can be viewed within our framework.", "startOffset": 180, "endOffset": 187}, {"referenceID": 9, "context": "This formulation (see Section 3) also yields an explicit formula and is related to the Kernel Mean Matching algorithm [10] (see the discussion in Section 2), although with a different optimization procedure.", "startOffset": 118, "endOffset": 122}, {"referenceID": 32, "context": ", [34, 1]) can be used and may have computational advantages.", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": ", [34, 1]) can be used and may have computational advantages.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": "Finally, in Section 6 we discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [10] and LSIF [13] as well as the base-line thresholded inverse kernel density estimator (TIKDE) and importance sampling (when available).", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Finally, in Section 6 we discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [10] and LSIF [13] as well as the base-line thresholded inverse kernel density estimator (TIKDE) and importance sampling (when available).", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "The problem of density estimation has a long history in classical statistical literature and a rich variety of methods are available [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Some of the related older work includes density estimation for inverse problems [7] and the literature on deconvolution, e.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": ", [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": "In the last few years the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [19] and, in particular to the form of transfer learning known as covariate shift [24].", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "In the last few years the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [19] and, in particular to the form of transfer learning known as covariate shift [24].", "startOffset": 238, "endOffset": 242}, {"referenceID": 2, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 7, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 11, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 26, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 9, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 27, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 16, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 106, "endOffset": 140}, {"referenceID": 9, "context": "Some of the work on covariate shift, ratio density estimation and other closely related settings includes [36, 3, 8, 13, 28, 10, 29, 12, 18] The algorithm most closely related to our approach is Kernel Mean Matching (KMM) [10].", "startOffset": 222, "endOffset": 226}, {"referenceID": 7, "context": "Also, since there is no regularizing term, the problem is less stable (see Section 6 for some experimental comparisons) and the theoretical analysis is harder (however, see [8] and the recent paper [35] for some nice theoretical analysis of KMM in certain settings).", "startOffset": 173, "endOffset": 176}, {"referenceID": 33, "context": "Also, since there is no regularizing term, the problem is less stable (see Section 6 for some experimental comparisons) and the theoretical analysis is harder (however, see [8] and the recent paper [35] for some nice theoretical analysis of KMM in certain settings).", "startOffset": 198, "endOffset": 202}, {"referenceID": 11, "context": "Another related recent algorithm is Least Squares Importance Sampling (LSIF) [13], which attempts to estimate the density ratio by choosing a parametric linear family of functions and choosing a function from this family to minimize the L2,p distance to the density ratio.", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "A similar setting with the Kullback-Leibler distance (KLIEP) was proposed in [29].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": ", [26, 22, 21]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 20, "context": ", [26, 22, 21]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 19, "context": ", [26, 22, 21]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 4, "context": ", [5, 25] in the Tikhonov regularization or other regularization frameworks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 23, "context": ", [5, 25] in the Tikhonov regularization or other regularization frameworks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 29, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 63, "endOffset": 67}, {"referenceID": 5, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 12, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 207, "endOffset": 211}, {"referenceID": 8, "context": "we note interesting methods for density estimation proposed in [31] and estimating the support of density through spectral regularization in [6], as well as robust density estimation using RKHS formulations [14] and conditional density [9].", "startOffset": 236, "endOffset": 239}, {"referenceID": 31, "context": "We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [33, 23].", "startOffset": 136, "endOffset": 144}, {"referenceID": 21, "context": "We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [33, 23].", "startOffset": 136, "endOffset": 144}, {"referenceID": 1, "context": ", [2].", "startOffset": 2, "endOffset": 5}, {"referenceID": 13, "context": "Finally, the setting in this paper is connected to the large literature on integral equations [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 30, "context": "In particular, we note [32], which analyzes the classical Fredholm problem using regularization for noisy data.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "The direct consequence of this is the Representer Theorem, which allows us to write solutions to various optimization problems over H in terms of linear combinations of kernels supported on sample points (see [26] for an in-depth discussion or the RKHS theory and the issues related to learning).", "startOffset": 209, "endOffset": 213}, {"referenceID": 9, "context": "We note the connection between this formulation of using the RKHS norm as a loss function and the KMM algorithm [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Interestingly a somewhat similar formula arises in [13] as unconstrained LSIF, with a different functional basis (kernels centered at the points of the sample zq) and in a setting not directly related to RKHS inference.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": ", many problems involving MCMC) where q(x) is known explicitly (possibly up to a multiplicative constant), while sampling from q is expensive or even impossible computationally [17].", "startOffset": 177, "endOffset": 181}, {"referenceID": 28, "context": ", [30]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "We note that this way of measuring the error is related to the LSIF [13] and KLIEP [29], algorithms.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "We note that this way of measuring the error is related to the LSIF [13] and KLIEP [29], algorithms.", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "Two ways of resampling, using the features of the data and using the label information, are used (along the lines similar to those proposed in [8]).", "startOffset": 143, "endOffset": 146}], "year": 2013, "abstractText": "In this paper we address the problem of estimating the ratio q p where p is a density function and q is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another. It is often referred as importance sampling in statistical inference and is also closely related to the problem of covariate shift in transfer learning as well as to various MCMC methods. It may also be useful for separating the underlying geometry of a space, say a manifold, from the density function defined on it. Our approach is based on reformulating the problem of estimating q p as an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel in the case of densities defined on R, compact domains in R and smooth d-dimensional sub-manifolds of the Euclidean space. We also show experimental results including applications to classification and semi-supervised learning within the covariate shift framework and demonstrate some encouraging experimental comparisons. We also show how the parameters of our algorithms can be chosen in a completely unsupervised manner.", "creator": "LaTeX with hyperref package"}}}