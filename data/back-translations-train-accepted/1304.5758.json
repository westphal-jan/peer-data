{"id": "1304.5758", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2013", "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling", "abstract": "We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We show that for any prior distribution, the Thompson Sampling strategy achieves a Bayesian regret bounded from above by $14 \\sqrt{n K}$. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by $1/20 \\sqrt{n K}$.", "histories": [["v1", "Sun, 21 Apr 2013 15:58:56 GMT  (6kb)", "https://arxiv.org/abs/1304.5758v1", null], ["v2", "Thu, 3 Oct 2013 00:48:53 GMT  (26kb)", "http://arxiv.org/abs/1304.5758v2", "A previous version appeared under the title 'A note on the Bayesian regret of Thompson Sampling with an arbitrary prior'"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["s\u00e9bastien bubeck", "che-yu liu"], "accepted": true, "id": "1304.5758"}, "pdf": {"name": "1304.5758.pdf", "metadata": {"source": "CRF", "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling", "authors": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "emails": ["sbubeck@princeton.edu,", "cheliu@princeton.edu"], "sections": [{"heading": null, "text": "This result is not improvable in the sense that there is a prior distribution, so that each algorithm has a Bayesian regret limited from below by 120 \u221a nK. We also examine the case of Priors for hiring Bubeck et al. [2013] (where the optimal mean is as well known as a lower limit on the smallest gap) and we show that in this case the regret of Thompson Sampling is indeed uniformly limited over time, showing that Thompson Sampling can greatly exploit the beautiful properties of these Priors. [2013] IntroductionIn this paper, we are interested in the Bayesian Multi-Weapon Bandit problem that can be described as a consequence. Let it be a known distribution over a few sentences, and let it be a random variable that is distributed accordingly."}], "references": [{"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Further optimal regret bounds for thompson sampling, 2012b. arXiv:1209.3353", "author": ["S. Agrawal", "N. Goyal"], "venue": null, "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Audibert and Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2010}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning Journal,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Bounded regret in stochastic multi-armed bandits", "author": ["S. Bubeck", "V. Perchet", "P. Rigollet"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J.C. Gittins"], "venue": "Journal Royal Statistical Society Series B,", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "Thompson sampling: an asymptotically optimal finitetime analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematics Society,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": null, "citeRegEx": "Russo and Roy.,? \\Q2013\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": "Bulletin of the American Mathematics Society,", "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}], "referenceMentions": [{"referenceID": 2, "context": "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK.", "startOffset": 30, "endOffset": 57}, {"referenceID": 2, "context": "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK.", "startOffset": 30, "endOffset": 82}, {"referenceID": 2, "context": "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by 1 20 \u221a nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.", "startOffset": 30, "endOffset": 486}, {"referenceID": 5, "context": "On the other hand the point of view initially developed in Robbins [1952] leads to a learning problem.", "startOffset": 59, "endOffset": 74}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting.", "startOffset": 91, "endOffset": 122}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute).", "startOffset": 91, "endOffset": 271}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s\u22651 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent. In general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent\u2019s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let \u03c0t be the posterior distribution on \u03b8 given the history Ht = (I1, X1,1, .", "startOffset": 91, "endOffset": 1487}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s\u22651 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent. In general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent\u2019s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let \u03c0t be the posterior distribution on \u03b8 given the history Ht = (I1, X1,1, . . . , It\u22121, XIt\u22121,TIt\u22121 (t\u22121)) of the algorithm up to the beginning of round t. Then Thompson Sampling first draws a parameter \u03b8 from \u03c0t (independently from the past given \u03c0t) and it pulls It \u2208 argmaxi\u2208[K] \u03bci(\u03b8). Recently there has been a surge of interest in this simple policy, mainly because of its flexibility to incorporate prior knowledge on the arms, see for example Chapelle and Li [2011]. For a long time the theoretical properties of Thompson Sampling remained elusive.", "startOffset": 91, "endOffset": 2011}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al.", "startOffset": 103, "endOffset": 129}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b].", "startOffset": 103, "endOffset": 153}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior.", "startOffset": 103, "endOffset": 180}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior. The first result in this spirit was obtained very recently by Russo and Roy [2013] who showed that for any prior distribution \u03c00 Thompson Sampling always satisfies BRn \u2264 5 \u221a nK logn.", "startOffset": 103, "endOffset": 490}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1.", "startOffset": 20, "endOffset": 46}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees.", "startOffset": 20, "endOffset": 280}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where \u03bc\u2217 and \u03b5 > 0 are known values such that for any \u03b8 \u2208 \u0398, first there is a unique best arm {i\u2217(\u03b8)} = argmaxi\u2208[K] \u03bci(\u03b8), and furthermore \u03bci\u2217(\u03b8)(\u03b8) = \u03bc \u2217, and \u2206i(\u03b8) := \u03bci\u2217(\u03b8)(\u03b8)\u2212 \u03bci(\u03b8) \u2265 \u03b5, \u2200i 6= i\u2217(\u03b8).", "startOffset": 20, "endOffset": 526}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where \u03bc\u2217 and \u03b5 > 0 are known values such that for any \u03b8 \u2208 \u0398, first there is a unique best arm {i\u2217(\u03b8)} = argmaxi\u2208[K] \u03bci(\u03b8), and furthermore \u03bci\u2217(\u03b8)(\u03b8) = \u03bc \u2217, and \u2206i(\u03b8) := \u03bci\u2217(\u03b8)(\u03b8)\u2212 \u03bci(\u03b8) \u2265 \u03b5, \u2200i 6= i\u2217(\u03b8). In other words the value of the best arm is known as well as a non-trivial lower bound on the gap between the values of the best and second best arms. For this problem a new algorithm was proposed in Bubeck et al. [2013] (which we call the BPR policy), and it was shown that the BPR policy satisfies", "startOffset": 20, "endOffset": 983}, {"referenceID": 4, "context": "Thus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve.", "startOffset": 142, "endOffset": 161}, {"referenceID": 4, "context": "Thus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve. It is natural to view the assumptions of the BPR setting as a prior over the reward distributions and to ask what regret guarantees attains Thompson Sampling in that situation. More precisely we consider Thompson Sampling with Gaussian reward distributions and uniform prior over the possible range of parameters. We then prove individual regret bounds for any sub-Gaussian distributions (similarly to Bubeck et al. [2013]).", "startOffset": 142, "endOffset": 600}, {"referenceID": 0, "context": "5, Bubeck and Cesa-Bianchi 1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(\u03b8) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.", "startOffset": 60, "endOffset": 86}, {"referenceID": 0, "context": "5, Bubeck and Cesa-Bianchi 1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(\u03b8) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.", "startOffset": 60, "endOffset": 166}, {"referenceID": 6, "context": "This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this.", "startOffset": 82, "endOffset": 103}, {"referenceID": 6, "context": "This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this. Proof We decompose the proof into three steps. We denote i\u2217(\u03b8) \u2208 argmaxi\u2208[K] \u03bci(\u03b8), in particular one has It = i\u2217(\u03b8(t)). Step 1: rewriting of the Bayesian regret in terms of upper confidence bounds. This step is given by [Proposition 1, Russo and Roy [2013]] which we reprove for the sake of completeness.", "startOffset": 82, "endOffset": 387}, {"referenceID": 2, "context": "Inspired by the MOSS strategy of Audibert and Bubeck [2009] we will now take", "startOffset": 33, "endOffset": 60}, {"referenceID": 2, "context": "Next we extract the following inequality from Audibert and Bubeck [2010] (see p2683\u20132684), for any i \u2208 [K], P(\u03bci \u2212Bi,t \u2265 u) \u2264 4K nu2 log (\u221a n K u ) + 1 nu2/K \u2212 1 .", "startOffset": 46, "endOffset": 73}, {"referenceID": 6, "context": "3 Thompson Sampling in the two-armed BPR setting Following [Section 2, Bubeck et al. [2013]] we consider here the two-armed bandit problem with sub-Gaussian reward distributions (that is they satisfy Ee\u03bb(X\u2212\u03bc) \u2264 e\u03bb2/2 for all \u03bb \u2208 R) and such that one reward distribution has mean \u03bc\u2217 and the other one has mean \u03bc\u2217 \u2212\u2206 where \u03bc\u2217 and \u2206 are known values.", "startOffset": 71, "endOffset": 92}, {"referenceID": 6, "context": "The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean \u03bc\u2217 and gap \u2206.", "startOffset": 107, "endOffset": 128}, {"referenceID": 6, "context": "The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean \u03bc\u2217 and gap \u2206. Theorem 2 The policy of Figure 1 has regret bounded as Rn \u2264 \u2206+ 578 \u2206 , uniformly in n. Note that we did not try to optimize the numerical constant in the above bound. Figure 2 shows an empirical comparison of the policy of Figure 1 with Policy 1 of Bubeck et al. [2013]. Note in particular that a regret bound of order 16/\u2206 was proved for the latter algorithm and the (limited) numerical simulation presented here suggests that Thompson Sampling outperforms this strategy.", "startOffset": 107, "endOffset": 516}, {"referenceID": 6, "context": "Policy 1 from Bubeck et al.[2013] Policy of Figure 1", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "Policy 1 from Bubeck et al.[2013] Policy of Figure 1", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "Figure 2: Empirical comparison of the policy of Figure 1 and Policy 1 of Bubeck et al. [2013] on Gaussian reward distributions with variance 1.", "startOffset": 73, "endOffset": 94}], "year": 2013, "abstractText": "We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit as the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by 1 20 \u221a nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.", "creator": "LaTeX with hyperref package"}}}