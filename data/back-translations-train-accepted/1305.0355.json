{"id": "1305.0355", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2013", "title": "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition", "abstract": "In the high-dimensional regression model a response variable is linearly related to $p$ covariates, but the sample size $n$ is smaller than $p$. We assume that only a small subset of covariates is `active' (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefficients through the Lasso ($\\ell_1$-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. In this paper we study the `Gauss-Lasso' selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate `generalized irrepresentability condition' (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.", "histories": [["v1", "Thu, 2 May 2013 07:25:52 GMT  (2565kb,D)", "http://arxiv.org/abs/1305.0355v1", "32 pages, 3 figures"]], "COMMENTS": "32 pages, 3 figures", "reviews": [], "SUBJECTS": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "authors": ["adel javanmard", "andrea montanari"], "accepted": true, "id": "1305.0355"}, "pdf": {"name": "1305.0355.pdf", "metadata": {"source": "CRF", "title": "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition", "authors": ["Adel Javanmard", "Andrea Montanari"], "emails": ["adelj@stanford.edu"], "sections": [{"heading": null, "text": "A common approach is the estimation of the regression coefficients by the lasso (\"1-regulated smallest squares\"), which is known to correctly identify the active amount only if the irrelevant covariates are approximately orthogonal to the relevant ones, as quantified by the so-called \"unrepresentability.\" In this paper, we examine the \"Gauss lasso\" selector, a simple two-step method that first solves the lasso and then performs ordinary smallest squares limited to the lasso-active quantity.We formulate the \"general condition of unrepresentability\" (GIC), an assumption that is much weaker than the non-representability. We prove that the Gauss lasso under GIC correctly reproduces the active quantity.Contents"}, {"heading": "1 Introduction 2", "text": "An example..................................................................................................................................."}, {"heading": "2 Deterministic designs 7", "text": ""}, {"heading": "3 Random Gaussian designs 10", "text": "3.1 The n = \u221e problem.......................................... 11 3.2 The high-dimensional problem.................................."}, {"heading": "4 UCI communities and crimes data example 14", "text": "* Faculty of Electrical Engineering, Stanford University. Email: adelj @ stanford.edu \u2020 Faculty of Electrical Engineering and Faculty of Statistics, Stanford University. Email: montanar @ stanford.eduar Xiv: 130 5.03 55v1 [m. ath. ST] 2M ay2 01"}, {"heading": "5 Proof of Theorems 2.5 and 2.7 15", "text": "5.1 Proof of theorem 2.5.............................................................................................................."}, {"heading": "6 Proof of Theorems 3.4 and 3.7 18", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Proof of technical lemmas 23", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Generalized irrepresentability vs. irrepresentability 28", "text": "References 32"}, {"heading": "1 Introduction", "text": "In this paper, we look at the high-dimensional setting, in which the number of parameters exceeds the sample size, i.e., p > n, but the number of non-zero problems of smaller magnitudes is lower than the number of smaller phenomena. (2) We look at the high-dimensional setting, in which the number of parameters exceeds the number of smaller phenomena. (2) We want to exceed the number of smaller phenomena. (2) We want to estimate the number of parameters higher than the number of smaller phenomena. (2) We believe that the number of parameters exceeds the number of smaller phenomena. (2) We want to exceed the number of smaller phenomena. (2) We look at the high-dimensional setting, in which the number of parameters exceeds the number of parameters. (2) We want to exceed the number of small phenomena of small phenomena. (2) We look at the high-dimensional setting, in which the number of parameters exceeds the number of parameters."}, {"heading": "1.1 An example", "text": "To illustrate the range of new cases covered by our results, it is instructive to consider a simple example. A detailed discussion of this calculation can be found in Appendix B. The example corresponds to a Gaussian random principle, i.e., lines XT1,... X T n are i.i.d. realizations of a pvariant normal distribution with an average zero. We write Xi = (Xi, 1, Xi, 2,..., Xi, p) T for the components of Xi. However, the response variable refers linearly to the first s0 covariants. Yi = Phenomen0.1Xi, 1 + Phenomen0.2Xi, 2 + \u00b7 p capability of Xi, s0 + Wi, where Wi \u00b2 N (0, 2) and we assume that the phenomena 0, i > 0 are all i \u2264 s0 and variable. Specifically S = {1,...., s0, s0}."}, {"heading": "1.2 Further related work", "text": "The limited isometry property [CT05, CT07] (or the associated limited eigenvalue [BRT09] = guarantees higher efficiency = compatibility conditions [vdGB09]) has been used to establish guarantees for the estimation and model selection errors of the lasso or similar approaches. In particular, however, the same conditions can be used to prove model selection guarantees. Zhou [Zho10] investigates a multi-step threshold procedure whose first steps coincide with the Gauss lasso model. While the main objective of this work is to prove a high consistency of the 2 consistency with a sparse estimated model, the author also demonstrates a partial model selection."}, {"heading": "1.3 Notations", "text": "For a matrix A and a series of indices I, J, we let AJ denote the submatrix containing only the columns in J and AI, J the submatrix formed by the rows in I and the columns in J. Similarly, for a vector v vI is the restriction of v to indices in I. Furthermore, notation A \u2212 1I, I stands for the inversion of AI, I, i.e. A \u2212 1 I, I = (AI, I) \u2212 1. The maximum and the minimum singular values of A are each denoted by \u03c3max (A) and \u03c3min (A)."}, {"heading": "2 Deterministic designs", "text": "Here are some useful properties of the lasso estimator: 1. We first look at the zero noise problem W = 0 and in this case prove several useful properties of the lasso estimator. In particular, we show that there is a threshold for the regularization parameter below which the support of the lasso estimator remains the same and contains Supp (\u03b80). Furthermore, the support of the lasso estimator is not much greater than Supp (\u03b80).2. We then turn to the problem of noise development and introduce the general non-representability (GIC), which is motivated by the properties of the lasso in the case of zero noise. We prove that under GIC (and other technical conditions) the signed support of the lasso estimator is most likely to be the same as the one in the zero noise problem. 3. We show that the Gauss lasso selector correctly recovers the signed support of the Hig0."}, {"heading": "2.1 Zero-noise problem", "text": "Just remember that the number of people who are staying in a country, compared to the number of people who are staying in a country, compared to a country where they are staying in a country, compared to a country where they are staying in a country, compared to a country where they are staying in another country, compared to a country where they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living in which they are living, in which they are living, in which they are living in which they are living, in which they are living, in which they are living, in which they are living in which they are living in which they are living, in which they are living, in which they are living in which they are living, in which they are living, in which they are living, in which they are in which they are living, in which they are in which they are living, they are living, in which they are living, in which they are in which they are living, in which they are living, in which they are in which they are living."}, {"heading": "2.2 Noisy problem", "text": "We start with a standard characterization of the character (3), and allow the signature of the character (3), the signature of the character (3), the signature of the character (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2), the signature (2, the signature (2), the signature (2, the signature (2), the signature (2, the signature (2), the signature (2), the signature (2, the signature (2), the signature (2), the signature (2, the signature (2), the signature (2), the signature (2, the signature (2), the signature (2, the signature (2), the signature (2), the signature (2, the signature (2), the signature (2), the signature (2, the signature (2), the signature (2), the signature (the signature (2, the 2, the 2, the signature (2, the 2), the signature (the 2, the 2, the signature, the 2, the signature (the 2, the 2, the 2, the 2), the signature (the 2, the 2, the signature (the 2), the signature (the 2, the 2, the signature), the signature, the signature (the 2, the signature (the 2, the 2, the 2, the 2, the 2, the signature, the 2, the signature (the 2, the 2, the 2, the 2, the 2, the 2, the 2, the signature, the 2, the signature, the 2, the 2, the 2, the signature, the 2, the signature, the 2, the 2, the signature, the 2, the signature"}, {"heading": "3 Random Gaussian designs", "text": "In the previous section, we examined the case of deterministic design models that allowed a simple analysis. Here, we consider the random design model, which requires a more comprehensive analysis. Within the random Gaussian design model, the lines Xi are distributed for some (unknown) covariance matrix \u03a3. To investigate the performance of the Gaussian lasso selector in this case, we first define the population estimator. Given that the population threshold is Rp \u00b7 p, \u03a3 0, \u03b80, Rp and \u04450, the population value estimator. (Population value estimator.) Population level estimator. (Population value.). (Population value.). (Population value.)...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.1 The n =\u221e problem", "text": "In this section we deduce several useful properties of the problem at the population level (17). If we compare Eqs (5) and (17), the payers are defined in a very similar way (the former is defined in relation to the covariance matrix, and the latter is defined in relation to \u03a3), and as we see, the payers also possess the properties that are constant in Section 2.1. Let the eigenvalue constant for the covariance matrix be constant (s, c0). (18) The proofs of the following Lemmas are very similar to the corresponding signs in Section 2.1 and are omitted.Lemma 3.1."}, {"heading": "3.2 The high-dimensional problem", "text": "Let us consider the lasso estimator as (3). - Let us remember the notations, which in this case depend on random design quantities. - Let us consider the Gaussian random model with covariance 0, and let us assume that a random design model with covariance 0, and let us assume that a random design model with covariance 0, and a random design model with random design quantities 0, 0, - 1, - 1, - p defines the deterministic set and the vector in Lemma 3.2. - Assumption that (i) We have random design quantities 0, - 0, T0, - 1, - 1, - 1, - p the deterministic set and the vector defined in Lemma 3.2. - Assumption that (i) We have random design quantities 0, - 0, T0, - 1, - 2, - 2, - the random design quantities, 2, 2 - the random definition."}, {"heading": "4 UCI communities and crimes data example", "text": "We consider a problem in predicting the rate of violent crime in different communities within the USA, based on other demographic attributes of communities = 1994. We evaluate the performance of the Gauss lasso selector on the UCI communities and crime datasets [FA10]. The dataset consists of a universal response variable and 122 predictable attributes for 1994 communities. The response variable is the total number of violent crimes per 100K population. The covariants are quantitative, including e.g. the average family income, the percentage of the unemployed population and the operational budget of the police. We consider a linear model as in (2) and perform model-selected models using the Gausslasso selector and the lasso estimator. We do the following pre-processing steps: (i) Any missing value is replaced by the mean of the non-missing values of this attribute for other communities; (ii) We eliminate the attributes of 16 attributes together."}, {"heading": "5 Proof of Theorems 2.5 and 2.7", "text": "In this section we will prove theorems 2.5 and 2.7 by using lemmas 2.1 to 2.4, the latter of which are documented in the appendices."}, {"heading": "5.1 Proof of Theorem 2.5", "text": "By condition (iii) in the explanation of the lasso solution in the order of less than 0.005."}, {"heading": "5.2 Proof of Theorem 2.7", "text": "Remember that T = supp (zipp \u2212 zipp \u2212 zipp \u2212 np \u2212 np np np \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 pnp \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 pnp \u2212 pnp \u2212 pnp \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np \u2212 np"}, {"heading": "6 Proof of Theorems 3.4 and 3.7", "text": "According to condition (iii) in the explanation of the theorem, we have the second inequality due to Lemma 3.2. Therefore, following Lemma 3.2, we have characters (\u03a3 \u2212 1T0, T0v0, T0] = v0 and that supp (v0) = T0 contains the true support. If we apply Lemma 3.3 and apply the general assumption of inrepresentability, we have the assumption that we contain the characters T0c, T0v0, T0v0, T0v0, T0. (32) v0, T0 = characters (v0, T0 \u2212 Tv0, T0 \u2212 T0). (33) Furthermore, we show the condition (T0v0 \u2212 Tv0, Tv0 \u2212 Tv0, Tv0 \u2212 v0, if Eqs (11) and the condition (1Tv0, Tv0 \u2212 Tv0, Tv0 \u2212 Tv0)."}, {"heading": "6.1 Proof of Eq. (34)", "text": "It is immediately apparent that Equation (34) applies if the following standards apply: T1 \u0445. (T2). (37) To prove inequalities (36) and (37), it is useful to take the following sentence from the random matrix theory 6.1 ([DS01, Wai09, Ver12]. For k. (37). Let X. \u2212 Rn. \u2212 k be a random matrix with i.i.d rows drawn from N (0, \u03a3). Then the following sentence applies to all t. (DS01, Wai09, Ver12). Sentence 6.1 (DS01, Ver12). For k. \u2212 n."}, {"heading": "6.1.1 Bounding T1", "text": "The argument in [Wai09] works under the condition of unrepresentability (see Eq. (26), and we modify it to apply it to the current situation, i.e., to generalized unrepresentability. We start with the condition of unrepresentability on XT0. \u2212 T0c, xj is a zero-mean Gaussian vector, and we can divide it into a linear correlated part plus an uncorrelated part asxTj = [1 T0, T0 XTT0 + T j, where j [0] Rn has i.e. entries distributed as a linear part plus an uncorrelated part asxTj = [0]."}, {"heading": "6.1.2 Bounding T2", "text": "s leave m = (1 / \u03bb) (r-T0c \u2212 \u03a3-T0c, T0\u03a3-1 T0 \u2212 T0 r-T0). If we set r, we get m = (1 / \u03bb) (r-T0c \u2212 \u03a3-T0c, T0\u0440 \u2212 1 T0, T0 r-T0). Since W-N (0, \u03c32In \u00b7 n), caused by X, the variable mj = x T-X-T0 W / (n\u03bb) is normal with a deviation at most. Starting with W-N, the variable property of orthogonal extrapolations is normal. Now, let us define event E as follows: E-N event {xj-2 < 2n-P-2)."}, {"heading": "6.2 Proof of Eq. (35)", "text": "Next, we prove that Eq. (35). (33).................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6.2.2 Bounding T4", "text": "Lemma 6.3. The following applies: P (T4 \u2264 3\u03c3 \u221a 2c1 log pnCmin) \u2265 1 \u2212 2e \u2212 t0 2 \u2212 2p1 \u2212 c1. (50) Lemma 6.3 is documented in section A.7. From the last Lemma it follows that equation (49) probably holds at least 1 \u2212 2e \u2212 t0 2 \u2212 2p1 \u2212 c1, provided that 3\u03c3 \u221a 2c1 log pnCmin \u2264 c2\u03bb."}, {"heading": "6.3 Summary: Proof of Theorem 3.4", "text": "If we now combine the boundaries of T1,.. T4, we obtain that t0 log p, eqs. (34) and (35) are held simultaneously for n \u2265 max (M1, M3), with a probability of at least 1 \u2212 pe \u2212 n / 10 \u2212 6e \u2212 t0 / 2 \u2212 8p1 \u2212 c1. Hence the sign (\u03b8-n (\u03bb)) = v0."}, {"heading": "6.4 Proof of Theorem 3.7", "text": "Note that the matrix XT0 is a random Gaussian matrix, the rows of which are independently drawn in the form of N (0, T0, T0) (remember that T0 is a deterministic set determined by the population problem).Therefore, using theorem 3.4 to determine the probability that T is 6 = T0, the proof runs along the same lines as the proof of theorem 2.7."}, {"heading": "Acknowledgements", "text": "A.J. is supported by a Caroline and Fabian Pease Stanford Graduate Fellowship. This work was partially supported by the NSF CAREER Award CCF-0743978, the NSF Grant DMS-0806211 and the AFOSR / DARPA scholarships FA9550-12-1-0411 and FA9550-13-1-0036."}, {"heading": "A Proof of technical lemmas", "text": "It is easy to recognize that this character (u) (u) (u) (u) (u) (u) (u) (u)) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u) (u)) (u) (u) (u)."}, {"heading": "B Generalized irrepresentability vs. irrepresentability", "text": "In this appendix, we will discuss the example in more detail in Section 1.1. The goal is to develop a certain intuition regarding the validity of generalized inrepresentability and to compare it with the standard inrepresentability."}], "references": [{"title": "Bolasso: model consistent lasso estimation through the bootstrap", "author": ["Francis R Bach"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Statistical significance in high-dimensional linear models, arXiv:1202.1377", "author": ["P. B\u00fchlmann"], "venue": null, "citeRegEx": "B\u00fchlmann,? \\Q2012\\E", "shortCiteRegEx": "B\u00fchlmann", "year": 2012}, {"title": "Statistics for high-dimensional data, SpringerVerlag", "author": ["Peter B\u00fchlmann", "Sara van de Geer"], "venue": null, "citeRegEx": "B\u00fchlmann and Geer,? \\Q2011\\E", "shortCiteRegEx": "B\u00fchlmann and Geer", "year": 2011}, {"title": "Examples of basis pursuit", "author": ["S.S. Chen", "D.L. Donoho"], "venue": "Proceedings of Wavelet Applications in Signal and Image Processing III (San Diego, CA),", "citeRegEx": "Chen and Donoho,? \\Q1995\\E", "shortCiteRegEx": "Chen and Donoho", "year": 1995}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E. Candes", "J.K. Romberg", "T. Tao"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Decoding by linear programming", "author": ["E.J. Cand\u00e9s", "T. Tao"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Cand\u00e9s and Tao,? \\Q2005\\E", "shortCiteRegEx": "Cand\u00e9s and Tao", "year": 2005}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Cand\u00e9s", "T. Tao"], "venue": "Annals of Statistics", "citeRegEx": "Cand\u00e9s and Tao,? \\Q2007\\E", "shortCiteRegEx": "Cand\u00e9s and Tao", "year": 2007}, {"title": "Local operator theory, random matrices and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook on the Geometry of Banach spaces,", "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "Annals of Statistics", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "UCI machine learning repository (communities and crime data set), http://archive.ics.uci.edu/ml, 2010, University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Frank", "A. Asuncion"], "venue": null, "citeRegEx": "Frank and Asuncion,? \\Q2010\\E", "shortCiteRegEx": "Frank and Asuncion", "year": 2010}, {"title": "Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory", "author": ["Adel Javanmard", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1301.4240,", "citeRegEx": "Javanmard and Montanari,? \\Q2013\\E", "shortCiteRegEx": "Javanmard and Montanari", "year": 2013}, {"title": "Chi-squared oracle inequalities, State of the Art in Probability and Statistics (M", "author": ["I. Johnstone"], "venue": "IMS Lecture Notes, Institute of Mathematical Statistics,", "citeRegEx": "Johnstone,? \\Q2001\\E", "shortCiteRegEx": "Johnstone", "year": 2001}, {"title": "Asymptotics for lasso-type estimators", "author": ["K. Knight", "W. Fu"], "venue": "Annals of Statistics", "citeRegEx": "Knight and Fu,? \\Q2000\\E", "shortCiteRegEx": "Knight and Fu", "year": 2000}, {"title": "The concentration of measure phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs,", "citeRegEx": "Ledoux,? \\Q2001\\E", "shortCiteRegEx": "Ledoux", "year": 2001}, {"title": "Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators", "author": ["Karim Lounici"], "venue": "Electronic Journal of statistics", "citeRegEx": "Lounici,? \\Q2008\\E", "shortCiteRegEx": "Lounici", "year": 2008}, {"title": "Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer", "author": ["Jie Peng", "Ji Zhu", "Anna Bergamaschi", "Wonshik Han", "Dong-Young Noh", "Jonathan R Pollack", "Pei Wang"], "venue": "The Annals of Applied Statistics", "citeRegEx": "Peng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2010}, {"title": "A simple and efficient algorithm for gene selection using sparse logistic regression, Bioinformatics", "author": ["Shirish Krishnaj Shevade", "S. Sathiya Keerthi"], "venue": null, "citeRegEx": "Shevade and Keerthi,? \\Q2003\\E", "shortCiteRegEx": "Shevade and Keerthi", "year": 2003}, {"title": "Regression shrinkage and selection with the Lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc B", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "On asymptotically optimal confidence regions and tests for high-dimensional models, arXiv:1303.0518", "author": ["S. van de Geer", "P. B\u00fchlmann", "Y. Ritov"], "venue": null, "citeRegEx": "Geer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Geer et al\\.", "year": 2013}, {"title": "Introduction to the non-asymptotic analysis of random matrices, Compressed Sensing: Theory and Applications (Y.C", "author": ["R. Vershynin"], "venue": "Eldar and G. Kutyniok, eds.),", "citeRegEx": "Vershynin,? \\Q2012\\E", "shortCiteRegEx": "Vershynin", "year": 2012}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming", "author": ["M.J. Wainwright"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Wainwright,? \\Q2009\\E", "shortCiteRegEx": "Wainwright", "year": 2009}, {"title": "Thresholded Lasso for high dimensional variable selection and statistical estimation, arXiv:1002.1583v2", "author": ["S. Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2010\\E", "shortCiteRegEx": "Zhou", "year": 2010}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "Confidence Intervals for Low-Dimensional Parameters in High-Dimensional", "author": ["C.-H. Zhang", "S.S. Zhang"], "venue": "Linear Models,", "citeRegEx": "Zhang and Zhang,? \\Q2011\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2011}], "referenceMentions": [], "year": 2013, "abstractText": "In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is \u2018active\u2019 (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefficients through the Lasso (`1-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called \u2018irrepresentability\u2019 condition. In this paper we study the \u2018Gauss-Lasso\u2019 selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate \u2018generalized irrepresentability condition\u2019 (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.", "creator": "LaTeX with hyperref package"}}}