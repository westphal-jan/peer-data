{"id": "1305.1363", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2013", "title": "One-Pass AUC Optimization", "abstract": "AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.", "histories": [["v1", "Tue, 7 May 2013 00:30:32 GMT  (104kb)", "https://arxiv.org/abs/1305.1363v1", "Proceeding of 30th International Conference on Machine Learning"], ["v2", "Thu, 16 May 2013 13:24:37 GMT  (107kb)", "http://arxiv.org/abs/1305.1363v2", "Proceeding of 30th International Conference on Machine Learning"]], "COMMENTS": "Proceeding of 30th International Conference on Machine Learning", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wei gao", "rong jin", "shenghuo zhu", "zhi-hua zhou"], "accepted": true, "id": "1305.1363"}, "pdf": {"name": "1305.1363.pdf", "metadata": {"source": "CRF", "title": "One-Pass AUC Optimization", "authors": ["Wei Gao", "Rong Jin", "Shenghuo Zhu", "Zhi-Hua Zhou"], "emails": ["gaow@lamda.nju.edu.cn", "rongjin@cse.msu.edu", "zsh@nec-labs.com", "zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 130 5.13 63v2 [Keywords: AUC optimization, rank learning, large learning, random projection, square loss"}, {"heading": "1. Introduction", "text": "AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measurement that is widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011). Many algorithms have been developed to optimize AUC based on replacement losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).In this paper, we focus on AUC optimization, which requires only one run of training examples, which is particularly important for applications with large data or streamingc \u00a9 Gao, Jin, Zhu and Zhou.data, where a large volume of data occurs in a short period of time, making it impossible to store the entire amount of data in memory before an optimization process is applied."}, {"heading": "2. Preliminaries", "text": "We use X-Rd to denote an instance space and Y = {+ 1, \u2212 1} the problem set, and leave D an unknown (underlying) distribution over X-Y. A training sample of n + positive instances and n \u2212 negative onesS = {(x + 1, \u2212 1), (x + 2, + 1),.., (x + n +, + 1), (x \u2212 1, \u2212 1), (x \u2212 2, \u2212 1),.., (x \u2212 n \u2212, \u2212 1)} is drawn identically and independently according to the distribution D, not fixing n + and n \u2212 before selecting the training sample. Let f: X \u2212 R be a really evaluated function. Then, the AUC of the function f on the sample S is defined so that i = 1n \u2212 \u2211 j = 1I = 1I [f (x + i) > f \u2212 j \u2212 n = f (x \u2212 j \u2212 za) n = f (x \u2212 n) n problem where the optimization is a function, is the function W = 1I."}, {"heading": "3. The OPAUC Approach", "text": "In order to address the challenge of one-pass AUC optimization, we propose to use the square loss in Eq (= 1), i.e., L (w) = 1 (1), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 2 (2), 4 (2), 4 (2), 4 (2), 4 (2), 4 (4), 4 (4), 4), 4 (2), 5 (2), 5 (5), 5 (5), 5, 5 (5), 5, 5 (5), 5, 5, 5 (5), 4, 5 (4), 4, 5 (5), 4, 5 (5), 5 (5), 5 (5), 4, 5 (5), 5 (5), 5, 5 (5), 4, 5 (5), 5, 5, 5 (5), 5, 5 (5), 4, 5 (5), 5, 5, 5 (5), 5, 5 (5), 5 (4), 5, 5, 5 (5), 5 (4), 5 (4, 5 (5), 5, 5 (5), 5, 5 (5), 5, 5 (2), 5, 5 (2, 5 (2), 5 (5), 5 (2, 5), 5 (2, 5 (2), 5 (5), 5 (2, 5 (2), 5 (2), 5, 5 (4), 5 (4), 5, 5 (2, 5 (5 (2), 5 (2), 5, 5 (2), 5, 5 (2), 5 (2, 5 (2), 5 (2), 5, 5 (2), 5 (2), 5 (2), 5 (2, 5 (2), 5 (2), 5, 5 ("}, {"heading": "4. Main Theoretical Result", "text": "In this section we present the most important theoretical results for our proposed algorithm, the following theorem shows the consistency of the square loss, and the detailed proof is given in Section 5.1.Theorem 1 for square losses (t) = (1 \u2212 t) 2, the actual convergence rate for algorithm 1 (f, x, x, x \u2032) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x) = (f, x, x, x) = (f, x, x, x (f, x, x) = (f, x, x, x) = (f, (f, x) =, (f, f, x, f =, x, f =, x, x, x (f) =, (f, x) =, (f, f, x, f, x, f, x (f) =, x (f, x) =, x (f, f, x, f, x, x (f, x) =, x (f, x) =, x (f, x, f, f, x (f) =, x (f) =, x (f) =, x (f, x, x (f), x (f) =, x (f, f), x (f, f, x (f), x (f), x (f (f, f), f = f, x (f), x (f), x (f), x (f =, x (f, f, x, f), x (f, x (f = f), x (f, x (f), x (f), x (f), x (f), x (f, x (f), x (f = f), x (f, x (f, f, f, x (f), x (f = f), x (f), x, x (f, x (f), x"}, {"heading": "5. Proofs", "text": "In this section we present detailed evidence for our most important theorems."}, {"heading": "5.1 Proof of Theorem 1", "text": "Leave X = {x1, x2,., xn} with instance marginal probability pi = Pr [xi] and conditional probability pi = Pr [y = + 1 | xi], and we will by the expected risk pi (f) = C0 + 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 i = jpipj (1 \u2212 2) (f (xi) \u2212 f (xj)) + 1 \u2212 i) (f (xj) \u2212 f (xi) \u2212 f (xi)))))) where (t) = (1 \u2212 t) 2 and C0 is a constant in relation to f (xi) (1 \u2264 i \u2264 n). According to the analysis of (Gao and Zhou, 2012, it is sufficient to prove that for each optimal solution f, i.e., rating (f) = instance marginal probability pi (f), f (f), we f (xi) > f (xj) if we have an optimal solution."}, {"heading": "5.2 Proof of Theorem 2", "text": "This proof is motivated by (Shalev-Shwartz, 2007; Srebro et al., 2010). (RecallLt (w) = \u03bb2 | w | 2 + p = p = p = p = p = p = p = p = p = p = p (p = p = p = p = p (w). (12) It is easy to deduce that p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p = p = p p p = p = p = p p p = p p p p p p p p = p = p = p p p p p p p p p p p = p p p p p p p = p p p p p p p p p p p p p p p p p p p p"}, {"heading": "5.3 Proof of Theorem 4", "text": "Before the detailed detection of Theorem 4 we start with some useful results: Lemma 5 = > Let's make S1 = > Let's make (s1i) and S2 = diag (s2i) two d + d diagonal matrices, so that s1i is 6 = 0 and s21i + s22i = 1 for all i. For a Gaussian random matrix R \u2212 Rd \u00b7 s, we set Z = S1S1 + S2RR \u2212 S2 and r = 2 2i, and the following holdPr [\u03bb1 (Z)."}, {"heading": "6. Experiments", "text": "We evaluate the performance of OPAUC using benchmark data sets or high-dimensional data sets in Section 6.1 and 6.2 respectively. We then examine the influence of parameters in Section 6.3."}, {"heading": "6.1 Comparison on Benchmark Data", "text": "We are conducting our experiments on sixteen benchmark datasets1,2,3 as summarized in Table 1. Some datasets have been used in previous studies for AUC optimization, while the others are large ones requiring a one-pass method. Properties have been scaled to [\u2212 1, 1] for all datasets. \u2022 Multi-class datasets have been changed into binary ones by random partitioning classes into two groups, where each group has the same number of classs.1. http: / / www.sigkdd.org / kddcup / 2. http: / www.ics.uci.edu /. mlearn / MLRepository.html 3. http: / / www.csie.edu.edu.e."}, {"heading": "6.2 Comparison on High-Dimensional Data", "text": "Next, we will explore the performance of online algorithms to approximate the full covariance matrices cited by OPAUCr. Six datasets4,5 with nearly 50,000 characteristics are used, as summarized in Table 4. news20.binary dataset contains two classes that are suspended from news20 dataset. Original News20 and the sector are multi-level datasets that include the different classes in our experiments, and we also use sector.lvr dataset, which considers the largest class as positive, while the union of other classes is considered negative. Original ecml2012 and rcv1v2 are multi-label datasets, we only consider the label with the largest population, and we remove the features in ecml2012 dataset that take zero values for all instances."}, {"heading": "6.3 Parameter Influence", "text": "We examine the influence of parameters in this section. Figure 3 shows that the step size \u03b7t should not be set to values greater than 1, whereas there is a relatively large range between [2 \u2212 12, 2 \u2212 4] in which OPAUC achieves good results. Figure 4 shows that OPAUC is not sensitive to the value of the regularization parameter \u03bb because it is not set to a large value. Figure 5 shows that OPAUCr is not sensitive to the values of the rank \u03c4, and it works well even if it is equal to 50; this confirms theorem 4 that a relatively small \u03c4 value is sufficient to produce a good approximation performance. Figure 6 compares the influence of the iterations for OPAUC, OAMseq, and OAMgra, and it can be observed that OPAUC converges faster than the other two algorithms, which confirms our theoretical argument in section 4."}, {"heading": "7. Conclusion", "text": "In this paper, we examine a one-time AUC optimization that requires training data to be traversed only once without storing the entire dataset. A major challenge lies in the fact that AUC is measured using a sum of losses defined by pairs of instances from different classes. We propose the OPAUC approach, which provides a square loss and requires the storage of only the first and second statistics for the training examples obtained. A nice feature of OPAUC is that its memory requirement is O (d2), where d is the data dimension, regardless of the number of training examples. To be able to handle high-dimensional data, we develop an approximate strategy by using low-grade matrices. The effectiveness of our proposed approach is tested both theoretically and empirically. Specifically, the performance of OPAUC is significantly better than the modern online AUC optimization approaches, even highly competitive with batch-learning approaches, and significantly better than any other approach that is comparable to hundreds of thousands of data sets."}], "references": [{"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "AUC optimization vs. error rate minimization", "author": ["C. Cortes", "M. Mohri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cortes and Mohri.,? \\Q2004\\E", "shortCiteRegEx": "Cortes and Mohri.", "year": 2004}, {"title": "A coherent interpretation of AUC as a measure of aggregated classification performance", "author": ["P.A. Flach", "J. Hern\u00e1ndez-Orallo", "C.F. Ramirez"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Flach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Flach et al\\.", "year": 2011}, {"title": "On the consistency of AUC optimization", "author": ["W. Gao", "Z.-H. Zhou"], "venue": "CoRR, 1208.0645v3,", "citeRegEx": "Gao and Zhou.,? \\Q2012\\E", "shortCiteRegEx": "Gao and Zhou.", "year": 2012}, {"title": "Tail bounds for all eigenvalues of a sum of random matrices", "author": ["Alex Gittens", "Joel A. Tropp"], "venue": "CoRR, abs/1104.4513v2,", "citeRegEx": "Gittens and Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Gittens and Tropp.", "year": 2011}, {"title": "A method of comparing the areas under receiver operating characteristic curves derived from the same cases", "author": ["J.A. Hanley", "B.J. McNeil"], "venue": null, "citeRegEx": "Hanley and McNeil.,? \\Q1983\\E", "shortCiteRegEx": "Hanley and McNeil.", "year": 1983}, {"title": "Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion", "author": ["P.C. Hansen"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Hansen.,? \\Q1987\\E", "shortCiteRegEx": "Hansen.", "year": 1987}, {"title": "Optimising area under the ROC curve using gradient descent", "author": ["A. Herschtal", "B. Raskutti"], "venue": "In Proceedings of the 21st International Conference on Machine Learning, Alberta,", "citeRegEx": "Herschtal and Raskutti.,? \\Q2004\\E", "shortCiteRegEx": "Herschtal and Raskutti.", "year": 2004}, {"title": "Random design analysis of ridge regression", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "Joachims.,? \\Q2005\\E", "shortCiteRegEx": "Joachims.", "year": 2005}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "High dimensional statistical inference and random matrices", "author": ["I. Johnstone"], "venue": "In Proceedings of the International Congress of Mathematicians,", "citeRegEx": "Johnstone.,? \\Q2006\\E", "shortCiteRegEx": "Johnstone.", "year": 2006}, {"title": "Bipartite ranking through minimization of univariate loss", "author": ["W. Kotlowski", "K. Dembczynski", "E. H\u00fcllermeier"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Kotlowski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kotlowski et al\\.", "year": 2011}, {"title": "Exploratory undersampling for class-imbalance learning", "author": ["X.-Y. Liu", "J. Wu", "Z.-H. Zhou"], "venue": "IEEE Trans. Systems, Man, and Cybernetics - B,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Basic principles of ROC analysis", "author": ["C.E. Metz"], "venue": "Seminars in Nuclear Medicine,", "citeRegEx": "Metz.,? \\Q1978\\E", "shortCiteRegEx": "Metz.", "year": 1978}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2003\\E", "shortCiteRegEx": "Nesterov.", "year": 2003}, {"title": "The case against accuracy estimation for comparing induction algorithms", "author": ["F.J. Provost", "T. Fawcett", "R. Kohavi"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "Provost et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Provost et al\\.", "year": 1998}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Margin-based ranking and an equivalence between AdaBoost and RankBoost", "author": ["C. Rudin", "R.E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin and Schapire.,? \\Q2009\\E", "shortCiteRegEx": "Rudin and Schapire.", "year": 2009}, {"title": "Online Learning: Theory, Algorithms, and Applications", "author": ["S. Shalev-Shwartz"], "venue": "PhD thesis, Hebrew University of Jerusalem,", "citeRegEx": "Shalev.Shwartz.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2007}, {"title": "Smoothness, low noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Online AUC maximization", "author": ["P. Zhao", "S. Hoi", "R. Jin", "T. Yang"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al.", "startOffset": 40, "endOffset": 77}, {"referenceID": 5, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al.", "startOffset": 40, "endOffset": 77}, {"referenceID": 16, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 1, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 13, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 2, "context": "Introduction AUC (Area Under ROC curve) (Metz, 1978; Hanley and McNeil, 1983) is an important performance measure that has been widely used in many tasks (Provost et al., 1998; Cortes and Mohri, 2004; Liu et al., 2009; Flach et al., 2011).", "startOffset": 154, "endOffset": 238}, {"referenceID": 7, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 10, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 18, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 12, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 21, "context": "Many algorithms have been developed to optimize AUC based on surrogate losses (Herschtal and Raskutti, 2004; Joachims, 2006; Rudin and Schapire, 2009; Kotlowski et al., 2011; Zhao et al., 2011).", "startOffset": 78, "endOffset": 193}, {"referenceID": 0, "context": "Although many online learning algorithms have been developed to find the optimal solution of some performance measures by only scanning the training data once (Cesa-Bianchi and Lugosi, 2006), few effort addresses one-pass AUC optimization.", "startOffset": 159, "endOffset": 190}, {"referenceID": 0, "context": "Although many online learning algorithms have been developed to find the optimal solution of some performance measures by only scanning the training data once (Cesa-Bianchi and Lugosi, 2006), few effort addresses one-pass AUC optimization. Unlike the classical classification and regression problems where the loss function can be calculated on a single training example, AUC is measured by the losses defined over pairs of instances from different classes, making it challenging to develop algorithms for one-pass optimization. An online AUC optimization algorithm was proposed very recently by Zhao et al. (2011). It is based on the idea of reservoir sampling, and achieves a solid regret bound by only storing \u221a T instances, where T is the number of training examples.", "startOffset": 160, "endOffset": 615}, {"referenceID": 21, "context": "Zhao et al. (2011) addressed this challenge by exploiting the reservoir sampling technique.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "In contrast, loss functions such as hinge loss are proven to be inconsistent with AUC (Gao and Zhou, 2012).", "startOffset": 86, "endOffset": 106}, {"referenceID": 11, "context": "An alternative approach for the high-dimensional case is through the random projection (Johnstone, 2006; Hsu et al., 2012).", "startOffset": 87, "endOffset": 122}, {"referenceID": 8, "context": "An alternative approach for the high-dimensional case is through the random projection (Johnstone, 2006; Hsu et al., 2012).", "startOffset": 87, "endOffset": 122}, {"referenceID": 21, "context": "Compared to the online AUC optimization algorithm (Zhao et al., 2011), which achieves at most O(1/ \u221a T ) convergence rate, our proposed algorithm clearly reduce the regret.", "startOffset": 50, "endOffset": 69}, {"referenceID": 17, "context": "The faster convergence rate of our proposed algorithm owes to the smoothness of the square loss, an important property that has been explored by some studies of online learning (Rakhlin et al., 2012) and generalization error bound analysis (Srebro et al.", "startOffset": 177, "endOffset": 199}, {"referenceID": 20, "context": ", 2012) and generalization error bound analysis (Srebro et al., 2010).", "startOffset": 48, "endOffset": 69}, {"referenceID": 6, "context": "To capture the skewed eigenvalue distribution, we introduce the concept of effective numerical rank (Hansen, 1987) that generalizes the rank of matrix: Definition 3 For a positive constant \u03bc > 0 and semi-positive definite matrix M \u2208 Rd\u00d7d of eigenvalues {\u03bdi}, the effective numerical rank w.", "startOffset": 100, "endOffset": 114}, {"referenceID": 3, "context": "According to the analysis of (Gao and Zhou, 2012), it suffices to prove that, for every optimal solution f , i.", "startOffset": 29, "endOffset": 49}, {"referenceID": 19, "context": "2 Proof of Theorem 2 This proof is motivated from (Shalev-Shwartz, 2007; Srebro et al., 2010).", "startOffset": 50, "endOffset": 93}, {"referenceID": 20, "context": "2 Proof of Theorem 2 This proof is motivated from (Shalev-Shwartz, 2007; Srebro et al., 2010).", "startOffset": 50, "endOffset": 93}, {"referenceID": 4, "context": "Proof This proof technique is motivated from (Gittens and Tropp, 2011) by adding a bias matrix.", "startOffset": 45, "endOffset": 70}, {"referenceID": 21, "context": "In addition to state-of-the-art online AUC approachesOAMseq andOAMgra (Zhao et al., 2011), we also compare with: \u2022 online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss (Kotlowski et al.", "startOffset": 70, "endOffset": 89}, {"referenceID": 12, "context": ", 2011), we also compare with: \u2022 online Uni-Exp: An online learning algorithm which optimizes the (weighted) univariate exponential loss (Kotlowski et al., 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al.", "startOffset": 137, "endOffset": 161}, {"referenceID": 9, "context": ", 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al.", "startOffset": 179, "endOffset": 195}, {"referenceID": 10, "context": ", 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al.", "startOffset": 280, "endOffset": 296}, {"referenceID": 12, "context": ", 2011); \u2022 online Uni-Squ: An online learning algorithm which optimizes the (weighted) univariate square loss; \u2022 SVM-perf: A batch learning algorithm which directly optimizes AUC (Joachims, 2005); \u2022 batch SVM-OR: A batch learning algorithm which optimizes the pairwise hinge loss (Joachims, 2006); \u2022 batch LS-SVM: A batch learning algorithm which optimizes the pairwise square loss; \u2022 batch Uni-Log: A batch learning algorithm which optimizes the (weighted) univariate logistic loss (Kotlowski et al., 2011); \u2022 batch Uni-Squ: A batch learning algorithm which optimizes the (weighted) univariate square loss.", "startOffset": 483, "endOffset": 507}, {"referenceID": 21, "context": "For OAMseq and OAMgra, the buffer sizes are fixed to be 100 as recommended in (Zhao et al., 2011).", "startOffset": 78, "endOffset": 97}, {"referenceID": 12, "context": ", class ratios) are chosen as done in (Kotlowski et al., 2011).", "startOffset": 38, "endOffset": 62}], "year": 2013, "abstractText": "AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires going through the training data only once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second-order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high-dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low-rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.", "creator": "LaTeX with hyperref package"}}}