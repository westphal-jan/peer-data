{"id": "1305.2532", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2013", "title": "Learning Policies for Contextual Submodular Prediction", "abstract": "Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on no-regret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.", "histories": [["v1", "Sat, 11 May 2013 18:09:52 GMT  (169kb,D)", "http://arxiv.org/abs/1305.2532v1", "13 pages. To appear in proceedings of the International Conference on Machine Learning (ICML), 2013"]], "COMMENTS": "13 pages. To appear in proceedings of the International Conference on Machine Learning (ICML), 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["st\u00e9phane ross", "jiaji zhou", "yisong yue", "debadeepta dey", "drew bagnell"], "accepted": true, "id": "1305.2532"}, "pdf": {"name": "1305.2532.pdf", "metadata": {"source": "META", "title": "Learning Policies for Contextual Submodular Prediction", "authors": ["Stephane Ross", "Jiaji Zhou", "Yisong Yue", "Andrew Bagnell"], "emails": ["stephaneross@cmu.edu", "jiajiz@andrew.cmu.edu", "yisongyue@cmu.edu", "debadeep@cs.cmu.edu", "dbagnell@ri.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are not a mere formation, but a group of people who are able to move, who are able to move, and who are able to move."}, {"heading": "2. Related Work", "text": "The problem of learning to optimize submodal reward functions from data, both with and without contextual tasks, is becoming increasingly important in machine learning due to its diverse application areas. Generally, there are two main approaches to this setting: the first approach aims to identify a model within a parametric family of submodular functions and then use the resulting model to make new predictions; the second attempt to directly predict a list of elements by transforming the overall problem into several simpler learning processes is 2008; Yue & Guestrin, 2011; Lin & Bilmes, 2012; Raman et al."}, {"heading": "3. Background", "text": "Let S specify the amount of possible items from which we can choose (e.g. ads, sentences, grasping). Our goal is to select a list of items that L S obeys to maximize a reward function f, which has the following properties: 31. Monotonicity: For all items L1, L2, f (L1) \u2264 f (L1, L2) and f (L2) \u2264 f (L1, L2) 2. Submodularity: For all lists L1, L2 and items s, s, S, f (L1, L2) \u2265 f (L1, L2) \u2212 f (L1, L2). Here, it denotes the chain operator. Intuitively, monotonicity implies that adding more items never hurts, and submodularity the notion of a decreased return (i.e. adding an item to a long list increases the target rather than adding less to a shorter list)."}, {"heading": "3.1. Learning Problem", "text": "Our task is to effectively develop good lists of the given length k under an unknown distribution of states k =. (e.g. distribution of users or documents we must summarize) We consider two cases: context-free and contextual.context-free. (In context-free case we have no secondary information about the current state (i.e. we do not observe anything about x). We quantify the performance of each list L by its expected value: F (L) = Ex \u0445 D [fx (L)].Note: F (L) is also monotonously submodular. (So the klairvoyant greedy list with perfect knowledge of D can find a list L, so that F (L). (1 / e) F (L).k), were L."}, {"heading": "4. Context-free List Optimization", "text": "nlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr in red eeisrrrrrrrteeVnlrrrrrrrrrrrrrr in the eeisrrrrrrrrrrrrr in the eeisrrrrrrrrteeeVnlrrrrrrrrrrrrrrrrrr in the eeisrrrrrrrrrrr in the eeisrrrrrrrrrrrrrrteeeeeeVnlrrrrrrrrrrrrrrrrrrrrrteeeeeeeeeeVnlrrrrrrrrrrrrrrrrrrrrrrrteeeeeeeeeeVnlrrrrrrrrrrrrrrrrrrrrrteeeeeeeeeeVnllrrrrrrrrrrrrrrrrrrrrrrrrrrteeeeeeVnlrrrrrrrrrrrrrrrrrrrrrrteeeVnlrrrrrrrrrrrrrrteeeeVnlrrrrrrrrrrrrrrrteeeeVnlrrrrrrrrrrrrrrrrrteeVnlrrrrrrrrrrrteeeeeVnlrrrrrrrrrrrrrrrrrrrrrteeeeeeVnlrrrrrrrrrrrrrrr"}, {"heading": "4.1. Theoretical Guarantees", "text": "This leads to a surprising fact: It is possible to construct a list from a stationary distribution, which achieves the same guarantee as the clairvoyant greedy algorithms. 86We also consider a similar algorithm conversion in min-sum coverage, where the theory requires a higher benefit than the real algorithm conversion. 86We are considering a similar algorithm conversion."}, {"heading": "5. Contextual List Optimization with Stationary Policies", "text": "We will now consider the contextual setting in which the characteristics of each state xt are observed before selecting the list. As already mentioned, our goal here is to compete with the best list of strategies (\u03c01, \u03c02,.., \u03c0k) from a hypotheses category. Each of these strategies is assumed to select an item based exclusively on the characteristics of the state x. We will consider the embedding of strategies in a larger list in which strategies are based on both state and partially selected list. (x, L) corresponds to the item that the policy chooses to append to list L of the given state. We will learn a policy or distribution of strategies from which attempts are made to generalize list building across multiple positions. (x.10We will represent an extension of SCP to the contextual set-10Compliting against the best list of strategies in general is difficult, as they may be added to modularize the list better (if they fall into subarity)."}, {"heading": "5.1. No-Regret Cost-Sensitive Classification", "text": "After transforming our problem into a cost-sensitive online classification, we now present approaches that can be used to reach multiple problem areas without regret. (For limited political classes, all cost-sensitive online algorithms can be restored such as Weighted Majority (Kalai & Vempala, 2005). (Weighted Majority, however, retains a distribution of policies based on the loss of \"t\" (\u03c0) of each individual risk and achieves regret at a rate of R = s. \"This is similar to DAgger et al., 2011a; b; Ross & Bagnell, 2012) developed for sequential prediction problems such as imitated learning. (Our work can be seen as specializing in DAgger for submodular optimization.) We make sure that we learn that our strategies are selected for good prediction points."}, {"heading": "5.2. Theoretical Guarantees", "text": "We provide contextual performance guarantees for SCP that relate performance to the submodular list of optimization tasks. (We provide contextual performance guarantees for SCP that relate performance to the submodular list.) (We regret the regret of the corresponding cost-sensitive classification task.) Let's record the loss of individual policies using cost-sensitive classification examples (vti, cti, wti, mi = 1 in algorithm 2 for the state xt. (We use {t} Tt = 1 as a result of losses to the online learning problem.) For a deterrent online algorithm that selects the sequence of policies. (Tt = 1, the regret isR = 1 \"t\")."}, {"heading": "6. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Robotic Manipulation Planning", "text": "We applied SCP to a manipulation planning task for a 7 degree freedom robot manipulator. The goal is to predict a series of initial trajectories to maximize the chance that one of them will lead to a collision-free trajectory. We use local trajectory optimization techniques such as CHOMP (Ratliff et al., 2009), which have proven effective in quickly locating collision-free trajectories using local disturbances of an initial trajectory. Note that selecting a series of initial trajectories is important, as local techniques such as CHOMP often get stuck in local optimization.14We use the dataset from (Dey et al., 2012). It consists of 310 training environments and 212 test environments of random obstacle configurations around a target, and 30 initial trajectories. In each environment, each seed trajectory has 17 features that we assume will describe the gap in the current trajectory, but the 13space characteristics of the scenario will always indicate that in the scenario."}, {"heading": "6.2. Personalized News Recommendation", "text": "We have created a stochastic user simulation based on 75 user preferences derived from a user study in (Yue & Guestrin, 2011). Using this simulation as a training oracle, our goal is to learn to recommend articles to each user (depending on their contextual characteristics) in order to minimize the failure case where the user does not like any of the recommendations. 17 articles are represented by features and user preferences by linear weights. We have derived user contexts by grouping users into groups and using corrupt group memberships as contexts.We perform a quintuple cross-validation: In each fold, we train SCP and ConSeqOpt based on the preferences of 40 users, use 20 users for validation, and then test 15 users at the Heldout. Training, validation and tests are all performed via simulation."}, {"heading": "6.3. Document Summarization", "text": "In the extractive multi-document summary task, the goal is to extract sentences (with character budget B) to maximize coverage of humanly annotated summaries. Following the experimental setup of (Lin & Bilmes, 2010) and (Kulesza & Taskar, 2011), we use the current list w.r.t. Any initial situation, 0 otherwise.16When a successful seed is found, the benefits at later positions are 0. This effectively discards training environments for training classifiers lower in the list in ConSeqOpt.17Also known as task (Radlinski et al., 2008).Data from the Document Understanding Conference (DUC) 2003 and 2004 (Task 2)."}, {"heading": "Acknowledgements", "text": "This research was supported in part by the NSF NRI Purposeful Prediction Project and ONR MURI's Decentralized Reasoning in Reduced Information Spaces and Provably Stable Vision-Based Control. Yisong Yue was also partially supported by ONR (PECASE) N000141010672 and ONR Young Investigator Program N00014-08-1-0752. We thank Martial Hebert for valuable discussions and support, the simplest being the average square distance of tf-idf vectors. Performance was very stable across several characteristics. The experiments presented use three types: 1) following the idea of similarity as a volume metric (Kulesza & Taskar, 2011) we calculate the square volume of parallelopiped, spanned by the TF-IDF vectors of the sets in the set Lt, k-si; 2) the product between det (BMS, k-si) and the quality characteristics; 3) the minimum distance between the quality characteristics of each element and the element."}, {"heading": "A. Proofs of Theoretical Results", "text": "This appendix contains the evidence for the various theoretical results presented in this Paper.A.1. PreliminariesWe begin with the proof of a number of lemmas about monotonous submodular functions that will be useful to prove our main results.Lemma 1. Let S be a set and f be a monotonous submodular function (A) that is defined on the list of items in S. For each list A and B we have that Bi is the list of the first i items in B. We have the: f (A) \u2212 f (A), f (A), f (B), f), f (B), f (A), f (A), f (A), f), f), f (A), b), b), b), b), b (B (B), and bi the ith item in B. We have this: f (A) \u2212 f (A), f (A), f (A), f (B), b), b), b), b) (B)."}, {"heading": "In particular, for \u03b1 = exp(\u2212k/|B|):", "text": "The proof follows a similar proof to the previous problem. Remember that by the monotone property (Eb) [f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 b (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (A) \u2212 f (\u2212 f (A) \u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) \u2212 f (\u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (\u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (\u2212 f (A) \u2212 f) \u2212 f (A) \u2212 f (A) u (A) u (A) u (A) u ("}], "references": [{"title": "The multiplicative weights update method: A meta-algorithm and applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f3", "Freund", "Yoav", "Schapire", "Robert"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Error limiting reductions between classification tasks", "author": ["Beygelzimer", "Alina", "Dani", "Varsha", "Hayes", "Thomas", "Langford", "John", "Zadrozny", "Bianca"], "venue": "In ICML. ACM,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2005}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Overview of duc 2005", "author": ["Dang", "Hoa Trang"], "venue": "In DUC,", "citeRegEx": "Dang and Trang.,? \\Q2005\\E", "shortCiteRegEx": "Dang and Trang.", "year": 2005}, {"title": "Contextual sequence optimization with application to control library optimization", "author": ["Dey", "Debadeepta", "Liu", "Tian Yu", "Hebert", "Martial", "Bagnell", "J. Andrew (Drew"], "venue": "In RSS,", "citeRegEx": "Dey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dey et al\\.", "year": 2012}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V.S. Mirrokni", "J. Vondrak"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Feige et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feige et al\\.", "year": 2011}, {"title": "Multiple choice learning: Learning to produce multiple structured outputs", "author": ["Guzman-Rivera", "Abner", "Batra", "Dhruv", "Kohli", "Pushmeet"], "venue": "In NIPS,", "citeRegEx": "Guzman.Rivera et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guzman.Rivera et al\\.", "year": 2012}, {"title": "A support vector method for multivariate performance measures", "author": ["Joachims", "Thorsten"], "venue": "In ICML. ACM,", "citeRegEx": "Joachims and Thorsten.,? \\Q2005\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2005}, {"title": "Efficient algorithms for online decision problems", "author": ["Kalai", "Adam", "Vempala", "Santosh"], "venue": null, "citeRegEx": "Kalai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2005}, {"title": "Learning determinantal point processes", "author": ["Kulesza", "Alex", "Taskar", "Ben"], "venue": "In UAI,", "citeRegEx": "Kulesza et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "Chin-Yew"], "venue": "In Text Summarization Branches Out: ACL-04 Workshop,", "citeRegEx": "Lin and Chin.Yew.,? \\Q2004\\E", "shortCiteRegEx": "Lin and Chin.Yew.", "year": 2004}, {"title": "A class of submodular functions for document summarization", "author": ["Lin", "Hui", "Bilmes", "Jeff"], "venue": "In ACL-HLT,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Lin", "Hui", "Bilmes", "Jeff"], "venue": "In UAI,", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "The Weighted Majority Algorithm", "author": ["Littlestone", "Nick", "Warmuth", "Manfred"], "venue": "INFORMATION AND COMPUTATION,", "citeRegEx": "Littlestone et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone et al\\.", "year": 1994}, {"title": "Learning diverse rankings with multiarmed bandits", "author": ["Radlinski", "Filip", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "In ICML,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Online learning to diversify from implicit feedback", "author": ["Raman", "Karthik", "Shivaswamy", "Pannaga", "Joachims", "Thorsten"], "venue": "In KDD,", "citeRegEx": "Raman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raman et al\\.", "year": 2012}, {"title": "Chomp: Gradient optimization techniques for efficient motion planning", "author": ["Ratliff", "Nathan", "Zucker", "Matt", "Bagnell", "J. Andrew", "Srinivasa", "Siddhartha"], "venue": null, "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Agnostic system identification for model-based reinforcement learning", "author": ["Ross", "Stephane", "Bagnell", "J. Andrew"], "venue": "In ICML,", "citeRegEx": "Ross et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2012}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "Stephane", "Gordon", "Geoff", "Bagnell", "J. Andrew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["Ross", "Stephane", "Munoz", "Daniel", "Bagnell", "J. Andrew", "Hebert", "Martial"], "venue": "In CVPR,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "An online algorithm for maximizing submodular functions", "author": ["M. Streeter", "D. Golovin"], "venue": "In NIPS,", "citeRegEx": "Streeter and Golovin,? \\Q2008\\E", "shortCiteRegEx": "Streeter and Golovin", "year": 2008}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Streeter", "Matthew", "Golovin", "Daniel"], "venue": "Technical Report CMU-CS-07-171,", "citeRegEx": "Streeter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2007}, {"title": "Online learning of assignments", "author": ["Streeter", "Matthew", "Golovin", "Daniel", "Krause", "Andreas"], "venue": "In NIPS,", "citeRegEx": "Streeter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2009}, {"title": "Linear submodular bandits and their application to diversified retrieval", "author": ["Yue", "Yisong", "Guestrin", "Carlos"], "venue": "In NIPS,", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "Predicting diverse subsets using structural svms", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In ICML,", "citeRegEx": "Yue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "For example, recommending a diverse set of news articles increases the chance that a user would like at least one article (Radlinski et al., 2008).", "startOffset": 122, "endOffset": 146}, {"referenceID": 16, "context": "The first approach (Yue & Joachims, 2008; Yue & Guestrin, 2011; Lin & Bilmes, 2012; Raman et al., 2012) involves identifying the parameterization that best matches the submodular rewards of the training instances.", "startOffset": 19, "endOffset": 103}, {"referenceID": 16, "context": "Furthermore, while good sample complexity results are known, these guarantees only hold under strong realizability assumptions where submodular rewards can be modeled exactly by such linear combinations (Yue & Guestrin, 2011; Raman et al., 2012).", "startOffset": 203, "endOffset": 245}, {"referenceID": 15, "context": "The second, a learning reduction approach, by contrast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter & Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012).", "startOffset": 168, "endOffset": 259}, {"referenceID": 23, "context": "The second, a learning reduction approach, by contrast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter & Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012).", "startOffset": 168, "endOffset": 259}, {"referenceID": 5, "context": "The second, a learning reduction approach, by contrast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter & Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012).", "startOffset": 168, "endOffset": 259}, {"referenceID": 5, "context": "In (Dey et al., 2012), this strategy was extended to the contextual setting by a reduction to cost-sensitive classification.", "startOffset": 3, "endOffset": 21}, {"referenceID": 16, "context": "This generality comes at the expense of being significantly less data-efficient than methods that make realizability assumptions such as (Yue & Guestrin, 2011; Raman et al., 2012), as the existing approach learns a different classifier for each position in the list.", "startOffset": 137, "endOffset": 179}, {"referenceID": 5, "context": "Examples include the notion of \u201cmultiple choice\u201d learning as in (Dey et al., 2012; Guzman-Rivera et al., 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 7, "context": "Examples include the notion of \u201cmultiple choice\u201d learning as in (Dey et al., 2012; Guzman-Rivera et al., 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 15, "context": ", 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al., 2008) where success is measured by", "startOffset": 139, "endOffset": 163}, {"referenceID": 5, "context": "It can be shown that F obeys both monotonicity and submodularity with respect to appending policies (Dey et al., 2012).", "startOffset": 100, "endOffset": 118}, {"referenceID": 1, "context": "Algorithms that meet these requirements include Randomized Weighted Majority (Littlestone & Warmuth, 1994), Follow the Leader (Kalai & Vempala, 2005), EXP3 (Auer et al., 2003), and many others.", "startOffset": 156, "endOffset": 175}, {"referenceID": 1, "context": "EXP3 (Auer et al., 2003)).", "startOffset": 5, "endOffset": 24}, {"referenceID": 5, "context": "Although Algorithm 1 uses only a single instance of an online learner subroutine, it achieves the same performance guarantee as prior work (Streeter & Golovin, 2008; Dey et al., 2012) that employ k separate instances of an online learner.", "startOffset": 139, "endOffset": 183}, {"referenceID": 6, "context": "This fact can also be seen as a special case of a more general result proven in prior related work that analyzed randomized set selection strategies to optimize submodular functions (Feige et al., 2011).", "startOffset": 182, "endOffset": 202}, {"referenceID": 1, "context": "Analogous to the context-free setting, we can also extend to partial feedback settings where f is only partially measurable by using contextual bandit algorithms such as EXP4 (Auer et al., 2003) as the online learner (Update).", "startOffset": 175, "endOffset": 194}, {"referenceID": 2, "context": "We briefly describe two such reductions from (Beygelzimer et al., 2005):", "startOffset": 45, "endOffset": 71}, {"referenceID": 5, "context": "This matches similar guarantees provided in (Dey et al., 2012).", "startOffset": 44, "endOffset": 62}, {"referenceID": 5, "context": "Despite having similar guarantees, we intuitively expect SCP to outperform (Dey et al., 2012) in practice because SCP can use all data to train a single predictor, instead of being split to train k separate ones.", "startOffset": 75, "endOffset": 93}, {"referenceID": 17, "context": "We use local trajectory optimization techniques such as CHOMP (Ratliff et al., 2009), which have proven effective in quickly finding collision-free trajectories using local perturbations of an initial trajectory.", "startOffset": 62, "endOffset": 84}, {"referenceID": 5, "context": "We use the dataset from (Dey et al., 2012).", "startOffset": 24, "endOffset": 42}, {"referenceID": 5, "context": "In addition to the base features, we add features of the Following (Dey et al., 2012), we employ a reduction of cost-sensitive classification to regression as explained in Section 5.", "startOffset": 67, "endOffset": 85}, {"referenceID": 5, "context": "We compare SCP to ConSeqOpt (Dey et al., 2012) (which learns k separate predictors), and Regression (regress success rate from features to sort seeds; this accounts for relevance but not diversity).", "startOffset": 28, "endOffset": 46}, {"referenceID": 15, "context": "Also known as abandonment (Radlinski et al., 2008).", "startOffset": 26, "endOffset": 50}, {"referenceID": 6, "context": "2 in (Feige et al., 2011)).", "startOffset": 5, "endOffset": 25}, {"referenceID": 0, "context": "using directly the benefits as rewards) (Arora et al., 2012), since the rewards at each update are in [0, k\u2032], we have that with the best learning rate in hindsight : E[R] \u2264 2Z \u221a k\u2032 ln |\u03a0\u0303|.", "startOffset": 40, "endOffset": 60}, {"referenceID": 3, "context": "if not a doubling trick can be used to get the same regret bound within a small constant factor (Cesa-Bianchi et al., 1997)", "startOffset": 96, "endOffset": 123}], "year": 2013, "abstractText": "Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on noregret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.", "creator": "LaTeX with hyperref package"}}}