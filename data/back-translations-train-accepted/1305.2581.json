{"id": "1305.2581", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2013", "title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent", "abstract": "Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of \\cite{nesterov2007gradient}.", "histories": [["v1", "Sun, 12 May 2013 12:46:25 GMT  (105kb,D)", "http://arxiv.org/abs/1305.2581v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["shai shalev-shwartz", "tong zhang 0001"], "accepted": true, "id": "1305.2581"}, "pdf": {"name": "1305.2581.pdf", "metadata": {"source": "CRF", "title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent", "authors": ["Shai Shalev-Shwartz", "Tong Zhang"], "emails": [], "sections": [{"heading": null, "text": "Our main contribution is the introduction of an accelerated minibatch version of SDCA and the detection of a fast convergence rate for this method. We discuss an implementation of our method via a parallel computing system and compare the results with both the vanilla stochastic dual coordinate ascendant method and the accelerated deterministic gradient descent method by Nesterov [2007]."}, {"heading": "1 Introduction", "text": "We consider the following generic optimization problem as follows: Let's solve a problem: \"We are a sequence of vector convector functions from Rd to R, and let g: Rd \u2192 R is a strongly convex regulation function.\" Our goal is to solve a dual problem where both methods (x) = [1n n n n \"n\" n \"n\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "2 Main Results", "text": "Our main result is a limitation on the number of iterations required by the ASDCA to find a -precise solution. In our analysis, we consider only the squared Euclidean norm regularization, g (x) = squared Euclidean norm, where the squared Euclidean norm is the Euclidean norm and \u03bb > 0 is a regularizing parameter. Furthermore, we assume that the squared Euclidean norm is smooth in relation to the squared norm (z)."}, {"heading": "It follows that after performing", "text": "In this case, the dominant factor of the limit to t is becomesnm\u03b8 = n m \u00b7 max {1 / 3}. (5) Table 1 summarizes several interesting cases and compares the iteration limit of ASDCA with the iteration limit of 2 / 3} (4) = max {nm, \u221a n / m\u03a9, 1 / mq, n1 / 3 (Higm) 2 / 3}. (5) Table 1 summarizes several interesting cases and compares the iteration limit of ASDCA with the iteration limit of the SDCA algorithm (as analyzed in Shalev-Shwartz and Zhang [2013], with the iteration limit of SDCA."}, {"heading": "3 Parallel Implementation", "text": "In recent years we have had to deal with a variety of problems that affect each other in the way they affect each other in the way they affect each other in the way they affect each other in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way and in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way and in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way they do in the way and in the way they do in the way they do in the way they do in the way they do in the way they do in the way and in the way they do in the way they do in the way they do in the way and in the way they do in the way they do in the way they do in the way and in the way they do in the way they do in the way and in the way they do in the way they do in the way and in the way they do in the way they do in the way and in the way they do in the way they do in the way and in the manner and in the way they do in the way they do in the way they do in the way and in the way they do in the way and in the way they do in the way and in the way they do in the way they do in the way and in the way and in the way they do in the way and in the way they do and in the way they do in the way and in the way and in the way they do in the way and in the way they do in the way and in the way and in the way they do in the way and in the way and in the way they do in the way and in the way and in the way and in the manner and in the way they"}, {"heading": "4 Experimental Results", "text": "In this section we show how ASDCA interpolates between SDCA and AGD. All our experiments are carried out for the task of binary classification with a smooth variant of the hinge loss (see ShalevShwartz and Zhang [2013]). Specifically, a series of marked examples can be shown (v1, y1),. (vm, ym) in which the channels of CA are defined for each i, vi, Rd and yi. (x) to beB (x) = 0 yix > vi > 11 / 2 \u2212 yix > vi < 0 1 2 (1 \u2212 yix > vi) 2 o.w.We also set the control function to g (x) = 2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x \u00b2 x 2, in which we set the regulation parameters of Siix > vi > vi > vi < 0 1 (1 \u2212 yix > vi) 2 < 2 (3).We set the control function to g (x) > ixx xx xx xx xx xx xx xx xx xx xx xx xx x 2, in which we set the regulation parameters of iix > vi > vi > vi > vi > vi \u00b2 ixx \u00b2 ixxx \u00b2 ixxx \u00b2 ixxx \u00b2 ixxx \u00b2 ixx \u00b2 ixxx \u00b2 ixx \u00b2 ixxx \u00b2 ixx \u00b2 ixx \u00b2 xx xx xx xx xx xx xx xx xx xx xx xx xx (x)."}, {"heading": "5 Proof", "text": "We will use the following notation: f (x) \u2212 \u2212 n \u00b2 = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "6 Discussion and Related Work", "text": "We have shown, both theoretically and empirically, that the resulting algorithm interpolates between the vanilla stochastic coordinate descend algorithm and the accelerated gradient descend algorithm. However, the use of mini-batches in stochastic learning has attracted a lot of attention in recent years. E.g. ShalevShwartz et al. [2007] reported experiments showing that the application of small mini-batches in the Stochastic Gradient Descent (SGD) reduced the required number of iterations. Dekel et al. [2012] and Agarwal and Duchi [2012] provided an analysis of SGD with mini-batches for frictionless loss functions. Cotter et al. [2011] investigated SGD and accelerated versions of this SGD with mini-batches and Tak\u00e1c et al. [2013] investigated SDCA with minibatches for SVMs. Duchi al. [2010] investigated dual problem in distributed networks as the average function underlying networks."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u00edk", "John Langford"], "venue": "arXiv preprint arXiv:1110.4198,", "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "Distributed learning, communication complexity and privacy", "author": ["Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour"], "venue": "arXiv preprint arXiv:1204.3514,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Parallel coordinate descent for l1regularized loss minimization", "author": ["Joseph K Bradley", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin"], "venue": "In ICML,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1106.4574,", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "Protocols for learning classifiers on distributed data", "author": ["Hal Daume III", "Jeff M Phillips", "Avishek Saha", "Suresh Venkatasubramanian"], "venue": "arXiv preprint arXiv:1202.6078,", "citeRegEx": "III et al\\.,? \\Q2012\\E", "shortCiteRegEx": "III et al\\.", "year": 2012}, {"title": "Distribution-calibrated hierarchical classification", "author": ["Ofer Dekel"], "venue": "In NIPS,", "citeRegEx": "Dekel.,? \\Q2010\\E", "shortCiteRegEx": "Dekel.", "year": 2010}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Distributed dual averaging in networks", "author": ["John Duchi", "Alekh Agarwal", "Martin J Wainwright"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Strongly-Convex Optimization with Finite Training Sets", "author": ["Nicolas Le Roux", "Mark Schmidt", "Francis Bach"], "venue": "arXiv preprint arXiv:1202.6258,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Algorithms and hardness results for parallel large margin learning", "author": ["Phil Long", "Rocco Servedio"], "venue": "In NIPS,", "citeRegEx": "Long and Servedio.,? \\Q2011\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2011}, {"title": "Graphlab: A new framework for parallel machine learning", "author": ["Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M Hellerstein"], "venue": "arXiv preprint arXiv:1006.4990,", "citeRegEx": "Low et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Low et al\\.", "year": 2010}, {"title": "Distributed graphlab: A framework for machine learning and data mining in the cloud", "author": ["Yucheng Low", "Danny Bickson", "Joseph Gonzalez", "Carlos Guestrin", "Aapo Kyrola", "Joseph M Hellerstein"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Low et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Low et al\\.", "year": 2012}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yurii Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q2007\\E", "shortCiteRegEx": "Nesterov.", "year": 2007}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright"], "venue": "arXiv preprint arXiv:1106.5730,", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "arXiv preprint arXiv:1212.0873,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2012}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In INTERNATIONAL CONFERENCE ON MACHINE LEARNING,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Mini-batch primal and dual methods", "author": ["Martin Tak\u00e1c", "Avleen Bijral", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "mization. Journal of Machine Learning Research,", "citeRegEx": "Tak\u00e1c et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tak\u00e1c et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].", "startOffset": 219, "endOffset": 235}, {"referenceID": 12, "context": "In particular, Nesterov [2007] proposed an accelerated gradient descent (AGD) method for solving (1).", "startOffset": 15, "endOffset": 31}, {"referenceID": 5, "context": ", Dekel et al. [2012]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": ", Dekel et al. [2012]). This is also the application scenario we have in mind, and will be discussed in greater details in Section 3. Recently, Tak\u00e1c et al. [2013] studied mini-batch variants of SDCA in the context of the Support Vector Machine (SVM) problem.", "startOffset": 2, "endOffset": 164}, {"referenceID": 5, "context": ", Dekel et al. [2012]). This is also the application scenario we have in mind, and will be discussed in greater details in Section 3. Recently, Tak\u00e1c et al. [2013] studied mini-batch variants of SDCA in the context of the Support Vector Machine (SVM) problem. They have shown that the naive mini-batching method, in whichm dual variables are optimized in parallel, might actually increase the number of iterations required. They then describe several \u201csafe\u201d mini-batching schemes, and based on the analysis of Shalev-Shwartz and Zhang [2013], have shown several speed-up results.", "startOffset": 2, "endOffset": 542}, {"referenceID": 5, "context": ", Dekel et al. [2012]). This is also the application scenario we have in mind, and will be discussed in greater details in Section 3. Recently, Tak\u00e1c et al. [2013] studied mini-batch variants of SDCA in the context of the Support Vector Machine (SVM) problem. They have shown that the naive mini-batching method, in whichm dual variables are optimized in parallel, might actually increase the number of iterations required. They then describe several \u201csafe\u201d mini-batching schemes, and based on the analysis of Shalev-Shwartz and Zhang [2013], have shown several speed-up results. However, their results are for the non-smooth case and hence they do not obtain linear convergence rate. In addition, the speed-up they obtain requires some spectral properties of the training examples. We take a different approach and employ Nesterov\u2019s acceleration method, which has previously been applied to mini-batch SGD optimization. This paper shows how to achieve acceleration for SDCA in the mini-batch setting. The pseudo code of our Accelerated Mini-Batch SDCA, abbreviated by ASDCA, is presented below. An exception is the recent analysis given in Le Roux et al. [2012] for a variant of SGD.", "startOffset": 2, "endOffset": 1163}, {"referenceID": 12, "context": "Table 1 summarizes several interesting cases, and compares the iteration bound of ASDCA to the iteration bound of the vanilla SDCA algorithm (as analyzed in Shalev-Shwartz and Zhang [2013]) and the Accelerated Gradient Descent (AGD) algorithm of Nesterov [2007]. In the table, we ignore constants and logarithmic factors.", "startOffset": 246, "endOffset": 262}, {"referenceID": 0, "context": "In the calculations below, we use the following facts: \u2022 If each node holds a d-dimensional vector, we can compute the sum of these vectors in timeO(d log(s)) by applying a \u201ctree-structure\u201d summation (see for example the All-Reduce architecture in Agarwal et al. [2011]).", "startOffset": 248, "endOffset": 270}, {"referenceID": 1, "context": "Dekel et al. [2012] and Agarwal and Duchi [2012] gave an analysis of SGD with mini-batches for smooth loss functions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Dekel et al. [2012] and Agarwal and Duchi [2012] gave an analysis of SGD with mini-batches for smooth loss functions.", "startOffset": 0, "endOffset": 49}, {"referenceID": 1, "context": "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Tak\u00e1c et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Tak\u00e1c et al. [2013] studied SDCA with minibatches for SVMs.", "startOffset": 0, "endOffset": 107}, {"referenceID": 1, "context": "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Tak\u00e1c et al. [2013] studied SDCA with minibatches for SVMs. Duchi et al. [2010] studied dual averaging in distributed networks as a function of spectral properties of the underlying graph.", "startOffset": 0, "endOffset": 167}, {"referenceID": 1, "context": "Cotter et al. [2011] studied SGD and accelerated versions of SGD with mini-batches and Tak\u00e1c et al. [2013] studied SDCA with minibatches for SVMs. Duchi et al. [2010] studied dual averaging in distributed networks as a function of spectral properties of the underlying graph. However, all of these methods have a polynomial dependence on 1/ , while we consider the strongly convex and smooth case in which a log(1/ ) rate is achievable.2 It is interesting to note that most3 of these papers focus on mini-batches as the method of choice for distributing SGD or SDCA, while ignoring the option to divide the data by features instead of by examples. A possible reason is the cost of opening communication sockets as discussed in Section 3. There are various practical considerations that one should take into account when designing a practical system for distributed optimization. We refer the reader, for example, to Dekel [2010], Low et al.", "startOffset": 0, "endOffset": 929}, {"referenceID": 0, "context": "[2010, 2012], Agarwal et al. [2011], Niu et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 0, "context": "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al.", "startOffset": 14, "endOffset": 55}, {"referenceID": 0, "context": "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al. [2012], Balcan et al.", "startOffset": 14, "endOffset": 162}, {"referenceID": 0, "context": "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al. [2012], Balcan et al. [2012]. See also Long and Servedio [2011].", "startOffset": 14, "endOffset": 184}, {"referenceID": 0, "context": "[2010, 2012], Agarwal et al. [2011], Niu et al. [2011]. The more general problem of distributed PAC learning has been studied recently in Daume III et al. [2012], Balcan et al. [2012]. See also Long and Servedio [2011]. In particular, they obtain algorithm with O(log(1/ )) communication complexity.", "startOffset": 14, "endOffset": 219}], "year": 2013, "abstractText": "Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the minibatch setting that is often used in practice. Our main contribution is to introduce an accelerated minibatch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of Nesterov [2007].", "creator": "LaTeX with hyperref package"}}}