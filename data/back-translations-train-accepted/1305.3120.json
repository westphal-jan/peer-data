{"id": "1305.3120", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2013", "title": "Optimization with First-Order Surrogate Functions", "abstract": "In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions. First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms. Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.", "histories": [["v1", "Tue, 14 May 2013 11:49:34 GMT  (152kb)", "http://arxiv.org/abs/1305.3120v1", "to appear in the proceedings of ICML 2013; the arxiv paper contains the 9 pages main text followed by 26 pages of supplemental material. International Conference on Machine Learning (ICML 2013) (2013)"]], "COMMENTS": "to appear in the proceedings of ICML 2013; the arxiv paper contains the 9 pages main text followed by 26 pages of supplemental material. International Conference on Machine Learning (ICML 2013) (2013)", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["julien mairal"], "accepted": true, "id": "1305.3120"}, "pdf": {"name": "1305.3120.pdf", "metadata": {"source": "META", "title": "Optimization with First-Order Surrogate Functions", "authors": ["Julien Mairal"], "emails": ["julien.mairal@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 130 5.31 20v1 [st at.M L] 14 May 201 3"}, {"heading": "1. Introduction", "text": "The principle of isolating an objective function is often referred to as majority minimization (Lange et al., 2000), and each iteration pushes the objective function downward, creating the hope of a local optimum; a large number of existing procedures can be interpreted from this standpoint, such as gradient-based or proximal methods (see Nesterov, 2007; Beck & Teboulle, 2009), EM algorithms (see Neal & Hinton, 1998), DC programming (Horst & Thoai, 1999), which mutually reinforce each other (Collins et al., 2002; Pietra et al., 2001), and some varying Bayes techniques (Wainwright & Jordan, 2008; Seeger & Wipf, 2010)."}, {"heading": "2. Basic Optimization Scheme", "text": "Considering a conventional subset of Rp and a continuous function f: Rp \u2022 R, we are interested in studying the Majorization Minimization Schema presented in Algorithm 1 and its variants. This procedure is based on the concept of the replacement functions that are minimized with each iteration instead of f. 2Algorithm 1 Basic Schema-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-0; N (Number of Items). 1: for n = 1,.., N do 2: Compute a Surrogate function-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-Import-"}, {"heading": "Assume that g is in SL,\u03c1(f, \u03ba), then, for all \u03b8 in \u0398,", "text": "Note that this concept differs from the terminology of machine learning, where a \"surrogate\" often refers to a fixed convex upper limit of non-convex (0 \u2212 1) losses. Evidence of this problem is relatively simple, but due to space constraints, all the evidence in this paper is presented as complementary material."}, {"heading": "2.1. Convergence Analysis", "text": "For general non-convex problems that have convergence to a global (or local) minimum, this is out of reach, and classical analyses examine asymptotic stationary point states instead (see e.g. Bertsekas, 1999). To do this, we make the mild assumption that for all subjects a local minimum of f (os) is the directional derivative f (os) of f (os) not negative for all subjects. Naturally, this leads us to consider the following asymptotic condition in order to assess the quality of a sequence (see Borwein & Lewis, 2006) for a local minimum of f (os)."}, {"heading": "2.2. Examples of Surrogate Functions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Lipschitz Gradient Surrogates.", "text": "If f is differentiable, and if f is L-Lipschitz, f admits the following majority substitution in S2L, L (f, B): g: \u03b8 7 \u2192 f (B) + \u0435f (B) (A) + L 2 (B). In addition, g is in SL, L (F), and if f is strongly convex in \u00b5, g is in SL \u2212 \u00b5, L (F, E). Note also that minimizing g amounts to performing a classical downward step."}, {"heading": "Proximal Gradient Surrogates.", "text": "Suppose f splits into f = f1 + f2, where f1 is distinguishable by an L-Lipschitz gradient. Then, f admits the following replacement of the majority in S2L (f, B): g: \u03b8 7 \u2192 f1 (B) + servation f1 (B) (B) + L2 (A). The approximation error g \u2212 f is actually the same as in the previous paragraph and therefore: \u2022 if f1 is convex, g is in SL (F, E). If f2 is also convex, g is in SL, L (F, E). \u2022 if f1 is strongly convex, g is in SL \u2212 \u00b5 (F, E). If f2 is also convex, g is in SL \u2212 \u00b5, L (F, E). Minimizing g amounts to performing a proximal gradient step (see Nesterov, 2007; Beck & Teboulle, 2009)."}, {"heading": "DC Programming Surrogates.", "text": "Suppose f = f1 + f2, where f2 is concave and differentiable with an L2 Lipschitz gradient, then the following function g is a majority substitute in SL2 (f, \u0443): g: \u03b8 7 \u2192 f1 (\u03b8) + f2 (\u043a) + \u0439f2 (\u043a) (\u03b8 \u2212 \u043a), which is the root of DC (difference of convex functions) programming (see Horst & Thoai, 1999), and is also used indirectly in reweighted 1 algorithms (Cande, s et al., 2008) to minimize Rp + a cost function of the form \u03b8 7 \u2192 f1 (\u03b8) + \u03bb, p i = 1 log (successi + \u03b5)."}, {"heading": "Variational Surrogates.", "text": "Let f be a real function defined on Rp1 \u00b7 Rp2. Let 1 Rp1 and 2 Rp2 be two convex sets. Define f \u00b2 as f \u00b2 (\u03b81), min\u03b82 \u00b2 2 f (\u03b81, \u03b82) and assume that \u2022 \u03b81 7 \u2192 f (\u03b81, \u03b82) is distinguishable for all \u03b82 in \u03ba2; \u2022 \u03b82 7 \u2192 \u04451f (\u03b81, \u03b82) for all \u04451 in Rp1 is L-Lipschitz; 3 \u00b7 \u04451 7 \u2192 \u04451f (\u03b81, \u04452) is L \u00b2 -Lipschitz for all \u04452 in \u04452; \u2022 \u04452 7 \u2192 f (\u04451, \u04452) is strongly convex for all \u04451 in Rp1. Let us then fix the following function, namely a majestic substitute in S2L \u00b2 for some L \u00b2 > 0: g: proportional 1 \u2192 f \u00b2 (1)."}, {"heading": "Saddle Point Surrogates.", "text": "Let's make the same assumptions as in the previous paragraph, but with the following differences: 3The notation \"1\" denotes the gradient w.r.t. \"\" 2 \"7\" f \"(\" 1, \"\" 2 \") is for all phenomena\" 1 \";\" 1 \"7\" f \"(\" 1, \"\" 2 \") is for all phenomena\" 2 \";\" f \"(\" 1, \"\" 2 \"),\" 2 \"f\" (\"1,\" \"2\"). Then \"f\" is \"convex\" and the following function is a majestic substitute function in \"S2L\" (f, \"\" 1 \"): g\" 1 \"7\" f \"(\" 1, \"\" 2 \") +\" L \"2\" 1 \"\u2212 1\" 22, where \"L,\" max \"(2L2 /,\" L \"). If\" 1 \"7\" f \"(\" 2 \") is affin affin, we can choose\" L \"2\" instead."}, {"heading": "Jensen Surrogates.", "text": "According to Lange et al. (2000), we consider a convex function f: R 7 \u2192 R, a vector x in Rp, and define f: Rp \u2192 R as f (p), f (x). Let us leave w as a weight vector in Rp + so that f can be distinguished with an L-lipschitz gradient and wi = 0, whenever xi 6 = 0. Then, for any other function in Rpg, we define: 7 \u2192 p = 1wif (xi wi) + x (xi wi), if f is distinguishable with an L-lipschitz gradient, and wi, then g is in SL \"(f) with \u2022 L\" p \"i = 1wif\" (xi wi) + x \"p\" (xi wi), if f \"p\" p \"p\" (xi) + x, \"if f\" p \"x,\" and wi \"xi\" x \"x, then g.\""}, {"heading": "Quadratic Surrogates.", "text": "If f is doubly differentiable and allows a matrix H, so that H \u2212 2f is always clearly positive, the following function is a majority first-order substitute substance: g: \u03b8 7 \u2192 f (\u0443) + \u0445 f (\u043a) (\u03b8 \u2212 \u0443) + 1 2 (\u03b8 \u2212 \u0443) H (\u043a \u2212 \u043a).The Lipschitz constant of A (g \u2212 f) is the largest eigenvalue of H \u2212 2f (\u03b8) over A. Such substitute substitutions often appear in machine learning literature and statistics (Bo \ufffd hning & Lindsay, 1988; Khan et al., 2010).We have shown that there are many rules for forming first-order substitutions. The choice between one and another mainly depends on how easy it is to build the substitute substitute (do we have to estimate a previously unknown Lipschitz constant?), and how cheaply it can be minimized."}, {"heading": "3. Block Coordinate Scheme", "text": "In this section we introduce a block coordinate widening of algorithm 1 under the premise that \u2022 both parts are separable - that is, it can be written like a cartesian product that does not exceed 1. \u2212 \u2212 \u2212 --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >"}, {"heading": "4. Frank-Wolfe Scheme", "text": "In this section, we show how to use surrogates to generalize the Frank Wolfe Method, an old convex optimization technology that has regained some popularity in machine learning (Zhang, 2003; Harchaoui et al., 2013; Hazan & Kale, 2012; Zhang et al., 2012). We place this approach in Algorithm 3.Algorithm 3.Algorithm 3.Algorithm 3.Algorithm 3.Algorithm 4.0; N (Number of iterations). 1: for n = 1,.., N: L2: Line search in SL, L (f, investn \u2212 1). 3: Calculate a search direction.n: \"Search results.n\" Search results.n \"Search results in SL, L (f, investn \u2212 1). 4: Line search:\" Search results in SL, L (f) \u2212 1."}, {"heading": "5. Accelerated Scheme", "text": "A popular scheme for convex optimization is the method of accelerated proximal gradient (Nesterov, 2007; Beck & Teboulle, 2009).By using replacement functions, we use similar ideas in Algorithm 4. Using the \"proximal gradient surrogates\" and using \u00b5 = 0, it corresponds to Beck & Teboulle's FISTA method (2009).Algorithm 4 consists of iterative minimization of a surrogate calculated to a point. \u2212 1 Extrapolation of Vicn \u2212 1 and Victory \u2212 n \u2212 2. It leads to better convergence rates, as shown in the next proposition, by adapting a Nesterov evidence technology (2004).Algorithm 4: Accelerated schema input implication-0; N \u2212 Update (strong convergence parameters); 1: Convergence rate shown in the next proposition."}, {"heading": "6. Incremental Scheme", "text": "These sections are devoted to objective functions f, which could be divided into many components: f (\u03b8) = 1TT = 1f t (\u03b8). (2) Most classic methods that exploit such a structure are able to identify themselves and their variants (see Bottou, 2010). (2) It is that they are able to consider the solution as a problem that we do not consider. (1) Another popular algorithm is stochastic mirroring (see Juditsky & Nemirovski, 2011) for the general non-smooth convex problem, a setting that we do not always admit since non-smooth functions. (Recently, it was shown by Shalev-Schwartz & Zhang (2012). (2012)"}, {"heading": "7. Experiments", "text": "In this section, we show that MISO is efficient in solving major machine learning problems."}, {"heading": "7.1. Experimental Setting", "text": "We consider this optimization problem to be a problem where the regulator is either the 1- or the square 2-norm. The yt's are in {\u2212 1, + 1} and the xt's are vectors in Rp with unit 2-norm. We use four classic data sets described in the following table: name m p memory size (GB) alpha 250 000 500 density 1 rcv1 781 265 47 152 sparse 0.95 covtype 581 012 54 dense 0.11 ocr 2 500 000 1 155 dense 23.1Three data sets, alpha, rcv1 and ocr were compiled by Pascal AG."}, {"heading": "7.2. On Implementing MISO", "text": "The objective function (3) is divided into m-components f: \u03b8 7 \u2192 log (1 + e \u2212 ytxt). (3) Therefore, it is natural to consider the incremental scheme of section 6 together with the proximal gradient surrogates of section 2.2. (4) Specifically, we build a surrogate solution n on the iteration n of MISO. (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5. (5). (5). (5.). (5.). (5. (5). (5.). (5.). (5. (5). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5. (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5. (5.). (5.). (5.). (5.). (5.). (5.). (5.). (5. (5.). (5.). (5.). (5. (5.). (5. (5.). (5.). (5.). (5.). (5. (5.). (5.). (5.). (5. (5. (5.). (5.). (5. (5.). (5.). (5.). (5.). (5. (5.). (5. (5.). (5.). (5. (5.). (5.). (5.). (5"}, {"heading": "7.3. \u21132-Regularized Logistic Regression", "text": "We compare LIBLINEAR, FISTA, SAG, ASGD, SGD, MISO1, MISO2 and MISO2 with T = 1000 blocks (grouping some observations into minibatches).LIBLINEAR was performed with option -s 0 -e 0.000001. Implementation of SAG involves a heuristic line search in the same spirit as MISO2 introduced by Le Roux et al. (2012).Each method was stopped after 50 runs of data. We considered three regimes, high (\u03bb = 10 \u2212 3), medium (\u03bb = 10 \u2212 5) and low (\u03bb = 10 \u2212 7).In Figure 1 we present the values of objective function during the optimization of the medium regime, both in terms of runs over the data and training time. The regimes low and high are given only as complementary material."}, {"heading": "7.4. \u21131-Regularized Logistic Regression", "text": "Since SAG, SGD, and ASGD cannot handle minibatch regulation (MISO2 b1000), we compare LIBLINEAR, FISTA, SHOTGUN, and MISO here. We use option -s 6 -e 0.000001 for LIBLINEAR. We proceed as in Section 7.3, taking into account three regulatory regimes that yield different degrees of rarity. We report the results for one of them in Figure 2 and provide the rest as complementary material. In this experiment, our method outperforms other competitors, except LIBLINEAR for the rcv1 dataset, when high precision is required (and regulation is low)."}, {"heading": "8. Conclusion", "text": "In this paper, we have introduced a flexible optimisation framework based on the calculation of \"surrogate functions.\" We have revised numerous schemes and discovered new ones. We have examined convergence guarantees for non-convexe problems and convergence rates for convexes for each of them. In particular, our methodology led us to design an in-effective transition of data / dataset alphaD ista nce too ptim um0 5 10 15 20 \u2212 25 30 35 40 45 50 10 \u2212 510 \u2212 410 \u2212 410 \u2212 1100 training times (sec) / dataset alphaD ista too ptim 100 101 102 10 \u2212 510 \u2212 410 \u2212 410 \u2212 IB \u2212 410 \u2212 410 \u2212 410 \u2212 410 \u2212 10 \u2212 10 \u2212 10 \u2212 10 10 10 10 \u2212 10 \u2212 10 \u2212 10 \u2212 410 \u2212 410 \u2212 410 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 10 10 10 10 \u2212 10 10 10 10 10 \u2212 10 10 10 10 \u2212 10 10 10 \u2212 10 \u2212 10 10 10 \u2212 10 10 \u2212 10 10 \u2212 10 \u2212 10 \u2212 10 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10 \u2212 10"}, {"heading": "Acknowledgments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Bradley, J.K., Kyrola, A., Bickson, D., and Guestrin, C.", "text": "Parallel coordinate descent for l1-regularized loss minimization. In Proc. ICML, 2011.Cande, E.J., Wakin, M., and Boyd, S.P. Enhancing sparsity by reweighted, 1 minimization. J. Fourier Anal. Appl., 14 (5): 877-905, 2008.Collins, M., Schapire, R.E., and Singer, Y. Logistic regression, AdaBoost and Bregman distances. Mach. Learn., 48 (1): 253-285, 2002.Daubechies, I., Defrise, M., and De Mol, C. An iterative threshold algorithm for linear inverse problems with a sparity constraint. Commun. Pur. Appl. Math., 57 (11): 1413-1457, 2004."}, {"heading": "Della Pietra, S., Della Pietra, V., and Lafferty, J. Duality", "text": "and Auxiliary Functions for Bregman Distances. Technical Report, CMU-CS-01-109, 2001. Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. LIBLINEAR: A library for large linear classification. J. Mach. Learn. Res., 9: 1871-1874, 2008."}, {"heading": "Gasso, G., Rakotomamonjy, A., and Canu, S. Recovering", "text": "IEEE T. Signal Process., 57 (12): 4686-4698, 2009.Harchaoui, Z., Juditsky, A., and Nemirovski, A. Conditional Gradient Algorithms for Standard-Controlled Smooth Convex Optimization. Preprint arXiv: 1302.2325v4, 2013.Hazan, E. and Kale, S. Projection-Free Online Learning. In Proc. ICML, 2012.Horst, R. and Thoai, N.V. DC Programming: Overview. J. Optim. Theory App., 103 (1): 1-43, 1999.Juditsky, A. and Nemirovski, A. First-Order Methods for Nonsmooth Convex Large-Scale Optimization, I: General-Purpose Methods. In Optimization for Machine Learning. MIT Press, 2011.Khan, E., Marlin, B., Bouchard, G. and K. for Mixed Data Variation Factor."}, {"heading": "Lacoste-Julien, S., Jaggi, M., Schmidt, M., and Pletscher,", "text": "P. Block coordinate Frank-Wolfe optimization for structural SVMs. In Proc. ICML, 2013."}, {"heading": "Lange, K., Hunter, D.R., and Yang, I. Optimization", "text": "Transfer using objective substitute functions. J. Comput. Graph. Stat., 9 (1): 1-20, 2000."}, {"heading": "Le Roux, N., Schmidt, M., and Bach, F. A stochastic", "text": "Gradient method with exponential convergence rate for finite training sets. In Adv. NIPS, 2012."}, {"heading": "Lee, D.D. and Seung, H.S. Algorithms for non-negative", "text": "Matrix factorization. In Adv. NIPS, 2001."}, {"heading": "Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online", "text": "Learning for matrix factorization and sparse coding. J. Mach. Learning. Res., 11: 19-60, 2010.Neal, R.M. and Hinton, G.E. A look at the EM algorithm that justifies incremental, sparse, and other variants. Learning in Graphic Models, 89: 355-368, 1998.Nesterov, Y. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004.Nesterov, Y. Gradient Methods for Minimizing Composite Lens Functions. Technical Report, CORE Discussion Paper, 2007.Nesterov, Y. Efficiency of Coordinate Descendancy Methods in Large-Scale Optimization Problems. SIAM J. Optimiz., 22 (2): 341-362, 2012."}, {"heading": "Nesterov, Y. and Polyak, B.T. Cubic regularization of", "text": "Newton Method and its Global Performance. Math. Program., 108 (1): 177-205, 2006.Richta \u0301 rik, P. and Taka \u0301 c, M. Iteration Complexity of Randomized Block Descent Methods to Minimize Composite Function. Math. Program., 2012.Seeger, M.W. and Wipf, D.P. Variational Bayesian inference techniques. IEEE Signal Proc. Mag., 27 (6): 81-91, 2010."}, {"heading": "Shalev-Schwartz, S. and Zhang, T. Proximal stochastic", "text": "Dual Coordinate Ascent. arXiv 1211.2717v1, 2012."}, {"heading": "Shalev-Shwartz, S. and Tewari, A. Stochastic methods for", "text": "\u2022 1 Regular loss minimization. In Proc. ICML, 2009.Tseng, P. and Yun, S. A method for coordinate descend method for nonsmooth separable minimization. Math. Program., 117: 387-423, 2009."}, {"heading": "Wainwright, M.J. and Jordan, M.I. Graphical models,", "text": "Appendix B contains useful mathematical results that are used in the appendix. Appendix I, 2012: Supplementary material optimisation with First-Order Surrogate FunctionsOutline. Appendix I: Supplementary material optimisation with First-Surrogate FunctionsOutline. Appendix I: Supplementary material optimisation with First-Order Surrogate FunctionsOutline. Appendix II: Supplementary material optimisation with First-Surrogate FunctionsOutline. Appendix II: Supplementary material optimisation with First-Order Surrogate FunctionsOutline."}, {"heading": "A. Mathematical Background", "text": "Most of them can be found in the classic textbooks for optimization (e.g. Bertsekas, 1999; Boyd & Vandenberghe, 2004; Borwein & Lewis, 2006; Nocedal & Wright, 2006; Nesterov, 2004). Consider a function f:. \".\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"..\" \"\".. \"\".. \"\" \"..\" \"\".. \"\" \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"\".. \"\".. \"\" \"\".. \"\".. \"\" \"\".. \"\" \"..\". \".\". \"\" \"\". \"\" \"\".. \"\" \"\". \"\" \"\" \"..\" \".\" \"\". \"\" \"\" \".\" \"\""}, {"heading": "B. Useful Mathematical Results", "text": "Let f: Rp \u2192 R be differentiable and L-Lipschitz be continuous. Then, this problem is classic for all \u03b8 (see Nesterov, 2004, Lemma 1.2.3 and its proof). Note that Equation (6) does not imply the gradient of a differentiated function f to be L-Lipschitz continuous. Equivalence applies only in some cases, such as the following Lemma B.2 (relation between quadratic surrogates and Lipschitz constants)."}, {"heading": "Proof.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "First point:", "text": "A proof for the first point can be found in Nesterov (2004, Theorem 2.1.5)."}, {"heading": "Second point:", "text": "s fix the two values in Rp. Since f is doubly differentiable, we have for all imbalances in Rp, f, f, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "Third point:", "text": "Evidence of the third point is more difficult due to the lack of assumptions about the smoothness of the second point. However, then it turns out that the explicit assumption that \"f\" is a continuous Lipschitz function is essentially such that we can show that Eq. (6) provides us with a Lipschitz constant developed by Clarke (1983), which allows us to follow a similar evidence for the two-dimensional Lipschitz function (Clarke, 1983, Proposition 2.6.5), taking advantage of the fact that a Lipschitz function is differentiable almost anywhere in the world (Rademacher theorem), which allows us to follow a similar evidence for the two-dimensional Lipschitz function. More specifically, we have that the \"f\" Lipschitz function is differentiable almost everywhere in the world. Let's call the Hessian matrix function 2f (Rp) at a point where it exists."}, {"heading": "Lemma B.7 (Differentiability of Optimal Value Functions).", "text": "Let us consider a function f as defined in Lemma B.6 and with the same properties. Let us define the optimal value function f (\u03b81), min\u03b82 (const1), f (const1), f (const1), namely if f), is differentiable, and f), if f), and f), and f), if f), and f), namely 1), then L \"-Lipschitz is continuous for all phenomena 2, the function f\" is convex and f, \"and f\", \"Lipschitz is continuous with constant L\"; 2. if f \", 7\" (const1, \"\u04452), is concave for all phenomena 2, the function f\" is concave \"and\" f, \"Lipschitz is continuous with constant L,\" 3. if Lipschitz is continuous with constant L. \""}, {"heading": "Proof of the first point:", "text": "If f is convex in Phenomen1 and Phenomen2 in common, it is easy to prove that f is also convex (Boyd & Vandenberghe, 2004, Section 3.2.5). By explicitly shifting the quantity o upwards in Equation (8), applying the L-Lipschitz continuity of \"1f\" in its first argument and the inequality g (1) \u2264 0, we have 0 \u2264 f (1) \u2212 f (1) \u2212 f (1) \u2212 f (1) \u2212 f (1) \u2212 f (1 \u2212 Phenomen1) \u2264 L \u00b2 2 \u2212 Phenomen1 \u2212 22, we can apply Lemma B.2 to ensure that f-Lipschitz is continuous."}, {"heading": "Proof of the second point:", "text": "\u2212 f is a meaningful superiority of convex functions and is therefore convex (see Boyd & Vandenberghe, 2004, Section 3.2.3).Then we have from Equation (8) and using the concavity of inequality 1 7 \u2192 f (determination 1, determination 2): f (determination 1) \u2212 f (determination 1) + g (determination 1) + g (determination 1) (determination 1 \u2212 determination 1).So 0 \u2264 \u2212 f (determination 1) + f (determination 1) + f (determination 1) (determination 1 \u2212 determination 1) \u2264 | g (determination 1) | \u2264 L2\u00ba determination 1 \u2212 determination 22 where the last inequality in Equation (9) was shown.Then we can apply Lemma B.2 to the convex function \u2212 f and we get the desired Lipschitz constant 2L 2\u00b5."}, {"heading": "Proof of the third point:", "text": "If phenomena 1 7 \u2192 f (determination 1, determination 2) are affine, the difference is 1f (determination 1, determination 2) \u2212 1f (determination 1, determination 2) \u2212 1f (determination 1, determination 2) \u2212 1f (determination 1, determination 2) \u2212 1f (determination 1, determination 2) \u2212 2 \u2264 L (determination 2) \u2212 2 \u2264 L (determination 2) \u2212 2 \u2264 L2\u00b5 (determination 1 \u2212 2) \u2212 2, the upper limit of the gradient of g being shown in the proof of Lemma B.6.Lemma B.8 (Pythagoras relation). Let it happen, in Rp."}, {"heading": "Proof.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of the first point:", "text": "Let us define the right term l as follows: \"l\" (\u03b8) = \"l\": \"l\": \"l\": \"l\": \"l\": \"l\": \"l\": \"l\": \"l\": \"l\": \"l\": \"l\": \"c\": c \"c: c\" c: c \"c: c\" c: c \"c: c\" c \"c: c\" c \"c: c\" c \"c: c\" c \"c: c\" c \"c\" c \"c\" c: c \"c\" c \"c\": c \"c: c\" c: c \"c\" c: c \"c\" c: c \"c\" c: c \"c\" c \": c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \": c\" c \"c\" c \"c\" c \": c\" c \"c\" c. \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"\" c \"c\" c \"\" c \"c\" c \"\" c \"\" c \"c\" \"c\" c \"c\" c \"c\""}, {"heading": "C. Mechanisms to Construct First-Order Surrogate Functions", "text": "We start with a fundamental problem that gives us elementary techniques to combine replacement functions.Lemma C.1 (Combination rules for Majorcan surrogate mothers of first order). Consider two functions f: Rp \u2192 R and f: Rp \u2192 R, and Majorcan substitute functions g in SL (f) and g \"in SL (f). Then the following combination rule applies: \u2022 Linear combination: for all \u03b1, \u03b2 > 0, \u03b1g + \u03b2g\" is a Majorcan substitute function in SL (f) + \u03b2L \"(f); \u2022 Transitivity: Consider g\" a Majorcan substitute function in SL \"(g). Then g is\" a Majorcan substitute function in SL. \""}, {"heading": "Lipschitz Gradient Surrogates.", "text": "If f is differentiable and if f is L-Lipschitz, we consider the following surrogates: g: \u03b8 7 \u2192 f (\u0443) + \u0435\u0441f (\u0443) (\u03b8 \u2212 \u043a) + L 2 \u0445 \u03b8 \u2212 \u0432. Applying Lemma B.1 and examining the error of approximation h, g \u2212 f we immediately get that g is a majority substitute in S2L, L (f, \u0445). If f is convex, we can use Lemma B.9 to prove that g is in SL, L (f, \u0445) and SL \u2212 \u00b5, L (f) if f is \u00b5-strongly convex."}, {"heading": "Proximal Gradient Surrogates.", "text": "Suppose f splits into f = f1 + f2, where f1 is distinguishable by an L-Lipschitz gradient. Then, we have presented the following substitute: \u03b8 7 \u2192 f1 (\u0443) + \u0435\u0441f1 (\u0443) (\u043a \u2212 \u0432) + L2 \u0445 \u03b8 \u2212 \u0432 22 + f2 (\u03b8). According to the same arguments as in the previous paragraph, we have that g is in S2L (f, \u0445). Besides, if f1 is convex, g is in SL (f, \u0445). If f2 is also convex, g is in SL, L (f, \u0445). If f1 is strongly convex, g is in SL \u2212 \u00b5 (f, \u0445). If f2 is also convex, g is in SL \u2212 \u00b5, L (f, \u0445)."}, {"heading": "DC Programming Surrogates.", "text": "Suppose f = f1 + f2, where f2 is concave and differentiable with an L2 Lipschitz gradient. Then, we have presented the following substitute: \u03b8 7 \u2192 f1 (\u03b8) + f2 (\u0443) + \u0430 f2 (\u0443) (\u03b8 \u2212 \u043a). It is easy to see that g is a majority surrogate, since f2 is concave and below its tangents. It is also easy to see that the approximation error g \u2212 f has a continuous L2 Lipschitz gradient."}, {"heading": "Variational Surrogates.", "text": "Let f be a function defined on FP1 \u00b7 FP2. Let's assume that \"1\" FP1 and \"2\" FP2 are two convex sentences. Define f \"as f\" as f \"(1),\" 2 \"f\" (1) and assume that \"1\" 7 \"f\" (1) is distinguishable for all \"2\"; \"2\" 7 \"f\" (1) is \"L-Lipschitz\" for all \"1\" in FP1; \"1\" 7 \"1f\" (2) is \"L\" -Lipschitz \"for all\" 2 \";\" 2 \"7\" f \"(1) is\" strongly convex \"for all\" 1 \"in FP1.\" Let's fix \"1\" in \"1.\" Then we can show that the following function is \"2\" Lipitz, \"\" Lipschitz \"is an orange substitute for\" 2 \"(f)."}, {"heading": "Saddle Point Surrogates.", "text": "Let us make the same assumptions as in the previous paragraph with the following exceptions: \u2022 \u03b82 7 \u2192 f (\u03b81, \u03b82) is strongly concave for all \u03b81 in Rp1 \u00b5; \u2022 \u03b81 7 \u2192 f (\u03b81, \u03b82) is convex for all \u03b82 in \u04212; \u2022 f (\u03b81), max\u03b82, \u04452 f (\u03b81, \u04452).Then f = convex is considered the peak of the convex functions (see Boyd & Vandenberghe, 2004) and we can show that the following function is a majority surrogate in S2L (f = 1, \u04451): g: \u04451 7 \u2192 f (\u04451, \u04452) + L \u00b2 2, where L \u00b2 \u00b2, max (2L2 / \u00b5, L \u00b2)."}, {"heading": "Jensen Surrogates.", "text": "Let us remember the definition of Jensen surrogates. Following Lange et al. (2000), let us consider a convex function f: R 7 \u2192 R, a vector x in Rp, and define f: Rp \u2192 R as f: 0 = 0. Let w be a weight vector in Rp +, so that w: 0, w: 1 = 1, and wi: 0 always xi: 6 = 0. Then let us consider the following function g for each other in Rpg: 7 \u2192 p: i = 1wif (xi wi: 2) + x: 0. Let us suppose that f is distinguishable with an L-Lipschitz gradient and wi: | xi / x: 0 for each other in Rpg: 7 \u2192 p: i = 1wif (xi wi: 1) + x: 0."}, {"heading": "Quadratic Surrogates.", "text": "If f can be differentiated twice and a matrix H permits the number 2f-H to always be unambiguously positive, the following function is a majority surrogate of the first order: g: \u03b8 7 \u2192 f (\u0443) + \u0442 f (\u0443) (\u03b8 \u2212 \u043a) + 1 2 (\u03b8 \u2212 \u043a) H (\u03b8 \u2212 \u043a). The fact that it is a majority number is only an application of the mean theorem."}, {"heading": "D. Additional Optimization Scheme: Block Frank-Wolfe", "text": "In this section, we provide an additional optimization scheme that combines the ideas of sections 4 and 3 with separation assumptions for the surrogates gn and preservation. It leads to a block coordinate version of the Frank Wolfe optimization scheme that is presented in algorithm 6 and generalizes a procedure recently introduced by Lacoste-Julien et al. (2013). (More specifically, the algorithm of Lacoste-Julien et al. (2013) corresponds to the use of a square replacement component provided by Lemma B.1 when f is smooth with L-Lipschitz gradient, and performs a line search for the function f instead of gn. Our approach on the other hand can afford to have a non-smooth component in f, and in this sense is more general. (2013) Lacoste-Julien et al. (2013) also represents a duality gap and various extensions and applications that we do not include in our algorithm."}, {"heading": "E. Proofs of Lemmas and Propositions", "text": "We present in this section the evidence for the various terms and phrases of the essay."}, {"heading": "E.1. Proof of Lemma 2.1", "text": "Proof. The first inequality is a direct application of Lemma B.1 to the function h at the point \u0445, at which we notice that h (\u0443) = 0 and \u0435h (\u0432) = 0. Then, for all \u03b8 in \u0432, we have the second inequality from the first. If g is strongly convex, we can additionally exploit the growth property of the second order of g, which is represented in Lemma B.5, and the third inequality results from the second inequality."}, {"heading": "E.2. Proof of Proposition 2.1", "text": "The proof that (f (\u03b8n) n \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "E.3. Proof of Proposition 2.2", "text": "Proof. We prove separately the two parts of the sentence. Not strongly convexed case: we define hn, gn \u2212 f the approximation error function for iteration n. From Lemma 2.1 (with g = gn \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "E.4. Proof of Proposition 2.3", "text": "Proof. We prove the two parts of the proposition separately."}, {"heading": "Non-strongly convex case:", "text": "By Lemma 2.1 (with g = gn, p = p = p = p = p = p, p = p, p = p, p = p, p = p), we have (with g = p, p = p, p = p, p = p), we have (with g = p, p = p, p = p, p = p, p = p), p (, p), p (, p), p (, p (, p). (11) If we sum up this inequality, we have (f (p, p), p (, p), p (, p), p (, p), p (, p), p (, p), p (, p), p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "E.5. Proof of Proposition 2.4", "text": "Proof. We prove the two parts of the proposition separately."}, {"heading": "Non-strongly convex case:", "text": "Following a similar pattern as in Proposition 2.2 and using Lemma B.3 on the approximation error functions hn instead of Lemma 2.1, we have again the proof of Proposition 2.2, f (270) - f (270) - f (270) - f (1) (1) - f (336) + - 3MR36. Denotation by rn, f (270) \u2212 f and by solving this optimization problem we have \u2022 if rn \u2212 1 MR3 / 2, then \u03b1 = 1 and rn = 1 MR3 / 6; \u2022 otherwise we have the quality (MR3) - 1 (MR3) - 1 (MR3), andrn \u2212 1 (rn \u2212 1 9MR3). \u2212 It follows that we \u2212 n \u2212 1 / 2 \u2212 r \u2212 1 and \u2212 2MR3 / - 1 (MR1) - 1 (MR3) - 1 (MR3) (1) (1) (1) (1) (3) (1) (1) (1) (1)."}, {"heading": "E.6. Proof of Proposition 3.1", "text": "Proof. We proceed in several steps and adapt the convergence evidence of Proposition 2.1 to our new environment."}, {"heading": "Definition of an approximate surrogate g\u0304n:", "text": "We recursively define the order of functions (g \u2212 n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n (n (n) n (n (n (n (n (n) n (n (n (n) n (n (n (n) n (n (n (n (n) n (n (n) n (n (n) n (n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n) n (n (n) n (n) n (n (n) n (n (n) n (n (n) n (n) n) n) n (n (n) n) n) n n (n (n) n) n (n) n n) n n n n n (n n n (n (n) n n (n) n n n) n n n n n (n n n (n n n) n (n n (n n (n) n n) n n) n n n (n n n) n n (n n n n) n n n n) n n n n (n n n n (n n n n n (n n n n (n) n n n n (n n n n) n n) n n (n) n n n n n (n n n) n n n n) n n n n n n (n n n n n (n n n n n) n n) n n n n"}, {"heading": "E.7. Proof of Proposition 3.2", "text": "To show the convergence rates of (E [f (n)) and (E), we need to match the proof of Proposition 2,2 to the stochastic blocking. Let's show the convergence rates of (E) and (P (n) as minimizers of the replacement function gn (n) and (P) as minimizers of the replacement function gn (n). Since the indices (n) and (n) are uniformly randomly recorded, we have the following conditional probabilities: P (n) and P (n) = convergence in \u2212 1). We can then get the following imbalances for all subjects in E (f)."}, {"heading": "E.8. Proof of Proposition 3.3", "text": "The proof that (f (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically) n (empirically"}, {"heading": "E.9. Proof of Proposition 4.1", "text": "The proof: We have of the strong convexity of gn: f (270 \u00b0 C) \u2264 gn (270 \u00b0 C) \u2264 \u03b1 [0.1] (1 \u2212 \u03b1) gn (270 \u00b0 C) + \u03b1gn (270 \u00b0 C) \u2212 L2 \u03b1gn (1 \u2212 \u03b1) \u0445n \u2212 1 \u2212 \u03bdn \u0445 22, whereby \"n\" is defined in algorithm 3. Let us now consider \"n\" such that f (270 \u00b0 C) = f. \"Then we have\" n \"(270 \u00b0 C) = f.\" Then we have \"n\" (270 \u00b0 C) \u2264 \u03b1 [0.1] (1 \u2212 c c c c c c c c c \"c c c\" c c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \"c\" c \""}, {"heading": "E.10. Proof of Proposition 5.1", "text": "Evidence. We follow the evidence techniques introduced by Nesterov (2004) using the so-called \"estimation sequences\" and adapt the evidence from Nesterov (2004, Theorem 2.2.8) more closely to deal with our replacement functions."}, {"heading": "Preliminaries:", "text": "We rely heavily on Lemma 2.1, which we remember here and which we are extending. We define: p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p"}, {"heading": "Initialization of the induction for n = 1:", "text": "Let us first initialize the induction by showing that H1 is true. We notice that A1 = L and \u03b31 = \u03c1 are chosen in such a way that we can define the first and third conditions of H1. The second condition is simply an application of Lemma 2.1 when we notice that Kh0 = Kh0. The last condition is also fulfilled because Kh1 = Kh1 = Kh1 (since \u03b21 = 0 in the algorithm)."}, {"heading": "Induction argument:", "text": "As we have shown that H1 is true, we now proceed from Hn \u2212 1 for n + 2 and show Hn \u2212 n + 1 (1). We define g + 1 (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1). (1. (1). (1). (1). (1. (1). (1). (1. (1). (1). (1. (1). (1). (1). (1. (1). (1). (1. (1). (1). (1). (1. (1). (1). (1. (). (1). (1). (1. (1). (1). (1. (). (1). (1. (1). (1). (1. (1). (1). (1). (1. (1). (1). (1. (1). (1). (1).). (1. (1. (1).). (1). (1.). (1.). (1). (1.). (1. (1). (1.). (1). (1). (1. (1.).). (1.). (1. (1.).). (1. (1.). (1.). (1.).). (1.). (1.).)."}, {"heading": "Obtaining the convergence rate:", "text": "Since Hn applies to all n \u2265 1, we have f (\u03b8n) \u2212 f \u2264 An2 \u0432\u0438\u0441\u0442\u0438\u0441\u04420 \u043222 and must therefore calculate the convergence rate of the sequence An in order to prove the main result. Here, we follow the proof of Nesterov (2004, Lemma 2.2.4). First, let us consider the case \u00b5 = 0. It is easy to show by induction that for all n \u2265 1 An = La2n \u2212 1an (an \u2212 1 + an) = an \u2212 1 an \u2212 1 = an \u2212 1an = a2n \u2212 1 \u2212 a2n \u2212 an (an \u2212 1 + an) = a2n \u2212 1an (an \u2212 1 + an) = an \u2212 1 + an (an \u2212 1 + an), showing the relation a2n = (an) a2n \u2212 1 and the fact that a \u2264 an \u2212 1 \u2212 n \u2212 n has a convergence."}, {"heading": "E.11. Proof of Proposition 6.1", "text": "Proof. The evidence is very similar to that of Proposition 3.1. We proceed in several steps."}, {"heading": "Almost sure convergence of f(\u03b8n):", "text": "(n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n (n) (n) (n) (n (n (n) (n) (n) (n) (n (n (n) (n) (n) (n (n) (n (n) (n (n) (n (n) (n (n (n) (n (n (n) (n) (n (n) (n) (n (n) (n (n) (n) (n (n) (n) (n) (n (n (n (n) (n (n) (n) (n ("}, {"heading": "E.12. Proof of Proposition 6.2", "text": "Proof. The almost certain convergence of f (\u03b8n) was shown in sentence 6.1. We now prove the proposition in several steps and begin with some preliminary steps."}, {"heading": "Preliminaries:", "text": "Using the following conditional probability distribution, let us note that these points are drawn according to the following distribution: P (\u03batn \u2212 1 = \u03b8n \u2212 1 | \u03b8n \u2212 1) = \u03b4 and P (\u03batn \u2212 1 = \u0445tn \u2212 2 | \u0445tn \u2212 1) = 1 \u2212 \u043c, with all t in {1,..., T} and all n \u2265 1, E [ziptn \u2212 \u03batn \u2212 1,. 22] = E [E [ziptn \u2212 1,.] = \u0441\u0442tn \u2212 1,. The other relation we need is an extension of Lemma 2.1 to the incremental setting. For all cases in this part, we can be proportional to this position (investtn \u2212 2,."}, {"heading": "Monotonic decrease of E[f(\u03b8n)]:", "text": "Note that E [gt] nn \u2212 n (p) = E [g] n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (p) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n (f) n) (f) n (f) n) (f) n) (f) n) (f) n (f) n) (f) n (f) n) (f) n (f) n) (f) (f) n), f (f) n), f () (f), f (f) n (), f (), f () n ()."}, {"heading": "E.13. Proof of Proposition D.1", "text": "If we prove that due to the separability of the substitute function gn after a few calculations E [f (\u03b8n) | \u03b8n \u2212 1] \u2264 E [gn (\u03b8n) | \u03b8n \u2212 1] \u2264 (1 \u2212 \u03b4) f (\u03b8n \u2212 1) + \u03b4 min \u03b1 [0,1] gn ((((1 \u2212 \u03b1) \u03b8n \u2212 1)).After the proof for Proposition 4.1 we have E [f (\u03b8n) | \u03b8n \u2212 1] \u2264 (1 \u2212 \u0421n) f (\u041an \u2212 1) + \u03b4 min \u03b1 [0,1] [(1 \u2212 \u03b1) suffn \u2212 1).After the proof for Proposition 4.1 we have E [f (\u03b8n) | \u03b8n \u2212 1] \u2264 (1 \u2212 \u0421rn) f (\u041an \u2212 1) + \u04211) f. If we assume the expectation and definition of rn \u2212 f, we obtain proof that we have the same probability (1 \u2212 \u04211) and the same probability (2.1 \u2212 \u04211 \u2212 \u04211) and vice versa we have the same probability as the proof."}, {"heading": "F. Additional Experimental Results", "text": "Figures 3 and 4 show benchmarks for logistic regressions with a different regularization parameter than Figure 1. Similarly, we present benchmarks for logistic regressions in Figures 5 and 6 with a different splitting level than Figure 2."}, {"heading": "Supplementary References", "text": "Boyd, S. P. and Vandenberghe, L. Convex Optimization. Cambridge University Press, 2004.Clarke, F. H. Optimization and Nonsmooth Analysis. John Wiley, 1983.Danskin, J. M. The Theory of Max Min and its Application to Problems of Weapon Allocation. O \ufffd konometry und Unternehmensforschung, 1967.Nocedal, J. and Wright, S. J. Numerical Optimization. Springer Verlag, 2006. 2nd edition."}], "references": [{"title": "Optimization and Nonsmooth Analysis", "author": ["F.H. Clarke"], "venue": "John Wiley,", "citeRegEx": "Clarke,? \\Q1983\\E", "shortCiteRegEx": "Clarke", "year": 1983}, {"title": "The theory of max-min, and its application to weapons allocation problems", "author": ["J.M. Danskin"], "venue": "O\u0308konometrie und Unternehmensforschung,", "citeRegEx": "Danskin,? \\Q1967\\E", "shortCiteRegEx": "Danskin", "year": 1967}, {"title": "Numerical optimization", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The proof exploits some results from nonsmooth analysis developed by Clarke (1983). We essentially use a mean value theorem for multi-dimensional Lipschitz functions (Clarke, 1983, Proposition 2.", "startOffset": 69, "endOffset": 83}, {"referenceID": 1, "context": "Note that this lemma is a variant of a theorem introduced by Danskin (1967). We first prove the differentiability of f before detailing how to obtain the Lipschitz constants.", "startOffset": 61, "endOffset": 76}], "year": 2013, "abstractText": "In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions. First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or FrankWolfe algorithms. Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.", "creator": "LaTeX with hyperref package"}}}