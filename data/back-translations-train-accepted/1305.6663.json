{"id": "1305.6663", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2013", "title": "Generalized Denoising Auto-Encoders as Generative Models", "abstract": "Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).", "histories": [["v1", "Wed, 29 May 2013 00:25:54 GMT  (690kb,D)", "http://arxiv.org/abs/1305.6663v1", null], ["v2", "Sun, 2 Jun 2013 00:03:48 GMT  (744kb,D)", "http://arxiv.org/abs/1305.6663v2", null], ["v3", "Fri, 7 Jun 2013 16:46:15 GMT  (744kb,D)", "http://arxiv.org/abs/1305.6663v3", null], ["v4", "Mon, 11 Nov 2013 02:27:55 GMT  (784kb,D)", "http://arxiv.org/abs/1305.6663v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "li yao", "guillaume alain", "pascal vincent"], "accepted": true, "id": "1305.6663"}, "pdf": {"name": "1305.6663.pdf", "metadata": {"source": "CRF", "title": "Generalized Denoising Auto-Encoders as Generative Models", "authors": ["Yoshua Bengio", "Li Yao"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It's just a matter of time until it's as far as it's been for the last few years, until it's as far as it's been for the last few years, until it's that far again. (It's just a matter of time until it's that far again.) It's just a matter of time until it's that far again. (It's just a matter of time until it's that far again.) It's just a matter of time until it's that far again. (It's just a matter of time until it's that far again.) Until it's that far again, until it's that far again. \"(It's just a matter of time until it's that far again, until it's that far again.) It's just a matter of time until it's that far again.\" (It's just a matter of time until it's that far again.)"}, {"heading": "2 Generalizing Denoising Auto-Encoders", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Definition and Training", "text": "Let P (X) be the data generating distribution over the observed random variable X. Let C be a predefined corruption process that stochastically maps an X into an X through conditional distribution C (X). The training data for the generalized denoising auto-encoder is a set of pairs (X, X) within a family of distributions indexed by Commerce. The training procedure for the denoising auto-encoder can generally be formulated in such a way that it can predict X by a possibly regulated maximum probability, i.e. the generalization performance that this training criterion provides to minimize isL (\u03b8) = E [logPledge value (X | X)]."}, {"heading": "2.2 Sampling", "text": "We define the following pseudo-Gibbs-Markov chain associated with P\u03b8: Xt-P\u03b8 (X | X-X-t-1) X-t-C (X-Xt) (3), which can be initialized by an arbitrary choice X0. This is the process by which we generate samples Xt according to the implicitly learned model by defining the transition operator T (Xt | Xt-1), which defines a conditional distribution for Xt, independent of t, so that the sequence of Xt forms a homogeneous Markov chain. If the asymptotic marginal distribution of Xt exists, we call this distribution division (X), and we show below that it consistently estimates P (X). Note that the above chain does not represent an orderly Gibbs chain in general, because there is no guarantee that the distribution of the Xt-1 and C (X-X) distributions are unequal to each other, that we are in a relationship with an X that is unique to each other."}, {"heading": "2.3 Consistency", "text": "We only have access to a finite number of training examples, but the empirical distribution approaches the distribution chain that generates the data. To compensate for the finite distribution chain, we generally present a (possibly data-dependent) distribution chain in which we select the regularization coefficient according to the number of training examples. (X) We define the number of training examples n, with which the number of training examples n, 0 as n, 0 as n, 0 as n, 0 as n, 0 as n, 0 as n, 0 as n, 0 as. (i.e.) We convert the generalization error, eq. 1) so that consistent estimators remain consistent. (We define the number of training examples n n.) We define Tn as the transition operator Tn (Xt) =."}, {"heading": "2.4 Locality of the Corruption and Energy Function", "text": "In fact, most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are able to survive on their own. (...)"}, {"heading": "3 Reducing the Spurious Modes with Walkback Training", "text": "In other words, the model would go if we use the generative Markov chain to be a kind of \"negative example,\" which we consider to be \"negative example,\" which we consider to be \"negative example.\" (\"It.\") \"It.\" (\"It.\") \"It.\" (\"It.\") \"It.\" (\"It.\") \"It.\" (\"It.\") \"It.\" (\"It.\") \"It.\" (\"It.\") \"(\" It. \")\" (\"It.\" (\"It.\") \"(\" It. \"\" \"(\" It. \")\" (\"It.\") \"(\" It. \")\" (\"It.\" (\")\" (\"It.\") \"(\" It. \"(\") \"(\" It. \"(\") \"(\" It. \")\" (\"It.\" (\")\" (\"It.\" (\")\" (\"It.\" (\")\" (\"It.\") \"(\" (\") (\" It. \"(\") \"(\") (\"It.\" (\") (\" It. \"(\") \"(\" It. \"(\") (\") (\" It. \"(\") (\"It.\" (\") (\" (\") (\" It. \"(\") (\") (\" (\"It.\" (\").\" (\"It.\" (\") (\") (\"(\" It. \").\" (\"(\") \"(\" (\")\" (\"It.\" (\").\" (\")\" (\"(\" It. \").\" (\"(\") \"(\" It. (\")\" (\"(\")). (\"(It. (\" (\"))) (\" (It. (\"(\" It. \"(\")). (\"It. (\" (\")). (\" (\"It. (\")) (\"(\" (\"It. (\"). (\"))) (\" (\"(\" (\"(It. (\"))) (\"(\" (\"(\" It. (\")). (\" (\")) (\" ("}, {"heading": "4 Experimental Validation", "text": "This also applies to the asymptotic estimation results presented by Alain and Bengio. First, we validate the above theorems in a case where the asymptotic limit (of enough data and sufficient capacities) can be reached, i.e. in a low-dimensional, non-parametric environment in which the learner has sufficient examples and a parameterization that can capture any P (X) X. Figure 2 shows the distribution obtained by the Markov chain when the data comes from a discrete distribution."}, {"heading": "5 Conclusion and Future Work", "text": "We have demonstrated that training a model of denocialization is a way of implicitly estimating the underlying process of data generation, and a simple Markov chain that alternates the sample from the denocializing model and the corruption process converges to this estimator. This provides a means of generating data from any denocializing auto encoder (if corruption is not degenerated, or more precisely, if the above chain converges), and we have empirically confirmed these results, both in a non-parametric environment and with real data. This study has also proposed a variant of the training process called walkback training, which approaches the target distribution more quickly. One of the findings emerging from the theoretical results presented here is that in order to reach the asymptotic limit of fully capturing the data distribution P (X), it may be necessary for the P (X | X) distributions of the model to better represent the multidistributions given by X (X)."}, {"heading": "Acknowledgments", "text": "The authors would like to acknowledge the stimulating discussions with and the help of Aaron Courville, Ian Goodfellow Roland Memisevic and Vincent Dumoulin as well as the support of NSERC, CIFAR (YB is a CIFAR Fellow) and the Canada Research Chairs."}], "references": [{"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["G. Alain", "Y. Bengio"], "venue": "International Conference on Learning Representations (ICLR\u20192013).", "citeRegEx": "Alain and Bengio,? 2013", "shortCiteRegEx": "Alain and Bengio", "year": 2013}, {"title": "Non-local manifold Parzen windows", "author": ["Y. Bengio", "H. Larochelle", "P. Vincent"], "venue": "NIPS\u201905, pages 115\u2013122. MIT Press.", "citeRegEx": "Bengio et al\\.,? 2006a", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Nonlocal estimation of manifold structure", "author": ["Y. Bengio", "M. Monperrus", "H. Larochelle"], "venue": "Neural Computation, 18(10).", "citeRegEx": "Bengio et al\\.,? 2006b", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "ICML\u20192013.", "citeRegEx": "Bengio et al\\.,? 2013a", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI).", "citeRegEx": "Bengio et al\\.,? 2013b", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ICML\u20192012.", "citeRegEx": "Boulanger.Lewandowski et al\\.,? 2012", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["D. Heckerman", "D.M. Chickering", "C. Meek", "R. Rounthwaite", "C. Kadie"], "venue": "Journal of Machine Learning Research, 1, 49\u201375.", "citeRegEx": "Heckerman et al\\.,? 2000", "shortCiteRegEx": "Heckerman et al\\.", "year": 2000}, {"title": "Products of experts", "author": ["G.E. Hinton"], "venue": "ICANN\u20191999.", "citeRegEx": "Hinton,? 1999", "shortCiteRegEx": "Hinton", "year": 1999}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, 18, 1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Estimation of non-normalized statistical models using score matching", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, 6, 695\u2013709.", "citeRegEx": "Hyv\u00e4rinen,? 2005", "shortCiteRegEx": "Hyv\u00e4rinen", "year": 2005}, {"title": "Regularized estimation of image statistics by score matching", "author": ["D. Kingma", "Y. LeCun"], "venue": "J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1126\u20131134.", "citeRegEx": "Kingma and LeCun,? 2010", "shortCiteRegEx": "Kingma and LeCun", "year": 2010}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011), volume 15 of JMLR: W&CP.", "citeRegEx": "Larochelle and Murray,? 2011", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Texture modeling with convolutional spike-and-slab RBMs and deep extensions", "author": ["H. Luo", "P.L. Carrier", "A. Courville", "Y. Bengio"], "venue": "AISTATS\u20192013.", "citeRegEx": "Luo et al\\.,? 2013", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Boureau", "Y.-L.", "Y. LeCun"], "venue": "NIPS\u201907, pages 1185\u20131192, Cambridge, MA. MIT Press.", "citeRegEx": "Ranzato et al\\.,? 2008", "shortCiteRegEx": "Ranzato et al\\.", "year": 2008}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "ICML\u20192011.", "citeRegEx": "Rifai et al\\.,? 2011", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "ICML\u20192012.", "citeRegEx": "Rifai et al\\.,? 2012a", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["S. Rifai", "Y. Bengio", "Y. Dauphin", "P. Vincent"], "venue": "ICML\u20192012.", "citeRegEx": "Rifai et al\\.,? 2012b", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "B. Marlin", "N. de Freitas"], "venue": "In ICML\u20192011. ACM", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "A connection between score matching and denoising autoencoders", "author": ["P. Vincent"], "venue": "Neural Computation, 23(7).", "citeRegEx": "Vincent,? 2011", "shortCiteRegEx": "Vincent", "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "Different variants of auto-encoders and sparse coding have been, along with RBMs, among the most successful building blocks in recent research in deep learning (Bengio et al., 2013b).", "startOffset": 160, "endOffset": 182}, {"referenceID": 13, "context": "Some of the first ideas about the probabilistic interpretation of auto-encoders were proposed by Ranzato et al. (2008): they were viewed as approximating an energy function through the reconstruction error, i.", "startOffset": 97, "endOffset": 119}, {"referenceID": 13, "context": "Some of the first ideas about the probabilistic interpretation of auto-encoders were proposed by Ranzato et al. (2008): they were viewed as approximating an energy function through the reconstruction error, i.e., being trained to have low reconstruction error at the training examples and high reconstruction error elsewhere (through the regularizer, e.g., sparsity or otherwise, which prevents the auto-encoder from learning the identity function). An important breakthrough then came, yielding a first formal probabilistic interpretation of regularized auto-encoders as models of the input distribution, with the work of Vincent (2011). This work showed that some denoising auto-encoders correspond to a Gaussian RBM and that minimizing the denoising reconstruction error (as a squared error) estimates the energy function through a regularized form of score matching, with the regularization disappearing as the amount of corruption noise goes to 0, and then converging to the same", "startOffset": 97, "endOffset": 638}, {"referenceID": 9, "context": "solution as score matching (Hyv\u00e4rinen, 2005).", "startOffset": 27, "endOffset": 44}, {"referenceID": 10, "context": "This connection and its generalization to other energy functions, giving rise to the general denoising score matching training criterion, is discussed in several papers (Kingma and LeCun, 2010; Swersky et al., 2011; Alain and Bengio, 2013).", "startOffset": 169, "endOffset": 239}, {"referenceID": 17, "context": "This connection and its generalization to other energy functions, giving rise to the general denoising score matching training criterion, is discussed in several papers (Kingma and LeCun, 2010; Swersky et al., 2011; Alain and Bengio, 2013).", "startOffset": 169, "endOffset": 239}, {"referenceID": 0, "context": "This connection and its generalization to other energy functions, giving rise to the general denoising score matching training criterion, is discussed in several papers (Kingma and LeCun, 2010; Swersky et al., 2011; Alain and Bengio, 2013).", "startOffset": 169, "endOffset": 239}, {"referenceID": 15, "context": "Another breakthrough has been the development of an empirically successful sampling algorithm for contractive auto-encoders (Rifai et al., 2012a), which basically involves composing encoding, decoding, and noise addition steps.", "startOffset": 124, "endOffset": 145}, {"referenceID": 0, "context": "The last step in this development (Alain and Bengio, 2013) generalized the result from Vincent (2011) by showing that when a denoising auto-encoder (or a contractive auto-encoder with the contraction on the whole encode/decode reconstruction function) is trained with small Gaussian corruption and squared error loss, it estimates the score (derivative of the log-density) of the underlying data-generating distribution, which is proportional to the difference between reconstruction and input.", "startOffset": 34, "endOffset": 58}, {"referenceID": 0, "context": "The last step in this development (Alain and Bengio, 2013) generalized the result from Vincent (2011) by showing that when a denoising auto-encoder (or a contractive auto-encoder with the contraction on the whole encode/decode reconstruction function) is trained with small Gaussian corruption and squared error loss, it estimates the score (derivative of the log-density) of the underlying data-generating distribution, which is proportional to the difference between reconstruction and input.", "startOffset": 35, "endOffset": 102}, {"referenceID": 7, "context": "We find that we can improve the sampling behavior by using the model itself to define the corruption process, yielding a training procedure that has some surface similarity to the contrastive divergence algorithm (Hinton, 1999; Hinton et al., 2006).", "startOffset": 213, "endOffset": 248}, {"referenceID": 8, "context": "We find that we can improve the sampling behavior by using the model itself to define the corruption process, yielding a training procedure that has some surface similarity to the contrastive divergence algorithm (Hinton, 1999; Hinton et al., 2006).", "startOffset": 213, "endOffset": 248}, {"referenceID": 6, "context": "In that respect, the situation is similar to the sampling procedure for dependency networks (Heckerman et al., 2000), in that the pairs (Xt, X\u0303t\u22121) are not guaranteed to have the same asymptotic distribution as the pairs (Xt, X\u0303t) as t \u2192 \u221e.", "startOffset": 92, "endOffset": 116}, {"referenceID": 1, "context": "This idea is already behind the non-local manifold Parzen windows (Bengio et al., 2006a) and non-local manifold tangent learning (Bengio et al.", "startOffset": 66, "endOffset": 88}, {"referenceID": 2, "context": ", 2006a) and non-local manifold tangent learning (Bengio et al., 2006b) algorithms: the local density around a point X\u0303 can be approximated by a multivariate Gaussian whose covariance matrix has leading eigenvectors that span the local tangent of the manifold near which the data concentrates (if it does).", "startOffset": 49, "endOffset": 71}, {"referenceID": 14, "context": "The idea of a locally Gaussian approximation of a density with a manifold structure is also exploited in the more recent work on the contractive auto-encoder (Rifai et al., 2011) and associated sampling procedures (Rifai et al.", "startOffset": 158, "endOffset": 178}, {"referenceID": 16, "context": ", 2011) and associated sampling procedures (Rifai et al., 2012b).", "startOffset": 43, "endOffset": 64}, {"referenceID": 0, "context": "that idea comes from the result from Alain and Bengio (2013): when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density suffices to fully capture the joint distribution.", "startOffset": 37, "endOffset": 61}, {"referenceID": 7, "context": "The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).", "startOffset": 138, "endOffset": 173}, {"referenceID": 8, "context": "The spirit of this procedure is thus very similar to the CD-k (Contrastive Divergence with k MCMC steps) procedure proposed to train RBMs (Hinton, 1999; Hinton et al., 2006).", "startOffset": 138, "endOffset": 173}, {"referenceID": 0, "context": "This is also true of the asymptotic estimation results presented by Alain and Bengio (2013).", "startOffset": 68, "endOffset": 92}, {"referenceID": 1, "context": "In comparison, Bengio et al. (2013a) report a NLL of 244 for RBMs using the same setting.", "startOffset": 15, "endOffset": 37}, {"referenceID": 8, "context": "Although it is trivial to apply these results to a deep auto-encoder (which can be obtained by stacking shallow ones), it would be interesting to investigate architectures in which the associated sampling procedure is more like that of deep belief networks (Hinton et al., 2006), since it has clearly been shown that sampling from higher level representations yields better mixing and better samples (Bengio et al.", "startOffset": 257, "endOffset": 278}, {"referenceID": 3, "context": ", 2006), since it has clearly been shown that sampling from higher level representations yields better mixing and better samples (Bengio et al., 2013a; Luo et al., 2013).", "startOffset": 129, "endOffset": 169}, {"referenceID": 12, "context": ", 2006), since it has clearly been shown that sampling from higher level representations yields better mixing and better samples (Bengio et al., 2013a; Luo et al., 2013).", "startOffset": 129, "endOffset": 169}], "year": 2017, "abstractText": "Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).", "creator": "LaTeX with hyperref package"}}}