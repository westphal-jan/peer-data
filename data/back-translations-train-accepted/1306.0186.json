{"id": "1306.0186", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2013", "title": "RNADE: The real-valued neural autoregressive density-estimator", "abstract": "We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.", "histories": [["v1", "Sun, 2 Jun 2013 09:37:53 GMT  (315kb,D)", "https://arxiv.org/abs/1306.0186v1", "12 pages, 3 figures, 3 tables, 2 algorithms"], ["v2", "Thu, 9 Jan 2014 11:14:27 GMT  (316kb,D)", "http://arxiv.org/abs/1306.0186v2", "12 pages, 3 figures, 3 tables, 2 algorithms. Merges the published paper and supplementary material into one document"]], "COMMENTS": "12 pages, 3 figures, 3 tables, 2 algorithms", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["benigno uria", "iain murray", "hugo larochelle"], "accepted": true, "id": "1306.0186"}, "pdf": {"name": "1306.0186.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["b.uria@ed.ac.uk", "i.murray@ed.ac.uk", "hugo.larochelle@usherbrooke.ca"], "sections": [{"heading": "1 Introduction", "text": "The number of parameters necessary to describe a real distribution grows exponentially in its dimensionality, so that a structure or regularity must be imposed that is derived from generic models. For example, the real density-density-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives-derivatives"}, {"heading": "2 Background: Autoregressive models", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide f\u00fc die r die r die r die r die r die r die r die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f der f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e the f the f die f the f the f die f the f the f the f die f the f the f die f die f the f the f"}, {"heading": "3 Real-valued neural autoregressive density estimators", "text": "The derivation of the Gaussians instead of the distribution of Bernoulli is the result of a hidden adjustment to the conditions of a Gaussian RBM model. However, we have rejected this approach because the effects of the Gaussian RBM model are well documented [13, 14]: its isotropic noise model does not give competitive estimates. The main characteristic of NADE is the binding of its input-to-hidden weight. The initial layer was \"unbound\" to the RBM to give the model greater flexibility. We take this idea further to represent any one-dimensional conditional distribution with a mixture of Gaussians."}, {"heading": "4 Experiments", "text": "We compared RNADE with mixtures of Gaussian (MoG) and factor analyzers (MFA), which represent surprisingly strong baselines in some tasks [20, 21]. Given the notoriously poor performance of discrete mixtures [4, 5], we limited our experiments to modelling continuous attributes. However, it would be easy to include both discrete and continuous variables in a NADE-like architecture."}, {"heading": "4.1 Low-dimensional data", "text": "In fact, most of them will be able to move to a different world in which they are able to escape than to another world in which they are able to escape."}, {"heading": "4.2 Natural image patches", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) It is not the case that they are able to understand the rules. (...) It is not the case that they are able to understand the rules. (...) \"It is not the case that they are able to understand the rules. (...)\" \"It is the case that they are not able to abide by the rules. (...)\" \"It is the case that\" (...). \"(...)\" (...). \"(...)\" ((...). \"(...)\" ((().) \"(...)\" ((().) \"((().)\" ((().) \"((().)\" ((().) \"(...\") (() (().) (() (())."}, {"heading": "4.3 Speech acoustics", "text": "We also measured the ability of RNADE to model small ranges of speech spectrograms extracted from the TIMIT dataset [29], which contained 11 images from 20 filter banks plus energy, totaling 231 dimensions per data point. This filter bank encoding is common in speech recognition and better for visualization than the more commonly used MFCC features. A good generative language model could be used for example for denosis or speech recognition tasks. We equipped the models with the standard TIMIT training environment and compared RNADE with a MoG by measuring their log probability on the full TIMIT core test dataset. The RNADE model has 1024 reflected linear hidden units and a mix of 20 one-dimensional Gaussian components per output. Given the larger scale of this dataset, hyperparameter decisions were made manually using validation data, the same frequency and training elements for a miniature RNADE."}, {"heading": "5 Discussion", "text": "This year, it has come to the point where it will be able to put itself at the top, and in the way in which it is able to put itself at the top."}, {"heading": "Acknowledgments", "text": "We thank John Bridle, Steve Renals, Amos Storkey and Daniel Zoran for their useful interactions. A Python implementation of RNADE is available at http: / / www.benignouria.com / en / research / RNADE.Algorithm 1 \u00b7 \u00b7 \u00b7 Calculation of p (x) a (x) a (x) a) a (from 1 to D) 1 (from 1 to D) 1 (from 1 to D) Conversion factors hd (from 1 to 2). Conversion Factors hd."}], "references": [{"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Estimation of a multivariate density", "author": ["T. Cacoullos"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1966}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "In Proceedings of the 25th International Conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Journal of Machine Learning Research W&CP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": "Technical Report NCRG 4288,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Does the wake-sleep algorithm produce good density estimators", "author": ["B.J. Frey", "G.E. Hinton", "P. Dayan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Y. Bengio", "S. Bengio"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Discussion of the neural autoregressive distribution estimator", "author": ["Y. Bengio"], "venue": "Journal of Machine Learning Research W&CP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Mixtures of conditional Gaussian scale mixtures applied to multiscale image representations", "author": ["L. Theis", "R. Hosseini", "M. Bethge"], "venue": "PLoS ONE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Gaussian process networks", "author": ["N. Friedman", "I. Nachman"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["I. Murray", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "In all likelihood, deep belief is not enough", "author": ["L. Theis", "S. Gerwinn", "F. Sinz", "M. Bethge"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "author": ["M.A. Ranzato", "G.E. Hinton"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A spike and slab restricted Boltzmann machine", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, W&CP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neuro-computing: algorithms, architectures and applications, pages 227\u2013236", "author": ["J.S. Bridle"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "SHORTEN: simple lossless and near-lossless waveform compression", "author": ["T. Robinson"], "venue": "Technical Report CUED/F-INFENG/TR.156,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Deep mixtures of factor analysers", "author": ["Y. Tang", "R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Natural images, Gaussian mixtures and dead leaves", "author": ["D. Zoran", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Mixed cumulative distribution networks", "author": ["R. Silva", "C. Blundell", "Y.W. Teh"], "venue": "Journal of Machine Learning Research W&CP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "The EM algorithm for mixtures of factor analyzers", "author": ["Z. Ghahramani", "G.E. Hinton"], "venue": "Technical Report CRG-TR-96-1, University of Toronto,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "Mixture of factor analyzers", "author": ["J. Verbeek"], "venue": "Matlab implementation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Graphical models for machine learning and digital communication", "author": ["B. Frey"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Timit acoustic-phonetic continuous speech corpus", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren", "V. Zue"], "venue": "Linguistic Data Consortium,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1993}, {"title": "No More Pesky Learning Rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "In Proceedings of the 30th international conference on Machine learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R. Adams"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Arxiv preprint arXiv:1207.0580,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "It has long been appreciated that large classes of densities can be estimated consistently by kernel density estimation [2], and a large mixture of Gaussians can closely represent any density.", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "In practice, a parametric mixture of Gaussians seems to fit the distribution over patches of pixels and obtains state-of-the-art restorations [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "Restricted Boltzmann Machines (RBMs), which are undirected graphical models, fit samples of binary vectors from a range of sources better than mixture models [4, 5].", "startOffset": 158, "endOffset": 164}, {"referenceID": 4, "context": "Restricted Boltzmann Machines (RBMs), which are undirected graphical models, fit samples of binary vectors from a range of sources better than mixture models [4, 5].", "startOffset": 158, "endOffset": 164}, {"referenceID": 4, "context": "The Neural Autoregressive Distribution Estimator (NADE) overcomes these difficulties [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "We use the parameter sharing previously introduced by NADE, combined with mixture density networks [6], an existing flexible approach to modeling real-valued distributions with neural networks.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "Both NADE [5] and our RNADE model are based on the chain rule (or product rule), which factorizes any distribution over a vector of variables into a product of terms: p(x) = \u220fD d=1 p(xd | x<d), where x<d denotes all attributes preceding xd in a fixed arbitrary ordering of the attributes.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "For binary data, each conditional distribution can be modeled with logistic regression, which is called a fully visible sigmoid belief network (FVSBN) [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 7, "context": "Neural networks can also be used for each binary prediction task [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "The neural autoregressive distribution estimator (NADE) also uses neural networks for each conditional, but with parameter sharing inspired by a mean-field approximation to Restricted Boltzmann Machines [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 4, "context": "Untying these weights gave better statistical performance on a range of tasks, with negligible extra computational cost [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "NADE has recently been extended to count data [9].", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "The possibility of extending generic neural autoregressive models to continuous data has been mentioned [8, 10], but has not been previously explored to our knowledge.", "startOffset": 104, "endOffset": 111}, {"referenceID": 9, "context": "The possibility of extending generic neural autoregressive models to continuous data has been mentioned [8, 10], but has not been previously explored to our knowledge.", "startOffset": 104, "endOffset": 111}, {"referenceID": 10, "context": "An autoregressive mixture of experts with scale mixture model experts has been developed as part of a sophisticated multi-resolution model specifically for natural images [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 11, "context": "In more general work, Gaussian processes have been used to model the conditional distributions of a fully visible Bayesian network [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "limitations of the Gaussian-RBM are well documented [13, 14]: its isotropic conditional noise model does not give competitive density estimates.", "startOffset": 52, "endOffset": 60}, {"referenceID": 13, "context": "limitations of the Gaussian-RBM are well documented [13, 14]: its isotropic conditional noise model does not give competitive density estimates.", "startOffset": 52, "endOffset": 60}, {"referenceID": 14, "context": "Approximating a more capable RBM model, such as the mean-covariance RBM [15] or the spike-and-slab RBM [16], might be a fruitful future direction.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "Approximating a more capable RBM model, such as the mean-covariance RBM [15] or the spike-and-slab RBM [16], might be a fruitful future direction.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "That is, the outputs are mixture density networks [6], with a shared hidden layer, using the same parameter tying as NADE.", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "As discussed by Bengio [10], as an RNADE (or a NADE) with sigmoidal units progresses across the input dimensions d \u2208 {1 .", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Previous work on neural networks with real-valued outputs has found that rectified linear units can work better than sigmoidal non-linearities [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "The softmax [18] ensures the mixing fractions are positive and sum to one, the exponential ensures the standard deviations are positive.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "We compared RNADE to mixtures of Gaussians (MoG) and factor analyzers (MFA), which are surprisingly strong baselines in some tasks [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 20, "context": "We compared RNADE to mixtures of Gaussians (MoG) and factor analyzers (MFA), which are surprisingly strong baselines in some tasks [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 3, "context": "Given the known poor performance of discrete mixtures [4, 5], we limited our experiments to modeling continuous attributes.", "startOffset": 54, "endOffset": 60}, {"referenceID": 4, "context": "Given the known poor performance of discrete mixtures [4, 5], we limited our experiments to modeling continuous attributes.", "startOffset": 54, "endOffset": 60}, {"referenceID": 21, "context": "We first considered five UCI datasets [22], previously used to study the performance of other density estimators [23, 20].", "startOffset": 113, "endOffset": 121}, {"referenceID": 19, "context": "We first considered five UCI datasets [22], previously used to study the performance of other density estimators [23, 20].", "startOffset": 113, "endOffset": 121}, {"referenceID": 19, "context": "[20], we eliminated discrete-valued attributes and an attribute from every pair with a Pearson correlation coefficient greater than 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The MFA models were trained using the EM algorithm [24, 25], the number of components and factors were crossvalidated.", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": "The MFA models were trained using the EM algorithm [24, 25], the number of components and factors were crossvalidated.", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "Unfortunately we could not reproduce the data-folds used by previous work, however, our improvements are larger than those demonstrated by a deep mixture of factor analyzers over standard MFA [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "Following the recent work of Zoran and Weiss [3], we use 8-by-8-pixel patches of monochrome natural images, obtained from the BSDS300 dataset [26] (Figure 1 gives examples).", "startOffset": 45, "endOffset": 48}, {"referenceID": 24, "context": "Following the recent work of Zoran and Weiss [3], we use 8-by-8-pixel patches of monochrome natural images, obtained from the BSDS300 dataset [26] (Figure 1 gives examples).", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "We then divided by 256, making each pixel take a value in the range [0, 1].", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "In previous experiments, Zoran and Weiss [3] subtracted the mean pixel value from each patch, reducing the dimensionality of the data by one: the value of any pixel could be perfectly predicted as minus the sum of all other pixel values.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "Such a model could obtain arbitrarily high model likelihoods, so unfortunately the likelihoods reported in previous work on this dataset [3, 20] are difficult to interpret.", "startOffset": 137, "endOffset": 144}, {"referenceID": 19, "context": "Such a model could obtain arbitrarily high model likelihoods, so unfortunately the likelihoods reported in previous work on this dataset [3, 20] are difficult to interpret.", "startOffset": 137, "endOffset": 144}, {"referenceID": 20, "context": "Recent work by Zoran and Weiss [21], projects the data on the leading 63 eigenvectors of each component, when measuring the model likelihood [27].", "startOffset": 31, "endOffset": 35}, {"referenceID": 4, "context": "Perhaps surprisingly, but consistent with previous results on NADE [5] and by Frey [28], randomizing the order of the pixels made little difference to these results.", "startOffset": 67, "endOffset": 70}, {"referenceID": 25, "context": "Perhaps surprisingly, but consistent with previous results on NADE [5] and by Frey [28], randomizing the order of the pixels made little difference to these results.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "We also measured the ability of RNADE to model small patches of speech spectrograms, extracted from the TIMIT dataset [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 5, "context": "Mixture Density Networks (MDNs) [6] are a flexible conditional model of probability densities, that can capture skewed, heavy-tailed, and multi-modal distributions.", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "For image patches, a scale mixture can work well [11], and could be explored within our framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Also, methods for choosing hyperparameters more efficiently than grid search have been recently developed [31, 32].", "startOffset": 106, "endOffset": 114}, {"referenceID": 29, "context": "Also, methods for choosing hyperparameters more efficiently than grid search have been recently developed [31, 32].", "startOffset": 106, "endOffset": 114}, {"referenceID": 30, "context": "These, and several other recent improvements in the neural network field, like dropouts [33], should be directly applicable to RNADE, and possibly obtain even better performance than shown in this work.", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "Performance on image patches was close to a recently reported state-of-the-art mixture model [3], and RNADE outperformed mixture models on all other datasets considered.", "startOffset": 93, "endOffset": 96}, {"referenceID": 31, "context": "Theano [34]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "Following [6], we define \u03c6i(xd |x<d) as the density of xd under the i-th component of the conditional:", "startOffset": 10, "endOffset": 13}], "year": 2014, "abstractText": "We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of onedimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradientbased optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.", "creator": "LaTeX with hyperref package"}}}