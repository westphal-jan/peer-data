{"id": "1306.0386", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration", "abstract": "Given a Markov Decision Process (MDP) with $n$ states and $m$ actions per state, we study the number of iterations needed by Policy Iteratio n (PI) algorithms to converge. We consider two variations of PI: Howard's PI that changes all the actions with a positive advantage, and Sim plex-PI that only changes one action with maximal advantage. We show that Howard's PI terminates after at most $ n(m-1) \\lceil \\frac{1}{1-\\gamma}\\log (\\frac{1}{1-\\gamma}) \\rceil $ iterations, improving by a factor $O(\\log n)$ a result by Hansen et al. (2013), while Simplex-PI terminates after at most $ n(m-1) \\lceil \\frac{n}{1-\\gamma} \\log (\\frac{n}{1-\\gamma})\\rceil $ iterations, improving by a factor 2 a result by Ye (2011). We then consider bounds that are independent of the discount factor $\\gamma$. When the MDP is deterministic, we show that Simplex-PI terminates after at most $ 2 n^2 m (m-1) \\lceil 2 (n-1) \\log n \\rceil \\lceil 2 n \\log n \\rceil = O(n^4 m^2 \\log^2 n) $ iterations, improving by a factor $O(n)$ a bound obtained by Post and Ye (2012). We generalize this result to general MDPs under some structural assumptions: given a measure of the maximal transient time $\\tau_t$ and the maximal time $\\tau_r$ to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most $ n^2 m (m-1) (\\lceil \\tau_r \\log (n \\tau_r) \\rceil +\\lceil \\tau_r \\log (n \\tau_t) \\rceil) \\lceil {\\tau_t} \\log (n (\\tau_t+1)) \\rceil = \\tilde O (n^2 \\tau_t \\tau_r m^2) $ iterations. We explain why similar results seem hard to derive for Howard's PI. Finally, under the additional (restrictive) assumption that the MDP is weakly-communicating, we show that Simplex-PI and Howard's PI terminate after at most $n(m-1) (\\lceil \\tau_t \\log n \\tau_t \\rceil + \\lceil \\tau_r \\log n \\tau_r \\rceil) =\\tilde O(nm (\\tau_t+\\tau_r))$ iterations.", "histories": [["v1", "Mon, 3 Jun 2013 12:48:27 GMT  (29kb)", "https://arxiv.org/abs/1306.0386v1", null], ["v2", "Thu, 6 Jun 2013 14:14:54 GMT  (29kb)", "http://arxiv.org/abs/1306.0386v2", "Markov decision processes ; Dynamic Programming ; Analysis of Algorithms"], ["v3", "Mon, 24 Jun 2013 14:09:56 GMT  (30kb)", "http://arxiv.org/abs/1306.0386v3", "Markov decision processes ; Dynamic Programming ; Analysis of Algorithms"], ["v4", "Wed, 10 Feb 2016 09:09:49 GMT  (33kb)", "http://arxiv.org/abs/1306.0386v4", "Markov decision processes, Dynamic Programming, Analysis of Algorithms, Mathematics of Operations Research, INFORMS, 2016"]], "reviews": [], "SUBJECTS": "math.OC cs.AI cs.DM cs.RO", "authors": ["bruno scherrer"], "accepted": true, "id": "1306.0386"}, "pdf": {"name": "1306.0386.pdf", "metadata": {"source": "CRF", "title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration", "authors": ["Bruno Scherrer"], "emails": ["bruno.scherrer@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 130 6.03 86v4 [m. ath. OC] 1 0Fe b20 16 (m 1 \u2212 \u03b3log (1 \u2212 \u03b3)) iterations that improve a result of Hansen et al (2013) by the factor O (log n), while simplex PI ends after at most O (nm 1 \u2212 \u03b3log (1 \u2212 \u03b3) iterations, and a result of Ye (2011) improves by the factor O (log n). Among some structural properties of the MDP, we then consider limits that are independent of the discount factor \u03b3: Interest rates are the same for all states and policies - in terms of the expected time spent in transient states and the inversion of the frequency of visits to recurring states, as the process assumes uniform distribution. In fact, we show that simplex PI ends after most O iterations (n3m2\u03c4t\u03c4t\u03c4t\u03c4t\u03c4r), and the inversion of the frequency of visits to recurring states, as the process is based on uniform distribution."}, {"heading": "1 Introduction", "text": "We are looking at a discrete dynamic system, the state transition phase of which depends on a control in which the state space X is of limited size. If at the state border i = maximum size n is defined, the action is chosen from a series of permissible actions. (In this context, we are looking for a stationary deterministic policy which is a function: X) to the next state j, so that (Ai) 1 \u2264 i \u2264 n form of a partition of A. The action a) is a reward r (i).R where the instantaneous reward function is a stationary deterministic policy which is a function: X \u2192 A that the cards turn states into adequate actions (for all i, i).R where the instantaneous reward function is. (i) We are looking for a stationary deterministic policy which is a function which we convert into adequate states (i)."}, {"heading": "2 Bounds with respect to a fixed discount factor \u03b3 < 1", "text": "A key observation for both algorithms that are central to the results we will discuss is that the sequences they generate satisfy some contraction vectors.3 For each vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "3 Bounds for Simplex-PI that are independent of \u03b3", "text": "In this section, we will describe some limitations that do not depend on \u03b3, but are based on some structural properties of the MDP. (Post and Ye (2013) recently show the following result for deterministic MDP. Theorem 5 (Post and Ye (2013))) However, if the MDP is deterministic, Simplex PI ends at most after O (n3m2 log2 n) iteration.5Note that this is also the case in Corollary 1. Given a deterministic MDP policy, states are either on cycles or on paths induced by \u03c0. The core of the evidence is based on the following lemmas, which overall show that significant progress is made each time a new cycle appears; in other words, significant progress is made. (Post and Ye (2013, Lemma 3.4) If the MDP is deterministic, after O (nlog2m n) iterations, either Simplex cycles or a cycle are completed."}, {"heading": "4 Similar results for Howard\u2019s PI?", "text": "One may then wonder whether similar results can be derived for Howard's PI. Unfortunately, and as briefly mentioned by Post and Ye (2013), the line of analysis developed for Simplex-PI is unlikely to be easily adaptable to Howard's PI, because it can simultaneously influence several actions in such a way that the policy improvement turns out to be small. We can go into more detail about what actually happens in the approaches we have described so far. On the one hand, it is possible to write counterparts to Lemmas 4 and 6 for Howard's PI (see Section 10 for evidence).Lemma 8. If the MDP is deterrent to write either Howard's PI results or a new cycle after most n iterations. Lemma 9."}, {"heading": "5 Contraction property for Howard\u2019s PI (Proof of Lemma 2)", "text": "For each of these, we have adopted and received the maximum standard: \"v\u03c0\" - \"v\u041ak\" - \"v\u0421k\" - \"v\u0421k\" - \"1\" - \"v\u0421k\" - \"v\u0421k\" - 1 \"-\" T\u0421kv\u0421k \"-\" T\u0421kvvvk \"-\" v\u0432\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "6 Contraction property for Simplex-PI (Proof of Lemma 3)", "text": "The proof we provide here is very close to that of Ye (2011). We make it available here for completeness, and also because it is similar to the evidence we will provide for the boundaries that are independent of the CAP. On the one hand, using the term 10 for each k: v\u03c0k + 1 \u2212 v\u03c0k = (I \u2212 \u03b3P\u03c0k + 1) \u2212 1a\u03c0k + 1\u03c0k, {(I \u2212 \u03b3P\u03c0k + 1) \u2212 1 \u2212 I \u2265 0 and a\u03c0k + 1\u03c0k \u2265 0}, which means that we multiply by the vector 1T on the left, i.e. 1 T (v\u03c0k + 1 \u2212 v\u03c0k) \u2265 1 Ta\u03c0k + 1\u03c0k."}, {"heading": "7 A bound for Howard\u2019s PI when \u03b3 < 1 (Proof of Theorem 3)", "text": "Although the overall line or arguments are derived from the results originally given by Ye (2011) and adjusted by Hansen et al. (2013), our proof is somewhat more direct and leads to better results.For each k we have: \u2212 a\u03c0k\u03b5 = (I \u2212 \u03b3P\u03c0k) {v \u2212 v\u03c0k) {v \u0445 \u2212 v\u03c0k. {v \u00b2 \u2212 v\u03c0k \u2265 0 and P\u03c0k \u2265 0} Through the optimism of \u03c0, \u2212 a \u043d\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "8 A bound for Simplex-PI when \u03b3 < 1 (Proof of Theorem 4)", "text": "We have (by definition of Simplex-PI): a\u03c0k + 1\u03c0k (sk) = up, s \u2212 up, s \u2212 up, s \u2212 s \u2212 up, s \u2212 up, s \u2212 down, s \u2212 down, s \u2212 down, s \u2212 down, k \u2212 k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down \u2212 down, k \u2212 down \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down, k \u2212 down"}, {"heading": "9 A general bound for Simplex-PI (Proof of Theorem 6)", "text": "The evidence we provide here is strongly inspired by that for the deterministic case of Post and Ye (2013): the steps (a set of lemmas) are similar. There are mainly two differences. First, our arguments are more direct in the sense that we do not refer to linear programming, but only provide simple linear algebra arguments. Second, it is more general: for each policy we consider the set of transient states (recurring classes) instead of the series of path states (each cycle); it slightly complicates the arguments, with the most complicated extension being the second part of the proof of the upcoming lemma. Consider the vector x\u03c0 = (i \u2212 PT\u03c0) \u2212 11, which provides a discounted measure of the state visions induced by a policy. Starting from the uniform distribution U: The most complicated extension is the second part of the proof of the upcoming lemma."}, {"heading": "9.1 Part 1: Recurrent classes are created often", "text": "Lemma 12: Suppose one moves from politics to politics without creating a recurring class. Let's let the final politics be before either a new recurring class appears or Simplex-PI comes to an end. Then1 T (v\u03c0 \u2020 \u2212 v\u03c0)."}, {"heading": "9.2 Part 2: A new recurrent class implies a significant step towards the optimal value", "text": "We now proceed to the second part of the evidence and begin by proving Lemma 7 (originally page 6) (originally page 19). Lemma 7. If Simplex-PI changes from zipper to zipper, where it is a new, recurring class, we must, on the one hand, deal with the fact that there is a new, recurring class R (necessarily it includes zipper), then we have a new, recurring class R (necessarily it includes zipper), then we have a new, new class R (necessarily it includes zipper), we have zipper (necessarily it includes zipper) \u2212 zipper state as it is (zipper). \u2212 zipper (zipper) zipper (zipper) zipper (necessarily it includes zipper) zipper, then we have a new, new class R (necessarily it includes zipper) zipper (zipper) zipper (zipper) zipper (zipper) zipper) zipper (zipper) zipper (zipper) zipper) zipper (zipper) zipper) zipper) zipper (zipper) zipper) zipper) zipper) zipper (zipper) zipper) zipper) zipper (zipper) zipper) zipper) zipper) zipper) zipper) zipper (zipper) zipper) zipper) zipper) zipper) zipper), we have then zipper) zipper) zipper) zipper) zipper (zipper) zipper (zipper) zipper) zipper), we have a new, zipper) zipper) zipper) zipper (zipper) zipper) zipper) zipper) zipper) zipper) zipper), we have a new, zipper) zipper (zipper) zipper) zipper), zipper) zipper) zipper), zipper), zipper), zipper) zipper), zipper), zipper), zipper), zipper), zipper), zipper), zipper) we have a new zipper), zipper), zipper (zipper), zipper), zipper), zipper), zipper), zipper)."}, {"heading": "10 Cycle and recurrent classes creations for Howard\u2019s PI (Proofs", "text": "If the MDP is deterministic, then, after at most n iterations, either Howard's PI ends or a new cycle appears. Proof. Consider a sequence of l-generated policies \u03c01, \u00b7 \u00b7 \u00b7, \u03c0l from an initial policy \u03c00, so that no new cycle appears. By induction, we have a sequence of l-generated policies \u03c01, \u00b7 \u00b7 \u00b7 \u00b7, \u03c0l from an initial policy \u03c00, so that no new cycle appears."}, {"heading": "11 A bound for Howard\u2019s PI and Simplex-PI under Assump-", "text": "We believe here that state space is divided into two propositions: T is the set of states that are immutable from all points of view, and R is the set of states that are immutable from all points of view. Thus, we will examine the convergence of the two algorithms in two steps: we will first tie the number of iterations to R, and then add the number of iterations for T, since convergence has occurred to R. We will increase the number of iterations to R; we will then add the number of iterations to T, since all states have entered R."}], "references": [{"title": "Policy iteration for perfect information stochastic mean payoff games", "author": ["M. Akian", "S. Gaubert"], "venue": null, "citeRegEx": "Akian and Gaubert,? \\Q2013\\E", "shortCiteRegEx": "Akian and Gaubert", "year": 2013}, {"title": "Neurodynamic Programming", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific. Fearnley, J", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Worst-case Analysis of Strategy Iteration and the Simplex Method", "author": ["Berlin", "B. Huppert", "W. Willems"], "venue": "Ph.D. thesis,", "citeRegEx": "Berlin et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Berlin et al\\.", "year": 1979}, {"title": "turn-based stochastic games with a constant discount factor", "author": ["R. Hollanders", "J. Delvenne", "R. Jungers"], "venue": "J. ACM ,", "citeRegEx": "Hollanders et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hollanders et al\\.", "year": 2012}, {"title": "Improved bound on the worst case", "author": ["R. Hollanders", "B. Gerencs\u00e9r", "J. Delvenne", "R. Jungers"], "venue": "IEEE conference on Decision and control", "citeRegEx": "Hollanders et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hollanders et al\\.", "year": 2014}, {"title": "On the complexity of policy iteration", "author": ["Y. Mansour", "S. Singh"], "venue": "In UAI ,", "citeRegEx": "Mansour and Singh,? \\Q1999\\E", "shortCiteRegEx": "Mansour and Singh", "year": 1999}, {"title": "The simplex method is strongly polynomial for deterministic Markov decision", "author": ["Y. Ye"], "venue": "Markov decision processes. INFORMS Journal on Computing,", "citeRegEx": "I. and Ye,? \\Q2013\\E", "shortCiteRegEx": "I. and Ye", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "The tuple \u3008X, (Ai)i\u2208X , p, r, \u03b3\u3009 is called a Markov Decision Process (MDP) (Puterman, 1994; Bertsekas and Tsitsiklis, 1996), and the associated problem is known as stochastic optimal control.", "startOffset": 75, "endOffset": 123}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting.", "startOffset": 306, "endOffset": 331}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting. For Simplex-PI, however, the line of analysis is slightly trickier: it amounts to bound the improvement in value at individual states and requires a bit of bookkeeping; the technique we use is to our knowledge original. The bound for Simplex-PI is a factor O(n) larger than that for Howard\u2019s PI. However, since one changes only one action per iteration, each iteration has a complexity that is in a worst-case sense lower by a factor n: the update of the value can be done in time O(n) through the Sherman-Morrisson formula, though in general each iteration of Howard\u2019s PI, which amounts to compute the value of some policy that may be arbitrarily different from the previous policy, may require O(n) time. Thus, it is remarkable that both algorithms seem to have a similar complexity. The linear dependency of the bound for Howard\u2019s PI with respect to m is optimal (Hansen, 2012, Chapter 6.4). The linear dependency with respect to n or m (separately) is easy to prove for Simplex-PI; we conjecture that Simplex-PI\u2019s complexity is proportional to nm, and thus that our bound is tight for a fixed discount factor. The dependency with respect to the term 1 1\u2212\u03b3 may be improved, but removing it is impossible for Howard\u2019s PI and very unlikely for Simplex-PI. Fearnley (2010) describes an MDP for which Howard\u2019s PI requires an exponential (in n) number of iterations for \u03b3 = 1 and Hollanders et al.", "startOffset": 306, "endOffset": 1640}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting. For Simplex-PI, however, the line of analysis is slightly trickier: it amounts to bound the improvement in value at individual states and requires a bit of bookkeeping; the technique we use is to our knowledge original. The bound for Simplex-PI is a factor O(n) larger than that for Howard\u2019s PI. However, since one changes only one action per iteration, each iteration has a complexity that is in a worst-case sense lower by a factor n: the update of the value can be done in time O(n) through the Sherman-Morrisson formula, though in general each iteration of Howard\u2019s PI, which amounts to compute the value of some policy that may be arbitrarily different from the previous policy, may require O(n) time. Thus, it is remarkable that both algorithms seem to have a similar complexity. The linear dependency of the bound for Howard\u2019s PI with respect to m is optimal (Hansen, 2012, Chapter 6.4). The linear dependency with respect to n or m (separately) is easy to prove for Simplex-PI; we conjecture that Simplex-PI\u2019s complexity is proportional to nm, and thus that our bound is tight for a fixed discount factor. The dependency with respect to the term 1 1\u2212\u03b3 may be improved, but removing it is impossible for Howard\u2019s PI and very unlikely for Simplex-PI. Fearnley (2010) describes an MDP for which Howard\u2019s PI requires an exponential (in n) number of iterations for \u03b3 = 1 and Hollanders et al. (2012) argued that this holds also when \u03b3 is in the vicinity of 1.", "startOffset": 306, "endOffset": 1770}, {"referenceID": 0, "context": "For Howard\u2019s PI, the resulting arguments constitute a rather simple extension\u2014the overall line of analysis ends up being very simple, and we consequently believe that it could be part of an elementary course on Policy Iteration; note that a similar improvement and analysis was discovered independently by Akian and Gaubert (2013) in a slightly more general setting. For Simplex-PI, however, the line of analysis is slightly trickier: it amounts to bound the improvement in value at individual states and requires a bit of bookkeeping; the technique we use is to our knowledge original. The bound for Simplex-PI is a factor O(n) larger than that for Howard\u2019s PI. However, since one changes only one action per iteration, each iteration has a complexity that is in a worst-case sense lower by a factor n: the update of the value can be done in time O(n) through the Sherman-Morrisson formula, though in general each iteration of Howard\u2019s PI, which amounts to compute the value of some policy that may be arbitrarily different from the previous policy, may require O(n) time. Thus, it is remarkable that both algorithms seem to have a similar complexity. The linear dependency of the bound for Howard\u2019s PI with respect to m is optimal (Hansen, 2012, Chapter 6.4). The linear dependency with respect to n or m (separately) is easy to prove for Simplex-PI; we conjecture that Simplex-PI\u2019s complexity is proportional to nm, and thus that our bound is tight for a fixed discount factor. The dependency with respect to the term 1 1\u2212\u03b3 may be improved, but removing it is impossible for Howard\u2019s PI and very unlikely for Simplex-PI. Fearnley (2010) describes an MDP for which Howard\u2019s PI requires an exponential (in n) number of iterations for \u03b3 = 1 and Hollanders et al. (2012) argued that this holds also when \u03b3 is in the vicinity of 1. Though a similar result does not seem to exist for Simplex-PI in the literature, Melekopoglou and Condon (1994) consider four variations of PI that all switch one action per iteration, and show through specifically designed MDPs that they may require an exponential (in n) number of iterations when \u03b3 = 1.", "startOffset": 306, "endOffset": 1942}, {"referenceID": 3, "context": "In the simplest\u2014 deterministic\u2014case, the complexity is still an open problem: the currently best-known lower bound is O(n) (Hansen and Zwick, 2010), while the best known upper bound is O( n n ) Mansour and Singh (1999); Hollanders et al.", "startOffset": 194, "endOffset": 219}, {"referenceID": 3, "context": "In the simplest\u2014 deterministic\u2014case, the complexity is still an open problem: the currently best-known lower bound is O(n) (Hansen and Zwick, 2010), while the best known upper bound is O( n n ) Mansour and Singh (1999); Hollanders et al. (2014). On the positive side, an adaptation of the line of proof we have considered so far can be carried out under the following assumption.", "startOffset": 220, "endOffset": 245}], "year": 2016, "abstractText": "Given a Markov Decision Process (MDP) with n states and a total number m of actions, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal \u03b3-discounted policy. We consider two variations of PI: Howard\u2019s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard\u2019s PI terminates after at most O ( m 1\u2212\u03b3 log ( 1 1\u2212\u03b3 )) iterations, improving by a factor O(log n) a result by Hansen et al (2013), while Simplex-PI terminates after at most O ( nm 1\u2212\u03b3 log ( 1 1\u2212\u03b3 )) iterations, improving by a factor O(log n) a result by Ye (2011). Under some structural properties of the MDP, we then consider bounds that are independent of the discount factor \u03b3: quantities of interest are bounds \u03c4t and \u03c4r\u2014uniform on all states and policies\u2014respectively on the expected time spent in transient states and the inverse of the frequency of visits in recurrent states given that the process starts from the uniform distribution. Indeed, we show that Simplex-PI terminates after at most \u00d5 ( nm\u03c4t\u03c4r ) iterations. This extends a recent result for deterministic MDPs by Post & Ye (2013), in which \u03c4t \u2264 1 and \u03c4r \u2264 n; in particular it shows that Simplex-PI is strongly polynomial for a much larger class of MDPs. We explain why similar results seem hard to derive for Howard\u2019s PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that both Howard\u2019s PI and Simplex-PI terminate after at most \u00d5(m(n\u03c4t + n\u03c4r)) iterations.", "creator": "LaTeX with hyperref package"}}}