{"id": "1306.0543", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "Predicting Parameters in Deep Learning", "abstract": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy.", "histories": [["v1", "Mon, 3 Jun 2013 19:16:26 GMT  (324kb,D)", "https://arxiv.org/abs/1306.0543v1", null], ["v2", "Mon, 27 Oct 2014 11:49:08 GMT  (340kb,D)", "http://arxiv.org/abs/1306.0543v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["misha denil", "babak shakibi", "laurent dinh", "marc'aurelio ranzato", "nando de freitas"], "accepted": true, "id": "1306.0543"}, "pdf": {"name": "1306.0543.pdf", "metadata": {"source": "CRF", "title": "Predicting Parameters in Deep Learning", "authors": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "emails": ["misha.denil@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk", "laurent.dinh@umontreal.ca", "ranzato@fb.com"], "sections": [{"heading": "1 Introduction", "text": "It is now possible to train networks with tens of millions [13] or even more than a billion parameters [7, 16], but the largest networks (i.e. those of Dean et al.) are trained with asynchronous techniques that are not far from each other. In this context, many copies of the model parameters are distributed across many machines and updated independently of each other. An additional synchronization mechanism coordinates between machines to ensure that different copies of the same set of parameters are not far from each other. A major drawback of this technique is that training is very inefficient as it makes use of parallel resources. [7] Where the gains from distribution are greatest, the distribution of the model over 81 machines reduces the training time per minibatch by a factor of 12, and the increase to 128 machines reaches a speed factor of approximately 14."}, {"heading": "2 Low rank weight matrices", "text": "Deep networks consist of several layers of transformations of the form h = g (vW), where v is an nv-dimensional input, h is an nh-dimensional output, and W is an nv \u00b7 nh parameter matrix. A column of W contains the weights that bind each unit in the visible layer to a single unit in the hidden layer. We can reduce the number of free parameters by representing W as the product of two matrices W = UV, where U is nv \u00b7 n\u03b1 and V is n\u03b1 \u00b7 nh. By making n\u03b1 much smaller than nv and nh, we achieve a considerable reduction in the number of parameters. In principle, learning factor weight matrices is straightforward. We simply replace W with UV in the objective function and calculate derivatives in relation to U and V instead of W. In practice, this approach does not represent a preform and learning a complete weight matrix is straightforward."}, {"heading": "3 Feature prediction", "text": "To do this, consider the weights associated with a single hidden unit as a function w: W \u2192 R. A simple regression model that is suitable here is a linear combination of basic functions. In this view, the columns of U form a dictionary of basic functions, and the characteristics of the network are linear combinations of these characteristics parameterized by V."}, {"heading": "3.1 Choice of dictionary", "text": "One obvious choice is to train a single, unattended matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix mat"}, {"heading": "3.2 A concrete example", "text": "In this section, we describe the process of feature prediction, since it refers to features derived from image fields in which a kernel ridge regression is used, since the intuition is strongest in this case. We postpone the discussion of how to select a kernel for deep layers as well as for non-image data in the visible layer to a later section. In these settings, the prediction process is formally identical, but the intuition is less clarity.If v is a vectorized image field corresponding to the visible layer of a standard neural network, then the hidden activity induced by this patch is given by h = g (vW), and W = [w1,... wnh] a weight matrix is a weight matrix whose columns correspond to the visible layer of a neural network, then the hidden activity induced by this patch is given by W (nessv = 1)."}, {"heading": "3.3 Interpretation as pooling", "text": "So far, we have motivated our technique as a method for predicting characteristics in a neural network; however, the same approach can also be interpreted as a linear pooling process. Remember that the hidden activations in a standard neural network are given before applying nonlinearity by g \u2212 1 (h) = vW. Our motivation runs along the lines of replacing W with U\u03b1W\u03b1 and discussing the relationship between W and its predicted counterpart. Alternatively, we can write g \u2212 1 (h) = v\u03b1W\u03b1, where v\u03b1 = vU\u03b1 is a linear transformation of the data. Under this interpretation, we can imagine that a predicted layer consists of two layers internally."}, {"heading": "3.4 Columnar architecture", "text": "The prediction process we have described so far assumes that U\u03b1 is the same for all characteristics, but this can be too restrictive. To continue with the intuition that filters should be smooth local edge detectors, we could choose \u03b1 to give a high resolution in a local area of pixel space while using a more economical representation in the rest of the room. Of course, in this case we would want to choose several different alpha detectors, each of which concentrates high resolution information in different regions. It is easy to extend the prediction of characteristics to this setting while using several different index sets in the rest of the room."}, {"heading": "3.5 Constructing dictionaries", "text": "The appropriate choice of the dictionary inevitably depends on the structure of the weight space. If the weight space has a topological structure in which we expect smoothness, for example, if the weights correspond to pixels in an image field, we can choose a kernel-based dictionary to enforce the kind of smoothness we expect. If there is no topological structure we can exploit, we suggest using data-driven dictionaries. An obvious choice here is to use a flat, uncontrolled feature learning to create a dictionary for the layer. Another option is to construct data-driven cores for the ridge regression. Simple decisions here are the use of empirical covariance or empirical quadratic covariance of the hidden units averaged over the data. As the correlations in hidden activities depend on the weights in the lower layers, we cannot automatically process these layers."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Multilayer perceptron", "text": "We are conducting some initial experiments using MLPs [24] to demonstrate the effectiveness of our technique. We are training several MLP models on MNIST using different dictionary building strategies, different number of columns and different degree of reduction in the number of dynamic parameters used in each feature. We chose these permutations on MNIST because they are small enough to provide broad coverage. The networks in this experiment all have two hidden layers with a 784-500-10 architecture and use a sigmoid activation function. The last layer is a softmax classifier. In all cases, we preform parameter prediction in the first and second layer; the last Softmax layer is never predicted. This layer contains approximately 1% of the total network parameters, so significant savings are possible even if features in this layer are not predicted; the last Softmax layer is never predicted."}, {"heading": "4.2 Convolutional network", "text": "Figure 5 shows the performance of a convential layer [17] on CIFAR-10. The first convential layer filters the 32 x 32 x 3 input image with 48 filters of the size 8 x 8 x 3. The second convential layer applies 64 filters of the size 8 x 8 x 48 to the output of the first layer. The third convential layer further transforms the output of the second layer by applying 64 filters of the size 5 x 5 x 64. The output of the third layer takes place on a fully bonded layer with 500 hidden units and finally into a Softmax layer with 10 outputs. Again, we do not reduce the parameters in the last Softmax layer. The convential layers each have one column and the fully bonded layer has five columns. Convential layers have a natural topological structure to exploit, so we use a dictionary constructed with the square exponential core in each convential layer."}, {"heading": "4.3 Reconstruction ICA", "text": "Reconstruction ICA [15] is a method for learning supercomplete ICA models that resembles a linear autoencoder network. We show that we use parameters in RICA both on CIFAR-10 and on STL-10. To use RICA as a classifier, we follow the method of Coates et al. [6]. Figure 6 (left) shows the results of parameter prediction with RICA on CIFAR-10 and STL-10. RICA is a single-layer architecture, and we predict parameters on a square exponential kernel dictionary with a length scale of 1.0. The Nokernel line shows the performance of RICA without feature prediction on the same task. In both cases, we are able to predict more than half of the dynamic parameters without significant decrease in accuracy. Figure 6 (right) compares the performance of two RICA models with the same number of dynamic parameters."}, {"heading": "5 Related work and future directions", "text": "In fact, most of them are able to surpass themselves by embarking on the search for new ways to travel the world."}, {"heading": "6 Conclusion", "text": "The idea is orthogonal but complementary to recent advances in deep learning, such as dropout, corrected units, and maximization. It creates many avenues for future work, such as improving large-scale industrial implementations of deep networks, but it also raises the question of whether we have the right parameterization in deep learning."}], "references": [{"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Technical Report arXiv:1305.0445, Universit\u00e9 de Montr\u00e9al", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Schmidhuber"], "venue": "IEEE Computer Vision and Pattern Recognition, pages 3642\u20133649", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "High-performance neural networks for visual object classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Masci"], "venue": "arXiv:1102.0183", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Emergence of object-selective features in unsupervised feature learning", "author": ["A. Coates", "A. Karpathy", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 2690\u20132698", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 2528\u20132536", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "Artificial Intelligence and Statistics", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 1232\u20131240", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable stacking and learning for building deep architectures", "author": ["L. Deng", "D. Yu", "J. Platt"], "venue": "International Conference on Acoustics, Speech, and Signal Processing, pages 2133\u20132136", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Emergence of complex-like cells in a temporal product network with local receptive fields", "author": ["K. Gregor", "Y. LeCun"], "venue": "arXiv preprint arXiv:1006.0448", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["C. G\u00fcl\u00e7ehre", "Y. Bengio"], "venue": "International Conference on Learning Representations", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "CoRR, abs/1207.0580", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, pages 1106\u20131114", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Dimensionality reduction and prior knowledge in e-set recognition", "author": ["K. Lang", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1990}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 24:1017\u20131025", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Building highlevel features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "International Conference on Machine Learning", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimal brain damage", "author": ["Y. LeCun", "J.S. Denker", "S. Solla", "R.E. Howard", "L.D. Jackel"], "venue": "Advances in Neural Information Processing Systems, pages 598\u2013605", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1990}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["K.-F. Lee", "H.-W. Hon"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(11):1641\u20131648", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1989}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proc. 27th International Conference on Machine Learning, pages 807\u2013814. Omnipress Madison, WI", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Factored 3-way restricted Boltzmann machines for modeling natural images", "author": ["M. Ranzato", "A. Krizhevsky", "G.E. Hinton"], "venue": "Artificial Intelligence and Statistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning separable filters", "author": ["R. Rigamonti", "A. Sironi", "V. Lepetit", "P. Fua"], "venue": "IEEE Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Double sparsity: learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, 58:1553\u20131564", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(6088):533\u2013536", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1986}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M. Ranzato", "D. Buchman", "B. Marlin", "N. Freitas"], "venue": "International Conference on Machine Learning, pages 1201\u20131208", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "A neural support vector network architecture with adaptive kernels", "author": ["P. Vincent", "Y. Bengio"], "venue": "International Joint Conference on Neural Networks, pages 187\u2013192", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 12, "context": "It is now possible to train networks with tens of millions [13] or even over a billion parameters [7, 16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "It is now possible to train networks with tens of millions [13] or even over a billion parameters [7, 16].", "startOffset": 98, "endOffset": 105}, {"referenceID": 15, "context": "It is now possible to train networks with tens of millions [13] or even over a billion parameters [7, 16].", "startOffset": 98, "endOffset": 105}, {"referenceID": 6, "context": "[7]) are trained using asynchronous SGD.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "A major drawback of this technique is that training is very inefficient in how it makes use of parallel resources [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "[7], where the gains from distribution are largest, distributing the model over 81 machines reduces the training time per mini-batch by a factor of 12, and increasing to 128 machines achieves a speedup factor of roughly 14.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Other approaches to distributed learning of neural networks involve training in batch mode [8], but these methods have not been scaled nearly as far as their online counterparts.", "startOffset": 91, "endOffset": 94}, {"referenceID": 11, "context": "Our technique is also completely orthogonal to the choice of activation function as well as other learning optimizations; it can work alongside other recent advances in neural network training such as dropout [12], rectified units [20] and maxout [9] without modification.", "startOffset": 209, "endOffset": 213}, {"referenceID": 19, "context": "Our technique is also completely orthogonal to the choice of activation function as well as other learning optimizations; it can work alongside other recent advances in neural network training such as dropout [12], rectified units [20] and maxout [9] without modification.", "startOffset": 231, "endOffset": 235}, {"referenceID": 8, "context": "Our technique is also completely orthogonal to the choice of activation function as well as other learning optimizations; it can work alongside other recent advances in neural network training such as dropout [12], rectified units [20] and maxout [9] without modification.", "startOffset": 247, "endOffset": 250}, {"referenceID": 5, "context": "The intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13].", "startOffset": 252, "endOffset": 259}, {"referenceID": 12, "context": "The intuition motivating the techniques in this paper is the well known observation that the first layer features of a neural network trained on natural image patches tend to be globally smooth with local edge features, similar to local Gabor features [6, 13].", "startOffset": 252, "endOffset": 259}, {"referenceID": 24, "context": "One way to achieve this is via kernel ridge regression [25].", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "This particular order of operations may result in computational improvements if the number of hidden channels is larger than n\u03b1, or if the elements of U\u03b1 are separable [22].", "startOffset": 168, "endOffset": 172}, {"referenceID": 23, "context": "We perform some initial experiments using MLPs [24] in order to demonstrate the effectiveness of our technique.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "the phones in each utterance using a bigram language model, and confusions between certain sets of phones were ignored as described in [19].", "startOffset": 135, "endOffset": 139}, {"referenceID": 16, "context": "Figure 5 shows the performance of a convnet [17] on CIFAR-10.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "Reconstruction ICA [15] is a method for learning overcomplete ICA models which is similar to a linear autoencoder network.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "An early approach is the technique of \u201cOptimal Brain Damage\u201d [18] which uses approximate second derivative information to remove parameters from an already trained network.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "The most common approach to limiting the number of parameters is to use locally connected features [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "The size of the parameterization of locally connected networks can be further reduced by using tiled convolutional networks [10] in which groups of feature weights which tile the input", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Convolutional neural networks [13] are even more restrictive and force a feature to have tied weights for all receptive fields.", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "[23] involves approximating linear dictionaries with other dictionaries in a similar manner to how we approximate network features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] study approximating convolutional filter banks with linear combinations of separable filters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] control the number of parameters by removing connections between layers in a convolutional network at random.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Lang and Hinton [14] tried imposing explicit smoothness constraints through regularization but found it to universally reduce performance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "[11] have demonstrated that prior knowledge is extremely important during learning, which highlights the importance of introducing it effectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Recent work has shown that state of the art results on several benchmark tasks in computer vision can be achieved by training neural networks with several columns of representation [2, 13].", "startOffset": 181, "endOffset": 188}, {"referenceID": 12, "context": "Recent work has shown that state of the art results on several benchmark tasks in computer vision can be achieved by training neural networks with several columns of representation [2, 13].", "startOffset": 181, "endOffset": 188}, {"referenceID": 1, "context": "The use of different preprocessing for different columns of representation is of particular relevance [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "Unlike the work of [2], we do not consider deep columns in this paper; however, collimation is an attractive way for increasing parallelism within a network, as the columns operate completely independently.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "Our approach is superficially similar to the factored RBM [21, 26], whose parameters form a 3tensor.", "startOffset": 58, "endOffset": 66}, {"referenceID": 25, "context": "Our approach is superficially similar to the factored RBM [21, 26], whose parameters form a 3tensor.", "startOffset": 58, "endOffset": 66}, {"referenceID": 4, "context": "Other works have focused on learning receptive fields directly [5], and would be interesting to incorporate with our technique.", "startOffset": 63, "endOffset": 66}, {"referenceID": 26, "context": "Using different types of kernels to encode different types of prior knowledge on the weight space, or even learning the kernel functions directly as part of the optimization procedure as in [27] are possibilities that deserve exploration.", "startOffset": 190, "endOffset": 194}, {"referenceID": 9, "context": "This has parallels to other work on inducing topology in representations [10] as well as work on learning pooling structures in deep networks [4].", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "This has parallels to other work on inducing topology in representations [10] as well as work on learning pooling structures in deep networks [4].", "startOffset": 142, "endOffset": 145}], "year": 2014, "abstractText": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy.", "creator": "LaTeX with hyperref package"}}}