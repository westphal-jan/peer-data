{"id": "1306.0686", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2013", "title": "Online Learning under Delayed Feedback", "abstract": "Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.", "histories": [["v1", "Tue, 4 Jun 2013 07:39:21 GMT  (56kb)", "https://arxiv.org/abs/1306.0686v1", "Extended version of a paper accepted to ICML-2013"], ["v2", "Wed, 5 Jun 2013 01:01:04 GMT  (53kb)", "http://arxiv.org/abs/1306.0686v2", "Extended version of a paper accepted to ICML-2013"]], "COMMENTS": "Extended version of a paper accepted to ICML-2013", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["pooria joulani", "andr\u00e1s gy\u00f6rgy", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1306.0686"}, "pdf": {"name": "1306.0686.pdf", "metadata": {"source": "META", "title": "Online Learning under Delayed Feedback", "authors": ["Pooria Joulani"], "emails": ["pooria@ualberta.ca", "gyorgy@ualberta.ca", "szepesva@ualberta.ca"], "sections": [{"heading": null, "text": "ar Xiv: 130 6.06 86v2 [cs.LG] 5 Jun 201 3"}, {"heading": "1. Introduction", "text": "In this paper, we examine sequential learning when feedback on the forecaster's predictions may be delayed, for example in web advertising, where information about whether a user has clicked on a particular ad returns to the engine with a delay: after an ad has been selected while waiting for the information when the user clicks or not, the engine must provide ads to other users. Also, the click information can be aggregated and then sent periodically to the module that decides on the ads, leading to further delays. (Li et al., 2010; Dudik et al., 2011) Another example is distributed learning, where the dissemination of information under the nodes causes delays (Agarwal & Duchi, 2011).Procedure of the 30th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W & CP Volume 28. Copyright 2013 by the author."}, {"heading": "2. The delayed feedback model", "text": "We are looking at a general model of online learning that we call the partial monitoring problem with the page in education. In this model, the forecaster (decision maker) must make a sequence of predictions (actions) that may be based on any ancillary information, and for each prediction he receives a reward and feedback where the feedback is delayed. Formally, he considers a set of possible ancillary information X, a set of possible predictions that include a set of possible feedback values H, at any time immediately t = 1, 2,.., the forecaster receives some ancillary information xt, X; then, possibly based on the ancillary information that the forecaster predicts some values while the environment chooses a reward function rt R; finally, the forecaster receives reward rt (xt, at) and some timed feedback from Ht."}, {"heading": "2.1. Related work", "text": "The effects of delayed feedback have been studied in recent years under various online learning scenarios G \u0445 n \u2264 \u03c4max, when delays have an upper limit \u03c4max, and we show that the new limits of the partial monitoring problem are automatically applicable to the other, spatial, cases, and in most cases provide improved outcomes. A brief summary, together with the contributions of this paper, is in Table 1. To show the best of our knowledge, Weinberger & Ordentlich (2002) were the first to analyze the delayed feedback problem; they looked at the adversarial full information gap with a fixed, known delay constant. They showed that the minimax optimal solution is not able to operate the optimal optimal predictors on the subsampled reward sequences."}, {"heading": "3. Black-Box Algorithms for Delayed Feedback", "text": "In this section, we provide black box algorithms for the delayed feedback problem. We assume that there is a basic algorithm base for solving the prediction problem immediately. Often, we do not determine the assumptions underlying the limits of regret for these algorithms, and assume that the problem we are looking at differs from the original problem only because of the delays. For example, in the opposite environment, Base may be based on the assumption that the reward functions are selected in a forgettable or unforgettable way (i.e. independent or not from the predictions of the forecaster). First, we consider the contrary case in Section 3.1, and then in Section 3.2 narrower limits for the stochastic case."}, {"heading": "3.1. Adversarial setting", "text": "We say that a prediction algorithm has banished a regret or an expected regret if the base is not optimal. \"We say that a prediction algorithm has banished a regret or an expected regret if the base triggers a regret.\" We say that the base triggers a regret. \"We say that the base triggers a regret.\" We say that the base triggers a regret. \"We say that the base triggers a regret.\" We say that the base triggers a regret, that the base triggers a regret. \"We say that the base triggers a regret.\" We say that the base triggers a regret. \"We say that the base triggers a regret.\" We say that the base triggers a regret, that the base triggers a regret. \"We say that the base triggers a regret, that the base triggers a regret.\""}, {"heading": "3.2. Finite stochastic setting", "text": "In this section, we consider the case when the prediction A of the prediction is finite; without loss of generality, we assume that we have A = {1, 2,., K}. We also assume that there is no lateral information (i.e. that the main assumption in this section is that the results (bt) t are more than 1 form of an i.d.sequence, which is also independent of the predictions of the prediction of the prediction, where we can repeat the predictions described below for each value of the page information individually). The main assumption in this section is that the results (bt) t are more than 1 form of an i.d. sequence, which is also independent of the predictions of the prediction. If B is, this leads to the standard i.d. partial monitoring (IPM), while the conventional multiarmed bandit (MAB) setting is restored if the feedback is the reward of the last prediction, which is (at =)."}, {"heading": "4. UCB for the Multi-Armed Bandit Problem with Delayed Feedback", "text": "While the algorithms in the previous section provide an easy way to convert algorithms designed for the non-delayed case into algorithms that can handle delays in feedback, improvements can be achieved by making changes within existing non-delayed algorithms while maintaining their theoretical qualities when the delays are large. We look at the stochastic, multi-armed bandit problem and extend the UCB family of algorithms (Auer et al., 2002; Garivier & Cappe, 2011) to delayed setup. The proposed changes are quite natural, and the common features of the UCB algorithms allow a uniform way of extending their performance guarantees to delayed setup (up to an additive penalty for delays)."}, {"heading": "5. Conclusion and future work", "text": "We analyzed the effects of feedback delays in online learning problems. We examined the partial monitoring case (which also covers the complete information and bandit settings) and provided general algorithms that turn forecasters designed for the non-delayed case into those that handle delayed feedback. It turns out that the price of the delay is a multiplicative increase in regret for adverse problems and only an additive increase in stochastic problems. Although we believe these results are qualitatively correct, we have no lower limits to prove this (matching lower limits are available only for the complete case of information). It also turns out that the most important quantity that determines the performance of our algorithms is G'n, the maximum number of missing rewards. It is interesting to note that G'n is the maximum number of servers that result in a multi-server waiting system with infinitely more servers and we may have found the most deterrent results of a particular chain of arrival times even though it is usable to those specific ones."}, {"heading": "6. Acknowledgements", "text": "This work was supported by Alberta Innovates Technology Futures and NSERC."}, {"heading": "A. Proof of Lemma 4", "text": "In this appendix, we prove lemmas 4 used in the i.i.d. partial monitoring (Section 3.2). To this end, we first need two more lemmas. The first dilemma shows that the i.i.d. property of a sequence of random variables is preserved under an independent random reorder of this sequence. (Let us (Xt) t be a random sequence of independent, identically distributed random variables. (Let us reorder this sequence according to an independent random permutation, then the resulting sequence is i.d. with the same distribution as (Xt) t is a random sequence. (Let the reordered sequence be by (Zt) t random distribution. It is sufficient to show that for all n random permutations N, for all n random permutations, y2,..) we haveP {Z1 random, Z2 random, y2."}, {"heading": "B. UCB for the Multi-Armed Bandit Problem with Delayed Feedback", "text": "This appendix describes the framework we have described in Section 4 for analyzing UCB type algorithms in delayed settings and provides the missing evidence. The regret of a UCB algorithm is usually analyzed by looking at the (expected) number of times of a suboptimal prediction (e.g. by Auer et al. (2002)) when this prediction is made at least once (for a sufficiently large prediction), and using concentration imbalances suitable for the specific form of upper confidence to show that it is unlikely to make this suboptimal prediction more than once, because observing the reward distribution is sufficient to distinguish it from the optimal prediction with high reliability."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Fischer", "Paul"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Prediction, Learning, and Games", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Lugosi", "G\u00e1bor"], "venue": null, "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Stochastic Processes", "author": ["Doob", "Joseph L"], "venue": null, "citeRegEx": "Doob and L.,? \\Q1953\\E", "shortCiteRegEx": "Doob and L.", "year": 1953}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Garivier", "Aur\u00e9lien", "Capp\u00e9", "Olivier"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Garivier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garivier et al\\.", "year": 2011}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Hoeffding", "Wassily"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding and Wassily.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding and Wassily.", "year": 1963}, {"title": "Slow learners are fast", "author": ["Langford", "John", "Smola", "Alexander", "Zinkevich", "Martin"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Improving on-line learning", "author": ["Mesterharm", "Chris J"], "venue": "PhD thesis,", "citeRegEx": "Mesterharm and J.,? \\Q2007\\E", "shortCiteRegEx": "Mesterharm and J.", "year": 2007}, {"title": "The Theory of the Riemann ZetaFunctions", "author": ["Titchmarsh", "Edward Charles", "Heath-Brown", "David Rodney"], "venue": null, "citeRegEx": "Titchmarsh et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Titchmarsh et al\\.", "year": 1987}, {"title": "On delayed prediction of individual sequences", "author": ["Weinberger", "Marcelo J", "Ordentlich", "Erik"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Weinberger et al\\.,? \\Q1959\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 1959}, {"title": "UCB1 under delayed feedback: Proof of Theorem 7 Below comes the proof of Theorem 7 for the Delayed-UCB1 algorithm (Section 4). Proof of Theorem 7. Following the outline of the previous section, we can bound the summation in (8) using the same analysis as in the original UCB1 paper", "author": ["algorithms. B"], "venue": "(Auer et al.,", "citeRegEx": "B.1.,? \\Q2002\\E", "shortCiteRegEx": "B.1.", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "To resolve this problem, we work out modifications of variants of the UCB algorithm (Auer et al., 2002) for stochastic bandit problems with delayed feedback that have much smaller complexity than the black-box algorithms.", "startOffset": 84, "endOffset": 103}, {"referenceID": 5, "context": "Stochastic Feedback General (Adversarial) Feedback L No R(n) \u2264 R(n) +O(E [ \u03c4 t ] ) R(n) \u2264 O(\u03c4const)\u00d7R(n/\u03c4const) Side (Agarwal & Duchi, 2011) (Weinberger & Ordentlich, 2002) Full Info Info (Langford et al., 2009) (Agarwal & Duchi, 2011) L L Side Info R(n) \u2264 R(n) +O(D) R(n) \u2264 O(D\u0304)\u00d7R\u2032(n/D\u0304) (Mesterharm, 2007) (Mesterharm, 2007) No Side R(n) \u2264 C1R(n) + C2\u03c4max log(\u03c4max) R(n) \u2264 O(\u03c4const)\u00d7R(n/\u03c4const) Bandit Info (Desautels et al.", "startOffset": 188, "endOffset": 211}, {"referenceID": 5, "context": "Langford et al. (2009) showed that under the usual conditions, a sufficiently slowed-down version of the mirror descent algorithm achieves optimal decay rate of the average regret.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Langford et al. (2009) showed that under the usual conditions, a sufficiently slowed-down version of the mirror descent algorithm achieves optimal decay rate of the average regret. Mesterharm (2005; 2007) considered another variant of the full information setting, using an adversarial model on the delays in the label prediction setting, where the forecaster has to predict the label corresponding to a side information vector xt. While in the full information online prediction problem Weinberger & Ordentlich (2002) showed that the regret increases by a multiplicative factor of \u03c4const, in the work of Mesterharm (2005; 2007) the important quantity becomes the maximum/average gap defined as the length of the largest time interval the forecaster does not receive feedback.", "startOffset": 0, "endOffset": 519}, {"referenceID": 5, "context": "Langford et al. (2009) showed that under the usual conditions, a sufficiently slowed-down version of the mirror descent algorithm achieves optimal decay rate of the average regret. Mesterharm (2005; 2007) considered another variant of the full information setting, using an adversarial model on the delays in the label prediction setting, where the forecaster has to predict the label corresponding to a side information vector xt. While in the full information online prediction problem Weinberger & Ordentlich (2002) showed that the regret increases by a multiplicative factor of \u03c4const, in the work of Mesterharm (2005; 2007) the important quantity becomes the maximum/average gap defined as the length of the largest time interval the forecaster does not receive feedback. Mesterharm (2005; 2007) also shows that the minimax regret in the adversarial case increases multiplicatively by the average gap, while it increases only in an additive fashion in the stochastic case, by the maximum gap. Agarwal & Duchi (2011) considered the problem of online stochastic optimization and showed that, for i.", "startOffset": 0, "endOffset": 1021}, {"referenceID": 0, "context": "We consider the stochastic multi-armed bandit problem, and extend the UCB family of algorithms (Auer et al., 2002; Garivier & Capp\u00e9, 2011) to the delayed setting.", "startOffset": 95, "endOffset": 138}, {"referenceID": 0, "context": "As an example, the UCB1 algorithm (Auer et al., 2002) uses UCBs of the form Bi,s,t = \u03bc\u0302i,s + \u221a 2 log(t)/s, where \u03bc\u0302i,s = 1 s \u2211s t=1 h \u2032 i,t is the average of the first s observed rewards.", "startOffset": 34, "endOffset": 53}], "year": 2013, "abstractText": "Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.", "creator": "LaTeX with hyperref package"}}}