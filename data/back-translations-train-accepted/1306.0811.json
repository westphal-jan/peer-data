{"id": "1306.0811", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2013", "title": "A Gang of Bandits", "abstract": "Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to \"share\" signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure.", "histories": [["v1", "Tue, 4 Jun 2013 14:24:31 GMT  (934kb,D)", "https://arxiv.org/abs/1306.0811v1", null], ["v2", "Fri, 25 Oct 2013 16:32:25 GMT  (957kb,D)", "http://arxiv.org/abs/1306.0811v2", "NIPS 2013"], ["v3", "Mon, 4 Nov 2013 10:07:42 GMT  (959kb,D)", "http://arxiv.org/abs/1306.0811v3", "NIPS 2013"]], "reviews": [], "SUBJECTS": "cs.LG cs.SI stat.ML", "authors": ["nicol\u00f2 cesa-bianchi", "claudio gentile", "giovanni zappella"], "accepted": true, "id": "1306.0811"}, "pdf": {"name": "1306.0811.pdf", "metadata": {"source": "CRF", "title": "A Gang of Bandits", "authors": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile"], "emails": ["nicolo.cesa-bianchi@unimi.it", "claudio.gentile@uninsubria.it", "giovanni.zappella@unimi.it"], "sections": [{"heading": null, "text": "ar Xiv: 130 6.08 11v3 ["}, {"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2 Related work", "text": "The benefit of using social relationships to improve the quality of recommendations is a recognized fact in the literature of content recommendation systems - see, for example, [5, 13, 18] and the survey [3]. Linear models for contextual bandits were introduced in [4]. Their application to personalized content recommendations was pioneered in [15], where the LinUCB algorithm was introduced. An analysis by LinUCB was provided in the subsequent paper [9]. To the best of knowledge, this is the first work that combines contextual bandits with social graph information. Nontextual stochastic bandits in social networks were, however, examined in a recently published independent paper [20]. Other work, such as [2, 19] consider contextual bandits by assuming metric or probabilistic dependencies on the product space of contexts and actions. Another standpoint where each action discloses information about the payoffs of other actions is the one that is not provided in the context [7, although this one is examined by the 16]."}, {"heading": "3 Learning model", "text": "We assume that the social relations via user as a well-known undirected and connected diagram G = (V, E), in which V = {1,., n} represent a set of users, and the edges in E represent the social links via user pairs. Remember that a graph G in relation to its laplace matrix L = [Li, j] n, and 0 otherwise.Learning proceeds in a sequential manner: In each step t = 1, 2,., the learner receives a user index, which it together with a set of context vectors Cit = {xt, xt, 2, 2."}, {"heading": "4 Algorithm and regret analysis", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it"}, {"heading": "4.1 Regret Analysis", "text": "We now provide a regret analysis for GOB.Lin, based on the high probability analysis contained in [1] (Theorem 2 therein), which can be seen as a combination of the multitask kernel contained in [8, 17, 12] and a version of the linear bandit algorithm described and analyzed in [1]. (Theorem 1.) Let us leave the GOB.Lin algorithm of Figure 2 on graph G = (V, E), V = {1,.. (n), which on each node i V vector ui Rd. DefineL (u1,. \u2212 un) = [V, E), V = 1,. (i, j).E ui \u2212 2,. Let us also leave the sequence of context vectors xt, k, k, so that the sequence of context vectors xt, k, B, for all k = 1."}, {"heading": "5 Experiments", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "A Appendix", "text": "This appendix contains the proof for Theorem 1.Prooft. \u2212 bt = > K = > K = > K = > K = > K = > K = > K = (u > 1, u > 2,. \u2212 b = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D, K and D = D = D = D = D \u2212 T = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D, D = D = D = D = D = D, D = D = D = D = D = D = D, D = D = D = D = D = D = D, D = D = D = D = D = D = D = D, D = D = D = D = D = D = D = D = D, D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D, D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D = D"}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Y. Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical models for bandit problems", "author": ["K. Amin", "M. Kearns", "U. Syed"], "venue": "Proceedings of the Twenty-Seventh Conference Uncertainty in Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Algorithms and methods in recommender systems", "author": ["D. Asanov"], "venue": "Berlin Institute of Technology, Berlin, Germany", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Using confidence bounds for exploration-exploitation trade-offs", "author": ["P. Auer"], "venue": "Journal of Machine Learning Research, 3:397\u2013422", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Movie recommendation using random walks over the contextual graph", "author": ["T. Bogers"], "venue": "CARS\u201910: Proceedings of the 2nd Workshop on Context-Aware Recommender Systems", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "and T", "author": ["I. Cantador", "P. Brusilovsky"], "venue": "Kuflik. 2nd Workshop on Information Heterogeneity and Fusion in Recommender Systems ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Leveraging side observations in stochastic bandits", "author": ["S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat"], "venue": "Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence, pages 142\u2013151", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Linear algorithms for online multitask classification", "author": ["G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile"], "venue": "Journal of Machine Learning Research, 11:2597\u2013 2630", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Contextual bandits with linear payoff functions", "author": ["W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics, pages 208\u2013214", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiclass classification with bandit feedback using adaptive regularization", "author": ["K. Crammer", "C. Gentile"], "venue": "Machine Learning, 90(3):347\u2013383", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Weighted graph cuts without eigenvectors a multilevel approach", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(11):1944\u20131957", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201904, pages 109\u2013117, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Personalized recommendation of social software items based on social relations", "author": ["I. Guy", "N. Zwerdling", "D. Carmel", "I. Ronen", "E. Uziel", "S. Yogev", "S. Ofek- Koifman"], "venue": "Proceedings of the Third ACM Conference on Recommender Sarxiv ystems, pages 53\u201360. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Bandit problems in networks: Asymptotically efficient distributed allocation rules", "author": ["S. Kar", "H.V. Poor", "S. Cui"], "venue": "Decision and Control and European Control Conference (CDC-ECC), 2011 50th IEEE Conference on, pages 1771\u20131778. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "Proceedings of the 19th International Conference on World Wide Web, pages 661\u2013670. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "From bandits to experts: On the value of sideobservations", "author": ["S. Mannor", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems, pages 684\u2013692", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernels for multi\u2013task learning", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Advances in Neural Information Processing Systems, pages 921\u2013928", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "How social relationships affect user similarities", "author": ["A. Said", "E.W. De Luca", "S. Albayrak"], "venue": "Proceedings of the 2010 Workshop on Social Recommender Systems, pages 1\u20134", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "Journal of Machine Learning Research \u2013 Proceedings Track, 19:679\u2013702", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-armed bandits in the presence of side observations in social networks", "author": ["B. Swapna", "A. Eryilmaz", "N.B. Shroff"], "venue": "Proceedings of 52nd IEEE Conference on Decision and Control (CDC)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Gossip-based distributed stochastic bandit algorithms", "author": ["B. Sz\u00f6r\u00e9nyi", "R. Busa-Fekete", "I. Hegedus", "R. Orm\u00e1ndi", "M. Jelasity", "B. K\u00e9gl"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "Proceedings of the 26th International Conference on Machine Learning, pages 1113\u20131120. Omnipress", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "g, [5, 7, 15] ).", "startOffset": 3, "endOffset": 13}, {"referenceID": 6, "context": "g, [5, 7, 15] ).", "startOffset": 3, "endOffset": 13}, {"referenceID": 14, "context": "g, [5, 7, 15] ).", "startOffset": 3, "endOffset": 13}, {"referenceID": 7, "context": ", [8]), is defined in terms of the Laplacian matrix of the graph.", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", [5, 13, 18] and the survey [3].", "startOffset": 2, "endOffset": 13}, {"referenceID": 12, "context": ", [5, 13, 18] and the survey [3].", "startOffset": 2, "endOffset": 13}, {"referenceID": 17, "context": ", [5, 13, 18] and the survey [3].", "startOffset": 2, "endOffset": 13}, {"referenceID": 2, "context": ", [5, 13, 18] and the survey [3].", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Linear models for contextual bandits were introduced in [4].", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "Their application to personalized content recommendation was pioneered in [15], where the LinUCB algorithm was introduced.", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "An analysis of LinUCB was provided in the subsequent work [9].", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "However, non-contextual stochastic bandits in social networks were studied in a recent independent work [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 1, "context": "Other works, such as [2, 19], consider contextual bandits assuming metric or probabilistic dependencies on the product space of contexts and actions.", "startOffset": 21, "endOffset": 28}, {"referenceID": 18, "context": "Other works, such as [2, 19], consider contextual bandits assuming metric or probabilistic dependencies on the product space of contexts and actions.", "startOffset": 21, "endOffset": 28}, {"referenceID": 6, "context": "A different viewpoint, where each action reveals information about other actions\u2019 payoffs, is the one studied in [7, 16], though without the context provided by feature vectors.", "startOffset": 113, "endOffset": 120}, {"referenceID": 15, "context": "A different viewpoint, where each action reveals information about other actions\u2019 payoffs, is the one studied in [7, 16], though without the context provided by feature vectors.", "startOffset": 113, "endOffset": 120}, {"referenceID": 13, "context": "A non-contextual model of bandit algorithms running on the nodes of a graph was studied in [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "More recently, a new model of distributed non-contextual bandit algorithms has been presented in [21], where the number of communications among the nodes is limited, and all the nodes in the network have the same best action.", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": ", (it\u22121, Cit\u22121 , at\u22121) ], we take the general approach of [1], and assume that for any fixed i \u2208 V and x \u2208 R, the variable i(x) is conditionally sub-Gaussian with variance parameter \u03c3 > 0, namely, Et [ exp(\u03b3 i(x)) ] \u2264 exp ( \u03c3 \u03b3/2 ) for all \u03b3 \u2208 R and all x, i.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": ", [8]), and assume that \u2211", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": ", [9]) operating on the context vectors contained in Cit .", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": ", [1, 9, 10].", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": ", [1, 9, 10].", "startOffset": 2, "endOffset": 12}, {"referenceID": 9, "context": ", [1, 9, 10].", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": "This algorithm can be seen as a version of LinUCB [9], a linear bandit algorithm derived from LinRel [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "This algorithm can be seen as a version of LinUCB [9], a linear bandit algorithm derived from LinRel [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "Lin that relies on the high probability analysis contained in [1] (Theorem 2 therein).", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": ", [8, 17, 12] and a version of the linear bandit algorithm described and analyzed in [1].", "startOffset": 2, "endOffset": 13}, {"referenceID": 16, "context": ", [8, 17, 12] and a version of the linear bandit algorithm described and analyzed in [1].", "startOffset": 2, "endOffset": 13}, {"referenceID": 11, "context": ", [8, 17, 12] and a version of the linear bandit algorithm described and analyzed in [1].", "startOffset": 2, "endOffset": 13}, {"referenceID": 0, "context": ", [8, 17, 12] and a version of the linear bandit algorithm described and analyzed in [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": ",[8], Section 4.", "startOffset": 1, "endOffset": 4}, {"referenceID": 0, "context": "More precisely, we created a n \u00d7 n symmetric noise matrix of random numbers in [0, 1], and we selected a threshold value such that the expected number of matrix elements above this value is exactly some chosen noise rate parameter.", "startOffset": 79, "endOffset": 85}, {"referenceID": 5, "context": "fm and Delicious were created by the Information Retrieval group at Universidad Autonoma de Madrid for the HetRec 2011 Workshop [6] with the goal of investigating the usage of heterogeneous information in recommendation systems.", "startOffset": 128, "endOffset": 131}, {"referenceID": 10, "context": ", in [11].", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "Future work will consider experiments against different methods for sharing contextual and feedback information in a set of users, such as the feature hashing technique of [22].", "startOffset": 172, "endOffset": 176}], "year": 2013, "abstractText": "Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to \u201cshare\u201d signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure. 1 ar X iv :1 30 6. 08 11 v3 [ cs .L G ] 4 N ov 2 01 3", "creator": "LaTeX with hyperref package"}}}