{"id": "1306.0940", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2013", "title": "(More) Efficient Reinforcement Learning via Posterior Sampling", "abstract": "Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, \\emph{posterior sampling for reinforcement learning} (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\\tilde{O}(\\tau S \\sqrt{AT})$ bound on the expected regret, where $T$ is time, $\\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.", "histories": [["v1", "Tue, 4 Jun 2013 23:00:56 GMT  (179kb,D)", "https://arxiv.org/abs/1306.0940v1", "10 pages"], ["v2", "Wed, 25 Sep 2013 05:14:31 GMT  (183kb,D)", "http://arxiv.org/abs/1306.0940v2", "10 pages"], ["v3", "Thu, 26 Sep 2013 00:38:51 GMT  (183kb,D)", "http://arxiv.org/abs/1306.0940v3", "10 pages"], ["v4", "Wed, 13 Nov 2013 19:31:26 GMT  (183kb,D)", "http://arxiv.org/abs/1306.0940v4", "10 pages"], ["v5", "Thu, 26 Dec 2013 09:20:29 GMT  (184kb,D)", "http://arxiv.org/abs/1306.0940v5", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ian osband", "daniel russo 0001", "benjamin van roy"], "accepted": true, "id": "1306.0940"}, "pdf": {"name": "1306.0940.pdf", "metadata": {"source": "CRF", "title": "(More) Efficient Reinforcement Learning via Posterior Sampling", "authors": ["Ian Osband", "Benjamin Van Roy"], "emails": ["iosband@stanford.edu", "bvr@stanford.edu", "djrusso@stanford.edu"], "sections": [{"heading": null, "text": "\u221a AT) tied to expected regret, where T is the time, \u03c4 the episode length, and S and A the cardinalities of the state and action space. This limit is one of the first for an algorithm that is not based on optimism, and corresponds to the state of the art for each learning algorithm for amplification. We show by simulation that PSRL significantly exceeds existing algorithms with similar limits of regret."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "2 Problem formulation", "text": "We look at the problem of learning to optimize a random finite horizon MDP M = (S, A, RM, PM, \u03c4, \u03c1) in repeated finite episodes of interaction. S is the state space, A is the room for action, RMa (s) is a probability distribution of reward that is realized in the selection of measures for a while in states whose support [0, 1], PMa (s) is the likelihood of transition to state s. \"2We change the terminology, since PSRL is neither optimal, nor the initial state distribution. We define the MDP and any other random variables that we consider with1For an MDP, this could be an earlier transition dynamic and reward distribution."}, {"heading": "3 Posterior sampling for reinforcement learning", "text": "The use of posterior sampling for reinforcement learning (PSRL) was first proposed by Strens [16]. PSRL starts with a prior distribution via MDPs with the states S, actions A and horizon \u03c4. At the beginning of each kst episode, a random MDP Mk from the subsequent distribution determined by the available history Htk. PSRL calculates and then follows the policy \u00b5k = \u00b5Mk via episode k.Algorithm: posterior sampling for reinforcement learning (PSRL) Data: Priority distribution f, t = 1 for episodes k = 1, 2,.. dosample Mk \u0445 f (\u00b7 Htk) compute \u00b5k = \u00b5Mk for timesteps j = 1,.., Promises dosample and application at = \u00b5k (st, j) sampling of the policy vasterioryms and st + 1 endWe show PSRRL explicit performance guarantees for problems related to the algorithm-based learning."}, {"heading": "3.1 Main results", "text": "The following result defines the limits of regret for PSFL. Limits have the expected regret, and to our knowledge they provide the first guarantees of an algorithm that is not based on optimism: Theorem 1. If f is the distribution of regret, then E [Regret (T, \u03c0PS\u03c4)] = O (\u03c4S \u221a AT log (SAT))) (1) This result applies to any previous distribution of MDPs and therefore applies to an immense class of models. To accommodate this generality, the expected regret includes below the previous distribution (sometimes called Bayes risk or Bayes regret). We believe that this is a natural measure of performance, but should stress that it is more common in the literature to account for a tied regret under a worst MDP instance. The next result establishes a link between these notions of regret and regret (sometimes referred to as Bayes risk or Bayes regret)."}, {"heading": "4 True versus sampled MDP", "text": "A simple observation that is central to our analysis is that at the beginning of each kth episode M * and Mk are equally distributed, which allows us to relate quantities that depend on the true but unknown MDP M *. Readers unfamiliar with the theory of measurement can imagine this as \"all the information known just before the beginning of the tk period.\" When we say that a random variable is equal to X (Htk) - measurable, this intuitively means that although X is random, it is known deterringly given the information contained in Htk. The following problem is a direct consequence of this observation [15]. Lemma 1 (Posterior Sampling). If f is the distribution of M *, then for any function (Htk)."}, {"heading": "5 Analysis", "text": "An essential tool in our analysis will be dynamic programming or the Bellman operator T M\u00b5, which is defined for each MDP M = (S, A, RM, PM, \u03c4, \u03c1), stationary policy \u00b5: S \u2192 A and value function V: S \u2192 R by T M\u00b5 V (s): = R M \u00b5 (s, \u00b5) + \u2211 s \"S PM\u00b5 (s) (s \u2032 | s) V (s \u2032).This process provides the expected value of a state s in which we follow the policy \u00b5 under the laws of M, for a single step. The following problem gives a concise form to the dynamic programming paradigm in relation to the Bellman operator. Lemma 2 (Dynamic Programming Equation). For each MDP M = (S, A, RM, PM, Thailand, Thailand, Thailand, Thailand) and the policy \u00b5: S \u00b7 {1,.,.,.,.,."}, {"heading": "5.1 Rewriting regret in terms of Bellman error", "text": "It is not to be expected that the dynamic programming equation is applied inductively: (V k\u00b5k, 1 \u2212 V \u0445 \u00b5k (\u00b7, i), 1) (stk + 1) (T k + 1) (stk + 1) = (T k \u00b5k (\u00b7, 1) (DP) (\u00b7, 1) V k \u00b5k (\u00b7, 2 \u2212 T) (stk +, 1 \u2212 stk (\u00b7, 1) (V k \u00b5k), 2 (stk), 2 (stk + 1) (stk + DP), 2 \u2212 T) (stk + 1) (S {P) (\u00b7, 1) (stk), 2 \u2212 stk (stk), 2 \u2212 stk (stk), 2 \u2212 stk (stk), 2 \u2212 stk (stk), 2 \u2212 stk (stk), 2 \u2212 T (stk + DP), 2 \u2212 T (stk), 1 \u2212 T (stk + 1) (S {P), 1 \u2212 T (stk) (stk), 2 \u2212 stk (stk) (stk), 2 \u2212 stk (stk), 2 \u2212 stk (stk) (stk (stk), 2 \u2212 stk (stk), 2 \u2212 stk (stk (stk), 2 \u2212 stk (stk), K (stk (stk), K (stk (stk), K (stk), K (K (k), K (K (k), K (K (K, K), K (K (K, K), K (k), K (k), K (k), K (k, K (k), K (k), K (k, K (k), K (k), K (k), K (k, K (k), K (k, K (k), K (k), K (k (k), K (k), K (k (k), K (k (k (k), k (k), K (k (k (k), k (k (k), k (k, k), k (k (k), k (k (k), k ("}, {"heading": "5.2 Introducing confidence sets", "text": "The last section reduced the regret of the algorithm to its expected Bellman error. We will continue with the argument that the sample Bellman operator T k\u00b5k (\u00b7, i) focuses on the actual Bellman operator T-K (\u00b7, i). To do this, we introduce a high probability similar to the analysis used in [4] and [5]. Let P-ta (\u00b7, s) denote the transition period t observed after the sample (s, a) and let R-ta (s) specify the empirical average reward. Finally, define Ntk (s, a) = 1 t = 1 (st, at) = (s, a) the number of times (s, a) and let R-ta (s) specify the empirical average reward."}, {"heading": "6 Simulation results", "text": "We compare the performance of PSRL with that of UCRL2 [4]: an optimistic algorithm with similar limits of regret. We use the standard example of RiverSwim [21] as well as several randomly generated MDPs. We provide results both in the episodic case where the state is reset at each step = 20 steps, and in the environment without episodic restart. RiverSwim consists of six states that are arranged in a chain, as shown in Figure 1. The actor begins in the extreme left state and has the choice of swimming to the left or right at each step. Swimming to the left (with the current) is always successful, but swimming to the right (against the current) often fails. The actor receives a small reward for reaching the most left state, but the optimal policy is to swim to the right and receive a much higher reward."}, {"heading": "6.1 Learning in MDPs without episodic resets", "text": "Even in cases where there is no actual restart of episodes, it can be shown that the regret of the PSRL is directed against all strategies that work over a horizon of \u03c4 or less [6]. Any setting with the discount factor \u03b1 can be learned for a certain period of time. An attractive feature of UCRL2 [4] and REGAL [5] is that they learn this optimal timeframe. Instead of calculating a new policy after a fixed number of timeframes, they start a new episode when the total number of visits to a state shareholder pair doubles. We can apply the same rule for episodes of PSRL horizon case as shown in Figure 2. Using optimism with KL divergence instead of L1 balls has also shown an improved performance over UCRL2 [22], but their regret remains orders of magnitude greater than PSRL on RiverSwim."}, {"heading": "7 Conclusion", "text": "We establish posterior sampling for amplification learning not only as a heuristic but as a demonstrably efficient learning algorithm. We present O-Bayesian remorse limits, which are among the first for an algorithm that is not motivated by optimism and is close to the state of the art for any amplification learning algorithm. These limits apply regardless of previous or model structure in expectation. PSRL is conceptually simple, computationally efficient and can easily incorporate prior knowledge. Compared to feasible optimistic algorithms, we believe that PSRL is often statistically more efficient, easier to implement and mathematically cheaper. We show that PSRL performs well in simulation across multiple domains. We believe that there is a strong argument for the broader adoption of algorithms based on posterior sampling both in theory and practice."}, {"heading": "Acknowledgments", "text": "Osband and Russo are supported by Stanford Graduate Fellowships courtesy of PACCAR inc. and Burt and Deedee McMurty, respectively. This work was supported in part by the National Science Foundation's CMMI-0968707 award."}, {"heading": "A Relating Bayesian to frequentist regret", "text": "Let M be any family of MDPs with a probability below zero, then apply to any > 0 > 12: P (T, \u03c0PS\u03c4) TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs > TDPs"}, {"heading": "B Bounding the sum of confidence set widths", "text": "We are interested in the limitation of min (st, at) = (s, a) and (s, a)."}], "references": [{"title": "Optimal adaptive policies for markov decision processes", "author": ["A.N. Burnetas", "M.N. Katehakis"], "venue": "Mathematics of Operations Research, 22(1):222\u2013255", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Stochastic systems: estimation", "author": ["P.R. Kumar", "P. Varaiya"], "venue": "identification and adaptive control. Prentice-Hall, Inc.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in applied mathematics, 6(1):4\u201322", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1985}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "The Journal of Machine Learning Research, 99:1563\u20131600", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps", "author": ["P.L. Bartlett", "A. Tewari"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 35\u201342. AUAI Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "R-max-a general polynomial time algorithm for nearoptimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research, 3:213\u2013231", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "On the sample complexity of reinforcement learning", "author": ["S.M. Kakade"], "venue": "PhD thesis, University of London", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning, 49(2-3):209\u2013232", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, 25(3/4):285\u2013294", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1933}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A modern Bayesian look at the multi-armed bandit", "author": ["S.L. Scott"], "venue": "Applied Stochastic Models in Business and Industry, 26(6):639\u2013658", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Further optimal regret bounds for Thompson sampling", "author": ["S. Agrawal", "N. Goyal"], "venue": "arXiv preprint arXiv:1209.3353", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["S. Agrawal", "N. Goyal"], "venue": "arXiv preprint arXiv:1209.3352", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Thompson sampling: an asymptotically optimal finite time analysis", "author": ["E. Kauffmann", "N. Korda", "R. Munos"], "venue": "International Conference on Algorithmic Learning Theory", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": "CoRR, abs/1301.2609", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "Proceedings of the 17th International Conference on Machine Learning, pages 943\u2013950", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 513\u2013520. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 956\u2013963. ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient bayes-adaptive reinforcement learning using samplebased search", "author": ["A. Guez", "D. Silver", "P. Dayan"], "venue": "arXiv preprint arXiv:1205.3109", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Approaching bayes-optimalilty using monte-carlo tree search", "author": ["J. Asmuth", "M.L. Littman"], "venue": "Proc. 21st Int. Conf. Automat. Plan. Sched., Freiburg, Germany", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Journal of Computer and System Sciences, 74(8):1309\u20131331", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimism in reinforcement learning based on kullbackleibler divergence", "author": ["S. Filippi", "O. Capp\u00e9", "A. Garivier"], "venue": "CoRR, abs/1004.5229", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We consider the classical reinforcement learning problem of an agent interacting with its environment while trying to maximize total reward accumulated over time [1, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 1, "context": "We consider the classical reinforcement learning problem of an agent interacting with its environment while trying to maximize total reward accumulated over time [1, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 2, "context": "To offset this, the majority of provably efficient learning algorithms use a principle known as optimism in the face of uncertainty [3] to encourage exploration.", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "Many authors have provided strong theoretical guarantees for optimistic algorithms [4, 5, 6, 7, 8].", "startOffset": 83, "endOffset": 98}, {"referenceID": 4, "context": "Many authors have provided strong theoretical guarantees for optimistic algorithms [4, 5, 6, 7, 8].", "startOffset": 83, "endOffset": 98}, {"referenceID": 5, "context": "Many authors have provided strong theoretical guarantees for optimistic algorithms [4, 5, 6, 7, 8].", "startOffset": 83, "endOffset": 98}, {"referenceID": 6, "context": "Many authors have provided strong theoretical guarantees for optimistic algorithms [4, 5, 6, 7, 8].", "startOffset": 83, "endOffset": 98}, {"referenceID": 7, "context": "Many authors have provided strong theoretical guarantees for optimistic algorithms [4, 5, 6, 7, 8].", "startOffset": 83, "endOffset": 98}, {"referenceID": 8, "context": "The idea of posterior sampling goes back to 1933 [9] and has been applied successfully to multi-armed bandits.", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "Despite its long history, posterior sampling was largely neglected by the multi-armed bandit literature until empirical studies [10, 11] demonstrated that the algorithm could produce state of the art performance.", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "Despite its long history, posterior sampling was largely neglected by the multi-armed bandit literature until empirical studies [10, 11] demonstrated that the algorithm could produce state of the art performance.", "startOffset": 128, "endOffset": 136}, {"referenceID": 11, "context": "This prompted a surge of interest, and a variety of strong theoretical guarantees are now available [12, 13, 14, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 12, "context": "This prompted a surge of interest, and a variety of strong theoretical guarantees are now available [12, 13, 14, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 13, "context": "This prompted a surge of interest, and a variety of strong theoretical guarantees are now available [12, 13, 14, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 14, "context": "This prompted a surge of interest, and a variety of strong theoretical guarantees are now available [12, 13, 14, 15].", "startOffset": 100, "endOffset": 116}, {"referenceID": 15, "context": "PSRL was originally introduced in the context of reinforcement learning by Strens [16] under the name \u201cBayesian Dynamic Programming\u201d,2 where it appeared primarily as a heuristic method.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "In reference to PSRL and other \u201cBayesian RL\u201d algorithms, Kolter and Ng [17] write \u201clittle is known about these algorithms from a theoretical perspective, and it is unclear, what (if any) formal guarantees can be made for such approaches.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "BOSS [18] introduces a more complicated version of PSRL that samples many MDPs, instead of just one, and then combines them into an optimistic environment to guide exploration.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "BEB [17] adds an exploration bonus to states and actions according to how infrequently they have been visited.", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "We show it is not always necessary to introduce optimism via a complicated construction, and that the simple algorithm originally proposed by Strens [16] satisfies strong bounds itself.", "startOffset": 149, "endOffset": 153}, {"referenceID": 3, "context": "First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally efficient both relative to many optimistic methods, which require simultaneous optimization across a family of plausible environments [4, 5, 18], and to computationally intensive approaches that attempt to approximate the Bayes-optimal solutions directly [18, 19, 20].", "startOffset": 244, "endOffset": 254}, {"referenceID": 4, "context": "First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally efficient both relative to many optimistic methods, which require simultaneous optimization across a family of plausible environments [4, 5, 18], and to computationally intensive approaches that attempt to approximate the Bayes-optimal solutions directly [18, 19, 20].", "startOffset": 244, "endOffset": 254}, {"referenceID": 17, "context": "First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally efficient both relative to many optimistic methods, which require simultaneous optimization across a family of plausible environments [4, 5, 18], and to computationally intensive approaches that attempt to approximate the Bayes-optimal solutions directly [18, 19, 20].", "startOffset": 244, "endOffset": 254}, {"referenceID": 17, "context": "First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally efficient both relative to many optimistic methods, which require simultaneous optimization across a family of plausible environments [4, 5, 18], and to computationally intensive approaches that attempt to approximate the Bayes-optimal solutions directly [18, 19, 20].", "startOffset": 365, "endOffset": 377}, {"referenceID": 18, "context": "First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally efficient both relative to many optimistic methods, which require simultaneous optimization across a family of plausible environments [4, 5, 18], and to computationally intensive approaches that attempt to approximate the Bayes-optimal solutions directly [18, 19, 20].", "startOffset": 365, "endOffset": 377}, {"referenceID": 19, "context": "First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally efficient both relative to many optimistic methods, which require simultaneous optimization across a family of plausible environments [4, 5, 18], and to computationally intensive approaches that attempt to approximate the Bayes-optimal solutions directly [18, 19, 20].", "startOffset": 365, "endOffset": 377}, {"referenceID": 3, "context": "Although our analysis of posterior sampling is closely related to the analysis in [4], this worst-case bound has no impact on the algorithm\u2019s actual performance.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "We demonstrate through a computational study in Section 6 that PSRL outperforms the optimistic algorithm UCRL2 [4]: a competitor with similar regret bounds over some example MDPs.", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": "S is the state space, A is the action space, R a (s) is a probability distribution over reward realized when selecting action a while in state s whose support is [0, 1], P a (s\u2032|s) is the probability of transitioning to state s\u2032 if action a is selected while at state s, \u03c4 is the time horizon, and \u03c1 the initial state distribution.", "startOffset": 162, "endOffset": 168}, {"referenceID": 15, "context": "3 Posterior sampling for reinforcement learning The use of posterior sampling for reinforcement learning (PSRL) was first proposed by Strens [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "We show PSRL obeys performance guarantees intimately related to those for learning algorithms based upon OFU, as has been demonstrated for multi-armed bandit problems [15].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "Algorithms such as UCRL2 [4] are computationally tractable, but must resort to separately bounding RMa (s) and P a (s) with high probability for each s, a.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "State-of-the-art guarantees similar to Theorem 1 are satisfied by the algorithms UCRL2 [4] and REGAL [5] for the case of non-episodic RL.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "State-of-the-art guarantees similar to Theorem 1 are satisfied by the algorithms UCRL2 [4] and REGAL [5] for the case of non-episodic RL.", "startOffset": 101, "endOffset": 104}, {"referenceID": 14, "context": "The following lemma is an immediate consequence of this observation [15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "To do this, we introduce high probability confidence sets similar to those used in [4] and [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "To do this, we introduce high probability confidence sets similar to those used in [4] and [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Lemma 17 of [4] shows3 P(M\u2217 / \u2208Mk) \u2264 1/m for this choice of \u03b2k(s, a), which implies", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "We compare performance of PSRL to UCRL2 [4]: an optimistic algorithm with similar regret bounds.", "startOffset": 40, "endOffset": 43}, {"referenceID": 20, "context": "We use the standard example of RiverSwim [21], as well as several randomly generated MDPs.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "3Our confidence sets are equivalent to those of [4] when the parameter \u03b4 = 1/m.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "Even in cases where there is no actual reset of episodes, one can show that PSRL\u2019s regret is bounded against all policies which work over horizon \u03c4 or less [6].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "One appealing feature of UCRL2 [4] and REGAL [5] is that they learn this optimal timeframe \u03c4 .", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "One appealing feature of UCRL2 [4] and REGAL [5] is that they learn this optimal timeframe \u03c4 .", "startOffset": 45, "endOffset": 48}, {"referenceID": 21, "context": "Using optimism with KL-divergence instead of L1 balls has also shown improved performance over UCRL2 [22], but its regret remains orders of magnitude more than PSRL on RiverSwim.", "startOffset": 101, "endOffset": 105}], "year": 2013, "abstractText": "Most provably-efficient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an \u00d5(\u03c4S \u221a AT ) bound on expected regret, where T is time, \u03c4 is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.", "creator": "LaTeX with hyperref package"}}}