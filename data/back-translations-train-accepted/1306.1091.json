{"id": "1306.1091", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2013", "title": "Deep Generative Stochastic Networks Trainable by Backprop", "abstract": "Recent work showed that denoising auto-encoders can be interpreted as generative models. We generalize these results to arbitrary parametrizations that learn to reconstruct their input and where noise is injected, not just in input, but also in intermediate computations. We show that under reasonable assumptions (the parametrization is rich enough to provide a consistent estimator, and it prevents the learner from just copying its input in output and producing a dirac output distribution), such models are consistent estimators of the data generating distributions, and that they define the estimated distribution through a Markov chain that consists at each step in re-injecting sampled reconstructions as a sequence of inputs into the unfolded computational graph. As a consequence, one can define deep architectures similar to deep Boltzmann machines in that units are stochastic, that the model can learn to generate a distribution similar to its training distribution, that it can easily handle missing inputs, but without the troubling problem of intractable partition function and intractable inference as stumbling blocks for both training and using these models. In particular, we argue that if the underlying latent variables of a graphical model form a highly multi-modal posterior (given the input), none of the currently known training methods can appropriately deal with this multi-modality (when the number modes is much greater than the number of MCMC samples one is willing to perform, and when the structure of the posterior cannot be easily approximated by some tractable variational approximation). In contrast, the proposed models can simply be trained by back-propagating the reconstruction error (seen as log-likelihood of reconstruction) into the parameters, benefiting from the power and ease of training recently demonstrated for deep supervised networks with dropout noise.", "histories": [["v1", "Wed, 5 Jun 2013 13:01:14 GMT  (1044kb,D)", "https://arxiv.org/abs/1306.1091v1", "arXiv admin note: text overlap witharXiv:1305.0445"], ["v2", "Fri, 7 Jun 2013 16:55:38 GMT  (1045kb,D)", "http://arxiv.org/abs/1306.1091v2", "arXiv admin note: text overlap witharXiv:1305.0445"], ["v3", "Fri, 25 Oct 2013 07:04:58 GMT  (1554kb,D)", "http://arxiv.org/abs/1306.1091v3", "arXiv admin note: text overlap witharXiv:1305.0445"], ["v4", "Wed, 18 Dec 2013 19:46:07 GMT  (1563kb,D)", "http://arxiv.org/abs/1306.1091v4", "arXiv admin note: text overlap witharXiv:1305.0445"], ["v5", "Sat, 24 May 2014 00:05:18 GMT  (1525kb,D)", "http://arxiv.org/abs/1306.1091v5", "arXiv admin note: text overlap witharXiv:1305.0445, Also published in ICML'2014"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1305.0445", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "eric laufer", "guillaume alain", "jason yosinski"], "accepted": true, "id": "1306.1091"}, "pdf": {"name": "1306.1091.pdf", "metadata": {"source": "META", "title": "Deep Generative Stochastic Networks Trainable by Backprop", "authors": ["Yoshua Bengio", "\u00c9ric Thibodeau-Laufer", "Guillaume Alain"], "emails": ["FIND.US@ON.THE.WEB"], "sections": [{"heading": null, "text": "P (X) XC (X-X) X-P (X-X) P (X) XP (H-X) HP (X-H) Figure 1. Top: A denoting auto encoder defines an estimated Markov chain, in which the transition operator first scans a corrupt X of C (X-X) and then scans a reconstruction of P (X-X) trained to estimate the basic truth P (X-X). Note that P (X-X) for each given X value is a much simpler (approximately unimodal) distribution than the basic truth P (X) and its partition function is therefore easier to approximate. Top: General GSN allows the use of arbitrary latent variables H in addition to X, where the Markov chain state (and mixing) includes both X and H. Here H is the angle over the origin."}, {"heading": "1 Introduction", "text": "This year, it will be one of the first studies to be carried out in the region."}, {"heading": "2 Summing over too many major modes", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "3 Generative Stochastic Networks", "text": "Suppose the problem we face is often much easier to learn than the problem we know. (...) Suppose we have the problem we face in order to solve an unknown problem. (...) Suppose we have the problem we know. (...) Suppose we have the problem we know. (...) Suppose we have the problem we know. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we have the problem. (...) Suppose we. (...) Suppose we. (...) Suppose we. (...) Suppose we. (...) We. (...) Suppose we. (...) Suppose we. (...) We. (...) Suppose we. (...) We. (...) Suppose we. (...)"}, {"heading": "3.1 Generative denoising autoencoders with local noise", "text": "The most important theories in Bengio et al. (2013c), reproduced below, require that the Markov chain (1) is ergodic (1). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0). (0. (0). (0). ("}, {"heading": "3.2 Generalizing the denoising autoencoder to GSNs", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3.3 Alternate parametrization with deterministic functions of random quantities", "text": "There are several equivalent ways to express a GSN. One of the interesting formulations is to use deterministic functions of random variables to express the densities (f, g) used in Theorem 2. With this approach, we define Ht + 1 = f\u03b81 (Xt, Zt, Ht) for an independent noise source Zt, and we insist that Xt cannot be exactly recovered from Ht + 1. The advantage of this formulation is that one can reconstruct loglikelihood logP (X1 = x0 | H1 = f (X0, H0) into all parameters of f and g (a similar idea was independently proposed in Kingma, 2013) and also be exploited in (Rezende et al., 2014). For the rest of this paper, we will use such a deterministic function f instead of pointing f to a probability density function."}, {"heading": "3.4 Handling missing inputs or structured output", "text": "In general, a simple way to deal with missing inputs is to limit the observed inputs and then apply the Markov chain with the caveat that the observed inputs are fixed and not resampled at each time step, while the unobserved inputs are sampled anew each time, conditioned by the inputs harnessed. It is readily demonstrated that this procedure results in a sample from the corresponding conditional distribution: Statement 1: If a subset of x (s) of the elements of X is fixed (not resampled) while the ReminderX (\u2212 s) is stochastically updated during the Markov chain of Theorem 2, but using P (Xt), X (s) of the elements of X (s), then the asymptotic distribution of the Markov chain is produced samples of X (\u2212 s)."}, {"heading": "3.5 Dependency Networks as GSNs", "text": "The dependency networks (Heckerman et al., 2000) are models in which one estimates the size ratios Pi (xi | x \u2212 i), where x \u2212 i x\\ xi, i.e. the number of variables comprising other than the i-th definition, can be parameterized separately, which does not guarantee that there is a connection of which they are the conditions. Instead of the ordered pseudo-Gibbs samplers, which are defined in Heckerman et al. (2000), which parameterizes each variable in the order x1, x2, etc., we can consider dependency networks within the GSN framework by defining a proper Markov chain in which in each step is randomly selected which variable resample can be realized. Thus, the corruption process consists only of H = f (X, Z) = X \u2212 s, where the completion of Xs is, with a randomly selected group of elements possibly limited by X."}, {"heading": "4 Experimental Example of GSN", "text": "In fact, most of us are able to reform ourselves in the way that they do it: in the way that they do it, in the way that they do it, in the way that they do it, in the way that they do it, in the way that they do it."}, {"heading": "5 Conclusion", "text": "We have introduced a new approach to generative model formation, called Generative Stochastic Networks (GSN), which is an alternative to maximum probability, with the aim of avoiding the insoluble marginalization and the risk of poor approximation of these marginalizations. The training process is more akin to functional approximation than unattended learning, because the reconstruction distribution is easier than data distribution, often unimodal (proven to be so within the limit of very low noise), which allows to train uncontrolled models that capture data-generating distribution simply by means of backprojection and gradient descent (in a computational diagram that includes noise injection).The proposed theoretical results state that under mild conditions (in particular that the noise injected into the networks prevents perfect reconstruction), the formation of the model for denoization and reconstruction of its observations (through a powerful family of reconstruction distributions) is sufficient to generate a data-chain."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge the stimulating discussions and help of Vincent Dumoulin, Pascal Vincent, Yao Li, Aaron Courville, Ian Goodfellow and Hod Lipson, as well as funding from NSERC, CIFAR (YB is Senior Fellow of CIFAR), NASA (JY is a Research Fellow in Space Technology) and the Canada Research Chairs."}, {"heading": "A Supplemental Experimental Results", "text": "In fact, it is a way in which people in the USA and in other parts of the world, those in the USA, in Europe and in the United States of America, in Europe and in the United States of America, in Europe and in the United States of America, in the USA and in the United States of America, in Europe and in the United States of America, in Europe and in the United States of America, in the EU and in the United States of America, in the USA and in the United States of America, in the USA and in the United States of America, in the USA and in the United States of America, in the EU and in the USA, in the EU and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA and in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA and in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}], "references": [{"title": "2013c) we used a Gaussian Parzen density, which (in addition to being lower rather than upper bounds) makes the numbers not comparable with the AIS log-likelihood upper bounds for binarized images reported in some papers for the same", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2013\\E", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In NIPS\u20192006,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Learning deep architectures for AI", "author": ["Bengio", "Yoshua"], "venue": "Now Publishers,", "citeRegEx": "Bengio and Yoshua.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Bengio", "Yoshua"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Better mixing via deep representations", "author": ["Bengio", "Yoshua", "Mesnil", "Gr\u00e9goire", "Dauphin", "Yann", "Rifai", "Salah"], "venue": "In ICML\u201913,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models. In NIPS26", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "Nips Foundation,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Quickly generating representative samples from an RBM-derived process", "author": ["Breuleux", "Olivier", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "Neural Computation,", "citeRegEx": "Breuleux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Breuleux et al\\.", "year": 2011}, {"title": "Comparison of perturbation bounds for the stationary distribution of a markov chain", "author": ["Cho", "Grace E", "Meyer", "Carl D", "Carl", "D. Meyer"], "venue": "Linear Algebra Appl,", "citeRegEx": "Cho et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2000}, {"title": "Phone recognition with the mean-covariance restricted Boltzmann machine", "author": ["Dahl", "George E", "Ranzato", "Marc\u2019Aurelio", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "In NIPS\u20192010,", "citeRegEx": "Dahl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2010}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder", "author": ["L. Deng", "M. Seltzer", "D. Yu", "A. Acero", "A. Mohamed", "G. Hinton"], "venue": "In Interspeech", "citeRegEx": "Deng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2010}, {"title": "Multi-prediction deep Boltzmann machines. In NIPS26", "author": ["Goodfellow", "Ian J", "Mirza", "Mehdi", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "Nips Foundation,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["Heckerman", "David", "Chickering", "David Maxwell", "Meek", "Christopher", "Rounthwaite", "Robert", "Kadie", "Carl"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Heckerman et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 2000}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Consistency of pseudolikelihood estimation of fully visible boltzmann machines", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2006\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2006}, {"title": "Fast gradient-based inference with continuous latent variable models in auxiliary form", "author": ["Kingma", "Diederik P"], "venue": "Technical report,", "citeRegEx": "Kingma and P.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and P.", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS\u20192012", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Efficient sparse coding algorithms", "author": ["Lee", "Honglak", "Battle", "Alexis", "Raina", "Rajat", "Ng", "Andrew"], "venue": "In NIPS\u201906,", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "Texture modeling with convolutional spikeand-slab RBMs and deep extensions", "author": ["Luo", "Heng", "Carrier", "Pierre Luc", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In AISTATS\u20192013,", "citeRegEx": "Luo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "Sum-product networks: A new deep architecture", "author": ["Poon", "Hoifung", "Domingos", "Pedro"], "venue": "In UAI\u20192011, Barcelona,", "citeRegEx": "Poon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2011}, {"title": "Efficient learning of sparse representations with an energybased model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "In NIPS\u20192006,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "Technical report,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A generative process for sampling contractive auto-encoders", "author": ["Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal"], "venue": "In ICML\u201912,", "citeRegEx": "Rifai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2012}, {"title": "R\u00e9seaux de neurones \u00e0 relaxation entra\u0131\u0302n\u00e9s par crit\u00e8re d\u2019autoencodeur d\u00e9bruitant", "author": ["Savard", "Fran\u00e7ois"], "venue": "Master\u2019s thesis, U. Montre\u0301al,", "citeRegEx": "Savard and Fran\u00e7ois.,? \\Q2011\\E", "shortCiteRegEx": "Savard and Fran\u00e7ois.", "year": 2011}, {"title": "Perturbation theory and finite markov chains", "author": ["Schweitzer", "Paul J"], "venue": "Journal of Applied Probability,", "citeRegEx": "Schweitzer and J.,? \\Q1968\\E", "shortCiteRegEx": "Schweitzer and J.", "year": 1968}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Seide", "Frank", "Li", "Gang", "Yu", "Dong"], "venue": "In Interspeech", "citeRegEx": "Seide et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2011}, {"title": "Learning continuous attractors in recurrent networks", "author": ["Seung", "Sebastian H"], "venue": "In NIPS\u201997,", "citeRegEx": "Seung and H.,? \\Q1998\\E", "shortCiteRegEx": "Seung and H.", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "ICML", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 13, "context": "(2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoder variants (Bengio et al.", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": ", 2006), auto-encoder variants (Bengio et al., 2007; Vincent et al., 2008), and sparse coding variants (Lee et al.", "startOffset": 31, "endOffset": 74}, {"referenceID": 28, "context": ", 2006), auto-encoder variants (Bengio et al., 2007; Vincent et al., 2008), and sparse coding variants (Lee et al.", "startOffset": 31, "endOffset": 74}, {"referenceID": 18, "context": ", 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007).", "startOffset": 36, "endOffset": 76}, {"referenceID": 21, "context": ", 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007).", "startOffset": 36, "endOffset": 76}, {"referenceID": 9, "context": "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.", "startOffset": 162, "endOffset": 220}, {"referenceID": 10, "context": "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.", "startOffset": 162, "endOffset": 220}, {"referenceID": 26, "context": "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.", "startOffset": 162, "endOffset": 220}, {"referenceID": 17, "context": ", 2011) and object recognition (Krizhevsky et al., 2012).", "startOffset": 31, "endOffset": 56}, {"referenceID": 17, "context": "The latest breakthrough in object recognition (Krizhevsky et al., 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 14, "context": ", 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al., 2012).", "startOffset": 155, "endOffset": 176}, {"referenceID": 0, "context": "Research in deep learning (see Bengio (2009) and Bengio et al.", "startOffset": 31, "endOffset": 45}, {"referenceID": 0, "context": "Research in deep learning (see Bengio (2009) and Bengio et al. (2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al.", "startOffset": 31, "endOffset": 71}, {"referenceID": 13, "context": "On the other hand, progress with deep unsupervised architectures has been slower, with the established options with a probabilistic footing being the Deep Belief Network (DBN) (Hinton et al., 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov & Hinton, 2009).", "startOffset": 176, "endOffset": 197}, {"referenceID": 11, "context": "Although single-layer unsupervised learners are fairly well developed and used to pre-train these deep models, jointly training all the layers with respect to a single unsupervised criterion remains a challenge, with a few techniques arising to reduce that difficulty (Montavon & Muller, 2012; Goodfellow et al., 2013).", "startOffset": 268, "endOffset": 318}, {"referenceID": 14, "context": "For example, one could use a convolutional architecture with max-pooling for parametric parsimony and computational efficiency, or dropout (Hinton et al., 2012) to prevent co-adaptation of hidden representations.", "startOffset": 139, "endOffset": 160}, {"referenceID": 0, "context": "We strengthen the consistency theorems introduced in Bengio et al. (2013c) by showing that the corruption distribution may be purely local, not requiring support over the whole domain of the visible variables (Section 3.", "startOffset": 53, "endOffset": 75}, {"referenceID": 0, "context": "Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al.", "startOffset": 313, "endOffset": 335}, {"referenceID": 0, "context": "Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al. (2013). The experimental results show that such a model with latent states indeed mixes better than", "startOffset": 313, "endOffset": 357}, {"referenceID": 12, "context": "6 \u2013 Dependency networks: Finally, an unexpected result falls out of the GSN theory: it allows us to provide a novel justification for dependency networks (Heckerman et al., 2000) and for the first time define a proper joint distribution between all the visible variables that is learned by such models (Section 3.", "startOffset": 154, "endOffset": 178}, {"referenceID": 0, "context": "For example consider the denoising transitions studied by Bengio et al. (2013c) and illustrated in Figure 1, where x\u0303t\u22121 is a stochastically corrupted version of xt\u22121 and we learn the denoising distribution P (x|x\u0303).", "startOffset": 58, "endOffset": 80}, {"referenceID": 0, "context": "Alain & Bengio (2013) showed that denoising autoencoders with small Gaussian corruption and squared error loss estimated the score (derivative of the log-density with respect to the input) of continuous observed random variables.", "startOffset": 8, "endOffset": 22}, {"referenceID": 0, "context": "Alain & Bengio (2013) showed that denoising autoencoders with small Gaussian corruption and squared error loss estimated the score (derivative of the log-density with respect to the input) of continuous observed random variables. More recently, Bengio et al. (2013c) generalized this to arbitrary variables (discrete, continuous or both), arbitrary corruption (not necessarily asymptotically small), and arbitrary loss function (so long as they can be seen as a loglikelihood).", "startOffset": 8, "endOffset": 267}, {"referenceID": 0, "context": "Beyond proving that P (X|X\u0303) is sufficient to reconstruct the data density, Bengio et al. (2013c) also demonstrated a method of sampling from a learned, parametrized model of the density, P\u03b8(X), by running a Markov chain that alternately adds noise using C(X\u0303|X) and denoises by sampling from the learned P\u03b8(X|X\u0303), which is trained to approximate the true P (X|X\u0303).", "startOffset": 76, "endOffset": 98}, {"referenceID": 0, "context": "The main theorem in Bengio et al. (2013c), reproduced below, requires that the Markov chain be ergodic.", "startOffset": 20, "endOffset": 42}, {"referenceID": 0, "context": "The following theorem is proven by Bengio et al. (2013c).", "startOffset": 35, "endOffset": 57}, {"referenceID": 8, "context": "A good overview of results from perturbation theory discussing stationary distributions in finite state Markov chains can be found in (Cho et al., 2000).", "startOffset": 134, "endOffset": 152}, {"referenceID": 22, "context": "The advantage of that formulation is that one can directly back-propagated the reconstruction loglikelihood logP (X1 = x0|H1 = f(X0, Z0, H0)) into all the parameters of f and g (a similar idea was independently proposed in (Kingma, 2013) and also exploited in (Rezende et al., 2014)).", "startOffset": 260, "endOffset": 282}, {"referenceID": 0, "context": "Theoretical evidence comes from Alain & Bengio (2013): when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density suffices to fully capture the joint distribution.", "startOffset": 40, "endOffset": 54}, {"referenceID": 12, "context": "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x\u2212i), where x\u2212i denotes x \\ xi, i.", "startOffset": 20, "endOffset": 44}, {"referenceID": 12, "context": "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x\u2212i), where x\u2212i denotes x \\ xi, i.e., the set of variables other than the i-th one, xi. Note that each Pi may be parametrized separately, thus not guaranteeing that there exists a joint of which they are the conditionals. Instead of the ordered pseudo-Gibbs sampler defined in Heckerman et al. (2000), which resamples each variable xi in the order x1, x2, .", "startOffset": 21, "endOffset": 399}, {"referenceID": 11, "context": "This has been exploited in order to obtain much better samples using the associated GSN Markov chain than by sampling from the corresponding DBM (Goodfellow et al., 2013).", "startOffset": 145, "endOffset": 170}, {"referenceID": 0, "context": "The proposition can be proven by immediate application of Theorem 1 from Bengio et al. (2013c) with the above definitions of the GSN.", "startOffset": 73, "endOffset": 95}, {"referenceID": 0, "context": "The proposition can be proven by immediate application of Theorem 1 from Bengio et al. (2013c) with the above definitions of the GSN. This joint stationary distribution can exist even if the conditionals are not consistent. To show that, assume that some choice of (possibly inconsistent) conditionals gives rise to a stationary distribution \u03c0. Now let us consider the set of all conditionals (not necessarily consistent) that could have given rise to that \u03c0. Clearly, the conditionals derived from \u03c0 is part of that set, but there are infinitely many others (a simple counting argument shows that the fixed point equation of \u03c0 introduces fewer constraints than the number of degrees of freedom that define the conditionals). To better understand why the ordered pseudo-Gibbs chain does not benefit from the same properties, we can consider an extended case by adding an extra component of the state X , being the index of the next variable to resample. In that case, the Markov chain associated with the ordered pseudo-Gibbs procedure would be periodic, thus violating the ergodicity assumption of the theorem. However, by introducing randomness in the choice of which variable(s) to resample next, we obtain aperiodicity and ergodicity, yielding as stationary distribution a mixture over all possible resampling orders. These results also show in a novel way (see e.g. Hyv\u00e4rinen (2006) for earlier results) that training by pseudolikelihood or generalized pseudolikelihood provides a consistent estimator of the associated joint, so long as the GSN Markov chain defined above is ergodic.", "startOffset": 73, "endOffset": 1388}, {"referenceID": 0, "context": "The proposition can be proven by immediate application of Theorem 1 from Bengio et al. (2013c) with the above definitions of the GSN. This joint stationary distribution can exist even if the conditionals are not consistent. To show that, assume that some choice of (possibly inconsistent) conditionals gives rise to a stationary distribution \u03c0. Now let us consider the set of all conditionals (not necessarily consistent) that could have given rise to that \u03c0. Clearly, the conditionals derived from \u03c0 is part of that set, but there are infinitely many others (a simple counting argument shows that the fixed point equation of \u03c0 introduces fewer constraints than the number of degrees of freedom that define the conditionals). To better understand why the ordered pseudo-Gibbs chain does not benefit from the same properties, we can consider an extended case by adding an extra component of the state X , being the index of the next variable to resample. In that case, the Markov chain associated with the ordered pseudo-Gibbs procedure would be periodic, thus violating the ergodicity assumption of the theorem. However, by introducing randomness in the choice of which variable(s) to resample next, we obtain aperiodicity and ergodicity, yielding as stationary distribution a mixture over all possible resampling orders. These results also show in a novel way (see e.g. Hyv\u00e4rinen (2006) for earlier results) that training by pseudolikelihood or generalized pseudolikelihood provides a consistent estimator of the associated joint, so long as the GSN Markov chain defined above is ergodic. This result can be applied to show that the multi-prediction deep Boltzmann machine (MPDBM) training procedure introduced by Goodfellow et al. (2013) also corresponds to a GSN.", "startOffset": 73, "endOffset": 1740}, {"referenceID": 0, "context": "An interesting source of inspiration regarding this question is a recent paper on estimating or propagating gradients through stochastic neurons (Bengio, 2013).", "startOffset": 145, "endOffset": 159}, {"referenceID": 0, "context": "The walkback algorithm was proposed in Bengio et al. (2013c) to make training of generalized denoising autoencoders (a special case of the models studied here) more efficient.", "startOffset": 39, "endOffset": 61}, {"referenceID": 0, "context": "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013b). Networks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.", "startOffset": 147, "endOffset": 169}, {"referenceID": 0, "context": "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013b). Networks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.e., the computational graph separates into separate ones for each reconstruction step in the walkback algorithm). They all have tanh hidden units and pre- and post-activation Gaussian noise of standard deviation 2, applied to all hidden layers except the first. In addition, at each step in the chain, the input (or the resampled Xt) is corrupted with salt-and-pepper noise of 40% (i.e., 40% of the pixels are corrupted, and replaced with a 0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with good results obtained after around 100 epochs. Hidden layer sizes vary between 1000 and 1500 depending on the experiments, and a learning rate of 0.25 and momentum of 0.5 were selected to approximately minimize the reconstruction negative log-likelihood. The learning rate is reduced multiplicatively by 0.99 after each epoch. Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator).", "startOffset": 147, "endOffset": 1175}, {"referenceID": 0, "context": "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013b). Networks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.e., the computational graph separates into separate ones for each reconstruction step in the walkback algorithm). They all have tanh hidden units and pre- and post-activation Gaussian noise of standard deviation 2, applied to all hidden layers except the first. In addition, at each step in the chain, the input (or the resampled Xt) is corrupted with salt-and-pepper noise of 40% (i.e., 40% of the pixels are corrupted, and replaced with a 0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with good results obtained after around 100 epochs. Hidden layer sizes vary between 1000 and 1500 depending on the experiments, and a learning rate of 0.25 and momentum of 0.5 were selected to approximately minimize the reconstruction negative log-likelihood. The learning rate is reduced multiplicatively by 0.99 after each epoch. Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator). This can be seen as an lower bound on the true log-likelihood, with the bound converging to the true likelihood as we consider more samples and appropriately set the smoothing parameter of the Parzen estimator1 Results are summarized in Table 1. The test set Parzen log-likelihood bound was not used to select among model architectures, but visual inspection of samples generated did guide the preliminary search reported here. Optimization hyper-parameters (learning rate, momentum, and learning rate reduction schedule) were selected based on the reconstruction loglikelihood training objective. The Parzen log-likelihood bound obtained with a two-layer model on MNIST is 214 (\u00b1 standard error of 1.1), while the log-likelihood bound obtained by a single-layer model (regular denoising auto-encoder, DAE in the table) is substantially worse, at -152\u00b12.2. In comparison, Bengio et al. (2013b) report a log-likelihood bound of -244\u00b154 for RBMs and 138\u00b12 for a 2-hidden layer DBN, using the same setup.", "startOffset": 147, "endOffset": 2373}, {"referenceID": 0, "context": "However, in this paper, to be consistent with the numbers given in Bengio et al. (2013b) we used a Gaussian Parzen density, which makes the numbers not comparable with the AIS loglikelihood upper bounds for binarized images reported in other papers for the same data.", "startOffset": 67, "endOffset": 89}, {"referenceID": 18, "context": "Gaussian mixture rather than a Bernoulli mixture to compute the likelihood, but we can compare with Rifai et al. (2012); Bengio et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 0, "context": "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013c). Theorem 2 requires H0 to have the same distribution as H1 (given X0) during training, and the main paper suggests a way to achieve this by initializing each training chain with H0 set to the previous value of H1 when the same example X0 was shown.", "startOffset": 147, "endOffset": 169}, {"referenceID": 7, "context": "Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator).", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "In comparison, Bengio et al. (2013c) report a log-likelihood bound of -244\u00b154 for RBMs and 138\u00b12 for a 2-hidden layer DBN, using the same setup.", "startOffset": 15, "endOffset": 37}], "year": 2014, "abstractText": "We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining. P(X)", "creator": "LaTeX with hyperref package"}}}