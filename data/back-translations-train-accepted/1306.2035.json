{"id": "1306.2035", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2013", "title": "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation", "abstract": "While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.", "histories": [["v1", "Sun, 9 Jun 2013 16:28:56 GMT  (26kb)", "http://arxiv.org/abs/1306.2035v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["martin azizyan", "aarti singh", "larry a wasserman"], "accepted": true, "id": "1306.2035"}, "pdf": {"name": "1306.2035.pdf", "metadata": {"source": "CRF", "title": "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation\u2217", "authors": ["Martin Azizyan", "Aarti Singh", "Larry Wasserman"], "emails": [], "sections": [{"heading": null, "text": "This is just one example of how the way in which statistical (and computational) methods are applied can be degraded to high dimensions. To see why a type of variable selection might be useful, several authors consider variable selection for clustering. However, there does not seem to be any theoretical results that justify the advantage of variable selection in high-dimensional settings. To see why a type of variable selection might be useful, consider clustering n subjects with a vector of genes for each subject. Typically, d is much larger than n, suggesting that statistical clustering methods work poorly."}], "references": [{"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In Learning Theory,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Arora and Kannan.,? \\Q2001\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2001}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Isotropic pca and affine-invariant clustering", "author": ["S Charles Brubaker", "Santosh S Vempala"], "venue": "In Building Bridges,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Learning mixtures of gaussians using the k-means algorithm", "author": ["Kamalika Chaudhuri", "Sanjoy Dasgupta", "Andrea Vattani"], "venue": "arXiv preprint arXiv:0912.0086,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Matrix Computations. Johns Hopkins Studies in the Mathematical Sciences", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "Pairwise variable selection for high-dimensional modelbased clustering", "author": ["Jian Guo", "Elizaveta Levina", "George Michailidis", "Ji Zhu"], "venue": null, "citeRegEx": "Guo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2010}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu and Kakade.,? \\Q2013\\E", "shortCiteRegEx": "Hsu and Kakade.", "year": 2013}, {"title": "Disentangling gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Kalai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "In Learning Theory,", "citeRegEx": "Kannan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "Concentration inequalities and model selection", "author": ["Pascal Massart"], "venue": null, "citeRegEx": "Massart.,? \\Q2007\\E", "shortCiteRegEx": "Massart.", "year": 2007}, {"title": "Penalized model-based clustering with application to variable selection", "author": ["Wei Pan", "Xiaotong Shen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Pan and Shen.,? \\Q2007\\E", "shortCiteRegEx": "Pan and Shen.", "year": 2007}, {"title": "Variable selection for model-based clustering", "author": ["Adrian E Raftery", "Nema Dean"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Raftery and Dean.,? \\Q2006\\E", "shortCiteRegEx": "Raftery and Dean.", "year": 2006}, {"title": "A two-round variant of em for gaussian mixtures", "author": ["Leonard J. Schulman", "Sanjoy Dasgupta"], "venue": "In Proc. 16th UAI (Conference on Uncertainty in Artificial Intelligence),", "citeRegEx": "Schulman and Dasgupta.,? \\Q2000\\E", "shortCiteRegEx": "Schulman and Dasgupta.", "year": 2000}, {"title": "Regularized k-means clustering of high-dimensional data and its asymptotic consistency", "author": ["Wei Sun", "Junhui Wang", "Yixin Fang"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Sun et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Introduction to Nonparametric Estimation", "author": ["A.B. Tsybakov"], "venue": null, "citeRegEx": "Tsybakov.,? \\Q2009\\E", "shortCiteRegEx": "Tsybakov.", "year": 2009}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vempala and Wang.,? \\Q2004\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "Minimax sparse principal subspace estimation in high dimensions", "author": ["Vincent Q Vu", "Jing Lei"], "venue": "arXiv preprint arXiv:1211.0373,", "citeRegEx": "Vu and Lei.,? \\Q2012\\E", "shortCiteRegEx": "Vu and Lei.", "year": 2012}, {"title": "A framework for feature selection in clustering", "author": ["Daniela M Witten", "Robert Tibshirani"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Witten and Tibshirani.,? \\Q2010\\E", "shortCiteRegEx": "Witten and Tibshirani.", "year": 2010}, {"title": "2, where \u03c1 denotes the Hamming distance between two vectors (Tsybakov", "author": [], "venue": "Let \u03a9 = {\u03c9 \u2208 {0,", "citeRegEx": "\u2265,? \\Q2009\\E", "shortCiteRegEx": "\u2265", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001).", "startOffset": 34, "endOffset": 50}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001).", "startOffset": 34, "endOffset": 80}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension.", "startOffset": 84, "endOffset": 108}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be \u221a d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension.", "startOffset": 84, "endOffset": 312}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be \u221a d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = \u03a9(d log d) samples.", "startOffset": 84, "endOffset": 551}, {"referenceID": 0, "context": "Pairwise methods are developed in Dasgupta (1999), Schulman and Dasgupta (2000) and Arora and Kannan (2001). These methods require the mean separation to increase with dimension. The first one requires the separation to be \u221a d while the latter two improve it to d. To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. The assumption that the components are spherical was removed in Brubaker and Vempala (2008). Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = \u03a9(d log d) samples. Other spectral methods include Kannan et al. (2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013).", "startOffset": 84, "endOffset": 744}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013).", "startOffset": 8, "endOffset": 39}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm.", "startOffset": 8, "endOffset": 65}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components.", "startOffset": 8, "endOffset": 203}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al.", "startOffset": 8, "endOffset": 376}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians.", "startOffset": 8, "endOffset": 401}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed.", "startOffset": 8, "endOffset": 547}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously.", "startOffset": 8, "endOffset": 2197}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering.", "startOffset": 8, "endOffset": 2311}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al.", "startOffset": 8, "endOffset": 2410}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al.", "startOffset": 8, "endOffset": 2429}, {"referenceID": 0, "context": "(2005), Achlioptas and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions together with the method of moments to derive an effective algorithm. Kalai et al. (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. A similar approach is given in Belkin and Sinha (2010). Chaudhuri et al. (2009) give a modified k-means algorithm for estimating a mixture of two Gaussians. For the large mean separation setting \u03bc > 1, Chaudhuri et al. (2009) show that n = \u03a9\u0303(d/\u03bc) samples are needed. They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in d and \u03bc. When the mean separation is small \u03bc < 1, they show that n = \u03a9\u0303(d/\u03bc) samples are sufficient for accurate estimation. Our results for the small mean separation setting provide a matching necessary condition. Most of these papers are concerned with computational efficiency and do not give precise, statistical minimax upper and lower bounds. None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. In this paper we use the probability of misclassifying a future observation, relative to how the correct distribution clusters the observation, as our loss function. This should not be confused with the probability of attributing a new observation to the wrong component of the mixture. The latter loss does not to tend to zero as the sample size increases. Our loss is similar to the excess risk used in classification where we compare the misclassification rate of a classifier to the misclassification rate of the Bayes optimal classifier. Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. There are some relevant recent papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010). The applied bioinformatics literature also contains a huge number of heuristic methods for this problem.", "startOffset": 8, "endOffset": 2451}, {"referenceID": 18, "context": "We conjecture that the lower bound is tight and that the gap could be closed by using a sparse principal component method as in Vu and Lei (2012) to find the relevant features.", "startOffset": 128, "endOffset": 146}, {"referenceID": 16, "context": ", 0), \u03c1(\u03c9i, \u03c9j) \u2265 m8 for all 0 \u2264 i < j \u2264 M , and M \u2265 2, where \u03c1 denotes the Hamming distance between two vectors (Tsybakov (2009)).", "startOffset": 114, "endOffset": 130}, {"referenceID": 11, "context": ", \u03c9M \u2208 \u03a9 such that \u03c1(\u03c9i, \u03c9j) > s/2 for all 0 \u2264 i < j \u2264 M , and log(M + 1) \u2265 s 5 log ( m s ) (Massart (2007), Lemma 4.", "startOffset": 93, "endOffset": 108}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.", "startOffset": 25, "endOffset": 75}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al.", "startOffset": 25, "endOffset": 100}, {"referenceID": 11, "context": "Existing methods such as Pan and Shen (2007); Witten and Tibshirani (2010); Raftery and Dean (2006); Sun et al. (2012) and Guo et al.", "startOffset": 25, "endOffset": 119}, {"referenceID": 7, "context": "(2012) and Guo et al. (2010) provide promising numerical evidence that variable selection does improve high dimensional clustering.", "startOffset": 11, "endOffset": 29}, {"referenceID": 16, "context": "(Tsybakov (2009)).", "startOffset": 1, "endOffset": 17}, {"referenceID": 16, "context": "and M \u2265 2, where \u03c1 denotes the Hamming distance between two vectors (Tsybakov (2009)).", "startOffset": 69, "endOffset": 85}, {"referenceID": 11, "context": "In particular, setting \u03b1 = 3/4 and \u03b2 = 1/3, we have that \u03c1(\u03c9i, \u03c9j) > s/2, log(M + 1) \u2265 s 5 log ( m s ) , as long as s \u2264 m/4 (Massart (2007), Lemma 4.", "startOffset": 125, "endOffset": 140}], "year": 2013, "abstractText": "While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.", "creator": "LaTeX with hyperref package"}}}