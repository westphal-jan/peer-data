{"id": "1306.2685", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2013", "title": "Flexible sampling of discrete data correlations without the marginal distributions", "abstract": "Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size.", "histories": [["v1", "Wed, 12 Jun 2013 01:13:46 GMT  (2997kb,AD)", "http://arxiv.org/abs/1306.2685v1", null], ["v2", "Thu, 8 Aug 2013 18:23:45 GMT  (3495kb,AD)", "http://arxiv.org/abs/1306.2685v2", "An overhauled version of the experimental section has been added in the supplementary material"], ["v3", "Thu, 14 Nov 2013 15:31:46 GMT  (3356kb,AD)", "http://arxiv.org/abs/1306.2685v3", "An overhauled version of the experimental section moved to the main paper. Old experimental section moved to supplementary material"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO", "authors": ["alfredo a kalaitzis", "ricardo bezerra de andrade e silva"], "accepted": true, "id": "1306.2685"}, "pdf": {"name": "1306.2685.pdf", "metadata": {"source": "CRF", "title": "Flexible Sampling for the Gaussian Copula Extended Rank Likelihood Model", "authors": ["Alfredo Kalaitzis", "Ricardo Silva"], "emails": ["a.kalaitzis@ucl.ac.uk", "ricardo@stats.ucl.ac.uk"], "sections": [{"heading": "1 Contribution", "text": "In fact, the fact is that most of us are able to survive ourselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world. \"He stressed:\" I don't think we will be able to change the world. \"He stressed:\" I don't think we will be able to change the world. \"He stressed:\" I don't think we will be able to change the world, and that we will be able to change the world. \""}, {"heading": "2 Gaussian Copulas and the Extended Rank Likelihood", "text": "It is not difficult to see that any multivariate Gaussian copula is fully defined by a correlation matrix C (since marginal distributions have no free parameters). In practice, the following equivalent generative model is used to define a sample U according to a Gaussian copula GC (C): 1. Sample Z from a zero mean Gaussian with covariance matrix C 2. To obtain a model for variables {y1, y2, yp} with marginal distributions Fj (\u00b7) and copula GC (C), each Uj follows a uniform distribution in [0, 1]. To obtain a model for variables {y1, y2, yp} with marginal distributions Fj (\u00b7) and copula GC (C), one can add the deterministic level yj = F \u2212 1j (uj)."}, {"heading": "3 Exact HMC for truncated Gaussian distributions", "text": "Hoffs algorithm modifies the positions of all Z (i) j associated with a certain discrete value of Yj, conditioned by the remaining points. As the number of data points increases, the spread of the hard boundaries of Z (i) j, given by data points of Zj in conjunction with other planes of Yj, decreases the space in which variables Z (i) j can move over time. To improve mixing, we try to derive from the common Gaussian distribution of all latent variables Z (i) j, i = 1. n, conditioned by other columns of data, so that the constraints between them are satisfied, thus maintaining order at the observation level. Standard Gibbs approaches to sampling from truncated Gaussian variables reduce the problem to sampling from normal truncated Gaussies."}, {"heading": "3.1 Hamiltonian Monte Carlo (HMC) for the Gaussian Distribution", "text": "Hamilton's Monte Carlo Method [14] is an MCMC method that adds auxiliary variables to the sampling space so that (ideally) deterministic movements in the articular space bring the sampling space to potentially distant locations in the original variable space. Deterministic movements generally cannot be performed, but this is possible in the Gaussian case.1Initially, samples are taken from an isotropic Gaussian space, and in the end the Gibbs samples are transformed back into the original space.The shape of the Hamilton case for the general d-dimensional Gaussian case is medium \u00b5 and precision matrix M: H = 1 2 x \u2212 Mx \u2212 r > x + 2 s > M \u2212 1s, (3) where M is also known as mass matrix in the current context, r = M\u00b5 and s is the velocity velocity velocity. Both x and s are distributed Gaussian x, so that this Hamiltonian position can be regarded as the negative product of two independent equilibriums."}, {"heading": "3.2 Sampling with linear constraints", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "4 HMC for the Gaussian Copula Extended Rank Likelihood Model", "text": "There are some discrete data Y-Rn-p, which is the task that we have made our own in order to examine the underlying Gaussize. (It is the only one that we are able to stay in the world.) It is the only one that we are able to stay in the world. (It is the only one that we are able to stay in the world. (It is the only one that we are able to stay in the world, in which we are able to stay in the world.) It is the only one that we are able to live in the world. (It is the only one that we are able to live in the world. (It is the only one that we are able to stay in the world, in which we are able to stay in the world, in which we are able to stay in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in the world, in which we are able to live in which we are able to live in the world, in the world, in which we are able to live in which we are able to live in the world, in the world, in which we are able to live in which we are able to live in the world, in the world, in the world, in which we are able to live in the world, in which we"}, {"heading": "5 Illustration", "text": "Theoretically, only a mixture of improvements to the algorithm outlined by Hoff is to be expected, and we do not believe that the calculation costs of HMC with ranking probability limitations can be reduced by more than one constant factor at worst. Empirically, however, it is not obvious how these theoretical advantages will be reflected in practice, since the HMC method has a considerable overhead per iteration, nor is it clear how the better mixing will really pay off. We are conducting a computational experiment to highlight the strengths and weaknesses of the proposed sampler over the simple but potentially effective Hoff algorithm. The desirable statistical properties of the extended ranking probability are discussed in detail elsewhere in the given references, and as such, we will focus exclusively on the computational aspects of inference."}, {"heading": "5.1 Setup", "text": "We generate synthetic data from a 10-dimensional multivariate binary distribution with a complete, randomly selected copula correlation matrix. A data set of 10,000 samples is used in the next section as a detailed case study and provides a typical scenario. Posterior inference is done using an inverse wishart before with 12 degrees of freedom and an identity matrix scaled by 12 as in [7], and defining the implicit previous correlation matrix via the copula correlation matrix by standardizing the matrices given by that previous one. We focus on binary models because this allows us to assess the effect of a single boundary and how exactly that affects a barrier Hoffs algorithm. We focus on complete correlation matrices instead of sparse inverse matrices [2] or low-level decompositions [13] because this allows us to minimize the intermization of our other evaluation properties and parameters."}, {"heading": "5.2 Illustrative Result and Analysis", "text": "In order to achieve a reasonable runtime for our HMC implementation7, we reduced our travel time to the relatively small \u03c0 / 100. Figure 3 summarizes the most important results with some decisions made by copula7Written in unoptimized MATLAB code, which is believed to significantly slow down the process due to long loops of jumping steps that could be handled much better in a compiled language. Note that in our implementation, the cost of each HMC step was about 50 times higher than the cost of a Hoff step. V2xV9V1xV5V3xV10Correlation coefficient from the 10-dimensional case out. The complementary material has the full matrix. Color-coded representations of copula correlations are shown, with sample-to-sample variability for the HMC case relatively low after 30 iterations.It is clear that the need to reduce the travel time will not generally guarantee a high sample size."}, {"heading": "6 Conclusion", "text": "Simultaneous sampling of large random vectors to improve mixing is generally a very difficult problem, and therefore clever methods such as HMC or elliptical section collections [12] are needed. We expect that the method developed here will not only be useful for people with data analysis problems within the large family of Gaussian copula extended rank probability models, but that the method itself and its behavior could provide some new insights into MCMC sampling in confined spaces in general. Another direction for future work is to explore methods for elliptical copula and related possible extensions of general HMC for non-Gaussian copula models."}], "references": [{"title": "Discrete Multivariate Analysis: Theory and Practice", "author": ["Y. Bishop", "S. Fienberg", "P. Holland"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1975}, {"title": "Copula Gaussian graphical models and their application to modeling functional disability data", "author": ["A. Dobra", "A. Lenkoski"], "venue": "Annals of Applied Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Use of the Hough transformation to detect lines and curves in pictures", "author": ["R.O. Duda", "P.E. Hart"], "venue": "Communications of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1972}, {"title": "Copulas and machine learning", "author": ["G. Elidan"], "venue": "Proceedings of the Copulae in Mathematical and Quantitative Finance workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Semiparametric principal component analysis", "author": ["F. Han", "H. Liu"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Extending the rank likelihood for semiparametric copula estimation", "author": ["P. Hoff"], "venue": "Annals of Applied Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "On the identification of the convex hull of a finite set of points in the plane", "author": ["R. Jarvis"], "venue": "Information Processing Letters,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1973}, {"title": "Multivariate Models and Dependence Concepts", "author": ["H. Joe"], "venue": "Chapman-Hall,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Learning with tree-averaged densities and distributions", "author": ["S. Kirshner"], "venue": "Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Elliptical slice sampling", "author": ["I. Murray", "R. Adams", "D. MacKay"], "venue": "JMLR Workshop and Conference Proceedings: AISTATS 2010,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Bayesian Gaussian copula factor models for mixed data", "author": ["J. Murray", "D. Dunson", "L. Carin", "J. Lucas"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "MCMC using Hamiltonian dynamics", "author": ["R. Neal"], "venue": "Handbook of Markov Chain Monte Carlo,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "An Introduction to Copulas", "author": ["R. Nelsen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Exact Hamiltonian Monte Carlo for truncated multivariate Gaussians", "author": ["A. Pakman", "L. Paninski"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Gaussian Processes for Machine Learning", "author": ["C. Rasmussen", "C. Williams"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6].", "startOffset": 188, "endOffset": 192}, {"referenceID": 5, "context": "There are many ways of constructing multivariate discrete distributions: from full contingency tables in the small dimensional case [1], to structured models given by sparsity constraints [11] and (hierarchies of) latent variable models [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 14, "context": "More recently, the idea of copula modeling [15] has been combined with such standard building blocks.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Our contribution is a novel algorithm for efficient Markov chain Monte Carlo (MCMC) for the copula framework introduced by [7], extending algorithmic ideas introduced by [16].", "startOffset": 123, "endOffset": 126}, {"referenceID": 15, "context": "Our contribution is a novel algorithm for efficient Markov chain Monte Carlo (MCMC) for the copula framework introduced by [7], extending algorithmic ideas introduced by [16].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "A copula is a continuous cumulative distribution function (CDF) with uniformly distributed univariate marginals in the unit interval [0, 1].", "startOffset": 133, "endOffset": 139}, {"referenceID": 14, "context": "For discrete distributions, this decomposition is not unique but still well-defined [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "For a recent discussion on the applications of copulas from a machine learning perspective, [4] provides an overview.", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "[10] is an early reference in machine learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The core idea dates back at least to the 1950s [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "It is not hard to check this breaks down in the discrete case [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 16, "context": "Readers familiar with probit models will recognize the similarities to models where an underlying latent Gaussian field is discretized into observable integers as in Gaussian process classifiers and ordinal regression [17].", "startOffset": 218, "endOffset": 222}, {"referenceID": 6, "context": "In what follows, we describe in Section 2 the Gaussian copula and the general framework for constructing Bayesian estimators of Gaussian copulas by [7], the extended rank likelihood framework.", "startOffset": 148, "endOffset": 151}, {"referenceID": 15, "context": "A recent general approach for MCMC in constrained Gaussian fields by [16] can in principle be directly applied to this problem as a blackbox, but at a cost that scales quadratically in sample size and as such it is not practical in general.", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "It is clear that each Uj follows a uniform distribution in [0, 1].", "startOffset": 59, "endOffset": 65}, {"referenceID": 6, "context": "Based on this observation, [7] considers the event Z \u2208 D, where D is the set of values of Z in Rn\u00d7p obeying those constraints, that is Z \u2208 Rn\u00d7p : max{z j s.", "startOffset": 27, "endOffset": 30}, {"referenceID": 12, "context": "Nevertheless, it is possible to show that under some mild conditions that there is information in the extended rank likelihood to consistently estimate C [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 6, "context": ", to understand relationships between social indicators as in [7] and [13]) and copula-based dimensionality reduction (a generalization of correlation-based principal component analysis, e.", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": ", to understand relationships between social indicators as in [7] and [13]) and copula-based dimensionality reduction (a generalization of correlation-based principal component analysis, e.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": ", [5]); second, MCMC inference in the extended rank likelihood is considerably simpler than with the joint likelihood, since dropping marginal models will remove complicated entanglements between C and \u03b8F .", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "The standard model with a full correlation matrix C can be further refined to take into account structure implied by sparse inverse correlation matrices [2] or low rank decompositions via higher-order latent variable models [13], among others.", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "The standard model with a full correlation matrix C can be further refined to take into account structure implied by sparse inverse correlation matrices [2] or low rank decompositions via higher-order latent variable models [13], among others.", "startOffset": 224, "endOffset": 228}, {"referenceID": 15, "context": "In the following, we briefly describe the methodology recently introduced by [16] that deals with the problem of sampling from log(x) \u221d \u2212 12x >Mx + r>x , where x, r \u2208 R and M is positive definite, with linear constraints of the form f> j x \u2264 gj , where fj \u2208 R, j = 1 .", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Hamiltonian Monte Carlo [14] is a MCMC method that extends the sampling space with auxiliary variables so that (ideally) deterministic moves in the joint space brings the sampler to potentially far places in the original variable space.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "It can be easily shown that the Markov chain of sampled positions has the desired equilibrium distribution N ( \u03bc,M\u22121 ) [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "We also point the reader to [16] for a more detailed discussion of this implementation.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "A representation known in image processing as the Hough transform [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Thus, this search strategy takes O(n) time in practice to complete, mirroring the analysis of other output-sensitive algorithms such as the gift wrapping algorithm for computing convex hulls [8].", "startOffset": 191, "endOffset": 194}, {"referenceID": 6, "context": "Posterior inference is done using a inverse Wishart prior with 12 degrees of freedom and a identity matrix scaled by 12, as in [7], and defining the implied prior over the copula correlation matrix by standardization of matrices given by this prior.", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "We focus on full correlation matrices instead of sparse inverse matrices [2] or low-rank decompositions [13] because this allows us a minimal interference of the mixing properties of other parameters and latent variables on our evaluation.", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "We focus on full correlation matrices instead of sparse inverse matrices [2] or low-rank decompositions [13] because this allows us a minimal interference of the mixing properties of other parameters and latent variables on our evaluation.", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "While the plain full correlation model might have other simple ways of being initialized, this might not be clear with other extended rank likelihood models [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 11, "context": "Sampling large random vectors simultaneously in order to improve mixing is in general a very hard problem, and this is why clever methods such as HMC or elliptical slice sampling [12] are necessary.", "startOffset": 179, "endOffset": 183}], "year": 2017, "abstractText": "Learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. More recently, the framework of copula modeling has gained popularity due to its modular parameterization of joint distributions. Among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. More radically, the extended rank likelihood approach of Hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. The main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. Inference is typically done in a Bayesian framework with Gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increase quadratically with the number of data points. The result is slow mixing when using off-the-shelf Gibbs sampling. We present an efficient algorithm based on recent advances on constrained Hamiltonian Markov chain Monte Carlo that is simple to implement and does not require paying for a quadratic cost in sample size.", "creator": "LaTeX with hyperref package"}}}