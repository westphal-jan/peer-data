{"id": "1306.3203", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2013", "title": "Bregman Alternating Direction Method of Multipliers", "abstract": "The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman di- vergence to replace squared Euclidean distance as a proximal function. In this paper, we simi- larly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM (BADMM), which uses Bregman divergences as proximal functions in updates. BADMM allows the use of different Bregman divergences for different variable updates and involves alternating MDA-style updates, including alternating additive and alternating multiplicative updates as special cases. BADMM provides a unified framework for ADMM and its variants, including generalized ADMM and inexact ADMM. We establish the global convergence for BADMM. We present promising preliminary empirical results for BADMM applied to opti- mization over doubly stochastic matrices.", "histories": [["v1", "Thu, 13 Jun 2013 19:22:16 GMT  (254kb)", "http://arxiv.org/abs/1306.3203v1", null], ["v2", "Tue, 1 Jul 2014 05:57:36 GMT  (325kb)", "http://arxiv.org/abs/1306.3203v2", null], ["v3", "Tue, 8 Jul 2014 03:55:36 GMT  (625kb)", "http://arxiv.org/abs/1306.3203v3", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["huahua wang", "arindam banerjee"], "accepted": true, "id": "1306.3203"}, "pdf": {"name": "1306.3203.pdf", "metadata": {"source": "CRF", "title": "Bregman Alternating Direction Method of Multipliers", "authors": ["Huahua Wang", "Arindam Banerjee"], "emails": ["huwang@cs.umn.edu", "banerjee@cs.umn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 6,32 03v1 [m. ath. OK] 1 3Ju n"}, {"heading": "1 Introduction", "text": "In the last few years we have been dealing with a number of problems that affect themselves in the way in which they act in the way in which they act in the way in which they have developed in recent years: in the way in which they are able to behave in the way in which they act in the way in which they are able in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in the way in which they act in which they act in the way in which they act in the way in which they act in which they act in the way in which they act in the way in which they act in which they act in the way in which they act in the way in which they act in which they act in the way in which they act in the way in which they act in the way in which they act in which they act in which they act in the way in which they act in which they act in the way in which they act in which they act in which they act in the way in which they act in which they act in the way in which they act in the way in which they act in which they act in which they act in which they act in the way in which they act in the way in which they act in which they act in which they act in which they act in which they act in which they act in which they act in"}, {"heading": "2 Bregman Alternating Direction Method of Multipliers", "text": "cf. let us know that we have the quadratic function in the relative interior of a convex sentence (cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf.. cf.. cf.. cf.. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf."}, {"heading": "2.1 Scenario 1: Exact BADMM Update", "text": "If \u03c1x = \u03c1z = 0, BADMM simply uses a single Bregman divergence to replace the square penalty term in the ADMM. This scenario is particularly useful if a single Bregman divergence \u03c6 can produce efficient algorithms for both the x and z updates. In a special case, such as consensus optimization [4], if A = \u2212 I, B = I, c = 0, (7) results in text + 1 = argmin x Xf (x) + < yt, \u2212 x + zt > + \u03c1B\u03c6 (x, zt). (10) This special case is similar to case 2 in scenario 2. If f is a linear function and X is the unit simplex, we have multiplicative updates when using the KL divergence. If the z update is also a multiplicative update, we have alternating multiplicative updates. In Section 4, we will double stocking the minimization of this scenario."}, {"heading": "2.2 Scenario 2: Inexact BADMM Update", "text": "This scenario is particularly useful when it is expensive to solve the x problem exactly in scenario 1. (<) Instead, we solve the x problem inaccurately by adding another Bregman divergence Bx. (<) Since there are two Bregman divergences that are easy to solve and no assumptions about A and B. Bregman divergence Bx can be selected in such a way that (7) an efficient solution can be found. (In the following three specific cases, we will show that the x update can be efficiently solved by linearising some terms, say that Bx defines the Bregman divergence to x. We denote the sum of \u03b2xBx and Bx as the new Bregman divergence Bx."}, {"heading": "3 Convergence Analysis of BADMM", "text": "In this section, we will first discuss the assumptions required in the convergence analysis of BADMM. Then, we will determine the global convergence for BADMM (29). Finally, we will show O (1 / T) convergence rate for the objective and remaining equality condition. \u2212 \u2212 \u2212 We need the following assumption in determining the convergence of BADMM: Assumption 1 (a) f: Rn1 \u2192 R convergence for the objective and remaining equality condition. \u2212 R convergence (b) An optimal solution exists. (c) The Bregman divergence B\u03c6 is defined on a strongly convex function. \u2212 R convergence in relation to a p-standard. \u2212 Bn2 \u2212 R convergence is closed, correct and convergex5. (b) The optimal solution exists. (c) Bregman divergence is strongly defined on a p-\u03b1 function."}, {"heading": "4 Application: Doubly Stochastic Matrices", "text": "In this section we consider the problem of minimizing a loss function in a double stochastic matrix (1). < p > p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p (1) p) p (1) p (1) p (1) p) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p) p (1) p (1) p) p \"p (1) p\" p \"p\" p \"p\" p \"p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p) p (1) p (1) p) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p) p (1) p (1) p (1) p) p (1) p) p (1 p) p) p (1 p) p (1) p) p (1) p (1) p) p (1) p) p) p) p (1 p) p) p) p (1 p) p) p) p) p (1 p (1 p) p) p) p (1 p (1 p) p (1) p (1 p) p) p) p (1 p (1) p) p (1 p) p (1 p) p) p (1 p) p) p (1 p) p) p (1 p) p"}, {"heading": "5 Conclusions", "text": "In this thesis, we have generalized the alternating direction of multipliers (ADMM) to Bregman ADMM, much like mirror descent generalizes gradient descent. BADMM defines a uniform framework for ADMM, generalized ADMM, and imprecise ADMM. BADMM behaves like an alternating proximal point method with Bregman divergence or alternating mirror descent, including alternating additive updates and alternating multiplicative updates as special cases. We illustrate the potential advantage of BADMM in optimizing double stochastic matrices. While classical approaches require projection onto the constraint set, BADMM provides a projection-free algorithm with a loop."}, {"heading": "A Convergence of BADMM with Large Step Size", "text": "Assuming that yt is limited, the following theorem requires a large increment to achieve the convergence of BADMM.Theorem 3 (Let the sequences {xt, zt, yt} of Bregman ADMM (7) - (9) and {x, z, y) (23) - (25).Let the assumption 1 hold and the sequence 2 (Dy).\u2212 If you set the sequence x = (9), we have the sequence T and the sequence T for any positive constant c1, c2, then R (t + 1) converts the sequence to zero.If you set the sequence 2 (9), we have the sequence Axe + 1 + Bzt + 1 \u2212 c 22 (1) and the sequence 2 (z)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman di-<lb>vergence to replace squared Euclidean distance as a proximal function. In this paper, we simi-<lb>larly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM<lb>(BADMM), which uses Bregman divergences as proximal functions in updates. BADMM<lb>allows the use of different Bregman divergences for different variable updates and involves<lb>alternating MDA-style updates, including alternating additive and alternating multiplicative<lb>updates as special cases. BADMM provides a unified framework for ADMM and its variants,<lb>including generalized ADMM and inexact ADMM. We establish the global convergence for<lb>BADMM. We present promising preliminary empirical results for BADMM applied to opti-<lb>mization over doubly stochastic matrices.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}