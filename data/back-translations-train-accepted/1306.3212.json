{"id": "1306.3212", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2013", "title": "Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation", "abstract": "The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to other state-of-the-art methods.", "histories": [["v1", "Thu, 13 Jun 2013 19:51:59 GMT  (160kb)", "http://arxiv.org/abs/1306.3212v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["cho-jui hsieh", "m\u00e1ty\u00e1s a sustik", "inderjit s dhillon", "pradeep ravikumar"], "accepted": true, "id": "1306.3212"}, "pdf": {"name": "1306.3212.pdf", "metadata": {"source": "CRF", "title": "Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation", "authors": ["Cho-Jui Hsieh", "M\u00e1ty\u00e1s A. Sustik", "Inderjit S. Dhillon", "Pradeep Ravikumar"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 6.32 12v1 [cs.LG] 1 3"}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Background and Related work", "text": "It is not as if this is a problem that can be written by the authors of the inverse covariance matrix as a solution to the following regularized log determination method: argmin X + tr) + tr (SX) + tr (SX) + tr (1). (2) The regulation method of the inverse covariance matrix can be written as a solution to the following regularized log determination method: argmin X + tX (SX) + tr (1). (2) The regulation method promotes thrift in the inverse covariance matrix, and therefore encourages an economical graphical structure. (2) We are looking at a generalized solution in which we apply a generalized model."}, {"heading": "3 Quadratic Approximation Method", "text": "In fact, it is a matter of a manner in which the various types of experimental series capable of combining the different types of experimental series with each other are identified and compared. (...) It is the manner in which the individual experimental series are separated from each other. (...) It is the manner in which the individual experimental series are separated from each other in the individual experimental series in the individual experimental series in the individual experimental series in the order of the sequence of the experimental series in which the sequence of the experimental sequence in the sequence of the sequence of the experimental sequence is classified in the sequence of the sequence of the experimental sequence in the sequence of the sequence of the experimental sequence in the sequence of the sequence of the experimental sequence in the sequence of the sequence of the experimental sequence in the sequence of the sequence of the experimental sequence in the sequence in the sequence of the sequence of the experimental sequence in the sequence in the sequence in the sequence of the sequence of the experimental sequence in the sequence in the sequence in the sequence of the experimental sequence in the sequence in the sequence in the sequence of the experimental sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence of the experimental sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence of the sequence of the experimental sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence of the sequence of the experimental sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence of the experimental sequence in the sequence in the sequence in the sequence in the sequence of the"}, {"heading": "3.1 Computing the Newton Direction", "text": "In order to calculate the Newton direction, we have to solve the lasso problem (7) (7). [Boyd and Vandenberghe, 2009, Chapter A.4.3], the gradient and the Hessian for g (X) = \u2212 log detX + tr (SX) = S \u2212 X \u2212 1 and 2g (X) = X \u2212 1. (11) To formulate our problem accordingly, we can verify that we have a minimum for a symmetric matrix (X \u2212 1t). (X \u2212 1t)."}, {"heading": "3.2 Computing the Step Size", "text": "Following the calculation of the Newton direction D * + D * J (X) (limited to the subset of variables J), we must find a step quantity \u03b1 (0, 1] that ensures a positive definition of the next iterate X + \u03b1D * and a sufficient reduction in the objective function.We assume the property Armijos (Bertsekas [1995], Tseng and Yun [2007]) and try step quantities \u03b1 (\u03b20, \u03b21, \u03b22, \u03b22,.} with a constant decrease in the property 0 < \u03b2 < 1 (typically \u03b2 = 0.5) until we find the smallest k N with \u03b1 = \u03b2k, so that the X + \u03b1D size is positively defined and (b) satisfies the following sufficient decrease in properties: f (X + \u03b1D)."}, {"heading": "3.3 Identifying which variables to update", "text": "In this section, we will use the stationary condition of the Gaussian MLE problem to select a subset of variables to be updated in each Newton direction. (33) We will define the free set of variables as: Xij, if the free set of variables would not be changed, if the free set of variables would not be changed. (33) We will now show that a Newton update is limited to the free set of variables. (33) We would not change any of the coordinates in this set. (30) We will show that the inner coordinate level, according to the update in (16), would not change these coordinates so that they would not change, as they do not lead to the beginning of the condition of differential points."}, {"heading": "3.4 The block-diagonal structure of X\u2217", "text": "It was recently shown by (Mazumder and Hastie [2012], Witten et al. [2011] that when the threshold covariance matrix of Eij = S (Sij, \u03bb) = character (Sij) max (| Sij | \u03bb, 0) is defined, the block diagonal structure is as follows: E = E1 0... 0 0 0 E2.. 0......... 0 0 0 0 0 Ek. This result can be extended to the case when the elements are punished differently, i.e., the quantum structure is different. Then, when Eij = S (Sij, \u03bbij) is diagonal, the quantum structure is diagonal."}, {"heading": "4 Convergence Analysis", "text": "In section 3 we presented the main ideas of our QUIC algorithm. In this section we first prove that QUIC approaches the global optimum, and then show that the convergence rate is square. Banerjee et al. [2008] showed that this result is bound for the specific case where the optimization problem (2) has a unique global optimum and the eigenvalues of the primary optimal solution (3). In the following theorem we show this result for more general cases where only the extra-diagonal elements need to be positioned. Theorem 1. There is a unique minimizer X for the optimization problem (3). Proof. According to Lemma 3, the level set U defined in (25) contains all iterates, and it is included in the compact sentence S."}, {"heading": "4.1 Convergence Guarantee", "text": "To show that QUIC is approaching the optimal solution, let us consider a more general setting of the quadratic approach algorithms: with each iteration, the iterate Yt is updated from Yt = Yt + \u03b1tDJt (Yt), where Jt is a subset of variables selected in the iteration t, DJt (Yt), the Newton direction is limited to Jt defined by (20) and is the step size selected by the Armijo rule in Section 3.2. The algorithms are summarized in Algorithm 3. Similar to the block descent of Tseng and Yun [2007], we assume that the index set Jt fulfills some kind of condition: j = 0,..., T \u2212 1Jt + j N = 1, 2,. (39) Algorithm 2: QUadratic approach method for economical learning (QUadratic approach method)."}, {"heading": "4.2 Asymptotic Convergence Rate", "text": "We assume that F & # 252; for simplicity, that F & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for & # 252; for simplicity, for simplicity, for & # 252; 252; for simplicity, for simplicity, for & # 252; for simplicity, for simplicity, for & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, for simplicity, & # 252; for simplicity, & # 252; for simplicity, for simplicity, for simplicity, & # 252; for simplicity, and # 252; for simplicity, for simplicity, & 252; for simplicity, for simplicity, for simpli"}, {"heading": "5 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Stopping condition for solving the sub-problems", "text": "In the convergence analysis of section 4, we assumed that each Newton direction Dt is calculated exactly by solving the lasso partial problem (20). In our implementation, we use an iterative solver to calculate Dt, which, after a finite series of iterations, solves the problem only with a certain accuracy. In the first experiment, we investigate how the different accuracy with which we compute the Newton direction affects the overall performance. In Figure 1, we plot the total running times for the ER biology dataset [Li and Toh, 2010] correspond to different numbers of internal iterations that converge in the coordinate descendant of QUIC. We can observe that QUIC converges faster with an internal iteration at the beginning, but ultimately only achieves a linear convergence rate, while QUIC converses more slowly with 20 internal iterations at the beginning, but ultimately achieves quadratic convergence. On the basis of this observation, we make an adaptation condition for the number of convergence, where we set the IC is the beginning."}, {"heading": "5.2 Comparisons with other methods", "text": "In this section, we compare the performance of QUIC on both synthetic and real data sets with other state-of-the-art methods. We implemented QUIC in C + +, and all ex-experiments were performed on 2.83GHz Xeon X5440 machines with 32G RAM and Linux OS.We include the following algorithms in our comparisons: \u2022 ALM: the Alternating Linearization Method proposed by Scheinberg et al. [2010]. We use their MATLAB source code for the experiments. \u2022 ADMM: another implementation of the alternative linearization method implemented by Boyd et al al. [2012]. The Matlab code can be obtained from http: / / www.stanford.edu / ~ boyd / papers / admm /. We found that the default parameters (which we note are independent of the regulatory penalty) result in slow convergence."}, {"heading": "5.2.1 Experiments on synthetic datasets", "text": "We first compare the runtimes of the various methods on synthetic data, which we consider to be two values; we generate the following two types of graph structures for the underlying Gaussian Markov Random Fields: \u2022 Chain Graphs: The ground truth inverse covariance matrix \u03a3 - 1 is set to be \u03a3 \u2212 1i, i \u2212 1 = \u2212 0.5 and \u03a3 \u2212 1i, i = 1.25.Given the inverse covariance matrix \u03a3 \u2212 1, we draw a limited number, n = p / 2 i.i.d. samples from the corresponding GMRF distribution to simulate the high-dimensional setting (Table 1 shows the attributes of the synthetic datasets we use in timing comparisons).The dimensional varies of {1000, 4000, 10000}. For chain graphs, we choose so that the solution has (approximately) the correct number of non-zero elements."}, {"heading": "5.2.2 Experiments on real datasets", "text": "We use the real-world biology datasets pre-processed by Li and Toh [2010] to compare the performance of our method with other state-of-the-art methods. In the first set of experiments, we set the regularization parameter \u03bb to 0.5, which achieves reasonable economy for the following datasets: estrogen (p = 692), arbidopsis (p = 834), leukemia (p = 1, 225), hereditary (p = 1, 869). In Figure 3, we plot the relative error (f (Xt) \u2212 f (X)) / f (X \u0445) (on a log scale) free datasets against time in seconds. We can observe from Figure 3 that under this setting large and sparse solution - QUIC can be seen to achieve superlinear convergence, while other methods have superlinear convergence, while for most linear datasets, we can see a linear convergence set times faster than the total UIC, when we can see a higher UIC and QIC is expected."}, {"heading": "5.3 Block-diagonal structure", "text": "As discussed earlier, Mazumder and Hastie [2012], Witten et al. [2011] showed that if the threshold covariance matrix E = max (| S | \u2212 \u03bb, 0) is block diagonal, then the problem can of course be decomposed into partial problems. However, this observation was implemented in the latest version of Glasso. At the end of Section 3, we will discuss that the fixed / released selection can automatically identify the block diagonal structure of the threshold matrix, and therefore QUIC can benefit from block diagonal structure even if we do not explicitly decompose the matrix. In the following experiment, we will show that by entering sample variance S with block diagonal structure represented by E (see Section 3.4), QUIC still exceeds a Glasso diagonal structure. Furthermore, we will show that if some off-diagonal elements are included in the problem, while QUIC is still efficient due to its fixed / onal selection, it is so economical."}, {"heading": "Acknowledgements", "text": "This research was supported by NSF grants IIS-1018426 and CCF-0728879. ISD would like to thank Kim-Chuan Toh for providing data sets and the IPM code and Katya Scheinberg and Shiqian Ma for providing the ALM implementation."}], "references": [{"title": "Convergence rates of gradient methods for high-dimensional statistical recovery", "author": ["A. Agarwal", "S. Negahban", "M. Wainwright"], "venue": "In NIPS,", "citeRegEx": "Agarwal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2010}, {"title": "d\u2019Aspremont. Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data", "author": ["O. Banerjee", "L.E. Ghaoui"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Banerjee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Nonlinear programming", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas.", "year": 1995}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2009\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2009}, {"title": "Matlab scripts for alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Technical report,", "citeRegEx": "Boyd et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2012}, {"title": "First-order methods for sparse covariance selection", "author": ["A. d\u2019Aspremont", "O. Banerjee", "L. El Ghaoui"], "venue": "SIAM Journal on Matrix Analysis and its Applications,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Daubechies et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Daubechies et al\\.", "year": 2004}, {"title": "Projected subgradient methods for learning sparse Gaussians", "author": ["J. Duchi", "S. Gould", "D. Koller"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Newton\u2019s method and the Goldstein step-length rule for constrained minimization problems", "author": ["J.C. Dunn"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Dunn.,? \\Q1980\\E", "shortCiteRegEx": "Dunn.", "year": 1980}, {"title": "Pathwise coordinate optimization", "author": ["J. Friedman", "T. Hastie", "H. H\u00f6fling", "R. Tibshirani"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "URL http://dx.doi.org/10.1093/biostatistics/kxm045", "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Sparse inverse covariance matrix estimation using quadratic approximation", "author": ["C.-J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P. Ravikumar"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hsieh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2011}, {"title": "A divide-and-conquer method for sparse inverse covariance estimation", "author": ["C.-J. Hsieh", "I.S. Dhillon", "P. Ravikumar", "A. Banerjee"], "venue": "In NIPS,", "citeRegEx": "Hsieh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2012}, {"title": "Proximal Newton-type methods for convex optimization", "author": ["J.D. Lee", "Y. Sun", "M.A. Saunders"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Constrained minimization methods. U.S.S.R", "author": ["E.S. Levitin", "B.T. Polyak"], "venue": "Computational Math. and Math. Phys.,", "citeRegEx": "Levitin and Polyak.,? \\Q1966\\E", "shortCiteRegEx": "Levitin and Polyak.", "year": 1966}, {"title": "An inexact interior point method for L1-reguarlized sparse covariance selection", "author": ["L. Li", "K.-C. Toh"], "venue": "Mathematical Programming Computation,", "citeRegEx": "Li and Toh.,? \\Q2010\\E", "shortCiteRegEx": "Li and Toh.", "year": 2010}, {"title": "Smooth optimization approach for sparse covariance selection", "author": ["Z. Lu"], "venue": "SIAM Journal of Optimization,", "citeRegEx": "Lu.,? \\Q2009\\E", "shortCiteRegEx": "Lu.", "year": 2009}, {"title": "Exact covariance thresholding into connected components for large-scale graphical lasso", "author": ["R. Mazumder", "T. Hastie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mazumder and Hastie.,? \\Q2012\\E", "shortCiteRegEx": "Mazumder and Hastie.", "year": 2012}, {"title": "The group lasso for logistic regression", "author": ["L. Meier", "S. Van de Geer", "P. B\u00fchlmann"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Meier et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2008}, {"title": "Newton-like methods for sparse inverse covariance estimation", "author": ["P. Olsen", "F. Oztoprak", "J. Nocedal", "S. Rennie"], "venue": "Technical report,", "citeRegEx": "Olsen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Olsen et al\\.", "year": 2012}, {"title": "The conjugate gradient method in extremal problems. U.S.S.R", "author": ["B.T. Polyak"], "venue": "Computational Mathematics and Mathematical Physics,", "citeRegEx": "Polyak.,? \\Q1969\\E", "shortCiteRegEx": "Polyak.", "year": 1969}, {"title": "High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence", "author": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": null, "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Learning sparse Gaussian Markov networks using a greedy coordinate ascent approach", "author": ["K. Scheinberg", "I. Rish"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Scheinberg and Rish.,? \\Q2010\\E", "shortCiteRegEx": "Scheinberg and Rish.", "year": 2010}, {"title": "Sparse inverse covariance selection via alternating linearization methods", "author": ["K. Scheinberg", "S. Ma", "D. Glodfarb"], "venue": null, "citeRegEx": "Scheinberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Scheinberg et al\\.", "year": 2010}, {"title": "Graphical model structure learning with l1-regularization", "author": ["M. Schmidt"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "Schmidt.,? \\Q2010\\E", "shortCiteRegEx": "Schmidt.", "year": 2010}, {"title": "Optimizing costly functions with simple constraints: A limited-memory projected quasi-Newton algorithm", "author": ["M. Schmidt", "E. Van Den Berg", "M.P. Friedl", "K. Murphy"], "venue": "In Proc. of Conf. on Artificial Intelligence and Statistics,", "citeRegEx": "Schmidt et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "A coordinate gradient descent method for nonsmooth separable minimization", "author": ["P. Tseng", "S. Yun"], "venue": "Mathematical Programming,", "citeRegEx": "Tseng and Yun.,? \\Q2007\\E", "shortCiteRegEx": "Tseng and Yun.", "year": 2007}, {"title": "New insights and faster computations for the graphical lasso", "author": ["D.M. Witten", "J.H. Friedman", "N. Simon"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Witten et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2011}, {"title": "Coordinate descent algorithms for lasso penalized regression", "author": ["T.T. Wu", "K. Lange"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Wu and Lange.,? \\Q2008\\E", "shortCiteRegEx": "Wu and Lange.", "year": 2008}, {"title": "A comparison of optimization methods and software for large-scale L1-regularized linear classification", "author": ["G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2010}, {"title": "An improved GLMNET for L1-regularized logistic regression", "author": ["G.-X. Yuan", "C.-H. Ho", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "Model selection and estimation in the Gaussian graphical model", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "Yuan and Lin.,? \\Q2007\\E", "shortCiteRegEx": "Yuan and Lin.", "year": 2007}, {"title": "A coordinate gradient descent method for L1-regularized convex minimization", "author": ["S. Yun", "K.-C. Toh"], "venue": "Computational Optimizations and Applications,", "citeRegEx": "Yun and Toh.,? \\Q2011\\E", "shortCiteRegEx": "Yun and Toh.", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Accordingly, recent papers by Banerjee et al. [2008], Friedman et al.", "startOffset": 30, "endOffset": 53}, {"referenceID": 1, "context": "Accordingly, recent papers by Banerjee et al. [2008], Friedman et al. [2008], Yuan and Lin [2007] have proposed an estimator that minimizes the Gaussian negative log-likelihood regularized by the l1 norm of the entries (typically restricted to those on the off-diagonal) of the inverse covariance matrix, which encourages sparsity in its entries.", "startOffset": 30, "endOffset": 77}, {"referenceID": 1, "context": "Accordingly, recent papers by Banerjee et al. [2008], Friedman et al. [2008], Yuan and Lin [2007] have proposed an estimator that minimizes the Gaussian negative log-likelihood regularized by the l1 norm of the entries (typically restricted to those on the off-diagonal) of the inverse covariance matrix, which encourages sparsity in its entries.", "startOffset": 30, "endOffset": 98}, {"referenceID": 13, "context": "A preliminary version of this paper appeared in the NIPS 2011 conference [Hsieh et al., 2011].", "startOffset": 73, "endOffset": 93}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]).", "startOffset": 177, "endOffset": 199}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al.", "startOffset": 177, "endOffset": 1946}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al. [2010]), which we then solve using coordinate descent.", "startOffset": 177, "endOffset": 1970}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al. [2010]), which we then solve using coordinate descent. A key facet of our method is that we are able to reduce the computational cost of a coordinate descent update from the naive O(p2) to O(p) complexity by exploiting the structure present in the problem, and by a careful arrangement and caching of the computations. Furthermore, an Armijo-rule based step size selection rule ensures sufficient descent and positive definiteness of the intermediate iterates. Finally, we use the form of the stationary condition characterizing the optimal solution to focus the Newton direction computation on a small subset of free variables, but in a manner that preserves the strong convergence guarantees of second-order descent. We note that when the solution has a block-diagonal structure as described in Mazumder and Hastie [2012], Witten et al.", "startOffset": 177, "endOffset": 2787}, {"referenceID": 0, "context": "For such large-scale optimization problems arising from high-dimensional statistical estimation, standard optimization methods typically suffer sub-linear rates of convergence (Agarwal et al. [2010]). This would be too expensive for the Gaussian MLE problem, since the number of matrix entries scales quadratically with the number of nodes. Luckily, the log-determinant problem has special structure; the log-determinant function is strongly convex and one can thus obtain linear (i.e. geometric) rates of convergence via the state-of-the-art methods. However, even linear rates in turn become infeasible when the problem size is very large, with the number of nodes in the thousands and the number of matrix entries to be estimated in the millions. Here we ask the question: can we obtain superlinear rates of convergence for the optimization problem underlying the l1-regularized Gaussian MLE? For superlinear rates, one has to consider second-order methods which at least in part use the Hessian of the objective function. There are however some caveats to the use of such second-order methods in high-dimensional settings. First, a straightforward implementation of each second-order step would be very expensive for high-dimensional problems. Secondly, the log-determinant function in the Gaussian MLE objective acts as a barrier function for the positive definite cone. This barrier property would be lost under quadratic approximations so there is a danger that Newton-like updates will not yield positive-definite matrices, unless one explicitly enforces such a constraint in some manner. In this paper, we present QUIC (QUadratic approximation of Inverse Covariance matrices), a second-order algorithm, that solves the l1-regularized Gaussian MLE. We perform Newton steps that use iterative quadratic approximations of the Gaussian negative log-likelihood. The computation of the Newton direction is a Lasso problem (Meier et al. [2008], Friedman et al. [2010]), which we then solve using coordinate descent. A key facet of our method is that we are able to reduce the computational cost of a coordinate descent update from the naive O(p2) to O(p) complexity by exploiting the structure present in the problem, and by a careful arrangement and caching of the computations. Furthermore, an Armijo-rule based step size selection rule ensures sufficient descent and positive definiteness of the intermediate iterates. Finally, we use the form of the stationary condition characterizing the optimal solution to focus the Newton direction computation on a small subset of free variables, but in a manner that preserves the strong convergence guarantees of second-order descent. We note that when the solution has a block-diagonal structure as described in Mazumder and Hastie [2012], Witten et al. [2011], the fixed/free set selection in QUIC can automatically identify this sparsity structure and avoid updates to the off-diagonal block elements.", "startOffset": 177, "endOffset": 2809}, {"referenceID": 17, "context": "We summarize the experimental results in Section 5, where we compare the algorithm using both real data and synthetic examples from Li and Toh [2010]. We observe that our algorithm performs overwhelmingly better (quadratic instead of linear convergence) than existing solutions described in the literature.", "startOffset": 132, "endOffset": 150}, {"referenceID": 28, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al.", "startOffset": 119, "endOffset": 139}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al.", "startOffset": 140, "endOffset": 163}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al. [2008], Ravikumar et al.", "startOffset": 140, "endOffset": 187}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al. [2008], Ravikumar et al. [2011], Duchi et al.", "startOffset": 140, "endOffset": 212}, {"referenceID": 1, "context": "For further details on the background and utility of l1 regularization in the context of GMRFs, we refer the reader to Yuan and Lin [2007], Banerjee et al. [2008], Friedman et al. [2008], Ravikumar et al. [2011], Duchi et al. [2008]. Due in part to its importance, there has been an active line of work on efficient optimization methods for solving (2) and (3).", "startOffset": 140, "endOffset": 233}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso.", "startOffset": 0, "endOffset": 326}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM.", "startOffset": 0, "endOffset": 631}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM.", "startOffset": 0, "endOffset": 696}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al.", "startOffset": 0, "endOffset": 872}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package.", "startOffset": 0, "endOffset": 980}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO.", "startOffset": 0, "endOffset": 1168}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO. However, each coordinate update of SINCO has a time complexity of O(p2), which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in O(p) operations. This trick is one of the key advantages of our proposed method, QUIC. One common characteristic of the above methods is that they are first-order iterative methods that mainly use gradient information at each step. Such first-order methods have become increasingly popular in recent years for high-dimensional problems in part due to their ease of implementation, and because they require very little computation and memory at each step. The caveat is that they have at most linear rates of convergence (Bertsekas [1995]).", "startOffset": 0, "endOffset": 2101}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO. However, each coordinate update of SINCO has a time complexity of O(p2), which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in O(p) operations. This trick is one of the key advantages of our proposed method, QUIC. One common characteristic of the above methods is that they are first-order iterative methods that mainly use gradient information at each step. Such first-order methods have become increasingly popular in recent years for high-dimensional problems in part due to their ease of implementation, and because they require very little computation and memory at each step. The caveat is that they have at most linear rates of convergence (Bertsekas [1995]). To achieve superlinear convergence rates, one has to consider second-order methods, which have attracted some attention only recently for the sparse inverse covariance estimation problem. Li and Toh [2010] handle the non-smoothness of the l1 regularization in the objective function by doubling the number of variables, and solving the resulting constrained optimization problem by an inexact interior point method.", "startOffset": 0, "endOffset": 2309}, {"referenceID": 1, "context": "Banerjee et al. [2008] propose a block-coordinate descent method to solve the dual problem (4), by updating one row and column of W at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov\u2019s first order method. Friedman et al. [2008] follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso. In other approaches, the dual problem (4) is treated as a constrained optimization problem, for which Duchi et al. [2008] apply a projected subgradient method called PSM, while Lu [2009] proposes an accelerated gradient descent method called VSM. Other first-order methods have been pursued to solve the primal optimization problem (2). d\u2019Aspremont et al. [2008] apply Nesterov\u2019s first order method to (2) after smoothing the objective function; Scheinberg et al. [2010] apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the ALM software package. In Scheinberg and Rish [2010], the authors propose to directly solve the primal problem by a greedy coordinate descent method called SINCO. However, each coordinate update of SINCO has a time complexity of O(p2), which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in O(p) operations. This trick is one of the key advantages of our proposed method, QUIC. One common characteristic of the above methods is that they are first-order iterative methods that mainly use gradient information at each step. Such first-order methods have become increasingly popular in recent years for high-dimensional problems in part due to their ease of implementation, and because they require very little computation and memory at each step. The caveat is that they have at most linear rates of convergence (Bertsekas [1995]). To achieve superlinear convergence rates, one has to consider second-order methods, which have attracted some attention only recently for the sparse inverse covariance estimation problem. Li and Toh [2010] handle the non-smoothness of the l1 regularization in the objective function by doubling the number of variables, and solving the resulting constrained optimization problem by an inexact interior point method. Schmidt et al. [2009] propose a second order Projected Quasi-Newton method (PQN) that solves the dual problem (4), since the dual objective function is smooth.", "startOffset": 0, "endOffset": 2541}, {"referenceID": 13, "context": "Subsequent to the preliminary version of this paper (see [Hsieh et al., 2011]), Olsen et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 13, "context": "Subsequent to the preliminary version of this paper (see [Hsieh et al., 2011]), Olsen et al. [2012] have proposed generalizations to our framework to allow various inner solvers such as FISTA, conjugate gradient (CG), and LBFGS to be used, in addition to our proposed coordinate descent scheme.", "startOffset": 58, "endOffset": 100}, {"referenceID": 29, "context": "Following the approach of Tseng and Yun [2007] and Yun and Toh [2011], we build a quadratic approximation around any iterate Xt for this composite function by first considering the second-order Taylor expansion of the smooth component g(X):", "startOffset": 26, "endOffset": 47}, {"referenceID": 29, "context": "Following the approach of Tseng and Yun [2007] and Yun and Toh [2011], we build a quadratic approximation around any iterate Xt for this composite function by first considering the second-order Taylor expansion of the smooth component g(X):", "startOffset": 26, "endOffset": 70}, {"referenceID": 28, "context": "Note that it can be rewritten as a standard Lasso regression problem [Tibshirani, 1996]:", "startOffset": 69, "endOffset": 87}, {"referenceID": 25, "context": "This variant of Newton method for such composite objectives is also referred to as a \u201cproximal Newton-type method,\u201d and was empirically studied in Schmidt [2010]. Tseng and Yun [2007] considered the more general case where the Hessian \u2207g(Xt) is replaced by any positive definite matrix.", "startOffset": 147, "endOffset": 162}, {"referenceID": 25, "context": "This variant of Newton method for such composite objectives is also referred to as a \u201cproximal Newton-type method,\u201d and was empirically studied in Schmidt [2010]. Tseng and Yun [2007] considered the more general case where the Hessian \u2207g(Xt) is replaced by any positive definite matrix.", "startOffset": 147, "endOffset": 184}, {"referenceID": 15, "context": "See also the recent paper by Lee et al. [2012], where convergence properties of such general proximal Newton-type methods are discussed.", "startOffset": 29, "endOffset": 47}, {"referenceID": 20, "context": "Many efficient optimization methods exist that solve Lasso regression problems, such as the coordinate descent method [Meier et al., 2008], the gradient projection method [Polyak, 1969], and iterative shrinking methods [Daubechies et al.", "startOffset": 118, "endOffset": 138}, {"referenceID": 22, "context": ", 2008], the gradient projection method [Polyak, 1969], and iterative shrinking methods [Daubechies et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 10, "context": "In Friedman et al. [2007], Wu and Lange [2008], the authors show that coordinate descent methods are very efficient for solving Lasso type problems.", "startOffset": 3, "endOffset": 26}, {"referenceID": 10, "context": "In Friedman et al. [2007], Wu and Lange [2008], the authors show that coordinate descent methods are very efficient for solving Lasso type problems.", "startOffset": 3, "endOffset": 47}, {"referenceID": 3, "context": "We adopt Armijo\u2019s rule (Bertsekas [1995],Tseng and Yun [2007]) and try step-sizes \u03b1 \u2208 {\u03b20, \u03b21, \u03b22, .", "startOffset": 24, "endOffset": 41}, {"referenceID": 3, "context": "We adopt Armijo\u2019s rule (Bertsekas [1995],Tseng and Yun [2007]) and try step-sizes \u03b1 \u2208 {\u03b20, \u03b21, \u03b22, .", "startOffset": 24, "endOffset": 62}, {"referenceID": 1, "context": "We note that the conclusion of the lemma also holds if the conditions on \u039b and S are replaced by only the requirement that the diagonal elements of \u039b are positive, see Banerjee et al. [2008]. We emphasize that Lemma 3 allows the extension of the convergence results to the practically important case when the regularization does not penalize the diagonal.", "startOffset": 168, "endOffset": 191}, {"referenceID": 29, "context": "The attractive facet of this modification is that it leverages the sparsity of the solution and intermediate iterates in a manner that falls within the block coordinate descent framework of Tseng and Yun [2007]. The index sets J1, J2, .", "startOffset": 190, "endOffset": 211}, {"referenceID": 29, "context": "The attractive facet of this modification is that it leverages the sparsity of the solution and intermediate iterates in a manner that falls within the block coordinate descent framework of Tseng and Yun [2007]. The index sets J1, J2, . . . corresponding to the block coordinate descent steps in the general setting of Tseng and Yun [2007][p.", "startOffset": 190, "endOffset": 340}, {"referenceID": 32, "context": "A similar (so called shrinking) strategy is used in SVM and l1-regularized logistic regression problems as mentioned in Yuan et al. [2010]. In our experiments, we demonstrate that this strategy reduces the size of the free set very quickly.", "startOffset": 120, "endOffset": 139}, {"referenceID": 19, "context": "4 The block-diagonal structure of X It has been shown recently by (Mazumder and Hastie [2012],Witten et al.", "startOffset": 67, "endOffset": 94}, {"referenceID": 19, "context": "4 The block-diagonal structure of X It has been shown recently by (Mazumder and Hastie [2012],Witten et al. [2011]) that when the thresholded covariance matrixE defined byEij = S(Sij , \u03bb) = sign(Sij)max(|Sij |\u2212 \u03bb, 0) has the following block-diagonal structure:", "startOffset": 67, "endOffset": 115}, {"referenceID": 13, "context": "Then, if Eij = S(Sij , \u03bbij) is block diagonal, so is the solution X \u2217 of (3), see Hsieh et al. [2012]. Thus each X\u2217 i can be computed independently.", "startOffset": 82, "endOffset": 102}, {"referenceID": 1, "context": "Banerjee et al. [2008] showed that for the special case where \u039bij = \u03bb the optimization problem (2) has a unique global optimum and that the eigenvalues of the primal optimal solutionX\u2217 are bound.", "startOffset": 0, "endOffset": 23}, {"referenceID": 29, "context": "Similar to the block coordinate descent framework of Tseng and Yun [2007], we assume the index set Jt satisfies a Gauss-Seidel type of condition:", "startOffset": 53, "endOffset": 74}, {"referenceID": 15, "context": "2 Asymptotic Convergence Rate Newton methods on constrained minimization problems: The convergence rate of the Newton method on bounded constrained minimization has been studied in Levitin and Polyak [1966] and Dunn [1980].", "startOffset": 181, "endOffset": 207}, {"referenceID": 9, "context": "2 Asymptotic Convergence Rate Newton methods on constrained minimization problems: The convergence rate of the Newton method on bounded constrained minimization has been studied in Levitin and Polyak [1966] and Dunn [1980]. Here, we briefly mention their results.", "startOffset": 211, "endOffset": 223}, {"referenceID": 9, "context": "1 in Dunn [1980]).", "startOffset": 5, "endOffset": 17}, {"referenceID": 9, "context": "1 in Dunn [1980]). Assume F is strictly convex, has a unique minimizer x\u2217 in \u03a9, and that \u22072F (x) is Lipschitz continuous. Then for all x0 sufficiently close to x\u2217, the sequence {xk} generated by (43) converges quadratically to x \u2217. This theorem is proved in Dunn [1980]. In our case, the objective function f(X) is non-smooth so that Theorem 3 does not directly apply.", "startOffset": 5, "endOffset": 270}, {"referenceID": 17, "context": "In Figure 1 we plot the total run times for the ER biology dataset from [Li and Toh, 2010] correspond to different numbers of inner iterations used in the coordinate descent solver of QUIC.", "startOffset": 72, "endOffset": 90}, {"referenceID": 25, "context": "\u2022 ALM: the Alternating Linearization Method proposed by Scheinberg et al. [2010]. We use their MATLAB source code for the experiments.", "startOffset": 56, "endOffset": 81}, {"referenceID": 5, "context": "\u2022 ADMM: another implementation of the alternating linearization method implemented by Boyd et al. [2012]. The matlab code can be downloaded from http://www.", "startOffset": 86, "endOffset": 105}, {"referenceID": 9, "context": "\u2022 glasso: the block coordinate descent method proposed by Friedman et al. [2008]. We use the latest version glasso 1.", "startOffset": 58, "endOffset": 81}, {"referenceID": 8, "context": "\u2022 PSM: the Projected Subgradient Method proposed by Duchi et al. [2008]. We use the MATLAB source code available at http://www.", "startOffset": 52, "endOffset": 72}, {"referenceID": 8, "context": "\u2022 PSM: the Projected Subgradient Method proposed by Duchi et al. [2008]. We use the MATLAB source code available at http://www.cs.ubc.ca/~schmidtm/Software/PQN.html. \u2022 SINCO: the greedy coordinate descent method proposed by Scheinberg and Rish [2010]. The code can be downloaded from https://projects.", "startOffset": 52, "endOffset": 251}, {"referenceID": 17, "context": "\u2022 IPM: An inexact interior point method proposed by Li and Toh [2010]. The source code can be downloaded from http://www.", "startOffset": 52, "endOffset": 70}, {"referenceID": 17, "context": "\u2022 IPM: An inexact interior point method proposed by Li and Toh [2010]. The source code can be downloaded from http://www.math.nus.edu.sg/~mattohkc/Covsel-0.zip. \u2022 PQN: the projected quasi-Newton method proposed by Schmidt et al. [2009]. The source code can be downloaded from http://www.", "startOffset": 52, "endOffset": 236}, {"referenceID": 17, "context": "\u2022 Graphs with Random Sparsity Structures: We use the procedure mentioned in Example 1 in Li and Toh [2010] to generate inverse covariance matrices with random non-zero patterns.", "startOffset": 89, "endOffset": 107}, {"referenceID": 17, "context": "We use the real world biology datasets preprocessed by Li and Toh [2010] to compare the performance of our method with other state-of-the-art methods.", "startOffset": 55, "endOffset": 73}, {"referenceID": 19, "context": "3 Block-diagonal structure As discussed earlier, Mazumder and Hastie [2012], Witten et al.", "startOffset": 49, "endOffset": 76}, {"referenceID": 19, "context": "3 Block-diagonal structure As discussed earlier, Mazumder and Hastie [2012], Witten et al. [2011] showed that when the thresholded covariance matrix E = max(|S|\u2212\u03bb, 0) is block-diagonal, then the problem can be naturally decomposed into sub-problems.", "startOffset": 49, "endOffset": 98}], "year": 2013, "abstractText": "The l1-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton\u2019s method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to other state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}