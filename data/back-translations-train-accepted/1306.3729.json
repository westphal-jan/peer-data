{"id": "1306.3729", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2013", "title": "Spectral Experts for Estimating Mixtures of Linear Regressions", "abstract": "Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima. In this paper, we develop a new computationally efficient and provably consistent estimator for a mixture of linear regressions, a simple instance of a discriminative latent-variable model. Our approach relies on a low-rank linear regression to recover a symmetric tensor, which can be factorized into the parameters using a tensor power method. We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM).", "histories": [["v1", "Mon, 17 Jun 2013 03:02:05 GMT  (2364kb,D)", "http://arxiv.org/abs/1306.3729v1", "Accepted at ICML 2013. Includes supplementary material"]], "COMMENTS": "Accepted at ICML 2013. Includes supplementary material", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["arun tejasvi chaganty", "percy liang"], "accepted": true, "id": "1306.3729"}, "pdf": {"name": "1306.3729.pdf", "metadata": {"source": "META", "title": "Spectral Experts for Estimating Mixtures of Linear Regressions", "authors": ["Arun Tejasvi Chaganty", "Percy Liang"], "emails": ["chaganty@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": null, "text": "Last modified: June 18, 2013"}, {"heading": "1. Introduction", "text": "In fact, most people are able to decide whether they will be able to play by the rules, or whether they will be able to play by the rules, or whether they will be able to play by the rules."}, {"heading": "1.1. Notation", "text": "Let [n] s [s] s [s] s [s] s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\""}, {"heading": "2. Model", "text": "The mixture of linear regression models (Viele & Tong, 2002) defines a conditional distribution over a response y [R] given covariates x [Rd]. Let k be the number of mixture components. Generating y given x comprises three steps: (i) Draw a mixture component h [k] according to mixing ratios \u03c0 = (\u03c01,.., \u03c0k); (ii) Draw observation noise from a known zero-mean sound distribution E, and (iii) set y deterministically on the basis of h and. Compact: h multinomial (\u03c0), (1) \u0445 E, (2) y = \u03b2Th x +. (3) The parameters of the model are proportions for the mixture components."}, {"heading": "3. Spectral Experts algorithm", "text": "In this section, we describe our spectral expert algorithms for estimating model parameters. < < 3) The algorithms consist of two steps: (i) low-rank regression to estimate certain symmetric tensors; and (ii) tensor factorization to recover the parameters. < 2) The two steps can be efficiently performed using convex optimization and tensor power method. < 3) The first key step depends on the definition of the model that we use y = \u03b2 > h x +. The challenge is that the regression coefficients \u03b2h depend on the random h method. The first key step is to determine the averages of this randomness by setting averageregression coefficients M1 def = 1. Now, we can express y as a linear function of x with non-random coefficients."}, {"heading": "4. Theoretical results", "text": "In this section we provide theoretical guarantees for the algorithm of the spectral experts. Our main result shows that the parameters estimates of the composite parameters and various covariance matrices converge at a 1 \u00b0 n rate, which polynomically depends on the boundaries of the parameters, covariates and noise. Suppose that each dataset Dp (for p = 1, 2, 3) consists of n i.i.d. points, which are drawn independently of a mixture of linear regression models with parameters of other experts. Suppose that each dataset Dp (for p = 1, 2, 3) consists of n i.d. points, which are independently drawn from a mixture of linear regression models with parameters of other types."}, {"heading": "4.1. Identifiability from moments", "text": "In ordinary linear regression, the regression coefficients \u03b2-Rd are identifiable if and only if the data have the full rank: E [x '2] 0, and beyond that, identification \u03b2 requires only the moments E [xy] and E [x' 2] (by observing the optimum conditions for (4). However, these two moments only allow us to restore M1. Theorem 1 shows that if we have the analogies of higher order, E [x'py p] and E [x '2p] for p'p \"(1, 2, 3}, we can then identify the parameters \u03b8 = (\u03c0, B), provided the following identification condition applies: E [cvec\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p. \"{1, 2\" p \"p\" p. \"This identification condition justifies a little care, because we can get into difficulties when components of x\" x."}, {"heading": "4.2. Analysis of low-rank regression", "text": "In this section, we will delineate the error of the compound parameter, which is based on the low-rank regression frame of Tomioka et al. (2011) for tensors, which is based on Negahban & Wainwright (2009) for matrices, and the main calculation refers to the control of the noise-rapping range by Tomioka et al. (2011) includes various polynomic combinations of the mixing noise. First, let us define a notation that combines the three regressions ((((8), (9) and (10)), and define the observation operator Xp (Mp): Rd'p \u2192 Rn mapping-compound parameter Mp: Xp (Mp; D) i def = < Mp >, (Lemp) and the strong qualities."}, {"heading": "4.3. Analysis of the tensor factorization", "text": "After limiting the error in estimating the composition parameters M-2 and M-3, we will now investigate how this error spreads through the tensor factorization step of algorithm 1, which involves brightening, applying the robust tensor power method (Anandkumar et al., 2012c), and whitening. Lemma 4. Let M-3 = 1-h\u03b2-3 h. Let M-2-M2-op and M-3-M3-op both be smaller than T-k (M2) 5 / 2kp-M2-1 / 2p-M3-op, for some < 12. Then there is a permutation of the indices, so that the parameters found in step 2 of algorithm 1 fulfill the following with a probability of at least 1-2:"}, {"heading": "4.4. Synthesis", "text": "By using Lemma 1, Lemma 2, and Lemma 3, we can control the Frobenius norm of error at the moments that the operator norm limits directly upwards: If n \u2265 max {n1, n2}, then we can control the Frobenius norm of error at the moments that the operator norm limits directly upwards: If n \u2265 max {n1, n2}, then the above limit is applied to the compound parameter error and the recovery error. (12) We complete the proof by using Lemma 4 with the above limit for the compound parameter error and the recovery error."}, {"heading": "5. Empirical evaluation", "text": "In this section, we examine the empirical properties of our algorithm using simulated data. Our main finding is that Spectral Experts alone achieves higher parameter errors than EM, but this is not the complete story. If we initialize EM with the estimates returned by Spectral Experts, we end up with much better estimates than EM from a random initialization."}, {"heading": "5.1. Experimental setup", "text": "Algorithms We experimented with three algorithms. The first algorithm (Spectral) is simply the spectral function = \u00b1 105 \u00b1 105. We set the regularization strengths \u03bb (2) n = 1 105 \u221a n and \u03bb (3) n = 1 103 \u221a n; the algorithm was not very sensitive to these decisions. We solved the low-ranking regression to estimate M2 and M3 using a standard convex optimizer, CVX (Grant & Boyd, 2012). The second algorithm (EM) is EM, where the \u03b2's are initialized by a standard standard and was set to the uniform distribution plus a few small perturbations. We performed EM for 1000 iterations \u2212. In the final algorithm (Spectral + EM) we initialized EM with the output of spectral experts.Data We created synthetic data as follows: We created a vector that we sampled uniformly, EM."}, {"heading": "5.2. Results", "text": "Table 1 represents the Frobenius standard of difference between true and estimated parameters for the model averaged over 20 different random instances for each feature and 10 attempts for each instantie.The experiments were performed with n = 500,000 samples. One of the main reasons for the high deviation is the deviation between random instances; some are easy for EM to find global minimums and others more difficult for EM to locate. In general, while spectral experts did not restore the parameters themselves extremely well, it provided a good initialization for EM.To examine the stability of the solutions returned by spectral experts, we look at the histogram in Figure 1, which shows the recovery errors of the algorithms over 170 attempts on a dataset with b = 1, d = 4, k = 3. Typically, spectral experts provided a stable solution. If these parameters were close enough to the true parameters, we found that EM was almost always based on global optimization."}, {"heading": "6. Conclusion", "text": "In this paper, we developed a computationally efficient and statistically consistent estimator for the mix of linear regressions. Our algorithm, Spectral Experts, regresses to superior powers of the data with a regulator that promotes a low ranking structure, followed by a tensor factorization to restore the actual parameters. Empirically, we found Spectral Experts to be an excellent initializer for EM.Acknowledgements We would like to thank Lester Mackey for his fruitful suggestions and the anonymous reviewers for their helpful comments."}, {"heading": "A. Proofs: Regression", "text": "We assume that data will be provided to us (xi, yi). < p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "B. Proofs: Tensor Decomposition", "text": "M3, M3 =, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2 = 2, M3 = 2, M3 = 3, M3 = 2, M3 = 2, M2 = 2, M2 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 3, M3 = 2, M3 = 3, M3 = 2, M2 = 2, M2 =, M3 = M3, M3 = 3, M3 = 2, M3 = 3, M3 = 2, M2 = 2, M3 = M3, M3 = 3, M3 = 2, M3 = 2, M2 = 2, M3 = 2, M3 = 2, M3 = 2, M2 = 2, M3 = 2, M3 = 3, M3 = 2, M3 = 3, M3 = 2, M3 = 2, M2 = 2, M2 = 2, M3, M3 = 3, M3 = 3, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3 = 2, M3, M3 =, M3 = 2, M3, M3 = 2 = 2, M3 = 2, M3, M3 = 2 = 2 =, M3, M3, M3 = 2 =, M3, M3 =, M3, M3 =, M3 = 2 =, M3, M3 = 2, M3, M3 =, M3, M3 = 2, M3 = 2 =, M3, M3 =, M3, M3, M3, M3 =, M3, M3, M3 = 2 =, M3, M3 = 2, M3, M3 =, M3, M3 =, M3, M3, M3, M3, M3, M3 ="}, {"heading": "C. Basic Lemmas", "text": "In this section we have summarized some standard results that we use for completeness. Let X, X1, \u00b7 \u00b7, Xn, \u00b7 Rd are i.i.d. samples from a distribution with limited support (1 / 2).Proof. Define Zi = Xi \u2212 E [X].The quantity we want to bind can be expressed as follows: f (Z1, Z2, \u00b7 \u00b7 \u00b7, Zn) = 2 \u2264 2M (1 / 2).Proof. Define Zi = Xi \u2212 E [X].The quantity we want to bind is as follows: f (Z1, Z2, \u00b7, Zn) = 2 \u2264 2M (1 / 2).Proof. Define Zi = Xi \u2212 E [X].The quantity we want to bind can be expressed as follows: f (Z1, Z2, \u00b7, Zn)."}], "references": [{"title": "A method of moments for mixture models and hidden Markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Anandkumar", "Anima", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "CoRR, abs/1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Spectral learning of general weighted automata via constrained matrix completion", "author": ["B. Balle", "M. Mohri"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Balle and Mohri,? \\Q2012\\E", "shortCiteRegEx": "Balle and Mohri", "year": 2012}, {"title": "A spectral learning algorithm for finite state transducers", "author": ["B. Balle", "A. Quattoni", "X. Carreras"], "venue": "In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "Balle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2011}, {"title": "Learning mixtures of spher", "author": ["D. Hsu", "S.M. Kakade"], "venue": "http://cvxr.com/cvx,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Modeling with mix", "author": ["Viele", "Kert", "Tong", "Barbara"], "venue": null, "citeRegEx": "Viele et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Viele et al\\.", "year": 2011}, {"title": "\u03b7p(x)x \u2297p, where we have used \u03b7p to represent the vector [\u03b7p(x)]x\u2208Dp", "author": ["Tomioka"], "venue": null, "citeRegEx": "Tomioka,? \\Q2011\\E", "shortCiteRegEx": "Tomioka", "year": 2011}, {"title": "2012c, Theorem 5.1) to bound the error in the eigenvalues", "author": ["Anandkumar"], "venue": null, "citeRegEx": "Anandkumar,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Anandkumar et al. (2012c) showed that for M2 and M3 of the forms in (5), it is possible to efficiently accomplish this.", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "One line of work has focused on observable operator models (Hsu et al., 2009; Song et al., 2010; Parikh et al., 2012; Cohen et al., 2012; Balle et al., 2011; Balle & Mohri, 2012) in which a re-parametrization of the true parameters are recovered, which suffices for prediction and density estimation.", "startOffset": 59, "endOffset": 178}, {"referenceID": 3, "context": "Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers).", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": "Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers).", "startOffset": 66, "endOffset": 177}, {"referenceID": 3, "context": "Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers). Similar to Spectral Experts, Balle & Mohri (2012) used a two-step approach, where convex optimization is first used to estimate moments (the Hankel matrix in their case), after which these moments are subjected to spectral decomposition.", "startOffset": 66, "endOffset": 304}, {"referenceID": 2, "context": "First, we bound the error in the compound parameters estimates M\u03022, M\u03023 using results from Tomioka et al. (2011). Then we use results from Anandkumar et al.", "startOffset": 65, "endOffset": 113}, {"referenceID": 0, "context": "Then we use results from Anandkumar et al. (2012c) to convert this error into a bound on the actual parameter estimates \u03b8\u0302 = (\u03c0\u0302, B\u0302) derived from the robust tensor power method.", "startOffset": 25, "endOffset": 51}, {"referenceID": 4, "context": "In this section, we will bound the error of the compound parameter estimates \u2016\u22062\u2016F and \u2016\u22063\u2016F , where \u22062 def = M\u03022 \u2212 M2 and \u22063 def = M\u03023 \u2212 M3. Our analysis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Negahban & Wainwright (2009) for matrices.", "startOffset": 79, "endOffset": 226}, {"referenceID": 4, "context": "In this section, we will bound the error of the compound parameter estimates \u2016\u22062\u2016F and \u2016\u22063\u2016F , where \u22062 def = M\u03022 \u2212 M2 and \u22063 def = M\u03023 \u2212 M3. Our analysis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Negahban & Wainwright (2009) for matrices.", "startOffset": 79, "endOffset": 284}, {"referenceID": 5, "context": "Lemma 1 (Tomioka et al. (2011), Theorem 1).", "startOffset": 9, "endOffset": 31}], "year": 2013, "abstractText": "Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima. In this paper, we develop a new computationally efficient and provably consistent estimator for a mixture of linear regressions, a simple instance of a discriminative latentvariable model. Our approach relies on a lowrank linear regression to recover a symmetric tensor, which can be factorized into the parameters using a tensor power method. We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM). Last Modified: June 18, 2013", "creator": "LaTeX with hyperref package"}}}