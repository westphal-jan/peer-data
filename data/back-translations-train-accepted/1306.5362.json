{"id": "1306.5362", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2013", "title": "A Statistical Perspective on Algorithmic Leveraging", "abstract": "One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method.", "histories": [["v1", "Sun, 23 Jun 2013 00:31:15 GMT  (119kb,D)", "http://arxiv.org/abs/1306.5362v1", "44 pages, 17 figures"]], "COMMENTS": "44 pages, 17 figures", "reviews": [], "SUBJECTS": "stat.ME cs.LG stat.ML", "authors": ["ping ma", "michael w mahoney", "bin yu 0001"], "accepted": true, "id": "1306.5362"}, "pdf": {"name": "1306.5362.pdf", "metadata": {"source": "CRF", "title": "A Statistical Perspective on Algorithmic Leveraging", "authors": ["Ping Ma", "Michael W. Mahoney", "Bin Yu"], "emails": ["pingma@illinois.edu.", "mmahoney@cs.stanford.edu.", "binyu@stat.berkeley.edu."], "sections": [{"heading": null, "text": "In this paper, we provide a simple but effective framework for evaluating the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. Specifically, we derive results for bias and variance for multiple versions of leverage-based sampling, both conditionally and unconditionally based on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominate the other. This result is particularly striking given the known result that from the algorithmic perspective of worst-case analysis, leverage-based sampling results dominate compared to uniform sampling. Based on these theoretical results, we propose two new leveraging algorithms and analyze them: One constructs a smaller leveraging squares problem with \"shrunk\" leverage-weighted sampling or other (LEV) problem with smaller and weighted sampling."}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to feel how they are, to behave as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves, as if they were able to survive themselves."}, {"heading": "2 Background, Notation, and Related Work", "text": "In this section, we will give a brief overview of relevant background information, including our notation for linear models, an overview of the algorithmic leveraging approach, and an overview of related work in statistics and computer science."}, {"heading": "2.1 Least-squares and Linear Models", "text": "We start with the relevant background and the notation, whose columns contain the singular values of the singular values. (...) We start with the relevant background and the notation. (...) Since a n \u00b7 p matrix X and a n-dimensional vector y = \u03b2 = \u03b2 = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S"}, {"heading": "2.2 Algorithmic Leveraging for Least-squares Approximation", "text": "\"Here we will review relevant work on random sample algorithms for the calculation of approximate solutions to the general excessive LS problem (11, 27, 10). These algorithms choose (generally, inconsistently) a partial sample of the data, e.g. a small number of rows of X and the corresponding elements of y, and then perform (typically weighted) LS on the partial sample. These algorithms do not make assumptions about the input dataX and y, except that n p.A is a prototype example of this approach given by the following meta algorithms [11, 27, 10] which we call subsampleLS, and which takes as input a n \u00d7 p matrix X, where n p, a vector y and a probability distribution {2} ni = 1, and which as output represents an approximate solution \u03b2-ols."}, {"heading": "2.3 Running Time Considerations", "text": "The runtimes of these algorithms depend on both the time to construct the probability distribution and the time to solve the subsampled problem. (For UNIF, the former is trivial and the latter depends on the size of the subproblem.) For estimators who depend on the exact or approximate size of the subproblem (remember the flexibility in Eqn. (7) provided by \u03b3) leverage scores, runtime is dominated by the exact or approximate calculation of these scores. A na ve algorithm involves using a QR decomposition or the thin SVD of X to obtain the exact leverage scores. Unfortunately, this exact algorithm O (np2) takes time and is therefore no faster than solving the original LS problem. Of greater interest is the algorithm of [10], which calculates relative error approximations to all the leverage scores of X."}, {"heading": "2.4 Additional Related Work", "text": "Our lever-based methods for estimating \u03b2 are related to resampling methods such as the bootstrap [13], and many of these resampling methods have desirable asymptotic properties [40]. Resampling methods in linear models have been extensively studied in [43] and are related to the jackknife [31, 32, 22, 14]. They produce resamples of similar size to the complete data, whereas algorithmic leveraging is primarily interested in constructing partial problems that are much smaller than the complete data. Moreover, the goal of resampling has traditionally been to perform statistical inferences and not improve the runtime of an algorithm, except in the most recent work [23]. Other related work in statistics include [20, 37, 26, 4, 36]."}, {"heading": "3 Bias and Variance Analysis of Subsampling Estimators", "text": "In this section, we will develop analytical methods to investigate the bias and variances of the samples described in Section 2.2. Analyzing these samples is difficult for at least two reasons: firstly, there are two layers of randomness in the estimators, i.e., the randomness inherent in the linear regression model and the random sample of a particular sample from the linear model; and secondly, the estimators of random samples depend on the inversion of the random sample, which is a nonlinear function. To simplify the analysis, we will analyze a Taylor series to consider the estimators \"substance samples as linear combinations of random matrices, and we will look at presets and variances that are both conditioned and not conditioned."}, {"heading": "3.1 Traditional Weighted Sampling Estimators", "text": "\"We begin with the weighing and variance of the same distribution.\" Thus, this section does not apply to the results of VUNI.W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \".W\".W \"W\".W \".W\".W \".W\" W \".W\".W \".W\" W \".W\".W \".W\" W \".W\".W \"W\".W \".W\" W \".W\".W \".W.W.W.W\" W.W.W.W \"W.W.W\" W.W.W.W \"W\" W.W.W.W.W.W. W \"W\" W.W \"W.W\" W.W.W.W. W. W \"W. W\" W \"W\" W \"W\" W.W \"W.W\" W.W. W \"W. W\" W \"W\" W \"W\".W. W. W \"W\" W. W \"W\" W \"W\" W \"W\" W \".W\".W \"W\" W \".W\" W \".W\".W \""}, {"heading": "3.2 Leverage-based Sampling and Uniform Sampling Estimators", "text": "In this case, the evidence collection of the algorithmic leveraging method may well be different from the algorithmic properties. Prior has adopted an algorithmic perspective that focuses on providing worst-case runtimes for arbitrary matrices. (i.e., focusing explicitly or implicitly biasing toward high leverage components.)"}, {"heading": "3.3 Novel Leveraging Estimators", "text": "In light of Lemmas 3 and 4, we look at several ways to leverage the complementary strengths of the LEV and UNIF processes. Let's remember that we would like to sample probabilities that are \"close\" to those defined by the empirical statistical leverage scores. We would like to identify at least large leverage scores to maintain rank. This helps ensure that the linear regime of Taylor expansion is large, and it also helps to ensure that the scale of variance is p / r and not n / r. But we would like to avoid recalculation by 1 / hii when certain leverage scores are extremely low, thereby avoiding excessive variance estimates."}, {"heading": "3.3.1 The Shrinked Leveraging (SLEV) Estimator", "text": "= Consider first the SLEV method. As described in Section 2.2, this involves scanning and reweighting with respect to a distribution that is a convex combination of the empirical distribution results and the uniform distribution. That is, let \u03c0Lev denote a distribution defined by the normalized debt results (i.e., \u03c0Levi = hii / p or \u03c0Lev is constructed from the output of the algorithm of [10] that calculates relative-error approximations to the indebtedness results), and let \u03c0Unif denote the uniform distribution (i.e., \u03c0Unifi = 1 / n, for all i [n]); then the sampling probabilities for the SLEV method is derived from the formal equality Lev i + (1 \u2212 \u03b1), the Unif i, (21), where \u03b1 (0, 1) and LEV is the inaccuracy. Since SLEV is the solution of a weighted LS problem of the form of LEV, we have provided the LEV expressions directly (LEq6)."}, {"heading": "3.3.2 The Unweighted Leveraging (LEVUNW) Estimator", "text": "'So it's not that we don't have the unweighted results of the unweighted S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160 & # 160; S & # 160; S & # 160 & # 160; S & # 160; S & # 160; S & # 160; S & # 160 & # 160; S & # 160 & # 160 & & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160; S & # 160 & # 160 & # 160; S & # 160; S & # 160; S & # 160; S & # 160 & # 160; S & # 160; S & # 160; S & # 160; S & # 160 & # 160; S & # 160 & # 160; S & # 160; S & # 160; S & &"}, {"heading": "4 Main Empirical Evaluation", "text": "In this section we describe the main part of our empirical analysis of the behaviour of biases and deviations of the sample estimators described in Section 2.2. Further empirical results are presented in Section 5. In these two sections we consider both synthetic data and real data selected to illustrate the extreme properties of the sample methods in realistic environments. We will use the MSE as a yardstick to compare the various sample estimators; however, since we are interested in both the bias and variant characteristics of our estimates, we will present the results for both the bias and the variance separately. Here is a brief outline of the main results of this section. \u2022 In Section 4.1 we will derive our synthetic data from three standard distributions and they are designed so that they provide relatively realistic synthetic ELV ELV examples in which the ELV values are ELV ELELELELELELELELELELELELELELELELELELELELELELELELELELELELLEELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELELEL"}, {"heading": "4.1 Description of Synthetic Data", "text": "We look at synthetic data of 1000 runs generated by y = X\u03b2 +, where \u0445 N (0, 9In), where several different values of n and p are considered, leading to both \"very rectangular\" and \"moderately rectangular\" matrices X. Design matrix X is generated by one of three different distribution classes introduced below. These three distributions were chosen because the first shows almost uniform distribution results, the second shows slightly uneven distribution results, and the third shows very unequal distribution results. We have a n \u00b7 p matrix X of multivariate normal N (1p, \u03a3), where the second element of distribution results 2 \u00d7 0.5 | i \u2212 j |, and where we have set \u03b2 = (110, 0.11p \u2212 20.110) T."}, {"heading": "4.2 Leveraging Versus Uniform Sampling on Synthetic Data", "text": "Here we describe the properties of LEV versus UNIF for synthetic data. See Figures 1, 2 and 3 for the results on data matrices with n = 1000 and p = 10, 50 and 100, respectively. (The results for data matrices for n = 5000 and other values of n are similar.) In any case, we generate a single matrix from this distribution (which we then fixed to generate the y vectors) and \u03b20 was set to be the all-one vector; and then we performed the sampling process several times, typically about 1000 times, to obtain reliable estimates for the distortions and variances. In each of the figures 1, 2 and 3, the top panel is the variance, the bottom panel is the square bias; both the bias and the variance, we have plotted the results in log scale; and in each illustration, the first column is the model of LEX, the middle panel is the LEX and the right column is the column of LEX and the column is the LEX."}, {"heading": "4.3 Improvements from Shrinked Leveraging and Unweighted Leveraging", "text": "Here we will describe how our proposed SLEV and LEVUNW methods tend to be quite similar in these numbers, but can nevertheless lead to improvements in terms of LEV and UNIF. (Remember that LEV can lead to large MSE results by inflating very small leverage values.) The SLEV method addresses this by taking into account a convex combination of the uniform distribution and distribution of leverage score values, thereby creating a lower limit for the leverage score matrices (for GA, T3, and T1 data) of size n \u00d7 p, where n = 1000 and p = 10, 50 and 100, respectively. In each case LEV, SLEV for three different values of the convex combination parameters \u03b1 and LEVUNW were considered."}, {"heading": "4.4 Conditional Bias and Variance", "text": "Here we will describe the properties of conditional bias and variance among various subsample estimators, which will provide a more direct comparison with Eqns. (14) and (15) of Lemma 2 and the corresponding limits of Lemma 6. These will also provide a more direct comparison with previous work, which adopted an algorithmic perspective on algorithmic leveraging [11, 27, 10]. Consider Figure 8, which presents our most important empirical results for conditional bias and deviations. As before, matrices were generated from GA, T3, and T1; and we have the empirical bias and deviation of UNIF, LEV, SLEV with \u03b1 = 0.9, and LEVUNW - in all cases determined by empirical data y. Several observations are worth noting that in GA the deviations are all very similar, and the biases are, with the exception of LEVLEVUNW."}, {"heading": "5 Additional Empirical Evaluation", "text": "In this section, we provide additional empirical results (of a more specific nature than those presented in Section 4). This can happen if you are extremely aggressive when downsampling with SLEV, but it is much more common with UNIF, even if you scan many constraints. In both cases, the behavior of bias and variance is very different than if rank is maintained. \u2022 Then, in Section 5.2, we summarize our results into synthetic data when calculating the leverage values roughly using the fast approximation algorithm of [10]. Among other things, we describe the runtime of this algorithm and illustrate that it can solve larger problems than can be solved with conventional deterministic methods, and we will evaluate the unconditional distortion and deviation from SLEV if this algorithm uses two safe results for the same work."}, {"heading": "5.1 Leveraging and Uniform Estimates for Singular Subproblems", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.2 Approximate Leveraging via the Fast Leveraging Algorithm", "text": "Here we describe the fast randomized results of X [10] to calculate approximate results for the leverage scores of X = 000 exact results, which are used instead of the exact leverage scores in LEV, SLEV and LEVUNW. To begin with, we offer a brief description of the scores of [10], which are used as input an arbitrary n \u00b7 p matrix X. \u2022 Let R examine the R matrix from a QR decomposition of [1X] and return the leverage scores of the matrix XR \u2212 n random matrix 1 and a p \u00b7 r2 random matrix, which are used for both types of calculation results. \u2022 Let R examine the R matrix from a QR decomposition of [1X] and return the leverage scores of the matrix XR \u2212 1 and r2. Appropriate decisions of r1 and r2, if one opts for a hadamard-based random scores, then we will select random scores from a QR-based algorithm running this random 1X in a matrix."}, {"heading": "5.3 Illustration of the Method on Real Data", "text": "Here we illustrate our methods using two real sets of data from two genetic problems with which we already have experience [9, 28]. The first set of data has relatively uniform leverage values, while the second set has slightly more uneven leverage values. These two examples merely illustrate that our observations of synthetic data also apply to more realistic data that we have previously studied. Further information on the application of these ideas in genetics can be found in previous work on PCA-related SNPs [35, 34]."}, {"heading": "5.3.1 Linear model for bias correction in RNA-Seq data", "text": "To illustrate how our methods operate on a real dataset with almost uniform leverage values, let us consider an RNA-Seq dataset with n = 51, 751 read numbers from mouse embryonic stem cells [15]. Remember that RNA-Seq becomes the most important tool for transcription analysis; it generates digital signals by receiving tens of millions of short readings; and after it has been assigned to the genome, RNA-Seq data can be aggregated by a sequence of short-read counts. Recent work has found that short-read counts have significant sequence bias [25]. Here we are looking at a simplified linear model of [9] for correcting sequence bias in RNA-Seq. Let us nij count readings assigned to the genome, starting with the most recent nucleotide gene, where read values go from 100, i = 1, and Li = 1."}, {"heading": "5.3.2 Linear model for predicting gene expressions of cancer patient", "text": "To illustrate how our methods develop in relation to real data with moderately uneven leverage values, we consider a microarray data set presented in [33] (and also in [28]) for 46 cancer patients in relation to n = 5, 520 genes. Here, we randomly select the gene expression of one patient as an answer y and use the gene expression of the remaining patients as a predictive value (so p = 45); and we predict the gene expression of the selected patient using gene expressions of other patients through a linear model. We adjust the linear model using subsample algorithms with nine different subsample sizes. See Figure 15 for a summary of our results. In the left panel of Figure 15, we draw the histogram of the sample sample sample leverage of the sample probabilities. Note that the distribution is severely distorted and a whole number of probabilities is significantly greater than the average sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample V."}, {"heading": "6 Discussion and Conclusion", "text": "In this paper, we have taken a statistical perspective on algorithmic leveraging, and we have shown how this leads to improved performance of this paradigm on real and synthetic data. However, particularly from the algorithmic perspective of worst-case analysis, leverage-based sampling clearly delivers better results in the worst-case scenario than a standardized sampling methodology. However, our statistical analysis shows that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominate the others. Based on this, we have developed new statistically inspired leveraging algorithms that achieve improved statistical performance while maintaining the algorithmic utility of the usual leverage-based methodology."}, {"heading": "A Asymptotic Analysis and Toy Data", "text": "In this case, we will consider several estimates that illustrate various aspects of algorithmic leveraging. Although the results of this section are not used elsewhere, and therefore some readers prefer this section, we include it to take our approaches to ideas that may be more familiar than certain readers who use a natural method to compare two approaches in which the two approaches correspond to a given standard of performance. One such standard is efficiency, which refers to \"spread out\" over the estimators. In this case, the lesser variance, which is more efficient, is the estimator."}, {"heading": "B Appendix: Proofs of our main results", "text": "In this section we will provide evidence for several of our main findings."}], "references": [{"title": "Faster dimension reduction", "author": ["N. Ailon", "B. Chazelle"], "venue": "Communications of the ACM, 53(2):97\u2013104", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Strong consistency of least squares estimates in normal linear regression", "author": ["T.W. Anderson", "J.B. Taylor"], "venue": "Annals of Statistics, 4(4):788\u2013790", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Blendenpik: Supercharging LAPACK\u2019s leastsquares solver", "author": ["H. Avron", "P. Maymounkov", "S. Toledo"], "venue": "SIAM Journal on Scientific Computing, 32:1217\u20131236", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "and W", "author": ["P.J. Bickel", "F. Gotze"], "venue": "R. van Zwet. Resampling fewer than n observations: gains, losses, and remedies for losses. Statistica Sinica, 7:1\u201331", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Influential observations", "author": ["S. Chatterjee", "A.S. Hadi"], "venue": "high leverage points, and outliers in linear regression. Statistical Science, 1(3):379\u2013393", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1986}, {"title": "The Fast Cauchy Transform and faster robust linear regression", "author": ["K.L. Clarkson", "P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "X. Meng", "D.P. Woodruff"], "venue": "Technical report. Preprint: arXiv:1207.4684 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "Technical report. Preprint: arXiv:1207.6365 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistics for Spatial Data", "author": ["N. Cressie"], "venue": "Wiley, New York", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Bias correction in RNA-Seq short-read counts using penalized regression", "author": ["D. Dalpiaz", "X. He", "P. Ma"], "venue": "Statistics in Biosciences, 5(1):88\u201399", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "D.P. Woodruff"], "venue": "Journal of Machine Learning Research, 13:3475\u2013 3506", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sampling algorithms for `2 regression and applications", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1127\u20131136", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Faster least squares approximation", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan", "T. Sarl\u00f3s"], "venue": "Numerische Mathematik, 117(2):219\u2013249", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Bootstrap methods: another look at the jackknife", "author": ["B. Efron"], "venue": "The Annals of Statistics, 7(1):1\u201326", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1979}, {"title": "A leisurely look at the bootstrap", "author": ["B. Efron", "G. Gong"], "venue": "the jackknife, and cross-validation. The American Statistician, 37(1):36\u201348", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1983}, {"title": "Stem cell transcriptome profiling via massive-scale mRNA sequencing", "author": ["N. Cloonan"], "venue": "Nature Methods,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Asymptotics for lasso-type estimators", "author": ["W. Fu", "K. Knight"], "venue": "Annals of Statistics, 28:1356\u2013 1378", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "Technical report. Preprint: arXiv:1303.1849 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "Johns Hopkins University Press, Baltimore", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Matrix Algebra from A Statistician\u2019s perspective", "author": ["D.A. Harville"], "venue": "Springer-Verlag, New York", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Jackknifing in unbalanced situations", "author": ["D.V. Hinkley"], "venue": "Technometrics, 19(3):285\u2013292", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1977}, {"title": "The hat matrix in regression and ANOVA", "author": ["D.C. Hoaglin", "R.E. Welsch"], "venue": "The American Statistician, 32(1):17\u201322", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1978}, {"title": "The infinitesimal jackknife", "author": ["L. Jaeckel"], "venue": "Bell Laboratories Memorandum, MM:72\u20131215\u201311", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1972}, {"title": "The big data bootstrap", "author": ["A. Kleiner", "A. Talwalkar", "P. Sarkar", "M. Jordan"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Strong consistency of least squares estimates in multiple regression", "author": ["T.L. Lai", "H. Robbins", "C.Z. Wei"], "venue": "Proceedings of National Academy of Sciences, 75(7):3034\u20133036", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1978}, {"title": "Modeling non-uniformity in short-read rates in RNA-seq data", "author": ["J. Li", "H. Jiang", "W.H. Wong"], "venue": "Genome Biology, 11:R50", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Rejection control and sequential importance sampling", "author": ["J.S. Liu", "R. Chen", "W.H. Wong"], "venue": "Journal of the American Statistical Association, 93(443):1022\u20131031", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Randomized algorithms for matrices and data", "author": ["M.W. Mahoney"], "venue": "Foundations and Trends in Machine Learning. NOW Publishers, Boston", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of National Academy of Sciences, 106:697\u2013702", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "author": ["X. Meng", "M.W. Mahoney"], "venue": "Technical report. Preprint: arXiv:1210.3135 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "LSRN: A parallel iterative solver for strongly over- or under-determined systems", "author": ["X. Meng", "M.A. Saunders", "M.W. Mahoney"], "venue": "Technical report. Preprint: arXiv:1109.5981 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "The jackknife\u2013a review", "author": ["R.G. Miller"], "venue": "Biometrika, 61(1):1\u201315", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1974}, {"title": "An unbalanced jackknife", "author": ["R.G. Miller"], "venue": "The Annals of Statistics, 2(5):880\u2013891", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1974}, {"title": "J", "author": ["T. Nielsen", "R.B. West", "S.C. Linn", "O. Alter", "M.A. Knowling"], "venue": "O\u2019Connell, S. Zhu, M. Fero, G. Sherlock, J. R. Pollack, P. O. Brown, D. Botstein, and M. van de Rijn. Molecular characterisation of soft tissue tumours: a gene expression study. Lancet, 359(9314):1301\u2013 1307", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Ancestry informative markers for finescale individual assignment to worldwide populations", "author": ["P. Paschou", "J. Lewis", "A. Javed", "P. Drineas"], "venue": "Journal of Medical Genetics, page doi:10.1136/jmg.2010.078212", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "PCA-correlated SNPs for structure identification in worldwide human populations", "author": ["P. Paschou", "E. Ziv", "E.G. Burchard", "S. Choudhry", "W. Rodriguez-Cintron", "M.W. Mahoney", "P. Drineas"], "venue": "PLoS Genetics, 3:1672\u20131686", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Subsampling", "author": ["D.N. Politis", "J.P. Romano", "M. Wolf"], "venue": "Springer-Verlag, New York", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "The Bayesian bootstrap", "author": ["D.B. Rubin"], "venue": "The Annals of Statistics, 9(1):130\u2013134", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1981}, {"title": "Asymptotic relative efficiency in estimation", "author": ["R. Serfling"], "venue": "Miodrag Lovric, editor, International Encyclopedia of Statistical Sciences, pages 68\u201372. Springer", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "On resampling methods for variance estimation and related topics", "author": ["J. Shao"], "venue": "PhD thesis, University of Wisconsin at Madison", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1987}, {"title": "The Jackknife and Bootstrap", "author": ["J. Shao", "D. Tu"], "venue": "Springer-Verlag, New York", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1995}, {"title": "Efficient computing of regression diagnostics", "author": ["P.F. Velleman", "R.E. Welsch"], "venue": "The American Statistician, 35(4):234\u2013242", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1981}, {"title": "Applied Linear Regression", "author": ["S. Weisberg"], "venue": "Wiley, New York", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "Jackknife", "author": ["C.F.J. Wu"], "venue": "bootstrap and other resampling methods in regression analysis. The Annals of Statistics, 14(4):1261\u20131295", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1986}, {"title": "Quantile regression for large-scale applications", "author": ["J. Yang", "X. Meng", "M.W. Mahoney"], "venue": "Technical report. Preprint: arXiv:1305.0087 ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 26, "context": "Motivated by this, there has been a great deal of work on developing algorithms for matrix-based machine learning and data analysis problems that construct the random sample in a nonuniform data-dependent fashion [27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": ", least-squares approximation [11, 12], least absolute deviations regression [6, 29], and low-rank matrix approximation [28, 7].", "startOffset": 30, "endOffset": 38}, {"referenceID": 11, "context": ", least-squares approximation [11, 12], least absolute deviations regression [6, 29], and low-rank matrix approximation [28, 7].", "startOffset": 30, "endOffset": 38}, {"referenceID": 5, "context": ", least-squares approximation [11, 12], least absolute deviations regression [6, 29], and low-rank matrix approximation [28, 7].", "startOffset": 77, "endOffset": 84}, {"referenceID": 28, "context": ", least-squares approximation [11, 12], least absolute deviations regression [6, 29], and low-rank matrix approximation [28, 7].", "startOffset": 77, "endOffset": 84}, {"referenceID": 27, "context": ", least-squares approximation [11, 12], least absolute deviations regression [6, 29], and low-rank matrix approximation [28, 7].", "startOffset": 120, "endOffset": 127}, {"referenceID": 6, "context": ", least-squares approximation [11, 12], least absolute deviations regression [6, 29], and low-rank matrix approximation [28, 7].", "startOffset": 120, "endOffset": 127}, {"referenceID": 9, "context": "Typically, the leverage scores are computed approximately [10, 6], or otherwise a random projection [1, 6] is used to precondition by approximately uniformizing them [12, 3, 30].", "startOffset": 58, "endOffset": 65}, {"referenceID": 5, "context": "Typically, the leverage scores are computed approximately [10, 6], or otherwise a random projection [1, 6] is used to precondition by approximately uniformizing them [12, 3, 30].", "startOffset": 58, "endOffset": 65}, {"referenceID": 0, "context": "Typically, the leverage scores are computed approximately [10, 6], or otherwise a random projection [1, 6] is used to precondition by approximately uniformizing them [12, 3, 30].", "startOffset": 100, "endOffset": 106}, {"referenceID": 5, "context": "Typically, the leverage scores are computed approximately [10, 6], or otherwise a random projection [1, 6] is used to precondition by approximately uniformizing them [12, 3, 30].", "startOffset": 100, "endOffset": 106}, {"referenceID": 11, "context": "Typically, the leverage scores are computed approximately [10, 6], or otherwise a random projection [1, 6] is used to precondition by approximately uniformizing them [12, 3, 30].", "startOffset": 166, "endOffset": 177}, {"referenceID": 2, "context": "Typically, the leverage scores are computed approximately [10, 6], or otherwise a random projection [1, 6] is used to precondition by approximately uniformizing them [12, 3, 30].", "startOffset": 166, "endOffset": 177}, {"referenceID": 29, "context": "Typically, the leverage scores are computed approximately [10, 6], or otherwise a random projection [1, 6] is used to precondition by approximately uniformizing them [12, 3, 30].", "startOffset": 166, "endOffset": 177}, {"referenceID": 26, "context": "A detailed discussion of this approach can be found in the recent review monograph on randomized algorithms for matrices and matrix-based data problems [27].", "startOffset": 152, "endOffset": 156}, {"referenceID": 2, "context": "This algorithmic leveraging paradigm has already yielded impressive algorithmic benefits: by preconditioning with a high-quality numerical implementation of a Hadamard-based random projection, the Blendenpik code of [3] \u201cbeats Lapack\u2019s1 direct dense least-squares solver by a large margin on essentially any dense tall matrix;\u201d the LSRN algorithm of [30] preconditions with a high-quality numerical implementation of a normal random projection in order to solve large over-constrained least-squares problems on clusters with high communication cost, e.", "startOffset": 216, "endOffset": 219}, {"referenceID": 29, "context": "This algorithmic leveraging paradigm has already yielded impressive algorithmic benefits: by preconditioning with a high-quality numerical implementation of a Hadamard-based random projection, the Blendenpik code of [3] \u201cbeats Lapack\u2019s1 direct dense least-squares solver by a large margin on essentially any dense tall matrix;\u201d the LSRN algorithm of [30] preconditions with a high-quality numerical implementation of a normal random projection in order to solve large over-constrained least-squares problems on clusters with high communication cost, e.", "startOffset": 350, "endOffset": 354}, {"referenceID": 5, "context": ", on Amazon Elastic Cloud Compute clusters; the solution to the `1 regression or least absolute deviations problem as well as to quantile regression problems can be approximated for problems with billions of constraints [6, 44]; and CUR-based low-rank matrix approximations [28] have been used for structure extraction in DNA SNP matrices of size thousands of individuals by hundreds of thousands of SNPs [35, 34].", "startOffset": 220, "endOffset": 227}, {"referenceID": 43, "context": ", on Amazon Elastic Cloud Compute clusters; the solution to the `1 regression or least absolute deviations problem as well as to quantile regression problems can be approximated for problems with billions of constraints [6, 44]; and CUR-based low-rank matrix approximations [28] have been used for structure extraction in DNA SNP matrices of size thousands of individuals by hundreds of thousands of SNPs [35, 34].", "startOffset": 220, "endOffset": 227}, {"referenceID": 27, "context": ", on Amazon Elastic Cloud Compute clusters; the solution to the `1 regression or least absolute deviations problem as well as to quantile regression problems can be approximated for problems with billions of constraints [6, 44]; and CUR-based low-rank matrix approximations [28] have been used for structure extraction in DNA SNP matrices of size thousands of individuals by hundreds of thousands of SNPs [35, 34].", "startOffset": 274, "endOffset": 278}, {"referenceID": 34, "context": ", on Amazon Elastic Cloud Compute clusters; the solution to the `1 regression or least absolute deviations problem as well as to quantile regression problems can be approximated for problems with billions of constraints [6, 44]; and CUR-based low-rank matrix approximations [28] have been used for structure extraction in DNA SNP matrices of size thousands of individuals by hundreds of thousands of SNPs [35, 34].", "startOffset": 405, "endOffset": 413}, {"referenceID": 33, "context": ", on Amazon Elastic Cloud Compute clusters; the solution to the `1 regression or least absolute deviations problem as well as to quantile regression problems can be approximated for problems with billions of constraints [6, 44]; and CUR-based low-rank matrix approximations [28] have been used for structure extraction in DNA SNP matrices of size thousands of individuals by hundreds of thousands of SNPs [35, 34].", "startOffset": 405, "endOffset": 413}, {"referenceID": 20, "context": "This is in spite of the central role of statistical leverage, a traditional concept from regression diagnostics [21, 5, 41].", "startOffset": 112, "endOffset": 123}, {"referenceID": 4, "context": "This is in spite of the central role of statistical leverage, a traditional concept from regression diagnostics [21, 5, 41].", "startOffset": 112, "endOffset": 123}, {"referenceID": 40, "context": "This is in spite of the central role of statistical leverage, a traditional concept from regression diagnostics [21, 5, 41].", "startOffset": 112, "endOffset": 123}, {"referenceID": 9, "context": "By using a recentlydeveloped algorithm of [10] to compute fast approximations to the statistical leverage scores, we also demonstrate a regime for large data where our shrinked leveraging procedure is better algorithmically, in the sense of computing an answer more quickly than the usual black-box leastsquares solver, as well as statistically, in the sense of having smaller mean squared error than n\u00e4\u0131ve uniform sampling.", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "Using one of several related methods [18], this LS problem can be solved exactly in O(np2) time (but, as we will discuss in Section 2.", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "In this case, the unknown coefficient \u03b20 can be estimated via maximum-likelihood estimation as \u03b2\u0302ols = argmin\u03b2||y \u2212X\u03b2|| = (XTX)\u22121XTy, (3) in which case the predicted response vector is \u0177 = Hy, where H = X(XTX)\u22121XT is the so-called Hat Matrix, which is of interest in classical regression diagnostics [21, 5, 41].", "startOffset": 300, "endOffset": 311}, {"referenceID": 4, "context": "In this case, the unknown coefficient \u03b20 can be estimated via maximum-likelihood estimation as \u03b2\u0302ols = argmin\u03b2||y \u2212X\u03b2|| = (XTX)\u22121XTy, (3) in which case the predicted response vector is \u0177 = Hy, where H = X(XTX)\u22121XT is the so-called Hat Matrix, which is of interest in classical regression diagnostics [21, 5, 41].", "startOffset": 300, "endOffset": 311}, {"referenceID": 40, "context": "In this case, the unknown coefficient \u03b20 can be estimated via maximum-likelihood estimation as \u03b2\u0302ols = argmin\u03b2||y \u2212X\u03b2|| = (XTX)\u22121XTy, (3) in which case the predicted response vector is \u0177 = Hy, where H = X(XTX)\u22121XT is the so-called Hat Matrix, which is of interest in classical regression diagnostics [21, 5, 41].", "startOffset": 300, "endOffset": 311}, {"referenceID": 20, "context": "The statistical leverage scores have been used historically to quantify the extent to which an observation is an outlier [21, 5, 41], and they will be important for our main results below.", "startOffset": 121, "endOffset": 132}, {"referenceID": 4, "context": "The statistical leverage scores have been used historically to quantify the extent to which an observation is an outlier [21, 5, 41], and they will be important for our main results below.", "startOffset": 121, "endOffset": 132}, {"referenceID": 40, "context": "The statistical leverage scores have been used historically to quantify the extent to which an observation is an outlier [21, 5, 41], and they will be important for our main results below.", "startOffset": 121, "endOffset": 132}, {"referenceID": 17, "context": "(4), the exact computation of hii, for i \u2208 [n], requires O(np2) time [18] (but, as we will discuss in Section 2.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "2 Algorithmic Leveraging for Least-squares Approximation Here, we will review relevant work on random sampling algorithms for computing approximate solutions to the general overconstrained LS problem [11, 27, 10].", "startOffset": 200, "endOffset": 212}, {"referenceID": 26, "context": "2 Algorithmic Leveraging for Least-squares Approximation Here, we will review relevant work on random sampling algorithms for computing approximate solutions to the general overconstrained LS problem [11, 27, 10].", "startOffset": 200, "endOffset": 212}, {"referenceID": 9, "context": "2 Algorithmic Leveraging for Least-squares Approximation Here, we will review relevant work on random sampling algorithms for computing approximate solutions to the general overconstrained LS problem [11, 27, 10].", "startOffset": 200, "endOffset": 212}, {"referenceID": 10, "context": "A prototypical example of this approach is given by the following meta-algorithm [11, 27, 10], which we call SubsampleLS, and which takes as input an n\u00d7 p matrix X, where n p, a vector y, and a probability distribution {\u03c0i}i=1, and which returns as output an approximate solution \u03b2\u0303ols, which is an estimate of \u03b2\u0302ols of Eqn.", "startOffset": 81, "endOffset": 93}, {"referenceID": 26, "context": "A prototypical example of this approach is given by the following meta-algorithm [11, 27, 10], which we call SubsampleLS, and which takes as input an n\u00d7 p matrix X, where n p, a vector y, and a probability distribution {\u03c0i}i=1, and which returns as output an approximate solution \u03b2\u0303ols, which is an estimate of \u03b2\u0302ols of Eqn.", "startOffset": 81, "endOffset": 93}, {"referenceID": 9, "context": "A prototypical example of this approach is given by the following meta-algorithm [11, 27, 10], which we call SubsampleLS, and which takes as input an n\u00d7 p matrix X, where n p, a vector y, and a probability distribution {\u03c0i}i=1, and which returns as output an approximate solution \u03b2\u0303ols, which is an estimate of \u03b2\u0302ols of Eqn.", "startOffset": 81, "endOffset": 93}, {"referenceID": 10, "context": "There are several distributions that have been considered previously [11, 27, 10].", "startOffset": 69, "endOffset": 81}, {"referenceID": 26, "context": "There are several distributions that have been considered previously [11, 27, 10].", "startOffset": 69, "endOffset": 81}, {"referenceID": 9, "context": "There are several distributions that have been considered previously [11, 27, 10].", "startOffset": 69, "endOffset": 81}, {"referenceID": 10, "context": ", see below or see [11, 27]).", "startOffset": 19, "endOffset": 27}, {"referenceID": 26, "context": ", see below or see [11, 27]).", "startOffset": 19, "endOffset": 27}, {"referenceID": 10, "context": "On the other hand, it has been shown that, for a parameter \u03b3 \u2208 (0, 1] to be tuned, if \u03c0i \u2265 \u03b3 hii p , and r = O(p log(p)/(\u03b3 )), (7) then the following relative-error bounds hold: ||y \u2212X\u03b2\u0303ols||2 \u2264 (1 + )||y \u2212X\u03b2\u0302ols||2 and (8) ||\u03b2\u0302ols \u2212 \u03b2\u0303ols||2 \u2264 \u221a ( \u03ba(X) \u221a \u03be\u22122 \u2212 1 ) ||\u03b2\u0302ols||2, (9) where \u03ba(X) is the condition number of X and where \u03be = ||UUy||2/||y||2 is a parameter defining the amount of the mass of y inside the column space of X [11, 27, 10].", "startOffset": 433, "endOffset": 445}, {"referenceID": 26, "context": "On the other hand, it has been shown that, for a parameter \u03b3 \u2208 (0, 1] to be tuned, if \u03c0i \u2265 \u03b3 hii p , and r = O(p log(p)/(\u03b3 )), (7) then the following relative-error bounds hold: ||y \u2212X\u03b2\u0303ols||2 \u2264 (1 + )||y \u2212X\u03b2\u0302ols||2 and (8) ||\u03b2\u0302ols \u2212 \u03b2\u0303ols||2 \u2264 \u221a ( \u03ba(X) \u221a \u03be\u22122 \u2212 1 ) ||\u03b2\u0302ols||2, (9) where \u03ba(X) is the condition number of X and where \u03be = ||UUy||2/||y||2 is a parameter defining the amount of the mass of y inside the column space of X [11, 27, 10].", "startOffset": 433, "endOffset": 445}, {"referenceID": 9, "context": "On the other hand, it has been shown that, for a parameter \u03b3 \u2208 (0, 1] to be tuned, if \u03c0i \u2265 \u03b3 hii p , and r = O(p log(p)/(\u03b3 )), (7) then the following relative-error bounds hold: ||y \u2212X\u03b2\u0303ols||2 \u2264 (1 + )||y \u2212X\u03b2\u0302ols||2 and (8) ||\u03b2\u0302ols \u2212 \u03b2\u0303ols||2 \u2264 \u221a ( \u03ba(X) \u221a \u03be\u22122 \u2212 1 ) ||\u03b2\u0302ols||2, (9) where \u03ba(X) is the condition number of X and where \u03be = ||UUy||2/||y||2 is a parameter defining the amount of the mass of y inside the column space of X [11, 27, 10].", "startOffset": 433, "endOffset": 445}, {"referenceID": 10, "context": "This is the basic algorithmic leveraging algorithm that was originally proposed in [11], where the exact empirical statistical leverage scores of X were first used to construct the subsample and reweight the subproblem, and it\u2019s solution will be denoted by \u03b2\u0303LEV .", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "(11) Whereas the previous estimators all follow the basic framework of sampling and rescaling/reweighting according to the same distribution (which is used in worst-case analysis to control the properties of both eigenvalues and eigenvectors and provide unbiased estimates of certain quantities within the analysis [11, 27, 10]), with LEVUNW they are essentially done according to two different distributions\u2014the reason being that not rescaling leads to the same solution as rescaling with the uniform distribution.", "startOffset": 315, "endOffset": 327}, {"referenceID": 26, "context": "(11) Whereas the previous estimators all follow the basic framework of sampling and rescaling/reweighting according to the same distribution (which is used in worst-case analysis to control the properties of both eigenvalues and eigenvectors and provide unbiased estimates of certain quantities within the analysis [11, 27, 10]), with LEVUNW they are essentially done according to two different distributions\u2014the reason being that not rescaling leads to the same solution as rescaling with the uniform distribution.", "startOffset": 315, "endOffset": 327}, {"referenceID": 9, "context": "(11) Whereas the previous estimators all follow the basic framework of sampling and rescaling/reweighting according to the same distribution (which is used in worst-case analysis to control the properties of both eigenvalues and eigenvectors and provide unbiased estimates of certain quantities within the analysis [11, 27, 10]), with LEVUNW they are essentially done according to two different distributions\u2014the reason being that not rescaling leads to the same solution as rescaling with the uniform distribution.", "startOffset": 315, "endOffset": 327}, {"referenceID": 9, "context": "Of greater interest is the algorithm of [10] that computes relative-error approximations to all of the leverage scores of X in o(np2) time.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "In more detail, given as input an arbitrary n\u00d7p matrix X, with n p, and an error parameter \u2208 (0, 1), the main algorithm of [10] (described also in Section 5.", "startOffset": 123, "endOffset": 127}, {"referenceID": 9, "context": "This algorithm runs in roughly O(np log(p)/ ) time,3 which for appropriate parameter settings is o(np2) time [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "The running time of the relative-error approximation algorithm of [10] depends on the time needed to premultiply X by a randomized Hadamard transform (i.", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": ", Blendenpik [3], as well as LSRN [30], which extends these implementations 3 In more detail, the asymptotic running time of the main algorithm of [10] is O ( np ln ( p \u22121 ) + np \u22122 lnn+ p \u22122 (lnn) ( ln ( p \u22121 ))) .", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": ", Blendenpik [3], as well as LSRN [30], which extends these implementations 3 In more detail, the asymptotic running time of the main algorithm of [10] is O ( np ln ( p \u22121 ) + np \u22122 lnn+ p \u22122 (lnn) ( ln ( p \u22121 ))) .", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": ", Blendenpik [3], as well as LSRN [30], which extends these implementations 3 In more detail, the asymptotic running time of the main algorithm of [10] is O ( np ln ( p \u22121 ) + np \u22122 lnn+ p \u22122 (lnn) ( ln ( p \u22121 ))) .", "startOffset": 147, "endOffset": 151}, {"referenceID": 2, "context": "See [3, 30] for details, and see [17] for the application of these methods to the fast computation of leverage scores.", "startOffset": 4, "endOffset": 11}, {"referenceID": 29, "context": "See [3, 30] for details, and see [17] for the application of these methods to the fast computation of leverage scores.", "startOffset": 4, "endOffset": 11}, {"referenceID": 16, "context": "See [3, 30] for details, and see [17] for the application of these methods to the fast computation of leverage scores.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "Below, we will evaluate an implementation of a variant of the main algorithm of [10] in the software environment R.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "4 Additional Related Work Our leverage-based methods for estimating \u03b2 are related to resampling methods such as the bootstrap [13], and many of these resampling methods enjoy desirable asymptotic properties [40].", "startOffset": 126, "endOffset": 130}, {"referenceID": 39, "context": "4 Additional Related Work Our leverage-based methods for estimating \u03b2 are related to resampling methods such as the bootstrap [13], and many of these resampling methods enjoy desirable asymptotic properties [40].", "startOffset": 207, "endOffset": 211}, {"referenceID": 42, "context": "Resampling methods in linear models were studied extensively in [43] and are related to the jackknife [31, 32, 22, 14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "Resampling methods in linear models were studied extensively in [43] and are related to the jackknife [31, 32, 22, 14].", "startOffset": 102, "endOffset": 118}, {"referenceID": 31, "context": "Resampling methods in linear models were studied extensively in [43] and are related to the jackknife [31, 32, 22, 14].", "startOffset": 102, "endOffset": 118}, {"referenceID": 21, "context": "Resampling methods in linear models were studied extensively in [43] and are related to the jackknife [31, 32, 22, 14].", "startOffset": 102, "endOffset": 118}, {"referenceID": 13, "context": "Resampling methods in linear models were studied extensively in [43] and are related to the jackknife [31, 32, 22, 14].", "startOffset": 102, "endOffset": 118}, {"referenceID": 22, "context": "In addition, the goal of resampling is traditionally to perform statistical inference and not to improve the running time of an algorithm, except in the very recent work [23].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "Additional related work in statistics includes [20, 37, 26, 4, 36].", "startOffset": 47, "endOffset": 66}, {"referenceID": 36, "context": "Additional related work in statistics includes [20, 37, 26, 4, 36].", "startOffset": 47, "endOffset": 66}, {"referenceID": 25, "context": "Additional related work in statistics includes [20, 37, 26, 4, 36].", "startOffset": 47, "endOffset": 66}, {"referenceID": 3, "context": "Additional related work in statistics includes [20, 37, 26, 4, 36].", "startOffset": 47, "endOffset": 66}, {"referenceID": 35, "context": "Additional related work in statistics includes [20, 37, 26, 4, 36].", "startOffset": 47, "endOffset": 66}, {"referenceID": 10, "context": "However, for a fixed value of r, the linear approximation regime will be larger when the sample is constructed using information in the leverage scores\u2014since, among other things, using leverage scores in the sampling process is designed to preserve the rank of the subsampled problem [11, 27, 10].", "startOffset": 284, "endOffset": 296}, {"referenceID": 26, "context": "However, for a fixed value of r, the linear approximation regime will be larger when the sample is constructed using information in the leverage scores\u2014since, among other things, using leverage scores in the sampling process is designed to preserve the rank of the subsampled problem [11, 27, 10].", "startOffset": 284, "endOffset": 296}, {"referenceID": 9, "context": "However, for a fixed value of r, the linear approximation regime will be larger when the sample is constructed using information in the leverage scores\u2014since, among other things, using leverage scores in the sampling process is designed to preserve the rank of the subsampled problem [11, 27, 10].", "startOffset": 284, "endOffset": 296}, {"referenceID": 26, "context": "A detailed discussion of this last point is available in [27]; and these observations will be confirmed empirically in Section 5.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": ", explicitly or implicitly biasing toward high-leverage components, as is done in particular with the LEV procedure) provides uniformly superior worst-case algorithmic results, when compared with UNIF [11, 27, 10].", "startOffset": 201, "endOffset": 213}, {"referenceID": 26, "context": ", explicitly or implicitly biasing toward high-leverage components, as is done in particular with the LEV procedure) provides uniformly superior worst-case algorithmic results, when compared with UNIF [11, 27, 10].", "startOffset": 201, "endOffset": 213}, {"referenceID": 9, "context": ", explicitly or implicitly biasing toward high-leverage components, as is done in particular with the LEV procedure) provides uniformly superior worst-case algorithmic results, when compared with UNIF [11, 27, 10].", "startOffset": 201, "endOffset": 213}, {"referenceID": 19, "context": "(19), is the same as Hinkley\u2019s weighted jackknife variance estimator [20].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": ", \u03c0Lev i = hii/p, or \u03c0 Lev is constructed from the output of the algorithm of [10] that computes relative-error approximations to the leverage scores), and let \u03c0Unif denote the uniform distribution (i.", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "Finally, all of these observations also hold if, rather that using the exact leverage score distribution (which recall takes O(np2) time to compute), we instead use approximate leverage scores, as computed with the fast algorithm of [10].", "startOffset": 233, "endOffset": 237}, {"referenceID": 10, "context": "These will also provide a more direct comparison with previous work that has adopted an algorithmic perspective on algorithmic leveraging [11, 27, 10].", "startOffset": 138, "endOffset": 150}, {"referenceID": 26, "context": "These will also provide a more direct comparison with previous work that has adopted an algorithmic perspective on algorithmic leveraging [11, 27, 10].", "startOffset": 138, "endOffset": 150}, {"referenceID": 9, "context": "These will also provide a more direct comparison with previous work that has adopted an algorithmic perspective on algorithmic leveraging [11, 27, 10].", "startOffset": 138, "endOffset": 150}, {"referenceID": 9, "context": "2, we will summarize our results on synthetic data when the leverage scores are computed approximately with the fast approximation algorithm of [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "(7), then with very high probability the matrix X\u2217 does not loose rank [11, 27, 10].", "startOffset": 71, "endOffset": 83}, {"referenceID": 26, "context": "(7), then with very high probability the matrix X\u2217 does not loose rank [11, 27, 10].", "startOffset": 71, "endOffset": 83}, {"referenceID": 9, "context": "(7), then with very high probability the matrix X\u2217 does not loose rank [11, 27, 10].", "startOffset": 71, "endOffset": 83}, {"referenceID": 9, "context": "2 Approximate Leveraging via the Fast Leveraging Algorithm Here, we will describe using the fast randomized algorithm from [10] to compute approximations to the leverage scores of X, to be used in place of the exact leverage scores in LEV, SLEV, and LEVUNW.", "startOffset": 123, "endOffset": 127}, {"referenceID": 9, "context": "To start, we provide a brief description of the algorithm of [10], which takes as input an arbitrary n\u00d7 p matrix X.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "For appropriate choices of r1 and r2, if one chooses \u03a01 to be a Hadamard-based random projection matrix, then this algorithm runs in o(np2) time, and it returns 1 \u00b1 approximations to all the leverage scores of X [10].", "startOffset": 212, "endOffset": 216}, {"referenceID": 2, "context": "In addition, with a high-quality implementation of the Hadamard-based random projection, this algorithm runs faster than traditional deterministic algorithms based on Lapack for matrices as small as several thousand by several hundred [3, 17].", "startOffset": 235, "endOffset": 242}, {"referenceID": 16, "context": "In addition, with a high-quality implementation of the Hadamard-based random projection, this algorithm runs faster than traditional deterministic algorithms based on Lapack for matrices as small as several thousand by several hundred [3, 17].", "startOffset": 235, "endOffset": 242}, {"referenceID": 9, "context": "We have implemented in the software environment R two variants of this fast algorithm of [10], and we have compared it with QR-based deterministic algorithms also supported in R for", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "(In particular, note that here we do not consider Hadamard-based projections for \u03a01 or more sophisticated parallel and distributed implementations of these algorithms [3, 30, 17, 44].", "startOffset": 167, "endOffset": 182}, {"referenceID": 29, "context": "(In particular, note that here we do not consider Hadamard-based projections for \u03a01 or more sophisticated parallel and distributed implementations of these algorithms [3, 30, 17, 44].", "startOffset": 167, "endOffset": 182}, {"referenceID": 16, "context": "(In particular, note that here we do not consider Hadamard-based projections for \u03a01 or more sophisticated parallel and distributed implementations of these algorithms [3, 30, 17, 44].", "startOffset": 167, "endOffset": 182}, {"referenceID": 43, "context": "(In particular, note that here we do not consider Hadamard-based projections for \u03a01 or more sophisticated parallel and distributed implementations of these algorithms [3, 30, 17, 44].", "startOffset": 167, "endOffset": 182}, {"referenceID": 2, "context": ", with n = 106 or more, then one should probably not work within R and instead use Hadamardbased random projections for \u03a01 and/or the use of more sophisticated methods, such as those described in [3, 30, 17, 44]; here we simply evaluate an implementation of these methods in R.", "startOffset": 196, "endOffset": 211}, {"referenceID": 29, "context": ", with n = 106 or more, then one should probably not work within R and instead use Hadamardbased random projections for \u03a01 and/or the use of more sophisticated methods, such as those described in [3, 30, 17, 44]; here we simply evaluate an implementation of these methods in R.", "startOffset": 196, "endOffset": 211}, {"referenceID": 16, "context": ", with n = 106 or more, then one should probably not work within R and instead use Hadamardbased random projections for \u03a01 and/or the use of more sophisticated methods, such as those described in [3, 30, 17, 44]; here we simply evaluate an implementation of these methods in R.", "startOffset": 196, "endOffset": 211}, {"referenceID": 43, "context": ", with n = 106 or more, then one should probably not work within R and instead use Hadamardbased random projections for \u03a01 and/or the use of more sophisticated methods, such as those described in [3, 30, 17, 44]; here we simply evaluate an implementation of these methods in R.", "startOffset": 196, "endOffset": 211}, {"referenceID": 9, "context": ") CPU time for calculating exact leverage scores and approximate leverage scores using the BFast and GFast versions of the fast algorithm of [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "All in all, using the fast approximation algorithm of [10] to compute approximations to the leverage scores for use in LEV, SLEV, and LEVUNW leads to improved algorithmic performance, while achieving nearly identical statistical results as LEV, SLEV, and LEVUNW when the exact leverage scores are used.", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "3 Illustration of the Method on Real Data Here, we provide an illustration of our methods on two real data sets drawn from two problems in genetics with which we have prior experience [9, 28].", "startOffset": 184, "endOffset": 191}, {"referenceID": 27, "context": "3 Illustration of the Method on Real Data Here, we provide an illustration of our methods on two real data sets drawn from two problems in genetics with which we have prior experience [9, 28].", "startOffset": 184, "endOffset": 191}, {"referenceID": 34, "context": "For more information on the application of these ideas in genetics, see previous work on PCA-correlated SNPs [35, 34].", "startOffset": 109, "endOffset": 117}, {"referenceID": 33, "context": "For more information on the application of these ideas in genetics, see previous work on PCA-correlated SNPs [35, 34].", "startOffset": 109, "endOffset": 117}, {"referenceID": 9, "context": ") Comparison of variances and squared biases of the LEV, SLEV, and LEVUNW estimators in T3 data sets for n = 20000 and p = 5000 using BFast and GFast versions of the fast algorithm of [10].", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "1 Linear model for bias correction in RNA-Seq data In order to illustrate how our methods perform on a real data set with nearly uniform leverage scores, we consider an RNA-Seq data set containing n = 51, 751 read counts from embryonic mouse stem cells [15].", "startOffset": 253, "endOffset": 257}, {"referenceID": 24, "context": "Recent work found that short-read counts have significant sequence bias [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "Here, we consider a simplified linear model of [9] for correcting sequence bias in RNA-Seq.", "startOffset": 47, "endOffset": 50}, {"referenceID": 32, "context": "2 Linear model for predicting gene expressions of cancer patient In order to illustrate how our methods perform on real data with moderately nonuniform leverage scores, we consider a microarray data set that was presented in [33] (and also considered in [28]) for 46 cancer patients with respect to n = 5, 520 genes.", "startOffset": 225, "endOffset": 229}, {"referenceID": 27, "context": "2 Linear model for predicting gene expressions of cancer patient In order to illustrate how our methods perform on real data with moderately nonuniform leverage scores, we consider a microarray data set that was presented in [33] (and also considered in [28]) for 46 cancer patients with respect to n = 5, 520 genes.", "startOffset": 254, "endOffset": 258}, {"referenceID": 37, "context": "In this case, the smaller the variance, the more \u201cefficient\u201d is the estimator [38].", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Assuming XTX is nonsingular, for a LS estimator \u03b2\u0302ols to converge to true value \u03b20 in probability, it is sufficient and necessary that (XTX)\u22121 \u2192 0 as n\u2192\u221e [2, 24].", "startOffset": 154, "endOffset": 161}, {"referenceID": 23, "context": "Assuming XTX is nonsingular, for a LS estimator \u03b2\u0302ols to converge to true value \u03b20 in probability, it is sufficient and necessary that (XTX)\u22121 \u2192 0 as n\u2192\u221e [2, 24].", "startOffset": 154, "endOffset": 161}, {"referenceID": 1, "context": "Although we have stated this as an assumption, one typically assumes an n-dependence for \u03b1n [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "The usual assumption that is made (typically for analytical convenience) is that \u03b1n = n [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 27, "context": "In light of our empirical results in Section 4 and the empirical observation that leverage scores are often very nonuniform [28, 17], it is an interesting question to ask whether the common assumption that \u03b1n = n is too restrictive, e.", "startOffset": 124, "endOffset": 132}, {"referenceID": 16, "context": "In light of our empirical results in Section 4 and the empirical observation that leverage scores are often very nonuniform [28, 17], it is an interesting question to ask whether the common assumption that \u03b1n = n is too restrictive, e.", "startOffset": 124, "endOffset": 132}, {"referenceID": 10, "context": "development of the leveraging paradigm [11, 27, 10].", "startOffset": 39, "endOffset": 51}, {"referenceID": 26, "context": "development of the leveraging paradigm [11, 27, 10].", "startOffset": 39, "endOffset": 51}, {"referenceID": 9, "context": "development of the leveraging paradigm [11, 27, 10].", "startOffset": 39, "endOffset": 51}, {"referenceID": 7, "context": "The asymptotic properties of such design matrix are so-called \u201cin-fill\u201d asymptotics [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 41, "context": ", the first column of X is 1 vector; and, in this case, the hiis are bounded below by 1/n and above by 1/wi [42].", "startOffset": 108, "endOffset": 112}, {"referenceID": 31, "context": "It is worth noting that [32] showed \u03b1n = n in Assumption 1 implies that maxhii \u2192 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 38, "context": "4 shows that Miller\u2019s theorem does not hold for triangular array (with one pattern for even numbered observations and the other pattern for odd numbered observations) [39].", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "(27) To simplify (26), we need the following two results of matrix differentiation, \u2202Vec[X\u22121] \u2202(VecX)T = \u2212(X\u22121)T \u2297X\u22121, and \u2202Vec[AWB] \u2202wT = (B \u2297A) ] \u2202wT , (28) where the details on these two results can be found on page 366-367 of [19].", "startOffset": 230, "endOffset": 234}], "year": 2013, "abstractText": "One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method. In this paper, we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with \u201cshrinked\u201d leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). A detailed empirical evaluation of existing leverage-based methods as well as these two new methods is carried out on both synthetic and real data sets. The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance. For example, with the same computation reduction as in the original algorithmic leveraging approach, our proposed SLEV typically leads to improved biases and variances both unconditionally and conditionally (on the observed data), and our proposed LEVUNW typically yields improved unconditional biases and variances. \u2217Department of Statistics, University of Illinois at Urbana-Champaign, Champaign, IL 61820. Email: pingma@illinois.edu. \u2020Department of Mathematics, Stanford University, Stanford, CA 94305. Email: mmahoney@cs.stanford.edu. \u2021Department of Statistics, University of California at Berkeley, Berkeley, CA 94720. Email: binyu@stat.berkeley.edu. 1 ar X iv :1 30 6. 53 62 v1 [ st at .M E ] 2 3 Ju n 20 13", "creator": "LaTeX with hyperref package"}}}