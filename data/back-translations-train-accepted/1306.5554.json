{"id": "1306.5554", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2013", "title": "Correlated random features for fast semi-supervised learning", "abstract": "This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, XNV applies multiview regression using Canonical Correlation Analysis (CCA) on unlabeled data to bias the regression towards useful features. It has been shown that, if the views contains accurate estimators, CCA regression can substantially reduce variance with a minimal increase in bias. Random views are justified by recent theoretical and empirical work showing that regression with random features closely approximates kernel regression, implying that random views can be expected to contain accurate estimators. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude.", "histories": [["v1", "Mon, 24 Jun 2013 09:49:08 GMT  (327kb)", "https://arxiv.org/abs/1306.5554v1", "15 pages, 3 figures, 6 tables"], ["v2", "Tue, 5 Nov 2013 11:28:33 GMT  (639kb)", "http://arxiv.org/abs/1306.5554v2", "15 pages, 3 figures, 6 tables"]], "COMMENTS": "15 pages, 3 figures, 6 tables", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["brian mcwilliams", "david balduzzi", "joachim m buhmann"], "accepted": true, "id": "1306.5554"}, "pdf": {"name": "1306.5554.pdf", "metadata": {"source": "CRF", "title": "Correlated random features for fast semi-supervised learning", "authors": ["Brian McWilliams"], "emails": ["brian.mcwilliams@inf.ethz.ch", "david.balduzzi@inf.ethz.ch", "jbuhmann@inf.ethz.ch"], "sections": [{"heading": null, "text": "ar Xiv: 130 6.55 54v2 [st at.M L"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "2 Method", "text": "In this section, XNV, our semi-supervised learning method, is presented. The method is based on two main ideas: First, for two equally useful but sufficiently different views of a data set, punishing a regression according to the canonical standard (calculated via CCA) can significantly improve performance [7]; second, there is the Nystro-m method for constructing random features [1], which we use to construct the views."}, {"heading": "2.1 Multi-view regression", "text": "Suppose we have data T = (\u03b2, y1), (\u03b2, y1), (\u03b2, yn) for xi (RD) and yi (R), mashed according to the common distribution P (x, y). We assume we have two views of linear regressors that can be learned on these views. Assume 1 (Multi-view) = RM: x 7 (z)). Define (s) error function (g, x, y) = (g) \u2212 y and allow loss (g). Further, we allow L (Z) denote the space of linear maps from a linear space Z to the real one, and define: f (g)."}, {"heading": "2.2 Constructing random views", "text": "The idea behind the random attributes is rather to define a lower-dimensional image (xi \"): RD.\" RM. \"RM.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. \"R.\" R. R. R. \"R\" R. R \"R. R\" R. R. R \"R. R\" R. R \"R. R\" R. R \"R. R. R\" R \"R. R\" R \"R. R\" R. R \"R\" R. R \"R. R\" R \"R. R\" R \"R. R\" R \"R. R\" R \"R. R\" R \"R\" R. R \"R\" R \"R\" R. R \"R\" R \"R. R\" R \"R\" R. R \"R\" R \"R\" R \"R\" R \"R\" R. R \"R\" R \"R\" R \"R\" R. R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R. R \"R\" R \"R. R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R. R\" R \"R\" R \"R. R\" R \"R\" R \"R\" R \"R\" R \"R. R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R \"R\" R"}, {"heading": "Output: \u03b2\u0302", "text": "The other results of the study are: \"We are very satisfied with the results of the study.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\") \"We are very satisfied.\" (\"We are very satisfied.\")"}, {"heading": "3 Experiments", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4 Conclusion", "text": "We have introduced the XNV algorithm for semi-supervised learning. By combining two randomly generated views of Nystrom characteristics through an efficient implementation of CCA, XNV outperforms the previous state-of-the-art, SSSL, by 10-15% (depending on the number of points marked) on average over 18 datasets. Furthermore, XNV is over 3 orders of magnitude faster than SSSL for medium-sized datasets (N = 10,000) with further increases in N. An interesting direction of research is to investigate the use of the recently developed deep CCA algorithm, which as a pre-processing step extracts correlations between higher-order views [19]. In this work, we use a uniform sampling scheme for the Nystrom method for computational reasons, as it has been shown that it performs empirically well compared to more expensive systems [20]. Since CCA gives us a criterion measuring the important random characteristics, we aim to base information on active sampling based on the samples."}, {"heading": "SI.2 Comparison with Kernel Ridge Regression", "text": "We compare SSSLM and XNV with the kernel ridge regression (KRR). The following table shows the percentage improvement of the mean error of both methods over KRR, averaged over the 18 data sets according to the experimental procedure described in paragraph 3. The parameters \u03c3 (core width) and \u03b3 (firstpenalty) for KRR were selected by 5-fold cross-validation. We observe that both SSSLM and XNV exceed KRR by 50-60%. Importantly, this shows that our approach to SSSL exceeds the fully monitored baseline. SSSLM and XNV vs KRR n = 100 n = 200 n = 300 n = 400 n = 500 Avg error reduction for SSSLM 48% 52% 56% 58% 60% Avg error reduction for XNV 56% 62% 63% 63%%%%%%%%% 63%%%%"}, {"heading": "SI.3 Random Fourier features", "text": "Random Fourier attributes are a method for approximate displacement of invariant cores [6], i.e., where the individual attributes (xi, xi) are equal (xi \u2212 xi \u2032). Such a kernel function can be represented as such in terms of its inverse Fourier transformation and can therefore be interpreted as evolution [z (xi) z (xi \u2032) x (xi \u2032)))] for real attributes (xi, xi \u2032), where z (xi) = 1 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (xi (xi, xi, xi (xi, xi, xi, xi, xi, xi, xi, xi, xi xi, xi xi, xi xi xi xi, xi xi xi xi xi xi xi, xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi, xi xi xi xi xi xi, xi xi xi xi xi xi, xi xi xi xi xi xi xi xi, xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi xi))))."}, {"heading": "Output: \u03b2\u0302", "text": "It can be shown that views constructed using random Fourier features, with sufficiently many features, contain good approximations to a large class of functions with a high probability, see main theorem of [2]. We do not provide details, as XKS is consistently outperformed in practice by XNV. SI.4 Full XKS results For completeness, we report on the performance of the XKS algorithm. We use the same experimental setup as in Section 3. We compare the performance of XKS with a linear machine learned using M and 2M random Fourier features. Table 4 shows the performance improvement of XKS over RFFM / 2M averaged over the 18 datasets. Table 6 compares the prediction error and standard deviation for each of the datasets individually. Figure 3 shows the performance across the entire value range of n for all datasets of XKS. The relative performance of XKS over RFFM and RFFM is almost identical to the trend of SFFM and RFFS, with only following the trend of the RFFM 3."}], "references": [{"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C Williams", "M Seeger"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A Rahimi", "B Recht"], "venue": "In Adv in Neural Information Processing Systems (NIPS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "ZH: Nystr\u00f6m Method vs Random Fourier Features: A Theoretical and Empirical Comparison", "author": ["T Yang", "YF Li", "M Mahdavi", "R Jin", "Zhou"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A Gittens", "MW Mahoney"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Sharp analysis of low-rank kernel approximations", "author": ["F Bach"], "venue": "COLT", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["A Rahimi", "B Recht"], "venue": "In Adv in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Multi-view Regression Via Canonical Correlation Analysis", "author": ["S Kakade", "DP Foster"], "venue": "In Computational Learning Theory (COLT)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Relations between two sets of variates", "author": ["H Hotelling"], "venue": "Biometrika", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1936}, {"title": "Canonical Correlation Analysis: An Overview with Application to Learning Methods", "author": ["DR Hardoon", "S Szedmak", "J Shawe-Taylor"], "venue": "Neural Comp", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound", "author": ["M Ji", "T Yang", "B Lin", "R Jin", "J Han"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M Belkin", "P Niyogi", "V Sindhwani"], "venue": "JMLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A Blum", "T Mitchell"], "venue": "COLT", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Multiview clustering via Canonical Correlation Analysis", "author": ["K Chaudhuri", "SM Kakade", "K Livescu", "K Sridharan"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Multi-view predictive partitioning in high dimensions", "author": ["B McWilliams", "G Montana"], "venue": "Statistical Analysis and Data Mining", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "On the Nystr\u00f6m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning", "author": ["P Drineas", "MW Mahoney"], "venue": "JMLR", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Efficient Dimensionality Reduction for Canonical Correlation Analysis", "author": ["H Avron", "C Boutsidis", "S Toledo", "A Zouzias"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "An Analysis of Random Design Linear Regression", "author": ["D Hsu", "S Kakade", "T Zhang"], "venue": "COLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "A Risk Comparison of Ordinary Least Squares vs Ridge Regression", "author": ["PS Dhillon", "DP Foster", "SM Kakade", "LH Ungar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Deep Canonical Correlation Analysis", "author": ["G Andrew", "R Arora", "J Bilmes", "K Livescu"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Randomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2].", "startOffset": 185, "endOffset": 191}, {"referenceID": 1, "context": "Randomization has recently been considered as an alternative to optimization that, surprisingly, can yield comparable generalization performance at a fraction of the computational cost [1, 2].", "startOffset": 185, "endOffset": 191}, {"referenceID": 0, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 156, "endOffset": 161}, {"referenceID": 3, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 156, "endOffset": 161}, {"referenceID": 4, "context": "Among several different approaches, the Nystr\u00f6m method for low-rank kernel approximation [1] exhibits good theoretical properties and empirical performance [3\u20135].", "startOffset": 156, "endOffset": 161}, {"referenceID": 1, "context": "We investigate two ways of doing so: one based on the Nystr\u00f6m method and another based on random Fourier features (so-called kitchen sinks) [2, 6].", "startOffset": 140, "endOffset": 146}, {"referenceID": 5, "context": "We investigate two ways of doing so: one based on the Nystr\u00f6m method and another based on random Fourier features (so-called kitchen sinks) [2, 6].", "startOffset": 140, "endOffset": 146}, {"referenceID": 6, "context": "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.", "startOffset": 74, "endOffset": 80}, {"referenceID": 8, "context": "The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views.", "startOffset": 74, "endOffset": 80}, {"referenceID": 4, "context": "Recent theoretical work by Bach [5] shows that Nystr\u00f6m views can be expected to contain accurate estimators.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "We perform an extensive evaluation of XNV on 18 real-world datasets, comparing against a modified version of the SSSL (simple semi-supervised learning) algorithm introduced in [10].", "startOffset": 176, "endOffset": 180}, {"referenceID": 9, "context": "We chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "We chose SSSL since it was shown in [10] to outperform a state of the art algorithm, Laplacian Regularized Least Squares [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "Our approximate version of SSSL outperforms kernel ridge regression (KRR) by > 50% on the 18 datasets on average, in line with the results reported in [10], suggesting that we lose little by replacing the exact SSSL with our approximate implementation.", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].", "startOffset": 139, "endOffset": 146}, {"referenceID": 13, "context": "Multiple view learning was first introduced in the co-training method of [12] and has also recently been extended to unsupervised settings [13,14].", "startOffset": 139, "endOffset": 146}, {"referenceID": 6, "context": "Our algorithm builds on an elegant proposal for multi-view regression introduced in [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "First, given two equally useful but sufficiently different views on a dataset, penalizing regression using the canonical norm (computed via CCA), can substantially improve performance [7].", "startOffset": 184, "endOffset": 187}, {"referenceID": 0, "context": "The second is the Nystr\u00f6m method for constructing random features [1], which we use to construct the views.", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "Assumption 1 (Multi-view assumption [7]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables.", "startOffset": 31, "endOffset": 37}, {"referenceID": 8, "context": "Canonical correlation analysis [8, 9] extends principal component analysis (PCA) from one to two sets of variables.", "startOffset": 31, "endOffset": 37}, {"referenceID": 6, "context": "More formally: Theorem 1 (canonical ridge regression, [7]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "To ensure our method scales to large sets of unlabeled data, we use random features generated using the Nystr\u00f6m method [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "The idea behind random features is to instead define a lower-dimensional mapping, z(xi) : R \u2192 R M through a random sampling scheme such that [K]ii\u2032 \u2248 z(xi) z(xi\u2032 ) [6, 15].", "startOffset": 164, "endOffset": 171}, {"referenceID": 14, "context": "The idea behind random features is to instead define a lower-dimensional mapping, z(xi) : R \u2192 R M through a random sampling scheme such that [K]ii\u2032 \u2248 z(xi) z(xi\u2032 ) [6, 15].", "startOffset": 164, "endOffset": 171}, {"referenceID": 0, "context": "The Nystr\u00f6m method [1, 3] constructs a low-rank approximation to the Gram matrix as", "startOffset": 19, "endOffset": 25}, {"referenceID": 2, "context": "The Nystr\u00f6m method [1, 3] constructs a low-rank approximation to the Gram matrix as", "startOffset": 19, "endOffset": 25}, {"referenceID": 14, "context": "Constructing features in this way reduces the time complexity of learning a non-linear prediction function from O(N) to O(N) [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "(6): Proposition 2 (random Nystr\u00f6m view, [3]).", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "However, we reduce the runtime to O(NM) by applying a recently proposed randomized CCA algorithm of [16].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 3, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 4, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 14, "context": "The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3\u20135, 15].", "startOffset": 166, "endOffset": 175}, {"referenceID": 4, "context": "Recent work of Bach [5] provides theoretical guarantees on the quality of Nystr\u00f6m estimates in the fixed design setting that are relevant to our approach.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "1 Theorem 3 (Nystr\u00f6m generalization bound, [5]).", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": "For sufficiently large M (depending on \u03b7, see [5]), we have", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "We refer to this approach as Correlated Kitchen Sinks (XKS) after [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "It turns out that the performance of XKS is consistently worse than XNV, in line with the detailed comparison presented in [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 16, "context": "Extending to a random design requires techniques from [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "4 A fast approximation to SSSL The SSSL (simple semi-supervised learning) algorithm proposed in [10] finds the first s eigenfunctions \u03c6i of the integral operator LN in Eq.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "SSSL outperforms Laplacian Regularized Least Squares [11], a state of the art semi-supervised learning method, see [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "Second, instead of thresholding eigenfunctions, we use the easier to tune ridge penalty which penalizes directions proportional to the inverse square of their eigenvalues [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "As M increases, the span of L\u0302M tends towards that of LN [15].", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "The SSSL algorithm was shown to exhibit state-of-the-art performance over fully and semisupervised methods in scenarios where few labeled training examples are available [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "The SSSL algorithm of [10] is not computationally feasible on large datasets, since it has time complexity O(N).", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "We observe dramatic improvements, between 48% and 63%, consistent with the results observed in [10] for the exact SSSL algorithm.", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "Nystr\u00f6m features significantly outperform Fourier features, in line with observations in [3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "An interesting research direction is to investigate using the recently developed deep CCA algorithm, which extracts higher order correlations between views [19], as a preprocessing step.", "startOffset": 156, "endOffset": 160}], "year": 2013, "abstractText": "This paper presents Correlated Nystr\u00f6m Views (XNV), a fast semi-supervised algorithm for regression and classification. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude.", "creator": "LaTeX with hyperref package"}}}