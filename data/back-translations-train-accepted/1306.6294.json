{"id": "1306.6294", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2013", "title": "Learning Trajectory Preferences for Manipulators via Iterative Improvement", "abstract": "We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, while, nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalization ability of our algorithm on a variety of tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.", "histories": [["v1", "Wed, 26 Jun 2013 17:07:58 GMT  (882kb,D)", "https://arxiv.org/abs/1306.6294v1", "8 pages, 4 tables and 4 figures"], ["v2", "Tue, 5 Nov 2013 17:55:31 GMT  (1809kb,D)", "http://arxiv.org/abs/1306.6294v2", "9 pages. To appear in NIPS 2013"]], "COMMENTS": "8 pages, 4 tables and 4 figures", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.HC", "authors": ["ashesh jain", "brian wojcik", "thorsten joachims", "ashutosh saxena"], "accepted": true, "id": "1306.6294"}, "pdf": {"name": "1306.6294.pdf", "metadata": {"source": "CRF", "title": "Learning Trajectory Preferences for Manipulators via Iterative Improvement", "authors": ["Ashesh Jain", "Brian Wojcik", "Thorsten Joachims", "Ashutosh Saxena"], "emails": ["ashesh@cs.cornell.edu", "bmw75@cs.cornell.edu", "tj@cs.cornell.edu", "asaxena@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Related Work", "text": "Most of the research to date has focused on mimicking expert demonstrations, such as autonomous helicopter flights [1], ball-in-cup experiments [17], planning 2-D paths [27, 25, 26], etc. Such a setting (Learning from the Demonstration, LfD) is applicable to scenarios where an expert is aware of what constitutes a good path. In many scenarios, especially with high dose manipulators, it is extremely difficult to do so [2]. This is because users not only know the position of the end product, but also the complete configuration of the arm in a way that is spatially and temporally consistent."}, {"heading": "3 Learning and Feedback Model", "text": "We model the learning problem in the following way. For a given task, the robot is given a context x that describes the environment, objects, and any other input that is relevant to the problem. (The robot must figure out what a good trajectory is for that context.) Formally, we assume that the user has a scoring function that reflects how much he rates each trajectory for context x. (Note: The higher the score, the better the trajectory of the user, the less we can be observed directly, nor do we assume that the user can actually provide cardinal ratings according to this2Consider, reflecting the following analogy: In search engine results, it is much more difficult for a user to provide the best web pages for each query, but it is easier to provide the relative ranking of search results by clicking. Instead, we just assume that the user can provide preferences that reflect that function."}, {"heading": "4 Learning Algorithm", "text": "For each task, we model the scoring function of the user s * (x, y) with the following parameters: s (x, y; w) = w \u00b7 \u03c6 (x, y) (1) w is a weight vector to be learned, and \u03c6 (\u00b7) are characteristics that describe the trajectory y for the context x. We further divide the scoring function into two parts, one referring only to the objects with which the trajectory interacts, and the other to the object to be manipulated and the environment. s (x, y; wO, wE) = sO (x, y; wO) + sE (x, y; wE) = wO \u00b7 \u03c6O (x, y) + wE \u00b7 \u03c6E (x, y) (2) In the following, we now describe the characteristics for the two terms: O (\u00b7) and E (\u00b7)."}, {"heading": "4.1 Features Describing Object-Object Interactions", "text": "This attribute records the interaction between objects in the vicinity with the object being manipulated. We count q q = q waypoints of the path y as y1,.., yN and objects in the vicinity as O = {o1,.., oK}. The robot manipulates the object o = q O. Some waypoints of the path would be influenced by the other objects in the vicinity (e.g. in Figure 2, o1 and o2 influence the waypoint y3 due to their proximity. Specifically, we associate an object ok with a waypoint if the minimum distance to the collision is less than a threshold or if ok is below o. The edge connecting yj and ok is called (yj, ok) as (yE). Since these are the attributes [19] of the object that are really important in determining the quality of the path, we represent each object with its attributes. Specifically, we consider for each object a vector of M binary variables [lk, 1k, or an object of qq."}, {"heading": "4.2 Trajectory Features", "text": "We are now describing the characteristics that we have fragmented into three parts. (It's about the way people move, to gain the trust of users. \"(It's about the way people move). (It's about the way they move, the way they move. (It's about the way they move, the way they move.) (It's about the way people move, the way they move.) (It's about the way people move.) It's about the way people move, the way they move, the way they move. (It's about the way they move.) (It's about the way people move.) (It's about the way people move.) (It's about the way they move, the way they move, the way they move, the way they move, the way they move, the way they move.) (It's about the way they should move, the way people should move, the way they should move, the way they should move.) (It's about the way they're going to move, the way they're going to move, the way they're going to move, the way they're going to move, the way they're going to move, the way they're going to move.) (It's about the way they're about the way they're going to move, the way they're going to move, the way they're going to move, the way they're going to move, the way they're going to move.)"}, {"heading": "4.3 Computing Trajectory Rankings", "text": "(4) Note that this poses two challenges: First, the trajectory space is continuous and must be discarded to get Argmax in (4) tractable. Second, for a given set of {y (1),.., y (n)} discrete trajectories we have to calculate (4). Fortunately, the latter problem is easy to solve and simply boils down to sorting the trajectories according to their trajectory values (x, y (i); wO, wE)."}, {"heading": "4.4 Learning the Scoring Function", "text": "The goal is to learn the parameters wO and wE of the scoring function s (x, y; wO, wE) so that it can be used to evaluate the trajectories according to the preferences of the user. To this end, we adapt the Preference Perceptron algorithm [31] as detailed in algorithm 1. We call this algorithm the trajectory preference (TPP). Considering the context xt, the best rated trajector yt among the current parameters wO and wE, and the user feedback trajectory y y y y y y y y. (TPP updates the weights in the direction O (xt, y, t) \u2212 - and \u03c6O (xt) trajectory yt (xt, y, t). Despite its simplicity and although the algorithm does not typically receive the optimal trajectory, t = arg maxy s (xt, y) as feedback."}, {"heading": "5 Experiments and Results", "text": "This year is the highest in the history of the country."}, {"heading": "5.2 Results and Discussion", "text": "We have the quantity of data of 1300 people mentioned. How good is the generalization of preferences? We have only the object that is replaced by tomatoes. (B) Only the object that is in the vicinity changes."}, {"heading": "5.3 Robotic Experiment: User Study in learning trajectories", "text": "We are conducting a user study of our system on the Baxter robot on a variety of tasks of varying levels of difficulty, demonstrating that our approach is practicable and that the combination of re-rank and zero-G feedback allows users to train the robot in a few feedback steps. Experimental Setup: In this study, five users (who are not related to this work) used our system to train Baxter for food tasks using zero-G and re-rank feedback. Zero-G was provided kinesthetically on the robot, while re-rank was unlocked in a simulator (on a desktop computer). A series of 10 tasks with varying levels of difficulty were presented to users individually, and they were instructed to give feedback until they were satisfied with the best-placed track."}, {"heading": "6 Conclusion", "text": "Unlike traditional learning from demonstration approaches, our framework does not require the user to provide optimal trajectories as training data, but can learn from iterative improvements. Although our TPP learning algorithm requires only weak feedback, it has proven limitations and empirically performs well. In particular, we propose a set of trajectory characteristics for which the TPP tasks are well generalized, which the robot has never seen before. In addition to the batch experiments, robot experiments confirmed that incremental feedback generation is actually feasible and leads to good learning outcomes after just a few iterations. Recognition. We thank Shikhar Sharma for helping with the experiments. This research was supported by ARO, Microsoft Faculty Fellowship and NSF Career Award (to Saxena)."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "IJRR, 29(13)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Keyframe-based learning from demonstration", "author": ["B. Akgun", "M. Cakmak", "K. Jiang", "A.L. Thomaz"], "venue": "IJSR, 4(4):343\u2013355", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "The stochastic motion roadmap: A sampling framework for planning with markov motion uncertainty", "author": ["R. Alterovitz", "T. Sim\u00e9on", "K. Goldberg"], "venue": "RSS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information", "author": ["J.V.D. Berg", "P. Abbeel", "K. Goldberg"], "venue": "RSS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "On learning", "author": ["S. Calinon", "F. Guenter", "A. Billard"], "venue": "representing, and generalizing a task in a humanoid robot. IEEE Transactions on Systems, Man, and Cybernetics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Contextual sequence prediction with application to control library optimization", "author": ["D. Dey", "T.Y. Liu", "M. Hebert", "J.A. Bagnell"], "venue": "RSS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated Construction of Robotic Manipulation Programs", "author": ["R. Diankov"], "venue": "PhD thesis, CMU, RI", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "RSS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward optimal sampling in the space of paths", "author": ["C.J. Green", "A. Kelly"], "venue": "In ISRR", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Learning object arrangements in 3d scenes using human context", "author": ["Y. Jiang", "M. Lim", "A. Saxena"], "venue": "ICML", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to place new objects in a scene", "author": ["Y. Jiang", "M. Lim", "C. Zheng", "A. Saxena"], "venue": "IJRR, 31(9)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Hallucinated humans as the hidden context for labeling 3d scenes", "author": ["Y. Jiang", "H. Koppula", "A. Saxena"], "venue": "CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "KDD", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Cutting-plane training of structural svms", "author": ["T. Joachims", "T. Finley", "C. Yu"], "venue": "Mach Learn, 77(1)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Incremental sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "RSS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Grasping with application to an autonomous checkout robot", "author": ["E. Klingbeil", "D. Rao", "B. Carpenter", "V. Ganapathi", "A.Y. Ng", "O. Khatib"], "venue": "ICRA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "Machine Learning, 84(1)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Anticipating human activities using object affordances for reactive robotic response", "author": ["H.S. Koppula", "A. Saxena"], "venue": "RSS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H.S. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Randomized kinodynamic planning", "author": ["S.M. LaValle", "J.J. Kuffner"], "venue": "IJRR, 20(5):378\u2013400", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "RSS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["S. Levine", "V. Koltun"], "venue": "ICML", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Planning human-aware motions using a sampling-based costmap planner", "author": ["J. Mainprice", "E.A. Sisbot", "L. Jaillet", "J. Cort\u00e9s", "R. Alami", "T. Sim\u00e9on"], "venue": "ICRA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "volume 1. Cambridge University Press Cambridge", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to Search: Structured Prediction Techniques for Imitation Learning", "author": ["N. Ratliff"], "venue": "PhD thesis, CMU, RI", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Maximum margin planning", "author": ["N. Ratliff", "J.A. Bagnell", "M. Zinkevich"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Boosting structured prediction for imitation learning", "author": ["N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["N. Ratliff", "D. Silver", "J.A. Bagnell"], "venue": "Autonomous Robots, 27(1):25\u201353", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Chomp: Gradient optimization techniques for efficient motion planning", "author": ["N. Ratliff", "M. Zucker", "J.A. Bagnell", "S. Srinivasa"], "venue": "ICRA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "IJRR, 27(2)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Online structured prediction via coactive learning", "author": ["P. Shivaswamy", "T. Joachims"], "venue": "ICML", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Designing The User Interface: Strategies for Effective Human-Computer Interaction", "author": ["B. Shneiderman", "C. Plaisant"], "venue": "Addison-Wesley Publication", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatial reasoning for human robot interaction", "author": ["E.A. Sisbot", "L.F. Marin", "R. Alami"], "venue": "IROS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "A human aware mobile robot motion planner", "author": ["E.A. Sisbot", "L.F. Marin-Urias", "R. Alami", "T. Simeon"], "venue": "IEEE Transactions on Robotics", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "The Open Motion Planning Library", "author": ["I.A. Sucan", "M. Moll", "L.E. Kavraki"], "venue": "IEEE Robotics & Automation Magazine, 19(4):72\u201382", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient high dimensional maximum entropy modeling via symmetric partition functions", "author": ["P. Vernaza", "J.A. Bagnell"], "venue": "NIPS", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "A bayesian approach for policy learning from trajectory preference queries", "author": ["A. Wilson", "A. Fern", "P. Tadepalli"], "venue": "NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Making planned paths look more human-like in humanoid robot manipulation planning", "author": ["F. Zacharias", "C. Schlette", "F. Schmidt", "C. Borst", "J. Rossmann", "G. Hirzinger"], "venue": "ICRA", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "AAAI", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "[18]) them in existing path planners (such as [29, 35]) a priori.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[18]) them in existing path planners (such as [29, 35]) a priori.", "startOffset": 46, "endOffset": 54}, {"referenceID": 34, "context": "[18]) them in existing path planners (such as [29, 35]) a priori.", "startOffset": 46, "endOffset": 54}, {"referenceID": 30, "context": "In this work we propose an algorithm for learning user preferences over trajectories through interactive feedback from the user in a co-active learning setting [31].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 119, "endOffset": 122}, {"referenceID": 16, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 174, "endOffset": 186}, {"referenceID": 24, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 174, "endOffset": 186}, {"referenceID": 25, "context": "Most of the past research has focused on mimicking expert\u2019s demonstrations, for example, autonomous helicopter flights [1], ball-in-a-cup experiment [17], planning 2-D paths [27, 25, 26], etc.", "startOffset": 174, "endOffset": 186}, {"referenceID": 1, "context": "In many scenarios, especially involving high DoF manipulators, this is extremely challenging to do [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 38, "context": "Some later works in LfD provided ways for handling noisy demonstrations, under the assumption that demonstrations are either near optimal [39] or locally optimal [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "Some later works in LfD provided ways for handling noisy demonstrations, under the assumption that demonstrations are either near optimal [39] or locally optimal [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 36, "context": "A recent work [37] leverages user feedback to learn rewards of a Markov decision process.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "Our approach advances over [37] and Calinon et.", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "[5] in that it models sub-optimality in user feedback and theoretically converges to user\u2019s hidden score function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We also capture the necessary contextual information for household and assembly-line robots, while such context is absent in [5, 37].", "startOffset": 125, "endOffset": 132}, {"referenceID": 36, "context": "We also capture the necessary contextual information for household and assembly-line robots, while such context is absent in [5, 37].", "startOffset": 125, "endOffset": 132}, {"referenceID": 33, "context": "[34, 33] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[34, 33] and Mainprice et.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[23] planned trajectories satisfying user specified preferences in form of constraints on the distance of robot from user, the visibility of robot and the user arm comfort.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] used functional gradients [29] to optimize for legibility of robot trajectories.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[8] used functional gradients [29] to optimize for legibility of robot trajectories.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "(a) Re-ranking: We rank trajectories in order of their current predicted scores and visualize the ranking using OpenRave [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "A counterpart of this feedback is keyframe based LfD [2] where an expert demonstrates a sequence of optimal waypoints instead of the complete trajectory.", "startOffset": 53, "endOffset": 56}, {"referenceID": 18, "context": "Since it is the attributes [19] of the object that really matter in determining the trajectory quality, we represent each object with its attributes.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 149, "endOffset": 167}, {"referenceID": 0, "context": "For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 149, "endOffset": 167}, {"referenceID": 0, "context": "For example, if the set of possible properties are {heavy, fragile, sharp, hot, liquid, electronic}, then a laptop and a glass table can have labels [0, 1, 0, 0, 0, 1] and [0, 1, 0, 0, 0, 0] respectively.", "startOffset": 172, "endOffset": 190}, {"referenceID": 37, "context": "While a robot can reach the same operational space configuration for its wrist with different configurations of the arm, not all of them are preferred [38].", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "Furthermore, humans like to anticipate robots move and to gain users\u2019 confidence, robot should produce predictable and legible robot motion [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 18, "context": "In practice, such knowledge can be extracted using an object attribute labeling algorithm such as in [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 3, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 5, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 35, "context": "Previously both approaches [3, 4, 6, 36] have been studied.", "startOffset": 27, "endOffset": 40}, {"referenceID": 3, "context": "However, for high DoF manipulators sampling based approaches [4, 6] maintains tractability of the problem, hence we take this approach.", "startOffset": 61, "endOffset": 67}, {"referenceID": 5, "context": "However, for high DoF manipulators sampling based approaches [4, 6] maintains tractability of the problem, hence we take this approach.", "startOffset": 61, "endOffset": 67}, {"referenceID": 3, "context": "[4], we sample trajectories using rapidly-exploring random tree (RRT) [20].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[4], we sample trajectories using rapidly-exploring random tree (RRT) [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "5 Since our primary goal is to learn a score function on sampled set of trajectories we now describe our learning algorithm and for more literature on sampling trajectories we refer the readers to [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 30, "context": "To do so, we adapt the Preference Perceptron algorithm [31] as detailed in Algorithm 1.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "Despite its simplicity and even though the algorithm typically does not receive the optimal trajectory y\u2217 t = arg maxy s (xt, y) as feedback, the TPP enjoys guarantees on the regret [31].", "startOffset": 182, "endOffset": 186}, {"referenceID": 28, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [29] or optimal planners like RRT* [15] to produce reasonably good trajectories.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "The cost function (or its approximation) we learn can be fed to trajectory optimizers like CHOMP [29] or optimal planners like RRT* [15] to produce reasonably good trajectories.", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "Using these two parameters, the proof by [31] can be adapted to show that the expected average regret of", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 54, "endOffset": 62}, {"referenceID": 20, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "Our work complements previous works on grasping items [30, 21], pick and place tasks [11], and detecting bar code for grocery checkout [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 9, "context": "In previous work, such relations were considered in the context of scene understanding [10, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 11, "context": "In previous work, such relations were considered in the context of scene understanding [10, 12].", "startOffset": 87, "endOffset": 95}, {"referenceID": 19, "context": "\u2022 Geometric: It plans a path, independent of the task, using a BiRRT [20] planner.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "\u2022 Oracle-svm: This algorithm leverages the expert\u2019s labels on trajectories (hence the name Oracle) and is trained using SVM-rank [13] in a batch manner.", "startOffset": 129, "endOffset": 133}, {"referenceID": 25, "context": "\u2022 MMP-online: This is an online implementation of Maximum margin planning (MMP) [26, 28] algorithm.", "startOffset": 80, "endOffset": 88}, {"referenceID": 27, "context": "\u2022 MMP-online: This is an online implementation of Maximum margin planning (MMP) [26, 28] algorithm.", "startOffset": 80, "endOffset": 88}, {"referenceID": 13, "context": "At every iteration we train a structural support vector machine (SSVM) [14] using all previous feedback as training examples, and use the learned weights to predict trajectory scores for the next iteration.", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "We quantify the quality of a ranked list of trajectories by its normalized discounted cumulative gain (nDCG) [24] at positions 1 and 3.", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "Future research in human computer interaction, visualization and better user interface [32] could further reduce this time.", "startOffset": 87, "endOffset": 91}], "year": 2015, "abstractText": "We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.1", "creator": "LaTeX with hyperref package"}}}