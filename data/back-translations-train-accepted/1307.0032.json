{"id": "1307.0032", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2013", "title": "Memory Limited, Streaming PCA", "abstract": "We consider streaming, one-pass principal component analysis (PCA), in the high-dimensional regime, with limited memory. Here, $p$-dimensional samples are presented sequentially, and the goal is to produce the $k$-dimensional subspace that best approximates these points. Standard algorithms require $O(p^2)$ memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the {\\em spiked covariance model}, where $p$-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, $n$, scales proportionally with the dimension, $p$. Yet, all algorithms that provably achieve this, have memory complexity $O(p^2)$. Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provable bounds on sample complexity comparable to $p$. We present an algorithm that achieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able to compute the $k$-dimensional spike with $O(p \\log p)$ sample-complexity -- the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.", "histories": [["v1", "Fri, 28 Jun 2013 21:38:17 GMT  (46kb)", "http://arxiv.org/abs/1307.0032v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["ioannis mitliagkas", "constantine caramanis", "prateek jain 0002"], "accepted": true, "id": "1307.0032"}, "pdf": {"name": "1307.0032.pdf", "metadata": {"source": "CRF", "title": "Memory Limited, Streaming PCA", "authors": ["Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 7.00 32v1 [st at.M L] 2"}, {"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "3 Problem Formulation and Notation", "text": "We look at a streaming model where we get a point xt-Rp-1 (1) at any time. Furthermore, any vector that is not explicitly stored is never callable again. Our goal now is to calculate the uppermost k major components of the data: the k-dimensional subspace, which provides the best squared error estimate for the points. We assume a probable generative model from which the data is sampled in each step. (2) We assume that A-dimensional subspace is a fixed matrix, zt-Rk-1 is a multivariate normal random model, i.e., zt-Rp-1, Ik-k-k-k-k-k), and the vector wt-Rp-1 is the \"noise\" vector and is also assumed to be a multivariate normal distribution, i.e., wt-N-2Ip-p-p-p-p-p-p-p-p)."}, {"heading": "4 Algorithm and Guarantees", "text": "In this section, we present our proposed algorithm and its finite sample analysis, which is a block-by-block stochastic variant of the classic power method. Stochastic versions of the power method are already popular in the literature and are known to have good empirical performance; see [1] for a nice review of such methods. However, the main obstacle to the analysis of such stochastic power methods (as in (1)) is the potentially large variance of each step, mainly due to the high-dimensional regime we are looking at and the disappearing NRA. This motivated us to consider a modified stochastic power method algorithm, which has a built-in variance reduction step. At a high level, our method updates itself only once in a \"block\" and within a block, we calculate the noise to reduce the variance. In the following, we will first explain the main ideas of our method and our sample complexity proof for the rank of the simpler algorithm-1 and that they are similar to the rank of the algorithm-1."}, {"heading": "4.1 Rank-One Case", "text": "Our algorithm is a block-by-block method in which all n samples are divided into n / B blocks (for simplicity we assume n / B to be an integer). In the (\u03c4 + 1) -st block we then calculate \u03c4 + 1 = 1BB (\u03c4 + 1) \u2211 t = B\u03c4 + 1xtx tq\u03c4. (3) Then the iteration q\u03c4 is calculated with q\u03c4 + 1 = s\u043e + 1 / s\u03c4 + 1 \u0445 2. Note that s\u03c4 + 1 can easily be calculated online, requiring O (p) operations per step. In addition, the memory requirements in p are linear as well."}, {"heading": "4.1.1 Analysis", "text": "We present the complexity analysis of our proposed method (algorithm 1). We show that using O (4p log (p) / 2) samples, algorithm 1 gets a solution qT of accuracy (2). Set the total number of iterations T = 0 (log (p / o) protocol ((2 +.75) / (2 +.5))) and the block size B = 0 (1 + 3). Set the total number of iterations T = 0 (log (p / o) protocol ((2 +.75) / (2 +.5))) to block size B = 1 (1 + 3)."}, {"heading": "4.3 Perturbation-tolerant Subspace Recovery", "text": "While our results so far assume that A has exactly the k value and k a priori is known, we show here that these two values can be loosened; therefore, our results are within a fairly broad range. Let's let xt = Azt + wt be the t-th step sample, where A = U-V T-Rp \u00b7 r and U-Rp \u00b7 r is the true rank of A, which is unknown. However, we run algorithm 1 with the k value and the goal is to recover a subspace QT, s.t., QT is contained in U. We first observe that the distance function based on the largest main angle, which we use in the previous section, can be used directly for our more general setting. That is, dist (U, QT) = UT-QT-2 measures the component of QT \"outside\" the subspace U and the goal is to show that the component we use can be based on the largest main angle, which can be used directly for our more general setting, so that our setting can be easily modified as a more general one."}, {"heading": "5 Experiments", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "A Lemmas from Section 4.1", "text": "First, we give the statement of all the lemmas whose proofs we have omitted in the corpus of work. Then, we provide some results from the literature - what we call preliminary - and then we prove Theorem 3 and the supporting lemmas. 4. Let B, T and the data stream {xt} be defined as in Theorem 1. Let B, T and the data stream {xt} be defined as in Theorem 1. Let B, T and the data stream {xt} be defined as in Theorem 1. Let us then, w.p. 1 \u2212 C / T: u s4 (1 + 2) (1 \u2212 4 (1 + 2)) (1 \u2212 4 (1 + 2). Let B, T and the data stream {xt} be defined as in Theorem 1. Let us then, w.p. 1 \u2212 C / T: u s4 (1 \u2212 4) (1 \u2212 4 (1 + 2))) where st = 1 B BNP < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "B Lemmas from Section 4.2", "text": "Lemma 8. Let X, A, B, and T be defined as defined in Theorem 3. Also, let the variance of the sound level be F\u03c4 + 1 = 1 B, B\u03c4 < t \u2264 B (\u03c4 + 1) xtx t, and Q\u03c4 be the next iteration of algorithm 1. Then we have defined the variance of the sound level, F\u0442 + 1 = 1, w.p 1 \u2212 5C / T as defined in Lemma 8. Let X, A, B, F\u0442 + 1 \u2212 4C / T."}, {"heading": "C Preliminaries", "text": "s have a p-mesh of Sk \u2212 1 of [0, 1]. Then consider how many random vectors x1,., xn in Rp, n-p that have a sub-Gaussian distribution with parameter 1. Then for each p-p > 0 with probability at least 1 \u2212 3 of [19]. Consider independent random vectors x1,., xn in Rp, n-p that have a sub-Gaussian distribution with parameter 1. Then for each p > 0 with probability 1 \u2212 1 you have, 0 nn-p."}, {"heading": "D Proof of Theorem 3", "text": "Remember that our algorithm works in block by block; for each sample block we calculate the seigniorage + 1 = 1BB (\u03c4 + 1) - 1 = 1xtx + 1xtx - 1 - 1 - 1 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 3 - 2 - 2 - 2 - 2 - 3 - 2 - 3 - 3 - 3 - 2 - 2 - 3 - 3 - 3 - 3 - 3 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 4 - 4 4 4 - 4 4 4 - 4 4 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 4 4 4 4 4 4 4 4 4 4 4 - 4 4 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 3 - 2 - 2 - 2 - 2 - 2 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4 - 4"}, {"heading": "E Proof of Lemma 4", "text": "Note that, 1B: txtx t \u2212 uu \u2212 \u03c32I = uu 1B t (z2t \u2212 1) + 1B: t (wtw t \u2212 \u03c32I) + 1B: tztwtu + 1 B u tztw t. (17) We can now bind each of the above terms in the RHS separately. Using standard tail limits for covariance estimation (see Lemma 12), we can bind the first two terms (wtw t (wtw t) (wtw t): 1B: tw: t (z2t \u2212 1)."}, {"heading": "F Proof of Lemma 5", "text": "The proof: Let's q\u03c4 = \u221a 1 \u2212 \u03b4\u03c4u + \u0438 B) (21), u > t (now), where u > t (now) is the component of q\u03c4 that is orthogonal to u. Now, u s\u03c4 + 1 = 1B \u00b2 t (u xt) (x t qt) = 1B \u00b2 t (zt + u wt) (\u221a 1 \u2212 wt (zt + u wt) + \u0432 (t + u wt). (20) Now the first term above is a sum of B i.i.d. \u2212 chi-square variables and therefore using standard results (see Lemma 13), w.p (1 \u2212 C / T): 1B \u00b2 t (zt + u wt) w \u00b2) (1 \u2212 z \u2212 t \u00b2 t \u00b2 t (21), u > t \u00b2 t (see Lemma 13), w.p (1 \u2212 C / T), w \u00b2 t (1 \u2212 t), and v \u00b2 t (now)."}, {"heading": "G Proof of Lemma 6", "text": "Proof. Use of standard tail boundaries for Gaussian (see Lemma 13), as well as for probability 1 \u2212 exp (\u2212 C1p), where C1 > 0 is a universal constant. Furthermore, there is C0 > 0, s.t., with probability 0.99, | (= q0, 2q0) Tu | \u2265 C0. Consequently, there is C0 > 0, s.t., with probability 0.99, | (= q0, 2q0) Tu | \u2265 C0."}, {"heading": "H Proof of Lemma 2", "text": "Proof. We prove the problem by means of induction. The base case (for \u03c4 = 0) follows trivially. Now follows on the basis of the inductive hypothesis: \u03b4\u03c4 \u2264 \u03b3 2t\u03b401 \u2212 (1 \u2212 \u03b32t) \u03b40. That is, 1 \u03b4\u03c4 \u2265 1 \u2212 (1 \u2212 \u03b3 2t) \u03b40 \u03b32t\u03b40. Finally follows on the basis of the assumption: \u03b4\u03c4 + 1 \u2264 \u03b321 \u0441\u0442 \u2212 (1 \u2212 \u03b32) \u2264 \u03b321 \u2212 (1 \u2212 \u03b32t) \u03b40 \u03b32t\u03b40 \u2212 (1 \u2212 \u03b32). The problem results from the simplification of the above expression."}, {"heading": "I Proof of Lemma 8", "text": "Using the generative model (2) we get: U K K K K K K K K K K K K K K K K K K K K K K (1 B K) K K K K K K K K K (1 B K) K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K (1 B K) K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K"}, {"heading": "J Proof of Lemma 9", "text": "Similar to our proof for Lemma 8, we separate the \"error\" or deviation terms in the terms \"U\" and \"1Q\" and tie them using concentration limits. Now, we separate the \"error\" or \"deviation terms\" in the terms \"U\" and \"U.\" (U2U \"+\" E. \") Q\" 2 \"E.\" (29) where \"E2\" is the error matrix that represents the deviation of the estimated future \"futur\" + 1 from its mean value. That is, E \"1B,\" \"Txtx\" and \"U2U.\" (1B \"T.\") \"T.\" \u2212 I. \"(1B\" tztz \"T.\" (1B \"T.\") follows \"T.\" (1B \")."}, {"heading": "K Proof of Lemma 10", "text": "Proof. With step 2 of the algorithm 1: H = Q0R0. Let vk be the singular vector of U Q0 corresponding to the smallest singular value. Then, \u03c3k (U Q0) = U Q0R0R \u2212 10 vk 2 Q0R0 10 vk 2 R \u2212 10 vk 2 0 (U Q0R0) \u03c3k (R \u2212 10). (32) Well, \u03c3k (R \u2212 1 0) = 1 R0 2 = 1 Q0R0 2 = 1 H 2. Note that \"H 2\" is the spectral norm of a random matrix with i.i.d. Gaussian entries and can therefore be easily limited by standard results. Especially with Lemma 13 we get: \"H\" 2 \u2264 C1 \u2212 e \u2212 C2p, where C1, C2 > 0 are global constants. With theorem of 1.1 (now) p (17)."}], "references": [{"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "In 50th Allerton Conference on Communication,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["L. Balzano", "R. Nowak", "B. Recht"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Fast low-rank modifications of the thin singular value decomposition", "author": ["M. Brand"], "venue": "Linear algebra and its applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Incremental singular value decomposition of uncertain data with missing values", "author": ["Matthew Brand"], "venue": "Vision\u2014ECCV", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Numerical linear algebra in the streaming model", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the 41st annual ACM symposium on Theory of computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Tracking a few extreme singular values and vectors in signal processing", "author": ["P. Comon", "G.H. Golub"], "venue": "Proceedings of the IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM review,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Online robust subspace tracking from partial information", "author": ["J. He", "L. Balzano", "J. Lui"], "venue": "arXiv preprint arXiv:1109.3827,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Tracking the best linear predictor", "author": ["Mark Herbster", "Manfred K. Warmuth"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "On the distribution of the largest eigenvalue in principal components analysis.(english", "author": ["Iain M. Johnstone"], "venue": "Ann. Statist,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "On incremental and robust subspace learning", "author": ["Y. Li"], "venue": "Pattern recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Finite sample approximation results for principal component analysis: a matrix perturbation approach", "author": ["Boaz Nadler"], "venue": "The Annals of Statistics, page 2791\u20132817,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Fast collapsed gibbs sampling for latent dirichlet allocation", "author": ["Ian Porteous", "David Newman", "Alexander Ihler", "Arthur Asuncion", "Padhraic Smyth", "MaxWelling"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1951}, {"title": "EM algorithms for PCA and SPCA. Advances in neural information processing systems, page", "author": ["Sam Roweis"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Probabilistic principal component analysis", "author": ["Michael E. Tipping", "Christopher M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "How close is the sample covariance matrix to the actual covariance matrix", "author": ["R. Vershynin"], "venue": "Journal of Theoretical Probability,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "This direction was largely influenced by Johnstone\u2019s spiked covariance model, where data samples are drawn from a distribution whose (population) covariance is a low-rank perturbation of the identity matrix [11].", "startOffset": 207, "endOffset": 211}, {"referenceID": 17, "context": "Work initiated there, and also work done in [19] (and references therein) has explored the power of batch PCA in the p-dimensional setting with sub-Gaussian noise, and demonstrated that the singular value decomposition (SVD) of the empirical covariance matrix succeeds in recovering the principal components (extreme eigenvectors of the population covariance) with high probability, given n = O(p) samples.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": ", [21, 1].", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "A memory-light variant described in [1] typically requires much less memory, but there are no guarantees for this, and moreover, for certain problem instances, its memory requirement is on the order of p.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": ", [5, 13, 8].", "startOffset": 2, "endOffset": 12}, {"referenceID": 11, "context": ", [5, 13, 8].", "startOffset": 2, "endOffset": 12}, {"referenceID": 6, "context": ", [5, 13, 8].", "startOffset": 2, "endOffset": 12}, {"referenceID": 4, "context": "Indeed, it is straightforward to check that the guarantees presented in ([5, 8]) are not strong enough to guarantee recovery of the spike.", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "Indeed, it is straightforward to check that the guarantees presented in ([5, 8]) are not strong enough to guarantee recovery of the spike.", "startOffset": 73, "endOffset": 79}, {"referenceID": 3, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 5, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 14, "endOffset": 18}, {"referenceID": 1, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 37, "endOffset": 43}, {"referenceID": 7, "context": ", [4, 3], [6],[12] and more recently [2, 9]) seek to have the best subspace estimate at every time (i.", "startOffset": 37, "endOffset": 43}, {"referenceID": 14, "context": "In a Bayesian mindset, some researchers have come up with expectation maximization approaches [16, 18], that can be used in an incremental fashion.", "startOffset": 94, "endOffset": 102}, {"referenceID": 16, "context": "In a Bayesian mindset, some researchers have come up with expectation maximization approaches [16, 18], that can be used in an incremental fashion.", "startOffset": 94, "endOffset": 102}, {"referenceID": 13, "context": "Stochastic-approximation-based algorithms along the lines of [15] are also quite popular, because of their low computational and memory complexity, and excellent performance in practice.", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "They go under a variety of names, including Incremental PCA (though the term Incremental has been used in the online setting as well [10]), Hebbian learning, and stochastic power method [1].", "startOffset": 133, "endOffset": 137}, {"referenceID": 0, "context": "They go under a variety of names, including Incremental PCA (though the term Incremental has been used in the online setting as well [10]), Hebbian learning, and stochastic power method [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 0, "context": "where Proj(\u00b7) denotes the \u201cprojection\u201d that takes the SVD of the argument, and sets the top k singular values to 1 and the rest to zero (see [1] for further discussion).", "startOffset": 141, "endOffset": 144}, {"referenceID": 18, "context": "In this regime, it is well-known that batch-PCA is asymptotically consistent (hence recovering A up to unitary transformations) with number of samples scaling as n = O(p) [20].", "startOffset": 171, "endOffset": 175}, {"referenceID": 0, "context": "Stochastic versions of the power method are already popular in the literature and are known to have good empirical performance; see [1] for a nice review of such methods.", "startOffset": 132, "endOffset": 135}, {"referenceID": 12, "context": "We used three bag-of-words datasets from [14].", "startOffset": 41, "endOffset": 45}], "year": 2013, "abstractText": "We consider streaming, one-pass principal component analysis (PCA), in the highdimensional regime, with limited memory. Here, p-dimensional samples are presented sequentially, and the goal is to produce the k-dimensional subspace that best approximates these points. Standard algorithms require O(p2) memory; meanwhile no algorithm can do better than O(kp) memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the spiked covariance model, where p-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, n, scales proportionally with the dimension, p. Yet, all algorithms that provably achieve this, have memory complexity O(p2). Meanwhile, algorithms with memory-complexity O(kp) do not have provable bounds on sample complexity comparable to p. We present an algorithm that achieves both: it uses O(kp) memory (meaning storage of any kind) and is able to compute the k-dimensional spike with O(p log p) sample-complexity \u2013 the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.", "creator": "LaTeX with hyperref package"}}}