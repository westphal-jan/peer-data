{"id": "1307.1493", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2013", "title": "Dropout Training as Adaptive Regularization", "abstract": "Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learner, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.", "histories": [["v1", "Thu, 4 Jul 2013 21:33:56 GMT  (62kb,D)", "https://arxiv.org/abs/1307.1493v1", "10 pages"], ["v2", "Fri, 1 Nov 2013 17:56:35 GMT  (67kb,D)", "http://arxiv.org/abs/1307.1493v2", "11 pages. Advances in Neural Information Processing Systems (NIPS), 2013"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["stefan wager", "sida i wang", "percy liang"], "accepted": true, "id": "1307.1493"}, "pdf": {"name": "1307.1493.pdf", "metadata": {"source": "CRF", "title": "Dropout Training as Adaptive Regularization", "authors": ["Stefan Wager", "Sida Wang", "Percy Liang"], "emails": ["swager@stanford.edu,", "pliang}@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "It is indeed the case that most of them are able to outdo themselves, and that they are able to outdo themselves, \"he said.\" But it is not the case that they feel able to outdo themselves. \"He added,\" It is not as if they are able to outtrump themselves, but that they are able to outtrump themselves. \"He added,\" It is not as if they are able to outtrump themselves, as if they are able to outtrump themselves. \""}, {"heading": "2 Artificial Feature Noising as Regularization", "text": "We begin by discussing the general relationships between noise and regularization in generalized linear models (GLMs). (...) We will apply the mechanisms developed here. (...) We will apply the way in which noise (...) is applied. (...) We will apply the way in which noise (...) is applied. (...) We will apply the way in which noise (...) is applied. (...) The way in which noise (...) is applied is not the way in which noise (...) is applied. (...) We will apply the way in which noise (...) is applied. (...) We will apply the way in which noise (...) is applied. (...) We will apply the way in which noise (...) is applied. (...) We will apply the way in which noise (...) is applied. (...)"}, {"heading": "2.1 A Quadratic Approximation to the Noising Penalty", "text": "Although the noise of the penalty R results in an explicit approximation of the type used by [9, 10], the shape of R can be difficult to interpret. To gain more insight, we will work with a square approximation of the type used by [9, 10]. Although the first order Taylor extension of A is x \u00b7 \u03b2, we get that evolution (x-x) -square approximation to (x-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2) [A-\u03b2] (x-\u03b2) [x-\u03b2) [x-x]] -square approximation to (x-\u03b2) -square approximation to (5) Varos (x-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2) Varos (x-\u03b2)."}, {"heading": "3 Regularization based on Additive Noise", "text": "Having established the general quadratic noise regulator Rq, we now turn to the study of the effects of Rq for various probabilities (linear and logistic regression) and sound models (additive and dropout). In this section, we will warm up with additive noise; in section 4, we will turn to our main objective of interest, namely dropout noise. Linear regression is generated by adding noise with Var [\u03b5] = \u03c32Id \u00b7 d to the original feature vector x. Note that Varis [x-\u03b2] = \u03c32 \u0445\u03b2-22 and in the case of linear regression A (z) = 12z2, so that A \u2032 \u2032 (z) = 1. Applying these facts to (6) yields a simplified form for the quadratic regression (\u03b2) when the square noise penalty: Rq (\u03b2) = 12-\u03b2 \u2212 Thus we restore the well-known result."}, {"heading": "4 Regularization based on Dropout Noise", "text": "We remember that the dropout training corresponds to the dropout examples where the dropout rates are higher than the probability of failure. (...) If the dropout rates are so high that the dropout rates are so high that the dropout rates are so high as never before, we can check whether the dropout rates are so high that the dropout rates are so high as never before. (...) If the dropout rates are so high that the dropout rates are so high as never before, then the dropout rates are so high as never before. (...) If the dropout rates are so high as never before, then the dropout rates are so high. (...) If the dropout rates are so high that the dropout rates are so high as never before. (...) If the dropout rates are so high as never before, then the dropout rates are so high."}, {"heading": "4.1 A Simulation Example", "text": "To empirically test this intuition, we developed a simulation study in which the entire signal is grouped into 50 rare features, each of which is active only 4% of the time. To ensure that our experiment specifically captured the effect of the dropout training \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7."}, {"heading": "5 Dropout Regularization in Online Learning", "text": "There is a problem with classical SGD, that there can be slow learning processes, which can be highly discriminatory. (SGD). (SGD)..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "6 Semi-Supervised Dropout Training", "text": "Remember that the regulator R (\u03b2) in (5) is independent of the labels (\u03b2) and (S). As a result, we can use additional unlabeled training examples to estimate them more accurately. Suppose we have an unlabeled record {zi} of size m, and let \u03b1 (0, 1] be a discount factor for the unlabeled data. Then, we can set a semi-monitored penalty scale containing R (\u03b2) def = nn + \u03b1m (R (\u03b2) + \u03b1RUnlabeled (\u03b2), (19) where R (\u03b2) denotes the original penalty estimate and RUnlabeled (\u03b2) denotes the labeled data."}, {"heading": "7 Conclusion", "text": "We analyzed drop-out training as a form of adaptive regularization. This framework enabled us to uncover close links between drop-out training, adaptive balanced L2 regularization and AdaGrad; and led to a simple but effective method for semi-supervised training. There seem to be several ways to explore the link between drop-out training and adaptive regularization more deeply. In particular, it would be interesting to see whether the drop-out regularizer in neural networks takes a traceable and / or interpretable form and whether similar semi-supervised programs could be used to improve the results presented in [1]."}, {"heading": "A Appendix", "text": "A.1 Description of the simulation study section 4.1 gives the motivation for and a high-level description of our simulation study. Here we give a detailed description of the study.Generation of characteristics. Our simulation has 1050 characteristics. The first 50 discriminatory characteristics form 5 groups of 10 each; the last 1000 characteristics are annoying terms. Each xi has been generated independently as follows: 1. Select a group number g-1,..., 25 and a character sgn = \u00b1 1. 2. If g-5, drag the entries of xi with index between 10 (g-1) + 1 and 10 (g-1) + 10 uniformly from sgn \u00b7 Exp (C), where C is selected so that E [(xi) 2j] = 1 for all j. Set all other discriminatory characteristics to 0. If g-5, set all discriminatory characteristics to 0.3. Drag the last 1000 entries of xi independently from N (0).1 remark that this method guarantees that columns are generated from 2.0 to 1."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Dropout and other feature noising schemes control overfitting by artificially cor-<lb>rupting the training data. For generalized linear models, dropout performs a form<lb>of adaptive regularization. Using this viewpoint, we show that the dropout regular-<lb>izer is first-order equivalent to an L2 regularizer applied after scaling the features<lb>by an estimate of the inverse diagonal Fisher information matrix. We also establish<lb>a connection to AdaGrad, an online learning algorithm, and find that a close rel-<lb>ative of AdaGrad operates by repeatedly solving linear dropout-regularized prob-<lb>lems. By casting dropout as regularization, we develop a natural semi-supervised<lb>algorithm that uses unlabeled data to create a better adaptive regularizer. We ap-<lb>ply this idea to document classification tasks, and show that it consistently boosts<lb>the performance of dropout training, improving on state-of-the-art results on the<lb>IMDB reviews dataset.", "creator": "LaTeX with hyperref package"}}}