{"id": "1307.3176", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2013", "title": "Fast Gradient Descent for Drifting Least Squares Regression, with Application to Bandits", "abstract": "We improve the computational complexity of online learning algorithms that require to often recompute least squares regression estimates of parameters. We propose two stochastic gradient descent schemes with randomisation in order to efficiently track the true solutions of the regression problems achieving an O(d) improvement in complexity, where d is the dimension of the data. The first algorithm assumes strong convexity in the regression problem, and we provide bounds on the error both in expectation and high probability (the latter is often needed to provide theoretical guarantees for higher level algorithms). The second algorithm deals with cases where strong convexity of the regression problem cannot be guaranteed and uses adaptive regularisation. We again give error bounds in both expectation and high probability. We apply our approaches to the linear bandit algorithms PEGE and ConfidenceBall and demonstrate significant gains in complexity in both cases. Since strong convexity is guaranteed by the PEGE algorithm, we lose only logarithmic factors in the regret performance of the algorithm. On the other hand, in the ConfidenceBall algorithm we adaptively regularise to ensure strong convexity, and this results in an O(n^{1/5}) deterioration of the regret.", "histories": [["v1", "Thu, 11 Jul 2013 16:36:29 GMT  (27kb)", "http://arxiv.org/abs/1307.3176v1", null], ["v2", "Wed, 19 Feb 2014 00:27:18 GMT  (1545kb,D)", "http://arxiv.org/abs/1307.3176v2", null], ["v3", "Thu, 24 Jul 2014 14:29:52 GMT  (3757kb,D)", "http://arxiv.org/abs/1307.3176v3", null], ["v4", "Thu, 20 Nov 2014 12:40:48 GMT  (104kb,D)", "http://arxiv.org/abs/1307.3176v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nathaniel korda", "prashanth l a", "r\u00e9mi munos"], "accepted": true, "id": "1307.3176"}, "pdf": {"name": "1307.3176.pdf", "metadata": {"source": "CRF", "title": "Online gradient descent for least squares regression: Non-asymptotic bounds and application to bandits", "authors": ["Nathaniel Korda"], "emails": ["nathaniel.korda@inria.fr", "prashanth.la@inria.fr", "remi.munos@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 130 7,31 76v1 [cs.LG] 1 1Ju l 2"}, {"heading": "1 Introduction", "text": "The solution to the problem is very large, and the number of samples we use is large, and the number of samples we use is large."}, {"heading": "Main Contributions", "text": "We propose two online algorithms - both randomly evaluating random samples, one without regulation and another with adaptive regulation. These algorithms necessarily pursue the corresponding solution of the smallest squares and can be regarded as SGD schemes. The former converges at the rate of O (n \u2212 1 / 2). The rate of convergence of the latter depends on the rate of convergence of the regulated lowest squares, which in turn is problem-dependent. Both cause only O (d) per iteration. \u2022 We demonstrate the usefulness of our algorithms by applying them to the linear bandit setting, using them instead of solving the least square regression steps. For the PEGE algorithm of [7], we achieve an improvement in complexity while only causing a logarithmic deterioration of regret."}, {"heading": "2 Algorithms", "text": "First, in Section 2.1, we present an incremental online algorithm that solves a problem of regression with the least squares at each step. Next, we extend this algorithm to include (adaptive) regularization in Section 2.2. For each of these algorithms, we provide non-asymptotic limits, which include both a high probability and limits in anticipation of the removal of the online algorithm from the (unknown) parameter \u03b8."}, {"heading": "2.1 Random Online", "text": "Remember that \u03b8-n: = min \u03b8 1 2n \u2211 i = 1 (yi \u2212 \u03b8Txi) 2."}, {"heading": "Assumptions", "text": "(A1) Limit value of xn, i.e., supn, xn, xn, xn, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "2.2 Random Online with Regularisation", "text": "We can get rid of this dependency by introducing a regularization parameter, and so we have to adjust the solution approaches in each step by adapting to the size of the batch. Recall errors are: = argmino 12n. (yi \u2212 2) In an online setting, however, we expect to receive arbitrary amounts of information, and so we have to follow the following algorithm attempts to shadow the solutions more closely than n. (yin \u2212 2) The following algorithm attempts to shadow the solutions are as narrow as n \u2192. (yin \u2212 1) We (yin \u2212 1xin) xin \u2212 2) where we have the ability to shadow each other. (1)."}, {"heading": "3 Linear Bandits with Strongly Convex Arms", "text": "The observations yn = ln (xn) are interpreted as lost satisfactory E [ln (xn) | xn] = xTn\u043d, where there is an unknown parameter. (In this section we assume that D is a strongly convex set (see (A3) below) and the \"best action\" function designated by G (n) is assumed to be smooth in the unknown parameter that regulates the losses of the bandit algorithm (see (A4) below).Phased Exploration and Greedy Exploration (PEGE) the algorithm is a known algorithm in this set.PEGE consists of exploration elements and exploration elements. (see (A) Phased Exploration and Greedy Exploration (PEGE) the algorithm is a known algorithm in this set.PEGE consists of exploration elements and exploration elements."}, {"heading": "3.1 Analysis", "text": "We need the following additional assumptions of [7]: (A3 ') A base (b1,.., bd) D for Rd is provided to the entire algorithm. (A4) The function G: \u03b8 \u2192 argminx (D) is J-Lipschitz. Among the assumptions (A1), (A2), (A3') and (A4), it is not satisfied if D is discrete. We offer a slightly modified version of Theorem 3.1 of [7]: Theorem 7. Among the assumptions (A1), (A2), (A3 ') and (A4), the cumulative regret of PEGE with online GD is satisfactoryRn (C1): Theorem 7.1 of [7]: Theorem 7. Among the assumptions (n), (A1), (A2), (A3'), and (A), the cumulative regret of PEGE with online GD satisfaction \u2212 CG (1)."}, {"heading": "4 Linear Bandits with Non-Strongly Convex Arms", "text": "The setting here resembles that of the previous section, except that assumptions (A3 ') and (A4) do not apply. In other words, the arms are not associated with a strongly convex subset of Rd - a property of which it is not known that it applies in many practical environments, for example in message machines in which the arms are discrete. The well-known confidence ball algorithm of [2] works by making a minimum square estimate by using {(xi, yi)} ni = 1 in the order of magnitude O and using this estimate to greedily select an arm, i.e., argminx, D minv, B2n, {vTx}. The regret Rn with this algorithm has proved to be of the order of magnitude O (xi, yi) ni = 1 in [2].We propose an improvement of the confidence ball algorithm, i.e., D minv, B2\u00ba \u00ba, {vTx}. The regret Rn with this algorithm is of the order of magnitude O (2\u00ba, yi) ni = 1 in [2].We propose an improvement of the confidence ball algorithm, i.e., i.e., 2\u00ba \u00ba \u00ba \u00ba, \u00ba \u00ba \u00ba \u00ba \u00ba \u00ba \u00ba \u00ba, {vTx}. The regret Rn with this algorithm is of the regulated version of the online solution of the random solution of an estimate of an online, 2\u00ba \u00ba \u00ba \u00ba \u00ba \u00ba \u00ba \u00ba, what to the order of a confidence, 2\u00ba \u00ba, 2\u00ba, 2\u00ba, 2\u00ba, 2\u00ba to the improvement of the confidence ball algorithm 2\u00ba, 2\u00ba, 2\u00ba, 2\u00ba, 2\u00ba, 2\u00ba"}, {"heading": "4.1 Analysis", "text": "We make the following additional assumption: (A1 ') An upper limit that we can define as part for which we can define the upper limit (A1), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A1), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (A2), (2), (2), (2), (2), (A2), (A2), (A2), (2), (A2), (A2), (2), (2), (A2, (2), (A2), (A2), (2), (A2), (2), (A2), (2), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), (1), ("}, {"heading": "5 Conclusions", "text": "We have proposed two randomized SGD schemes for the problem of reducing complexity in the least quadratic regression problems, where the samples do not come from a probability distribution, but are instead given by a higher learning algorithm. In particular, if the higher algorithm can guarantee strong convexity in the data, we have proposed an unregulated scheme, and otherwise we have proposed an adaptively regulated scheme. We have set error limits both in expectation and in high probability for the two schemes. To demonstrate the applicability of these results, we have applied both schemes to the linear bandit problem, the unregulated scheme in the PEGE algorithm, and the regulated scheme in the ConfidenceBall algorithm. We have found that a strong convexity of the data is guaranteed by the PEGE algorithm that we will provide an acceleration of O (d) at a price of logarithmic regret only."}, {"heading": "A Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Random Online", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Theorem 1", "text": "We use the evidence technology from [4] to [5] to [6] to [6] to [7] to [7] to [8] to [8] to [8] to [8] to [8] to [9] to [9] to [9] to [9] to [10] to [11] to [11] to [11] to [12] to [12] to [12] to [12] to [12] to [12] to [12] to [12] to [12] to [15] to [15] to [16] to [16] to [16] to 17 [16] to 17 [16] to 17 [16] to 17 [12] to [12] to [12] to [12] to [12] to [12] to [12] to [12] to [14] to [14] to [15] to [15] to [16] to [16] to [16] to [16] to [16 [16] to 17 [16] to 16 to 17 \"to 17."}, {"heading": "Proof of Theorem 2", "text": "& & & # 252; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8220; (F & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8220; n & # 222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 220; n # 222; n & # 220; n & # 222; n & # 222; n & # 222; n & # 222; n & # 222; n & # 222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8225; n & # 220; n # 220; n # 220; n # 222; n & # 222; n & # 222; n & # 222; n & # 222; n & # 8222; n & # 8222; n & # 222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n"}, {"heading": "A.2 Regularized Random Online", "text": "The proof of the high probability of (adaptive) regularized constellation follows in a similar manner to the proof of theorem 1 and uses the fact that any F'n-converx-convervex-convervex-convervex-convervex-convervex-convervex-convervex-convervex-convervex-converv-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter-converter"}, {"heading": "A.3 Proof of Theorem 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 5", "text": "The proof: On the basis of the analysis of section 2.1 for the selection of the step variables \u03b3n = cn\u03b1 with c > 12\u00b5 and \u03bbn = \u00b5 / n1 \u2212 \u03b1, we obtain the following limit of (2,2) P (2,2) P (2,2) P (2,2) P (2,2) P (2,2) E (2,2) E (2,2) P (2,2) P (2,2) P (2,2) P (2,2) E (2,2) P (2,2) P (2,0) P (2,2) P (2,2) P (2,2) P (2,0) P (2,0) P (2,2))."}, {"heading": "Proof of Lemma 6", "text": "The detection that Vn = A 1x2Tn + 1x2Tn + 1x2Tn + 1x2T + 1x2T + 1x2T + 1x2T + 1x2T + 1x2T + 1Vn + 1Vn + 1Vn + 1Vn + 1Vn + 1Vn + 1Vn + 1Vn + 1Vn + 1Vn (1Vn + 1Vn), 1Vn + 1Vn (1Vn + 1Vn), 1Vn + 1Vn (1Vn + 1Vn), 1Vn (1Vn), 1Vn (1Vn), 1Vn (1Vn), 1Vn (1Vn), 1Vn (1Vn), 1Vn (1Vn), 1Vn (1Vn), 1Vn (1Vn, 1Vn, 1Vn (1Vn), 1Vn (1Vn, 1Vn, 1Vn), 1Vn (1Vn, 1Vn, 1Vn (1Vn), 1Vn (1Vn, 1Vn), 1Vn (1Vn, 1Vn, 1Vn), 1Vn (1Vn, 1Vn, 1Vn), 1Vn (1Vn, 1Vn, 1Vn), 1Vn (1Vn (1Vn, 1Vn, 1Vn, 1Vn, 1Vn), 1Vn (1Vn, 1Vn, 1Vn, 1Vn, 1Vn), 1Vn (1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Vn), 1Vn (1Vn (1Vn, 1Vn, 1Vn), 1Vn (1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Vn, 1Tn, 1Vn, 1Tn, 1Vn, 1Tn, 1Vn, 1Vn, 1Vn, 1"}], "references": [{"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["Francis Bach", "Eric Moulines"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P Hayes", "Sham M Kakade"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes", "author": ["Max Fathi", "Noufel Frikha"], "venue": "arXiv preprint arXiv:1301.7740,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Concentration Bounds for Stochastic Approximations", "author": ["Noufel Frikha", "Stphane Menozzi"], "venue": "Electron. Commun. Probab.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1109.5647,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Linearly parameterized bandits", "author": ["Paat Rusmevichientong", "John N. Tsitsiklis"], "venue": "Math. Oper. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Online learning as stochastic approximation of regularization paths", "author": ["Pierre Tarr\u00e8s", "Yuan Yao"], "venue": "arXiv preprint arXiv:1103.5538,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "As examples of a higher level learning algorithm using regression as a subroutine, we consider two linear bandit algorithms, the PEGE algorithm of [7] and the ConfidenceBall algorithm of [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "As examples of a higher level learning algorithm using regression as a subroutine, we consider two linear bandit algorithms, the PEGE algorithm of [7] and the ConfidenceBall algorithm of [2].", "startOffset": 187, "endOffset": 190}, {"referenceID": 6, "context": "The PEGE algorithm of [7] is designed for action sets D satisfying a strong convexity property (see assumption (A4) in Section 3).", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "For the PEGE algorithm of [7] our result achieves an O(d) improvement in complexity, while incurring only a logarithmic deterioration in the regret.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "For the ConfidenceBall algorithm of [2] we again achieve a complexity improvement of O(d), while incurring an O(n1/5) deterioration in the regret.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Non-asymptotic bounds in expectation for SGD schemes have been provided in [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "In the machine learning community, several algorithms have been proposed for minimizing the regret, for instance, [9, 5, 6] and these can be converted to find the minimizer of a (usually convex) function.", "startOffset": 114, "endOffset": 123}, {"referenceID": 5, "context": "In the machine learning community, several algorithms have been proposed for minimizing the regret, for instance, [9, 5, 6] and these can be converted to find the minimizer of a (usually convex) function.", "startOffset": 114, "endOffset": 123}, {"referenceID": 3, "context": "A closely related field is stochastic approximation (SA) and concentration bounds for SA algorithms have been provided in [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "Adaptive regularisation in the context of least squares regression has been analysed in [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Phased Exploration and Greedy Exploitation (PEGE) of [7] is a well-known algorithm in this setting.", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "1 Analysis We require the following extra assumptions from [7]: (A3\u2019) A basis {b1, .", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "1 of [7]: Theorem 7.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "6 of [7]: Lemma 8.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "5 of [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "1 of [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "The well-known confidence ball algorithm of [2] works by constructing a least squares estimate \u03b8\u0302 using {(xi, yi)}i=1 around \u03b8\u2217 and using this estimate to pick an arm greedily, i.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "The regret Rn with this algorithm was shown to be of the order O( \u221a dn lnn) in [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "In our setting, Theorem 4 serves as an analogue to Theorem 5 of [2].", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "References [1] Francis Bach, Eric Moulines, et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Varsha Dani, Thomas P Hayes, and Sham M Kakade.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Max Fathi and Noufel Frikha.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Noufel Frikha and Stphane Menozzi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Elad Hazan and Satyen Kale.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Paat Rusmevichientong and John N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Pierre Tarr\u00e8s and Yuan Yao.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "1 Random Online Proof of Theorem 1 We use the proof technique from [4] in arriving at the high-probability bounds.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "The main ingredients of this derivation can be found in the argument of page 15 in [3], however here we manage to give all the constants explicitly.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "In the second inequality we have used an Abel transform (see page 15 in [3], display (2.", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "For the last inequality we have noted, as in page 15 in [3], that", "startOffset": 56, "endOffset": 59}], "year": 2017, "abstractText": "We improve the computational complexity of online learning algorithms that require to often recompute least squares regression estimates of parameters. We propose two stochastic gradient descent schemes with randomisation in order to efficiently track the true solutions of the regression problems achieving an O(d) improvement in complexity, where d is the dimension of the data. The first algorithm assumes strong convexity in the regression problem, and we provide bounds on the error both in expectation and high probability (the latter is often needed to provide theoretical guarantees for higher level algorithms). The second algorithm deals with cases where strong convexity of the regression problem cannot be guaranteed and uses adaptive regularisation. We again give error bounds in both expectation and high probability. We apply our approaches to the linear bandit algorithms PEGE and ConfidenceBall and demonstrate significant gains in complexity in both cases. Since strong convexity is guaranteed by the PEGE algorithm, we lose only logarithmic factors in the regret performance of the algorithm. On the other hand, in the ConfidenceBall algorithm we adaptively regularise to ensure strong convexity, and this results in an \u00d5(n1/5)1 deterioration of the regret.", "creator": "LaTeX with hyperref package"}}}