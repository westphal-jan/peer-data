{"id": "1307.4145", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jul-2013", "title": "A Safe Screening Rule for Sparse Logistic Regression", "abstract": "The l1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the 0 components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Extensive experimental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general.", "histories": [["v1", "Tue, 16 Jul 2013 02:03:51 GMT  (304kb,D)", "https://arxiv.org/abs/1307.4145v1", null], ["v2", "Thu, 18 Jul 2013 22:57:16 GMT  (304kb,D)", "http://arxiv.org/abs/1307.4145v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jie wang", "jiayu zhou", "jun liu 0003", "peter wonka", "jieping ye"], "accepted": true, "id": "1307.4145"}, "pdf": {"name": "1307.4145.pdf", "metadata": {"source": "CRF", "title": "A Safe Screening Rule for Sparse Logistic Regression", "authors": ["Jie Wang", "Jiayu Zhou", "Jun Liu", "Peter Wonka", "Jieping Ye"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Basics and Motivations", "text": "In this section, we briefly review the basics of \"1 regulated LR\" and then motivate the general screening rules on CCT conditions. Suppose we get a set of training samples [xi] mi = 1 and the associated terms b < m, where xi < p and bi {1, \u2212 1} for all i,. < m}. The \"1 regulated logistic regression is: min \u03b2, c1m m \u00b2 i = 1 protocol (1 + exp (\u2212 < p and bi \u00b2 i > \u2212 bic)) + 1, (LRPPs), where \u03b2 < p and c \u00b2 < are the model parameters to be estimated, x \u00b2 i = bixi, and \u03bb > 0 is the yardstick. Leave the data matrix, the X < m \u00b2 p with the series x."}, {"heading": "3 Estimating the Upper Bound via Solving a Convex Optimiza-", "text": "In this section, we present a novel framework for estimating an upper limit. < p > p) In our Slores rule to be presented in Section 5, we set 0 and p \u00b2 0 to allow a closed form solution in Section 4. < p > p) We formulate the estimate of T (both) as a limited convex optimization problem in Section 5, showing that a closed form solution is allowed in Section 4. (1) and (2). We formulate the estimate of T (both), x), as a limited convex optimization problem in Section 5, showing that a closed form solution is allowed in Section 4. (2) For the double function g (both), it follows that [both] i = 1m Log (both). (Lemm)."}, {"heading": "4 Solving the Convex Optimization Problem (UBP)", "text": "In this section we show how to solve the konvexe optimization problem (42) using the standard Lagrangian multiplier method. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "5 The proposed Slores Rule for `1 Regularized Logistic Regression", "text": "With the help of (R1) we are now ready to construct the screening rules for the \"1 Regulated Logistic Regression.\" Using Korollary 5 we can see that the orthogonality between the jth attribute and the response vector b implies the absence of x-j from the resulting model. In the general case where Px-j 6 = 0, (R1) implies that if T-j attribute, x-j attribute and the response vector b miss, the absence of x-j from the resulting model."}, {"heading": "6 Experiments", "text": "We evaluate our screening rules using the new group data set [12] and the Yahoo website data sets [26], and the new group data set is cultivated using data from Koh et al. [12]. Yahoo data sets comprise 11 top-level categories, each of which is further divided into a set of sub-categories. However, in our experiment we construct five balanced binary classification data sets from computer, education, health, recreation and science topics. For each topic, we select samples from a sub-category as a positive class and sample an equal number of samples from the remaining sub-categories as a negative class. Statistics of the data sets are in Table 1.We compare the performance of Slores and the strict rule that achieves state-of-the-art performance for \"1 regulated LR. We do not include SAFE because it is less effective than strong features in discarded characteristics and requires much more computational time."}, {"heading": "6.1 Comparison of Performance", "text": "In this experiment, we evaluate the performance of the lores and the strong rule based on the rejection ratio. Figure 2 shows the rejection ratio of lores and the strong rule based on six real datasets. If \u03bb / \u03bbmax > 0.5, we can see that both slores and the strong rule are able to identify almost 100% of the inactive characteristics, i.e. characteristics with 0 coefficients in the solution vector. However, if \u03bb / \u03bbmax \u2264 0.5, the strong rule cannot detect the inactive characteristics. In contrast, we find that slores have a much greater ability to discard inactive characteristics for small \u03bb, even if \u03bb / \u03bbmax is close to 0.1. If we take the data point where \u03bb / \u03bbmax = 0.1, for example, Slores discards the inactive characteristics for the newsgroup dataset, Slores has about 99% inactive characteristics. In the other datasets, more than 80% of inactive characteristics are identified by slores."}, {"heading": "6.2 Comparison of Efficiency", "text": "We compare the efficiency of slores with the strong rule in this experiment. The data sets used to evaluate the rules are the same as in Section 6.1. The running time of the screening rules mentioned in Fig. 3 includes the calculation costs of the rules themselves and that of the solver after the screening. We record the running time of the screening rules against that of the solver without screening. As shown in Fig. 2, when \u03bb / \u03bbmax > 0.5, slores and strong rule discard almost 100% of the inactive features, the size of the feature matrix involved in the optimization of the problem (LRP\u03bb) decreases considerably. From Fig. 3, we can find that the efficiency is improved on average by about an order of magnitude compared to that of the solver without screening."}, {"heading": "7 Conclusions", "text": "Extensive numerical experiments with real data show that Slores exceeds existing, modern screening rules. We plan to expand the scope of Slores to more general, sparse formulations, including convex formulations such as group lasso, fused lasso '1 regulated SVM, and non-convex formulations such as \"p regulated problems where 0 < p < 1."}, {"heading": "A Deviation of the Dual Problem of the Sparse Logistic Regres-", "text": "The logistical regression problem is as follows: min \u03b2, c1m m \u00b2 i = 1 log (1 + exp (1 + exp (\u2212 < \u03b2, x \u00b2 i >)) + 1 log (1 + exp (1 + exp (\u2212 2))) + 2 log (1) + 2 log (1) in which \u03b2 < p and c < p and c < are the model parameters to be estimated, x \u00b2 m \u00b2 (1) and x \u00b2 (1 + exp (1). Let us specify the data matrix whose rows consist of x \u00b2 i. Denote the columns of X < p and c \u00b2 < p \u00b2 < p < p)."}, {"heading": "B Proof of the Existence of the Optimal Solution of (LRD\u03bb)", "text": "In this section we prove that the problem (LRDII) is practicable for all problems (LRDII). (LRDII) Then Lemma B unequivocally confirms the existence of the double optimal solution. (LRDII) Obviously the double optimal solution. (LRDII). (LRDII). (LRDII). (LRDII). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LRD). (LD). (LRD). (LRD). (LD. (LRD). (LD). (LD. (LRD). (LD). (LD). (LRD). (LD). (LD). (LRD). (LRD). (LRD). (LD). (LRD). (LRDII). (LRDII). (LRD). (LRDII). (LRD). (LRDII). (LRD). (LRD). (LRDII). (LRD). (LRD). (LRD). (LRDII). (LRD). (LRDII). (LRD). (LRD). (LRDII). (LRD). (LRD). (LRD). (LRD). (LRDII). (LRD). (LRD). (LRD). (LRD). (LRD). (LRDII). (LRD). (LRD). (LRD). (LRD). (LRDII). (LRD). (LRDII). (LRD). (LRD). (LR"}, {"heading": "C Proof of Lemma 1", "text": "Remember that the domain of g is such a domain of i. < m: [error] i (0, 1), i (1), i (1), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), 2 (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2), i (2),"}, {"heading": "D Proof of Lemma 3", "text": "The proof. After defining \u03bbmax in Eq. (1), it must also be j0. (1,.., p) in such a way that \u03bbmax = 1 m. (1,.) Suppose the value is empty, then the KKT condition for (LRD\u03bb) can in any case be: 0. (1,.) + p. (1,.) Suppose the value is empty, then the KKT condition for (LRD\u03bb) can in any case be: 0. (.) + p. (.) + p. (.) Suppose it is empty, then the KT condition for (.) is in any case. < p +. (.) p. (.) p. (.).)."}, {"heading": "E Proof of Theorem 4", "text": "proof. Remember that we must solve the following optimization problem: T (ziped, x-j; ziped, x-j-0): = max\u03b8 {| < \u03b8, x-j > |:]. (42) The feasibility of this solution implies that < \u03b8, b > = 0, i.e. that we belong to the orthogonal complement of the space spanned by b. Consequently, we can see that T (ziped, x-j, x-j-0) = 0, which completes the proof."}, {"heading": "F Proof of Corollary 5", "text": "Proof: We set \u03bb0 = \u03bbmax and \u03b8 \u0445 \u03bb0 = \u03b8 \u0445 \u03bbmax. It is clear that we have \u03bb0 > \u03bb > 0 and \u03b8 \u0445 \u03bb0 is known. Consequently, the assumptions in theorem 4 are fulfilled and thus T (\u03b8 \u0445 \u03bb, x-j; \u03b8 \u0445 \u03bb0) = 0.According to the rule in (R1) it is easy to see that [\u03b2 \u043a \u03bb] j = 0 completes the proof."}, {"heading": "G Proof of Lemma 6", "text": "We show that the state of the Slater is valid for (UBP). Remember that the possible problem set (UBP) is A\u03bb\u03bb0 = {\u03b8: \u03b8 \u03b8 - \u03b8 \u03b8 \u03bb0 \u0445 2 \u2264 r2, < TB, b > = 0, < TB, x = 0, < TB, x > \u2264 m\u03bb}. To show that the state of the Slater is valid, we need to look for a point that is such that the last two limitations of the A-Plan are automatically fulfilled. On the other hand, the 0-Plan-Goal (0, \u03bbmax], x-Plans, x-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Planltr Plans-Plans-Plans-Plans-Plans-2-Plans-Plans-Plans-Plans-Planltr Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Planltr Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Plans-Planlt;"}, {"heading": "H Proof of Lemma 7", "text": "The proof: The Lagrangian of the (UBP) group (UBP) is the (UBP) group (U1, U2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2, V2."}, {"heading": "I Proof of Theorem 8", "text": "In this section we show that the results in Theorem 8 apply, in the case where < Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, Px, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>The `1-regularized logistic regression (or sparse logistic regression) is a widely used method for si-<lb>multaneous classification and feature selection. Although many recent efforts have been devoted to its<lb>efficient implementation, its application to high dimensional data still poses significant challenges. In<lb>this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify<lb>the \u201c0\u201d components in the solution vector, which may lead to a substantial reduction in the number<lb>of features to be entered to the optimization. An appealing feature of Slores is that the data set needs<lb>to be scanned only once to run the screening and its computational cost is negligible compared to that<lb>of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse<lb>logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We<lb>have evaluated Slores using high-dimensional data sets from different applications. Extensive experi-<lb>mental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the<lb>efficiency of solving sparse logistic regression is improved by one magnitude in general.", "creator": "LaTeX with hyperref package"}}}