{"id": "1307.4653", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2013", "title": "A New Convex Relaxation for Tensor Completion", "abstract": "We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.", "histories": [["v1", "Wed, 17 Jul 2013 14:38:47 GMT  (26kb)", "http://arxiv.org/abs/1307.4653v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["bernardino romera-paredes", "massimiliano pontil"], "accepted": true, "id": "1307.4653"}, "pdf": {"name": "1307.4653.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["bernardino.paredes.09@ucl.ac.uk", "m.pontil@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 130 7.46 53v1 [cs.L"}, {"heading": "1 Introduction", "text": "In recent years, the number of working women in Germany has multiplied, with the result that the number of working women rose by only 0.2 percent in the first six months of this year."}, {"heading": "2 Preliminaries", "text": "In this section we begin by introducing some notations and then describing the learning problem. (W) \"That's the way we care about answering the question.\" (W) \"That's the way we care about answering the question.\" (W) \"That's the way we care about answering the question.\" (W) \"That's the way we care about answering the question.\" (W) \"That's the way we care about answering the question.\" (W) \"That's the way we care about answering the question.\" (W) \"That's the way we care about answering the question.\" (W) \"It's the way we care about answering the question.\" (W) \"It's the way we care about answering the question.\""}, {"heading": "3 Alternative Convex Relaxation", "text": "In this section, we will show that the tensor trace standard is not a narrow convex function. (...) In this section, we will show that the tensor trace standard is not a narrow convex function. (...) In this section, we will show that the tensor trace standard is not a narrow convex function. (...) In this section, we will show that the tensor trace standard in Equation (3) has a convex-lower limit to R than the convex wrapping is a demanding task and one must resort to approximations. (...) In this section, the authors point out that the tensor trace standard in Equation (3) has a convex-lower limit to R. (...) The convex wrapping is a demanding task and one must resort to approximations. (...) In this section, the authors point out that the tensor trace standard has a lower limit to R in the convex standard (3)."}, {"heading": "4 Optimization Method", "text": "In this section, we will explain how the regularization problem can be solved in connection with the proposed regularizer (6). To this end, we first recall the method of multipliers with changing directions (ADMM) [3], which was practically applied in [8, 21] to the regularization of tensor tracks."}, {"heading": "4.1 Alternating Direction Method of Multipliers (ADMM)", "text": "To explain the problem, we need to look at a general problem involving both tensors. (...) This problem is an error function, namely a function that is symmetric and invariant among permutations. (...) Especially if the problem is 1 norm, which then corresponds to the problem 1 norm. (...) If it is a function that is symmetric and invariant among permutations. (...) If we are the problem 1 norm, then it corresponds to the tensor norm, whereas if it implements the proposed regulation. (...) Problem (11) raises some difficulties because the terms under summing are interdependent, that is, the different matrices of W have rearranged the same elements in different ways."}, {"heading": "4.2 Computation of the Proximity Operator", "text": "To calculate the proximity operator of the function, we have to use several properties of the Proximity Calculus. (First, we use the formula (see e.g. [6]) Proximity Operator (x) = x \u2212 proxg (x) for g * = 1\u03b2\u043e \u0445 \u0445 \u0445 \u03b1. Next, we use a property of the conjugate functions from [20, 12] which states that g () = 1\u03b2 \u2212 proxg (\u03b2 \u00b7).Finally, we have this property (x) = 1 \u03b2 Proxigate functions from (\u03b2x).Algorithm 1 Calculation of the Proximity Computations from (\u03b2 \u00b7). Input: y Proximity Operators from [6], we have this property from (x) = 1 \u03b2 Proximity Operators from (\u03b2x).Algorithm 1 Calculation of the Proximity Computations from (y).Input: y Production Operators from Rd, \u03b2 > 0."}, {"heading": "5 Experiments", "text": "We have conducted a series of experiments to assess whether there is an advantage to use the proposed regulator over the Tensor-Trace standard for tensor completion. First, we have designed a synthetic experiment to evaluate the performance of both approaches under controlled conditions. Subsequently, we have tried both methods on two real data problems for tensor completion. In all cases, we have used a validation procedure to match the hyper-parameter present in both approaches between the values {10j = 7, \u2212 6,..., 0}. In our proposed approach, there is another hyper-parameter that needs to be specified. It should take the value of the Frobenius standard for each underlying tensor. As this is not known, we propose to use the estimate \u03b1 = 000, 000, the Frobenius standard for any underlying tensor."}, {"heading": "5.1 Synthetic Dataset", "text": "First we created a tensor W with ranks (12, 6, 3) that uses Tucker decomposition (see e.g. [15]) Wi1, i2, i3 = 12 x j1 = 16 x j2 = 13 x j3 = 1Cj1, j2, j3M (1) i1, j1M (2) i2, j2 M (3) i3, j3, (i1, i2, i3), [40] \u00b7 [20] \u00b7 [10] where each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1). We then have the basic truth sensor W0 with the equation W0i1, i3 = Wi1, i2, i3 \u2212 mean (W) std (W) std (W) i1, i2, i3wo mean (W) and d (W) are the averages."}, {"heading": "5.2 School Dataset", "text": "The first real data set we tried is the Inner London Education Authority (ILEA) dataset 3. It consists of exam grades from 0 to 70, out of 15,362 students, described by a set of attributes such as school and ethnic group. Most of these attributes are categorical, so we can imagine the exam grade prediction as a problem where each of the modes corresponds to a categorical attribute. In particular, we used the following attributes: school (139), gender (2), VR band (3), ethnic (11) and year (3), resulting in a 5-order tensor W-R139 x 2 x 3 x 11 x 3. We randomly selected 5% of the cases to create the test set and another 5% of the cases for the validation set. From the remaining instances, we selected m for several m values at random. This procedure was repeated 20 times and the average performance is displayed below this value."}, {"heading": "5.3 Video Completion", "text": "In the second real data experiment, we performed a video completion test. Any video can be treated as a 4th order tensor: \"width,\" \"height,\" \"RGB,\" \"video length,\" so we can use tensor3Available at http: / / www.bristol.ac.uk / cmm / learning / support / datasets / ilea567.zip.completion algorithms to recover a video from a few inputs, a procedure that can be useful for compression purposes. In our case, we used the Ocean video available at [16] This video sequence can be used as a tensor W-R160 x 112 x 3 x 32. We used randomly sampled m-tensor elements as training data, 5% of which are used as validation data, and the rest compiled the test set. After repeating this procedure ten times, we present the average results in Figure 2 (right). The proposed approach is noticeably better than the > 00p result, each time below this norm-00p, which we are clearly subtracted."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a convex relaxation for the mean of the matricization of a tensor. We compared this relaxation with a commonly used convex relaxation used in conjunction with tensor completion, which is based on the trace norm. We proved that this second relaxation is not narrow, and argued that the proposed convex regulator could be advantageous. Empirical comparisons indicate that our method consistently improves in terms of the error of estimation compared to tensor completion, while it is computationally comparable in terms of the range of problems we are looking at. In the future, it would be interesting to study methods to accelerate the calculation of our regulator's proximity operator and to investigate its usefulness for tensor completions such as multilinear multitask learning [19]."}, {"heading": "A A Useful Lemma", "text": "Lemma 4. Let C1,.., CN be convex subsets of a Euclidean space and let D = N = 1 Cn 6 = \u2205. Let g: Nn = 1 Cn \u2192 R and let h: D \u2192 R be the defined function for each x-D, as h (x) = g (x,..., x). Then for each x-D, the convex hull of g can not be larger than the convex hull from h to D. With this result, it is immediately possible to derive a convex lower limit for function R in Equation (2). Since the convex hull of g cannot be larger than the convex hull from h to D. Using this result, it is immediately possible to derive a convex lower limit for function R in Equation (2)."}, {"heading": "B Computation of an Approximated Projection", "text": "S2 In our problem, we can S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 \u2212 S2 S2 S2 \u2212 S2 S2 S2 \u2212 S2 S2 S2 \u2212 S2 S2 \u2212 S2 S2 \u2212 S2 S2 S2 \u2212 S2 S2"}], "references": [{"title": "Efficient first order methods for linear composite regularizers", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "L. Shen", "Y. Xu"], "venue": "arXiv:1104.1436", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix Analysis", "author": ["R. Bhatia"], "venue": "Springer Verlag", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Prentice-Hall", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, 3(1):1\u2013122", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Subgradient methods", "author": ["S. Boyd", "L. Xiao", "A. Mutapcic"], "venue": "Stanford University", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": "Fixed- Point Algorithms for Inverse Problems in Science and Engineering (H. H. Bauschke et al. Eds), pages 185\u2013212, Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S. Boyd"], "venue": "Proc. American Control Conference, Vol. 6, pages 4734\u20134739", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Problems, 27(2)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "3rd Edition. Johns Hopkins University Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Large-scale image classification with trace-norm regularization", "author": ["Z. Harchaoui", "M. Douze", "M. Paulin", "M. Dudik", "J. Malick"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition (CVPR), pages 3386\u20133393", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Convex Analysis and Minimization Algorithms", "author": ["J-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Part I. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Convex Analysis and Minimization Algorithms", "author": ["J-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Part II. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Topics in Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge University Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Multiverse recommendation: ndimensional tensor factorization for context-aware collaborative filtering", "author": ["A. Karatzoglou", "X. Amatriain", "L. Baltrunas", "N. Oliver"], "venue": "Proc. 4th ACM Conference on Recommender Systems, pages 79\u201386", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bade"], "venue": "SIAM Review, 51(3):455\u2013 500", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "Proc. 12th International Conference on Computer Vision (ICCV), pages 2114\u20132121", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient methods for minimizing composite objective functions", "author": ["Y. Nesterov"], "venue": "ECORE Discussion Paper, 2007/96", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "A simpler approach to matrix completion", "author": ["B. Recht"], "venue": "Journal of Machine Learning Research, 12:3413\u20133430", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "Proc. 30th International Conference on Machine Learning (ICML), pages 1444\u20131452", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimization Methods for Non-differentiable Functions", "author": ["N.Z. Shor"], "venue": "Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1985}, {"title": "R", "author": ["M. Signoretto"], "venue": "Van de Plas, B. De Moor, J.A.K. Suykens. Tensor versus matrix completion: a comparison with application to spectral data. IEEE Signal Processing Letters, 18(7):403\u2013406", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems (NIPS) 17, pages 1329\u20131336", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimation of low-rank tensors via convex optimization", "author": ["R. Tomioka", "K. Hayashi", "H. Kashima", "J.S.T. Presto"], "venue": "arXiv:1010.0789", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex tensor decomposition via structured Schatten norm regularization", "author": ["R. Tomioka", "T. Suzuki"], "venue": "arXiv:1303.6370", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical performance of convex tensor decomposition", "author": ["R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima"], "venue": "Advances in Neural Information Processing Systems (NIPS) 24, pages 972\u2013980", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 15, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 20, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 22, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 23, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 24, "context": "1 Introduction During the recent years, there has been a growing interest on the problem of learning a tensor from a set of linear measurements, such as a subset of its entries, see [8, 16, 21, 22, 24, 25, 26] and references therein.", "startOffset": 182, "endOffset": 209}, {"referenceID": 13, "context": "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "This methodology, which is also referred to as tensor completion, has been applied to various fields, ranging from collaborative filtering [14], to computer vision [16], to medical imaging [8], among others.", "startOffset": 189, "endOffset": 192}, {"referenceID": 21, "context": "Arguably the most widely used convex approach to tensor completion is based upon the extension of trace norm regularization [23] to that context.", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "This involves computing the average of the trace norm of each matricization of the tensor [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "A key insight behind using trace norm regularization for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix defined on the spectral unit ball [7].", "startOffset": 186, "endOffset": 189}, {"referenceID": 7, "context": "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].", "startOffset": 89, "endOffset": 100}, {"referenceID": 15, "context": "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].", "startOffset": 89, "endOffset": 100}, {"referenceID": 20, "context": "(2) Finding a convex relaxation of this regularizer has been the subject of recent works [8, 16, 22].", "startOffset": 89, "endOffset": 100}, {"referenceID": 12, "context": "[13]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13]) it is easily seen that \u03a8\u2217(S) is also a spectral function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "We refer to [7] for a detailed discussion of these ideas.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "For this purpose, we first recall the alternating direction method of multipliers (ADMM) [3], which was conveniently applied to tensor trace norm regularization in [8, 21].", "startOffset": 164, "endOffset": 171}, {"referenceID": 7, "context": "In order to overcome this difficulty, the authors of [8, 21] proposed to use ADMM as a natural way to decouple the regularization term appearing in problem (11).", "startOffset": 53, "endOffset": 60}, {"referenceID": 2, "context": "[3, 4]) is given by L (W ,B,A) = 1 \u03b3 E (W) + N \u2211", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4]) is given by L (W ,B,A) = 1 \u03b3 E (W) + N \u2211", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "(16) Step (16) is straightforward, whereas step (14) is described in [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "[6]) proxg\u2217 (x) = x\u2212proxg (x) for g\u2217 = 1 \u03b2\u03c9 \u03b1 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Next we use a property of conjugate functions from [20, 12], which states that g(\u00b7) = 1 \u03b2 \u03c9\u2217 \u03b1(\u03b2\u00b7).", "startOffset": 51, "endOffset": 59}, {"referenceID": 11, "context": "Next we use a property of conjugate functions from [20, 12], which states that g(\u00b7) = 1 \u03b2 \u03c9\u2217 \u03b1(\u03b2\u00b7).", "startOffset": 51, "endOffset": 59}, {"referenceID": 5, "context": "Finally, by the scaling property of proximity operators [6], we have that proxg (x) = 1 \u03b2 prox\u03b2\u03c9\u2217 \u03b1 (\u03b2x).", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In fact, according to [5], it is sufficient to compute an approximate projection, a step which we describe in Appendix B.", "startOffset": 22, "endOffset": 25}, {"referenceID": 14, "context": "[15]) Wi1,i2,i3 = 12 \u2211", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "j3=1 Cj1,j2,j3M (1) i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) \u2208 [40]\u00d7 [20]\u00d7 [10] where each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1).", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "j3=1 Cj1,j2,j3M (1) i1,j1M (2) i2,j2 M (3) i3,j3 , (i1, i2, i3) \u2208 [40]\u00d7 [20]\u00d7 [10] where each entry of the Tucker decomposition components is sampled from the standard Gaussian distribution N (0, 1).", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "However, as p increases the singular value decomposition routine, which is common to both methods, becomes the most demanding because it has a time complexity O (p) [9].", "startOffset": 165, "endOffset": 168}], "year": 2013, "abstractText": "We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.", "creator": "LaTeX with hyperref package"}}}