{"id": "1307.5101", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2013", "title": "Large-scale Multi-label Learning with Missing Labels", "abstract": "Multi-label classification problems abound in practice; as a result, many methods have recently been proposed for these problems. However, there are two key challenges that have not been adequately addressed: (a) the number of labels can be numerous, for example, in the millions, and (b) the test data can be riddled with missing labels. In this paper, we study a generic framework for multi-label classification that directly addresses the above challenges. In particular, we pose the problem as one of empirical risk minimization, where the prediction function is parameterized by a low-rank matrix. We show that our approach derives several existing label-compression based algorithms (such as the recently proposed CPLST method (Chen and Lin, 2012) in a principled manner. A key facet of our approach is that we handle missing labels in the training set by applying techniques from the domain of matrix completion. To develop a scalable algorithm that can handle a larger number of classes, we use the alternating minimization method to find the low-rank parameter matrix. Furthermore, for the special case of $L_2$ loss, we show that special structure in the problem can be exploited and the alternating minimization algorithm can be efficiently implemented. Finally, we present empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods. Moreover, we demonstrate scalability of our approach by applying it to a large Wikipedia based dataset that has 117,564 training data instances and 207,386 labels.", "histories": [["v1", "Thu, 18 Jul 2013 23:55:55 GMT  (601kb,D)", "https://arxiv.org/abs/1307.5101v1", null], ["v2", "Mon, 14 Oct 2013 22:33:17 GMT  (718kb,D)", "http://arxiv.org/abs/1307.5101v2", null], ["v3", "Mon, 25 Nov 2013 16:57:43 GMT  (734kb,D)", "http://arxiv.org/abs/1307.5101v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hsiang-fu yu", "prateek jain 0002", "purushottam kar", "inderjit s dhillon"], "accepted": true, "id": "1307.5101"}, "pdf": {"name": "1307.5101.pdf", "metadata": {"source": "CRF", "title": "Large-scale Multi-label Learning with Missing Labels", "authors": ["Hsiang-Fu Yu", "Prateek Jain"], "emails": ["rofuyu@cs.utexas.edu", "prajain@microsoft.com", "t-purkar@microsoft.com", "inderjit@cs.utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it's as if most people who are able to survive themselves are not able to survive themselves. (...) It's not as if they are able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they were able to survive themselves. (...) It's as if they do it, as if they do it, if they do it, if they do it, if they do it, if they do it, if they do it. (...) It's as if they do it, if they do it, if they do it, if they do it."}, {"heading": "2 Problem Formulation", "text": "In this section, we introduce a generic ERM model that assumes the loss of function between the individual learning points. \"We assume that we have a feature vector xi-Rd and a corresponding label vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector"}, {"heading": "3 Algorithms", "text": "In this section, we apply the alternating minimization technology for optimization (1) and (2). (2) For a matrix Z with a known low rank k, it is inefficient to represent it with d \u00b7 L, especially if d and L are large. (3) Therefore, we consider a low decomposition of the form Z = WHT, where W = Rd \u00b7 k and H = RL \u00b7 k. We further assume that r (Z) in r1 (W) + r2 (H).In the following sections, we present the results with the minimum regularization, i.e. (3)"}, {"heading": "3.1 Fast Operations for General Loss Functions with Missing Labels", "text": "We assume that the loss function is a general double differentiable function '(a, b), where a and b are constant. Leave' (a, b) = unsolved b '(a, b) = unsolved 2' (a, b). The course and the Hessian matrix for g '(w) are:' g (w) = unsolved (i, j) '(Yij, w)' (T x) = loss (a, b). (8) The course and the Hessian matrix (i, j) are: 'g (w) = unsolved (i, j). (9) A direct calculation of the losses g (w) and the Hessis (w)."}, {"heading": "3.2 Fast Operations for Squared Loss with Full Labels", "text": "The closed form solution of (3) is not ideal for two reasons: First, because it concerns the SVD of X and UTXY, the solution will indeed become unworkable if the number X is large. Second, because it is an unregulated solution, it may become overloaded. Second, because it is an unregulated solution, CPLST is similarly scalable and overmatched, since there will be no regulation and requirement for pseudo-conversions for X. If Y is fully observed, algorithm 1, which aims to handle missing labels at a general loss, is also not scalable, as if it is an O (X), whose cost per operation is prohibitive if n and L are large. Although the cost of a general loss, an O (nLk) seems inevitable, for the loss of L2."}, {"heading": "4 Generalization Error Bounds", "text": "In this section, we analyze the excessive risk limits for our learning model with the Action Standard. (Our analysis shows the superiority of our trace standard-based technique over BR and Frobe nius norm regularization.) We need a more careful analysis for our attitude, since standard results are not applicable to the presence of missing labels. (We get n training points (x1, y1),. (xn, yn) sampled i.d from distribution D, where yi, 1) L determines the pattern of missing labels. We get n training points (x1, y1). (xn, yn) sampled i.d from distribution D, where yi, 0, 1) L, the truth labels are vectors. However, we will only be able to observe the truth label vectors at random locations."}, {"heading": "5 Experimental Results", "text": "We present experimental results to evaluate our proposed algorithms in terms of accuracy and scalability. As we will see, the results clearly demonstrate the superiority of our method over other approaches. Datasets. We consider a variety of benchmark datasets, including four standard datasets (bibtex, delicious, eurlex, and nus-wide), two datasets with d L (autofood and compphys), and a very large scale Wikipedia-based datasets, which contains about 1M wikipages and 200K labels. See Table 2 for more information on the datasets. We conducted all the experiments on an Intel machine with 32 cores.Competing Methods. A list of the competing methods (including ours) is given below. Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are suboptimal."}, {"heading": "5.1 Results with full labels", "text": "Dre rf\u00fc nde eeirrrVnlrteeecrrrrrrrrrrrrrrcnlhSe ni red eeisrteeSrte\u00fccnh hsci, \"so sdsa rf\u00fc ide rf\u00fc ide rf\u00fc ide eeisrcnlhsrrrrrrrrrrrrrf\u00fc ide rf\u00fc ide eeisrf\u00fc ide eeisrf\u00fc ide eeisrteeeeeeeeeeeeSn\" nlrrrrrrrrrrrrreeeSn \"nlrrrrrrsasg os os os os rrrrrrteeeeeeeeeeerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr ide\" ide \"ide\" nlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrf\u00fc ide \"ide\" ide \"ide\" ide \"ide\" ide \"ide\" eteeteeteeteeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5.2 Results with missing labels", "text": "For experiments with missing labels, we compare LEML, BCS and BR. We implemented BR with missing labels by learning an L2-regulated binary classifier / regressor for each label on observed instances. Thus, the model derived from BR corresponds to the minimizer of (2) with Frobenius standards regularization. Table 5 shows the results when 20% entries were detected (i.e. 80% error rate) and the square loss function was used for training. We used k = 0.4L for both LEML and BCS. The results clearly show that LEML exceeds BCS and LEML in all three evaluation criteria. On bibtex, we continue to present the results for different rates of observed labels in Figure 2b and the results for different dimension reduction rates in Figure 2c. LEML clearly shows superior performance compared to other approaches, which confirms the theoretical performance of the general section 4 for better interpreting rates."}, {"heading": "6 Conclusion", "text": "In this paper, we investigated the problem of multi-level learning with missing labels in the standard ERM framework. We modeled our framework with rank constraints and regulators to increase scalability and efficiency. To solve the non-convex problem we obtained, we proposed an alternate minimization method that critically exploits the structure of the loss function to make our method scalable. We demonstrated that our learning framework allows for excessive risk limits that indicate a better generalization performance of our methods than the existing methods such as BR, which our experiments also confirmed. Our experiments also showed that our techniques are much more efficient than other large multi-level classifiers and perform better than the existing compression approaches. In future work, we would like to extend LEML to other (non-decompressible) loss functions such as ranking losses and study conditions, under which alternating minimization of our problem is guaranteed to be optimized globally."}, {"heading": "A Algorithm Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Conjugate Gradient for Squared Loss", "text": "In algorithm 3, we show the detailed conjugate gradient method used to solve (7) when using the quadratic loss. Note that the 2g value (w) is invariant to w because (7) is a quadratic problem due to the quadratic loss function. Algorithm 3 Conjugate gradient to the solution (7) with the quadratic loss \u2022 Set the initial value w0, r0 = -g (w0), d0 = r0. \u2022 For t = 0, 1, 2,.. - If the value rt value is small enough, stop the process and return. - \u03b1t = rTt rt dTt value 2g (w0) dt - wt + 1 = wt + \u03b1tdt - rt + 1 = rt \u2212 \u03b1tdt 2g (w0) dt - \u03b2t = rTt + 1rt + 1 rTt rt + 1 + 1 tdt"}, {"heading": "B Analyzing Trace Norm-bounded Predictors", "text": "In this section we will provide proof of theorems 3 and 4. Our proof will be provided by the proof of a uniform convergence style determined for empirical losses. More precisely, for both track norms and Frobenius regulations, we will show with a high probability L (Z) \u2264 L (Z) \u2264 L (Z) +. Let us suppose that Z \"arg min (Z) L (Z), then a similar analysis will show us with a high probability that L\" (Z) L (Z) + combine the two together with the fact that Z is an empirical risk combiner, i.e. L \"(Z) \u2264 L\" (Z) will provide the announced claim in the following form: L \"(Z) L (Z) +. Let us combine the two together with the fact that Z is an empirical risk combiner, i.e. L\" (Z) \u2264 L \"(Z) will provide the announced claim: L\" (Z)"}, {"heading": "B.1 Step 1: Bounding Excess Risk by Expected Supre\u0304mus Deviation", "text": "We will first analyze case s = 1 and later show how the analysis can be extended to s > 1. In this case we get n training points (xi, yi) and for each training point xi we get the value of a random label li (L) i.e. we get the true value of ylii. Thus, for each predictor Z (y) becomes the observed training loss of L (Z) = 1 n (ylii, Zli, xi).The population risk is determined by L (Z) = E (x, y, l) r'l (y, l) l (x; Z) z (x, l) r'l (y, l) l'l (y, x) l'l (y, x) l'l (y, x).We point out here that our subsequent analysis also applies to non-uniform labels."}, {"heading": "B.2 Step 2: Bounding Expected Supre\u0304mus Deviation by a Rademacher Average", "text": "We analyze now the prospective deviation. We haveE = xi, yi, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i,"}, {"heading": "B.3 Step 3: Estimating the Rademacher Average", "text": "We will now determine the following quantity: Rn (Z) = 1n E X, l, s sup Z = Z < Z, X l > {where X l is defined as above. Approaches to restrict such Rademacher averages in our case usually fall back on Martingale techniques Kakade et al. (2008) or the use of tools from the convex analysis Kakade et al. (2012) and decompose the Rademacher average. However, such decomposition processes will produce suboptimal results in our case. Instead, our proposed approach involves an application of the inequality of heat followed by an application of results from the random matrix theory to the spectral standard of a random matrix."}, {"heading": "B.3.1 Distribution Independent Bound", "text": "We apply the imbalance Ho \u00bc lders to obtain the following result: 1 n E X, l, s sup Z-Z < Z, X l > {\u2264 1 n E X, l, s sup Z-Z-Z-Z-Z-Z, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l, l"}, {"heading": "B.3.2 Tighter Bounds for Trace Norm Regularization", "text": "Note that in the above analysis we did not take advantage of the fact that the uppermost singular value of the matrixX l may be much smaller than the Frobenius standard. However, there are distributions in which the track standard guarantees better performance than the Frobenius standard. To better represent our limits, we model the data distribution D to X (or rather its marginal size) more carefully. Leave X: = E q xx > y and assume that the distribution D meets the following conditions: 1. The uppermost singular value of X is the X standard 2 = 12. The matrix X has a track tr (X) = 3. The distribution on X is sub-Gaussian i.e. For some cases > 0 we have for all v-Rd, E r-Exp (x > v) z standard, which is the standard."}, {"heading": "B.4 Step 4: Calculating the Spectral norm of a Random Matrix", "text": "& # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10;"}, {"heading": "C Lower Bounds for Uniform Convergence-based Proofs", "text": "In this section, we show that our analysis of theorems 3 and 4 is essentially narrow. In particular, we show for each case a data distribution in which the deviation of empirical losses from population risks to a constant factor is the same as predicted by the results. We present these lower limits in two separate subsections:"}, {"heading": "C.1 Lower Bound for Trace Norm Regularization", "text": "In this section, we will show that for the general distribution, theorem 3 is narrow. Let's remember that theorem 3 predicts that for a predictor Z learned with a trace standard, there is a constant probability (i.e., this result is narrow by showing the following lower limit: Claim 7. There is a data mark distribution and a loss function that minimizes empirical risk, such as Z, arg, inf, Z, tr, L, with constant probability, its population risk is lower than L (Z). There is a data mark distribution and a loss function, so that the empirical risk minimizer as Z, arg, inf, Z, tr, L, with constant probability, its population risks are lower than L (Z)."}, {"heading": "C.2 Lower Bound for Frobenius Norm Regularization", "text": "In this section we will prove that even for isotropic distributions, Frobenius norm regularization is not possible, O (1 \u00b0 nL) -style bounds as at trace norm regularization.Claim 8. So there is an isotropic, sub-Gaussian data distribution and a loss function such that the empirical risk minimizer learned as Z = arg inf. (Z) has, with constant probability, its population risk lowerbounded byL (Z). L (Z). L (Z) + an empirical risk minimizer learned as Z = arg inf. (Z). L. (Z) over the same distribution has, with probability at 1 \u2212 ius L. (Z)."}, {"heading": "D More Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.1 Speedup Results Due to Multi-core Computation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "D.2 Detailed Results with Full Labels", "text": "\u2022 Table 6 shows the top 1 accuracy results for the case with fully observed labels. \u2022 Table 7 shows the top 3 accuracy results for the case with fully observed labels. \u2022 Table 8 shows the top 5 accuracy results for the case with fully observed labels. \u2022 Table 9 shows the hammer loss results for the case with fully observed labels. \u2022 Table 10 shows the average AUC results for the case with fully observed labels."}, {"heading": "D.3 Detailed Results with Missing Labels", "text": "\u2022 Table 11 shows the top 1 accuracy results for cases with different missing ratios and dimension reduction rates. \u2022 Table 12 shows the top 3 accuracy results for cases with different missing ratios and dimension reduction rates. \u2022 Table 13 shows the top 5 accuracy results for cases with different missing ratios and dimension reduction rates. \u2022 Table 14 shows the hamming loss results for cases with different missing ratios and dimension reduction rates. \u2022 Table 15 shows the average AUC results for cases with different missing ratios and dimension reduction rates."}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Agrawal", "Rahul", "Gupta", "Archit", "Prabhu", "Yashoteja", "Varma", "Manik"], "venue": "In Proceedings of the International World Wide Web Conference,", "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Efficient multi-label ranking for multi-class learning: Application to object recognition", "author": ["Bucak", "Serhat Selcuk", "Mallapragada", "Pavan Kumar", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Proceedings of IEEE International Conference on Computer Vision,", "citeRegEx": "Bucak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2009}, {"title": "Multi-label learning with incomplete class assignments", "author": ["Bucak", "Serhat Selcuk", "Jin", "Rong", "Jain", "Anil K"], "venue": "In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Bucak et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bucak et al\\.", "year": 2011}, {"title": "Supervised learning of semantic classes for image annotation and retrieval", "author": ["Carneiro", "Gustavo", "Chan", "Antoni B", "Moreno", "Pedro J", "Vasconcelos", "Nuno"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Carneiro et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Carneiro et al\\.", "year": 2007}, {"title": "Feature-aware label space dimension reduction for multi-label classification", "author": ["Chen", "Yao-Nan", "Lin", "Hsuan-Tien"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan", "Rong-En", "Chang", "Kai-Wei", "Hsieh", "Cho-Jui", "Wang", "Xiang-Rui", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Large scale maxmargin multi-label classification with priors", "author": ["Hariharan", "Bharath", "Zelnik-Manor", "Lihi", "S.V.N. Vishwanathan", "Varma", "Manik"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Hariharan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2010}, {"title": "Multi-label prediction via compressed sensing", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Langford", "John", "Zhang", "Tong"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization", "author": ["Kakade", "Sham M", "Sridharan", "Karthik", "Tewari", "Ambuj"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kakade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2008}, {"title": "Regularization Techniques for Learning with Matrices", "author": ["Kakade", "Sham M", "Shalev-Shwartz", "Shai", "Tewari", "Ambuj"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kakade et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2012}, {"title": "Multilabel classification using bayesian compressed sensing", "author": ["Kapoor", "Ashish", "Viswanathan", "Raajay", "Jain", "Prateek"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kapoor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kapoor et al\\.", "year": 2012}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["Ledoux", "Michel", "Talagrand"], "venue": null, "citeRegEx": "Ledoux et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ledoux et al\\.", "year": 2002}, {"title": "Trust region Newton method for large-scale logistic regression", "author": ["Lin", "Chih-Jen", "Weng", "Ruby C", "Keerthi", "S. Sathiya"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "A Generalized Representer Theorem", "author": ["Sch\u00f6lkopf", "Bernhard", "Herbrich", "Ralf", "Smola", "Alex J"], "venue": "In 14th Annual Conference on Computational Learning Theory, pp", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Collaborative Filtering with the Trace Norm: Learning, Bounding, and Transducing", "author": ["Shamir", "Ohad", "Shalev-Shwartz", "Shai"], "venue": "In 24th Annual Conference on Learning Theory,", "citeRegEx": "Shamir et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2011}, {"title": "Canonical correlation analysis for multi-label classification: A least squares formulation, extensions and analysis", "author": ["Sun", "Liang", "Ji", "Shuiwang", "Ye", "Jieping"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Multi-label classification with principal label space transformation", "author": ["Tai", "Farbound", "Lin", "Hsuan-Tien"], "venue": "Neural Computation,", "citeRegEx": "Tai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices, chapter 5 of Compressed Sensing, Theory and Applications, pp. 210\u2013268", "author": ["Vershynin", "Roman"], "venue": null, "citeRegEx": "Vershynin and Roman.,? \\Q2012\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2012}, {"title": "Multi-Label Sparse Coding for Automatic Image Annotation", "author": ["Wang", "Changhu", "Yan", "Shuicheng", "Zhang", "Lei", "Hong-Jiang"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Large scale multi-label classification is an important learning problem with several applications to real-world problems such as image and video annotation Carneiro et al. (2007); Wang et al.", "startOffset": 156, "endOffset": 179}, {"referenceID": 2, "context": "Large scale multi-label classification is an important learning problem with several applications to real-world problems such as image and video annotation Carneiro et al. (2007); Wang et al. (2009) and query/keyword suggestions Agrawal et al.", "startOffset": 156, "endOffset": 199}, {"referenceID": 0, "context": "(2009) and query/keyword suggestions Agrawal et al. (2013). The goal in multi-label classification is to accurately predict a label vector y \u2208 {0, 1}L for a given data point x \u2208 Rd.", "startOffset": 37, "endOffset": 59}, {"referenceID": 0, "context": "(2009) and query/keyword suggestions Agrawal et al. (2013). The goal in multi-label classification is to accurately predict a label vector y \u2208 {0, 1}L for a given data point x \u2208 Rd. This problem has been studied extensively in the domain of structured output learning, where the number of labels is assumed to be small and the main focus is thus, on modeling inter-label correlations and using them to accurately predict the label vector Hariharan et al. (2010). Due to several motivating real-life applications, recent research on multi-label classification has largely shifted its focus to the other end of the spectrum where the number of labels is assumed to be extremely large, with the key challenge being the design of scalable algorithms that offer real-time predictions and have a small memory footprint.", "startOffset": 37, "endOffset": 462}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al.", "startOffset": 194, "endOffset": 212}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al.", "startOffset": 194, "endOffset": 230}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently.", "startOffset": 194, "endOffset": 252}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression.", "startOffset": 194, "endOffset": 467}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case.", "startOffset": 194, "endOffset": 1412}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case. However, the flexibility of the framework allows us to use other loss functions and regularizers that are useful for preventing overfitting and increasing scalability. Moreover, we can extend our formulation to handle missing labels; in contrast, most dimension reduction formulations (including CPLST) cannot accommodate missing labels. The ability to learn in the presence of missing labels is crucial as for most real-world applications, one cannot expect to accurately obtain (either through manual or automated labeling) all the labels for a given data point. For example, in image annotation, human labelers tag only prominent labels and typically miss out on several objects present in the image. Similarly, in online collections such as Wikipedia, where articles get tagged with categories, human labelers usually tag only with categories they know about. Moreover, there might be considerable noise/disagreement in the labeling. In order to solve for the low-rank linear model that results from our formulation, we use the popular alternating minimization algorithm that works well despite the non-convexity of the rank constraint. For general loss functions and trace-norm regularization, we exploit subtle structures present in the problem to design a fast conjugate gradient based method. For the special case of squared-L2 loss and trace-norm regularization, we further exploit the structure of the loss function to provide a more efficient and scalable algorithm. As compared to direct computation, our algorithm is O(d\u0304) faster, where d\u0304 is the average number of nonzero features in an instance. On the theoretical side, we perform an excess risk analysis for the trace-norm regularized ERM formulation with missing labels, assuming labels are observed uniformly at random. Our proofs do not follow from existing results due to missing labels and require a careful analysis involving results from random matrix theory. Our results show that while in general the low-rank promoting trace-norm regularization does not provide better bounds than learning a full-rank matrix (e.g. using Frobenius norm regularization), for several interesting data distributions, trace-norm regularization does indeed give significantly better bounds. More specifically, for isotropic data distributions, we show that trace-norm based methods have excess risk of O( 1 \u221a nL ) while full-rank learning can only guarantee O( 1 \u221a n ) excess risk, where n is the number of training points and L is the number of labels. Finally, we provide an extensive empirical evaluation of our method on a variety of benchmark datasets. In particular, we compare our method against three recent label compression based methods: CPLST Chen & Lin (2012), Bayesian-CS Kapoor et al.", "startOffset": 194, "endOffset": 4174}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case. However, the flexibility of the framework allows us to use other loss functions and regularizers that are useful for preventing overfitting and increasing scalability. Moreover, we can extend our formulation to handle missing labels; in contrast, most dimension reduction formulations (including CPLST) cannot accommodate missing labels. The ability to learn in the presence of missing labels is crucial as for most real-world applications, one cannot expect to accurately obtain (either through manual or automated labeling) all the labels for a given data point. For example, in image annotation, human labelers tag only prominent labels and typically miss out on several objects present in the image. Similarly, in online collections such as Wikipedia, where articles get tagged with categories, human labelers usually tag only with categories they know about. Moreover, there might be considerable noise/disagreement in the labeling. In order to solve for the low-rank linear model that results from our formulation, we use the popular alternating minimization algorithm that works well despite the non-convexity of the rank constraint. For general loss functions and trace-norm regularization, we exploit subtle structures present in the problem to design a fast conjugate gradient based method. For the special case of squared-L2 loss and trace-norm regularization, we further exploit the structure of the loss function to provide a more efficient and scalable algorithm. As compared to direct computation, our algorithm is O(d\u0304) faster, where d\u0304 is the average number of nonzero features in an instance. On the theoretical side, we perform an excess risk analysis for the trace-norm regularized ERM formulation with missing labels, assuming labels are observed uniformly at random. Our proofs do not follow from existing results due to missing labels and require a careful analysis involving results from random matrix theory. Our results show that while in general the low-rank promoting trace-norm regularization does not provide better bounds than learning a full-rank matrix (e.g. using Frobenius norm regularization), for several interesting data distributions, trace-norm regularization does indeed give significantly better bounds. More specifically, for isotropic data distributions, we show that trace-norm based methods have excess risk of O( 1 \u221a nL ) while full-rank learning can only guarantee O( 1 \u221a n ) excess risk, where n is the number of training points and L is the number of labels. Finally, we provide an extensive empirical evaluation of our method on a variety of benchmark datasets. In particular, we compare our method against three recent label compression based methods: CPLST Chen & Lin (2012), Bayesian-CS Kapoor et al. (2012), and WSABIE Weston et al.", "startOffset": 194, "endOffset": 4208}, {"referenceID": 7, "context": "The key idea in this technique is to reduce the dimensionality of the label-space by using either random projections or canonical correlation analysis (CCA) based projections Chen & Lin (2012); Hsu et al. (2009); Tai & Lin (2012); Kapoor et al. (2012). Subsequently. these methods perform prediction on the smaller dimensional label-space and then recover the original labels by projecting back onto the high dimensional label-space. In particular, Chen & Lin (2012) recently showed that by using a CCA type method with appropriate orthogonality constraints, one can design an efficient algorithm with both label-space as well as feature-space compression. However, this method is relatively rigid and cannot handle several important issues inherent to multi-label problems; see Section 2.1 for more details. In this paper we take a more direct approach by formulating the problem as that of learning a low-rank linear model Z \u2208 Rd\u00d7L s.t. ypred = ZTx. We cast this learning problem in the standard ERM framework that allows us to use a variety of loss functions and regularizations for Z. This framework unifies several existing dimension reduction approaches. In particular, we show that if the loss function is chosen to be the squared-L2 loss, then our proposed formulation has a closed form solution, and surprisingly, the conditional principal label space transformation (CPLST) method of Chen & Lin (2012) can be derived as a special case. However, the flexibility of the framework allows us to use other loss functions and regularizers that are useful for preventing overfitting and increasing scalability. Moreover, we can extend our formulation to handle missing labels; in contrast, most dimension reduction formulations (including CPLST) cannot accommodate missing labels. The ability to learn in the presence of missing labels is crucial as for most real-world applications, one cannot expect to accurately obtain (either through manual or automated labeling) all the labels for a given data point. For example, in image annotation, human labelers tag only prominent labels and typically miss out on several objects present in the image. Similarly, in online collections such as Wikipedia, where articles get tagged with categories, human labelers usually tag only with categories they know about. Moreover, there might be considerable noise/disagreement in the labeling. In order to solve for the low-rank linear model that results from our formulation, we use the popular alternating minimization algorithm that works well despite the non-convexity of the rank constraint. For general loss functions and trace-norm regularization, we exploit subtle structures present in the problem to design a fast conjugate gradient based method. For the special case of squared-L2 loss and trace-norm regularization, we further exploit the structure of the loss function to provide a more efficient and scalable algorithm. As compared to direct computation, our algorithm is O(d\u0304) faster, where d\u0304 is the average number of nonzero features in an instance. On the theoretical side, we perform an excess risk analysis for the trace-norm regularized ERM formulation with missing labels, assuming labels are observed uniformly at random. Our proofs do not follow from existing results due to missing labels and require a careful analysis involving results from random matrix theory. Our results show that while in general the low-rank promoting trace-norm regularization does not provide better bounds than learning a full-rank matrix (e.g. using Frobenius norm regularization), for several interesting data distributions, trace-norm regularization does indeed give significantly better bounds. More specifically, for isotropic data distributions, we show that trace-norm based methods have excess risk of O( 1 \u221a nL ) while full-rank learning can only guarantee O( 1 \u221a n ) excess risk, where n is the number of training points and L is the number of labels. Finally, we provide an extensive empirical evaluation of our method on a variety of benchmark datasets. In particular, we compare our method against three recent label compression based methods: CPLST Chen & Lin (2012), Bayesian-CS Kapoor et al. (2012), and WSABIE Weston et al. (2010). On almost all benchmark datasets, our method significantly outperforms these methods, both in the presence and absence of missing", "startOffset": 194, "endOffset": 4241}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels.", "startOffset": 109, "endOffset": 131}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al.", "startOffset": 109, "endOffset": 841}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al.", "startOffset": 109, "endOffset": 864}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al.", "startOffset": 109, "endOffset": 889}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al. (2012), or reduce the feature dimensionality, such as Sun et al.", "startOffset": 109, "endOffset": 927}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al. (2012), or reduce the feature dimensionality, such as Sun et al. (2011), or both, such as WSABIE Weston et al.", "startOffset": 109, "endOffset": 992}, {"referenceID": 0, "context": "Finally, we demonstrate the scalability of our method by applying it to a recently curated Wikipedia dataset Agrawal et al. (2013), that has 881,805 training samples and 213,707 labels. The results show that our method not only provides reasonably accurate solutions for such large-scale problems, but that the training time required is orders of magnitude shorter than several existing methods. Related Work. Typically, Binary Relevance (BR), which treats each label as an independent binary classification problem, is quite accurate for multi-label classification. However, for large number of labels, this method becomes infeasible due to increased model size and prediction time. Recently, techniques have been developed that either reduce the dimensionality of the labels, such as the Compressed Sensing Approach (CS) Hsu et al. (2009), PLST Tai & Lin (2012), CPLST Chen & Lin (2012), and Bayesian CS Kapoor et al. (2012), or reduce the feature dimensionality, such as Sun et al. (2011), or both, such as WSABIE Weston et al. (2010). Most of these existing techniques are tied to a specific loss function (e.", "startOffset": 109, "endOffset": 1038}, {"referenceID": 0, "context": ", Yij = 1 or 0), or missing (Yij =?); several other works have considered another setting where only positive labels are known and are given as 1 in the label matrix, while negative or missing values are all denoted by 0 Agrawal et al. (2013); Bucak et al.", "startOffset": 221, "endOffset": 243}, {"referenceID": 0, "context": ", Yij = 1 or 0), or missing (Yij =?); several other works have considered another setting where only positive labels are known and are given as 1 in the label matrix, while negative or missing values are all denoted by 0 Agrawal et al. (2013); Bucak et al. (2011). Note that although the above formulation is NP-hard in general due to the non-convex rank constraint, for convex loss functions, one can still utilize the standard alternating minimization method.", "startOffset": 221, "endOffset": 264}, {"referenceID": 0, "context": ", Yij = 1 or 0), or missing (Yij =?); several other works have considered another setting where only positive labels are known and are given as 1 in the label matrix, while negative or missing values are all denoted by 0 Agrawal et al. (2013); Bucak et al. (2011). Note that although the above formulation is NP-hard in general due to the non-convex rank constraint, for convex loss functions, one can still utilize the standard alternating minimization method. Moreover, for the special case of L2 loss, we can derive closed form solutions for the full-label case (1) and show connections to several existing methods. We would like to note that while the ERM framework is well known and standard, most existing multilabel methods for large number of labels motivate their work in a relatively ad-hoc manner. By studying this formal framework, we can show that existing methods like CPLST Chen & Lin (2012) are in fact a special case of this generic framework (see next section).", "startOffset": 221, "endOffset": 907}, {"referenceID": 5, "context": "Several linear classification/regression packages such as LIBLINEAR Fan et al. (2008) can handle such problems if {x\u0303ij : (i, j) \u2208 \u03a9} are available.", "startOffset": 68, "endOffset": 86}, {"referenceID": 12, "context": "Thus, to solve (7), we can apply CG for squared loss and the trust region Newton method (TRON) Lin et al. (2008) for the other two loss functions.", "startOffset": 95, "endOffset": 113}, {"referenceID": 8, "context": "Our proofs do not follow either from existing techniques for learning with matrix predictors (for instance Kakade et al. (2012)) or from results on matrix completion with trace norm regularization Shamir & Shalev-Shwartz (2011) due to the complex interplay of feature vectors and missing labels that we encounter in our learning model.", "startOffset": 107, "endOffset": 128}, {"referenceID": 8, "context": "Our proofs do not follow either from existing techniques for learning with matrix predictors (for instance Kakade et al. (2012)) or from results on matrix completion with trace norm regularization Shamir & Shalev-Shwartz (2011) due to the complex interplay of feature vectors and missing labels that we encounter in our learning model.", "startOffset": 107, "endOffset": 228}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al.", "startOffset": 13, "endOffset": 57}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al.", "startOffset": 13, "endOffset": 147}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al. (2012). 1.", "startOffset": 13, "endOffset": 169}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al. (2012). 1. LEML (Low rank Empirical risk minimization for Multi-Label Learning): our proposed method. We implemented CG with Algorithms 1 and 2 for squared loss, and TRON Lin et al. (2008) with Algorithm 1 for logistic and squared hinge loss.", "startOffset": 13, "endOffset": 351}, {"referenceID": 7, "context": "Note that CS Hsu et al. (2009) and PLST Tai & Lin (2012) are not included as they are shown to be suboptimal to CPLST and BCS in Chen & Lin (2012); Kapoor et al. (2012). 1. LEML (Low rank Empirical risk minimization for Multi-Label Learning): our proposed method. We implemented CG with Algorithms 1 and 2 for squared loss, and TRON Lin et al. (2008) with Algorithm 1 for logistic and squared hinge loss. 2. CPLST: the method proposed in Chen & Lin (2012). We used the code provided by the author.", "startOffset": 13, "endOffset": 456}, {"referenceID": 10, "context": "BCS: the Bayesian compressed sensing method of Kapoor et al. (2012). We used code provided by the authors to test this method.", "startOffset": 47, "endOffset": 68}, {"referenceID": 1, "context": "\u2022 Average AUC: we follow Bucak et al. (2009) to calculate area under ROC curve for each instance and report the average AUC among all test instances.", "startOffset": 25, "endOffset": 45}, {"referenceID": 19, "context": "Although this phenomenon is expected due to the sampling scheme in WSABIE Weston et al. (2010), it becomes more serious as L increases.", "startOffset": 74, "endOffset": 95}, {"referenceID": 8, "context": "Approaches to bounding such Rademacher average terms usually resort to Martingale techniques Kakade et al. (2008) or use of tools from convex analysis Kakade et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 8, "context": "Approaches to bounding such Rademacher average terms usually resort to Martingale techniques Kakade et al. (2008) or use of tools from convex analysis Kakade et al. (2012) and decompose the Rademacher average term.", "startOffset": 93, "endOffset": 172}], "year": 2013, "abstractText": "The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) the ability to tackle problems with a large number (say millions) of labels, and (b) the ability to handle data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions such as the squared loss function to offer efficient algorithms. We further show that our learning framework admits formal excess risk bounds even in the presence of missing labels. Our risk bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as the Wikipedia dataset.", "creator": "LaTeX with hyperref package"}}}