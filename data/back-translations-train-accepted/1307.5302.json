{"id": "1307.5302", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jul-2013", "title": "Kernel Adaptive Metropolis-Hastings", "abstract": "A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the purpose of sampling from a target distribution with strongly nonlinear support. The algorithm embeds the trajectory of the Markov chain into a reproducing kernel Hilbert space (RKHS), such that the feature space covariance of the samples informs the choice of proposal. The procedure is computationally efficient and straightforward to implement, since the RKHS moves can be integrated out analytically: our proposal distribution in the original space is a normal distribution whose mean and covariance depend on where the current sample lies in the support of the target distribution, and adapts to the local covariance structure of the target. Kernel Adaptive Metropolis-Hastings outperforms competing fixed and adaptive samplers on multivariate, highly nonlinear target distributions. Code may be downloaded at", "histories": [["v1", "Fri, 19 Jul 2013 18:26:34 GMT  (379kb,D)", "https://arxiv.org/abs/1307.5302v1", null], ["v2", "Thu, 13 Feb 2014 18:06:06 GMT  (3572kb)", "http://arxiv.org/abs/1307.5302v2", null], ["v3", "Thu, 12 Jun 2014 22:30:05 GMT  (2314kb)", "http://arxiv.org/abs/1307.5302v3", "Proceedings of the 31st International Conference on Machine Learning, Beijing, China, 2014; JMLR: W&amp;CP volume 32(2)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dino sejdinovic", "heiko strathmann", "maria lomeli garcia", "christophe andrieu", "arthur gretton"], "accepted": true, "id": "1307.5302"}, "pdf": {"name": "1307.5302.pdf", "metadata": {"source": "META", "title": "Kernel Adaptive Metropolis-Hastings", "authors": ["Dino Sejdinovic", "Heiko Strathmann", "Maria Lomeli Garcia", "Christophe Andrieu", "Arthur Gretton"], "emails": ["DINO@GATSBY.UCL.AC.UK", "UCABHST@GATSBY.UCL.AC.UK", "MLOMELI@GATSBY.UCL.AC.UK", "C.ANDRIEU@BRISTOL.AC.UK", "ARTHUR.GRETTON@GMAIL.COM"], "sections": [{"heading": null, "text": "ar Xiv: 130 7.53 02v3 [st at.M L] 12 Jun 20"}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has dealt with the question of whether the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU Commission, the EU, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU Commission, the EU Commission, the Commission, the EU Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission,"}, {"heading": "3. Sampling in RKHS", "text": "We will assume that a subset of the chain history, referred to as z = 1, n \u2264 t \u2212 1, is available. Our proposal is constructed by first associating the samples in the RKHS with the empirical covariance operator and then performing a gradient step in relation to a cost function associated with these samples.Gaussian Measurement of the covariance operator. We will work with the Gaussian Measurement of the RKHS Hk with the mean k (, y) and the covariance 2Cz, with z = {zi} ni = 1 being the subset of the chain history."}, {"heading": "4. MCMC Kameleon Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Proposal Distribution", "text": "We have a recipe to construct a proposal that is able to adapt to the local co-variance structure (\u03b2 = \u03b2 = \u03b2-x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4.2. Properties of the Algorithm", "text": "It is well known that a chain that persists in the distribution of supply does not have to adapt to the right target (Andrieu & Thoms, 2008).To guarantee convergence, we present adaptation probabilities (pt)."}, {"heading": "4.3. Examples of Covariance Structure for Standard Kernels", "text": "The proposed distributions in MCMC Kameleon depend on the choice of kernel k. To gain an intuition regarding their covariance structure, we give two examples below. \u2212 In the case of a linear nucleus k (x, x) = x x \u00b2, we get Mz, y = 2 [conservation xx'z1,.. \u2212 in the case of a linear nucleus k (x, x) = 2Z, the proposal is qz (\u00b7 y) = N (y, \u03b32I + 4\u03bd2Z HZ); thus, the proposal simply uses the scaled empirical covariance Z HZ \u2212 rn = 2Z, just like the standard adaptive metropolis (Haario et al., 1999), with an additional isotropic exploration component, and depends on the difference. \u2212 In the case of a Gaussian nucleus (x, x), we will have an additional isotropic exploration component, and in the case of variance, x and kernelk."}, {"heading": "5. Experiments", "text": "In the experiments we compare the following samplers: (SM) Standard Metropolis with the isotropic proposal q (\u00b7 | y) = N (y, \u03bd2I) and the scale \u03bd = 2.38 / \u221a d, (AMFS) Adaptive Metropolis with a learned covariance matrix and a specified scale \u03bd = 2.38 / \u221a d, (AM-LS) Adaptive Metropolis with a learned covariance matrix and scaling that was learned to bring the acceptance rate close to \u03b1 \u0445 = 0.234, as described in Andrieu & Thoms (2008, algorithm 4), and (KAMH-LS) MCMC Kameleon with the Learnedin scaling matrix in the same way (\u03b3 was fixed to 0.2), and which also stops the adaptation of the proposal after the chain has been burned in (in all experiments, algorithm 4), and (KAMH-MCMC) with the same method of calibration of the chain \u03b3 (in all experiments)."}, {"heading": "5.1. Pseudo-Marginal MCMC for GP Classification", "text": "In the first experiment, we illustrate the usefulness of the MCMC Kameleon samplers in the context of Bayesian classification with GPs (Williams & Barber, 1998), consider the common distribution of the latent variables f & # 252; for both (with covariant matrix X), and the hyperparameters evaluated for the latent variables f & # 252; for each one (K & # 246; rper), f & # 252; for each one (K & # 246; rper), f & # 252; for each one (K & # 246; rper)."}, {"heading": "5.2. Synthetic examples", "text": "In fact, the number of those who are able to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate, to retaliate."}, {"heading": "6. Conclusions", "text": "We have designed a simple, versatile, adaptable, grade-free MCMC sampler that builds a family of supply distributions based on the chain's sample history. These supply distributions automatically correspond to the local covariance structure of the target distribution in the current state of the chain. In experiments, the sampler exceeds existing approaches to nonlinear target distributions by both examining the overall support of these distributions and providing accurate empirical quantities indicating faster mixing. Possible enhancements include the inclusion of additional parametric information on target densities and exploring the target conflict between the level of chain history subsampling and the convergence of the sampler.Software.The Python implementation of MCMC Kameleon is available at https: / / github.com / karlnapf / kameleon-mc.Anowledgments.S., H.A.and L.A.G."}, {"heading": "A. Proofs", "text": "Then there is a Hilbert space (x, x, so k (x, x), so k (x, x), so k (x, x), so k (x, x), so k (x, x), so k (x), so k (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x), so (x)."}, {"heading": "B. Further details on synthetic experiments", "text": "Suggestion contours for the flower target. The d-dimensional flower target F (r0, A, \u03c9, \u03c3) is given by F (x; r0, A, \u03c9, \u03c3) = exp (\u2212 \u221a x21 + x 2 \u2212 r0 \u2212 A cos (\u03c9atan2 (x2, x1))) 2\u03c32) N (x3: d; 0, I) This distribution focuses on the r0 circle with a periodic disturbance (with amplitude A and frequency \u03c9) in the first two dimensions. For A = 0 we get a band around the r0 circle, which we call the ring target. Figure 6 shows the contour diagrams of the MCMC cameleon proposal distributions on two instances of the flower target. Convergence statistics for the banana target. Figure 7 illustrates how the norm of mean and quantity deviation (represented for 0.5 quantity) assumes the strongly distorted function of the whole banana chain along the persience target."}, {"heading": "C. Principal Components Proposals", "text": "An alternative approach to the standard adaptive Metropolis, discussed in Andrieu & Thoms (2008, algorithm 8), is to extract m \u2264 d principal eigenvalue eigenvector pairs {(\u03bbj, vj) \u00b7 mj = 1 from the estimated covariance matrix. (7) In other words: In view of the current chain state, the j-th eigendirection is chosen with the probability that the 2-dimensional eigendirection is chosen with the 2-dimensional eigendirection. (2) The 2-dimensional eigendirection is chosen with the probability that the 2-dimensional eigendirection is chosen with the 2-dimensional eigendirection. (1) The 2-dimensional eigendirection is proposed with the 2-dimensional eigendirection."}], "references": [{"title": "The pseudo-marginal approach for efficient Monte Carlo computations", "author": ["C. Andrieu", "G.O. Roberts"], "venue": "Ann. Statist.,", "citeRegEx": "Andrieu and Roberts,? \\Q2009\\E", "shortCiteRegEx": "Andrieu and Roberts", "year": 2009}, {"title": "A tutorial on adaptive MCMC", "author": ["C. Andrieu", "J. Thoms"], "venue": "Statistics and Computing,", "citeRegEx": "Andrieu and Thoms,? \\Q2008\\E", "shortCiteRegEx": "Andrieu and Thoms", "year": 2008}, {"title": "Joint measures and cross-covariance operators", "author": ["C. Baker"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Baker,? \\Q1973\\E", "shortCiteRegEx": "Baker", "year": 1973}, {"title": "Learning to find preimages", "author": ["G. Bakir", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bakir et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bakir et al\\.", "year": 2003}, {"title": "Reproducing Kernel Hilbert Spaces in Probability and Statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": null, "citeRegEx": "Berlinet and Thomas.Agnan,? \\Q2004\\E", "shortCiteRegEx": "Berlinet and Thomas.Agnan", "year": 2004}, {"title": "Integrating structured biological data by kernel maximum mean discrepancy", "author": ["K.M. Borgwardt", "A. Gretton", "M.J. Rasch", "Kriegel", "H.-P", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Bioinformatics (ISMB),", "citeRegEx": "Borgwardt et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Borgwardt et al\\.", "year": 2006}, {"title": "Pseudo-marginal Bayesian inference for Gaussian Processes", "author": ["M. Filippone", "M. Girolami"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Filippone and Girolami,? \\Q2014\\E", "shortCiteRegEx": "Filippone and Girolami", "year": 2014}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Efficient Metropolis jumping rules", "author": ["A. Gelman", "G.O. Roberts", "W.R. Gilks"], "venue": "In Bayesian statistics,", "citeRegEx": "Gelman et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 1994}, {"title": "Riemann manifold Langevin and Hamiltonian Monte Carlo methods", "author": ["M. Girolami", "B. Calderhead"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "Girolami and Calderhead,? \\Q2011\\E", "shortCiteRegEx": "Girolami and Calderhead", "year": 2011}, {"title": "A kernel method for the two-sample problem", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gretton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2007}, {"title": "Adaptive Proposal Distribution for Random Walk Metropolis Algorithm", "author": ["H. Haario", "E. Saksman", "J. Tamminen"], "venue": "Comput. Stat.,", "citeRegEx": "Haario et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Haario et al\\.", "year": 1999}, {"title": "Slice sampling covariance hyperparameters of latent Gaussian models", "author": ["I. Murray", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Murray and Adams,? \\Q2012\\E", "shortCiteRegEx": "Murray and Adams", "year": 2012}, {"title": "Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms", "author": ["G.O. Roberts", "J.S. Rosenthal"], "venue": "J. Appl. Probab., 44(2):458\u2013475,", "citeRegEx": "Roberts and Rosenthal,? \\Q2007\\E", "shortCiteRegEx": "Roberts and Rosenthal", "year": 2007}, {"title": "Langevin diffusions and Metropolis-Hastings algorithms", "author": ["G.O. Roberts", "O. Stramer"], "venue": "Methodol. Comput. Appl. Probab.,", "citeRegEx": "Roberts and Stramer,? \\Q2003\\E", "shortCiteRegEx": "Roberts and Stramer", "year": 2003}, {"title": "Integral transforms, reproducing kernels, and their applications", "author": ["S. Saitoh"], "venue": "Pitman Research Notes in Mathematics", "citeRegEx": "Saitoh,? \\Q1997\\E", "shortCiteRegEx": "Saitoh", "year": 1997}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "M\u00fcller", "K.-R"], "venue": "Neural Comput.,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Regularized principal manifolds", "author": ["A.J. Smola", "S. Mika", "B. Sch\u00f6lkopf", "R.C. Williamson"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Smola et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2001}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Sch\u00f6lkopf"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Universality, characteristic kernels and RKHS embedding of measures", "author": ["B. Sriperumbudur", "K. Fukumizu", "G. Lanckriet"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2011}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In Proc. of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Bayesian classification with Gaussian processes", "author": ["C.K.I. Williams", "D. Barber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Williams and Barber,? \\Q1998\\E", "shortCiteRegEx": "Williams and Barber", "year": 1998}], "referenceMentions": [{"referenceID": 16, "context": "In the present work, we develop an adaptive Metropolis-Hastings algorithm in which samples are mapped to a reproducing kernel Hilbert space, and the proposal distribution is chosen according to the covariance in this feature space (Sch\u00f6lkopf et al., 1998; Smola et al., 2001).", "startOffset": 231, "endOffset": 275}, {"referenceID": 18, "context": "In the present work, we develop an adaptive Metropolis-Hastings algorithm in which samples are mapped to a reproducing kernel Hilbert space, and the proposal distribution is chosen according to the covariance in this feature space (Sch\u00f6lkopf et al., 1998; Smola et al., 2001).", "startOffset": 231, "endOffset": 275}, {"referenceID": 8, "context": "Based on the chain history, they estimate the covariance of the target distribution and construct a Gaussian proposal centered at the current chain state, with a particular choice of the scaling factor from Gelman et al. (1996). More sophisticated schemes are presented by Andrieu & Thoms (2008), e.", "startOffset": 207, "endOffset": 228}, {"referenceID": 8, "context": "Based on the chain history, they estimate the covariance of the target distribution and construct a Gaussian proposal centered at the current chain state, with a particular choice of the scaling factor from Gelman et al. (1996). More sophisticated schemes are presented by Andrieu & Thoms (2008), e.", "startOffset": 207, "endOffset": 296}, {"referenceID": 8, "context": "38/ \u221a d is a fixed scaling factor from Gelman et al. (1996). This choice of scaling factor was shown to be optimal (in terms of efficiency measures) for the usual Metropolis algorithm.", "startOffset": 39, "endOffset": 60}, {"referenceID": 8, "context": "38/ \u221a d is a fixed scaling factor from Gelman et al. (1996). This choice of scaling factor was shown to be optimal (in terms of efficiency measures) for the usual Metropolis algorithm. While this optimality result does not hold for Adaptive Metropolis, it can nevertheless be used as a heuristic. Alternatively, the scale \u03bd can also be adapted at each step as in Andrieu & Thoms (2008, Algorithm 4) to obtain the acceptance rate from Gelman et al. (1996), a = 0.", "startOffset": 39, "endOffset": 455}, {"referenceID": 7, "context": "This feature map or embedding of a single point can be extended to that of a probability measure P on X : its kernel embedding is an element \u03bcP \u2208 Hk, given by \u03bcP = \u0301 k(\u00b7, x) dP (x) (Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004; Smola et al., 2007).", "startOffset": 181, "endOffset": 255}, {"referenceID": 17, "context": "This feature map or embedding of a single point can be extended to that of a probability measure P on X : its kernel embedding is an element \u03bcP \u2208 Hk, given by \u03bcP = \u0301 k(\u00b7, x) dP (x) (Berlinet & Thomas-Agnan, 2004; Fukumizu et al., 2004; Smola et al., 2007).", "startOffset": 181, "endOffset": 255}, {"referenceID": 19, "context": "Such kernels are said to be characteristic (Sriperumbudur et al., 2010; 2011), since each distribution is uniquely characterized by its embedding (in the same way that every probability distribution has a unique characteristic function).", "startOffset": 43, "endOffset": 77}, {"referenceID": 2, "context": "Next, the covariance operator CP : Hk \u2192 Hk for a probability measure P is given by CP = \u0301 k(\u00b7, x) \u2297 k(\u00b7, x) dP (x) \u2212 \u03bcP \u2297 \u03bcP (Baker, 1973; Fukumizu et al., 2004), where for a, b, c \u2208 Hk the tensor product is defined as (a \u2297 b)c = \u3008b, c\u3009Hk a.", "startOffset": 125, "endOffset": 161}, {"referenceID": 7, "context": "Next, the covariance operator CP : Hk \u2192 Hk for a probability measure P is given by CP = \u0301 k(\u00b7, x) \u2297 k(\u00b7, x) dP (x) \u2212 \u03bcP \u2297 \u03bcP (Baker, 1973; Fukumizu et al., 2004), where for a, b, c \u2208 Hk the tensor product is defined as (a \u2297 b)c = \u3008b, c\u3009Hk a.", "startOffset": 125, "endOffset": 161}, {"referenceID": 16, "context": "Our approach is based on the idea that the nonlinear support of a target density may be learned using Kernel Principal Component Analysis (Kernel PCA) (Sch\u00f6lkopf et al., 1998; Smola et al., 2001), this being linear PCA on the empirical covariance operator in the RKHS, Cz = 1 n \u2211n i=1 k(\u00b7, zi) \u2297 k(\u00b7, zi) \u2212 \u03bcz \u2297 \u03bcz, computed on the sample z defined above.", "startOffset": 151, "endOffset": 195}, {"referenceID": 18, "context": "Our approach is based on the idea that the nonlinear support of a target density may be learned using Kernel Principal Component Analysis (Kernel PCA) (Sch\u00f6lkopf et al., 1998; Smola et al., 2001), this being linear PCA on the empirical covariance operator in the RKHS, Cz = 1 n \u2211n i=1 k(\u00b7, zi) \u2297 k(\u00b7, zi) \u2212 \u03bcz \u2297 \u03bcz, computed on the sample z defined above.", "startOffset": 151, "endOffset": 195}, {"referenceID": 3, "context": "In general, this is a non-convex minimization problem, and may be difficult to solve (Bakir et al., 2003).", "startOffset": 85, "endOffset": 105}, {"referenceID": 11, "context": "Figure 1 plots g(x) and its gradients for several samples of \u03b2-coefficients, in the case where the underlying z-samples are from the two-dimensional nonlinear Banana target distribution of Haario et al. (1999). It can be seen that g may have multiple local minima, and that it varies most along the high-density regions of the Banana distribution.", "startOffset": 189, "endOffset": 210}, {"referenceID": 11, "context": "95% contours (red) of proposal distributions evaluated at a number of points, for the first two dimensions of the banana target of Haario et al. (1999). Underneath is the density heatmap, and the samples (blue) used to construct the proposals.", "startOffset": 131, "endOffset": 152}, {"referenceID": 11, "context": "In Haario et al. (2001), the proposal distribution is asymptotically symmetric due to the vanishing adaptation property.", "startOffset": 3, "endOffset": 24}, {"referenceID": 11, "context": ",\u2207xxzn|x=y ] = 2Z, so the proposal is given by qz(\u00b7|y) = N (y, \u03b3I + 4\u03bdZHZ); thus, the proposal simply uses the scaled empirical covariance ZHZ just like standard Adaptive Metropolis (Haario et al., 1999), with an additional isotropic exploration component, and depends on y only through the mean.", "startOffset": 182, "endOffset": 203}, {"referenceID": 11, "context": "We consider the following nonlinear targets: (1) the posterior distribution of Gaussian Process (GP) classification hyperparameters (Filippone & Girolami, 2014) on the UCI glass dataset, and (2) the synthetic banana-shaped distribution of Haario et al. (1999) and a flower-shaped disribution concentrated on a circle with a periodic perturbation.", "startOffset": 239, "endOffset": 260}, {"referenceID": 5, "context": "Second, the MMD (Borgwardt et al., 2006; Gretton et al., 2007) was computed between each sampler output and the benchmark sample, using the polynomial kernel (1 + \u3008\u03b8, \u03b8\u2032\u3009); i.", "startOffset": 16, "endOffset": 62}, {"referenceID": 10, "context": "Second, the MMD (Borgwardt et al., 2006; Gretton et al., 2007) was computed between each sampler output and the benchmark sample, using the polynomial kernel (1 + \u3008\u03b8, \u03b8\u2032\u3009); i.", "startOffset": 16, "endOffset": 62}, {"referenceID": 11, "context": "In Haario et al. (1999), the following family of nonlinear target distributions is considered.", "startOffset": 3, "endOffset": 24}, {"referenceID": 11, "context": "We compute the following measures of performance (similarly as in Haario et al. (1999); Andrieu & Thoms (2008)) based on the chain after burn-in: average acceptance rate, norm of the empirical mean (the true mean is by construction zero for all targets), and the deviation of the empirical quantiles from the true quantiles.", "startOffset": 66, "endOffset": 87}, {"referenceID": 11, "context": "We compute the following measures of performance (similarly as in Haario et al. (1999); Andrieu & Thoms (2008)) based on the chain after burn-in: average acceptance rate, norm of the empirical mean (the true mean is by construction zero for all targets), and the deviation of the empirical quantiles from the true quantiles.", "startOffset": 66, "endOffset": 111}], "year": 2014, "abstractText": null, "creator": "LaTeX with hyperref package"}}}