{"id": "1307.5870", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2013", "title": "Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery", "abstract": "Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms of the unfoldings of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a K-way tensor of length n and Tucker rank r from Gaussian measurements requires $\\Omega(rn^{K-1})$ observations. In contrast, a certain (intractable) nonconvex formulation needs only $O(r^K+nrK)$ observations. We introduce a very simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with $O(r^{\\lfloor K/2 \\rfloor} n^{\\lceil K/2 \\rceil})$ observations. While these results pertain to Gaussian measurements, simulations strongly suggest that the new norm also outperforms the sum of nuclear norms for tensor completion from a random subset of entries. Our lower bounds for the sum-of-nuclear-norm model follow from a new result on simultaneously structured models, which may be of independent interest for matrix and vector recovery problems.", "histories": [["v1", "Mon, 22 Jul 2013 20:23:29 GMT  (36kb,D)", "http://arxiv.org/abs/1307.5870v1", null], ["v2", "Thu, 15 Aug 2013 05:59:52 GMT  (33kb,D)", "http://arxiv.org/abs/1307.5870v2", "Slight modifications are made in this second version (mainly, Lemma 5)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["cun mu", "bo huang", "john wright", "donald goldfarb"], "accepted": true, "id": "1307.5870"}, "pdf": {"name": "1307.5870.pdf", "metadata": {"source": "CRF", "title": "Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery", "authors": ["Cun Mu", "Bo Huang", "John Wright", "Donald Goldfarb"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper, we look at the problem of restoring a multidimensional object whose entries are indexed by several continuous or discrete variables. (...) We have a very low spatial structure and a very low frequency / wavelength variable. (...) The general problem of estimating a low level of applications, both theoretical and applied, is for example the estimation of latent variable graphical models [AGH + 12] that classify audio [MSS06] [CC12], the processing of radar signals [DN10] to name a name.In this paper, we look at the problem of restoring aK paths or X models [AGH + 12], audio [MSS06], processing radar signals [DN10] to name a name."}, {"heading": "2 Bounds for Non-Convex Recovery", "text": "In this section, we present a viable point, which is described as optimal if there is no other solution than that we see ourselves as being able to regenerate. (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...) We are not able to regenerate (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...)."}, {"heading": "3 Convexification: Sum of Nuclear Norms?", "text": "G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G). (G (G). (G). (G). (G). (G). (G (G). (G). (G). (G). (G (G). (G). (G). (G (G). (G). (G). (G. (G). (G). (G). (G). (G). (G). (G). (G). (G). (G (G). (G). (G). (G). (G). (G). (G"}, {"heading": "4 A Better Convexification: Square Norm", "text": "The number of measurements promised by Corollary 2 and Theorem 3 is actually the same (up to constants) as the number of measurements required to regenerate a tensor X that has only a low rank. Since matrix norm (up to constants) is actually the same (up to constants) as the number of measurements necessary to regenerate a ranking (n1 + n2), the number of matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix is actually is actually the same (up to constants)."}, {"heading": "5 Simulation Results for Tensor Completion", "text": "By introducing appropriate incoherence conditions (and minor modifications in [Gro11]), it is possible to prove the restoration guarantees for each of the following programs: (5.2) Unlike the Gaussian Random Restoration Problem, due to the lack of sharp upper limits, we have no evidence that our square standard formulation exceeds the SNN model here. (5.2) Unlike the Gaussian Random Restoration Problem, due to the lack of sharp upper limits, we have no evidence that our square standard formulation exceeds the SNN model here. However, our simulation result below will strongly suggest that (5.2) is also much better than (5.1) the Utensor Random Restoration Problem. Our experiment is structured as follows."}, {"heading": "6 Conclusion", "text": "In this paper, we note several theoretical limits to the problem of low tensor recovery."}, {"heading": "A Proofs for Section 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 1.", "text": "The arguments used below are primarily adapted to [ENP11], where their interest is to determine the number of Gaussian measurements required to restore a matrix of low rank by minimizing rank. Note that each D-S2r and each i, < Gi, D > is a standard Gaussian random variable, and so is the P [< Gi, D > | < t < t] < 2t \u00b7 1 \u00b2 \u00b2 \u00b2. (A.1) Leave N in relation to S2r \u00b2. (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...)) (...) (...) (...) (...) (...) (...).) (...) (...) (...).) (...) (...) (...) (...) (...) (...) (...) (...) (...).) (...) (...) (...).) (...).) (...) (...) (...) (...)."}, {"heading": "Proof of Lemma 2.", "text": "Proof. This stems from the basic fact that for each tensor X and each matrix U with compatible size, which can be determined by direct calculation. Write what [[C; U1,..., UK]] \u2212 [C \"; U\" 1,.., U \"1,.., U\" K \"], [C\"; U \"1,..., U\" 1,..., U \"i, [C\"; U1,..., UK], [C \"; U\" 1,., U \"i,.], [C\"; U \"1,.,..., U\" i. \"\u2212 [C\"; U \"1,.,.., U\" i.,.,., U \"i.,.,., U\" i.,., \"U\" i. \""}, {"heading": "Proof of Lemma 3.", "text": "The idea of this proof is to construct a net for each component of Tucker decomposition and then to build these nets into a composite net with the desired cardinality.Denote C = {C-R2r-2r-2r-2r-2r-2r-2r-C-F = 1 and O = {U-Rn-r-r-U-I.} Denote C = {C-R2r-2r-2r-2r-2r-U = 1. Thus, in proposal 4 in [Ver07] and Lemma 5.2 in [Ver10] there is a \u03b5K + 1-net-C-C coverage net in relation to the Frobenius standard so that # C- \u2264 (3 (K + 1) -K-K) (2r) K-K, and there is a \u03b5K + 1-net-O coverage net in relation to the operator standard so that # O-R-2r-U."}, {"heading": "B Proofs for Section 3", "text": "The proof of the upper limit for the statistical dimension of the circular cone: \u03b4 (circulation (x0, \u03b8) = \u00b7 Bin (circulation) \u2264 n sin2 \u03b8 + 1.Proof. Denote circ (en, \u03b8) as circn (\u03b8), where en is the n-th standard basis for Rn. Furthermore, several results established in [ALMT13] prove that it is sufficient to test a case in which n (circle (en, \u03b8) + n (circle (x0, \u03b8)) is straight. Let us first consider the case in which n (circle) is straight. Let us define a discrete random variable V specified in (C.2) of [ALMT13]: vk = 1 (vk = 2) function: vk = 2 (cosk = 2) (V)."}, {"heading": "Proof of Corollary 4.", "text": "proof. Denote \u03bb = \u03b4 (C) \u2212 m. Following theorem 7.1 in [ALMT13] we have P [C-null (G) = {0}] \u2264 exp (\u2212 \u03bb 2 / 8min {\u03b4 (C), \u03b4 (C)} + \u03bb) \u2264 exp (\u2212 \u03bb 2 / 8\u03b4 (C) + \u03bb) \u2264 4 exp (\u2212 (\u03b4 (C) \u2212 m) 216\u03b4 (C))."}, {"heading": "Proof of Theorem 5.", "text": "Proof. Note that for each fixed m > 0 the function f: t \u2192 4 exp (\u2212 (t \u2212 m) 216t) decreases \u2265 m from now on. Then, due to inference 4 and the fact that \u03b4 (C) is definitively \u2212 1 \u2265 m, P [x0 is the only optimal solution for (3,3)] = P [C-null (G) = {0}] \u2264 4 exp (\u2212 (\u03b4 (C) \u2212 m) 216\u03b4 (C) \u2264 4 exp (\u2212 m \u2212 1) 216 (\u0432 \u2212 1)))."}, {"heading": "C Proofs for Section 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 5.", "text": "Proof. (1) According to the definition of X [j] it is sufficient to prove that the vectorization of the right side of (4,2) Vec (X (1) = Vec (1). Da X = \u2211 r i (1) i (1) i (2) i (2) i (\u00b7 \u00b7 a (K) i () () i () i () i () i a (K) i () i a (K \u2212 1) i () i () i () i () i a (K \u2212 1) i (i) i () i () i c () i c () i c), the last equality being derived from the fact that the last equality a (i) i (b) = i (i \u00b7 b) = i (i) i (i) i (i) i (i) i (c () i c () a ()."}, {"heading": "D Algorithms for Section 5", "text": "In this section we will discuss in detail our implementation of the accelerated linear problems. (D.1) By introducing the additional variable W and dividing X into X 1, X 2, \u00b7 \u00b7 \u00b7, XK, this problem can be easily verified. (D.1) By introducing the additional variable W and dividing X into X 1, X 2, \u00b7 \u00b7, XK, it may be easier to solve this problem. (D.1) The objective function is now separable. (X) (i) The accelerated linearized Bregman algorithms (ALB) algorithms proposed in [HMG13] is an efficient firststorder method designed to solve problems with non-smooth functions and linear constraints."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms of the unfoldings of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a K-way tensor of length n and Tucker rank r from Gaussian measurements requires \u03a9(rnK\u22121) observations. In contrast, a certain (intractable) nonconvex formulation needs only O(r +nrK) observations. We introduce a very simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with O(rbK/2cndK/2e) observations. While these results pertain to Gaussian measurements, simulations strongly suggest that the new norm also outperforms the sum of nuclear norms for tensor completion from a random subset of entries. Our lower bounds for the sum-of-nuclearnorm model follow from a new result on simultaneously structured models, which may be of independent interest for matrix and vector recovery problems.", "creator": "LaTeX with hyperref package"}}}