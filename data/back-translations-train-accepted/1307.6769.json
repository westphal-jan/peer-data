{"id": "1307.6769", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2013", "title": "Streaming Variational Bayes", "abstract": "We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation primitive function. We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections. We demonstrate the advantages of our algorithm over stochastic variational inference (SVI), both in the single-pass setting SVI was designed for and in the streaming setting, to which SVI does not apply.", "histories": [["v1", "Thu, 25 Jul 2013 15:03:40 GMT  (214kb,D)", "http://arxiv.org/abs/1307.6769v1", "21 pages"], ["v2", "Wed, 20 Nov 2013 23:29:01 GMT  (76kb,D)", "http://arxiv.org/abs/1307.6769v2", "25 pages, 3 figures, 1 table"]], "COMMENTS": "21 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tamara broderick", "nicholas boyd", "andre wibisono", "ashia c wilson", "michael i jordan"], "accepted": true, "id": "1307.6769"}, "pdf": {"name": "1307.6769.pdf", "metadata": {"source": "CRF", "title": "Streaming Variational Bayes", "authors": ["Tamara Broderick", "Nicholas Boyd", "Andre Wibisono", "Ashia C. Wilson", "Michael I. Jordan"], "emails": [], "sections": [{"heading": null, "text": "(A) Synchronous calculation of a Bayesian background. The framework performs streaming updates of the estimated background according to a custom approximation primitive function. We demonstrate the usefulness of our framework, with Variational Bayes (VB) representing the primitive by adapting the latent dirichlet assignment model to two large document collections. We demonstrate the advantages of our algorithm over Stochastic Variation Conclusions (SVI), both in the singlepass setting for which SVI was designed and in the streaming setting to which SVI does not apply."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have established themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) It is not the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is not the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...) It is the case that they abide by the rules. (...). (...). (...). (It is.). (It is. (...). (It is.) It is. (...). (It is. (...). (It is. (It is.) It is. (It is. (...). (It is. (It is.) It is. (It is. (...). (It is. (It is.). (It is. (It is.) It is. (It is. (It is.). (It is. (It is.)"}, {"heading": "2 Streaming, distributed, asynchronous Bayesian up-", "text": "Dating"}, {"heading": "2.1 Streaming Bayesian updating", "text": "Consider the data x1, x2,.. generates iid according to a distribution p (x | q) and assumes that a previous p (\u043a) was also specified. Then, the Bayes theorem gives us the posterior distribution of the data giving a collection of S data points, C1: = (x1,.., xS): p (xs | C1) = p (C1) \u2212 1 p (C1,.), where p (C1,..) = p (x1,.,.) Ss = 1 p (xs,.). Suppose we have seen and processed b \u2212 1 surveys, sometimes called minibatches, of data. Given the posterior p (,..) Cb \u2212 1,."}, {"heading": "2.2 Distributed Bayesian updating", "text": "The fact is that we are going to be able to be in a position, and that we are going to be able to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to be in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in."}, {"heading": "2.3 Asynchronous Bayesian updating", "text": "In practice, it is often the case that a single calculation thread takes longer than the rest. In this case, the processors known as masters can solve a sub-problem. When a worker is finished, he reports his solution to a single master processor. If the master gives the worker a new sub-problem without waiting for the other workers, he can reduce the downtime in the system. Our asynchronous algorithm is in the spirit of Hogwild! [1] To present the algorithm, we first describe an asynchronous calculation that we will not use in practice, but that will serve as a conceptual springboard."}, {"heading": "3 Case study: latent Dirichlet allocation", "text": "To use SDA-Bayes, one must specify: (1) a before the global parameter (s) q and (2) a posterior-approximating algorithm A. The following are examples of both decisions in the context of latent dirichlet allocation (LDA) [13]. LDA models the contents of D documents in a corpus. Topics that are potentially shared by multiple documents are described by topics. The unattended learning problem is to learn the topics and find out which topics occur in the documents. Formally, each topic (from K overall topics) is considered to be distributed over the V words in the vocabulary: \u03b2k = (\u03b2kv) Vv = 1. Each document is an admixture of topics. The words in document d are assumed to be interchangeable. Each word wdn belongs to a latent topic zdn, which is selected according to a document-specific distribution of topics zdn d."}, {"heading": "3.1 Posterior-approximation algorithms", "text": "To apply SDA-Bayes to LDA, we use the previous problem specified by the generative model. It remains to be seen whether we choose a posterior approximate algorithm. We consider two possibilities here: variational Bayes (VB) and expectation propagation (EP). Both primitives take Dirichlet distributions as precursors for \u03b2 and both supply Dirichlet distributions for the approximate posterior parameters \u03b2; therefore, the previous and approximate posterior distributions are in the same exponential family. Therefore, both VB and EP can be used as a prefix for A within the SDA-Bayes framework."}, {"heading": "3.2 Other single-pass algorithms for approximate LDA posteriors", "text": "Next, we look at two algorithms that traverse a data set only once (single pass) and compare them with each other in the evaluations (Sec. 4). Stochastic variation inference (BayI) [3, 4] is exactly the application of a particular version of stochastic gradient lineage to the same optimization problem. While stochastic gradient lineage can often be considered a streaming algorithm, the optimization problem itself depends on D via pD. \u2212 kD lineage to the same optimization problem. While stochastic gradient lineage can often be considered a streaming algorithm, the optimization problem itself depends on D via pD. \u2212 kD deviation on D data points."}, {"heading": "4 Evaluation", "text": "We compare SDA-Bayes with VB and EP primitives with SVI and SSU."}, {"heading": "4.1 Performance measure", "text": "We follow [4] (and further [15, 16]) in evaluating our algorithms by calculating the (approximate) predictive probability. Under this measurement, a higher value is better because a better model assigns a higher probability to the words provided. We calculate the predictive probability by first leaving aside the provided test documents C (test) from the complete corpus and then a subset of reserved test words Wd, test in each test document. The remaining (training) documents C (train) are used to estimate the global parameter posterior q (\u03b2), and the remaining (training) words Wd, train within the reserved test document are used to calculate the document-specific parameter posterior q (\u03b8d) 1. To calculate the predictive probability, an approximation is necessary because we do not know the predictive distribution - exactly how we try to learn the posterior distribution."}, {"heading": "4.2 Experiments", "text": "In fact, it is so that most of us are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he played the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, the \"the\" the \""}, {"heading": "5 Discussion", "text": "We have introduced SDA-Bayes, a framework for streaming distributed asynchronous calculations of an approximate Bayes background. Our framework makes streaming updates of the estimated background primitive according to a user-specific approximation. We have demonstrated the usefulness of our framework with Bayes variation as a primitive by adapting the latent Dirichlet assignment theme model to Wikipedia and Nature corpora. We have demonstrated the advantages of our algorithm over stochastic variation conclusions and the sufficient statistical update algorithm, especially with regard to the key question of obtaining posterior probabilities based on the number of documents seen so far, not on posterior probabilities for a fixed number of documents."}, {"heading": "Acknowledgments", "text": "We thank Matt Hoffman, Chong Wang and John Paisley for generously sharing their code and data. T. Broderick is supported by the Berkeley Fellowship. N. Boyd is supported. We chose 8 topics because less was too slow to get results, and anything larger was too high to generate memory demand in our system. A. C. Wilson is supported by the Chancellor's Fellowship at UC Berkeley. This research is supported in part by the NSF CISE Expedition Award CCF-1139158 and the DARPA XData Award FA875012-2-0331, as well as donations from Amazon Web Services, Google, SAP, Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo! This material is based on a portion of the work supported by Naval Research Office number 014 / 88."}, {"heading": "A Expectation Propagation", "text": "We begin by expressing the word assignments z in the posterior (7) of LDA, we can specify the number of times v in document d.Collapsed posterior. We begin by expressing the word assignments z in document d.Collapsed posterior. We begin by using the word assignments z in document d.Collapsed posterior. We begin by using the word assignments z in document d.Collapsed posterior. We begin by using the word assignments z in document d.Collapsed posterior. We begin by using the word assignments z in document d.Collapsed posterior. (i.e.) We begin by using the word assignments z in document d.Collapsed posterior. We begin by using the word assignments z in document d.Collapsed posterior. We begin by using the word assignments z in document d.Collapsed posterior. (i.e.) We begin by using the word assignments z in document d.Collapsed posterior."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We present SDA-Bayes, a framework for (S)treaming, (D)istributed,<lb>(A)synchronous computation of a Bayesian posterior. The framework makes<lb>streaming updates to the estimated posterior according to a user-specified<lb>approximation primitive function. We demonstrate the usefulness of our<lb>framework, with variational Bayes (VB) as the primitive, by fitting the la-<lb>tent Dirichlet allocation model to two large-scale document collections. We<lb>demonstrate the advantages of our algorithm over stochastic variational in-<lb>ference (SVI), both in the single-pass setting SVI was designed for and in<lb>the streaming setting, to which SVI does not apply.", "creator": "LaTeX with hyperref package"}}}