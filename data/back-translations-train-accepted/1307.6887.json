{"id": "1307.6887", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2013", "title": "Sequential Transfer in Multi-armed Bandit with Finite Set of Models", "abstract": "Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of \\textit{sequential transfer in online learning}, notably in the multi-armed bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.", "histories": [["v1", "Thu, 25 Jul 2013 22:17:12 GMT  (53kb)", "http://arxiv.org/abs/1307.6887v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["mohammad gheshlaghi azar", "alessandro lazaric", "emma brunskill"], "accepted": true, "id": "1307.6887"}, "pdf": {"name": "1307.6887.pdf", "metadata": {"source": "CRF", "title": "Sequential Transfer in Multi-armed Bandit with Finite Set of Models", "authors": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "emails": ["mazar@cs.cmu.edu", "alessandro.lazaric@inria.fr", "ebrun@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 7.68 87v1 [st at.M L] 25 Ju"}, {"heading": "1 Introduction", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "2 Preliminaries", "text": "We will look at a stochastic MAB problem defined by a series of arms A = {1,., K}, | A | = K, where each i-A problem is characterized by a distribution and the samples observed by each arm are independently and identically distributed. We will focus on the setting in which there are a series of models for which the best value of each model exists.,"}, {"heading": "3 Mult-armed Bandit with Finite Models", "text": "Before considering the transfer problem, we show that a simple variation on UCB enables the knowledge of the uncertainty that is always applied to be effectively used and to achieve a significant reduction in regret. Fig. 1 \"s mUCB (model UCB) algorithms take as input a number of models, including the current (unknown) model. At each step, the algorithm computes a subset containing only those models whose averages are compatible with the current estimates. It is sufficient that one arm does not meet the compatibility conditions to discard a model. Of all the models in Fig., mUCB first selects the model with the greatest optimal value and then extracts its corresponding optimal values. This choice is compatible with optimism about the principle of uncertainty of the applied uncertainty."}, {"heading": "4 Online Transfer with Unknown Models", "text": "We now consider the case when the set of models is unknown and the regret is accumulated over several tasks drawn from B. We introduce tUCB (Transfer-UCB), which transfers estimates of B, the accuracy of which is improved by episodes using a method-of-moments approach."}, {"heading": "4.1 The transfer-UCB Bandit Algorithm", "text": "It requires: number of arms K, number of models m, number of models m, constant C (\u03b8). Initialize estimated models (1 = 201xi). It requires: number of arms K, number of models m, constant C (\u03b8). (1 = 201xi). (1 = 201xi). (2). (2). (3). (3). (3). (3). (3). (4). (4). (4). (4). (3). (3). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (5). (5. (5). (5). (5. (5). (5). (5. (5). (5). (5). (5. (5). (5). (5). (5). (5). (5. (5). (5). (5). (5). (5). (4). (5). (4). (5). (5). (5). (5). (4). (4). (4)."}, {"heading": "4.2 Sample Complexity of the Robust Tensor Power Method", "text": "umUCB requires \u03b5j as input, i.e. the uncertainty of the model estimates. Although we need finite sample complexity limits beyond the accuracy of the tasks calculated by RTP, this error rate is directly affected by the error of the estimates M-2 and M-3. In Thm. 2 we prove that the following definition and assumption is required for our results. Definition 1: Let us quickly decrease these errors at the rate of \"1 / j.\" This result provides us with an upper limit for the error rate required to build the confidence intervals in umUCB. Definition 1: Let the results M2 = {2, MM} become the largest eigenvalues of the matrix M2."}, {"heading": "5 Numerical Simulations", "text": "In this section we report on the preliminary results of the tUCB on synthetic data. The goal is to illustrate and support the theoretical results so far. We define a set of results for the actual values, where each model has a different color and squares than the optimal arms (e.g., arm 2 is optimal for the UC2).This set of models is chosen to be challenging and to illustrate some interesting cases that are useful to understand the functioning of the algorithms. 6 models that differ only in their optimal arms and this makes it difficult to distinguish them (which is optimal for model 3 and thus potentially selected by mUCB), all models share exactly the same mean. This implies that no model can be discarded by pulling."}, {"heading": "6 Conclusions and Open Questions", "text": "In this paper, we present the transfer problem in the multi-armed UCB framework when a task is drawn from a finite set of bandit problems. We first introduced the bandit algorithm mUCB and demonstrated that it is able to fully exploit the previous knowledge of the set of bandit problems. This knowledge is then transferred to umUCB, which never performs worse than UCB and tends to approach the performance of mUCB. For these algorithms, we deduce regret and sample complexity, and we show preliminary numerical simulations from the samples collected through episodes. This knowledge is then transferred to umUCB, which never performs worse than UCB and tends to approach the performance of mUCB. For these algorithms, we deduce repentance and sample complexity, and we show preliminary numerical simulations from the samples collected through episodes using UCB. To achieve the best of our knowledge, this is the first set of modalities, the problem of the transmission strategy may be more interesting in the Optimal.B"}, {"heading": "A Table of Notation", "text": "Symbol ExplanationA series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of a series of models which consist of a series of models which consist of a series of a series of a series of models which consist of models which consist of a series of a series of models which consist of a series of models which consist of a series of a series of a series of models which consist of"}, {"heading": "B Proofs of Section 3", "text": "Lemma 1. mUCB never pulls the arms that are not optimal for at least one model, that is, that they are not optimistic. (Ti, n = 0 with probability 1,.) It is also noteworthy that the event E = 1 with probability 1,. (n) that the event E = 1 with probability 1,. (n) that the event E = 1 with probability 1,. (n) that the event E = 1 with probability P [E] 1 \u2212 2,. (n) is not true. (t) that the event E = 12Ti, t \u2212 1 log (mn2g) where Ti \u2212 1 is the number of movements occurring at the beginning of step t and m =. In the previous problem we implicitly assumed that the best choice in the definition of two models i, t has a logarithmic factor with min. (K).Lemma 3. All arms i are not optimal. (m)."}, {"heading": "D Proofs of Section 4.3", "text": "Lemma 8: In Episode j, the poor i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: i / 2: 2: i / 2: i / 2: i / 2: 2 / 3: i / 2: 2 / 3: i / 2: 2 / 3: i / 2: 2 / 3: 2 / 3: 3 / 3: 3 / 3: 3 / 4: 3 / 4: 3 / 4: 4: 4 / 4: 4 / 4: 4 / 4: 4: 4 / 4: 4 / 4: 4 / 4: 4 / 4: 4: 4 / 4: 4: 4 / 4: 4: 4 / 4: 4: 4 / 4: 4 / 4: 4 / 4: 4 / 4: 4: 4 / 4: 4: 4 / 4: 4: 4 / 5 5 / 4: 4: 4 5 5 / 4: 5 / 4: 4: 4: 4 4 4: 4 / 5 4: 4: 4 4: 5 / 4 4: 4: 4: 4 / 5: 4: 4: 4 / 4: 5: 4: 4: 4 / 4: 4: 5: 4: 4: 4: 4 / 4: 4: 5: 4: 4: 4: 4 / 4: 4: 5: 4: 4: 4: 4: 4 / 4: 4: 5: 4: 4: 4: 4: 4: 4 / 4: 4: 4: 5: 4: 4: 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4 / 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4"}, {"heading": "E Related Work", "text": "In this section, we review possible alternatives and a number of settings related to the problem we are looking at in this paper. Although we use RTP in tUCB to estimate the model, a large number of other algorithms could be used, especially those based on the Method of Moments (MoM). The main idea of MoM is to compare the empirical moments of the data with the model parameters, which give almost the same corresponding population numbers. In general, adapting the model parameters to the observed moment-solving systems requires high order polynomial equilibriums, which are often prohibitive. However, it is possible for a class of LVMs to estimate the moments efficiently."}, {"heading": "F Numerical Simulations", "text": "In Table 1 we give the actual values of the averages of the arms of the models on average, while in Table 2 we compare the complexity of UCB, UCB + and mUCB for all the different models and on average. Finally, the diagrams in Fig. 9 represent an extension to J = 10000 of the performance of tUCB for n = 5000 shown in the main text. 8 We only need to know the sum of the parameters of the Dirichlet distribution \u03b10.00.050.1 051015202530Model error C ompl exity."}], "references": [{"title": "Contextual bandit learning with predictable rewards", "author": ["A. Agarwal", "M. Dud\u00edk", "S. Kale", "J. Langford", "R.E. Schapire"], "venue": "In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS\u201912)", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S. Kakade", "Liu", "Y.-K"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": null, "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "In Proceeding of the 25th Annual Conference on Learning Theory (COLT\u201912),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Optimization for Machine Learning, chapter Bandit View on Noisy Optimization", "author": ["Audibert", "J.-Y", "S. Bubeck", "R. Munos"], "venue": null, "citeRegEx": "Audibert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2012}, {"title": "Finite-time analysis of the multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Linear algorithms for online multitask classification", "author": ["G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cavallanti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cavallanti et al\\.", "year": 2010}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Online multitask learning", "author": ["O. Dekel", "P.M. Long", "Y. Singer"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory (COLT\u201906),", "citeRegEx": "Dekel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2006}, {"title": "On upper-confidence bound policies for switching bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "In Proceedings of the 22nd international conference on Algorithmic learning theory,", "citeRegEx": "Garivier and Moulines,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Moulines", "year": 2011}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Langford and Zhang,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang", "year": 2007}, {"title": "Transfer in reinforcement learning: a framework and a survey", "author": ["A. Lazaric"], "venue": "Reinforcement Learning: State of the Art. Springer", "citeRegEx": "Lazaric,? \\Q2011\\E", "shortCiteRegEx": "Lazaric", "year": 2011}, {"title": "Online multi-task learning with hard constraints", "author": ["G. Lugosi", "O. Papaspiliopoulos", "G. Stoltz"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT\u201909)", "citeRegEx": "Lugosi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lugosi et al\\.", "year": 2009}, {"title": "Directed exploration in reinforcement learning with transferred knowledge", "author": ["T.A. Mann", "Y. Choe"], "venue": "In Proceedings of the Tenth European Workshop on Reinforcement Learning (EWRL\u201912)", "citeRegEx": "Mann and Choe,? \\Q2012\\E", "shortCiteRegEx": "Mann and Choe", "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Online learning of multiple tasks and their relationships", "author": ["A. Saha", "P. Rai", "H. Daum\u00e9 III", "S. Venkatasubramanian"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS\u201911),", "citeRegEx": "Saha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saha et al\\.", "year": 2011}, {"title": "Matrix perturbation theory", "author": ["G.W. Stewart", "Sun", "J.-g"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["P. Wedin"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Wedin,? \\Q1972\\E", "shortCiteRegEx": "Wedin", "year": 1972}, {"title": "However, this result does not provide us with the sample complexity bound on the estimation error of model means. Here we complete their analysis by proving a sample complexity bound on the l2-norm of the estimation error of the means \u2016\u03bc(\u03b8)\u2212 \u03bc\u0302(\u03b8)", "author": ["Anandkumar et al", "2012b", "Thm"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Lem. 6). (ii) we prove high probability bounds on the error \u01eb2 and \u01eb3 using some standard concentration inequality results (Lem", "author": ["\u2212 M"], "venue": null, "citeRegEx": "M3.,? \\Q2012\\E", "shortCiteRegEx": "M3.", "year": 2012}, {"title": "Langford and Zhang, 2007), at each step the learner observes a context xt and has to choose the arm which is best for the context. The contexts belong to an arbitrary (finite or continuous) space and are drawn from a stationary distribution. This scenario resembles our setting where tasks arrive in a sequence", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2012\\E", "shortCiteRegEx": "Agarwal", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Recently, multi-task and transfer learning received much attention in the supervised and reinforcement learning (RL) setting with both empirical and theoretical encouraging results (see recent surveys by Pan and Yang, 2010; Lazaric, 2011).", "startOffset": 181, "endOffset": 238}, {"referenceID": 7, "context": "On the other hand, the online learning setting (Cesa-Bianchi and Lugosi, 2006), where the learner is presented with samples in a sequential fashion, has been rarely considered (see Mann and Choe (2012) for an example in RL and Sec.", "startOffset": 47, "endOffset": 78}, {"referenceID": 15, "context": "The multi\u2013arm bandit (MAB) (Robbins, 1952) is a simple yet powerful framework formalizing the online learning with partial feedback problem, which encompasses a large number of applications, such as clinical trials, online advertisements and adaptive routing.", "startOffset": 27, "endOffset": 42}, {"referenceID": 5, "context": "If we can accurately estimate this LVM, we show that an extension of the UCB algorithm (Auer et al., 2002) is able to exploit this prior knowledge to reduce the regret through tasks (Sec.", "startOffset": 87, "endOffset": 106}, {"referenceID": 3, "context": "On the other hand, the online learning setting (Cesa-Bianchi and Lugosi, 2006), where the learner is presented with samples in a sequential fashion, has been rarely considered (see Mann and Choe (2012) for an example in RL and Sec.", "startOffset": 48, "endOffset": 202}, {"referenceID": 1, "context": "1 of Anandkumar et al. (2012b) and compute eigen-vectors/values {v\u0302(\u03b8)}, {\u03bb\u0302(\u03b8)} Compute \u03bc\u0302(\u03b8) = \u03bb\u0302(\u03b8)(\u0174 )v\u0302(\u03b8) for all \u03b8 \u2208 \u0398 return \u0398 = {\u03bc\u0302(\u03b8) : \u03b8 \u2208 \u0398}", "startOffset": 5, "endOffset": 31}, {"referenceID": 1, "context": "Anandkumar et al. (2012b) (Thm.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "5One may also estimate the constant C(\u0398) in an online fashion using doubling trick (Audibert et al., 2012).", "startOffset": 83, "endOffset": 106}, {"referenceID": 1, "context": "This is a direct consequence of the perturbation bound of Anandkumar et al. (2012b, Thm. 5.1), which is at the core of our sample complexity bound. 4The result of Anandkumar et al. (2012a) has the explicit dependency of order m3 on the number of model as well as implicit dependency of order m2 through the parameter \u03b10.", "startOffset": 58, "endOffset": 189}], "year": 2013, "abstractText": "Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi-armed bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.", "creator": "LaTeX with hyperref package"}}}