{"id": "1307.7577", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jul-2013", "title": "Safe Screening with Variational Inequalities and Its Application to Lasso", "abstract": "The model selected by sparse learning techniques usually has a few non-zero entries. Safe screening---which eliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution---is a technique for improving the computational efficiency while maintaining the same solution. In this note, we propose an approach called Sasvi (Safe screening with variational inequalities) and take LASSO for example in the analysis. Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on which a sure removal regularization parameter can be identified for each feature. In addition, the proposed Sasvi is readily extended for solving the generalized sparse linear models. Preliminary experimental results are reported.", "histories": [["v1", "Mon, 29 Jul 2013 13:45:58 GMT  (127kb,D)", "http://arxiv.org/abs/1307.7577v1", "20 pages"], ["v2", "Wed, 30 Oct 2013 15:00:04 GMT  (3312kb,D)", "http://arxiv.org/abs/1307.7577v2", "27 pages, 6 figures, 2 tables"], ["v3", "Mon, 12 May 2014 19:46:39 GMT  (3322kb,D)", "http://arxiv.org/abs/1307.7577v3", "Accepted by International Conference on Machine Learning 2014"]], "COMMENTS": "20 pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jun liu 0003", "zheng zhao", "jie wang", "jieping ye"], "accepted": true, "id": "1307.7577"}, "pdf": {"name": "1307.7577.pdf", "metadata": {"source": "CRF", "title": "Safe Screening With Variational Inequalities and Its Applicaiton to LASSO", "authors": ["Jun Liu", "Zheng Zhao", "Jie Wang", "Jieping Ye"], "emails": ["jun.liu@sas.com", "zheng.zhao@sas.com", "jie.wang.ustc@asu.edu", "jieping.ye@asu.edu"], "sections": [{"heading": "1 Introduction", "text": "The strict rules proposed in [2] work very well in practice, although they may erroneously discard active features. To remedy this, safe screening techniques [1, 3, 4] have been proposed that ellipse the features that are guaranteed to be missing for a target regulation parameter based on an existing solution.ar Xiv: 130 7.75 77v1 [cs.LG] These approaches examined the dual problem and limited a function of the dual variables to the target regulation parameter based on an existing solution.ar Xiv: 130 7.75 77v1 [cs.LG] In this note, we propose an approach called Sasvi (Safe screening with variational inequality) and take LASSO as an example of Sasvi variation in the analysis. Sasvi makes use of the sasmonic inequalities that provide sufficient and necessary optimal conditions for the dual problem."}, {"heading": "2 Safe Screening with Variational Inequalities", "text": "To simplify the discussion, we will take LASSO as an example and discuss the extension to generalized linear models such as logistic regression in Section 3.For each \u03bb > 0, the dual of the LASSO problem can be derived as follows: min\u03b2 {12% X\u03b2 \u2212 y | | 22 + \u03bb% 1} (1) = min\u03b2 maxalties {< y \u2212 X\u03b2, xi \u00b2 \u00b2 - 12%. (2) = maxalties min\u03b2 \u00b2 (2) = min\u03b2 \u00b2 (1) < y, \u03b8 > < 2). (4) Eq. (1) Eq. (1) is the (primal) lasso problem. (2) A dual variable is introduced. (2) It is obvious that the optimal variable (1) and the optimal variable (2). (4) Eq."}, {"heading": "2.1 Variational Inequality", "text": "Lemma 1 For the limited convex optimization problem: min x-G f (x), (9) where G begins convex and closed and f (\u00b7) is convex and differentiable, x-x is an optimal solution if < f (x), x-x > 0, x-x-G (10) Equation (10) is the so-called variation inequality for the problem in Equation (9). If we apply Lemma 1, we can represent the optimal conditions for the inequalities in Eq. 1 and 2 as the following variation inequalities: < \u03b8 1 \u2212 yh1, \u03b8 1 \u2212 0,."}, {"heading": "2.2 The Proposed Sasvi", "text": "In the proposed safe order with variable imbalances (Sasvi), we can abide by the rules. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "2.3 Relationship to Existing Approaches", "text": "Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3]; it can be observed that the SAFE approach begins with the assumption that the optimal solution to the dual problem is in Eq. (8). Denote G (2) = 12 - 12 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 - 2 -"}, {"heading": "2.4 Feature Sure Removal Parameter", "text": "& & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \u00b2 & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & &"}, {"heading": "3 Extension to Logistic Regression", "text": "The minor logistic regression can be written as follows: min \u03b2 = 1 log = 1 log (1 + exp (\u2212 yiui) + 1 log (\u2212 yiui) + 1 log (1 + exp) + 1 log (1 + exp). (51) Introduction of the dual variables z for the equality constraints ui = \u03b2 Txi, i = 1, 2.,., we havemax z \u03b2 (1 + exp). (1 log). (\u2212 xi) Introduction of the dual variables z for the equality constraints ui = \u03b2 Txi, i = 1, 2., we havemax z \u03b2 (1 + exp). (1 log (1 + exp). (\u2212 yiui))."}, {"heading": "4 Experiment", "text": "In this section, we list the experiments to evaluate the performance of the proposed screening method, that is, Sasvi. We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] that apply the screening method to the screening process, that is, all three methods are \"safe\" in the sense that the discarded features are guaranteed to have 0 coefficients in the real solution."}, {"heading": "5 Conclusion", "text": "In this note, we propose an approach called Sasvi (Safe Screening with Variable Inequalities). Sasvi takes advantage of the variable inequalities that provide the sufficient and necessary optimal conditions for the dual problem. Existing approaches can be interpreted as relaxed versions of the proposed Sasvi, and Sasvi thus offers a much stronger screening rule. Sasvi's monotonous properties are examined on the basis of a safe regulation parameter that can be identified for each feature. In addition, the proposed Sasvi can be easily expanded to solve the generalized sparse linear models."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Proof of Theorem 1", "text": "We start with two technical lemmas.Lemma 3 Let y 6 = 0. If 0 < \u03bb1 \u2264 \u0445 XTy."}, {"heading": "6.2 Proof of Theorem 2", "text": "We prove the four cases one by one. \u2212 If < b > > > > < a > > < b > q, a > | xj, a > xj, a > xj, a >. \u2212 30) are not x = \u00b1 xj, we have b = 0 and a 6 = 0, and < xj, a > 2 >. \u2212 22 \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 46,. \u2212 46,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 44,. \u2212 49,. \u2212 44,. \u2212 44,. \u2212 49,. \u2212 44,. \u2212 49,. \u2212 49,. \u2212 44,. \u2212 44,."}], "references": [{"title": "Safe feature elimination in sparse supervised learning", "author": ["Laurent El Ghaoui", "Vivian Viallon", "Tarek Rabbani"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["Robert Tibshirani", "Jacob Bien", "Jerome Friedman", "Trevor Hastie", "Noah Simon", "Jonathan Taylor", "Ryan J. Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Lasso screening rules via dual polytope projection", "author": ["Jie Wang", "Binbin Lin", "Pinghua Gong", "Peter Wonka", "Jieping Ye"], "venue": "Technical report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning sparse representations of high dimensional data on large scale dictionaries", "author": ["Zhen James Xiang", "Hao Xu", "Peter J. Ramadge"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "The strong rules proposed in [2] works very well in practice although they might mistakenly discard active features.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.", "startOffset": 42, "endOffset": 51}, {"referenceID": 2, "context": "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.", "startOffset": 42, "endOffset": 51}, {"referenceID": 3, "context": "To remedy this, safe screening techniques [1, 3, 4], which elliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution, have been proposed.", "startOffset": 42, "endOffset": 51}, {"referenceID": 0, "context": "3 Relationship to Existing Approaches Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "3 Relationship to Existing Approaches Figure 2 compares the Sasvi approach with the SAFE approach [1] and the DPP approach [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "The constraint for \u03b8\u2217 2 used by the SAFE [1] approach is the ball centered at C with radius being the smallest distance from C to the points in the line segment EG.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "The constraint for \u03b8\u2217 2 used by the DPP [3] approach is the ball centered at E with radius BC.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "which is the one used in the DPP approach [3].", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "Therefore, although the authors in [3] motivates the DPP approach from the viewpoint of Euclidean projection, the DPP approach can indeed be treated as generating the constraint for \u03b8\u2217 2 using the variational inequality in Eq.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "We compare the performance of Sasvi with the sequential SAFE rule [1] and the sequential DPP [3] which achieve the state-of-the-art performance for the Lasso problems.", "startOffset": 93, "endOffset": 96}], "year": 2017, "abstractText": "The model selected by sparse learning techniques usually has a few non-zero entries. Safe screening\u2014which eliminates the features that are guaranteed to be absent for a target regularization parameter based on an existing solution\u2014is a technique for improving the computational efficiency while maintaining the same solution. In this note, we propose an approach called Sasvi (Safe screening with variational inequalities) and take LASSO for example in the analysis. Sasvi makes use of the variational inequalities which provide the sufficient and necessary optimality conditions for the dual problem. The existing approaches can be casted as relaxed versions of the proposed Sasvi, and thus Sasvi provides a much stronger screening rule. The monotone properties of Sasvi are studied based on which a sure removal regularization parameter can be identified for each feature. In addition, the proposed Sasvi is readily extended for solving the generalized sparse linear models. Preliminary experimental results are reported.", "creator": "LaTeX with hyperref package"}}}