{"id": "1307.8187", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2013", "title": "Towards Minimax Online Learning with Unknown Time Horizon", "abstract": "We consider online learning when the time horizon is unknown. We apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown-horizon settings, one that assumes the horizon is chosen randomly according to some known distribution, and the other which allows the adversary full control over the horizon. For the random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. And for the adversarial horizon setting, we prove a nontrivial lower bound which shows that the adversary obtains strictly more power than when the horizon is fixed and known. Based on the minimax solution of the random horizon setting, we then propose a new adaptive algorithm which \"pretends\" that the horizon is drawn from a distribution from a special family, but no matter how the actual horizon is chosen, the worst-case regret is of the optimal rate. Furthermore, our algorithm can be generalized in many ways, including handling other unknown information and other online learning settings. Experiments show that our algorithm outperforms many other existing algorithms in an online linear optimization setting.", "histories": [["v1", "Wed, 31 Jul 2013 01:49:50 GMT  (33kb)", "http://arxiv.org/abs/1307.8187v1", null], ["v2", "Sun, 6 Oct 2013 18:49:58 GMT  (47kb)", "http://arxiv.org/abs/1307.8187v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "robert e schapire"], "accepted": true, "id": "1307.8187"}, "pdf": {"name": "1307.8187.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Unknown Time Horizon", "authors": ["Haipeng Luo"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 7.81 87v1 [cs.L"}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Repeated Games", "text": "The learner has access to N actions. In each round t = 1,.., T, (1) the learner chooses a distribution Pt over the actions; (2) the opponent reveals the loss vector Zt = (Zt, 1.,., Zt, N).LS, where Zt, i is the loss for action i for this round, and the loss space LS is a subset of [0, 1] N; (3) the learner suffers losses in this round. We also denote the cumulative loss of t = Pt \u00b7 Zt for this round.Note that the opponent can select the losses in round t with full knowledge of the history P1: t and Z1: t \u2212 1, i.e. all previous decisions of the learner and the opponent. We also denote the cumulative loss to round t for the learner and the actions of Lt = 1 scale."}, {"heading": "3 Minimax Solution for Fixed Horizon", "text": "Although our primary interest in this work is when the horizon is unknown to the learner, we first present some preliminary results on the basis that the horizon is known to both the learner and the opponent, which will later be useful for the unknown horizon case. If we treat the learner as an algorithm that takes the information from the previous rounds as input factors and a distribution Pt = Alg (P1: t \u2212 1, Z1: t \u2212 1) that the learner will play with the optimal solution in this fixed horizon setting, we can be considered a solution of minimax expressioninf Alg sup Z1.: T Reg (LT, MT \u2212 1) Alternatively, we can recursively define a function V (M, r) as: V (M, 0), \u2212 i Mi; V (M, r), min P (N), max. LS (P \u00b7 Z + Z, r \u2212 1), where we define a function."}, {"heading": "4 Playing without Knowing the Horizon", "text": "We now turn to the case where the horizon T is unknown to the learner, which is often more realistic in practice. There are several ways to model this setting: For example, the horizon can be selected in advance according to a fixed distribution, or it can even be chosen by the opponent. We will discuss these two variants separately.1For other loss dreams, it seems difficult to find the Minimax solutions. Nevertheless, we show the relationship of the values of the game among different loss dreams in the supplementary file, see Theorem 9."}, {"heading": "4.1 Random Horizon", "text": "Suppose the horizon T is chosen according to a fixed distribution Q known to both the learner and the opponent. Before the game begins, a random T is drawn, and neither the learner nor the opponent knows the actual value of T. The game ends after T rounds, and the learner aims to minimize the expectation of regret. On the basis of our previous notation, the problem can be formally defined, such as inf Alg sup Z1: \u221e ET \u0445 Q [Reg (LT, MT)], where we assume that the expectation is always finite. Sometimes, we omit the subscript T-Q for reasons of simplicity. For the base vector loss space, we again show the exact Minimax solution, which has a strong connection with the solution for the fixed horizon setting. Theorem 2. If LS = {e1,., eN}, then we omit the subscript T for reasons of simplicity."}, {"heading": "4.2 Adversarial Horizon", "text": "The most hostile setting is the one in which the horizon is completely controlled by the opponent, that is, we let the opponent decide whether to continue or end the game in each round, depending on the current situation. Note, however, that the value of the game increases on the horizon. So, if the opponent can set the horizon and his goal is still to maximize the regret, then the problem would not make sense, because the opponent would clearly choose to play the game forever and would never stop leading to endless regret. A reasonable way to address this problem is to scale the regret by the value of the fixed horizon game V (0, T), so that the scaled regret Reg (LT, MT) / V (0, T) indicates how much worse the regret is compared to the one that is optimal given the horizon. Under this setting, the corresponding minimax expression V = inf."}, {"heading": "5 A New Adaptive Algorithm", "text": "In Theorem 2, we proposed an algorithm that simply takes the conditional expectation of the distributions that we would have played if the horizon had been given. Note, however, that we should ask two questions: What distribution should we use? What can we say about the performance of the algorithm for an arbitrary horizon instead of in expectation? As a first attempt, we assume that we use a uniform distribution over 1..., T0, where T0 is a huge integer. From what we observe in some numerical calculations, Pt = E [PTt] tends to a uniform distribution in this case."}, {"heading": "6 Generalizations", "text": "It turns out that the idea of \"feigning a prior distribution\" is very applicable in online learning. Below, we will discuss a few generalizations, including hedge with general loss space, dealing with other unknown information outside the horizon, and an application for linear online optimization within a ball of two balls."}, {"heading": "6.1 Generalizing the Exponential Weights Algorithm", "text": "The optimal solution for the loss space [0, 1] N is unknown even for setting the fixed horizon. However, the generalization of the weighted majority algorithm of [12], Freund and Schapire [1, 13] have presented an algorithm that uses exponential weights that deal with this general loss space and can reach the O (\u221a T lnN) limit of regret. The algorithm takes the horizon T as a parameter and in round T simply selects Pt, i-Exp (\u2212 Mt \u2212 1, i), where there is p-limit (8 lnN) / T-limit of regret. It turns out that for most (T lnN) / 2. Auer et al. [8] has proposed a way to make this algorithm adaptive by simply setting a time-varying learning rate."}, {"heading": "6.2 First Order Regret Bound", "text": "So far, all the regrets we have discussed are limits with regard to the horizon, which are also referred to as a zero round. Refined limits have been studied in the literature [7]. For example, the first order, which depends on the loss of the best action m \u00b2 at the end of the game, is normally bound by the order O (\u221a m \u00b2 lnN). Again, using the exponential weighting algorithm with a slightly different learning rate \u03b7 = ln (1 + 270 (2 lnN) / m \u00b2), it is possible to show that regrets are at most 2m \u00b2 + lnN. In order to avoid exploiting this information, which is not available in practice, techniques such as the doubling trick or the learning rate varying over time can be applied. Alternatively, we show that the technique of \"pretending the distribution\" can also be applied here. Instead of using a discrete distribution on the horizon, it is now more sensible to use a continuous distribution on the signs of the best weight i = > 1 millimetre."}, {"heading": "6.3 Online Linear Optimization", "text": "All the examples we have shown so far do not have an explicit form for calculating the learner's strategy. However, in this section we will show an explicit algorithm for online linear optimization within a system in which the distribution techniques of the given distribution techniques are of limited application, which also illustrates the generality of our technique. Specifically, we will look at the following repeated game rules adopted by [6] (all standards are adopted by [6] other standards. If we let the learner choose a different vector, then the opponent chooses another vector that chooses another vector. The learner suffers loss xt for this round, and the learner's regret is pronounced after T-rounds."}, {"heading": "A Proof of Theorem 1 and Proposition 1", "text": "We first specify some properties of the function R: Proposition 2: For each vector M of N dimensions and integer r = | i = then, property 4. R (M, r) = a \u2212 R (M1 \u2212 A,., MN \u2212 a), r) for each real number a and r \u2265 0.Property 5. R (M, r) is not decreasing in Mi (M + N = 1,., N \u2212 Property 6. If r > 0, R (M, r) \u2212 R (M, r \u2212 1) \u2264 1 / N.Property 7. If r > 0, andPi = 1N + R (M + egg, r \u2212 1) \u2212 R (M, r) for each i = 1,., then P = (P1,., PN) is a distribution in the simple form., PN) is a distribution in the simple form (N).Production 2. We waive the proof for property 4 and 5, because it is simple."}, {"heading": "B Proof of Theorem 2", "text": "The proof that the definition V-T (M) = E-V (M, T-t + q = q = q = q (T-T) and q-T (T-T) (T-T) (Pi + q (T-T) (Pi + q)) (T-T) (max i-T (N) (N-T) (N-T) (Pi + q (T-T) (T-T) (T-T) (T-T) (T-T) (T-T) (N-T) (T-T) (T-T) (T) (T-T) (T-T) (T-T) (T-T)), starting with the cumulative loss vector M and assuming that both the learner and the opponent are optimal. This is similar to function V in the case of the fixed horizon and again the value of the game infAlg supZ1: t-T (LT, MT)."}, {"heading": "C Proof of Theorem 3", "text": "To prove this, we need to find out what V (0, T) is what V (0, T) is what V (0, T) is what V (0, T) is. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "D Proof of Theorem 5", "text": "s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 t \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 t \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u2212 t t + 1 (0). Claim 1. Let f (x) be a non-increasing non-negative function defined by R +. Then the following inequalities apply for each integer 0 < j \u2264 k + 1jf (x) dx \u2264 k \u00b2 i = jf (i) \u2264 kj \u2212 1 f (x) dxProof of of Theorem 5. After Theorem 4, it suffices for the upper limit V \u00b2 1 (0) and p \u00b2 Tst = 1 qtV \u00b2 t + 1 (0). Let St = sp \u00b2 t \u00b2 t \u2212 t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t = t t = t = t = t = t = t = t = t t t t t t"}, {"heading": "E Proof of Theorem 6", "text": "The proof: We will first show that Reg (LTs, MTs) \u2264 (lnN) \u00b7 E [1\u03b7T | T \u2265 TS + 1]. We will first show that Reg (LTs, MTs) \u2264 (LTs) \u2264 (lnN) \u00b7 lnq = (LT) \u00b7 E (LT = 1). The key point of proof for the non-adaptive version of the exponential weight algorithm is the use of \u03a6Tt as a \"potential\" function, and the change in potential before and after a single round [7]. Specifically, they have shown that the non-adaptive version of the exponential weight algorithm can also be based on this imbalance. The total loss of the learner after Ts rounds isLTs = Ts (Ts)."}, {"heading": "F Proof of Theorem 7", "text": "Proof. The main idea is similar to that of Theorem 6, but the details are much more technical. Let us first define several expressions: St, ollstmt mtdm md = 1 (d \u2212 1) md \u2212 \u2212 \u2212 \u2212 n, Pr [m < mt | m \u2265 mt \u2212 1] = 1St \u2212 mtmt \u2212 1dm md = 1 \u2212 (mt \u2212 1 mt) d \u2212 1, Y mt, N = 1exp (\u2212 mMt \u2212 1, i), sembmt, (1 + 1\u03b7m) lnY mt mt mt, Ut, E [m | m \u2265 mt mt \u2212 1].The proof begins with the following property of the exponential weights algorithm [7]: Pmt \u00b7 Zt \u2264 1 \u2212 e \u2212 m (lnm), (lnY mt mt mt mt mt mt mt mt mt mt mt mt), (lmt) n (1 mt) t, Ut \u00b2). The proof begins with the following property of the exponential weights, the algorithm [7]."}, {"heading": "G Proof of Theorem 8", "text": "Proof: The key property of the minimax algorithm Eq. (6) shown in [6] is the following: xTt \u00b7 wt = Ts = Ts; t = 1E [xTt \u00b7 wt]; t + wt = T \u2265 Tt] \u2264 Tt + 1.Based on this property, the loss of our algorithm after Ts rounds isTs; t = 1E [xTt | T \u2265 t] \u00b7 wt = Ts; t = Pr [T < t + 1 | T \u2265 t] \u2264 Ts; t = 1E [\u03a6Tt \u2212 \u03a6Tt + 1 | T \u2265 t]. Now define Ut = E [\u03a6Tt | T \u2265 t] and qt = Pr [T < t + 1 | T \u2265 t t]. By the fact that fT \u2012 t (t \u2032 Tt) = (1 \u2212 qt) fT \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t \u00b7 t = qt."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We consider online learning when the time horizon is unknown. We apply a<lb>minimax analysis, beginning with the fixed horizon case, and then moving on to<lb>two unknown-horizon settings, one that assumes the horizon is chosen randomly<lb>according to some known distribution, and the other which allows the adversary<lb>full control over the horizon. For the random horizon setting with restricted losses,<lb>we derive a fully optimal minimax algorithm. And for the adversarial horizon set-<lb>ting, we prove a nontrivial lower bound which shows that the adversary obtains<lb>strictly more power than when the horizon is fixed and known. Based on the mini-<lb>max solution of the random horizon setting, we then propose a new adaptive algo-<lb>rithm which \u201cpretends\u201d that the horizon is drawn from a distribution from a special<lb>family, but no matter how the actual horizon is chosen, the worst-case regret is of<lb>the optimal rate. Furthermore, our algorithm can be generalized in many ways,<lb>including handling other unknown information and other online learning settings.<lb>Experiments show that our algorithm outperforms many other existing algorithms<lb>in an online linear optimization setting.", "creator": "LaTeX with hyperref package"}}}