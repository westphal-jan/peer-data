{"id": "1308.1006", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Aug-2013", "title": "Fast Semidifferential-based Submodular Function Optimization", "abstract": "We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization. Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular min- imization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning. We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments.", "histories": [["v1", "Mon, 5 Aug 2013 15:19:48 GMT  (221kb,D)", "http://arxiv.org/abs/1308.1006v1", "This work appeared in Proc. International Conference of Machine Learning (ICML, 2013)"]], "COMMENTS": "This work appeared in Proc. International Conference of Machine Learning (ICML, 2013)", "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["rishabh k iyer", "stefanie jegelka", "jeff a bilmes"], "accepted": true, "id": "1308.1006"}, "pdf": {"name": "1308.1006.pdf", "metadata": {"source": "CRF", "title": "Fast Semidifferential-based Submodular Function Optimization", "authors": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff Bilmes"], "emails": ["rkiyer@u.washington.edu", "stefje@eecs.berkeley.edu", "bilmes@u.washington.edu"], "sections": [{"heading": "1 Introduction", "text": "It is about the question of whether and to what extent it is actually about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is"}, {"heading": "2 Motivation and Background", "text": "In fact, it is so that most of them are able to surpass themselves by going in search of their own identity. (...) Most of them are not able to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are unable to identify themselves. (...) Most of them are able to identify themselves. (...)"}, {"heading": "3 Submodular semi-differentials", "text": "We will first briefly introduce the submodular semi-differentials. In this thesis we assume that the normalized submodule functions (i.e., f (\u2205) = 0). The subdifferential f (Y) of a submodular set function f: 2V (R) for a set Y (Y) for all X V (V) for all X V (V) For a vector x RV and X V we write x (X) = x (j) - in such a case we say that x is a normalized modular function. We will define a subgradient to Y by hY V (V) for a vector x RV and X V (V) for the supergradients f (Y) for the supergradients f (Y)."}, {"heading": "4 The discrete MM framework", "text": "With the above semigradients, we can define a generic MM algorithm = narrowly delimited. In each iteration, the algorithm optimizes a modular approximation formed by the current solution Y. To minimize, we use an upper limit mgY (X) = f (Y) + gY (X) \u2212 gY (Y) \u2265 f (X), (4) and to maximize a lower limit mhY (X) = f (Y) + hY (X) \u2212 hY (Y) \u2212 hY (Y) \u2264 f (X). (5) Both limits are close to the current solution, satisfactory mgY (Y) = mhY (Y) = f (Y). In almost all cases, optimizing the modular approximation is much faster than optimizing the original cost function f. Algorithm 1 shows our discrete MM scheme for maximization (MMax) [and minimization (MMin)]."}, {"heading": "5 Submodular function minimization", "text": "For minimization problems, we use MMin with the supergradients g-X, g-X and g-X. In both unrestricted and restricted environments, this results in a number of new approaches to submodular minimization."}, {"heading": "5.1 Unconstrained Submodular Minimization", "text": "We start with unrestricted minimization, where C = 2V in problem 1 = > Each of the three supergradients yields a different variant of algorithm 1, and we will call the resulting algorithms MMin-I, II and III."}, {"heading": "5.2 Constrained submodular minimization", "text": "MMin generalizes more complex constraints than C = 2V in a simple way, and Theorem 5.3 still applies to more general grid or ring family constraints. Beyond grids, MMin applies to all types of constraints C, as long as we have an efficient algorithm that minimizes a non-negative modular cost function over C. In contrast to unlimited submodular minimization, such algorithms are very hard for cardinality limits, independent sets of a matroid, and many other combinatorial constraints such as trees, paths, or cuts. In contrast to unlimited submodular minimization, almost all cases of limited submodular minimization are very hard [44, 21, 12] and allow at most approximate solutions in polynomial time. In the next theorem, an upper limit is given for the approximation factor achieved by MMin-I for non-negative, non-decreasing cost functions."}, {"heading": "If the minimization in Step 4 is done with approximation factor \u03b2, then f(X\u0302) \u2264 \u03b2/(1\u2212 \u03baf )f(X\u2217).", "text": "Before we prove this result, we point out that a similar, slightly looser boundary for sections in [22] was shown by using a weaker idea of curvature. Note that the boundary in Theorem 5.9 is at most n1 + (n \u2212 1) (n \u2212 f) (n = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p = p = p = p = p = p p = p = p = p = p = p p = p p p p = p p = p = p = p p p p = p p p"}, {"heading": "5.3 Experiments", "text": "We will next see that, apart from its theoretical properties, MMin is in practice competitive with more complex algorithms (1). We implement and compare algorithms using Matlab and the SFO toolbox [24]. Unrestricted Minimization We first examine the results in Section 5.1 for the reduction of the mesh of possible minimizers. We measure the size of the new meshes in terms of the basics of processing. Applying MMin-I and II (mesh L +) to the test function of Iwata [10], we observe an average reduction of 99.5% in the mesh. MMin-III (mesh L) achieves only a reduction of about 60%. Average values are used for n between 20 and 120. In addition, we use concave over modular functions. w1 (X) + w2 (V\\ X) with randomly selected vectors w1, w2 in [0, 1] and n."}, {"heading": "6 Submodular maximization", "text": "Just as with minimization, submodular maximization gives us a family of algorithms in which each member is specified by a unique plan of subgradients. We select only subgradients that are vertices of the subdifferential, i.e., each subgradient corresponds to a permutation of V. In each of these decisions, MMax converges quickly. To limit runtime, we assume that we only proceed if we make sufficient progress, i.e., if f (Xt + 1) \u2265 (1 + 1) f (Xt).Lemma 6.1. MMax with X0 = argmaxj f (j) runs in time O (T log1 + 1), where T is the time for maximizing a modular function that is subject to X-C.Proof. If we let X be the optimal solution for observation, then we assume that we have the optimal solution for maximum observation."}, {"heading": "6.1 Unconstrained Maximization", "text": "In the x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "6.2 Constrained Maximization", "text": "In this last section, we analyze subgradients for maximizing according to the constraint X-K. Here, we assume that they are a monotone. (30) This definition could be partial if we arbitrarily arrange all remaining elements. (30) If we use the corresponding subgradients, we obtain a number of approximate results already after one iteration: Lemma 6.6. (30) Let us use the following approximate limit for X1 (1 \u2212 e \u2212 f) if we obtain a number of approximate results already after one iteration. (30) Let us use the following approximate limit for X1."}, {"heading": "6.3 Generality", "text": "The results of the study show that it is a submodular functioning that is able to find an optimal solution that leads to a solution with the same approximation factor within MMax. (38) Lemma 6.9 implies that there is a permutation (and equivalent subgradients) with which MMax finds the optimal solution in the first iteration. Known hardness results [7] imply that these are not obsolete in a polynomial way.We find the optimal solution in the first iteration."}, {"heading": "6.4 Experiments", "text": "We now empirically compare the variants of MMax with different subgradients. As a test function, we use the target of [29], f (X) = \u2211 i-V \u2211 j-X sij \u2212 \u03bb \u2211 i, j-X sij, where \u03bb is a redundancy parameter. This non-monotonous function was used to find the most diverse but relevant subset of objects in a large corpus. We use the target with both synthetic and real data. We generate 10 instances of random similarity matrices {sij} ij and vary between 0.5 and 1. Our real data is the selection problem of the language training data [29] on the TIMIT corpus [11], using the string kernel metric [41] for similarity. We use 20 \u2264 n \u2264 n \u2264 30, so that the exact solution can still be calculated using the algorithm of [14]."}, {"heading": "7 Discussion and Conclusions", "text": "This framework is similar to the class of algorithms for minimizing the difference between submodular functions [37, 17], and can be considered a special case of Microsoft's proximal minimization algorithm for submodular optimization; an alternative framework is based on easing the discrete optimization problem by using continuous expansion (the Lova-sz extension to minimize and multilinear expansion to maximize); relaxations have been applied to some limited [16] and unlimited [2] minimization problems, as well as solving discrete optimization problems by using continuous expansion (the Lova-sz extension to minimize and multilinear expansion to maximize NSX)."}], "references": [{"title": "Categorized bottleneck-minisum path problems on networks", "author": ["I. Averbakh", "O. Berman"], "venue": "Operations Research Letters, 16:291\u2013297,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning with Submodular functions: A convex Optimization Perspective", "author": ["F. Bach"], "venue": "Arxiv,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Interactive graph cuts for optimal boundary and region segmentation of objects in n-d images", "author": ["Y. Boykov", "M.P. Jolly"], "venue": "ICCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "A tight (1/2) linear-time approximation to unconstrained submodular maximization", "author": ["N. Buchbinder", "M. Feldman", "J. Naor", "R. Schwartz"], "venue": "FOCS,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Submodular set functions, matroids and the greedy algorithm: tight worstcase bounds and some generalizations of the Rado-Edmonds theorem", "author": ["M. Conforti", "G. Cornuejols"], "venue": "Discrete Applied Mathematics, 7(3):251\u2013274,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1984}, {"title": "Minimizing sparse high-order energies by submodular vertex-cover", "author": ["A. Delong", "O. Veksler", "A. Osokin", "Y. Boykov"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A threshold of ln n for approximating set cover", "author": ["U. Feige"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V. Mirrokni", "J. Vondr\u00e1k"], "venue": "SIAM J. COMPUT., 40(4):1133\u20131155,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Submodular functions and optimization, volume 58", "author": ["S. Fujishige"], "venue": "Elsevier Science,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "A submodular function minimization algorithm based on the minimum-norm base", "author": ["S. Fujishige", "S. Isotani"], "venue": "Pacific Journal of Optimization, 7:3\u201317,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Timit, acoustic-phonetic continuous speech corpus", "author": ["J. Garofolo", "L. Fisher Lamel", "J. W", "Fiscus", "D. Pallet", "N. Dahlgren"], "venue": "In DARPA,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Approximability of combinatorial problems with multi-agent submodular cost functions", "author": ["G. Goel", "C. Karande", "P. Tripathi", "L. Wang"], "venue": "FOCS,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Approximating submodular functions everywhere", "author": ["M.X. Goemans", "N.J.A. Harvey", "S. Iwata", "V. Mirrokni"], "venue": "SODA, pages 535\u2013544,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The maximization of submodular functions: Old and new proofs for the correctness of the dichotomy algorithm", "author": ["B. Goldengorin", "G.A. Tijssen", "M. Tso"], "venue": "University of Groningen,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "A tutorial on MM algorithms", "author": ["D.R. Hunter", "K. Lange"], "venue": "The American Statistician,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Submodular function minimization under covering constraints", "author": ["S. Iwata", "K. Nagano"], "venue": "In FOCS, pages 671\u2013680. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "UAI,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "The submodular Bregman and Lov\u00e1sz-Bregman divergences with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Mirror descent like algorithms for submodular optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "NIPS Workshop on Discrete Optimization in Machine Learning (DISCML),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Combinatorial Problems with submodular coupling in machine learning and computer vision", "author": ["S. Jegelka"], "venue": "PhD thesis, ETH Zurich,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximation bounds for inference using cooperative cuts", "author": ["S. Jegelka", "J.A. Bilmes"], "venue": "ICML,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["S. Jegelka", "J.A. Bilmes"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "On fast approximate submodular minimization", "author": ["S. Jegelka", "H. Lin", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "SFO: A toolbox for submodular function optimization", "author": ["A. Krause"], "venue": "JMLR, 11:1141\u20131144,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "JMLR, 9:235\u2013284,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "arXiv preprint arXiv:1207.6083,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-monotone submodular maximization under matroid and knapsack constraints", "author": ["J. Lee", "V.S. Mirrokni", "V. Nagarajan", "M. Sviridenko"], "venue": "STOC, pages 323\u2013332. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodular maximization over multiple matroids via generalized exchange properties", "author": ["Jon Lee", "Maxim Sviridenko", "Jan Vondr\u00e1k"], "venue": "In APPROX,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "How to select a good training-data subset for transcription: Submodular active selection for sequences", "author": ["H. Lin", "J. Bilmes"], "venue": "Interspeech,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["H. Lin", "J. Bilmes"], "venue": "NAACL,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "ACL,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal selection of limited vocabulary speech corpora", "author": ["H. Lin", "J. Bilmes"], "venue": "Interspeech,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodular function minimization", "author": ["S Thomas McCormick"], "venue": "Discrete Optimization, 12:321\u2013391,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "The EM algorithm and extensions", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": "New York,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Size-constrained submodular minimization through minimum norm base", "author": ["K. Nagano", "Y. Kawahara", "K. Aihara"], "venue": "ICML,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Minimum average cost clustering", "author": ["K. Nagano", "Y. Kawahara", "S. Iwata"], "venue": "NIPS,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "A submodular-supermodular procedure with applications to discriminative structure learning", "author": ["M. Narasimhan", "J. Bilmes"], "venue": "UAI,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Q-clustering", "author": ["M. Narasimhan", "N. Jojic", "J. Bilmes"], "venue": "NIPS, 18:979,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014i", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming, 14(1):265\u2013294,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1978}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J.B. Orlin"], "venue": "Mathematical Programming, 118(2):237\u2013251,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient computation of gapped substring kernels on large alphabets", "author": ["J. Rousu", "J. Shawe-Taylor"], "venue": "Journal of Machine Learning Research, 6(2):1323,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient minimization of decomposable submodular functions", "author": ["P. Stobbe", "A. Krause"], "venue": "NIPS,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "A note on maximizing a submodular set function subject to a knapsack constraint", "author": ["M. Sviridenko"], "venue": "Operations Research Letters, 32(1):41\u201343,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Z. Svitkina", "L. Fleischer"], "venue": "FOCS, pages 697\u2013706,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Minimum-energy broadcasting in static ad hoc wireless networks", "author": ["P.-J. Wan", "G. Calinescu", "X.-Y. Li", "O. Frieder"], "venue": "Wireless Networks, 8:607\u2013617,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2002}, {"title": "The concave-convex procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "NIPS,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 8, "context": "A function f : 2 \u2192 R is said to be submodular [9] if for all subsets S, T \u2286 V , it holds that f(S) + f(T ) \u2265 f(S \u222a T ) + f(S \u2229 T ).", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "While it has long been known [9] that submodular functions have tight subdifferentials, our results rely on a recently discovered property [18, 22] showing that submodular functions also have superdifferentials.", "startOffset": 29, "endOffset": 32}, {"referenceID": 17, "context": "While it has long been known [9] that submodular functions have tight subdifferentials, our results rely on a recently discovered property [18, 22] showing that submodular functions also have superdifferentials.", "startOffset": 139, "endOffset": 147}, {"referenceID": 21, "context": "While it has long been known [9] that submodular functions have tight subdifferentials, our results rely on a recently discovered property [18, 22] showing that submodular functions also have superdifferentials.", "startOffset": 139, "endOffset": 147}, {"referenceID": 2, "context": "MAP inference/Image segmentation: Markov Random Fields with pairwise attractive potentials are important in computer vision, where MAP inference is identical to unconstrained submodular minimization solved via minimum cut [3].", "startOffset": 222, "endOffset": 225}, {"referenceID": 21, "context": "A richer higher-order model can be induced for which MAP inference corresponds to Problem 1 where V is a set of edges in a graph, and C is a set of cuts in this graph \u2014 this was shown to significantly improve many image segmentation results [22].", "startOffset": 241, "endOffset": 245}, {"referenceID": 5, "context": "Moreover, [6] efficiently solve MAP inference in a sparse higher-order graphical model by restating the problem as a submodular vertex cover, i.", "startOffset": 10, "endOffset": 13}, {"referenceID": 37, "context": "Clustering: Variants of submodular minimization have been successfully applied to clustering problems [38, 36].", "startOffset": 102, "endOffset": 110}, {"referenceID": 35, "context": "Clustering: Variants of submodular minimization have been successfully applied to clustering problems [38, 36].", "startOffset": 102, "endOffset": 110}, {"referenceID": 31, "context": "Limited Vocabulary Speech Corpora: The problem of finding a maximum size speech corpus with bounded vocabulary [32] can be posed as submodular function minimization subject to a size constraint.", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "Alternatively, cardinality can be treated as a penalty, reducing the problem to unconstrained submodular minimization [23].", "startOffset": 118, "endOffset": 122}, {"referenceID": 43, "context": "Size constraints: The densest k-subgraph and size-constrained graph cut problems correspond to submodular minimization with cardinality constraints, problems that are very hard [44].", "startOffset": 177, "endOffset": 181}, {"referenceID": 43, "context": "in [44, 35].", "startOffset": 3, "endOffset": 11}, {"referenceID": 34, "context": "in [44, 35].", "startOffset": 3, "endOffset": 11}, {"referenceID": 44, "context": ", a spanning tree) minimizing a submodular cost function [45].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Such economies of scale, or \u201cright of usage\u201d properties are captured in the \u201cCategorized Bottleneck Path Problem\u201d \u2013 a shortest path problem with submodular costs [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 24, "context": "Sensor placement [25], document summarization [31] and speech data subset selection [29], for example, are instances of submodular maximization.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "Sensor placement [25], document summarization [31] and speech data subset selection [29], for example, are instances of submodular maximization.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "Sensor placement [25], document summarization [31] and speech data subset selection [29], for example, are instances of submodular maximization.", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "Determinantal Point Processes: The Determinantal Point Processes (DPPs) which have found numerous applications in machine learning [26] are known to be log-submodular distributions.", "startOffset": 131, "endOffset": 135}, {"referenceID": 32, "context": "Among the several algorithms [33] for the unconstrained variant of Problem 1, where C = 2 , the best complexity to date is O(n\u03b3 + n) [40] (\u03b3 is the cost of evaluating f).", "startOffset": 29, "endOffset": 33}, {"referenceID": 39, "context": "Among the several algorithms [33] for the unconstrained variant of Problem 1, where C = 2 , the best complexity to date is O(n\u03b3 + n) [40] (\u03b3 is the cost of evaluating f).", "startOffset": 133, "endOffset": 137}, {"referenceID": 41, "context": "This has motivated studies on faster, possibly special case or approximate, methods [42, 23].", "startOffset": 84, "endOffset": 92}, {"referenceID": 22, "context": "This has motivated studies on faster, possibly special case or approximate, methods [42, 23].", "startOffset": 84, "endOffset": 92}, {"referenceID": 43, "context": "Approximation algorithms for these problems with various techniques have been studied in [44, 16, 12, 21].", "startOffset": 89, "endOffset": 105}, {"referenceID": 15, "context": "Approximation algorithms for these problems with various techniques have been studied in [44, 16, 12, 21].", "startOffset": 89, "endOffset": 105}, {"referenceID": 11, "context": "Approximation algorithms for these problems with various techniques have been studied in [44, 16, 12, 21].", "startOffset": 89, "endOffset": 105}, {"referenceID": 20, "context": "Approximation algorithms for these problems with various techniques have been studied in [44, 16, 12, 21].", "startOffset": 89, "endOffset": 105}, {"referenceID": 38, "context": "Most such problems, however, admit constant-factor approximations, which are attained via very simple combinatorial algorithms [39, 4].", "startOffset": 127, "endOffset": 134}, {"referenceID": 3, "context": "Most such problems, however, admit constant-factor approximations, which are attained via very simple combinatorial algorithms [39, 4].", "startOffset": 127, "endOffset": 134}, {"referenceID": 14, "context": "Majorization-minimization (MM) algorithms are known to be useful in machine learning [15].", "startOffset": 85, "endOffset": 89}, {"referenceID": 33, "context": "Notable examples include the EM algorithm [34] and the convex-concave procedure [46].", "startOffset": 42, "endOffset": 46}, {"referenceID": 45, "context": "Notable examples include the EM algorithm [34] and the convex-concave procedure [46].", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "Discrete instances have been used to minimize the difference between submodular functions [37, 17], but these algorithms generally lack theoretical guarantees.", "startOffset": 90, "endOffset": 98}, {"referenceID": 16, "context": "Discrete instances have been used to minimize the difference between submodular functions [37, 17], but these algorithms generally lack theoretical guarantees.", "startOffset": 90, "endOffset": 98}, {"referenceID": 8, "context": "The subdifferential \u2202f (Y ) of a submodular set function f : 2 \u2192 R for a set Y \u2286 V is defined [9] analogously to the subdifferential of a continuous convex function: \u2202f (Y ) = {y \u2208 R : (1) f(X)\u2212 y(X) \u2265 f(Y )\u2212 y(Y ) for all X \u2286 V } For a vector x \u2208 R and X \u2286 V , we write x(X) = \u2211 j\u2208X x(j) \u2014 in such case, we say that x is a normalized modular function.", "startOffset": 94, "endOffset": 97}, {"referenceID": 21, "context": "Surprisingly, we can also define superdifferentials \u2202 (Y ) of a submodular function [22, 18] at Y : \u2202 (Y ) = {y \u2208 R : (3) f(X)\u2212 y(X) \u2264 f(Y )\u2212 y(Y ); for all X \u2286 V } We denote a generic supergradient at Y by gY .", "startOffset": 84, "endOffset": 92}, {"referenceID": 17, "context": "Surprisingly, we can also define superdifferentials \u2202 (Y ) of a submodular function [22, 18] at Y : \u2202 (Y ) = {y \u2208 R : (3) f(X)\u2212 y(X) \u2264 f(Y )\u2212 y(Y ); for all X \u2286 V } We denote a generic supergradient at Y by gY .", "startOffset": 84, "endOffset": 92}, {"referenceID": 17, "context": "We define three special supergradients \u011dY (\u201cgrow\u201d), \u01e7Y (\u201cshrink\u201d) and \u1e21Y as follows [18]: \u011dY (j) = f(j | V \\ {j}) \u011dY (j) = f(j | Y ) \u01e7Y (j) = f(j | Y \\ {j}) \u01e7Y (j) = f(j | \u2205) \u1e21Y (j) = f(j | V \\ {j}) } {{ } \u1e21Y (j) = f(j | \u2205) } {{ } for j \u2208 Y for j / \u2208 Y.", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "In addition, it is known that for relaxed instances of our problems, subgradient descent methods can suffer from slow convergence [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 22, "context": "MMin-I is very similar to the algorithms proposed in [23].", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "[9] Let L\u2217 be the lattice of the global minimizers of a submodular function f .", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "1 has been used to prune down the search space of the minimum norm point algorithm from the power set of V to a smaller lattice [2, 10].", "startOffset": 128, "endOffset": 135}, {"referenceID": 9, "context": "1 has been used to prune down the search space of the minimum norm point algorithm from the power set of V to a smaller lattice [2, 10].", "startOffset": 128, "endOffset": 135}, {"referenceID": 22, "context": "3 generalizes part of Lemma 3 in [23].", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "In consequence, a local minimum of a submodular function can be obtained in O(n), a fact that is of independent interest and that does not hold for local maximizers [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 8, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 16, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 13, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 13, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 9, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 15, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 3, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 12, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 1, "context": "Specifically, let w1 = [3, 9, 17, 14, 14, 10, 16, 4, 13, 2] and w2 = [\u22129, 4, 6,\u22121, 10,\u22124,\u22126,\u22121, 2,\u22128].", "startOffset": 23, "endOffset": 59}, {"referenceID": 0, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 32, "endOffset": 45}, {"referenceID": 5, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 32, "endOffset": 45}, {"referenceID": 6, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 32, "endOffset": 45}, {"referenceID": 9, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 54, "endOffset": 73}, {"referenceID": 3, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 54, "endOffset": 73}, {"referenceID": 5, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 54, "endOffset": 73}, {"referenceID": 6, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 54, "endOffset": 73}, {"referenceID": 7, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 54, "endOffset": 73}, {"referenceID": 9, "context": "Then we obtain L defined by A = [1, 6, 7, 10] and B = [1, 4, 6, 7, 8, 10].", "startOffset": 54, "endOffset": 73}, {"referenceID": 0, "context": "The tightened sublattice contains exactly the minimizer: A+ = B+ = X \u2217 = [1, 6, 7, 8, 10].", "startOffset": 73, "endOffset": 89}, {"referenceID": 5, "context": "The tightened sublattice contains exactly the minimizer: A+ = B+ = X \u2217 = [1, 6, 7, 8, 10].", "startOffset": 73, "endOffset": 89}, {"referenceID": 6, "context": "The tightened sublattice contains exactly the minimizer: A+ = B+ = X \u2217 = [1, 6, 7, 8, 10].", "startOffset": 73, "endOffset": 89}, {"referenceID": 7, "context": "The tightened sublattice contains exactly the minimizer: A+ = B+ = X \u2217 = [1, 6, 7, 8, 10].", "startOffset": 73, "endOffset": 89}, {"referenceID": 9, "context": "The tightened sublattice contains exactly the minimizer: A+ = B+ = X \u2217 = [1, 6, 7, 8, 10].", "startOffset": 73, "endOffset": 89}, {"referenceID": 43, "context": "As opposed to unconstrained submodular minimization, almost all cases of constrained submodular minimization are very hard [44, 21, 12], and admit at most approximate solutions in polynomial time.", "startOffset": 123, "endOffset": 135}, {"referenceID": 20, "context": "As opposed to unconstrained submodular minimization, almost all cases of constrained submodular minimization are very hard [44, 21, 12], and admit at most approximate solutions in polynomial time.", "startOffset": 123, "endOffset": 135}, {"referenceID": 11, "context": "As opposed to unconstrained submodular minimization, almost all cases of constrained submodular minimization are very hard [44, 21, 12], and admit at most approximate solutions in polynomial time.", "startOffset": 123, "endOffset": 135}, {"referenceID": 4, "context": "An important ingredient in the bound is the curvature [5] of a monotone submodular function f , defined as \u03baf = 1\u2212minj\u2208V f(j | V \\j) / f(j) (8) Theorem 5.", "startOffset": 54, "endOffset": 57}, {"referenceID": 21, "context": "Before proving this result, we remark that a similar, slightly looser bound was shown for cuts in [22], by using a weaker notion of curvature.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "9, we use the following result shown in [20]: f(X\u0302) \u2264 g(X \u2217)/f(i) 1 + (1\u2212 \u03baf )(g(X\u2217)/f(i)\u2212 1) f(X\u2217) (9)", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "9 yields a constant approximation factor and refines bounds for constrained minimization that are given in [12, 44].", "startOffset": 107, "endOffset": 115}, {"referenceID": 43, "context": "9 yields a constant approximation factor and refines bounds for constrained minimization that are given in [12, 44].", "startOffset": 107, "endOffset": 115}, {"referenceID": 30, "context": "For example, concave over modular functions were used in [31, 22].", "startOffset": 57, "endOffset": 65}, {"referenceID": 21, "context": "For example, concave over modular functions were used in [31, 22].", "startOffset": 57, "endOffset": 65}, {"referenceID": 0, "context": "These comprise, for instance, functions of the form f(X) = (w(X)), for some a \u2208 [0, 1] and a nonnegative weight vector w, whose curvature is \u03baf \u2248 1\u2212 a(j w(j) w(V ) ) 1\u2212a > 0.", "startOffset": 80, "endOffset": 86}, {"referenceID": 23, "context": "We implement and compare algorithms using Matlab and the SFO toolbox [24].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Applying MMin-I and II (lattice L+) to Iwata\u2019s test function [10], we observe an average reduction of 99.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "In addition, we use concave over modular functions \u221a w1(X) + \u03bbw2(V \\X) with randomly chosen vectors w1, w2 in [0, 1] n and n = 50.", "startOffset": 110, "endOffset": 116}, {"referenceID": 31, "context": "[32, 23] use functions of the form \u221a w1(\u0393(X)) + w2(V \\X), where \u0393(X) is the neighborhood function of a bipartite graph.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[32, 23] use functions of the form \u221a w1(\u0393(X)) + w2(V \\X), where \u0393(X) is the neighborhood function of a bipartite graph.", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "When used as a preprocessing step for the minimum norm point algorithm (MN) [10], this pruned lattice speeds up the MN algorithm accordingly, in particular for the speech data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "For constrained minimization, we compare MMin-I to two methods: a simple algorithm (MU) that minimizes the upper bound g(X) = \u2211 i\u2208X f(i) [12] (this is identical to the first iteration of MMin-I), and a more complex algorithm (EA) that computes an approximation to the submodular polyhedron [13] and in many cases yields a theoretically optimal approximation.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "For constrained minimization, we compare MMin-I to two methods: a simple algorithm (MU) that minimizes the upper bound g(X) = \u2211 i\u2208X f(i) [12] (this is identical to the first iteration of MMin-I), and a more complex algorithm (EA) that computes an approximation to the submodular polyhedron [13] and in many cases yields a theoretically optimal approximation.", "startOffset": 290, "endOffset": 294}, {"referenceID": 12, "context": "We use a very hard cost function [13]", "startOffset": 33, "endOffset": 37}, {"referenceID": 30, "context": "Functions of type (1) and (2) have been used in speech and computer vision [31, 22, 17] and have reduced curvature (\u03baf < 1).", "startOffset": 75, "endOffset": 87}, {"referenceID": 21, "context": "Functions of type (1) and (2) have been used in speech and computer vision [31, 22, 17] and have reduced curvature (\u03baf < 1).", "startOffset": 75, "endOffset": 87}, {"referenceID": 16, "context": "Functions of type (1) and (2) have been used in speech and computer vision [31, 22, 17] and have reduced curvature (\u03baf < 1).", "startOffset": 75, "endOffset": 87}, {"referenceID": 7, "context": "[8] shows that ES(f(S)) \u2265 1 4f(X \u2217).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "We now use a result by [8] showing that if a set X is a local optimum, then f(X) \u2265 13f(X \u2217) if f is a general non-negative submodular set function and f(X) \u2265 1 2f(X \u2217) if f is a symmetric submodular function.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "If the set is an \u03b7-approximate local optimum, we obtain a 13 \u2212 \u03b7 approximation [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "Note that even finding an exact local maximum is hard for submodular functions [8], and therefore it is necessary to resort to an \u03b7-approximate version, which converges to an \u03b7-approximate local maximum.", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "This schedule is equivalent to the deterministic local search (DLS) algorithm by [8], and therefore achieves an approximation factor of 1/3\u2212 \u03b7.", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Likewise, the deterministic bi-directional greedy algorithm by [4] induces a distinct permutation of the ground set.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Given an initial ordering \u03c4 , the bi-directional greedy algorithm by [4] generates a chain of sets.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "The last inequality follows from the approximation factor satisfied by S [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "Like its deterministic variant, the randomized bi-directional greedy algorithm by [4] can be shown to run MMax with a specific subgradient.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Like its deterministic counterpart, the Randomized bi-directional Greedy algorithm (RG) by [4] induces a (random) permutation \u03c3 based on an initial ordering \u03c4 .", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "The last inequality follows from a result in [4].", "startOffset": 45, "endOffset": 48}, {"referenceID": 38, "context": "The last inequality follows from [39, 5].", "startOffset": 33, "endOffset": 40}, {"referenceID": 4, "context": "The last inequality follows from [39, 5].", "startOffset": 33, "endOffset": 40}, {"referenceID": 29, "context": "The following result then follows from [30, 43].", "startOffset": 39, "endOffset": 47}, {"referenceID": 42, "context": "The following result then follows from [30, 43].", "startOffset": 39, "endOffset": 47}, {"referenceID": 26, "context": "In particular, some recent algorithms [27, 28] propose local search based techniques to obtain constant factor approximations for non-monotone submodular maximization under knapsack and matroid constraints.", "startOffset": 38, "endOffset": 46}, {"referenceID": 27, "context": "In particular, some recent algorithms [27, 28] propose local search based techniques to obtain constant factor approximations for non-monotone submodular maximization under knapsack and matroid constraints.", "startOffset": 38, "endOffset": 46}, {"referenceID": 6, "context": "Known hardness results [7] imply that this permutation may not be obtainable in polynomial time.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "To show that this holds for arbitrary X (and correspondingly at every iteration), we use that the submodular subdifferential can be expressed as a direct product between a submodular polyhedron and an anti-submodular polyhedron [9].", "startOffset": 228, "endOffset": 231}, {"referenceID": 8, "context": "Any problem involving an optimization over the sub-differential, can then be expressed as an optimization over a submodular polyhedron (which is a subdifferential at the empty set) and an anti-submodular polyhedron (which is a subdifferential at V ) [9].", "startOffset": 250, "endOffset": 253}, {"referenceID": 28, "context": "As a test function, we use the objective of [29], f(X) = \u2211 i\u2208V \u2211 j\u2208X sij \u2212 \u03bb \u2211 i,j\u2208X sij , where \u03bb is a redundancy parameter.", "startOffset": 44, "endOffset": 48}, {"referenceID": 28, "context": "Our real-world data is the Speech Training data subset selection problem [29] on the TIMIT corpus [11], using the string kernel metric [41] for similarity.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "Our real-world data is the Speech Training data subset selection problem [29] on the TIMIT corpus [11], using the string kernel metric [41] for similarity.", "startOffset": 98, "endOffset": 102}, {"referenceID": 40, "context": "Our real-world data is the Speech Training data subset selection problem [29] on the TIMIT corpus [11], using the string kernel metric [41] for similarity.", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "We use 20 \u2264 n \u2264 30 so that the exact solution can still be computed with the algorithm of [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "RS achieves a 1/4 approximation in expectation [8].", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "Importantly, the MMax variants are extremely fast, about 200-500 times faster than the exact branch and bound technique of [14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "This framework is akin to the class of algorithms for minimizing the difference between submodular functions [37, 17].", "startOffset": 109, "endOffset": 117}, {"referenceID": 16, "context": "This framework is akin to the class of algorithms for minimizing the difference between submodular functions [37, 17].", "startOffset": 109, "endOffset": 117}, {"referenceID": 18, "context": "divergences derived from submodular functions [19].", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "Relaxations have been applied to some constrained [16] and unconstrained [2] minimization problems as well as maximization problems [4].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "Relaxations have been applied to some constrained [16] and unconstrained [2] minimization problems as well as maximization problems [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "Relaxations have been applied to some constrained [16] and unconstrained [2] minimization problems as well as maximization problems [4].", "startOffset": 132, "endOffset": 135}], "year": 2013, "abstractText": "We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (suband super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization. Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular minimization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning. We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments.", "creator": "LaTeX with hyperref package"}}}