{"id": "1308.2218", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Aug-2013", "title": "Coding for Random Projections", "abstract": "The method of random projections has become very popular for large-scale applications in statistical learning, information retrieval, bio-informatics and other applications. Using a well-designed coding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed. In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that uniform quantization outperforms the standard existing influential method (Datar et. al. 2004). Indeed, we argue that in many cases coding with just a small number of bits suffices. Furthermore, we also develop a non-uniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM).", "histories": [["v1", "Fri, 9 Aug 2013 19:50:24 GMT  (192kb)", "http://arxiv.org/abs/1308.2218v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.IT math.IT stat.CO", "authors": ["ping li 0001", "michael mitzenmacher", "anshumali shrivastava"], "accepted": true, "id": "1308.2218"}, "pdf": {"name": "1308.2218.pdf", "metadata": {"source": "CRF", "title": "Coding for Random Projections", "authors": ["Ping Li"], "emails": ["pl314@rci.rutgers.edu", "michaelm@eecs.harvard.edu", "anshu@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 8,22 18"}, {"heading": "1 Introduction", "text": "The Random Projection method is popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25]. We will closely compare our method with the influential previous coding scheme in [8]. Consider two high-dimensional vectors, u, v, RD. The idea is to multiply them by a random normal projection matrix R, RD, k (where k, D)."}, {"heading": "1.1 Uniform Quantization", "text": "We are the first ones we arise with a small number of bits, which are based on a simple, uniform quantization: \"We are the first ones we put into the world.\" \"We are the first ones we put into the world.\" \"We are the first ones we put into the world.\" \"We are the first ones we put into the world.\" \"The second, third, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, sixth, sixth, seventh, seventh, sixth, seventh, sixth, sixth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, sixth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, sixth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, we are the first, we are the first, we are the first"}, {"heading": "1.2 Advantages over the Window-and-Offset Coding Scheme", "text": "[8] suggested the following well-known encoding scheme that uses window and a random offset: h (j) w, q (u) = x (j) + x (w), h (j) w, q (v) = x (j) w (5), where qj \u00b2 is uniform (0, w). [8] showed that the collision probability can be written asPw, q = Pr (h (j) w, q (u) = h (j) w, q (v)))) = x w01 \u221a d 2\u03c6 (t \u221a d) (1 \u2212 t w) dt (6), where d = | u \u2212 v | 2 = 2 (1 \u2212 \u03c1) the Euclidean distance between u and v. The difference between (5) and our proposal (4) is that we do not use the additional random calculation uniformly with q \u00b2 (0, w) (i.e. the offset)."}, {"heading": "1.3 Organization", "text": "In Section 2, we analyze the collision probability for the unified quantization scheme and then compare it with the collision probability of the known previous work [8], which uses an additional random offset. Since the collision probabilities are monotonous functions of similarity \u03c1, we can always estimate \u03c1 from the observed (empirical) collision probabilities. In Section 3, we theoretically compare the deviations of these two schemes and conclude that the random offset step in [8] is not necessary. In Section 4, we develop a 2-bit informal coding scheme and show that its performance is broadly consistent with the performance of the unified quantization scheme (requiring more bits to be stored). Interestingly, for a certain area of similarity \u03c1, we find that only one bit is needed. Therefore, Section 5 is dedicated to comparing the 1-bit scheme with our proposed methods, so that we are interested in the comparison of the 1-bit scheme with our proposed methods. The comparisons show that the 7."}, {"heading": "2 The Collision Probability of Uninform Quantization hw", "text": "To use our encoding scheme hw (4), we must increase the probability of collision Pw = Pr (h) q = q = q = q (u) = h (j) w (v)). From the practitioner's perspective, Pw is a monotonously increasing function of similarity between the two schemes, so it is a suitable encoding scheme. In other words, it does not matter whether Pw has a closed form as long as we can demonstrate its advantage over the alternative [8], whose collision probability is denoted by Pw, q. Note: Pw, q can be expressed in a closed form relative to the standard form."}, {"heading": "4 A 2-Bit Non-Uniform Coding Scheme", "text": "If we quantify the projected data according to the four regions (\u2212 \u221e, \u2212 w), [\u2212 w, 0), [0, w), [w, \u221e), we obtain a 2-bit non-uniform scheme. Theoretically, we can calculate the collision probability, which is called by Pw, 2, which in turn is a monotonically increasing function of similarity. With the projections, we can use the empirical observation of Pw, 2 and we can denounce this estimate by the collision probability, 2, which in turn is a monotonically increasing function of similarity between the two systems. We can denounce the empirical observation of Pw, 2 and we can provide the expressions for Pw, 2 and V ar (2). Theorem 4Pw, 2 = Pr (j) w, 2 = h (j) w, 2 (v)."}, {"heading": "6 An Experimental Study for Training Linear SVM", "text": "We conduct experiments with random projections for education (L2-regulated) linear SVM (e.g., LIBLINEAR [9]) on three high-dimensional datasets: ARCENE, FARM, URL, which are available from the UCI repository. The original source database has about 2.4 million examples (collected in 120 days) in 3231961 dimensions. The ARCENE datasets contain 100 training examples and 100 test examples in 10,000 dimensions. We implement the four coding schemes examined in this paper: hw, q, hw, 2, and h1. Recall hw, q [8] is based on uniform quantization plus a random offset to illustrate exactly how we use the coded data for education."}, {"heading": "7 Future Work", "text": "Linear estimators are extremely useful because they allow a highly efficient implementation of linear classifiers (e.g. linear SVM) and search methods near neighbors using hash tables. For applications that allow non-linear estimators (e.g. non-linear kernel SVM), we can significantly improve linear estimators by solving non-linear MLE equations (maximum probability), and the analysis is presented separately. Our work is to a certain extent inspired by recent work on b-bit minwise hashing [19, 20] which also suggested a coding scheme for low-level hashing and applied it to learning applications where the data is binary and sparse. Our work relates to general data types, as opposed to binary, sparse data. We expect coding methods to be valuable for other variations of random projections, including the number-minute sketch [5] and potential variants [26] of a very sparse improvement in case direction."}, {"heading": "8 Conclusion", "text": "A compact representation (encoding) of the projected data is crucial for efficient transmission, retrieval and energy consumption. We have compared a simple scheme based on uniform quantization with the influential encoding scheme using a random offset window [8]; our scheme appears operationally simpler, more precise, less sensitive to parameters (e.g. widow / garbage width w) and uses fewer bits. In addition, we are developing a 2-bit uneven encoding scheme that works in a similar way to a uniform quantification. Our experiments with linear SVM on several real high-dimensional datasets confirm the effectiveness of the two proposed encoding schemes. Based on theoretical analysis and empirical evidence, we recommend the use of the 2-bit uneven encoding scheme with the first garbage width = 0.7: 1, especially if the target similarity is high."}, {"heading": "A Proof of Lemma 1", "text": "The community density function (x, y) is f (x, y) \u2212 f (x, y) \u2212 f (x, y) \u2212 f (x, y) \u2212 f (x, y) \u2212 f (x, y) \u2212 f (x, y) \u2212 f (x, y) \u2212 f (x, y) \u2212 f (x) \u2212 f (x) \u2212 f (x) = f (x) \u2212 f (x) = 1 (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x, y) \u2212 f (x) \u2212 f (x) \u2212 f (x) = 1 (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x, y) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) = 1 (x) \u2212 f (x) \u2212 f (x) = 1 (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) \u2212 f (x) x (x) \u2212 f (x) x (x) \u2212 f (x (x) \u2212 f (x (x) \u2212 f (x) x (x (x) x (x (x) x (x (x (x) x (x) t (x (x (x) x (x (x) x (x) x (x (x) t) p x (x (x (x (x) p) p) p) p (x (x (x (x (x) p) p) p (x (x (x (x) p (x) p) p (x (x) p (x (x) p (x) p (x (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p) p (x) p (x (x) p (x) p) p) p (x (x) p) p (x (x) p (x"}, {"heading": "B Proof of Theorem 2", "text": "From the collision probability, Pw, q = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2p = 2"}, {"heading": "C Proof of Theorem 3", "text": "This proof is similar to the proof of the theory 2. To evaluate the asymptotic variance, we must use the Pw variance (1 + 2) (2 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (1 + 2) (2 + 2) (2 + 2) (1 \u2212 2) (1 \u2212 2) (1 \u2212 w w w w 2 (1 \u2212 w2) (1 \u2212 w2) (1 \u2212 w2) (1 \u2212 ww w w) (1 \u2212 w2) (1 \u2212 w w w w w) (1 \u2212 w2) (1 \u2212 w2) (1 \u2212 w2 w1 \u2212 w2) (1 \u2212 w2 w2) (1 \u2212 w2) (1 \u2212 w w w w w) (1 \u2212 w2) (1 \u2212 w w w w w w w) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2) (1 \u2212 w w w w w w) (1 \u2212 w2) (1 \u2212 w w w w w w w) (1 \u2212 w2 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2 \u2212 w2) (1 \u2212 w w w w (1 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w w w w w w (1 \u2212 w2) (1 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2) (1 \u2212 w w w w w (1 \u2212 w2) (1 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2 \u2212 w2 \u2212 w2 (1 \u2212 w w w w w) (1 \u2212 w2 \u2212 w2) (1 \u2212 w2) (1 \u2212 w"}, {"heading": "D Proof of Theorem 4", "text": "Pw, 2 = Pr, 2 = Pr, 2 \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, \u2212 Pr, Pr, \u2212 Pr, (Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, - Pr, Pr, Pr, Pr, Pr, - Pr, Pr, Pr, Pr, Pr, - Pr, Pr, Pr, Pr, Pr, Pr, - Pr, Pr, Pr, Pr, Pr, - Pr, Pr, Pr, Pr, - Pr, Pr, Pr, - Pr, Pr, Pr, - Pr, Pr, Pr, - Pr, Pr, - Pr, Pr, - Pr, Pr, - Pr, Pr, - Pr, - Pr, - Pr, - Pr, - Pr, Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr, - Pr"}], "references": [{"title": "Random projection in dimensionality reduction: Applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In KDD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Finding motifs using random projections", "author": ["Jeremy Buhler", "Martin Tompa"], "venue": "Journal of Computational Biology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S. Charikar"], "venue": "In STOC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "An improved data stream summary: the count-min sketch and its applications", "author": ["Graham Cormode", "S. Muthukrishnan"], "venue": "Journal of Algorithm,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In FOCS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Experiments with random projection", "author": ["Sanjoy Dasgupta"], "venue": "In UAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S. Mirrokn"], "venue": "In SCG,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Experiments with random projections for machine learning", "author": ["Dmitriy Fradkin", "David Madigan"], "venue": "In KDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Learning the structure of manifolds using random projections", "author": ["Yoav Freund", "Sanjoy Dasgupta", "Mayank Kabra", "Nakul Verma"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "An algorithm for finding nearest neighbors", "author": ["Jerome H. Friedman", "F. Baskett", "L. Shustek"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1975}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Michel X. Goemans", "David P. Williamson"], "venue": "Journal of ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In STOC,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In KDD,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Extensions of Lipschitz mapping into Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}, {"title": "Very sparse stable random projections for dimension reduction in l\u03b1 (0 < \u03b1 \u2264 2) norm", "author": ["Ping Li"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Improving random projections using marginal information", "author": ["Ping Li", "Trevor J. Hastie", "Kenneth W. Church"], "venue": "In COLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "b-bit minwise hashing", "author": ["Ping Li", "Arnd Christian K\u00f6nig"], "venue": "In WWW,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "One permutation hashing", "author": ["Ping Li", "Art B Owen", "Cun-Hui Zhang"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Sign stable projections, sign cauchy projections and chi-square kernels", "author": ["Ping Li", "Gennady Samorodnitsky", "John Hopcroft"], "venue": "Technical report,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["Christos H. Papadimitriou", "Prabhakar Raghavan", "Hisao Tamaki", "Santosh Vempala"], "venue": "In PODS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "The Random Projection Method", "author": ["Santosh Vempala"], "venue": "American Mathematical Society, Providence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Efficient nonnegative matrix factorization with random projections", "author": ["Fei Wang", "Ping Li"], "venue": "In SDM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "We demonstrate that uniform quantization outperforms the standard existing influential method [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 20, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 4, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 0, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 1, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 8, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 16, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 9, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 22, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 5, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 14, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 23, "context": "1 Introduction The method of random projections has become popular for large-scale machine learning applications such as classification, regression, matrix factorization, singular value decomposition, near neighbor search, bioinformatics, and more [22, 6, 1, 3, 10, 18, 11, 24, 7, 16, 25].", "startOffset": 248, "endOffset": 288}, {"referenceID": 13, "context": "In this paper, we study a number of simple and effective schemes for coding the projected data, with the focus on similarity estimation and training linear classifiers [15, 23, 9, 2].", "startOffset": 168, "endOffset": 182}, {"referenceID": 21, "context": "In this paper, we study a number of simple and effective schemes for coding the projected data, with the focus on similarity estimation and training linear classifiers [15, 23, 9, 2].", "startOffset": 168, "endOffset": 182}, {"referenceID": 7, "context": "In this paper, we study a number of simple and effective schemes for coding the projected data, with the focus on similarity estimation and training linear classifiers [15, 23, 9, 2].", "startOffset": 168, "endOffset": 182}, {"referenceID": 6, "context": "We will closely compare our method with the influential prior coding scheme in [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 16, "context": "This assumption is reasonable in practice [18].", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "This trick was also recently used for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 18, "context": "This trick was also recently used for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 102, "endOffset": 110}, {"referenceID": 10, "context": "Near neighbor search is a basic problem studied since the early days of modern computing [12] with applications throughout computer science.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "The use of coded projection data for near neighbor search is closely related to locality sensitive hashing (LSH) [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "Compared to [8], our proposed coding scheme has better performance for near neighbor search; the analysis will be reported in a separate technical report.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "2 Advantages over the Window-and-Offset Coding Scheme [8] proposed the following well-known coding scheme, which uses windows and a random offset: h w,q(u) = \u230a xj + qj w \u230b", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "[8] showed that the collision probability can be written as Pw,q =Pr ( h w,q(u) = h (j) w,q(v) )", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In summary, uniform quantization is simpler, more accurate, and uses fewer bits than the influential prior work [8] which uses the window with the random offset.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "3 Organization In Section 2, we analyze the collision probability for the uniform quantization scheme and then compare it with the collision probability of the well-known prior work [8] which uses an additional random offset.", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "In Section 3, we theoretically compare the estimation variances of these two schemes and conclude that the random offset step in [8] is not needed.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "In other words, it does not matter whether Pw has a closed-form expression, as long as we can demonstrate its advantage over the alternative [8], whose collision probability is denoted by Pw,q.", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Our proposed scheme (hw) has smaller collision probabilities than the existing scheme [8] (hw,q), especially when w > 2.", "startOffset": 86, "endOffset": 89}, {"referenceID": 11, "context": ", where V1 = \u03c0 (1\u2212 \u03c1)P1(1\u2212 P1) (20) This collision probability is widely known [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "The work of [4] also popularized the use 1-bit coding.", "startOffset": 12, "endOffset": 15}, {"referenceID": 16, "context": "The variance was analyzed and compared with a maximum likelihood estimator in [18].", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": ", LIBLINEAR [9]) on three high-dimensional datasets: ARCENE, FARM, URL, which are available from the UCI repository.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Recall hw,q [8] was based on uniform quantization plus a random offset, with bin width w.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "75) \u21d2 [1 0 0 0], x \u2208 [\u22120.", "startOffset": 6, "endOffset": 15}, {"referenceID": 0, "context": "75 0) \u21d2 [0 1 0 0], x \u2208 [0 0.", "startOffset": 8, "endOffset": 17}, {"referenceID": 0, "context": "75) \u21d2 [0 0 1 0], x \u2208 [0.", "startOffset": 6, "endOffset": 15}, {"referenceID": 0, "context": "75 \u221e) \u21d2 [0 0 0 1] This way, with k projections, for each feature vector, we obtain a new vector of length 4k with exactly k 1\u2019s.", "startOffset": 8, "endOffset": 17}, {"referenceID": 17, "context": "Recently, this strategy was adopted for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 18, "context": "Recently, this strategy was adopted for linear learning with binary data based on b-bit minwise hashing [19, 20].", "startOffset": 104, "endOffset": 112}, {"referenceID": 6, "context": "Recall hw,q [8] was based on uniform quantization plus a random offset, with bin length w.", "startOffset": 12, "endOffset": 15}, {"referenceID": 17, "context": "Our work is, to an extent, inspired by the recent work on b-bit minwise hashing [19, 20], which also proposed a coding scheme for minwise hashing and applied it to learning applications where the data are binary and sparse.", "startOffset": 80, "endOffset": 88}, {"referenceID": 18, "context": "Our work is, to an extent, inspired by the recent work on b-bit minwise hashing [19, 20], which also proposed a coding scheme for minwise hashing and applied it to learning applications where the data are binary and sparse.", "startOffset": 80, "endOffset": 88}, {"referenceID": 3, "context": "We expect coding methods will also prove valuable for other variations of random projections, including the count-min sketch [5] and related variants [26] and very sparse random projections [17].", "startOffset": 125, "endOffset": 128}, {"referenceID": 24, "context": "We expect coding methods will also prove valuable for other variations of random projections, including the count-min sketch [5] and related variants [26] and very sparse random projections [17].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "We expect coding methods will also prove valuable for other variations of random projections, including the count-min sketch [5] and related variants [26] and very sparse random projections [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 19, "context": "Another potentially interesting future direction is to develop refined coding schemes for improving sign stable projections [21] (which are useful for \u03c72 similarity estimation, a popular similarity measure in computer vision and NLP).", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "We have compared a simple scheme based on uniform quantization with the influential coding scheme using windows with a random offset [8]; our scheme appears operationally simpler, more accurate, not as sensitive to parameters (e.", "startOffset": 133, "endOffset": 136}], "year": 2013, "abstractText": "The method of random projections has become very popular for large-scale applications in statistical learning, information retrieval, bio-informatics and other applications. Using a well-designed coding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed. In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that uniform quantization outperforms the standard existing influential method [8]. Indeed, we argue that in many cases coding with just a small number of bits suffices. Furthermore, we also develop a nonuniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM).", "creator": "LaTeX with hyperref package"}}}