{"id": "1308.2853", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Aug-2013", "title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity", "abstract": "Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of \"higher order\" expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition.", "histories": [["v1", "Tue, 13 Aug 2013 13:16:10 GMT  (162kb)", "http://arxiv.org/abs/1308.2853v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR math.NA math.ST stat.ML stat.TH", "authors": ["anima anandkumar", "daniel j hsu", "majid janzamin", "sham m kakade"], "accepted": true, "id": "1308.2853"}, "pdf": {"name": "1308.2853.pdf", "metadata": {"source": "CRF", "title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity", "authors": ["Animashree Anandkumar", "Daniel Hsu", "Majid Janzamin"], "emails": ["dahsu@microsoft.com,", "skakade@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 8.28 53v1 [cs.LG] 1 3A ugKeywords: Overcomplete representations, theme models, general identification, decomposition of the tensor."}, {"heading": "1 Introduction", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcehnlrVo"}, {"heading": "1.1 Summary of results", "text": "In this paper, we set conditions for generic 1 model Identifiability of supercomplete theme models in view of observable moments of a particular order (i.e., we have a certain number of words in each doc-1A model is almost certain to be generically identifiable if all parameters in the parameter space are identifiable. Let's refer to Definition 1 for further debate.ument) We introduce the concept of theme persistence and analyze its impact on identifiability. We establish identifiability in the presence of a novel combinatorial object called perfect n-gram matching in the two-part theme-to-word graphics. Finally, we prove that random structured theme models meet these criteria and are thus identifiable in the complete regime. We first introduce the n-persistent word model, in which the parameters n-persistent theme model is determined by successive words."}, {"heading": "1.2 Overview of Techniques", "text": "The question we have asked ourselves is whether it is even possible to have a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word for a complete word."}, {"heading": "1.3 Related works", "text": "This year, the time has come for such an event to occur only once."}, {"heading": "2 Model", "text": "The notation: The set {1, 2,., n} stands for [n]: = [1, 2,.., n]. The set X = {1,..., p} denotes all ordered n-tuples generated from X. The cardinality of a set S is denoted by | S |. For any vector u (or matrix U), the column is denoted by Supp (u), and the number of 0-tuples is denoted by the number of non-zero entries of u, i.e., for a vector u-Rq, the diag (u) is the diagonal matrix with the vector u-Rq \u00b7 \u00b7 q on its diagonal. The slit space of a matrix A is denoted by Col (A)."}, {"heading": "2.1 Persistent topic model", "text": "In this section, the n-persistent theme model is introduced and this imposes an additional constraint, which is defined as a theme for the distribution of words at the level of popularity. [2] The n-persistent model is reduced to the theme of word admixture when the observed variables xl are generated by vectors a1. [3] This collection of vectors is related to the population structure or the theme-word matrix. [4] For example, ai is the conditional distribution of words that are subject i. [5] The latent variable h is an authoritative random vector h: = [6]."}, {"heading": "3 Sufficient Conditions for Generic Identifiability", "text": "In this section, the identity result is provided for the n-persistent theme model with access to (2n) -th order observed moment. First, sufficient deterministic conditions are provided on the population structure A for identification. Second, deterministic analysis specializes in a random structured model in theorem. Instead, we consider the concept of identifiability as generic identifiability. Definition 1 (Generic Identifiability) means that the population structure A is unambiguous up to permutation and scaling for all A-Rp-q. Instead, we consider a more relaxed notion of identifiability known as generic identifiability. Definition 1 (Generic Identifiability) We refer to a matrix A-Rp-q as a generic pattern."}, {"heading": "3.1 Deterministic conditions for generic identifiability", "text": "In this section, we will consider a fixed sparsity pattern with respect to the population structure A and establish the generic identifiability, unless zero entries of A are drawn from any sequential distribution. (Prior to providing the main result, a generalized notion of (perfect) matching for bipartite graphs is defined. (Finally, this condition is imposed on the bipartite graph Y between them by G (Y, X; E), which encodes the sparsity pattern of the population structure. (Considering the bipartite graph G (Y, X; E), bipartite matching for bipartite graphs is set between them by G (Y, X; E) with two dissociated nodes Y and an edge set E between them by G (Y, X; E). Given the bi-adjacency matrix A, the notation G (Y, X; A) is used to represent a bipartite graph with two disjointed sets Y and an edge set between them by G (E), which is called X (E)."}, {"heading": "3.2 Analysis under random topic-word graph structures", "text": "In this section, we specialize in the identification result to the random case. This result is based on more transparent conditions about the size and degree of the random two-sided graph G (Vh, Vo; A). We consider the random model in which in the two-sided graph G (Vh, Vo; A) each node i. \"(Vh, Vo; A) each node i.\" (Vo, Vo; A) random two-sided graph G (Vo; A) random two-sided graph. \"(Vo; A) random two-sided graph.\" (Vo. \"(Vo; o.).\" (Vo.). \"(Vo.).\" (Vo.) Vo. \"(Vo.).\" (Vo.) (Vo.) Vo. (Vo; Vo.). (Vo.) Vo. (Vo.) Vo. (Vo.). (Vo.). (Vo.). (Vo.). (Vo.) Vo. (Vo.). (Vo.). (Vo; Vo.). (Vo.) Vo.). (Vo.). (Vo.). (Vo.). (Vo.). (Vo; Vo.). (Vo.). (Vo. (Vo; Vo.).). (Vo.). (Vo. (Vo.).). (Vo.). (Vo.). (Vo.). (Vo. (Vo.). (Vo; Vo; Vo.). (Vo. (Vo; Vo.). (Vo.). (Vo.). (Vo; Vo.). (Vo; Vo.). (Vo; Vo. (Vo.). (Vo; Vo.). (Vo.). (Vo; Vo.). (Vo. (Vo; Vo.). (Vo.).). (and Vo."}, {"heading": "4 Identifiability via Uniqueness of Tensor Decompositions", "text": "In this section, we will characterize the moments of the n-persistent theme model using the model parameters, i.e. the theme-word matrix A and the moment of hidden variables. We will relate the identifiability of the theme model to the uniqueness of a particular class of tensor decompositions, which in turn will enable us to prove theorems 1 and 2. Then, we will discuss the special cases of the persistent theme model, namely the single-topic model (infinite-persistent theme model) and the bag-of-words addixture model (1-persistent theme model)."}, {"heading": "4.1 Moment characterization of the persistent topic model", "text": "The moment of characterization requires the following definition of an n-gram matrix."}, {"heading": "4.2 Tensor algebra of the model", "text": "In Section 4.1 we have provided a representation of the moment forms in the matrix form. We now offer the equivalent tensor representation of the moments. The tensor representation is more compact and transparent and allows us to compare the topic models under different degrees of persistence. We compare the derived tensor form with the known Tucker and CP decompositions. First, we present some tensor notations and definitions."}, {"heading": "4.2.1 Tensor notations and definitions", "text": "A real weighted order-n Rp is defined by the Vec (an operator), a real weighted order-n Rp (an operator) is defined by the Operator (an operator). In this paper, we limit ourselves to the case that the vector f = A (2, 1, 3, 1) is a fiber. A vector-u-Rp (u) is a vector that can fix all indices of A except one, e.g. for A (4). The vector f = A (2, 1, 3, 1) is a fiber. A vector-u-Rp (u) is the n-th order diagonally with vector u on itsdiagonal."}, {"heading": "4.2.2 Tensor representation of moments under topic model", "text": "For the n-persistent theme model, the 2m-th observed moment is denoted by T (n) 2m (x), which is the tensor form of the moment matrix M (n) 2m (x), characterized in term 1. It is given by T2m (x) (x) (i1, i2,..., i2m): = E [x1 (i1) x2 (i2) (i2) (2) (2) (2), where T2m (2), where T2m (2), R p.This tensor is characterized in the following situation and demonstrated in Appendix A.2. Lemma 2 (n-persistent theme-moment characterization in tensor form). The (2m) -th moment of the words, defined in Equation (18), is denoted for the n-persistent theme characterization."}, {"heading": "5 Proof Techniques and Auxiliary Results", "text": "The most important identification results are given in theorems 1 and 2 for deterministic and random cases of topic-word-graph structures. In this section, we provide a proof sketch of these results, and then propose auxiliary results for the existence of a perfect n-gram matching for random two-sided graphs and a lower limit for the cruscal rank of random matrices."}, {"heading": "5.1 Proof sketch", "text": "There is a hierarchy under the proposed conditions as follows: \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \",\" \",\" \",\" \",\" \",\" \",\" \"\", \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\", \"\" \"\" \",\" \"\" \"\" \"\""}, {"heading": "5.2 Analysis of Random Structures", "text": "The identifiability result for a random structured matrix A is provided for in Theorem 2 > \u03b2 = > \u03b2 = constant size and degree ratios 4 and 5 on the random matrix A are proposed in such a way that deterministic combinatorial conditions 2 and 3 on A. The details of these auxiliary results are provided in the following two sections. In Section 5.2.1, Theorem 3 demonstrates that a random bifurcated graph meeting appropriate size and degree boundaries is a perfect match (Condition 2), with a lower boundary on the cruscal rank of a random matrix A below size and degree boundaries, in Theorem 4 in Section 5.2.2, which implies the sick condition 3. Intuitions about why such size and degree conditions are required are mentioned in Section 3.2, where these conditions are proposed. 5.2.1 Existence of a perfect graph indicates that a random matrix exists."}, {"heading": "5.2.2 Lower bound on the Kruskal rank of random matrices", "text": "The following theorem specifies a lower limit on the cruscal rank of a random matrix A under measurement and degree constraints, as demonstrated in Appendix B.1 Theorem 4 (lower limit on the cruscal rank of random matrices). Consider a random matrix A-Rp-q in which there is a di-number of random unequal entries in column i. Letter b: = minimum requirements [q] di for any i-Rp-q entries. Suppose it meets the size condition q \u2264 (c pn) n (condition 4) for any constant 0 < c < 1 and the degree condition dmin \u2265 1 + \u03b2 log p for any constant \u03b2 > n \u2212 1log 1 / c (lowerbound in condition 5), and also A is general."}, {"heading": "Acknowledgements", "text": "The authors acknowledge useful discussions with Sina Jafarpour, Adel Javanmard, Alex Dimakis, Moses Charikar, Sanjeev Arora, Ankur Moitra and Kamalika Chaudhuri. A. Anandkumar is partially supported by Microsoft Faculty Fellowship, NSF Career Award CCF-1254106, NSF Award CCF-1219234, ARO Award W911NF-12-1-0404 and ARO YIP Award W911NF-13-1-0084. M. Janzamin is supported by NSF Award CCF-1219234, ARO Award W911NF-12-1-0404 and ARO YIP Award W911NF-13-1-0084. Appendix"}, {"heading": "A Proof of Deterministic Identifiability Result (Theorem 1)", "text": "First, we show identification results under an alternative set of conditions on the n-gram matrix, A-3, A-3, A-3, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-5, A-"}, {"heading": "B Proof of Random Identifiability Result (Theorem 2)", "text": "We provide detailed evidence for the steps shown in the evidence of random results in section 5.2.B.1, in which most of the steps of the existence of perfect N-grammar and cruscal sets in section 5.2.B.1 are proof sets in section 6.2.1 (see Figure 6). Definition J: = c pn. Definition of the partition of X-gramms in section X-grammar. Definition of X-gramms in section X-grammar. Definition of X-grammar in section X-grammar in section X.1 is the section in the section of X-gramys in section X-grammar, which are placed in each case in the section of X-gram- l. Definition of X-grammar in section X-grammar in section X-grammar in section X.1 is the section in the section of X-grammar in section X-grammar, \"in section X-grammar in section,\" in section X-grammar in section, \"in section X.\" In section X-grammar in section, \"in section X."}, {"heading": "C Relationship to CP Decomposition Uniqueness Results", "text": "In this section we present a more detailed comparison with some uniqueness results of supercomplete CP decomposition. (Here is the following CP decomposition for the third-order tensor T-Rp-s-q considered, T = r-i = 1ai-bi-q-ci, (41) where A = [a1 |...] The most important and general uniqueness of CP, called Kruskal condition, is provided in [15], where it is guaranteed that the above-mentioned CP decomposition is unique (A) + ill (B) + ill (C). The most important and general uniqueness result of CP, called Kruskal condition, is the condition for the condition M \u2212 asrified ability, whereby it is guaranteed that the above-mentioned CP decomposition is unique (B) + ill (C) + ill (C) + (C). Since then, several works have analyzed the uniqueness of CP decomposition."}], "references": [{"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "arXiv preprint arXiv:1206.5538,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning overcomplete representations", "author": ["Michael S. Lewicki", "Terrence J. Sejnowski", "Howard Hughes"], "venue": "Neural Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Local convergence of the alternating least squares algorithm for canonical tensor approximation", "author": ["Andr\u00e9 Uschmajew"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Latent Dirichlet Allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Inference of population structure using multilocus genotype", "author": ["J.K. Pritchard", "M. Stephens", "P. Donnelly"], "venue": "data. Genetics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Tensor Methods for Learning Latent Variable Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Under Review. J. of Machine Learning. Available at arXiv:1210.7559,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning Linear Bayesian Networks with Latent Variables", "author": ["A. Anandkumar", "D. Hsu", "A. Javanmard", "S.M. Kakade"], "venue": "ArXiv e-prints,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Sanjeev Arora", "Rong Ge", "Yoni Halpern", "David M. Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu"], "venue": "ArXiv 1212.4777,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "More factors than subjects, tests and treatments: an indeterminacy theorem for canonical decomposition and individual differences", "author": ["J.B. Kruskal"], "venue": "scaling. Psychometrika,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1976}, {"title": "Tensor decompositions and applications", "author": ["Tamara Kolda", "Brett Bader"], "venue": "SIREV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["Silvia Gandy", "Benjamin Recht", "Isao Yamada"], "venue": "Inverse Problems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Honglak Lee", "Andrew Y. Ng"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning", "author": ["Quoc V. Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Deep Learning for Signal and Information Processing", "author": ["Li Deng", "Dong Yu"], "venue": "NOW Publishers,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics", "author": ["J.B. Kruskal"], "venue": "Linear algebra and its applications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1977}, {"title": "Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability", "author": ["A. Bhaskara", "M. Charikar", "A. Vijayaraghavan"], "venue": "ArXiv 1304.8087,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Kruskal\u2019s permutation lemma and the identification of candecomp/parafac and bilinear models with constant modulus constraints", "author": ["Tao Jiang", "Nicholas D Sidiropoulos"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "A Link between the Canonical Decomposition in Multilinear Algebra and Simultaneous Matrix Diagonalization", "author": ["Lieven De Lathauwer"], "venue": "SIAM J. Matrix Analysis and Applications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Sufficient conditions for uniqueness in candecomp/parafac and indscal with random component", "author": ["Alwin Stegeman", "Jos M.F. Ten Berge", "Lieven De Lathauwer"], "venue": "matrices. Psychometrika,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Fourth-order cumulant-based blind identification of underdetermined mixtures", "author": ["L. De Lathauwer", "J. Castaing", "J.-F Cardoso"], "venue": "IEEE Tran. on Signal Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "On generic identifiability of 3-tensors of small rank", "author": ["Luca Chiantini", "Giorgio Ottaviani"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Refined methods for the identifiability of tensors", "author": ["Cristiano Bocci", "Luca Chiantini", "Giorgio Ottaviani"], "venue": "arXiv preprint arXiv:1303.6915,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "One example of general unidentifiable tensors", "author": ["Luca Chiantini", "Massimiliano Mella", "Giorgio Ottaviani"], "venue": "arXiv preprint arXiv:1303.6914,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Identifiability of parameters in latent structure models with many observed variables", "author": ["E.S. Allman", "C. Matias", "J.A. Rhodes"], "venue": "The Annals of Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "A semialgebraic description of the general markov model on phylogenetic trees", "author": ["Elizabeth S. Allman", "John A. Rhodes", "Amelia Taylor"], "venue": "Arxiv preprint arXiv:1212.1200,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Tensors: Geometry and applications, volume 128", "author": ["Joseph M Landsberg"], "venue": "American Mathematical Soc.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "A Method of Moments for Mixture Models and Hidden Markov Models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "In Proc. of Conf. on Learning Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "A Spectral Algorithm for Latent Dirichlet Allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S.M. Kakade", "Y.K. Liu"], "venue": "In Proc. of Neural Information Processing (NIPS),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "A Tensor Spectral Approach to Learning Mixed Membership Community Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Learning nonsingular phylogenies and hidden markov models", "author": ["E. Mossel", "S. Roch"], "venue": "The Annals of Applied Probability,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Full reconstruction of markov models on evolutionary trees: identifiability and consistency", "author": ["J.T. Chang"], "venue": "Mathematical Biosciences,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Learning mixtures of arbitrary distributions over large discrete domains", "author": ["Yuval Rabani", "Leonard Schulman", "Chaitanya Swamy"], "venue": "arXiv preprint arXiv:1212.1527,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Learning topic models\u2014going beyond svd", "author": ["Saneev Arora", "Rong Ge", "Ankur Moitra"], "venue": "In Symposium on Theory of Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Exact recovery of sparsely-used dictionaries", "author": ["Daniel A Spielman", "HuanWang", "JohnWright"], "venue": "In Proc. of Conf. on Learning Theory,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Dictionary learning algorithms for sparse representation", "author": ["Kenneth Kreutz-Delgado", "Joseph F. Murray", "Bhaskar D. Rao", "Kjersti Engan", "Te-Won Lee", "Terrence J. Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2003}, {"title": "An affine scaling methodology for best basis selection", "author": ["B. Rao", "K. Kreutz-Delgado"], "venue": "IEEE Tran. Signal Processing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Sparsity-based generalization bounds for predictive sparse coding", "author": ["Nishant A. Mehta", "Alexander G. Gray"], "venue": "In Proc. of the Intl. Conf. on Machine Learning (ICML), Atlanta,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Sparse coding for multitask and transfer learning", "author": ["Andreas Maurer", "Massimiliano Pontil", "Bernardino Romera-Paredes"], "venue": "ArxXiv preprint,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Ramsey theory reveals the conditions when sparse coding on subsampled data is unique", "author": ["Christopher J Hillar", "Friedrich T Sommer"], "venue": "arXiv preprint arXiv:1106.3616,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Posterior contraction of the population polytope in finite admixture models", "author": ["XuanLong Nguyen"], "venue": "arXiv preprint arXiv:1206.0068,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Exact recovery of sparsely-used dictionaries", "author": ["Daniel A. Spielman", "Huan Wang", "John Wright"], "venue": "ArxXiv preprint,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "On the uniqueness of multilinear decomposition of N-way arrays", "author": ["Nicholas D. Sidiropoulos", "Rasmus Bro"], "venue": "Journal of Chemometrics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2000}, {"title": "The tail of the hypergeometric distribution", "author": ["V. Chv\u00e1tal"], "venue": "Discrete Mathematics,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1979}, {"title": "On representatives of subsets", "author": ["Philip Hall"], "venue": "J. London Math. Soc.,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1935}], "referenceMentions": [{"referenceID": 0, "context": "Overcomplete representations, where the number of features can be greater than the dimensionality of the input data, have been extensively employed, and are arguably critical in a number of applications such as speech and computer vision [1].", "startOffset": 238, "endOffset": 241}, {"referenceID": 1, "context": "representations are known to be more robust to noise, and can provide greater flexibility in modeling [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "see [3].", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "In this paper, we characterize identifiability for a popular class of latent variable models, known as the admixture or topic models [4, 5].", "startOffset": 133, "endOffset": 139}, {"referenceID": 4, "context": "In this paper, we characterize identifiability for a popular class of latent variable models, known as the admixture or topic models [4, 5].", "startOffset": 133, "endOffset": 139}, {"referenceID": 5, "context": "[6\u20138].", "startOffset": 0, "endOffset": 5}, {"referenceID": 6, "context": "[6\u20138].", "startOffset": 0, "endOffset": 5}, {"referenceID": 7, "context": "[6\u20138].", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "In addition, we present trade-offs among the following quantities: number of topics, size of the word vocabulary, the topic persistence level, the order of the observed moments at hand, the minimum and maximum degrees of any topic in the topic-word bipartite graph, and the Kruskal rank [9] of the topic-word matrix, under which identifiability holds.", "startOffset": 287, "endOffset": 290}, {"referenceID": 9, "context": "Our identifiability results for persistent topic models imply uniqueness of a structured class of tensor decompositions, which is contained in the class of Tucker decompositions, but is more general than the candecomp/parafac (CP) decomposition [10].", "startOffset": 245, "endOffset": 249}, {"referenceID": 6, "context": "Recap of Identifiability Conditions in Under-complete Setting (Expansion Conditions on Topic-Word Matrix): Our approach is based on the recent results of [7], where conditions for identifiability of topic models are derived, given pairwise observed moments (specifically, cooccurrence of word-pairs in documents).", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Expansion conditions are imposed in [7] on the topic-word bipartite graph which imply that (generically) the sparsest vectors in the column span of A, denoted by Col(A), are the columns of A themselves.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "Therefore, the techniques derived in [7] are not directly applicable here since we consider overcomplete models.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "We can view the higher order moments as pairwise moments of another equivalent topic model, which enables us to apply the techniques of [7].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "Imposing the expansion conditions derived in [7] on A\u2299n implies that (generically) the sparsest vectors in Col(A\u2299n), are the columns of A\u2299n themselves.", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "We refer to this as the \u201cfirst-order\u201d approach since we directly impose the expansion conditions of [7] on A\u2299n, without exploiting the additional structure present in A\u2299n.", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Moreover, we establish that A\u2299n fails to expand on \u201csmall\u201d sets, as required in [7], when the degrees are sufficiently different 2.", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "This agrees with the intuition that when the topic-word matrix A has a larger Kruskal rank, it should be easier to identify A, since the Kruskal rank is related to the mutual incoherence 5 among the columns of A, see [11].", "startOffset": 217, "endOffset": 221}, {"referenceID": 0, "context": "[1,12\u201314], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.", "startOffset": 0, "endOffset": 9}, {"referenceID": 11, "context": "[1,12\u201314], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.", "startOffset": 0, "endOffset": 9}, {"referenceID": 12, "context": "[1,12\u201314], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.", "startOffset": 0, "endOffset": 9}, {"referenceID": 13, "context": "[1,12\u201314], and record huge gains over other approaches in a number of applications such as speech recognition and computer vision.", "startOffset": 0, "endOffset": 9}, {"referenceID": 8, "context": "The classical result by Kruskal provides conditions for uniqueness of a CP decomposition [9, 15], with recent extensions to the notion of robust identifiability [16].", "startOffset": 89, "endOffset": 96}, {"referenceID": 14, "context": "The classical result by Kruskal provides conditions for uniqueness of a CP decomposition [9, 15], with recent extensions to the notion of robust identifiability [16].", "startOffset": 89, "endOffset": 96}, {"referenceID": 15, "context": "The classical result by Kruskal provides conditions for uniqueness of a CP decomposition [9, 15], with recent extensions to the notion of robust identifiability [16].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "[17\u201323].", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[17\u201323].", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[17\u201323].", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[17\u201323].", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[17\u201323].", "startOffset": 0, "endOffset": 7}, {"referenceID": 21, "context": "[17\u201323].", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "[17\u201323].", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[24, 25], and the single-topic model, or more generally latent Dirichlet allocation (LDA).", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[24, 25], and the single-topic model, or more generally latent Dirichlet allocation (LDA).", "startOffset": 0, "endOffset": 8}, {"referenceID": 9, "context": "For a general overview of tensor decompositions, see [10,27].", "startOffset": 53, "endOffset": 60}, {"referenceID": 25, "context": "For a general overview of tensor decompositions, see [10,27].", "startOffset": 53, "endOffset": 60}, {"referenceID": 5, "context": "[6,28,29] provide an efficient moment-based approach for learning topic models, under constraints on the distribution of the topic proportions, e.", "startOffset": 0, "endOffset": 9}, {"referenceID": 26, "context": "[6,28,29] provide an efficient moment-based approach for learning topic models, under constraints on the distribution of the topic proportions, e.", "startOffset": 0, "endOffset": 9}, {"referenceID": 27, "context": "[6,28,29] provide an efficient moment-based approach for learning topic models, under constraints on the distribution of the topic proportions, e.", "startOffset": 0, "endOffset": 9}, {"referenceID": 28, "context": "In addition, the approach can handle a variety of latent variable models such as Gaussian mixtures, hidden Markov models (HMM) and community models [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 5, "context": "In [6], a tensor power method approach is analyzed and is shown to be an efficient guaranteed recovery method in the nondegenerate (i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 26, "context": "[28, 31, 32].", "startOffset": 0, "endOffset": 12}, {"referenceID": 29, "context": "[28, 31, 32].", "startOffset": 0, "endOffset": 12}, {"referenceID": 30, "context": "[28, 31, 32].", "startOffset": 0, "endOffset": 12}, {"referenceID": 19, "context": "[20], can be employed instead, albeit at a cost of higher computational complexity for overcomplete CP tensor decomposition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "See [28] for a detailed description of these works.", "startOffset": 4, "endOffset": 8}, {"referenceID": 31, "context": "[33] consider learning discrete mixtures given a large number of \u201cviews\u201d, and they refer to the number of views as the sampling aperture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8, 34] employ approaches based on non-negative matrix factorization (NMF) to recover the topic-word matrix.", "startOffset": 0, "endOffset": 7}, {"referenceID": 32, "context": "[8, 34] employ approaches based on non-negative matrix factorization (NMF) to recover the topic-word matrix.", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[7] which considers identifiability and learning of topic models under expansion conditions on the topic-word matrix.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "al [35] considers the problem of dictionary learning, which is closely related to the setting of [7], but in addition assumes that the coefficient matrix is random.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "al [35] considers the problem of dictionary learning, which is closely related to the setting of [7], but in addition assumes that the coefficient matrix is random.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "However, these works [7,35] can handle only the under-complete setting, where the number of topics is less than the dimensionality of the word vocabulary (or the number of dictionary atoms is less than the number of observations in [35]).", "startOffset": 21, "endOffset": 27}, {"referenceID": 33, "context": "However, these works [7,35] can handle only the under-complete setting, where the number of topics is less than the dimensionality of the word vocabulary (or the number of dictionary atoms is less than the number of observations in [35]).", "startOffset": 21, "endOffset": 27}, {"referenceID": 33, "context": "However, these works [7,35] can handle only the under-complete setting, where the number of topics is less than the dimensionality of the word vocabulary (or the number of dictionary atoms is less than the number of observations in [35]).", "startOffset": 232, "endOffset": 236}, {"referenceID": 1, "context": "There have been Bayesian as well as frequentist approaches for dictionary learning [2,36,37].", "startOffset": 83, "endOffset": 92}, {"referenceID": 34, "context": "There have been Bayesian as well as frequentist approaches for dictionary learning [2,36,37].", "startOffset": 83, "endOffset": 92}, {"referenceID": 35, "context": "There have been Bayesian as well as frequentist approaches for dictionary learning [2,36,37].", "startOffset": 83, "endOffset": 92}, {"referenceID": 1, "context": "employed in these works [2, 36, 37] have no performance guarantees.", "startOffset": 24, "endOffset": 35}, {"referenceID": 34, "context": "employed in these works [2, 36, 37] have no performance guarantees.", "startOffset": 24, "endOffset": 35}, {"referenceID": 35, "context": "employed in these works [2, 36, 37] have no performance guarantees.", "startOffset": 24, "endOffset": 35}, {"referenceID": 33, "context": "al [35] considers learning (undercomplete) dictionaries and provide guaranteed learning under the assumption that the coefficient matrix is random (distributed as Bernoulli-Gaussian variables).", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "Recent works [38, 39] provide generalization bounds for predictive sparse coding, where the goal of the learned representation is to obtain good performance on some predictive task.", "startOffset": 13, "endOffset": 21}, {"referenceID": 37, "context": "Recent works [38, 39] provide generalization bounds for predictive sparse coding, where the goal of the learned representation is to obtain good performance on some predictive task.", "startOffset": 13, "endOffset": 21}, {"referenceID": 38, "context": "Hillar and Sommer [40] consider the problem of identifiability of sparse coding and establish that when the dictionary succeeds in reconstructing a certain set of sparse vectors, then there exists a unique sparse coding, up to permutation and scaling.", "startOffset": 18, "endOffset": 22}, {"referenceID": 39, "context": "For A \u2208 Rp\u00d7q and B \u2208 Rm\u00d7n, the Kronecker product A\u2297B \u2208 Rpm\u00d7qn is defined as [41]", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "In this section, the n-persistent topic model is introduced and this imposes an additional constraint, known as topic persistence on the popular admixture model [4,5,42].", "startOffset": 161, "endOffset": 169}, {"referenceID": 4, "context": "In this section, the n-persistent topic model is introduced and this imposes an additional constraint, known as topic persistence on the popular admixture model [4,5,42].", "startOffset": 161, "endOffset": 169}, {"referenceID": 40, "context": "In this section, the n-persistent topic model is introduced and this imposes an additional constraint, known as topic persistence on the popular admixture model [4,5,42].", "startOffset": 161, "endOffset": 169}, {"referenceID": 40, "context": "This collection of vectors ai, i \u2208 [q], is referred to as the population structure or the topic-word matrix [42].", "startOffset": 108, "endOffset": 112}, {"referenceID": 5, "context": "We now describe a linear representation of the n-persistent topic model, on lines of [6], but with extensions to incorporate persistence.", "startOffset": 85, "endOffset": 88}, {"referenceID": 24, "context": "For a given sparsity pattern, the class of population structure matrices is said to be generically identifiable [25], if all the non-identifiable matrices form a set of Lebesgue measure zero.", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "Definition 4 (Kruskal rank, [15]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Identifiability of this model has been studied earlier [7] and we recall it below.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Remark 4 (Bag-of-words admixture model, [7]).", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "Given (2r)-th order observed moments with r \u2265 1, the structure of the popular bag-of-words admixture model and the (2r)-th order moment of hidden variables are identifiable, when A is full column rank and the following expansion condition holds [7] |NA(S)| \u2265 |S|+ dmax(A), \u2200S \u2286 Vh, |S| \u2265 2.", "startOffset": 245, "endOffset": 248}, {"referenceID": 6, "context": "Efficient l1-based recovery algorithms have been analyzed in [7,43] for the undercomplete case (n = 1).", "startOffset": 61, "endOffset": 67}, {"referenceID": 41, "context": "Efficient l1-based recovery algorithms have been analyzed in [7,43] for the undercomplete case (n = 1).", "startOffset": 61, "endOffset": 67}, {"referenceID": 19, "context": "Exploiting additional structure present in A\u2299n, for n > 1, such as rank-1 test devices proposed in [20] are interesting avenues for future investigation.", "startOffset": 99, "endOffset": 103}, {"referenceID": 41, "context": "The identifiability result for the random bag-of-words admixture model is comparable to the result in [43], which considers exact recovery of sparsely-used dictionaries.", "startOffset": 102, "endOffset": 106}, {"referenceID": 39, "context": "That is, A\u2299n is the column-wise n order Kronecker product of n copies of A, and is known as the Khatri-Rao product [41].", "startOffset": 115, "endOffset": 119}, {"referenceID": 39, "context": "For vectors ai \u2208 R pi , i \u2208 [n], the tensor outer product operator \u201c\u25e6\u201d is defined as [41]", "startOffset": 85, "endOffset": 89}, {"referenceID": 39, "context": "This type of rank is called CP (Candecomp/Parafac) tensor rank in the literature [41].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "Finally, the Tucker and CP (Candecomp/Parafac) representations are defined as follows [10,41].", "startOffset": 86, "endOffset": 93}, {"referenceID": 39, "context": "Finally, the Tucker and CP (Candecomp/Parafac) representations are defined as follows [10,41].", "startOffset": 86, "endOffset": 93}, {"referenceID": 6, "context": "Note that since we only require expansion on sets larger than Kruskal rank, the expansion condition (24) is a more relaxed condition compared to expansion condition proposed in [7, 43] for identifiability in the undercomplete regime.", "startOffset": 177, "endOffset": 184}, {"referenceID": 41, "context": "Note that since we only require expansion on sets larger than Kruskal rank, the expansion condition (24) is a more relaxed condition compared to expansion condition proposed in [7, 43] for identifiability in the undercomplete regime.", "startOffset": 177, "endOffset": 184}, {"referenceID": 6, "context": "The expansion condition for the bag-of-words admixture model is provided in (4), introduced in [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "Second, the expansion property (4), proposed in [7], needs to be satisfied for all subsets S with size |S| \u2265 2, which is a stricter condition than the one proposed here in (25), since we can have krank(A) \u226b 2.", "startOffset": 48, "endOffset": 51}, {"referenceID": 14, "context": "Note that for n \u2265 3, this full rank condition can be relaxed by Kruskal\u2019s condition for uniqueness of CP decomposition [15] and its generalization to higher order tensors [44].", "startOffset": 119, "endOffset": 123}, {"referenceID": 42, "context": "Note that for n \u2265 3, this full rank condition can be relaxed by Kruskal\u2019s condition for uniqueness of CP decomposition [15] and its generalization to higher order tensors [44].", "startOffset": 171, "endOffset": 175}, {"referenceID": 6, "context": "The result proposed in this lemma is similar to the parameter genericity condition in [7], but generalized for the n-gram matrix, A\u2299n.", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "2 in [7].", "startOffset": 5, "endOffset": 8}, {"referenceID": 43, "context": "The following tail bound for the hypergeometric distribution is provided [45,46]", "startOffset": 73, "endOffset": 80}, {"referenceID": 44, "context": "Theorem 6 (Hall\u2019s theorem, [47]).", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "The most important and general uniqueness result of CP, called Kruskal\u2019s condition, is provided in [15], where it is guaranteed that the above CP decomposition is unique if", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "One set of works assume that one of the components, say C, is full column rank [17, 18].", "startOffset": 79, "endOffset": 87}, {"referenceID": 17, "context": "One set of works assume that one of the components, say C, is full column rank [17, 18].", "startOffset": 79, "endOffset": 87}, {"referenceID": 17, "context": "it is shown in [18], for generic (fully dense) components A,B and C, if r \u2264 q and r(r \u2212 1) \u2264 p(p\u2212 1)s(s \u2212 1)/2, then the CP decomposition in (41) is generically unique.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Thus, the identifiability results of [18] are applicable to our setting, if we assume generic (i.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "We note that the size bound specified in [18] is comparable to the size bound derived in this paper (for random structured matrices), but we have additional degree considerations for identifiability.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "Analyzing the regime where the uniqueness conditions of [18] are satisfied under sparsity constraints is an interesting question for future investigation.", "startOffset": 56, "endOffset": 60}], "year": 2013, "abstractText": "Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of \u201chigher order\u201d expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition.", "creator": "LaTeX with hyperref package"}}}