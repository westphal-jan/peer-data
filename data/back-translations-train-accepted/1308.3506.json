{"id": "1308.3506", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2013", "title": "Computational Rationalization: The Inverse Equilibrium Problem", "abstract": "Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations.", "histories": [["v1", "Thu, 15 Aug 2013 20:43:47 GMT  (734kb,D)", "http://arxiv.org/abs/1308.3506v1", "In submission to JMLR, conference version:arXiv:1103.5254"]], "COMMENTS": "In submission to JMLR, conference version:arXiv:1103.5254", "reviews": [], "SUBJECTS": "cs.GT cs.LG stat.ML", "authors": ["kevin waugh", "brian d ziebart", "drew bagnell"], "accepted": true, "id": "1308.3506"}, "pdf": {"name": "1308.3506.pdf", "metadata": {"source": "CRF", "title": "Computational Rationalization: The Inverse Equilibrium Problem", "authors": ["Kevin Waugh"], "emails": ["waugh@cs.cmu.edu", "bziebart@uic.edu", "dbagnell@ri.cmu.edu"], "sections": [{"heading": null, "text": "In this work, we look at similar tasks in competitive and cooperative multi-agent domains. Unlike single-agent settings, a player here cannot myopically maximize his reward; he must speculate on how the other players might act to influence the outcome of the game. Using the Xiv: 1game theoretical concept of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior."}, {"heading": "1 Introduction", "text": "In fact, it is so that most people who are able to survive themselves are able to survive themselves by going in search of themselves. In the other world, it is so that they are able to survive themselves. In the third world, it is so that they are able to survive in the third world. In the third world, it is so that they are able to survive in the third world. In the third world, it is so, in the third world, it is so that they survive in the third world. In the third world, it is so that they have survived in the third world. In the third world, it is so that they have survived in the third world. In the third world, it is so, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world it is so, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third world, in the third, in the third in the third, third in the third, in the third in the third, third in the third, in the third in the third, third in the third, third in the third, third in the third, third in the third, third in the third, third in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third world, in the third, in the third, in the third, in the third."}, {"heading": "2 Related Work", "text": "In fact, most people are able to move to another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to change the world, in which they are able, in which they are able to change the world."}, {"heading": "3 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Notation", "text": "Let V be a Hilbert space with an inner product < \u00b7, \u00b7 >: V \u00b7 V \u2192 R. For each quantity of K V, let K \u0445 = {x | < x, y > \u2265 0, \u0445y-y-K} be its double cone. Let's allow v-1 = \u0445 K k = 1 | \u03b1k | where v = \u0445 Kk = 1 \u03b1kek if V has finite dimensions with an orthonormal basis {e1,..., eK}. Typically, let's take V = RK and use the default inner product."}, {"heading": "Game Theory", "text": "We are the canonical tool of game theorists to represent strategic interactions ranging from illustrative toy problems such as the \"Prisoner's Dilemma\" and the \"Battle of the Sexes\" game to important negotiations, collaborations and auctions. Contrary to the traditional definition [Osborne and Rubinstein, 1994], in this work we model games in which only the characteristics of the players themselves are known and not the utilities themselves. Definition 1. A vector-rated normal form game is a tuple of its behavior (N, ui) in which there is the finite amount of players playing. \u2022 N is the finite amount of players playing, \u2022 A = i is the set of outcomes or joint actions in which the finite amount of actions or strategies for player is i."}, {"heading": "4 Behavior Estimation in a Matrix Game", "text": "We now have the necessary tools to introduce our approach to imitating learning processes in multi-agent scenarios. First, we start from an idea of the rationality of the players. By using this assumption, we derive an estimation method with much better statistical properties than methods that are unaware of the structure of the game."}, {"heading": "4.1 Rationality and the ICE Polytope", "text": "We call the empirical distribution of the observations to extract the motives for the behavior so that we can imitate the players in similarly structured but unobserved games. First, let's just consider the estimation problem, while we assume that we will have access to the true behavior of the players by analyzing the error introduced by approaching the demonstrations. Imitation seems hard to rule out further assumptions. In particular, if the agents are not motivated or their intentions are not forced by the observed game, there is little hope of restoring the principled behavior in a new game."}, {"heading": "4.2 The Principle of Maximum Entropy", "text": "Since we are interested in the problem of statistical prediction of strategic behavior, we need to find a mechanism to solve the ambiguity that remains after the rationality constraints are taken into account. The principle of maximum1We can outline a simple counter-example. Consider a game with one player and three actions, x, y and y, where the benefit for playing x is zero and the benefit for playing y or y is one. If the true behavior always plays y, then the adjustment of the regret characteristics also forces the prediction to play y. The prediction of y also corresponds to the regret that though.entropy, due to Jaynes [1957], provides a well-founded method for choosing such a distribution. This choice leads not only to statistical guarantees for the resulting predictions, but to efficient optimization. The Shannon entropy of a common strategy that we apply is HUK (Ea)."}, {"heading": "4.3 Dual Optimization", "text": "In this section, we will derive and describe a procedure for optimizing the dual program for solving the MaxEnt ICE optimization problem. We will see that the dual multipliers can be interpreted as utility vectors and that the optimization in the dual program has arithmetic advantages. We will start by introducing the dual program.Theorem 7. The dual maximum entropy ICE optimization problem is the following non-smooth but convex program: Minimize the dual entropy optimization problem. We will derive the dual maximum entropy ICE optimization problem: minimize the dual entropy optimization method. We will derive the dual entropy solution in the appendix. Since the dual solution of the dual program is not empty relative, the strong duality due to Slater's condition - there is no duality gap. We can also use a dual solution for restoring the duality."}, {"heading": "5 Behavior Estimation in Parameterized Matrix", "text": "Games To take into account stochastic or different environments, today we look at distributions across games. For example, rain can affect travel time along certain routes and make certain means of transport less desirable or not available at all. Operationally, nature considers a game before we play it, based on a distribution known to the players. Players then determine as a group a common strategy based on the particular game and a result by a coordinating device. We have G designate our game class. As before, we observe a sequence of T independent observations of the game, but now, in addition to a result, we also observe the choice of nature at any time. Let's (t, at) Tt = 1 be the above-mentioned sequence of observations drawn from E and E, the true behavior. The empirical distribution of the observations, E and E, together are the proven characteristics of behavior."}, {"heading": "5.1 Behavior Estimation through Conditional ICE", "text": "In order to do so, our idea of a deviation must take into account the fact that it can be carried out in games with other structures. Operationally, one way of achieving this is to have a deviation when not applied to such a game, which allows a transfer over an infinitely large class. In the light of such a decision, we write the expected regret characteristics by deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation, and the anticipated regret by a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation from a deviation."}, {"heading": "5.2 Behavior Transfer without common deviations", "text": "A basic justification for inverse optimal control techniques that attempt to identify behavior in relation to utility functions is the ability to examine what behavior might result if the underlying decision problem were changed, while the interpretation of functions in utility functions remains the same [Ng and Russell, 2000, Ratliff et al., 2006]. This allows predicting the behavior of agents without remorse or agnostic sense in problems such as a robot encountering novel terrain [Silver et al., 2010] as well as the route recommendation for drivers traveling to invisible destinations [Ziebart et al., 2008b]. Econometrists are interested in similar situations, but for very different reasons. Typically, they aim to validate a model of market behavior from observations of product sales. In these models, companies assume a fixed pricing policy based on known demand. The econometrician uses this fixed policy together with product characteristics and sales data to estimate both the cost of use and the cost of use as well as the unknown functionality of production."}, {"heading": "6 Sample Complexity", "text": "In practice, we do not have full access to the true behavior of the agents - if we did, the prediction would be simple and we would not need an estimation technique. Instead, we can only approximate the desired expectations by establishing an average over a finite number of observations. Next, we will analyze the sensitivity of our approach to these types of errors. In real life, there are costs associated with collecting these observations, and therefore there are inherent limitations to the quality of this approach. Next, we will analyze the sensitivity of our approach to these types of errors. First, although the deviation is exponential in the number of players, our technique only applies to the expected regrets characteristics of the form of rationality. That is, we need these characteristics only roughly accurately, not to the probability of distribution."}, {"heading": "7 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Synthetic Routing Game", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "7.2 Market Entry Game", "text": "In fact, it is such that we are in a position to assert that we are in a position to assert that we are in a position to assert ourselves in the world, and that we are in a position to assert ourselves in the world, to assert that we are in a position to assert ourselves in the world, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, that we live, that we live, that we live, that we live, that we live, that we live, that we live, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what, what"}, {"heading": "7.3 Mid-scale Hotel Market Entry", "text": "In fact, most of them are able to survive on their own."}, {"heading": "8 Conclusion", "text": "In this article, we develop a novel approach to predicting behavior in strategic multi-agent domains. We show that by using a rationality assumption and the principle of maximum entropy, our method can be efficiently implemented while achieving good statistical performance at the same time. Empirically, we have demonstrated the effectiveness of our approach on two sets of market entry data. We have demonstrated both the robustness of our approach to errors in our assumptions and the importance of taking strategic interactions into account. Our future work will take into account two new directions: First, we will look at game classes in which action sets and players differ. A key advantage of our current approach is that they differ between training and testing, which we use only modestly in route prediction transfer experiments. It is a matter of examining from a statistical point of view novel notions of deviation and their corresponding equilibrium concepts. Second, we will look at different models of interactions, such as those that are more complex and complex models of expression within a Texas game."}, {"heading": "Acknowledgements", "text": "This work is supported by the ONR MURI grant N00014-09-1-1052 and the National Sciences and Engineering Research Council of Canada (NSERC). The authors thank Prof. Junichi Suzuki for providing the medium-sized aggregated hotel data and Alex Grubb for applying the density estimation code to the data sets."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Proof of Lemma 1", "text": "Evidence. The lower limit is considered as a consequence of \u03a6int swap.Since max x-Ai, y-Ai, y-Ai; x-y (\u03c3-Ai-w) \u2265 r\u0109i; x-x (\u03c3\u0442-w) = 0, Regrets Swap.Since max x-Ai, y-Ai-w) = max i-N-N-N-N-N-N-N-N-N-N-A-A; x-y-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A."}, {"heading": "Proof of Theorem 3", "text": "The proof for theorem 3 results directly from the following lemma.Lemma 4. For any power function w-K, rp-f (n), rp-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n), p-f (n)."}, {"heading": "Proof of Theorem 4", "text": "The proof for Theorem 4 is immediately derived from the following quote: Lemma 5. if the common strategy has an external regret, and if it is a 2-player and a constant sum in terms of w, then the marginal strategies form a 2-player-x and 3-player-y. we are given the marginal strategies x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "Proof of Theorem 7", "text": "The proof: The Legrange dual function, L (zipiert, \u03b1, \u03b2, u, v, x) = max. (p.a.), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c) c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c)), c), c), c), c)), c), c), c), c)), c)), c)), c), c)), c), c)), c), c), c)), c)), c), c)), c), c)), c)), c), c)), c), c)), c), c), (), c), c), (), (), (), (), (), ()), (), (), (), (),), (), c), (), (), ()), c), c), (), (), c), (), c), (), c), (), (), (),), (), (), (),),),))), (),"}, {"heading": "Proof of Theorem 9", "text": "(...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) ("}, {"heading": "Proof of Theorem 10", "text": "Proof. In contrast to Theorem 9, we will directly tie the error of regret. P [max f] f [n] f (n) f (n) f (n) f (n) f (n) f (w)] p [f) p [f) p (r) f (n) f (n) f (n) f (n) g (w))] p [f) p [r] f (c) f (n) p (w)] p (- 2) p (- 2) p (- 2) p (- 2) p (- 2) p (- 2) p (- 2)."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward; it must speculate on how the other agents may act to influence the game\u2019s outcome. Employing the 1 ar X iv :1 30 8. 35 06 v1 [ cs .G T ] 1 5 A ug 2 01 3 game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior.", "creator": "LaTeX with hyperref package"}}}