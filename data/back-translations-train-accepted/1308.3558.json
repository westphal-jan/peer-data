{"id": "1308.3558", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2013", "title": "Fast Stochastic Alternating Direction Method of Multipliers", "abstract": "In this paper, we propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, the proposed algorithm improves the convergence rate on convex problems from $O(\\frac 1 {\\sqrt{T}})$ to $O(\\frac 1 T)$, where $T$ is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.", "histories": [["v1", "Fri, 16 Aug 2013 05:48:29 GMT  (1625kb,D)", "http://arxiv.org/abs/1308.3558v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["wenliang zhong", "james tin-yau kwok"], "accepted": true, "id": "1308.3558"}, "pdf": {"name": "1308.3558.pdf", "metadata": {"source": "CRF", "title": "Fast Stochastic Alternating Direction Method of Multipliers", "authors": ["Leon Wenliang Zhong", "James T. Kwok"], "emails": [], "sections": [{"heading": null, "text": "This corresponds to the convergence rate of the batch ADMM algorithm, but without the need to visit all samples in each iteration. Experiments on the graph-based, fused lasso show that the new algorithm is significantly faster than the current state of the art. [1], [2], [3] considered the problems of formal x, ymps (x, y), characteristic clustering, regressionI. (1), where the method of changing direction of multipliers (ADMM) [1], [2]., [3]., the problems of formal x, ymps (x, y)."}, {"heading": "II. BATCH AND STOCHASTIC ADMM", "text": "As in a multiplier method, ADMM begins with the advanced Lagrangian problem (1): L (x, y, u) = 1 (x) + 1 (x) + 1 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 2 (x) + 1) (x) + 1) (x) + 1) (x) + 1) (x) + 1) (x) + 1) (x) + 1) (x) + 1) (x) + 1) (x) (x) + 1) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) + 1) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) + 1) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x) (x)"}, {"heading": "III. STOCHASTIC AVERAGE ADMM (SA-ADMM)", "text": "If one compares the update rules for x for the STOC-ADMM and OPG-ADMM ((10) and (12) with those of Batch1To avoid confusion with other stochastic variants of ADMM, this particular algorithm is subsequently called STOC-ADMM. ADMM (8), one can see that the empirical loss on the entire training set (namely 1n) is replaced by linear approximation based on a single sample plus a proximal term called \"x-xt\" + 1. This follows the standard approach taken by SGD, whereas the full gradient convergence rate has linear convergence, SGD only achieves sublinear convergence [13]. This is consistent with the result in Section II that the existing stochastic versions of the ADMM all have lower convergence rates than their batch."}, {"heading": "A. Algorithm", "text": "In the following, we assume that \"i is in (7) Lsmooth (e.g., the square loss and logistic loss).\" As in existing stochastic ADMM approaches [8], [6], [7], and [9], y and \u03b1 are still of (5), (6), and the key difference is on updating x (algorithm 1). Let's first consider the specific case in which [9], [10], and [10] in which we happen to have a sample k (t) uniform of {1, 2,., and then we update x asxt x + 1 \u2190 arg min x (x)."}, {"heading": "B. Discussion", "text": "In the specific case where it is otherwise reduced to minx \u03c6 (x) and the feasibility violation r (x, yt, \u03b1t), it can be dropped. The2Ouyang et al. [9] claimed that \u03b7t + 1 in (11) can lead to very slow convergence in practice, although there is no proof. Furthermore, \u03b7t (S-ADMM) is the most conservative (smallest) step size. [9] Therefore, the use of \u03b7t in all iterations can lead to very slow convergence in practice. [2: for t = 0, 1, for SA-ADMM).1: Initialize: x0, y0, \u03b10 and \u03c4i (\u2212 1) = 0, which can lead to very slow convergence in practice. [2: for t = 0, 1,."}, {"heading": "C. Convergence Analysis", "text": "The proposed ADMM algorithm has a comparable periteration complexity with respect to the existing stochastic versions in Section II. In this section, we show that it also has a much faster convergence rate. In ADMM's standard convergence analysis, equation (4) is used to update x [12], [8]. In the proposed algorithm, the loss and feasibility violation are linearized, making analysis more difficult. Moreover, our analysis is a non-trivial extension due to the presence of equality constraints and additional lagrange multipliers in the ADMM formula. Let's leave x x x x x x x x x x x x x x x x for a psd matrix H, Hx x x x x x."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we perform experiments on the generalized lasso model [20]: minx-D = 4 x-D = 4 x-D = 4 x-D = ADDA = ADDA = 3 x-D = 3 x-D = 3 x-D = 3 x-D = 3 x-D = 4 x-D = 4 x-D = 4 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = 5 x-D = x-D = 5 x-D = 5 x-D = 5 x-D = x-D = 5 x-D = 5 x-D = 5 x-D = x-D = 5 x-D = 5 x-D = x-D = 5 x-D = 5 x-D = x-D = 5 x-D = 5 x-D = 5 x-D = x-D = x-D = x-D = 5 x-D = 5 x-D = x-D = x-D = 5 x-D = x-D = 5 x-D = x-D = 5 x-D = x-D = 5 x-D = x-D = x-D = 5 x-D = x-D = x-D = 5 x-D = x-D = x-D = 5 x-D = 5 x-D = x-D = x-D = D"}, {"heading": "V. CONCLUSION", "text": "In this work, we have developed a novel stochastic algorithm that gradually approximates the full gradient in the linear ADMM formulation. It enjoys the same computational simplicity as existing stochastic ADMM algorithms, but has a fast convergence rate that is consistent with the batch ADMM. Empirical results on both general convex and strongly convex problems 3a9a, covertype and rcv1 come from the LIBSVM archive, quanta from the KDDCup-2004 and sido from the causality workbench website show its efficiency over batch and stochastic ADMM algorithms. In the future, we will investigate the theoretical convergence rate of the proposed algorithm on strongly convex problems."}], "references": [{"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["D. Gabay", "B. Mercier"], "venue": "Computers & Mathematics with Applications, vol. 2, no. 1, pp. 17\u201340, 1976.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Sur l\u2019approximation, par elements finis d\u2019ordre un, et la resolution, par penalisation-dualite, d\u2019une classe de problems de dirichlet non lineares", "author": ["R. Glowinski", "A. Marrocco"], "venue": "Revue Francaise d\u2019Automatique, Informatique, et Recherche Op\u00e9rationelle, vol. 9, pp. 41\u201376, 1975.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1975}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 2873\u20132908, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 2543\u20132596, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Dual averaging and proximal gradient descent for online alternating direction multiplier method", "author": ["T. Suzuki"], "venue": "Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA, USA, 2013, pp. 392\u2013400.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured sparsity via alternating direction methods", "author": ["Z. Qin", "D. Goldfarb"], "venue": "Journal of Machine Learning Research, vol. 13, pp. 1435\u2013 1468, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Online alternating direction method", "author": ["H. Wang", "A. Banerjee"], "venue": "Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, July 2012, pp. 1119\u20131126.  (a) a9a  (b) covertype (c) quantum  (d) rcv1 (e) sido Fig. 3. Objective value versus number of effective passes for the strongly convex problem.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic alternating direction method of multipliers", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA, USA, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Advanced Lectures on Machine Learning. Springer Verlag, 2004, p. 146168.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["W. Deng", "W. Yin"], "venue": "Rice University, Tech. Rep. TR12-14, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "On the O(1/t) convergence rate of alternating direction method", "author": ["B.S. He", "X.M. Yuan"], "venue": "SIAM Journal on Numerical Analysis, vol. 22, no. 4, pp. 1431\u20131448, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimization with first-order surrogate functions", "author": ["J. Mairal"], "venue": "Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA, USA, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N. Le Roux", "M. Schmidt", "F. Bach"], "venue": "Advances in Neural Information Processing Systems 25, 2012, pp. 2672\u20132680. [Online]. Available: http://books.nips.cc/papers/files/nips25/ NIPS2012 1246.pdf", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Proximal stochastic dual coordinate ascent", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "arXiv:1211.2717, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A unified primal-dual algorithm framework based on Bregman iteration", "author": ["X. Zhang", "M. Burger", "S. Osher"], "venue": "Journal of Scientific Computing, vol. 46, no. 1, pp. 20\u201346, Jan. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "On the linear convergence of the alternating direction method of multipliers", "author": ["M. Hong", "Z.-Q. Luo"], "venue": "University of Minnesota, Tech. Rep. arXiv:1208.3922, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Convex optimization with sparsity-inducing norms", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Optimization for Machine Learning, S. Sra, S. Nowozin, and S. Wright, Eds. MIT Press, 2011, pp. 19\u201353.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast iterative shrinkage-thresholding algo-  IEEE TRANSACTIONS, AUGUST 2013  7 rithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "The solution path of the generalized lasso", "author": ["R.J. Tibshirani", "J. Taylor"], "venue": "Annals of Statistics, vol. 39(3), pp. 1335\u20131371, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A multivariate regression approach to association analysis of a quantitative trait network", "author": ["S. Kim", "K.-A. Sohn", "E. Xing"], "venue": "Bioinformatics, vol. 25, no. 12, pp. i204\u2013i212, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data", "author": ["O. Banerjee", "L. El Ghaoui", "A. d\u2019Aspremont"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 485\u2013 516, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast Newton-type methods for total variation regularization", "author": ["A. Barbero", "S. Sra"], "venue": "Proceedings of the 28th International Conference on Machine Learning, New York, NY, USA, Jun. 2011, pp. 313\u2013320.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient algorithm for a class of fused lasso problems", "author": ["J. Liu", "L. Yuan", "J. Ye"], "venue": "Proceedings of the 16th International Conference on Knowledge Discovery and Data Mining, 2010, pp. 323\u2013332.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The alternating direction method of multipliers (ADMM) [1], [2], [3] considers problems of the form", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "The alternating direction method of multipliers (ADMM) [1], [2], [3] considers problems of the form", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "The alternating direction method of multipliers (ADMM) [1], [2], [3] considers problems of the form", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "In comparison with other state-of-theart optimization methods such as proximal gradient methods [4], [5], the use of ADMM has been shown to have faster convergence in several difficult structured sparse regularization problems [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "In comparison with other state-of-theart optimization methods such as proximal gradient methods [4], [5], the use of ADMM has been shown to have faster convergence in several difficult structured sparse regularization problems [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "In comparison with other state-of-theart optimization methods such as proximal gradient methods [4], [5], the use of ADMM has been shown to have faster convergence in several difficult structured sparse regularization problems [6].", "startOffset": 227, "endOffset": 230}, {"referenceID": 6, "context": "number of features and data set size are large [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Wang and Banerjee [8] first proposed the online ADMM, which learns from only one sample (or a small mini-batch) at a time.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "Very recently, three stochastic variants of ADMM are independently proposed [9], [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Very recently, three stochastic variants of ADMM are independently proposed [9], [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "Two are based on the stochastic gradient descent (SGD) [10], while one is based on regularized dual averaging (RDA) [5].", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "Two are based on the stochastic gradient descent (SGD) [10], while one is based on regularized dual averaging (RDA) [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 37, "endOffset": 40}, {"referenceID": 10, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 285, "endOffset": 289}, {"referenceID": 11, "context": "Specifically, the algorithms in [9], [6] all achieve a rate of O(1/ \u221a T ), where T is the number of iterations, for general convex problems and O(log T/T ) for strongly convex problems; whereas batch ADMM achieves convergence rates of O(1/T ) and O(\u03bc ) (where 0 < \u03bc < 1), respectively [11], [12].", "startOffset": 291, "endOffset": 295}, {"referenceID": 12, "context": "This gap in the convergence rates between stochastic and batch ADMM algorithms is indeed not surprising, as it is also observed between SGD and batch gradient descent in the analogous unconstrained optimization setting [13].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "Recently, there have been several attempts on bridging this gap [14], [13], [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Recently, there have been several attempts on bridging this gap [14], [13], [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Recently, there have been several attempts on bridging this gap [14], [13], [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "[14] proposed an approach whose per-iteration cost is as low as SGD, but can achieve linear convergence for strongly convex functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Using the scaled dual variable \u03b1t = \u03b2t/\u03c1, the ADMM update can be expressed as [1]:", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "To alleviate this problem, Wang and Banerjee [8] proposed the online ADMM that uses only one sample in each iteration.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "[9] proposed the stochastic ADMM1, which updates x as", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For the special case where B = \u2212I and c = 0, Suzuki [6] proposed a similar approach called online proximal gradient descent ADMM (OPG-ADMM), which uses the inexact Uzawa method [16] to further linearize the last term in (10), leading to", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "For the special case where B = \u2212I and c = 0, Suzuki [6] proposed a similar approach called online proximal gradient descent ADMM (OPG-ADMM), which uses the inexact Uzawa method [16] to further linearize the last term in (10), leading to", "startOffset": 177, "endOffset": 181}, {"referenceID": 5, "context": "Suzuki [6] also proposed another stochastic variant called RDA-ADMM based on the method of regularized dual averaging (RDA) [5] (again for the special case with B = \u2212I and c = 0), in which x is updated as", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "Suzuki [6] also proposed another stochastic variant called RDA-ADMM based on the method of regularized dual averaging (RDA) [5] (again for the special case with B = \u2212I and c = 0), in which x is updated as", "startOffset": 124, "endOffset": 127}, {"referenceID": 5, "context": "either the objective value [6] or a weighted combination of the objective value and feasibility violation [9].", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "either the objective value [6] or a weighted combination of the objective value and feasibility violation [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 10, "context": "However, in both cases (general and strongly convex), these convergence rates are inferior to their batch ADMM counterparts, which are O(1/T ) and O(\u03bc ) (with 0 < \u03bc < 1), respectively [11], [12], [17].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "However, in both cases (general and strongly convex), these convergence rates are inferior to their batch ADMM counterparts, which are O(1/T ) and O(\u03bc ) (with 0 < \u03bc < 1), respectively [11], [12], [17].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "However, in both cases (general and strongly convex), these convergence rates are inferior to their batch ADMM counterparts, which are O(1/T ) and O(\u03bc ) (with 0 < \u03bc < 1), respectively [11], [12], [17].", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "However, as is well-known, while the full gradient descent has linear convergence rate, SGD only achieves sublinear convergence [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "A pioneering approach along this line is the stochastic average gradient (SAG) [14], which considers the optimization of a strongly convex sum of smooth functions (minx 1 n \u2211n i=1 `i(x)).", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "Another closely related approach is the minimization by incremental surrogate optimization (MISO) [13], which replaces each `i by some \u201csurrogate\u201d function in an incremental manner similar to SAG.", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "As in existing stochastic ADMM approaches [8], [9], [6], y and \u03b1 are still updated by (5), (6), and the key difference is on the update of x (Algorithm 1).", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "As in existing stochastic ADMM approaches [8], [9], [6], y and \u03b1 are still updated by (5), (6), and the key difference is on the update of x (Algorithm 1).", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "As in existing stochastic ADMM approaches [8], [9], [6], y and \u03b1 are still updated by (5), (6), and the key difference is on the update of x (Algorithm 1).", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "A technique that has been popularly used in the recent ADMM literature is the inexact Uzawa method [16], which uses (2) to approximate r(x, yt, \u03b1t) by its upper bound:", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "where \u2207xr \u2261 \u03c1A (Axt +Byt \u2212 c+ \u03b1t) and LA is an upper bound on the eigenvalues of \u03c1AA [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "This is the standard proximal step popularly used in optimization problems with structured sparsity [18], [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "This is the standard proximal step popularly used in optimization problems with structured sparsity [18], [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": ", \u03a9(x) = \u2016x\u20161, \u2016x\u20162, \u2016x\u2016\u221e and various mixed norms [4]).", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "[9] claimed that \u03b7t+1 in (11) can be fixed at \u03b7T , though the proof is missing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "update rule in (15) then reduces to MISO using the Lipschitz gradient surrogate; and (20) corresponds to the proximal gradient surrogate [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "When \u03c8 6= 0, (17) can be regarded as a quadratic surrogate [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "proximal) gradient surrogate and quadratic surrogate, which can be easily seen to be another surrogate function in the sense of [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "However, unlike the proximal step in (22), this does not exploit the structure of \u03c6 and subgradient descent often has slow empirical convergence [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Moreover, the radius of the parameter space is also increased, leading to bigger constants in the big-Oh notation of the convergence rate [9], [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "Moreover, the radius of the parameter space is also increased, leading to bigger constants in the big-Oh notation of the convergence rate [9], [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 11, "context": "In the standard convergence analysis of ADMM, equation (4) is used for updating x [12], [8].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "In the standard convergence analysis of ADMM, equation (4) is used for updating x [12], [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "As in [9], we first establish convergence rates of the (x\u0304T , \u0233T ) solution in terms of a combination of the objective value and feasibility violation (weighted by \u03b3 > 0).", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "As discussed in [6], when B is invertible, this can be alleviated by obtaining y from x\u0304T as y(x\u0304T ) = B\u22121(c\u2212 Ax\u0304T ).", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "In this section, we perform experiments on the generalized lasso model [20]: minx\u2208Rd 1 n \u2211n i=1 `i(x) + \u03bb\u2016Ax\u20161, where A \u2208 Rm\u00d7d (for some m > 0) is a penalty matrix specifying the desired structured sparsity pattern of x.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "Here, we will focus on the graph-guided fused lasso [21], whose sparsity pattern is specified by a graph defined on the d variates of x.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "Following [9], we obtain this graph by sparse inverse covariance selection [22], [1].", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "Following [9], we obtain this graph by sparse inverse covariance selection [22], [1].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "Following [9], we obtain this graph by sparse inverse covariance selection [22], [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 22, "context": "While proximal methods have been used in the optimization of graph-guided fused lasso [23], [24], in general, the underlying proximal step does not have a closed-form because of the presence of A.", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "While proximal methods have been used in the optimization of graph-guided fused lasso [23], [24], in general, the underlying proximal step does not have a closed-form because of the presence of A.", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "n \u2211n i=1 `i(x), \u03c8(y) = \u03bb\u2016y\u20161 and with constraint Ax = y, has been shown to be more efficient [9], [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "n \u2211n i=1 `i(x), \u03c8(y) = \u03bb\u2016y\u20161 and with constraint Ax = y, has been shown to be more efficient [9], [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "1) two variants of the proposed method: i) SA-ADMM, which uses update rule (16); and ii) SA-IU-ADMM, which uses (19)) based on the inexact Uzawa method; 2) three existing stochastic ADMM algorithms: i) STOCADMM [9]; ii) OPG-ADMM [6]; and iii) RDA-ADMM [6]; 3) two deterministic ADMM variants: i) batch-ADMM, which is the batch version of SA-ADMM using full gradient (i.", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "1) two variants of the proposed method: i) SA-ADMM, which uses update rule (16); and ii) SA-IU-ADMM, which uses (19)) based on the inexact Uzawa method; 2) three existing stochastic ADMM algorithms: i) STOCADMM [9]; ii) OPG-ADMM [6]; and iii) RDA-ADMM [6]; 3) two deterministic ADMM variants: i) batch-ADMM, which is the batch version of SA-ADMM using full gradient (i.", "startOffset": 229, "endOffset": 232}, {"referenceID": 5, "context": "1) two variants of the proposed method: i) SA-ADMM, which uses update rule (16); and ii) SA-IU-ADMM, which uses (19)) based on the inexact Uzawa method; 2) three existing stochastic ADMM algorithms: i) STOCADMM [9]; ii) OPG-ADMM [6]; and iii) RDA-ADMM [6]; 3) two deterministic ADMM variants: i) batch-ADMM, which is the batch version of SA-ADMM using full gradient (i.", "startOffset": 252, "endOffset": 255}, {"referenceID": 7, "context": "We do not compare with the online ADMM [8] or a direct application of batch ADMM, as they require nonlinear optimization for the update of x.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Moreover, it has been shown that the online ADMM is slower than RDA-ADMM [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Experiments are performed on five binary classification data sets:3 a9a, covertype, quantum, rcv1, and sido (Table II), which have been commonly used [14], [6].", "startOffset": 150, "endOffset": 154}, {"referenceID": 5, "context": "Experiments are performed on five binary classification data sets:3 a9a, covertype, quantum, rcv1, and sido (Table II), which have been commonly used [14], [6].", "startOffset": 156, "endOffset": 159}], "year": 2013, "abstractText": "In this paper, we propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, the proposed algorithm improves the convergence rate on convex problems from O ( 1 \u221a T ) to O ( 1 T ) , where T is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.", "creator": "LaTeX with hyperref package"}}}