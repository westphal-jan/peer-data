{"id": "1308.4828", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2013", "title": "The Sample-Complexity of General Reinforcement Learning", "abstract": "We present a new algorithm for general reinforcement learning where the true environment is known to belong to a finite class of N arbitrary models. The algorithm is shown to be near-optimal for all but O(N log^2 N) time-steps with high probability. Infinite classes are also considered where we show that compactness is a key criterion for determining the existence of uniform sample-complexity bounds. A matching lower bound is given for the finite case.", "histories": [["v1", "Thu, 22 Aug 2013 11:39:06 GMT  (18kb)", "http://arxiv.org/abs/1308.4828v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "marcus hutter", "peter sunehag"], "accepted": true, "id": "1308.4828"}, "pdf": {"name": "1308.4828.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Peter Sunehag"], "emails": ["tor.lattimore@anu.edu.au", "marcus.hutter@anu.edu.au", "peter.sunehag@anu.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 130 8,48 28v1 [cs.LG] 2 2Table of Contents"}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Notation 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Finite Case 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Compact Case 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Unbounded Environment Classes 12", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Lower Bound 13", "text": "7 Conclusions 13 References 14 A Technical Results 15 B Constants 16 C Notation Table 16Keywords Reinforcement Learning; Sample Complexity; Exploration Exploitation."}, {"heading": "1 Introduction", "text": "It is not the first time in recent years that we have opted for a very specific class of environments in which it receives a reward, and an observation o that is generated by the environment and possibly depends on the whole story. We present a new enhancement of learning, called Maximum Exploration Learning (MERL), which is accepted as the input of an endless sequence of learning processes. (We have a new learning algorithm called Maximum Exploration Learning (MERL), which adopts a near-optimal sequence of learning processes. (We introduce a new type of learning process in which we apply a very specific class of environments that receives a reward and an observation o that is generated by the environment and arbitrarily depends on the entire story.) We present a new enhancement of learning, called Maximum Exploration Learning (MERL), which takes as input a near-optimal sequence of learning processes."}, {"heading": "2 Notation", "text": "The definition of environment is borrowed from the work of Hutter [2005], although the notation is somewhat more formal to simplify the application of the Martyrium Inequality. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3 Finite Case", "text": "The Maximum Exploration Reinforcement Algorithm is model-based in the sense that it knows all plausible environments reasonably well and otherwise explores them. Essentially, this method of exploration guarantees that MERL is nearly optimal whenever it is used and that the number of exploration phases is highly likely to be limited. The main difficulty is to specify what it means to be plausible. Previous authors working in finite environments, such as MDPs or bandits, have removed models for which the transition probabilities are not close enough to their empirical estimates. In the more general setting of this approach, states (histories) fail, so that sufficient empirical estimates cannot be gathered."}, {"heading": "4 Compact Case", "text": "In the last section, we presented MERL and demonstrated a sample complexity that is bound to the case when the ambient class is finite. In this section, we show that if the number of environments is unlimited, but compact with respect to the topology generated by a natural metric, the boundaries of sample complexity are still possible with a slight modification of MERL. The basic idea is to use the compactness to cover the space of environments with such spheres and to compute statistics about these spheres and not about individual environments. Since all environments in the same environments are sufficiently close, the resulting statistics may not be significantly different and all analyses go identical in the finite case. Define a topology to the space of all environments that are induced by the pseudo-metricd (1, 2): = sup h, p h, p h, p, p, p, p, p, p, p, h, p, p, h, h, h, h, h, h, h, h, h, h, p, h, p, u, u, u, p, p, p, u, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, u, p, p, p, p, p, p, p, p, p, p, p, p, p, p, u, p, p, p, p, p, p, u, p, p, p, p, p, p, p, u, p, p, p, p, p, p, p, u, p, p, p, p, p, p, p, p, u, p, p, p, p, p, p, p, p, p, p, u, p, p, p, p, p, p, p, p, p, u, p, p, p, p, p, p, p, p, p, p, u, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "5 Unbounded Environment Classes", "text": "Theorem 7. There is no compact M for which no agent has a finite PAC boundary. An obvious example is if M is the set of all environments, then for each policy M contains an environment designed to ensure that the policy is infinitely often suboptimal. A more interesting example is the class of all computable environments, which is not compact and does not allow algorithms with uniform finite sample complexity. See negative results from Lattimore and Hutter [2011b] for counter examples."}, {"heading": "6 Lower Bound", "text": "In certain cases, the boundary in Theorem 1 is very weak, while it is known that the true dependence is at most square. This should not be surprising, since information about the transitions for a state provides information about a large subset of M, not just a single environment. We show that the boundary in Theorem 1 is incorrigible for general environmental classes except for logarithmic factors. That is, there is a class of environments in which Theorem 1 is nearly dense. The simplest counterexample is a series of MDPs with four states, S = {0, 1, 1,} and N actions, A = {a1, \u00b7, aN}. The rewards and transitions are shown in Figure 3, in which the probabilities of transition depend on the action: Let us leave M = imperative, and N actions, A = {a1, \u00b7, aN} in the environment in which we (1) are fixed."}, {"heading": "7 Conclusions", "text": "Summary: The Maximum Exploration Reinforcement Learning algorithm was presented, and for finite classes of any environment, a sample complexity limit was set that is linear in the number of environments. We also presented lower limits that show that this can generally only be improved by logarithmic factors. Learning is also possible for compact classes with sample complexity depending on the size of the smallest coverage, where the distance between two environments is the difference in value functions across all policies and history sequences. Finally, sample complexity limits are typically not possible for non-compact classes of environments. Runtime of MERL can be arbitrarily large, as the compression of policy is the maximization of the environmental class used. Even considering the distribution of observation / reward classes in light of history, sample complexity limits can be compressed in constant time."}, {"heading": "M. Azar, R. Munos, and B. Kappen. On the sample complexity of reinforcement learning with a generative", "text": "Model. In Proceedings of the 29th International Conference on Machine Learning, New York, NY, USA, 2012. ACM."}, {"heading": "D. Chakraborty and P. Stone. Structure learning in ergodic factored mdps without knowledge of the", "text": "In Proceedings of the Twenty Eighth International Conference on Machine Learning (ICML '11), 2011. C. Diuk, L. Li, and B. Leffler. The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning. In Andrea Pohoreckyj Danyluk, Le \u0301 on Bottou, and Michael L. Littman, editors, Proceedings of the 26th Annual International Conference on Machine Learning (ICML 2009), pp. 249-256. ACM, 2009."}, {"heading": "E. Even-dar, S. Kakade, and Y. Mansour. Reinforcement learning in POMDPs without resets. In In", "text": "IJCAI, pages 690-695, 2005."}, {"heading": "M. Hutter. Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures.", "text": "In Proc. 15th Annual Conf. on Computational Learning Theory (COLT '02), Volume 2375 of LNAI, pp. 364-379, Sydney, 2002. Springer, Berlin. URL http: / / arxiv.org / abs / cs.AI / 0204040.M. Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin, 2005. URL http: / / www.hutter1.net / ai / uaibook.htm."}, {"heading": "T. Lattimore and M. Hutter. Time consistent discounting. In Jyrki Kivinen, Csaba Szepesva\u0301ri, Esko", "text": "Ukkonen and Thomas Zeugmann, Editor, Algorithmic Learning Theory, Volume 6925 of Lecture Notes in Computer Science. Springer Berlin / Heidelberg, 2011."}, {"heading": "T. Lattimore and M. Hutter. Asymptotically optimal agents. In Jyrki Kivinen, Csaba Szepesva\u0301ri, Esko", "text": "Ukkonen and Thomas Zeugmann, Editor, Algorithmic Learning Theory, Volume 6925 of Lecture Notes in Computer Science. Springer Berlin / Heidelberg, 2011b."}, {"heading": "T. Lattimore and M. Hutter. PAC bounds for discounted MDPs. Technical report, 2012. http://torlattimore.com/pubs/pac-tech.pdf.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S. Mannor and J. Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. J.", "text": "Mach. Learn. Res., 5: 623-648, December 2004. ISSN 1532-4435."}, {"heading": "M. Odalric-Ambrym, P. Nguyen, R. Ortner, and D. Ryabko. Optimal regret bounds for selecting the state", "text": "In Proceedings of the Thirtieth International Conference on Machine Learning (ICML '13), 2013.D. Ryabko and M. Hutter. On the possible of learning in reactive environment with arbitrary dependence. Theoretical Computer Science, 405 (3): 274-284, 2008.A. Strehl and M. Littman. A theoretical analysis of model-based interval estimation. In Proceedings of the 22nd International Conference on Machine Learning, ICML' 05, pp. 856-863, 2005.A. Strehl, L. Li, and M. Littman. Reinforcement learning in finite MDPs: PAC analysis. J. Mach. Learn. Res., 10: 2413-2444, December 2009.P. Sunehag and M. Hutter. Optimistic agents are asymptotically optimal. In Proceedings of the 25th Australasian AI conference, 2012."}, {"heading": "I. Szita and C. Szepesva\u0301ri. Model-based reinforcement learning with nearly tight exploration complexity", "text": "In the Proceedings of the 27th International Conference on Machine Learning, pp. 1031-1038, New York, NY, USA, 2010. ACM."}, {"heading": "A Technical Results", "text": "The result is largely due to the fact that a maximum is greater than an average."}, {"heading": "C Table of Notation", "text": "N Number of Candidate ModelsRequires Accuracy \u03b4 Probability that an algorithm makes more mistakes than its sample complexity in time steps tV \u03c0\u00b5 (h) Value of policy \u03c0 in the environment \u00b5 given the history of effective horizontal environmentsEmax high probability tied to number of exploration phasesE \u221e Number of exploration phasesE \u221e Number of exploration phasesE \u221e Number of exploration phasesE \u2015 (0, 1) Number of Kong exploration phasesEt (\u03bd, \u0435) Number of (\u03bd, \u0435) exploration phasesE \u2015 Number of effective exploration phases in time steps tX \u2012 (\u03bd, \u0432) i iith Test StatisticsNumber of exploration phases in time steps tFt \u2012"}], "references": [{"title": "Near-optimal regret bounds for reinforcement learning", "author": ["P. Auer", "T. Jaksch", "R. Ortner"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2010}, {"title": "On the sample complexity of reinforcement learning with a generative", "author": ["Learn. Res"], "venue": null, "citeRegEx": "Res.,? \\Q2010\\E", "shortCiteRegEx": "Res.", "year": 2010}, {"title": "Reinforcement learning in POMDPs without resets", "author": ["E. Even-dar", "S. Kakade", "Y. Mansour"], "venue": null, "citeRegEx": "Even.dar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Even.dar et al\\.", "year": 2009}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "Time consistent discounting", "author": ["Springer", "Berlin"], "venue": null, "citeRegEx": "Springer and Berlin,? \\Q2005\\E", "shortCiteRegEx": "Springer and Berlin", "year": 2005}, {"title": "Optimal regret bounds for selecting the state", "author": ["Mach. Learn"], "venue": null, "citeRegEx": "Learn.,? \\Q2004\\E", "shortCiteRegEx": "Learn.", "year": 2004}, {"title": "Optimistic agents are asymptotically optimal", "author": ["P. Sunehag", "M. Hutter"], "venue": null, "citeRegEx": "Sunehag and Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "More recent work on closing the gap between upper and lower bounds is by Szita and Szepesv\u00e1ri [2010], Lattimore and Hutter [2012], Azar et al.", "startOffset": 116, "endOffset": 130}, {"referenceID": 1, "context": "More recent work on closing the gap between upper and lower bounds is by Szita and Szepesv\u00e1ri [2010], Lattimore and Hutter [2012], Azar et al. [2012]. In the undiscounted case it is necessary to make some form of ergodicity assumption as without this regret bounds cannot be given.", "startOffset": 116, "endOffset": 150}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited.", "startOffset": 62, "endOffset": 81}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in.", "startOffset": 62, "endOffset": 229}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in. Even-dar et al. [2005] give essentially unimprovable exponential bounds on the sample-complexity of learning in finite partially observable MDPs.", "startOffset": 62, "endOffset": 277}, {"referenceID": 0, "context": "Nevertheless, our algorithm borrows some tricks used by UCRL2 Auer et al. [2010]. Previous work for more general environment classes is somewhat limited. For factored MDPs there are known bounds, see Chakraborty and Stone [2011] and references there-in. Even-dar et al. [2005] give essentially unimprovable exponential bounds on the sample-complexity of learning in finite partially observable MDPs. Odalric-Ambrym et al. [2013] show regret bounds", "startOffset": 62, "endOffset": 429}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b].", "startOffset": 191, "endOffset": 205}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b].", "startOffset": 191, "endOffset": 231}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b].", "startOffset": 191, "endOffset": 258}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b]. Perhaps the closest related worked is Diuk et al.", "startOffset": 191, "endOffset": 317}, {"referenceID": 3, "context": "As far as we know there has been no work on the sample-complexity of RL when environments are completely general, but asymptotic results have garnered some attention with positive results by Hutter [2002], Ryabko and Hutter [2008], Sunehag and Hutter [2012] and (mostly) negative ones by Lattimore and Hutter [2011b]. Perhaps the closest related worked is Diuk et al. [2009], which deals with a similar problem in the rather different setting of learning the optimal predictor from a class of N experts.", "startOffset": 191, "endOffset": 375}, {"referenceID": 3, "context": "2 Notation The definition of environments is borrowed from the work of Hutter [2005], although the notation is slightly more formal to ease the application of martingale inequalities.", "startOffset": 71, "endOffset": 85}, {"referenceID": 3, "context": "2 Notation The definition of environments is borrowed from the work of Hutter [2005], although the notation is slightly more formal to ease the application of martingale inequalities. General. N = {0, 1, 2, \u00b7 \u00b7 \u00b7 } is the natural numbers. For the indicator function we write [[x = y]] = 1 if x = y and 0 otherwise. We use \u2227 and \u2228 for logical and/or respectively. If A is a set then |A| is its size and A\u2217 is the set of all finite strings (sequences) over A. If x and y are sequences then x \u228f y means that x is a prefix of y. Unless otherwise mentioned, log represents the natural logarithm. For random variable X we write EX for its expectation. For x \u2208 R, \u2308x\u2309 is the ceiling function. Environments and policies. Let A, O and R \u2282 R be finite sets of actions, observations and rewards respectively and H := A \u00d7 O \u00d7 R. H\u221e is the set of infinite history sequences while H\u2217 := (A \u00d7 O \u00d7 R)\u2217 is the set of finite history sequences. If h \u2208 H\u2217 then l(h) is the number of action/observation/reward tuples in h. We write at(h), ot(h), rt(h) for the tth action/observation/reward of history sequence h. For h \u2208 H\u2217, \u0393h := {h\u2032 \u2208 H\u221e : h \u228f h\u2032} is the cylinder set. Let F := \u03c3({\u0393h : h \u2208 H\u2217}) and Ft := \u03c3({\u0393h : h \u2208 H\u2217 \u2227 l(h) = t}) be \u03c3-algebras. An environment \u03bc is a set of conditional probability distributions over observation/reward pairs given the history so far. A policy \u03c0 is a function \u03c0 : H\u2217 \u2192 A. An environment and policy interact sequentially to induce a measure, P\u03bc,\u03c0, on filtered probability space (H\u221e,F , {Ft}). For convenience, we abuse notation and write P\u03bc,\u03c0(h) := P\u03bc,\u03c0(\u0393h). If h \u228f h \u2032 then conditional probabilities are P\u03bc,\u03c0(h \u2032|h) := P\u03bc,\u03c0(h\u2032)/P\u03bc,\u03c0(h). Rt(h; d) := \u2211t+d k=t \u03b3 k\u2212trk(h) is the d-step return function and Rt(h) := limd\u2192\u221e Rt(h; d). Given history ht with l(ht) = t, the value function is defined by V \u03c0 \u03bc (ht; d) := E[Rt(h; d)|ht] where the expectation is taken with respect to P\u03bc,\u03c0(\u00b7|ht). V \u03c0 \u03bc (ht) := limd\u2192\u221e V \u03c0 \u03bc (ht; d). The optimal policy for environment \u03bc is \u03c0 \u2217 \u03bc := argmax\u03c0 V \u03c0 \u03bc , which with our assumptions is known to exist Lattimore and Hutter [2011a]. The value of the optimal policy is V \u2217 \u03bc := V \u03c0\u2217 \u03bc \u03bc .", "startOffset": 71, "endOffset": 2078}, {"referenceID": 0, "context": "This is comparable to the updating of confidence intervals for algorithms such as MBIE (Strehl and Littman, 2005) or UCRL2 (Auer et al., 2010).", "startOffset": 123, "endOffset": 142}, {"referenceID": 3, "context": "See negative results by Lattimore and Hutter [2011b] for counter-examples.", "startOffset": 38, "endOffset": 53}, {"referenceID": 3, "context": "[2009] and Lattimore and Hutter [2012] to show that no algorithm has sample-complexity less than O( N \u01eb2(1\u2212\u03b3)3 log 1 \u03b4 ).", "startOffset": 25, "endOffset": 39}], "year": 2013, "abstractText": "We present a new algorithm for general reinforcement learning where the true environment is known to belong to a finite class of N arbitrary models. The algorithm is shown to be near-optimal for all but O(N log N) time-steps with high probability. Infinite classes are also considered where we show that compactness is a key criterion for determining the existence of uniform sample-complexity bounds. A matching lower bound is given for the finite case.", "creator": "LaTeX with hyperref package"}}}