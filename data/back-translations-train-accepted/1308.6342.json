{"id": "1308.6342", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2013", "title": "Linear and Parallel Learning of Markov Random Fields", "abstract": "We introduce a new algorithm to learn the potentials of a large class of Markov Random Fields (MRFs) with exact and efficient inference. If every node in the MRF has a bounded number of neighbors, the proposed algorithm has linear complexity in the number of nodes in the MRF. For example, for a fully observed square lattice MRF, parameter estimation using the junction tree inference engine has exponential complexity, but our algorithm only has linear complexity. Moreover, the algorithm is naturally parallel and hence applicable to large scale data modeling. The algorithm applies to many practical MRFs of great interest, including 2D and 3D lattices, chimera models, and skip-chain conditional random fields.", "histories": [["v1", "Thu, 29 Aug 2013 01:55:37 GMT  (127kb,D)", "https://arxiv.org/abs/1308.6342v1", null], ["v2", "Tue, 1 Oct 2013 15:04:57 GMT  (130kb,D)", "http://arxiv.org/abs/1308.6342v2", null], ["v3", "Thu, 3 Oct 2013 15:05:06 GMT  (130kb,D)", "http://arxiv.org/abs/1308.6342v3", null], ["v4", "Wed, 5 Feb 2014 17:59:18 GMT  (391kb,D)", "http://arxiv.org/abs/1308.6342v4", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yariv dror mizrahi", "misha denil", "nando de freitas"], "accepted": true, "id": "1308.6342"}, "pdf": {"name": "1308.6342.pdf", "metadata": {"source": "META", "title": "Linear and Parallel Learning of Markov Random Fields", "authors": ["Yariv Dror Mizrahi", "Misha Denil", "Nando de Freitas"], "emails": ["yariv@math.ubc.ca", "misha.denil@cs.ox.ac.uk", "nando@cs.ox.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think they are able to survive themselves. \""}, {"heading": "2. Model Specification and Learning Objectives", "text": "We are interested in estimating the parameter vector \u03b8 of a positive distribution p (x | \u03b8) > 0 that meets the Markov properties of an undirected graph G. That is, a distribution that can be represented as a product of factors, one per maximum clique, p (x | \u03b8) = 1 Z (\u03b8), and Z (\u03b8) is the partition function specified by Z (\u03b8) = 1. (2) In such models, we often use exponential functions to represent the potentials associated with the variables in Clique c, and Z (\u03b8) is the partition function specified by Z (\u03b8) = 1, (3) in which E (xc | \u03b8c).R are used as exponential functions to represent the potentials."}, {"heading": "2.1. Maximum Likelihood", "text": "There is (generally) no complete form solution for estimating the parameters of an MRF, therefore gradient-based optimizers are required.Consider the fully observed maximum entropy model (x | \u03b8) = 1 Z (\u03b8) exp (\u2211 c \u03b8Tc \u03c6c (x)) (4), where c indexes the maximum cliques.The scaled log probability is derived from \"(\u03b8) = 1N \u2211 n = 1 log p (xn | \u03b8) = 1N \u2211 = 1 [xn) \u2212 logZ (\u03b8)], which is a convex function of the previous model.The derivation for the parameters of a certain clique, q, results from:\" throuq = 1N \u2211 = 1 [xn] \u0432\u0430\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "2.2. Maximum Pseudo-Likelihood", "text": "In order to overcome the persistent problem of computational expectations about the model distribution, the pseudo probability considers a simpler factorized objective function: \"PL (\u03b8) = 1N N N \u2211 n = 1 M \u2211 m = 1 log p (xmn | x \u2212 mn, \u03b8) (8), where x \u2212 mn denotes all components of the n-th data vector, except component m. (For models with sparse connectivity, we only have to condition the neighbors of the node m.) In the binary, log-linear case, the gradient of this object can be expressed in a contrasting form, namely in the form that x-mn is the data vector x-mn with the inverted th bit, i.e. x-imn = 1 \u2212 xmn, if i-mn (x-mn) and otherwise xmn (2010)."}, {"heading": "2.3. Model and Data Efficiency", "text": "There are two terms in the gradient of equation 7. The first term is an empirical expectation q, 1N \u2211 N = 1 \u03c6q (xn), and depends only on the data. The value of this term for each clique can be calculated before the start of parameter optimization, which makes this term of the gradient extremely cheap to evaluate during optimization. The data term in the maximum probability gradient is contrasted with an expectation of the model distribution E [\u03c6q (x) | \u03b8], which is a sum of exponentially many configurations. For large models, this term is intractable. We describe this situation by saying that the maximum probability estimate is efficient, because the terms that cover only the data can be efficiently calculated. However, the maximum probability is not effective for the model, since the model term in the gradient x is intractable, and the difficulty to evaluate it is the primary motivation for developing alternative objectives such as pseudo-probability."}, {"heading": "3. Algorithm Description", "text": "The LAP algorithm works by dividing the problem of common parameter estimation into several independent sub-problems q q, which can be solved in parallel. Once the sub-problems are solved, it combines the solutions for each sub-problem together to solve the whole problem. However, for a fixed clique q, we define its 1 neighborhood Aq = c-q 6 = \u2205 c, in order to contain all variables of q itself as well as the variables with at least one neighbor in q.LAP. We assume that we have a sub-problem for each maximum clique in the original problem by defining an additional MRF, Mq, via the variables in Aq. Details of the construction of the auxiliary MRF will be discussed later, because now we assume that we have an auxiliary clique on Aq and that it contains a clique about the variables in q, which is parameterized in the same way as q in the original problem."}, {"heading": "3.1. Construction of the Auxiliary MRF", "text": "The effectiveness of LAP stems from the correct construction of the utility MRF, Mq. As already mentioned, Mq must contain the clique q, which must be parameterized in the same way as in the common model. This requirement is clear from the previous section, otherwise the last step in algorithm 1 would be invalid. In the section of the analysis, we will see that it is desirable for Mq to be as close as possible to the boundary distribution on xAq. This means that we must include all cliques from the original MRF that are subsets of Aq. In addition, marginalization could introduce additional cliques that are not present in the original common distribution. It is clear that these cliques can only contain variables in Aq\\ q, but determining their exact structure in general is difficult. We consider three strategies for constructing utilities MRFs that differ in the way they generate clique structures on Aq\\ q."}, {"heading": "4. Experiments", "text": "In this section, we describe some experiments designed to show that the LAP estimator has good empirical performance. We focus on small models where the exact maximum probability can be traced so that performance can be measured. We decided to focus our experiments on demonstrating accuracy rather than on scalability, because the scaling and data efficiency characteristics of LAP are obvious. The purpose of the experiments in this section is to show two things: 1. The accuracy of the LAP estimates is no worse than their main competitor, pseudo-probability; and 2. LAP achieves good performance even when the exact marginal structure is not used. In all of our experiments, we compare the pseudo-probability estimate against LAP with the three different strategies for constructing the auxiliary structure discussed in the previous section. In each diagram, the lines of pseudo-probabilities marked with PL correspond to the maximum probability."}, {"heading": "5. Theory", "text": "In this section, we show that matching parameters in the connection and boundary distributions are valid, provided the parameters are chosen correctly, and we then prove the consistency of the LAP algorithm and illustrate the connection to maximum probability. Undirected probabilistic graphical models can be specified locally in terms of Markov properties and conditional independence, and globally in terms of an energy function \u2211 cE (xc | \u03b8c). The Hammersley-Clifford theorem (Hammersley & Clifford, 1971) establishes the equivalence of these two representations. An important fact that is often overlooked is that the energy function and partition function are not unique, but it is possible to achieve uniqueness for both functions by imposing normalization in terms of setting the random variables of potential."}, {"heading": "5.1. The LAP Argument", "text": "Suppose we have a Gibbs distribution p (xS | \u03b8), whose factors correspond to the clique system C, and let q \u00b2 C be a specific clique of interest. Letp (xAq | \u03c6) = 1Z (\u03c6) exp (\u2212 \u2211 c \u00b2 Cq E (xc | \u03c6c))), the boundary distribution is parameterized to Aq (with clique system Cq), so that the potentials are normalized with respect to zero. We can also write the boundary distribution p (\u2212 E (xAq | \u0445 q) = \u2211 S\\ Aq (xS | \u03b8) = 1Z (\u03b8) \u2211 S\\ Aq exp (\u2212 \u0445 C (xc | \u03b8c)) = 1Z (\u03b8) exp (\u2212 E (xq | \u03b8q) \u2212 \u0445q)."}, {"heading": "5.2. Consistency of LAP", "text": "Suppose we have N samples taken from this distribution. Suppose that the ML parameters are the ML parameters for \u03b8 in view of the data and the corresponding LAP estimate. Let q \u00b2 C be an arbitrary interest clique. Suffice it to show that it is the true parameter of the boundary distributions via xAq in normalized form, i.e. that q \u00b2 C is an arbitrary interest clique. Suffice it to show that it is the true parameter of the boundary distributions via xAq, i.e. that q \u00b2 C is an arbitrary interest clique. Suffice it to show that it is the true parameter of the boundary distributions via xAq in normalized form, i.e. that q \u00b2 C is an arbitrary interest clique."}, {"heading": "5.3. Relationship to maximum likelihood", "text": "We prove here that under certain (strong) premises we are exactly the same as the probability that distributional justice will occur. (...) The main result is that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that distributional justice (...) is so high that..."}, {"heading": "6. Conclusion", "text": "We have presented a distributed learning algorithm for practical MRFs, in which the parameters of each clique can be estimated in different machines; the algorithm is also data efficient in loglinear models, since estimating each clique parameter requires only access to sufficient local statistics of the data; these statistics are not only local in the 1-neighborhoods of each clique, but can also be predicted; our experiments suggest that the LAP estimators behave similarly to pseudo-probabilities and maximum probabilities for large sample sizes; however, these alternative estimators do not enjoy the same data and model efficiencies as LAP. Finally, we have proven that the proposed estimator.The work of Meng and colleagues considered only Gauss's graphic models likely to be the result of their linear algebra relaxation techniques; however, it seems feasible to apply their algorithm to the individual case. Comparing the two techniques is an immediate direction for future work on combining the different menu techniques would also provide an improvement."}], "references": [{"title": "A learning algorithm for Boltzmann machines", "author": ["D.H. Ackley", "G. Hinton", "Sejnowski"], "venue": "Cognitive Science,", "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Interactive digital photomontage", "author": ["A. Agarwala", "M. Dontcheva", "M. Agrawala", "S. Drucker", "A. Colburn", "B. Curless", "D. Salesin", "M. Cohen"], "venue": "In ACM SIGGRAPH,", "citeRegEx": "Agarwala et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Agarwala et al\\.", "year": 2004}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society. Series D,", "citeRegEx": "Besag,? \\Q1975\\E", "shortCiteRegEx": "Besag", "year": 1975}, {"title": "On sparse, spectral and other parameterizations of binary probabilistic models", "author": ["D. Buchman", "M.W. Schmidt", "S. Mohamed", "D. Poole", "N. de Freitas"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Buchman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Buchman et al\\.", "year": 2012}, {"title": "Toward the implementation of a quantum RBM", "author": ["M. Denil", "N. de Freitas"], "venue": "In NIPS Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Denil and Freitas,? \\Q2011\\E", "shortCiteRegEx": "Denil and Freitas", "year": 2011}, {"title": "Maximum likelihood estimation in log-linear models", "author": ["S.E. Fienberg", "A. Rinaldo"], "venue": "The Annals of Statistics,", "citeRegEx": "Fienberg and Rinaldo,? \\Q2012\\E", "shortCiteRegEx": "Fienberg and Rinaldo", "year": 2012}, {"title": "Introduction to random fields", "author": ["D. Griffeath"], "venue": "In Denumerable Markov Chains,", "citeRegEx": "Griffeath,? \\Q1976\\E", "shortCiteRegEx": "Griffeath", "year": 1976}, {"title": "Markov fields on finite graphs and lattices", "author": ["J.M. Hammersley", "P. Clifford"], "venue": "Unpublished manuscript,", "citeRegEx": "Hammersley and Clifford,? \\Q1971\\E", "shortCiteRegEx": "Hammersley and Clifford", "year": 1971}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2000\\E", "shortCiteRegEx": "Hinton", "year": 2000}, {"title": "Estimation of non-normalized statistical models using score", "author": ["A. Hyv\u00e4rinen"], "venue": "matching. JMLR,", "citeRegEx": "Hyv\u00e4rinen,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen", "year": 2005}, {"title": "An introduction to probabilistic graphical models", "author": ["M.I. Jordan"], "venue": null, "citeRegEx": "Jordan,? \\Q2002\\E", "shortCiteRegEx": "Jordan", "year": 2002}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "In ICML, pp", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Graphical models", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Markov random field modeling in image analysis", "author": ["S.Z. Li"], "venue": null, "citeRegEx": "Li,? \\Q2001\\E", "shortCiteRegEx": "Li", "year": 2001}, {"title": "Distributed parameter estimation via pseudo-likelihood", "author": ["Q. Liu", "A. Ihler"], "venue": "In ICML,", "citeRegEx": "Liu and Ihler,? \\Q2012\\E", "shortCiteRegEx": "Liu and Ihler", "year": 2012}, {"title": "Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood", "author": ["B. Marlin", "N. de Freitas"], "venue": "In UAI,", "citeRegEx": "Marlin and Freitas,? \\Q2011\\E", "shortCiteRegEx": "Marlin and Freitas", "year": 2011}, {"title": "Inductive principles for restricted Boltzmann machine learning", "author": ["B. Marlin", "K. Swersky", "B. Chen", "N. de Freitas"], "venue": "In AIStats,", "citeRegEx": "Marlin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2010}, {"title": "Distributed learning of Gaussian graphical models via marginal likelihoods", "author": ["Z. Meng", "D. Wei", "A. Wiesel", "A.O. Hero III"], "venue": "In AIStats,", "citeRegEx": "Meng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2013}, {"title": "Marginal likelihoods for distributed parameter estimation of Gaussian graphical models", "author": ["Z. Meng", "D. Wei", "A. Wiesel", "A.O. Hero III"], "venue": "Technical report,", "citeRegEx": "Meng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2014}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "Murphy,? \\Q2012\\E", "shortCiteRegEx": "Murphy", "year": 2012}, {"title": "High-dimensional Ising model selection using `1regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Annals of Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "Pseudolikelihood estimation for social networks", "author": ["D. Strauss", "M. Ikeda"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Strauss and Ikeda,? \\Q1990\\E", "shortCiteRegEx": "Strauss and Ikeda", "year": 1990}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Sutton and McCallum,? \\Q2012\\E", "shortCiteRegEx": "Sutton and McCallum", "year": 2012}, {"title": "On autoencoders and score matching for energy based models", "author": ["K. Swersky", "M.A. Ranzato", "D. Buchman", "B. Marlin", "N. Freitas"], "venue": "In ICML,", "citeRegEx": "Swersky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2011}, {"title": "An overview of composite likelihood methods", "author": ["C. Varin", "N. Reid", "D. Firth"], "venue": "Statistica Sinica,", "citeRegEx": "Varin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Varin et al\\.", "year": 2011}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Distributed covariance estimation in Gaussian graphical models", "author": ["A. Wiesel", "A.O. Hero III"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Wiesel and III,? \\Q2012\\E", "shortCiteRegEx": "Wiesel and III", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "Markov Random Fields (MRFs), also known as undirected probabilistic graphical models, are ubiquitous structured probability models that have significantly impacted a large number of fields, including computer vision (Li, 2001; Szeliski et al., 2008), computational photography and graphics (Agarwala et al.", "startOffset": 216, "endOffset": 249}, {"referenceID": 1, "context": ", 2008), computational photography and graphics (Agarwala et al., 2004), computational neuroscience (Ackley et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": ", 2004), computational neuroscience (Ackley et al., 1985), bioinformatics (Yanover et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 12, "context": ", 2007), sensor networks (Liu & Ihler, 2012), social networks (Strauss & Ikeda, 1990), Markov logic (Richardson & Domingos, 2006), natural language processing (Lafferty et al., 2001; Sutton & McCallum, 2012) and statistical physics (Kindermann & Snell, 1980).", "startOffset": 159, "endOffset": 207}, {"referenceID": 0, "context": ", 2004), computational neuroscience (Ackley et al., 1985), bioinformatics (Yanover et al., 2007), sensor networks (Liu & Ihler, 2012), social networks (Strauss & Ikeda, 1990), Markov logic (Richardson & Domingos, 2006), natural language processing (Lafferty et al., 2001; Sutton & McCallum, 2012) and statistical physics (Kindermann & Snell, 1980). As pointed out in Wainwright & Jordan (2008) there are also many applications in statistics, constraint satisfaction and combinatorial optimization, error-correcting codes and epidemiology.", "startOffset": 37, "endOffset": 394}, {"referenceID": 2, "context": "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyv\u00e4rinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).", "startOffset": 129, "endOffset": 263}, {"referenceID": 8, "context": "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyv\u00e4rinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).", "startOffset": 129, "endOffset": 263}, {"referenceID": 9, "context": "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyv\u00e4rinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).", "startOffset": 129, "endOffset": 263}, {"referenceID": 17, "context": "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyv\u00e4rinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).", "startOffset": 129, "endOffset": 263}, {"referenceID": 25, "context": "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyv\u00e4rinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).", "startOffset": 129, "endOffset": 263}, {"referenceID": 24, "context": "The intractability of exact maximum likelihood has prompted the introduction of many approximate methods of parameter estimation (Besag, 1975; Hinton, 2000; Hyv\u00e4rinen, 2005; Marlin et al., 2010; Varin et al., 2011; Marlin & de Freitas, 2011; Swersky et al., 2011).", "startOffset": 129, "endOffset": 263}, {"referenceID": 21, "context": "Several researchers have addressed this issue by proposing to approximate pseudolikelihood by disjointly optimizing each conditional and combining the parameters using some form of averaging (Ravikumar et al., 2010; Wiesel & Hero III, 2012; Liu & Ihler, 2012).", "startOffset": 191, "endOffset": 259}, {"referenceID": 19, "context": "At the time of revising this paper, the same authors have shown that the convergence rate to the true parameters with their method is comparable to centralized maximum likelihood estimation (Meng et al., 2014).", "startOffset": 190, "endOffset": 209}, {"referenceID": 14, "context": ", 2010; Wiesel & Hero III, 2012; Liu & Ihler, 2012). In this paper we introduce a new approach to parameter estimation in MRFs with untied parameters, which avoids the model inefficiency of maximum likelihood for an important class of models while preserving its data efficiency. Moreover, our algorithm is embarrassingly parallel and can be implemented in a distributed setting without modification. Our algorithm replaces the joint maximum likelihood problem with a collection of much smaller auxiliary maximum likelihood problems which can be solved independently. We prove that if the auxiliary problems satisfy certain conditions, the relevant parameters in the auxiliary problems converge to the values of the true parameters in the joint model. Our experiments show that good performance is achieved in this case and that good performance is still achieved when these conditions are not satisfied. Violating the conditions for convergence sacrifices theoretical guarantees in exchange for even further computational savings while maintaining good empirical performance. Under a strong assumption (which is generally not satisfied in practice) we prove that our algorithm is exactly equal to maximum likelihood on the full joint distribution. While not directly applicable, this result provides additional insight into why our approach is effective. A similar method was recently, and independently, introduced in the context of Gaussian graphical models by Meng et al. (2013). In that paper, the authors consider local neighborhoods of nodes, whereas we consider neighborhoods of cliques, and they rely on a convex relaxation via the Schur complement to derive their algorithm for inverse covariance estimation.", "startOffset": 33, "endOffset": 1483}, {"referenceID": 3, "context": "E(xc |\u03b8c) = \u2212\u03b8c \u03c6c(xc) where \u03c6c(xc) is a feature vector derived from the values of the variables xc, we have a maximum entropy or log-linear model (Wasserman, 2004; Buchman et al., 2012; Murphy, 2012).", "startOffset": 147, "endOffset": 200}, {"referenceID": 20, "context": "E(xc |\u03b8c) = \u2212\u03b8c \u03c6c(xc) where \u03c6c(xc) is a feature vector derived from the values of the variables xc, we have a maximum entropy or log-linear model (Wasserman, 2004; Buchman et al., 2012; Murphy, 2012).", "startOffset": 147, "endOffset": 200}, {"referenceID": 17, "context": "That is, x\u0304mn = 1 \u2212 xmn if i = m and xmn otherwise (Marlin et al., 2010).", "startOffset": 51, "endOffset": 72}, {"referenceID": 6, "context": "The proof can be found in (Griffeath, 1976; Bremaud, 2001):", "startOffset": 26, "endOffset": 58}, {"referenceID": 10, "context": "We will make use of the following characterization of maximum likelihood estimates, which is proved in (Jordan, 2002).", "startOffset": 103, "endOffset": 117}], "year": 2014, "abstractText": "We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields with untied parameters which is efficient for a large class of practical models. Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters.", "creator": "LaTeX with hyperref package"}}}