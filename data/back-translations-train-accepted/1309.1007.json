{"id": "1309.1007", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2013", "title": "Concentration in unbounded metric spaces and algorithmic stability", "abstract": "We prove an extension of McDiarmid's inequality for metric spaces with unbounded diameter. To this end, we introduce the notion of the {\\em subgaussian diameter}, which is a distribution-dependent refinement of the metric diameter. Our technique provides an alternative approach to that of Kutin and Niyogi's method of weakly difference-bounded functions, and yields nontrivial, dimension-free results in some interesting cases where the former does not. As an application, we give apparently the first generalization bound in the algorithmic stability setting that holds for unbounded loss functions. We furthermore extend our concentration inequality to strongly mixing processes.", "histories": [["v1", "Wed, 4 Sep 2013 12:40:31 GMT  (17kb)", "https://arxiv.org/abs/1309.1007v1", null], ["v2", "Wed, 11 Sep 2013 16:24:52 GMT  (19kb)", "http://arxiv.org/abs/1309.1007v2", null]], "reviews": [], "SUBJECTS": "math.PR cs.LG math.FA", "authors": ["aryeh kontorovich"], "accepted": true, "id": "1309.1007"}, "pdf": {"name": "1309.1007.pdf", "metadata": {"source": "CRF", "title": "Concentration in unbounded metric spaces and algorithmic stability", "authors": ["Aryeh Kontorovich"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 9.10 07v2 [with ath. PR] 1"}, {"heading": "1 Introduction", "text": "In fact, the results of a (sufficiently \"stable\") algorithm point in a (sufficiently \"limited\") way to a (sufficiently \"limited\") terminology. It turns out that the different notions of stability in relation to future data are naturally called into question in relation to the Lipschitz continuity of the algorithm (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010), while appropriate relaxations of the iid assumption are achieved using various types of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik et al.)."}, {"heading": "2 Preliminaries", "text": "A metric probability space (X, \u03c1, \u00b5) is a measurable space whose Borel algebra is induced by the metric plane, equipped with the probability quantity \u00b5. Our results are most cleanly represented when Xi is a discrete set, but they continue to hold verbatim for Borel probability measures on Polish spaces. It will be convenient to write the notation Xji = (Xi,., Xj) for all sequences, and the sequence concatenation is called multiplied. Random variables are capitalized (X), specified sequences are written in lowercase, the notation Xji = (Xi,., Xj) is used for all sequences, and the sequence concatenation is multiplied. We will often use the shorthand P (xji)."}, {"heading": "3 Related work", "text": "McDiarmid's inequality (1) suffers from the limitations mentioned above: it completely ignores distribution and is null and void, even if only one of the two factors is infinite. (1) To solve some of these problems, Kutin (2002) proposed extending McDiarmid's inequality to \"almost everywhere.\" (2) To formalize this, Kutin (2) and Niyogi (2) must define a quantitative difference that is independent. (3) Define X (1) inequality to \"almost everywhere\" Lipschitz (i). (3) Define (3) the quantitative difference between the two factors. (3) Kutin and Niyogi define a weakly differentiated difference between (b, c) ifP (X) and Niyogi (3)."}, {"heading": "4 Concentration via subgaussian diameter", "text": "McDiarmid's inequality (1) can be expressed in the notation of Section 2 as follows: We (Xi, \u03c1i, \u00b5i), i = 1,., n are a sequence of metric probability spaces and i = 2,. (15) We defined the subgaussian diameter SG (Xi) in Section 2, which shows in Lemma 1 that it never exceeds the metric diameter. We also demonstrated from an example that the former can be finite if the latter is infinite. (15) The main result of this section is that the diameter (Xi) can essentially be replaced by SG (Xi)."}, {"heading": "5 Application to algorithmic stability", "text": "We refer to the readers (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005) to the readers (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005) to the readers (Bousquet and Elisseeff, 2002) to the readers (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin and Yi, 2005) to the readers (Bousquet and Elisseeff) to the readers (Bousquet and Elisseeff) to the readers (Bousquet and Elisseeff) to the readers (Bousquet) to the readers (Bousquet and Yi) to the readers (Bousquet)."}, {"heading": "6 Relaxing the independence assumption", "text": "In this section, we generalize Theorem 1 to strongly mixing processes. To this end, we need some standard factors about the probability theory terms of coupling and transport (Lindvall, 2002; Villani, 2003, 2009). Given the probability measures \u00b5, \u00b5 \u00b2 on a measurable space X, a coupling of \u00b5, \u00b5 \u00b2 is any probability measure on X \u00b7 X with marginal \u00b5 and \u00b5 \u00b2, respectively. Characteristic is the quantity of all couplings, we have the number of coupling measures ({(x, y). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X)."}, {"heading": "7 Other Orlicz diameters", "text": "Let us recall the idea of an Orlicz standard, according to which a real random variable X (see e.g. Rao and Ren (1991)): \"X,\" \"inf,\" \"0: E,\" \"X,\" \"1,\" where \"R\" is a Young function - not negative, even, convex, and vanishing at 0. In this section we consider the Young functions p (x) = e, \"\" x, \"p,\" \",\" 1, \"\" 1, \"and their induced Orlicz norms. A random variable X is subgaussian if and only if\" X, \"\" p, \"\" 2. \"For p 6 = 2,\" X, \"\" \"p,\" \"\" p, \"\" p, \"p,\" p, \"p,\" \"p,\" \"p,\" \"\" p, \"\" \"p,\" \"and\" p. \"A direct consequence of Markov's metric is that any X\", \"\" \"\" \"\" \",\" p, \"\" \"and\" p. \""}, {"heading": "8 Discussion", "text": "We have shown a concentration imbalance for unlimited diameter metric spaces, shown their applicability to algorithmic stability with unlimited losses, and given an extension to non-independent sampling processes. Some fascinating questions remain: (i) How narrow is Theorem 1? First, there is the annoying matter of having a worse constant in the exponent (i.e. 1 / 2) than McDiarmid's (optimal) constant 2. Although this gap is not crucial, one would wish for a limit to restore McDiarmid's case of finite diameter. More importantly, is it the case that a finite subgaussian diameter is necessary for the subgaussian concentration of all Lipschitz functions? This is a limit that restores McDiarmid's case, given the metric probability spaces (Xi, ICI, ICI)."}, {"heading": "Acknowledgements", "text": "John Lafferty encouraged me to strive for a distributional refinement of McDiarmid's inequality, thanks also to Gideon Schechtman, Shahar Mendelson, Assaf Naor, Iosif Pinelis, and Csaba Szepesva \u0301 ri for the helpful correspondence, and to Roi Wei\u00df for the careful proofreading of the manuscript."}], "references": [{"title": "Stable transductive learning", "author": ["Ran El-Yaniv", "Dmitry Pechyony"], "venue": "In Learning theory,", "citeRegEx": "El.Yaniv and Pechyony.,? \\Q2007\\E", "shortCiteRegEx": "El.Yaniv and Pechyony.", "year": 2007}, {"title": "Measure Concentration of Strongly Mixing Processes with Applications", "author": ["Aryeh (Leonid) Kontorovich"], "venue": "PhD thesis,", "citeRegEx": "Kontorovich.,? \\Q2007\\E", "shortCiteRegEx": "Kontorovich.", "year": 2007}, {"title": "Concentration Inequalities for Dependent Random Variables via the Martingale Method", "author": ["Leonid (Aryeh) Kontorovich", "Kavita Ramanan"], "venue": "Ann. Probab.,", "citeRegEx": "Kontorovich and Ramanan.,? \\Q2008\\E", "shortCiteRegEx": "Kontorovich and Ramanan.", "year": 2008}, {"title": "Extensions to McDiarmid\u2019s inequality when differences are bounded with high probability", "author": ["Samuel Kutin"], "venue": "Technical Report TR-2002-04,", "citeRegEx": "Kutin.,? \\Q2002\\E", "shortCiteRegEx": "Kutin.", "year": 2002}, {"title": "Almost-everywhere algorithmic stability and generalization error", "author": ["Samuel Kutin", "Partha Niyogi"], "venue": "In UAI,", "citeRegEx": "Kutin and Niyogi.,? \\Q2002\\E", "shortCiteRegEx": "Kutin and Niyogi.", "year": 2002}, {"title": "Learning subgaussian classes: Upper and minimax bounds, arxiv:1305.4825", "author": ["Guillaume Lecu\u00e9", "Shahar Mendelson"], "venue": null, "citeRegEx": "Lecu\u00e9 and Mendelson.,? \\Q2013\\E", "shortCiteRegEx": "Lecu\u00e9 and Mendelson.", "year": 2013}, {"title": "The Concentration of Measure Phenomenon", "author": ["Michel Ledoux"], "venue": "Mathematical Surveys and Monographs", "citeRegEx": "Ledoux.,? \\Q2001\\E", "shortCiteRegEx": "Ledoux.", "year": 2001}, {"title": "Improved generalization bounds for largescale structured prediction", "author": ["Ben London", "Bert Huang", "Lise Getoor"], "venue": "In NIPS Workshop on Algorithmic and Statistical Approaches for Large Social Networks,", "citeRegEx": "London et al\\.,? \\Q2012\\E", "shortCiteRegEx": "London et al\\.", "year": 2012}, {"title": "Collective stability in structured prediction: Generalization from one example", "author": ["Ben London", "Bert Huang", "Benjamin Taskar", "Lise Getoor"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "London et al\\.,? \\Q2013\\E", "shortCiteRegEx": "London et al\\.", "year": 2013}, {"title": "Bounding d\u0304-distance by informational divergence: a method to prove measure concentration", "author": ["Katalin Marton"], "venue": "Ann. Probab.,", "citeRegEx": "Marton.,? \\Q1996\\E", "shortCiteRegEx": "Marton.", "year": 1996}, {"title": "On the method of bounded differences", "author": ["Colin McDiarmid"], "venue": "In J. Siemons, editor, Surveys in Combinatorics,", "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Stability bounds for stationary phi-mixing and beta-mixing processes", "author": ["Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mohri and Rostamizadeh.,? \\Q2010\\E", "shortCiteRegEx": "Mohri and Rostamizadeh.", "year": 2010}, {"title": "Rademacher complexity bounds for noni.i.d", "author": ["Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Mohri and Rostamizadeh.,? \\Q2008\\E", "shortCiteRegEx": "Mohri and Rostamizadeh.", "year": 2008}, {"title": "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization", "author": ["Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Rifkin"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Mukherjee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mukherjee et al\\.", "year": 2006}, {"title": "Stability results in learning theory", "author": ["Alexander Rakhlin", "Sayan Mukherjee", "Tomaso Poggio"], "venue": "Anal. Appl. (Singap.),", "citeRegEx": "Rakhlin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2005}, {"title": "In\u00e9galit\u00e9s de Hoeffding pour les fonctions lipschitziennes de suites", "author": ["Emmanuel Rio"], "venue": null, "citeRegEx": "Rio.,? \\Q1991\\E", "shortCiteRegEx": "Rio.", "year": 1991}, {"title": "Generalization bounds of erm algorithm with", "author": ["Bin Zou", "Zong-ben Xu", "Jie Xu"], "venue": "Springer-Verlag, Berlin,", "citeRegEx": "Zou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "It turns out that the various notions of stability are naturally expressed in terms of the Lipschitz continuity of the algorithm in question (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al.", "startOffset": 141, "endOffset": 246}, {"referenceID": 14, "context": "It turns out that the various notions of stability are naturally expressed in terms of the Lipschitz continuity of the algorithm in question (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005; Shalev-Shwartz et al., 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al.", "startOffset": 141, "endOffset": 246}, {"referenceID": 12, "context": ", 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al., 2009; Zou et al.; Mohri and Rostamizadeh, 2010; London et al., 2012, 2013; Shalizi and Kontorovich, 2013).", "startOffset": 111, "endOffset": 376}, {"referenceID": 11, "context": ", 2010), while appropriate relaxations of the iid assumption are achieved using various kinds of strong mixing (Karandikar and Vidyasagar, 2002; Gamarnik, 2003; Rostamizadeh and Mohri, 2007; Mohri and Rostamizadeh, 2008; Steinwart and Christmann, 2009; Steinwart et al., 2009; Zou et al.; Mohri and Rostamizadeh, 2010; London et al., 2012, 2013; Shalizi and Kontorovich, 2013).", "startOffset": 111, "endOffset": 376}, {"referenceID": 10, "context": "An elegant and powerful work-horse driving many of the aforementioned results is McDiarmid\u2019s inequality (McDiarmid, 1989): P(|\u03c6\u2212 E\u03c6| > t) \u2264 2 exp ( \u2212 2t 2 \u2211n i=1 w 2 i )", "startOffset": 104, "endOffset": 121}, {"referenceID": 9, "context": "Non-iid extensions of (1) have also been considered (Marton, 1996; Rio, 2000; Chazottes et al., 2007; Kontorovich and Ramanan, 2008).", "startOffset": 52, "endOffset": 132}, {"referenceID": 2, "context": "Non-iid extensions of (1) have also been considered (Marton, 1996; Rio, 2000; Chazottes et al., 2007; Kontorovich and Ramanan, 2008).", "startOffset": 52, "endOffset": 132}, {"referenceID": 13, "context": "This influential result has been invoked in a number of recent papers (El-Yaniv and Pechyony, 2006; Mukherjee et al., 2006; Hush et al., 2007; Agarwal and Niyogi, 2009; Shalev-Shwartz et al., 2010; Rubinstein and Simma, 2012).", "startOffset": 70, "endOffset": 225}, {"referenceID": 0, "context": ", 2007; Kontorovich and Ramanan, 2008). The distribution-free nature of McDiarmid\u2019s inequality makes it an attractive tool in learning theory, but also imposes inherent limitations on its applicability. Chief among these limitations is the inability of (1) to provide risk bounds for unbounded loss functions. Even in the bounded case, if the Lipschitz condition (2) holds not everywhere but only with high probability \u2014 say, with a much larger constant on a small set of exceptions \u2014 the bound in (1) still charges the full cost of the worst-case constant. To counter this difficulty, Kutin (2002); Kutin and Niyogi (2002) introduced an extension of McDiarmid\u2019s inequality to weakly difference-bounded functions and used it to analyze the risk of \u201calmost-everywhere\u201d stable algorithms.", "startOffset": 8, "endOffset": 599}, {"referenceID": 0, "context": ", 2007; Kontorovich and Ramanan, 2008). The distribution-free nature of McDiarmid\u2019s inequality makes it an attractive tool in learning theory, but also imposes inherent limitations on its applicability. Chief among these limitations is the inability of (1) to provide risk bounds for unbounded loss functions. Even in the bounded case, if the Lipschitz condition (2) holds not everywhere but only with high probability \u2014 say, with a much larger constant on a small set of exceptions \u2014 the bound in (1) still charges the full cost of the worst-case constant. To counter this difficulty, Kutin (2002); Kutin and Niyogi (2002) introduced an extension of McDiarmid\u2019s inequality to weakly difference-bounded functions and used it to analyze the risk of \u201calmost-everywhere\u201d stable algorithms.", "startOffset": 8, "endOffset": 624}, {"referenceID": 3, "context": "In order to address some of these issues, Kutin (2002); Kutin and Niyogi (2002) proposed an extension of McDiarmid\u2019s inequality to \u201calmost everywhere\u201d Lipschitz functions \u03c6 : Xn \u2192 R.", "startOffset": 42, "endOffset": 55}, {"referenceID": 3, "context": "In order to address some of these issues, Kutin (2002); Kutin and Niyogi (2002) proposed an extension of McDiarmid\u2019s inequality to \u201calmost everywhere\u201d Lipschitz functions \u03c6 : Xn \u2192 R.", "startOffset": 42, "endOffset": 80}, {"referenceID": 3, "context": "The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state \u2014 indeed, the present work was motivated in part by a desire for simpler tools. Assuming that \u03c6 is weakly difference-bounded by (b, c, \u03b4) with \u03b4 = exp(\u2212\u03a9(n)) (13) and c = O(1/n), their bound states that P(|\u03c6\u2212 E\u03c6| \u2265 t) \u2264 exp(\u2212\u03a9(nt)) (14) for a certain range of t and n. As noted by Rakhlin et al. (2005), the exponential decay assumption (13) is necessary in order for the Kutin-Niyogi method to yield exponential concentration.", "startOffset": 22, "endOffset": 382}, {"referenceID": 3, "context": "The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state \u2014 indeed, the present work was motivated in part by a desire for simpler tools. Assuming that \u03c6 is weakly difference-bounded by (b, c, \u03b4) with \u03b4 = exp(\u2212\u03a9(n)) (13) and c = O(1/n), their bound states that P(|\u03c6\u2212 E\u03c6| \u2265 t) \u2264 exp(\u2212\u03a9(nt)) (14) for a certain range of t and n. As noted by Rakhlin et al. (2005), the exponential decay assumption (13) is necessary in order for the Kutin-Niyogi method to yield exponential concentration. In contrast, the bounds we prove here (i) do not require |\u03c6(X)\u2212 \u03c6(X\u0303)| to be everywhere bounded as in (11) (ii) have a simple statement and proof, and generalize to non-iid processes with relative ease. We defer the quantitative comparisons between (14) and our results until the latter are formally stated in Section 4. In a different line of work, Bentkus (2008) considered an extension of Hoeffding\u2019s inequality to unbounded random variables.", "startOffset": 22, "endOffset": 872}, {"referenceID": 3, "context": "The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state \u2014 indeed, the present work was motivated in part by a desire for simpler tools. Assuming that \u03c6 is weakly difference-bounded by (b, c, \u03b4) with \u03b4 = exp(\u2212\u03a9(n)) (13) and c = O(1/n), their bound states that P(|\u03c6\u2212 E\u03c6| \u2265 t) \u2264 exp(\u2212\u03a9(nt)) (14) for a certain range of t and n. As noted by Rakhlin et al. (2005), the exponential decay assumption (13) is necessary in order for the Kutin-Niyogi method to yield exponential concentration. In contrast, the bounds we prove here (i) do not require |\u03c6(X)\u2212 \u03c6(X\u0303)| to be everywhere bounded as in (11) (ii) have a simple statement and proof, and generalize to non-iid processes with relative ease. We defer the quantitative comparisons between (14) and our results until the latter are formally stated in Section 4. In a different line of work, Bentkus (2008) considered an extension of Hoeffding\u2019s inequality to unbounded random variables. His bound only holds for sums (as opposed to general Lipschitz functions) and the summands must be non-negative (i.e., unbounded only in the positive direction). An earlier notion of \u201ceffective\u201d metric diameter in the context of concentration is that of metric space length (Schechtman, 1982). Another distribution-dependent refinement of diameter is the spread constant (Alon et al., 1998). Lecu\u00e9 and Mendelson (2013) gave minimax bounds for empirical risk minimization over subgaussian classes.", "startOffset": 22, "endOffset": 1372}, {"referenceID": 6, "context": "The strong integrability of \u03c6 \u2014 and in particular, finiteness of E\u03c6 \u2014 follow from exponential concentration (Ledoux, 2001).", "startOffset": 108, "endOffset": 122}, {"referenceID": 4, "context": "5 Application to algorithmic stability We refer the reader to (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005) for background on algorithmic stability and supervised learning.", "startOffset": 62, "endOffset": 138}, {"referenceID": 14, "context": "5 Application to algorithmic stability We refer the reader to (Bousquet and Elisseeff, 2002; Kutin and Niyogi, 2002; Rakhlin et al., 2005) for background on algorithmic stability and supervised learning.", "startOffset": 62, "endOffset": 138}, {"referenceID": 14, "context": "A variant of uniform stability in the sense of Rakhlin et al. (2005) \u2014 which is slightly more general than the homonymous notion in Bousquet and Elisseeff (2002)\u2014 may be defined as follows.", "startOffset": 47, "endOffset": 69}, {"referenceID": 14, "context": "A variant of uniform stability in the sense of Rakhlin et al. (2005) \u2014 which is slightly more general than the homonymous notion in Bousquet and Elisseeff (2002)\u2014 may be defined as follows.", "startOffset": 47, "endOffset": 162}, {"referenceID": 1, "context": "Kontorovich (2007); Kontorovich and Ramanan (2008) discuss how to handle conditioning on measure-zero sets and other technicalities.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Kontorovich (2007); Kontorovich and Ramanan (2008) discuss how to handle conditioning on measure-zero sets and other technicalities.", "startOffset": 0, "endOffset": 51}, {"referenceID": 9, "context": "The use of coupling and transportation techniques to obtain concentration for dependent random variables goes back to Marton (1996); Samson (2000); Chazottes et al.", "startOffset": 118, "endOffset": 132}, {"referenceID": 9, "context": "The use of coupling and transportation techniques to obtain concentration for dependent random variables goes back to Marton (1996); Samson (2000); Chazottes et al.", "startOffset": 118, "endOffset": 147}, {"referenceID": 9, "context": "The use of coupling and transportation techniques to obtain concentration for dependent random variables goes back to Marton (1996); Samson (2000); Chazottes et al. (2007).", "startOffset": 118, "endOffset": 172}], "year": 2013, "abstractText": "We prove an extension of McDiarmid\u2019s inequality for metric spaces with unbounded diameter. To this end, we introduce the notion of the subgaussian diameter, which is a distribution-dependent refinement of the metric diameter. Our technique provides an alternative approach to that of Kutin and Niyogi\u2019s method of weakly difference-bounded functions, and yields nontrivial, dimension-free results in some interesting cases where the former does not. As an application, we give apparently the first generalization bound in the algorithmic stability setting that holds for unbounded loss functions. We give two extensions of the basic concentration result: to strongly mixing processes and to other Orlicz norms.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}