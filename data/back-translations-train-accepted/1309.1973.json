{"id": "1309.1973", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2013", "title": "Regret-Based Multi-Agent Coordination with Uncertain Task Rewards", "abstract": "Many multi-agent coordination problems can be represented as DCOPs. Motivated by task allocation in disaster response, we extend standard DCOP models to consider uncertain task rewards where the outcome of completing a task depends on its current state, which is randomly drawn from unknown distributions. The goal of solving this problem is to find a solution for all agents that minimizes the overall worst-case loss. This is a challenging problem for centralized algorithms because the search space grows exponentially with the number of agents and is nontrivial for standard DCOP algorithms we have. To address this, we propose a novel decentralized algorithm that incorporates Max-Sum with iterative constraint generation to solve the problem by passing messages among agents. By so doing, our approach scales well and can solve instances of the task allocation problem with hundreds of agents and tasks.", "histories": [["v1", "Sun, 8 Sep 2013 16:20:06 GMT  (114kb,D)", "http://arxiv.org/abs/1309.1973v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["feng wu", "nicholas r jennings"], "accepted": true, "id": "1309.1973"}, "pdf": {"name": "1309.1973.pdf", "metadata": {"source": "CRF", "title": "Regret-Based Multi-Agent Coordination with Uncertain Task Rewards", "authors": ["Feng Wu", "Nicholas R. Jennings"], "emails": ["nrj}@ecs.soton.ac.uk"], "sections": [{"heading": "Introduction", "text": "However, in this model, agents are presented as decision variables and the tasks that can be assigned to them are variable domains. However, the synergies between the common tasks of the agents are specified as uncertain load values. Now, some tasks may require a subset of the team to work together, either because a single agent has insufficient skills to complete the task or teamwork can greatly improve performance. However, in both cases, the limitations are the usefulness of the common tasks of the agents. Once the DCOP model of the problem is achieved, we can solve it efficiently using optimal approaches such as ADOPT (Modi et al. 2005) and DPOP (Petcu & Foltings 2005) or approximate approaches such as DSA (Zhang et al. 2005), MGM (Maheswaran et al. 2004), and Max-Sum et al."}, {"heading": "The UR-DCOP Model", "text": "Formally, a distributed constraint optimization problem (DCOP) is called tuples M = < I, X, D, U >, where: \u2022 I = {1, \u00b7 \u00b7, n} is a set of agents that can be defined by 1, 2, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 events; \u2022 X = {x1, \u00b7, xn} is a set of decision variables in which xi is the variable of agent i; \u2022 D = {D1, \u00b7 \u00b7 \u00b7 \u00b7, Dn} is a set of finite domains for decision variables in which domainDi is a set of possible values for decision variables xi; \u2022 U = {U1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Um} is a set of soft constraints in which each constraint Uj: Dj1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Djk \u2192 < defines the value of possible mappings to subsets of decision variables in which Uj (1, \u00b7 \u00b7 \u00b7 \u00b7) is the function of variables."}, {"heading": "Solving UR-DCOPs", "text": "Generally, to calculate the minimax regret in Equation 7, we first have to calculate the optimal problem > > Regrets given a point of faith b and the current solution of agents x. \"< < < < < < < < < < < < < < < < < < < <"}, {"heading": "The Master Problem", "text": "The main problem of Equation 8, given the witness set G, can accordingly be described as follows: x = arg min x = max < b, x * G m = 1 [Uj (bj, x \u00b2 j) \u2212 Uj (bj, x \u00b2 j) \u00b7 each individual example is independent of other beliefs and x \u00b2 j (10). To this end, we consider the problem of minimizing a vector of remorse functions in G: V (x) = [V, < b, x \u00b2) > 1), which is connected only to the variable nodes it connects."}, {"heading": "The Subproblem", "text": "The partial problem in Equation 9 with the current solution x can be described as follows: < b, x, x, > = arg max b, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "Analysis and Discussion", "text": "The optimality of our algorithm inherited from Max-Sum depends on the structure of the factor diagram. < b > b > For an acyclic factor diagram, Max-Sum is known to approach the optimal solution for the DCOPs in finite rounds of message delivery (Farinelli et al. 2008). < b > The master problems in ICG-Max-Sum will approach the optimal solution for the acyclic factor graphs. Messages (vectors) in the master problems represent the regrettable values of all witness points in | G |. The sum operator adds all the regrettable components for each witness point, Uj (bj, x) \u2212 Uj (xj), sent from its adjacent nodes. The maximum operator selects the current minimax solutions and sends out the corresponding regrettable values."}, {"heading": "Empirical Evaluation", "text": "We tested the performance of our algorithms on randomly generated instances of task allocation problem that we used to motivate our work. We first create a problem with a set of tasks T and agentsA. Each task has a set of states S, from which we randomly select a problem as its task status. We then create a random graph with links between agents and tasks. Each link represents the fact that the agent can perform the task. The utility function (a Gaussian function whose mean and variance are randomly generated between the ranges of 80 to 100 and 0 to 80, respectively) of each task depends on all connected agents and their current state. In the experiments, we rely on the fact that not all tasks can be performed simultaneously because a task requires at least one agent. Thus, the agents must make a good choice to maximize team performance. We execute the algorithms to solve the problem and output its solution. Since the empathetic states of the solution are hidden, we want to be as close to the optimal as possible as possible."}, {"heading": "Conclusions", "text": "We introduced the ICG-Max-Sum algorithm to find robust solutions for UR-DCOPs. Specifically, we assume that the distributions of task states are unknown and use minimax regret to evaluate the worst-case loss. Building on the ideas of iterative constraint generation, we proposed a decentralized algorithm that can calculate minimax remorse and solution with Max-Sum. Similar to Max-Sum, it can exploit the interaction structures between agents and scale up to problems with a large number of agents. We evaluated the performance of our algorithms empirically using our motivating task assignment domains. Experimental results show that our algorithm has better scalability (e.g. 100 agents and 200 tasks) than the centralized method (ICG) and outperforms the most modern decentralized method (DSA), although it would be useful if communication was second, if it is not very game independent."}], "references": [{"title": "R", "author": ["S.M. Aji", "McEliece"], "venue": "J.", "citeRegEx": "Aji . McEliece 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "and Decker", "author": ["J. Atlas"], "venue": "K.", "citeRegEx": "Atlas . Decker 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "J", "author": ["Benders"], "venue": "F.", "citeRegEx": "Benders 1962", "shortCiteRegEx": null, "year": 1962}, {"title": "N", "author": ["A. Farinelli", "A. Rogers", "A. Petcu", "Jennings"], "venue": "R.", "citeRegEx": "Farinelli et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Faltings", "author": ["T. L\u00e9aut\u00e9"], "venue": "B.", "citeRegEx": "L\u00e9aut\u00e9 . Faltings 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed algorithms for dcop: A graphical-game-based approach", "author": ["Maheswaran"], "venue": null, "citeRegEx": "Maheswaran,? \\Q2004\\E", "shortCiteRegEx": "Maheswaran", "year": 2004}, {"title": "P", "author": ["Modi"], "venue": "J.; Shen, W.-M.; Tambe, M.; and Yoko, M.", "citeRegEx": "Modi et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "H", "author": ["D.T. Nguyen", "W. Yeoh", "Lau"], "venue": "C.", "citeRegEx": "Nguyen et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Faltings", "author": ["A. Petcu"], "venue": "B.", "citeRegEx": "Petcu . Faltings 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "C", "author": ["K. Regan", "Boutilier"], "venue": "2010. Robust policy computation in reward-uncertain MDPs using nondominated policies. In Proceedings of the 24th AAAI Conference on Artificial Intelligence, 1127\u2013", "citeRegEx": "Regan . Boutilier 2010", "shortCiteRegEx": null, "year": 1133}, {"title": "and Boutilier", "author": ["K. Regan"], "venue": "C.", "citeRegEx": "Regan . Boutilier 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "N", "author": ["A. Rogers", "A. Farinelli", "R. Stranders", "Jennings"], "venue": "R.", "citeRegEx": "Rogers et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "N", "author": ["R. Stranders", "F.M.D. Fave", "A. Rogers", "Jennings"], "venue": "R.", "citeRegEx": "Stranders et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Peled", "author": ["R. Zivan"], "venue": "H.", "citeRegEx": "Zivan . Peled 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2013, "abstractText": "Many multi-agent coordination problems can be represented as DCOPs. Motivated by task allocation in disaster response, we extend standard DCOP models to consider uncertain task rewards where the outcome of completing a task depends on its current state, which is randomly drawn from unknown distributions. The goal of solving this problem is to find a solution for all agents that minimizes the overall worst-case loss. This is a challenging problem for centralized algorithms because the search space grows exponentially with the number of agents and is nontrivial for standard DCOP algorithms we have. To address this, we propose a novel decentralized algorithm that incorporates Max-Sum with iterative constraint generation to solve the problem by passing messages among agents. By so doing, our approach scales well and can solve instances of the task allocation problem with hundreds of agents and tasks.", "creator": "LaTeX with hyperref package"}}}