{"id": "1309.7598", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2013", "title": "On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations", "abstract": "In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide means for drawing either approximate or unbiased samples from Gibbs' distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical \"high signal - high coupling\" regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds.", "histories": [["v1", "Sun, 29 Sep 2013 13:48:52 GMT  (1769kb,D)", "http://arxiv.org/abs/1309.7598v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tamir hazan", "subhransu maji", "tommi s jaakkola"], "accepted": true, "id": "1309.7598"}, "pdf": {"name": "1309.7598.pdf", "metadata": {"source": "CRF", "title": "On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations", "authors": ["Tamir Hazan", "Subhransu Maji", "Tommi Jaakkola"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Consequences in complex models drive much of the research in machine learning applications, from computer vision to natural language processing to computational biology. Examples are scene understanding [3], parsing [12], or protein design [18]. The consequence problem in such cases is the finding of probable structures, whether objects, parsers, or molecular arrangements. Each structure corresponds to an assignment of values to random variables, and the probability of assignment is based on the definition of potential functions in a Gibbs distribution. It is usually practicable to find only the most probable or maximum a-posteriori (MAP) assignment (structure), rather than stomping from the full Gibbs distribution. Considerable efforts have been made to develop algorithms to restore MAP assignments, either based on specific structural constraints such as supermodularity [11] or by deriving cut-based methods based on relations23, 18."}, {"heading": "2 Background", "text": "The statistical conclusion includes considerations about the states of discrete variables, whose configurations (assignments of values) determine the distribution of the individual structures of interest. We assume that the random distribution of the individual subjects in a discrete product room X = X1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Xn. The effective domain is implicitly defined by real evaluation potentials. (x) The effective distribution is implicitly defined by exclusions. (x) Whenever the real evaluated potential functions are mapped on the probability scale via the distribution of Gibbs: p x1, xn) = 1Z exp (x1, xn), where Z = xn), where Z = x1,..., xn exp (xn), xn exp (x1, xn exp)."}, {"heading": "3 Probable approximate samples from the Gibbs distribution", "text": "Sampling from the Gibbs distribution is inherently bound to the estimate of the partition function. Markov properties that simplify the distribution also decompile the calculation of the partition function. (Sampling from the Gibbs distribution is inherently bound to the estimate of the random problem. (Sampling from the Gibbs distribution is inherently bound to the calculation of the partition function.) Markov properties that simplify the distribution also decompile the calculation of the partition function. (Sampling from the Gibbs distribution is inherently bound to the estimate of the partition function. (Sampling from the Gibbs distribution is inherently bound to the estimate of the partition function.) Markov properties that simplify the distribution also decompile the calculation of the partition function. (Sampling from the Gibbs distribution is inherently bound to the estimate of the partition function. (Sampling from the Gibbs distribution is inherently bound to the estimate of the partition function.) Sampling from the Gibbs distribution is inherently bound to the estimate of the partition function."}, {"heading": "4 Unbiased sampling using sequential bounds on the partition function", "text": "In the following, we describe how to use random MAP perturbations to generate unbiased samples from the Gibbs distribution = 1. Sampling from the Gibbs distribution is inherently tied to the estimate of the partition function. Suppose we could calculate the partition function exactly, then we could have samples from the Gibbs distribution sequentially: for each dimension we capture with a probability proportional to the full Gibbs distribution. Instead, we construct a family of self-reducible boundaries that imitate the partition function, namely the summation of its exposures. These boundaries extend the one in [6] when limited to local perturbations. Let {.i} be a collection of variables."}, {"heading": "5 Lower bounds on the partition function", "text": "The realization of the partition function as expectation-optimization pair in theorem 1 provides efficiently calculable lower limits of the partition function. Intuitively, these limits correspond to moving expectations (or sums) within the maximization operations. Therefore, in the following, we present two lower limits derived along these lines, the first in expectation and the second in probability calculation. Consequence 1. Consider a family of subsets \u03b1 A and let x\u03b1 be a series of variables limited to the indices in \u03b1. Let us assume that the random variables in terms of probability number (x\u03b1) i.d. correspond to the Gumbel distribution with zero mean, x\u03b1, x\u03b1 Turn. Then we will have a series of variables {xi} i [max x) + Procession (x) + Proception (x\u03b1) and ki."}, {"heading": "6 Experiments", "text": "We evaluated our approach to spin-glass models. We evaluated our approach to spin-glass models. We evaluated our approach to spin-glass models. We evaluated our approach to spin-glass models. We only have a limited number of models that we assumed to have a lower probability. The spins interact with couplings in a grid graphical model. Attractive models are mathematically attractive because their MAP predictions can be efficiently calculated by graph-cut algorithms. We start by evaluating our lower limits, which are presented in Section 5, to 10 \u00d7 10 spingglass models. Corollare 1 presents a lower limit that holds in expectation. We evaluated these lower models in anticipation. We evaluated a lower limit that is presented in Section 5, to 10 \u00d7 10 spingglass models. Corollary 1 presents a lower limit in expectation that holds a lower limit in expectation that holds in expectation."}, {"heading": "7 Related work", "text": "Gibbs distribution plays a key role in many areas of science, including computer science, statistics and physics. To learn more about its role in machine learning, as well as its standard sample, we refer the interested reader to the textbook [10, 22]. Our work is based on maximum random variable statistics. For a comprehensive introduction to extreme value statistics, we refer the reader to [13].Gibbs distribution and its partition function can be realized from random MAP error statistics using the Gumbel distribution (see Theorem 1), [13, 16, 20, 6]. Recently [15, 16, 20] we examined the various aspects of random MAP predictions with low dimensional error. [15] describe samples from the Gaussian distribution with random Gaussian disturbances. [16] we show that random MAP predictions with low dimensional disturbances have similar statistics to those described by Gibbs distribution [20] random Gaussian models and their efficient perspectives."}, {"heading": "8 Discussion", "text": "Following [16, 20], we showed here that efficient MAP solvers can be used to generate approximate or unbiased samples from the Gibbs distribution if we happen to disrupt potential function. Since MAP predictions are not affected by fragmented energy landscapes, our approach is characterized by the \"High-Signal High-Coupling\" regime. As a by-product of our approach, we constructed lower limits for partition functions, which are both narrower and faster than previous approaches in the \"High-Signal High-Coupling\" regime. Our approach is based on random MAP disturbances, which estimate partition functions with expectation. In practice, we calculate empirical averages and standard techniques, for example, to measure concentration, which in practice proves to be less effective."}], "references": [{"title": "Optimization of structured mean field objectives", "author": ["Alexandre Bouchard-C\u00f4t\u00e9", "Michael I Jordan"], "venue": "In AUAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Dynamic programming and graph algorithms in computer vision", "author": ["P.F. Felzenszwalb", "R. Zabih"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "The complexity of ferromagnetic ising with local fields", "author": ["L.A. Goldberg", "M. Jerrum"], "venue": "Combinatorics Probability and Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures, volume 33", "author": ["E.J. Gumbel", "J. Lieblein"], "venue": "US Govt. Print. Office,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1954}, {"title": "On the partition function and random maximum a-posteriori perturbations", "author": ["T. Hazan", "T. Jaakkola"], "venue": "arXiv preprint arXiv:1206.6410,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Polynomial-time approximation algorithms for the ising model", "author": ["M. Jerrum", "A. Sinclair"], "venue": "SIAM Journal on computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Measuring uncertainty in graph cut solutions\u2013efficiently computing min-marginal energies using dynamic graph cuts", "author": ["Pushmeet Kohli", "Philip HS Torr"], "venue": "In ECCV,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Probabilistic graphical models", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Extreme value distributions: theory and applications", "author": ["S. Kotz", "S. Nadarajah"], "venue": "World Scientific Publishing Company,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Negative tree reweighted belief propagation", "author": ["Qiang Liu", "Alexander T Ihler"], "venue": "arXiv preprint arXiv:1203.3494,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Gaussian sampling by local perturbations", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. Int. Conf. on Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. IEEE Int. Conf. on Computer Vision (ICCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "The bethe partition function of log-supermodular graphical models", "author": ["Nicholas Ruozzi"], "venue": "arXiv preprint arXiv:1202.6035,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Tightening LP relaxations for MAP using message passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss"], "venue": "In Conf. Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Loop series and Bethe variational bounds in attractive graphical models", "author": ["E.B. Sudderth", "M.J. Wainwright", "A.S. Willsky"], "venue": "Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Randomized optimum models for structured prediction", "author": ["D. Tarlow", "R.P. Adams", "R.S. Zemel"], "venue": "In Proceedings of the Fifteenth Conference on Artificial Intelligence and Statistics: April,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "A new class of upper bounds on the log partition function", "author": ["M.J. Wainwright", "T.S. Jaakkola", "A.S. Willsky"], "venue": "Trans. on Information Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf)", "author": ["T. Werner"], "venue": "In CVPR, pages", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Approximating partition functions of the two-state spin system", "author": ["J. Zhang", "H. Liang", "F. Bai"], "venue": "Information Processing Letters,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Examples include scene understanding [3], parsing [12], or protein design [18].", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "Examples include scene understanding [3], parsing [12], or protein design [18].", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Examples include scene understanding [3], parsing [12], or protein design [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "Substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity [11] or by devising cutting-planes based methods on linear programming relaxations [18, 23].", "startOffset": 161, "endOffset": 165}, {"referenceID": 17, "context": "Substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity [11] or by devising cutting-planes based methods on linear programming relaxations [18, 23].", "startOffset": 244, "endOffset": 252}, {"referenceID": 22, "context": "Substantial effort has gone into developing algorithms for recovering MAP assignments, either based on specific structural restrictions such as super-modularity [11] or by devising cutting-planes based methods on linear programming relaxations [18, 23].", "startOffset": 244, "endOffset": 252}, {"referenceID": 15, "context": "Recently [16, 20] defined probability models that are based on low dimensional perturbations, and empirically tied them to Gibbs distributions.", "startOffset": 9, "endOffset": 17}, {"referenceID": 19, "context": "Recently [16, 20] defined probability models that are based on low dimensional perturbations, and empirically tied them to Gibbs distributions.", "startOffset": 9, "endOffset": 17}, {"referenceID": 5, "context": "[6] augmented these results by providing bounds on the partition function in terms of random MAP perturbations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "In such ragged energy landscapes classical methods for the Gibbs distribution such as Gibbs sampling and Markov chain Monte Carlo methods, remain computationally expensive [4, 24].", "startOffset": 172, "endOffset": 179}, {"referenceID": 23, "context": "In such ragged energy landscapes classical methods for the Gibbs distribution such as Gibbs sampling and Markov chain Monte Carlo methods, remain computationally expensive [4, 24].", "startOffset": 172, "endOffset": 179}, {"referenceID": 12, "context": "[13]), we obtain the following result.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] Let {\u03b3(x)}x\u2208X be a collection of i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "These upper bounds extend the one in [6] when restricted to local perturbations.", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "Interestingly, this average is the quality of the partition upper bound presented in [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Attractive models are computationally appealing as their MAP predictions can be computed efficiently by the graph-cut algorithm [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "We compared these bounds to the different forms of structured mean-field, taking the one that performed best: standard structured mean-field that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14].", "startOffset": 183, "endOffset": 189}, {"referenceID": 0, "context": "We compared these bounds to the different forms of structured mean-field, taking the one that performed best: standard structured mean-field that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14].", "startOffset": 183, "endOffset": 189}, {"referenceID": 13, "context": "We compared these bounds to the different forms of structured mean-field, taking the one that performed best: standard structured mean-field that we computed over the vertical chains [8, 1], and the negative tree re-weighted computed on the horizontal and vertical trees [14].", "startOffset": 271, "endOffset": 275}, {"referenceID": 18, "context": "We also compared to the sum-product belief propagation algorithm, which was recently proven to produce lower bounds for attractive models [19, 17].", "startOffset": 138, "endOffset": 146}, {"referenceID": 16, "context": "We also compared to the sum-product belief propagation algorithm, which was recently proven to produce lower bounds for attractive models [19, 17].", "startOffset": 138, "endOffset": 146}, {"referenceID": 3, "context": "Focusing on spin glass models with strong local field potentials, it is well know that one cannot produce unbiased samples from the Gibbs distributions in polynomial time [4].", "startOffset": 171, "endOffset": 174}, {"referenceID": 5, "context": "the computational complexity of our unbiased sampling procedure to the gap between the logarithm of the partition function and its upper bound in [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "Sampling from the Gibbs distribution in spin glass models with non-zero local field potentials is computationally hard [7, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 3, "context": "Sampling from the Gibbs distribution in spin glass models with non-zero local field potentials is computationally hard [7, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 20, "context": "Although we omit from the plots for clarity, our approximate sampling marginal probabilities compares those of the tree re-weighted belief propagation [21].", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "Lastly, we emphasize the importance of probabilistic reasoning over the current variational methods, such as tree re-weighted belief propagation [21] or max-marginal probabilities [9], that only generate probabilities over small subsets of variables.", "startOffset": 145, "endOffset": 149}, {"referenceID": 8, "context": "Lastly, we emphasize the importance of probabilistic reasoning over the current variational methods, such as tree re-weighted belief propagation [21] or max-marginal probabilities [9], that only generate probabilities over small subsets of variables.", "startOffset": 180, "endOffset": 183}, {"referenceID": 9, "context": "To learn more about its roles in machine learning, as well as its standard samplers, we refer the interested reader to the textbook [10, 22].", "startOffset": 132, "endOffset": 140}, {"referenceID": 21, "context": "To learn more about its roles in machine learning, as well as its standard samplers, we refer the interested reader to the textbook [10, 22].", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "For comprehensive introduction to extreme value statistics we refer the reader to [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].", "startOffset": 160, "endOffset": 175}, {"referenceID": 15, "context": "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].", "startOffset": 160, "endOffset": 175}, {"referenceID": 19, "context": "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].", "startOffset": 160, "endOffset": 175}, {"referenceID": 5, "context": "The Gibbs distribution and its partition function can be realized from the statistics of random MAP perturbations with the Gumbel distribution (see Theorem 1), [13, 16, 20, 6].", "startOffset": 160, "endOffset": 175}, {"referenceID": 14, "context": "Recently, [15, 16, 20] explore the different aspects of random MAP predictions with low dimensional perturbation.", "startOffset": 10, "endOffset": 22}, {"referenceID": 15, "context": "Recently, [15, 16, 20] explore the different aspects of random MAP predictions with low dimensional perturbation.", "startOffset": 10, "endOffset": 22}, {"referenceID": 19, "context": "Recently, [15, 16, 20] explore the different aspects of random MAP predictions with low dimensional perturbation.", "startOffset": 10, "endOffset": 22}, {"referenceID": 14, "context": "[15] describe sampling from the Gaussian distribution with random Gaussian perturbations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] show that random MAP predictors with low dimensional perturbations share similar statistics as the Gibbs distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] describe the Bayesian perspectives of these models and their efficient sampling procedures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]", "startOffset": 189, "endOffset": 193}, {"referenceID": 8, "context": "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]", "startOffset": 211, "endOffset": 214}, {"referenceID": 3, "context": "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]", "startOffset": 338, "endOffset": 345}, {"referenceID": 23, "context": "These probability models generate samples efficiently thorough optimization: they have statistical advantages over purely variational approaches such as tree re-weighted belief propagation [21] or max-marginals [9], and they are faster than standard Gibbs samplers and Markov chain Monte Carlo approaches when MAP prediction is efficient [4, 24]", "startOffset": 338, "endOffset": 345}, {"referenceID": 5, "context": "Our suggested samplers for the Gibbs distribution are based on low dimensional representation of the partition function, [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "Corollary 2 shows that the approximation scheme of [6] is in fact a lower bound that holds in probability.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "Structured mean-field methods are inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense [8, 1, 14].", "startOffset": 158, "endOffset": 168}, {"referenceID": 0, "context": "Structured mean-field methods are inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense [8, 1, 14].", "startOffset": 158, "endOffset": 168}, {"referenceID": 13, "context": "Structured mean-field methods are inner-bound methods where a simpler distribution is optimized as an approximation to the posterior in a KL-divergence sense [8, 1, 14].", "startOffset": 158, "endOffset": 168}, {"referenceID": 18, "context": "Surprisingly, [19, 17] have shown that the sum-product belief propagation provides a lower bound to the partition function for super-modular potential functions.", "startOffset": 14, "endOffset": 22}, {"referenceID": 16, "context": "Surprisingly, [19, 17] have shown that the sum-product belief propagation provides a lower bound to the partition function for super-modular potential functions.", "startOffset": 14, "endOffset": 22}, {"referenceID": 15, "context": "Following [16, 20], we showed here that one can take advantage of efficient MAP solvers to generate approximate or unbiased samples from the Gibbs distribution, when we randomly perturb the potential function.", "startOffset": 10, "endOffset": 18}, {"referenceID": 19, "context": "Following [16, 20], we showed here that one can take advantage of efficient MAP solvers to generate approximate or unbiased samples from the Gibbs distribution, when we randomly perturb the potential function.", "startOffset": 10, "endOffset": 18}], "year": 2013, "abstractText": "In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide means for drawing either approximate or unbiased samples from Gibbs\u2019 distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical \u201chigh signal high coupling\u201d regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds.", "creator": "LaTeX with hyperref package"}}}