{"id": "1310.0354", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2013", "title": "Deep and Wide Multiscale Recursive Networks for Robust Image Labeling", "abstract": "Feedforward multilayer networks trained by supervised learning have recently demonstrated state of the art performance on image labeling problems such as boundary prediction and scene parsing. As even very low error rates can limit practical usage of such systems, methods that perform closer to human accuracy remain desirable. In this work, we propose a new type of network with the following properties that address what we hypothesize to be limiting aspects of existing methods: (1) a `wide' structure with thousands of features, (2) a large field of view, (3) recursive iterations that exploit statistical dependencies in label space, and (4) a parallelizable architecture that can be trained in a fraction of the time compared to benchmark multilayer convolutional networks. For the specific image labeling problem of boundary prediction, we also introduce a novel example weighting algorithm that improves segmentation accuracy. Experiments in the challenging domain of connectomic reconstruction of neural circuity from 3d electron microscopy data show that these \"Deep And Wide Multiscale Recursive\" (DAWMR) networks lead to new levels of image labeling performance. The highest performing architecture has twelve layers, interwoven supervised and unsupervised stages, and uses an input field of view of 157,464 voxels ($54^3$) to make a prediction at each image location. We present an associated open source software package that enables the simple and flexible creation of DAWMR networks.", "histories": [["v1", "Tue, 1 Oct 2013 15:42:54 GMT  (4068kb,D)", "https://arxiv.org/abs/1310.0354v1", null], ["v2", "Wed, 30 Oct 2013 21:16:45 GMT  (4068kb,D)", "http://arxiv.org/abs/1310.0354v2", null], ["v3", "Fri, 6 Dec 2013 17:00:03 GMT  (4069kb,D)", "http://arxiv.org/abs/1310.0354v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["gary b huang", "viren jain"], "accepted": true, "id": "1310.0354"}, "pdf": {"name": "1310.0354.pdf", "metadata": {"source": "META", "title": "Deep and Wide Multiscale Recursive Networks for Robust Image Labeling", "authors": ["Gary B. Huang"], "emails": ["jainv}@janelia.hhmi.org"], "sections": [{"heading": "1 Introduction", "text": "In boundary prediction, the goal is to predict whether each pixel in an image belongs to the interior or boundary of an object [24]; in scene analysis, the goal is to associate with each pixel a multidimensional vector that identifies the category of the object to which that pixel belongs [9]. These types of tasks are distinguished from traditional object recognition, for which pixel-by-pixel mappings are usually irrelevant, and the goal is to generate a single global prediction of object identity. In analysis, pixel-by-pixel truth data is recently generated for caption tasks that have traditionally been solved by entirely hand-designed methods [24]. This enabled the use of learning methods that require comprehensive parent parameter learning."}, {"heading": "2 Networks for Image Labeling: Prior Work and Desiderata", "text": "In fact, it is the case that most of them will be able to abide by the rules which they have imposed on themselves, and that they will be able to abide by the rules which they have imposed on themselves. (...) It is not the case that they are able to abide by the rules. (...) It is not the case that they are able to change the rules, and it is not the case that they are able to abide by the rules. (...) It is the case that they are able to break the rules in order to break the rules. (...) It is the case that they are able to break the rules. (...) It is the case that they are able to break the rules. (...) (...) \"(...\" (...) \"(\") ((\") ((\") ((\") (\") ((\") ((\") (\") ((\") ((\") (\") (\") ((\") (\") ((\") () (() () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () (() () () () ((() () () (() (() (() () () (() (() (() () ((() () (() () ((() (() (() (() ((() (() (() ((() (() ((() (((() () (() () (((() () () () (() () (((() () (((() ((() (() ((() (() () ((((())) (((((())) (((("}, {"heading": "3 Deep and Wide Multiscale Recursive Networks", "text": "We formalize the problem of image caption as follows: In view of an input image I, we want to predict a vector of labeling Yl at each location. I is a three-dimensional size volume in the order of 10003 for the main dataset under consideration, and a vector of 3 labels (| Yl | = 3) is associated with each location, indicating the 3d connectivity of neighbors (see Section 4.1.2 for details of data and labels). To simplify notation, we treat location l as a single dimension in terms of operations that will be discussed later, such as pooling, but in practice such operations will be in 3D. In this section, we describe our proposed method for image labeling, Deep and Wide Multiscale Recursive (DAWMR) networks. DAWMR networks process images by recursively iterating a core network architecture. Overall, a DAWMR network can display dozens of individual processing layers between the raw inscription and final inscription."}, {"heading": "3.1 Single Iteration Core Architecture", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "4 Boundary Prediction Experiments", "text": "We conducted detailed experiments in boundary prediction using electron microscopic imaging of neural tissue, which has a significant impact on the feasibility of connectomics, an emerging effort in neurobiology to measure large-scale maps of neural circuits in the resolution of connectivity of individual synapses [23]. Reconstruction is currently the bottleneck in large-scale mapping projects due to the slow speed of purely manual reconstruction techniques. [14] Fully automated methods of reconstruction would therefore be ideal. Current pipelines typically begin with a boundary prediction step followed by over-segmentation of the resulting boundary map and finally the application of an agglomeration algorithm to assemble object fragments [1, 16]. Improvements in boundary prediction are desirable as certain types of errors (such as subtle subsegmentation) are sometimes difficult to correct in later stages of the reflection pipeline."}, {"heading": "4.1 Experimental Setup", "text": "Here we describe the details of the image data and the training / test sets. The experiments were carried out with our parallel computer software, which is available online; for further details see section C.1."}, {"heading": "4.1.1 Image Acquisition", "text": "Neuropil of drosophila melanogaster was imaged using focused ion beam scanning electron microscopy (FIB-SEM [18]) with a resolution of 8x8x8 nm. Tissue was produced using high-pressure freeze substitution and stained with heavy metals to obtain contrast during electron microscopy. Compared to conventional methods of electron microscopy such as mass-section transmission electron microscopy (ssTEM), FIB-SEM offers the ability to image tissues in all three spatial dimensions with very high resolution."}, {"heading": "4.1.2 Training, Test Sets", "text": "The first volume was used for training and the second for validation and verification. Initially, human commentators densely labeled subvolumes from both images, forming a preliminary training set, which is referred to in the sequel as a small training set (5.2 million labels), and the validation set (16 million labels), followed by an interactive process in which human commentators perform their machine-generated segmentations by visually checking dense reconstructions within small subvolumes and correcting errors. The correction interface enabled annotators to make pixel-level modifications, and the correction reading comments were then added to the small training set and validation to form the \"complete\" training set (120 million labels) and the test set (46 million labels), where the labels are binary and indicate the connectivity of adjacent validity."}, {"heading": "4.1.3 Evaluation Measures", "text": "In the face of densely labeled soil truth segmentation, performance can be measured in two ways: classification metrics for binary representations of segmentation (such as a boundary map or affinity graph) or segmentation-based metrics that interpret the volume as a cluster of pixels. In this paper, we report on both types of metrics. Since soil truth has a class imbalance that is biased toward positive (connected) edges, we report balanced class accuracy (symmetric-acc: 0.5 \u00b7 accuracy at positive edges + 0.5 \u00b7 accuracy at negative edges). We also calculate areas below the receiver operating threshold that form a characteristic curve when we vary the decision threshold for classifying positive / negative edges (AUC edge)."}, {"heading": "4.2 Model Selection Experiments on a Validation Set", "text": "Like other deep, multi-layered architectures, DAWMR networks have a number of model / architecture parameters that can be varied. In this section, we conduct model selection experiments with the validation set. Unless expressly stated otherwise, our experiments use the following setup: Feature extraction modules generate a feature representation of the dimension hul = 8000, individual filters use 3d 53 patches, and classification is done using a MLP with a single hidden layer with 200 hidden units, and are trained with a balanced selection of positive and negative training examples."}, {"heading": "4.2.1 Single-Iteration Classifiers and Comparison With Convolutional Networks", "text": "We will start by evaluating the performance of individual DAWMR classifiers and a monitored Convolutionary Network. We will consider five DAWMR architectures: 53 RF, single-scale vector quantization without pooling (SS), single-scale VQ with foveated representation (SS-FV), and multi-scale VQ with foveated representation (MS-FV). We will also test a version of the SS FV architecture with 2D filters (other architectures use 3D filters). For both architectures that use a foveated representation, we will bundle across a 53 neighborhood, and so the 53 RF and SS-FV architectures have the same field of view. Table 1 also provides an overview of architecture.Validation performance of individual iteration DAWMR networks using the feature extraction architectures mentioned above is shown in Table 2. The performance of a supervised Convolutionary Network (CN) is present and represents a strong baseline for comparing MMMR networks using the above-mentioned Extraction Layer."}, {"heading": "4.2.2 Recursive Multiscale Foveated Dropout (MS-FV-DO) Architecture", "text": "A specific core architecture can be applied recursively across multiple iterations, as discussed at the end of Section 3.1. In this section, we are experimenting with this approach using the multi-scale foveated dropout (MS-FV-DO) architecture. For the second and third iteration classifiers, which accept both an affinity graph and the original image as input, there is a model selection that is related to whether filters in the feature extraction phase only receive input from the image, only the affinity graph, or both. We found that dividing the feature sets into an equal number that look exclusively at each input channel works better than if all filters receive input from all input channels. Table 3 shows the results from recursive application of the MS-FV-DO architecture. Note that each iteration learns its own uncontrolled and monitored parameters, thereby tripling the model parameters used to generate the final output."}, {"heading": "4.2.3 Recursive MS-FV-DO Architecture with Local Error Density (LED) Weighting", "text": "Visual inspections of recursive predictions confirmed that boundary prediction generally improves over multiple iterations, but also showed that predictions did not improve at certain rare image locations, which were characterized by a specific characteristic: a high local density of boundary prediction errors that exist even in the first iteration. Locally correlated boundary prediction errors tend to cause errors in segmentation and are therefore avoidable. However, because these locations are rare, they have a negligible impact on the accuracy of boundary prediction (the metric is actually optimized during training).Previous work has addressed this problem by proposing learning algorithms that directly optimize segmentation performance [32, 12]. These algorithms are computationally expensive and can make the convergence of online gradient lineage sensitive to various parameter decisions in the loss function and optimization process."}, {"heading": "4.3 Test Set Evaluation and Comparison", "text": "Based on the experiments performed with the validation set, we selected some architectures for evaluation across the entire test set. Details of the architectures are reviewed in Table 5, summary results in Table 6, complete diagrams of limit prediction and segmentation performance in Figure 4, and 2d snippets of the predicted affinity diagrams in Figure 3. The results of the test set are consistent with the validation set experiments. Using a 5 million sample training set (\"sm\"), the MS-FV-DO architecture improves the segmentation performance of the DAWMR architecture with significantly less training time. Switching to a larger training set (\"lg\") improves MS-FV-DO limit prediction performance. 4 recursive iterations and LED weighting significantly improve the segmentation performance of the DAWMR architecture. The results also confirm previous observations that small differences in sedimentation accuracy can be associated with subprecision in the subset."}, {"heading": "5 Discussion", "text": "It is likely that each of these strategies offers different capabilities and advantages. In addition, it is shown that a non-recursive architecture that reaches a very large field of view is very capable of working in a single iteration and using an affinity graph that is worse than a third iteration. In addition, Table 8 shows that a non-recursive architecture that has a very large field of view is worse than a recursive architecture that has a smaller field of view."}, {"heading": "A Supplementary: Additional Model Selection", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "B Supplementary: Network Details", "text": "Here we present details and values of the parameters used in our model.B.1 Convolutionary networkThe Convolutionary Network was trained in accordance with the procedures outlined in previous work [13, 33]. We used sigmoid units and carried out greedy layer-by-layer training of the architecture: 5e5 updates after each layer was added, 2e6 updates for the final architecture. Networks trained with significantly fewer iterations exhibited significantly worse training performance. During the training, we used a balanced sampling strategy that alternated between negative and positive edge positions and selected a 53 cube around each edge as a minibatch. Learning rates were set to 0.1, with the exception of the last layer (set to 0.01). A square loss [33, 12] was optimized at a distance of 0.3.B.2 Multilayer perceptrons in DAWMR architectures were trained using minibatch sizes of 40 positive and balanced edges."}, {"heading": "C Supplementary: DAWMR Implementation and Training Time", "text": "In this section we describe the code implementation details and training time analysis for the mentioned networks."}], "references": [{"title": "3d segmentation of sbfsem images of neuropil by a graphical model over supervoxel boundaries", "author": ["B. Andres", "U. Koethe", "T. Kroeger", "M. Helmstaedter", "K.L. Briggman", "W. Denk", "F.A. Hamprecht"], "venue": "Medical image analysis, 16(4):796\u2013805,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. Le Cun"], "venue": "Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on, pages 489\u2013494. IEEE,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep neural networks segment neuronal membranes in electron microscopy images", "author": ["D. Ciresan", "A. Giusti", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Emergence of object-selective features in unsupervised feature learning", "author": ["A. Coates", "A. Karpathy", "A. Ng"], "venue": "Advances in Neural Information Processing Systems 25, pages 2690\u20132698,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 921\u2013928,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 2528\u20132536,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 215\u2013223,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. Collobert"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 224\u2013232,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "arXiv preprint arXiv:1202.2160,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectomic reconstruction of the inner plexiform layer in the mouse retina", "author": ["M. Helmstaedter", "K.L. Briggman", "S.C. Turaga", "V. Jain", "H.S. Seung", "W. Denk"], "venue": "Nature, 500(7461):168\u2013174,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Boundary Learning by Optimization with Topological Constraints", "author": ["V. Jain", "B. Bollmann", "M. Richardson", "D. Berger", "M. Helmstaedter", "K. Briggman", "W. Denk", "J. Bowden", "J. Mendenhall", "W. Abraham", "K. Harris", "N. Kasthuri", "K. Hayworth", "R. Schalek", "J. Tapia", "J. Lichtman", "H. Seung"], "venue": "Computer Vision and Pattern Recognition, IEEE Computer Society Conference on,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised learning of image restoration with convolutional networks", "author": ["V. Jain", "J.F. Murray", "F. Roth", "S.C. Turaga", "V. Zhigulin", "K.L. Briggman", "M.N. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "Computer Vision, IEEE International Conference on, 0:1\u20138,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Machines that learn to segment images: a crucial technology for connectomics", "author": ["V. Jain", "H. Seung", "S. Turaga"], "venue": "Current opinion in neurobiology,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural image denoising with convolutional networks", "author": ["V. Jain", "S. Seung"], "venue": "Advances in Neural Information Processing Systems, pages 769\u2013776,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to agglomerate superpixel hierarchies", "author": ["V. Jain", "S.C. Turaga", "K. Briggman", "M.N. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems, pages 648\u2013656,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146\u20132153", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "IEEE,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Serial section scanning electron microscopy of adult brain tissue using focused ion beam milling", "author": ["G. Knott", "H. Marchman", "D. Wall", "B. Lich"], "venue": "Journal of Neuroscience, 28(12):2959,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1106\u20131114,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1112.6209,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Markov random field models in computer vision", "author": ["S. Li"], "venue": "Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "The big and the small: challenges of imaging the brain\u2019s circuits", "author": ["J.W. Lichtman", "W. Denk"], "venue": "Science, 334(6056):618\u2013623,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["D.R. Martin", "C.C. Fowlkes", "J. Malik"], "venue": "IEEE Trans. Patt. Anal. Mach. Intell., pages 530\u2013549,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Off-road obstacle avoidance through end-to-end learning", "author": ["U. Muller", "J. Ben", "E. Cosatto", "B. Flepp", "Y.L. Cun"], "venue": "Advances in neural information processing systems, pages 739\u2013746,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "CNS: a GPU-based framework for simulating cortically-organized networks", "author": ["J. Mutch", "U. Knoblich", "T. Poggio"], "venue": "Technical report, Massachussetts Institute of Technology,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. R\u00e9", "S.J. Wright"], "venue": "arXiv preprint arXiv:1106.5730,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox"], "venue": "PLoS computational biology, 5(11):e1000579,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "A. Ng", "C. Manning"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129\u2013136,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Ilastik: Interactive learning and segmentation toolkit", "author": ["C. Sommer", "C. Straehle", "U. Kothe", "F.A. Hamprecht"], "venue": "Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on, pages 230\u2013233. IEEE,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, pages 1453\u20131484,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Maximin affinity learning of image segmentation", "author": ["S.C. Turaga", "K.L. Briggman", "M. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional networks can learn to generate affinity graphs for image segmentation", "author": ["S.C. Turaga", "J.F. Murray", "V. Jain", "F. Roth", "M. Helmstaedter", "K. Briggman", "W. Denk", "H.S. Seung"], "venue": "Neural Computation, 22(2):511\u2013538,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward objective evaluation of image segmentation algorithms", "author": ["R. Unnikrishnan", "C. Pantofaru", "M. Hebert"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(6):929,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 23, "context": "In boundary prediction, for example, the goal is to predict whether each pixel in an image belongs to the interior or boundary of an object [24]; in scene parsing, the goal is to associate with each pixel a multidimensional vector that denotes the category of object to which that pixel belongs [9].", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "In boundary prediction, for example, the goal is to predict whether each pixel in an image belongs to the interior or boundary of an object [24]; in scene parsing, the goal is to associate with each pixel a multidimensional vector that denotes the category of object to which that pixel belongs [9].", "startOffset": 295, "endOffset": 298}, {"referenceID": 23, "context": "Densely-labeled pixel-wise ground truth data sets have recently been generated for image labeling tasks that were traditionally solved by entirely hand-designed methods [24].", "startOffset": 169, "endOffset": 173}, {"referenceID": 8, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 18, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 11, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 2, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 14, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 13, "context": "For example, in the main application studied in this paper, reconstruction of neurons from nanometer-resolution electron microscopy images of brain tissue, even small pixel-wise error rates can catastrophically deteriorate the utility of automated analysis [14].", "startOffset": 257, "endOffset": 261}, {"referenceID": 16, "context": "Feature extraction is followed by additional processing layers that perform linear or nonlinear classification to generate the desired prediction variables [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "recently adapted convolutional networks to natural image scene labeling by training 2d networks that process the image at multiple scales [9].", "startOffset": 138, "endOffset": 141}, {"referenceID": 12, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 32, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 11, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 31, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 2, "context": ", 2d architectures with pooling operations and ensembles of multiple networks [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 30, "context": "This observation suggests that image labeling is a structured prediction problem in which statistics among output variables should be explicitly modeled [31].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "Given a noisy input image X , inference for p(Y|X) can thus involve both image-dependent aspects (to invert the observation model) as well as interactions among the random variables Y \u2208 Y that reflect prior statistics on valid label configurations [22].", "startOffset": 248, "endOffset": 252}, {"referenceID": 12, "context": "Multilayer network models for image analysis typically outperform MRFs, as the computational expense associated with probabilistic learning and inference substantially restricts the modeling capability of MRF models that are practical to work with [13, 15].", "startOffset": 248, "endOffset": 256}, {"referenceID": 14, "context": "Multilayer network models for image analysis typically outperform MRFs, as the computational expense associated with probabilistic learning and inference substantially restricts the modeling capability of MRF models that are practical to work with [13, 15].", "startOffset": 248, "endOffset": 256}, {"referenceID": 29, "context": "Such long training times can prohibit certain usage scenarios (for example, interactively adding new labeled data based on rapid classifier retraining [30]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28].", "startOffset": 124, "endOffset": 135}, {"referenceID": 3, "context": "Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28].", "startOffset": 124, "endOffset": 135}, {"referenceID": 27, "context": "Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28].", "startOffset": 124, "endOffset": 135}, {"referenceID": 6, "context": "We take advantage of the recent observation that unsupervised clustering and dictionary learning techniques can be used to efficiently learn thousands of features for image recognition tasks [7, 5].", "startOffset": 191, "endOffset": 197}, {"referenceID": 4, "context": "We take advantage of the recent observation that unsupervised clustering and dictionary learning techniques can be used to efficiently learn thousands of features for image recognition tasks [7, 5].", "startOffset": 191, "endOffset": 197}, {"referenceID": 4, "context": "Following Coates and Ng [5], the core vector quantization component in the feature extraction module consists of a dictionary learned using a greedy variant of orthogonal matching pursuit (OMP-1) and encoding using soft-thresholding with reverse polarity.", "startOffset": 24, "endOffset": 27}, {"referenceID": 12, "context": "This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classification is based on input from all feature maps in the final hidden layer from feature map values within a 5 pixel window centered at the location being classified.", "startOffset": 67, "endOffset": 79}, {"referenceID": 15, "context": "This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classification is based on input from all feature maps in the final hidden layer from feature map values within a 5 pixel window centered at the location being classified.", "startOffset": 67, "endOffset": 79}, {"referenceID": 32, "context": "This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classification is based on input from all feature maps in the final hidden layer from feature map values within a 5 pixel window centered at the location being classified.", "startOffset": 67, "endOffset": 79}, {"referenceID": 10, "context": "For the classification module, we use a multilayer perceptron (MLP) with a single hidden layer trained by mini-batch stochastic gradient descent [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 1, "context": "Pioneering work established graph transformer networks for solving segmentation and recognition tasks arising in automated document processing [2].", "startOffset": 143, "endOffset": 146}, {"referenceID": 28, "context": "More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8].", "startOffset": 83, "endOffset": 91}, {"referenceID": 15, "context": "More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8].", "startOffset": 83, "endOffset": 91}, {"referenceID": 7, "context": "More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 22, "context": "This application has significant implications for the feasability of \u2018connectomics\u2019, an emerging endeavour in neurobiology to measure large-scale maps of neural circuitry at the resolution of single-synapse connectivity [23].", "startOffset": 220, "endOffset": 224}, {"referenceID": 13, "context": "Reconstruction is currently the bottleneck in large-scale mapping projects due to the slow rate of purely manual reconstruction techniques [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "Current pipelines typically begin with a boundary prediction step, followed by oversegmentation of the resulting boundary map, and finally application of an agglomeration algorithm to piece together object fragments [1, 16].", "startOffset": 216, "endOffset": 223}, {"referenceID": 15, "context": "Current pipelines typically begin with a boundary prediction step, followed by oversegmentation of the resulting boundary map, and finally application of an agglomeration algorithm to piece together object fragments [1, 16].", "startOffset": 216, "endOffset": 223}, {"referenceID": 17, "context": "Neuropil from drosophila melanogaster was imaged using focused ion-beam scanning electron microscopy (FIB-SEM [18]) at a resolution of 8x8x8 nm.", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "connectivity through an affinity graph it is possible to represent situations (such as directly adjacent, distinct objects) that would be impossible to represent with a more typical pixel-wise exterior/interior labeling [32].", "startOffset": 220, "endOffset": 224}, {"referenceID": 33, "context": "Segmentation performance is then measured by computing the Rand Index [34].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10].", "startOffset": 260, "endOffset": 272}, {"referenceID": 32, "context": "The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10].", "startOffset": 260, "endOffset": 272}, {"referenceID": 9, "context": "The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10].", "startOffset": 260, "endOffset": 272}, {"referenceID": 25, "context": "The CN was trained on a GPU with an implementation based on the CNS framework [26].", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "Adding drop-out regularization (MS-FV-DO) improves performance of the single iteration DAWMR classifier even further [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "Previous work has addressed this issue by proposing learning algorithms that directly optimize segmentation performance [32, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "Previous work has addressed this issue by proposing learning algorithms that directly optimize segmentation performance [32, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "The results also confirm previous observations that small differences in boundary prediction accuracy may be associated with large differences in segmentation accuracy [12].", "startOffset": 168, "endOffset": 172}, {"referenceID": 26, "context": "Improvements in computing hardware, or fundamentally more parallel versions of stochastic gradient descent may enable larger convolutional network architectures in the future [27, 20].", "startOffset": 175, "endOffset": 183}, {"referenceID": 19, "context": "Improvements in computing hardware, or fundamentally more parallel versions of stochastic gradient descent may enable larger convolutional network architectures in the future [27, 20].", "startOffset": 175, "endOffset": 183}, {"referenceID": 20, "context": "This is in contrast to true \u2018end-to-end\u2019 learning, in which each step is optimized based on updates back-propagated from the final output [21, 25].", "startOffset": 138, "endOffset": 146}, {"referenceID": 24, "context": "This is in contrast to true \u2018end-to-end\u2019 learning, in which each step is optimized based on updates back-propagated from the final output [21, 25].", "startOffset": 138, "endOffset": 146}], "year": 2013, "abstractText": "Feedforward multilayer networks trained by supervised learning have recently demonstrated state of the art performance on image labeling problems such as boundary prediction and scene parsing. As even very low error rates can limit practical usage of such systems, methods that perform closer to human accuracy remain desirable. In this work, we propose a new type of network with the following properties that address what we hypothesize to be limiting aspects of existing methods: (1) a \u2018wide\u2019 structure with thousands of features, (2) a large field of view, (3) recursive iterations that exploit statistical dependencies in label space, and (4) a parallelizable architecture that can be trained in a fraction of the time compared to benchmark multilayer convolutional networks. For the specific image labeling problem of boundary prediction, we also introduce a novel example weighting algorithm that improves segmentation accuracy. Experiments in the challenging domain of connectomic reconstruction of neural circuity from 3d electron microscopy data show that these \u201cDeep And Wide Multiscale Recursive\u201d (DAWMR) networks lead to new levels of image labeling performance. The highest performing architecture has twelve layers, interwoven supervised and unsupervised stages, and uses an input field of view of 157,464 voxels (54) to make a prediction at each image location. We present an associated open source software package that enables the simple and flexible creation of DAWMR networks.", "creator": "LaTeX with hyperref package"}}}