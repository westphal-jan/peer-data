{"id": "1310.0509", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2013", "title": "Summary Statistics for Partitionings and Feature Allocations", "abstract": "Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.", "histories": [["v1", "Tue, 1 Oct 2013 22:34:18 GMT  (99kb)", "https://arxiv.org/abs/1310.0509v1", "NIPS 2013"], ["v2", "Thu, 3 Oct 2013 06:28:18 GMT  (99kb)", "http://arxiv.org/abs/1310.0509v2", "NIPS 2013"], ["v3", "Sat, 5 Oct 2013 18:26:44 GMT  (99kb)", "http://arxiv.org/abs/1310.0509v3", "Accepted to NIPS 2013:this https URL"], ["v4", "Mon, 25 Nov 2013 08:43:59 GMT  (99kb)", "http://arxiv.org/abs/1310.0509v4", "Accepted to NIPS 2013:this https URL"]], "COMMENTS": "NIPS 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["isik baris fidaner", "ali taylan cemgil"], "accepted": true, "id": "1310.0509"}, "pdf": {"name": "1310.0509.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["fidaner@alternatifbilisim.org", "taylan.cemgil@boun.edu.tr"], "sections": [{"heading": null, "text": "ar Xiv: 131 0.05 09v4 [cs.LG] 2 5N ov"}, {"heading": "1 Introduction", "text": "These models are based on non-parametric approaches such as the Dirichlet process (DP), which enables the formulation of efficient inference methods. [7] Studies on infinite blending models inspired the development of several other models, including Indian buffets (PDP) and constructions such as the Chinese restaurant process (CRP)."}, {"heading": "2 Basic definitions and the motivating problem", "text": "We start with basic definitions. A partitioning of a set of elements [n] = {1, 2,., n} is a set of blocks Z = {B1,. \u2212 Z |} such that Bi [n] and Bi [n] are for all i [1,., n), Bi [2], Bi [3], Bj [4], and [4], Bi [5], and [5], so that Z [n] is a partitioning of [n].3 Example sentence E = {Z (1),., T., Bi [5], Bj [6], and [6], and [6], and [6], is a multiplication such that Z [n] is for all t [1,.,.,., T.}. We are obliged to extract information from this sample. Our motivation is the following problem: a set of observed elements (x1,., xn) are characterized by an infinite model with component."}, {"heading": "3 Cumulative statistics to represent structure", "text": "We define the cumulative block size distribution or \"cumulative statistics\" in short because the function \u03c6k (Z) = \u2211 i [Bi | \u2265 k], which counts the size blocks of the partition at least k, can paraphrase the previous statistics: number of blocks as \u03c61 (Z), exact block size distribution as \u03c6k (Z) \u2212 \u03c6k + 1 (Z), and paired occurrence as \u03c62 (PROJ (Z, a, b)). Furthermore, cumulative statistics fulfill the following property: for partitioning [n], \u03c6 (Z) always adds up to n, just like a probability mass function, which adds up to 1. If blocks of Z are sorted according to their sizes and indicators [Bi], k] are arranged on a matrix, as in Figure 1a, they form a young graph showing that partitioning (Z) is always the conjugated partition of Z."}, {"heading": "4 Entropy to quantify segmentation", "text": "Shannon's entropy [17] can be an appropriate size to measure the \"segmentation\" in relation to the partitions (B = 10) that can be interpreted as probability distributions (20, 21). Since this interpretation does not cover the segment size, we will make an alternative, element-based definition of entropy. How can a block B inform us about its elements? Each element has a fraction of 1 / | B |, let's call this size per element segment size smaller if the block were larger. To quantify this information, we define per element information for a block B as the integral of segment size 1 / s. < n, the block provides positive information because the segment size is larger than minimal, and we know that its segment size could be smaller if the block were larger."}, {"heading": "5 Entropy agglomeration and experimental results", "text": "We want to summarize a sample that uses the proposed genes. Permutations, which provide lower entropy sequences, are significant for each individual entropy gene, but a workable algorithm can only include a small subset of n! -permutations. We define entropy-agglomeration algorithms: 1, 2, 2, 2, 2, 4, 4, 5, 5, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9"}, {"heading": "6 Conclusion", "text": "In this paper, we developed a novel approach to summarizing sample sets of partitioning and attribute mapping. After presenting the problem, we introduced cumulative statistics and cumulative distribution matrices for each of their permutations to represent a sample set in a systematic manner. We defined information per element to calculate entropy sequences for these permutations. We developed an entropy agglomeration algorithm (EA) that selects and visualizes a small subset of these entropy sequences. Finally, we experimented with different sets of data to demonstrate the method. Entropy agglomeration is a simple algorithm whose implementation does not require much knowledge, but which is conceptually based on the cumulative statistics we submitted. Since we primarily aimed to formulate a useful algorithm, we made only the essential definitions, and some points still need to be clarified."}, {"heading": "Acknowledgements", "text": "We would like to thank Ayc a Cankorur, Erkan Karabekmez, Duygu Dikiciog lu and Betu l K\u0131rdar from the Bog azic i University Chemical Engineering for introducing us to this problem through very helpful discussions, which were financed by the TU \"BI\" TAK (110E292) and BAP (6882-12A01D5)."}], "references": [{"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1973}, {"title": "The two-parameter Poisson\u2013Dirichlet distribution derived from a stable subordinator", "author": ["J. Pitman", "M. Yor"], "venue": "Annals of Probability,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Combinatorial Stochastic Processes", "author": ["J. Pitman"], "venue": "Lecture Notes in Mathematics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "A constructive definition of Dirichlet priors", "author": ["J. Sethuraman"], "venue": "Statistica Sinica,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Markov chain sampling methods for Dirichlet process mixture models, Journal of Computational and Graphical Statistics, 9:249\u2013265", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Modelling dyadic data with binary latent factors", "author": ["E. Meeds", "Z. Ghahramani", "R. Neal", "S. Roweis"], "venue": "In Advances in Neural Information Processing", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Hierarchical Dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "The Indian buffet process: An introduction and review", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Feature allocations, probability functions, and paintboxes", "author": ["T. Broderick", "J. Pitman", "M.I. Jordan"], "venue": "arXiv preprint arXiv:1301.6647", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Modelling genetic variations with fragmentationcoagulation processes", "author": ["Y.W. Teh", "C. Blundell", "L.T. Elliott"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Bayesian Nonparametric Models", "author": ["P. Orbanz", "Y.W. Teh"], "venue": "In Encyclopedia of Machine Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Bayesian infinite mixture model based clustering of gene expression profiles", "author": ["M. Medvedovic", "S. Sivaganesan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Bayesian mixture model based clustering of replicated microarray data", "author": ["M. Medvedovic", "K. Yeung", "R. Bumgarner"], "venue": "Bioinformatics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Contextspecific infinite mixtures for clustering gene expression profiles across diverse microarray", "author": ["Liu X", "S. Sivanagesan", "K.Y. Yeung", "J. Guo", "R.E. Bumgarner", "M. Medvedovic"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "A Mathematical Theory of Communication", "author": ["C.E. Shannon"], "venue": "Bell System Technical Journal 27(3):379\u2013423", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1948}, {"title": "Entropy and inference, revisited", "author": ["I. Nemenman", "F. Shafee", "W. Bialek"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Bayesian Entropy Estimation for Countable Discrete Distributions", "author": ["E. Archer", "I.M. Park", "J. Pillow"], "venue": "arXiv preprint arXiv:1302.0328", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On Generalized Entropy and Entropic Metrics", "author": ["D. Simovici"], "venue": "Journal of Multiple Valued Logic and Soft Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Counting distinctions: on the conceptual foundations of Shannon\u2019s information theory", "author": ["D. Ellerman"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Bayesian mixture modeling, in Maximum Entropy and Bayesian Methods", "author": ["R.M. Neal"], "venue": "Proceedings of the 11th International Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis, Seattle,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1992}, {"title": "Cluster analysis and display of genomewide expression patterns", "author": ["M.B. Eisen", "P.T. Spellman", "P.O. Brown", "D. Botstein"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of Eugenics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1936}, {"title": "Integrated genomic and proteomic analyses of a systematically perturbed metabolic", "author": ["T. Ideker", "V. Thorsson", "J.A. Ranish", "R. Christmas", "J. Buhler", "J.K. Eng", "R. Bumgarner", "D.R. Goodlett", "R. Aebersold", "L. Hood"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "The COW-2 International Organizations Dataset Version 2.0", "author": ["J.C. Pevehouse", "T. Nordstrom", "K. Warnke"], "venue": "Conflict Management and Peace Science 21(2):101-119", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].", "startOffset": 78, "endOffset": 84}, {"referenceID": 1, "context": "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].", "startOffset": 199, "endOffset": 202}, {"referenceID": 3, "context": "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].", "startOffset": 230, "endOffset": 233}, {"referenceID": 4, "context": "These models are based on nonparametric priors such as Dirichlet process (DP) [1, 2], its superclass Poisson-Dirichlet process (PDP) [3, 4] and constructions such as Chinese restaurant process (CRP) [5] and stick-breaking process [6] that enable formulations of efficient inference methods [7].", "startOffset": 290, "endOffset": 293}, {"referenceID": 5, "context": "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].", "startOffset": 84, "endOffset": 90}, {"referenceID": 6, "context": "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].", "startOffset": 84, "endOffset": 90}, {"referenceID": 7, "context": "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].", "startOffset": 157, "endOffset": 165}, {"referenceID": 8, "context": "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].", "startOffset": 157, "endOffset": 165}, {"referenceID": 9, "context": "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].", "startOffset": 222, "endOffset": 226}, {"referenceID": 10, "context": "Studies on infinite mixture models inspired the development of several other models [8, 9] including Indian buffet process (IBP) for infinite feature models [10, 11] and fragmentation-coagulation process for sequence data [12] all of which belong to Bayesian nonparametrics [13].", "startOffset": 274, "endOffset": 278}, {"referenceID": 11, "context": "This problem to \u2018summarize\u2019 the samples from the infinite mixture posterior was raised in bioinformatics literature in 2002 by Medvedovic and Sivaganesan for clustering gene expression profiles [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "But the question proved difficult and they \u2018circumvented\u2019 it by using a heuristic linkage algorithm based on pairwise occurence probabilities [15, 16].", "startOffset": 142, "endOffset": 150}, {"referenceID": 13, "context": "But the question proved difficult and they \u2018circumvented\u2019 it by using a heuristic linkage algorithm based on pairwise occurence probabilities [15, 16].", "startOffset": 142, "endOffset": 150}, {"referenceID": 14, "context": "showed in 2002 that the entropy [17] of a DP posterior was strongly determined by its prior hyperparameters [18].", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "showed in 2002 that the entropy [17] of a DP posterior was strongly determined by its prior hyperparameters [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "recently elaborated these results with respect to PDP [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "In other work, entropy was generalized to partitionings by interpreting partitionings as probability distributions [20, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "In other work, entropy was generalized to partitionings by interpreting partitionings as probability distributions [20, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 2, "context": ", zn) drawn from a two-parameter CRP prior with concentration \u03b1 and discount d [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 19, "context": "z \u223c CRP (z;\u03b1, d) \u03b8 \u223c p(\u03b8) xi | zi, \u03b8 \u223c F (xi | \u03b8 i) (1) In the conjugate case, all \u03b8 can be integrated out to get p(zi | z\u2212i, x) for sampling zi [22]: p(zi | z\u2212i, x) \u221d \u222b", "startOffset": 145, "endOffset": 149}, {"referenceID": 8, "context": "In addition, we aim to extract information from feature allocations, which constitute a superclass of partitionings [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "t=1 f(Z) \u2248 \u3008 f(Z) \u3009\u03c0(Z) (3) Which f(Z) would be a useful statistic for Z? Three statistics commonly appear in the literature: First one is the number of blocks |Z|, which has been analyzed theoretically for various nonparametric priors [2, 5].", "startOffset": 236, "endOffset": 242}, {"referenceID": 11, "context": "A common statistic is pairwise occurence, which is used to extract information from infinite mixture posteriors in applications like bioinformatics [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 8, "context": "Another statistic is exact block size distribution (referred to as \u2018multiplicities\u2019 in [11, 19]).", "startOffset": 87, "endOffset": 95}, {"referenceID": 16, "context": "Another statistic is exact block size distribution (referred to as \u2018multiplicities\u2019 in [11, 19]).", "startOffset": 87, "endOffset": 95}, {"referenceID": 14, "context": "4 Entropy to quantify segmentation Shannon\u2019s entropy [17] can be an appropriate quantity to measure \u2018segmentation\u2019 with respect to partitionings, which can be interpreted as probability distributions [20, 21].", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "4 Entropy to quantify segmentation Shannon\u2019s entropy [17] can be an appropriate quantity to measure \u2018segmentation\u2019 with respect to partitionings, which can be interpreted as probability distributions [20, 21].", "startOffset": 200, "endOffset": 208}, {"referenceID": 18, "context": "4 Entropy to quantify segmentation Shannon\u2019s entropy [17] can be an appropriate quantity to measure \u2018segmentation\u2019 with respect to partitionings, which can be interpreted as probability distributions [20, 21].", "startOffset": 200, "endOffset": 208}, {"referenceID": 14, "context": "Total weighted information for Z gives Shannon\u2019s entropy function [17] which can be written in terms of the cumulative statistics (assuming \u03c6n+1 = 0):", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "Entropy Agglomeration is inspired by \u2018agglomerative clustering\u2019, a standard approach in bioinformatics [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "To summarize partitionings of gene expressions, [14] applied agglomerative clustering by pairwise occurences.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "2) Iris flower data (Figure 9c): This well-known dataset contains 150 points on R from three flower species [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 22, "context": "3) Galactose data (Figure 9d): This is a dataset of gene expressions by 820 genes in 20 experimental conditions [25].", "startOffset": 112, "endOffset": 116}], "year": 2013, "abstractText": "Infinite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or find its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based definition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.", "creator": "LaTeX with hyperref package"}}}