{"id": "1310.1415", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2013", "title": "Narrowing the Gap: Random Forests In Theory and In Practice", "abstract": "Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoretically tractable variant of random regression forests and prove that our algorithm is consistent. We also provide an empirical evaluation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in practice. Our experiments provide insight into the relative importance of different simplifications that theoreticians have made to obtain tractable models for analysis.", "histories": [["v1", "Fri, 4 Oct 2013 22:33:35 GMT  (4536kb,D)", "http://arxiv.org/abs/1310.1415v1", "Under review by the International Conference on Machine Learning (ICML) 2014"]], "COMMENTS": "Under review by the International Conference on Machine Learning (ICML) 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["misha denil", "david matheson", "nando de freitas"], "accepted": true, "id": "1310.1415"}, "pdf": {"name": "1310.1415.pdf", "metadata": {"source": "META", "title": "Narrowing the Gap: Random ForestsIn Theory and In Practice", "authors": ["Misha Denil", "David Matheson"], "emails": ["mdenil@cs.ubc.ca", "davidm@cs.ubc.ca", "nando@cs.ox.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Random forests are a type of ensemble method that makes predictions by averaging over the predictions of several independent base models. Since its introduction by Breiman (2001), the Random Forests Framework has been extremely successful as a universal classification and regression method. Despite its widespread application, there remains a gap between the theoretical understanding of random forests and their practical application. A variety of random forest algorithms has appeared in literature, with great practical success. However, these algorithms are difficult to analyze, and the basic mathematical properties of even the original variant are still not well understood (Biau, 2012).Preliminary work. Review by the International Conference on Machine Learning (ICML). This state of things has led to a polarization between theoretical and empirical contributions in literature. Empirically focused work describes elaborate extensions of the basic random forest framework by adding specific refinements that advance the state but do not advance the performance."}, {"heading": "2. Related work", "text": "In fact, it is the case that most of them are able to survive themselves if they do not see themselves as being able to save themselves. (...) Most of them are able to save themselves. (...) Most of them are not able to save themselves. (...) Most of them are not able to save themselves. (...) Most of them are able to save themselves. (...) Most of them are not able to save themselves. (...) Most of them are not able to save themselves. (...) Most of them are able to save themselves. (...) Most of them are able to save themselves. (...) Most of them are able to save themselves. (...) Most of them are able to save themselves. (...) Most of them are not able to save themselves. (...)"}, {"heading": "3. Random Forests", "text": "In this section we briefly review the random forests framework. For a more comprehensive review, we refer the reader to Breiman (2001) and Criminisi et al. (2011). Random forests are built by combining the predictions of several trees, each of which is trained in isolation. Unlike the increase (Schapire & Freund, 2012), where the basic models are trained and combined with a sophisticated weight scheme, the trees are usually trained independently and the predictions of the trees are averaged. There are three main decisions that are made when building a random tree: (1) the method of splitting the leaves, (2) the type of prediction to be used in each leaf, and (3) the method of injecting randomness into the trees. Defining a method of splitting leaves requires selecting the shapes of the candidates as well as a method of evaluating the quality of each candidate."}, {"heading": "4. Algorithm", "text": "In this section we describe how our random forest algorithm works. Each tree in the random regression forest is constructed independently of each other. Unlike the random forests of Breiman (2001), we do not create bootstrapping between the different trees."}, {"heading": "4.1. Tree construction", "text": "Each node of the tree corresponds to a rectangular subset of FD, and at each step of the construction, a leaf of the tree is selected for expansion. In each tree, we randomly divide the data set into two parts, each of which plays a different role in the tree construction. We refer to points that are assigned to the different parts as structure and estimation points. Structure points may influence the shape of the tree. They are used to determine splinter dimensions and splinter points in each internal node of the tree. However, structure points must not affect the predictions made in the tree leaves. Estimates play a double role. These points are used to adjust the estimators in each leaf of the tree, but have no effect on the shape of the tree partition. The data is randomly divided into each part by assigning a part to the respective structure, whereby we cannot equate the required probability with a third partition."}, {"heading": "4.2. Leaf expansion", "text": "When a leaf is selected for expansion, we randomly select min (1 + Poisson (\u03bb), D) different candidate dimensions. We select a slit point for the leaf by searching over the candidate column points in each of the candidate dimensions. A crucial difference between our algorithm and standard random forests is how we search for slit points in the candidate dimensions. In a standard random forest, points are projected into the candidate dimension and each possible slit point is evaluated as a candidate slit point. In our algorithm, we limit the scope of the search by first selecting m of the structure points in the leaf and evaluating the candidate column points only over the range defined by these points. Limitation of the range in this way forces the trees to be (approximately) balanced, and is calculated in Figure 1.For each candidate slit point S, we calculate the reduction of the square error, Err (L) A = Nj = L-points (Ij = L) and (Ij = L-points are the children {A)."}, {"heading": "4.3. Prediction", "text": "Once the forest is trained, it can be used to make predictions for new unlabeled data points. To make a prediction for a query point x, each tree independently predicts jn (x) = 1Ne (An (x)) \u2211 Yi-An (x) Ii = e Yi and the forest gives on average the predictions of each tree (M) n (x) = 1M M M-j = 1 f jn (x) Here An (x) denotes the leaf that contains x, and Ne (An (x)) denotes the number of estimated points contained therein. Note that the predictions of each tree depend only on the estimated points in that tree; however, since points are independently assigned to the structures and estimated parts in each tree, structure points in a tree have the ability to contribute as estimated points in another tree."}, {"heading": "5. Consistency", "text": "In this section, we will prove the consistency of the random regression forest model described in this thesis. We designate a tree partition created by our algorithm on data Dn = {Xi, Yi} ni = 1 as fn. Since n varies, we obtain a sequence of classifiers and we are interested in showing that the sequence {fn} is more consistent than n. Specifically, definition 1 (X) - a sequence of estimators {fn} is consistent for a certain distribution to (X, Y) if the value of the risk functionR (fn) = EX, Z, Dn (X, Z) \u2212 f (X) - 2] converts to 0 as n \u2192 number of estimators where f (X) = E [Y = x] is the (unknown) regression function.To show that our random forest classifiers are consistent, we will define their structure as empirical estimation profiling.2."}, {"heading": "6. Discussion", "text": "In this section, we describe two different random forest models previously analyzed in the literature. We discuss some of the differences between them and the model in this paper, and the relationship of the three models to Breiman's original algorithm is simple."}, {"heading": "7. Experiments", "text": "In this section, we will empirically compare our algorithm with Biau08 and Biau12 (described in Section 6) and Breiman (the original algorithm described in Breiman (2001)) on several datasets. The purpose of these experiments is to provide insights into the relative effects of the various simplifications used to achieve theoretical traceability. To this end, we have decided to evaluate the different algorithms on several realistic tasks, including the extremely challenging common prediction problem from computer vision. Since the algorithms are each slightly differently parameterized, it is not possible to use the same parameters for all of them. Breiman and our own algorithm specify a minimum sheet size, which we set to 5 in accordance with Breiman's advice for regression (Breiman, 2001). Biau08 and Biau12 are parameterized in relation to a target number of sheets and not to a minimum sheet size. For these algorithms, we shall choose the number of 5 / n of sheets."}, {"heading": "7.1. UCI datasets", "text": "For our first series of experiments, we used four sets of data from the UCI repository. A summary of the data sets can be seen in Table 1. With the exception of diabetes, these data sets were selected for their relatively large number of instances and characteristics. In all experiments in this section, we follow Breiman's rule of thumb to use one third of the total number of attributes as candidate dimensions. All results in this section are the mean of five runs with five years validation. For our algorithm, we choose m = 1000 structure points for selecting the candidate dimensions. We experiment with other settings for m, but found that our results are very insensitive to these parameters. Figure 2 compares the performance of several random forest algorithms with the four UCI data sets. The clear trend here is that Breiman's algorithm executes our own algorithms, which in turn execute both algorithms."}, {"heading": "7.2. Kinect Pose Estimation", "text": "In this section, we evaluate our random forest algorithms on the challenging computer vision problem of predicting the location of human joints from a depth of image and corresponding body part labels. See Figure 4 for an example. Typically, the first step in a common position pipeline is to predict the labels of each pixel in the depth image. Since our primary goal is to evaluate regression models, we implement only the second step in the basic pipeline. Use depth images with the grounded truth body labels for each pixel as training data. We learn a regression model from a joint rather than build an end product."}, {"heading": "8. Conclusion", "text": "It is fascinating that an algorithm as simple and useful as random forests has proved so difficult to analyze. Motivated by this fact, we set ourselves the goal of narrowing the gap between the theory and practice of regression forests, and we succeeded significantly. In particular, we succeeded in deriving a new regression forest algorithm, proving that it is consistent, and showing that its empirical performance is closer to Breiman's popular model than previous theoretical variations.Our extensive empirical study, which compares the algorithm that is widely used in practice with more recent theoretical data from mocap.cs.cmu.eduvariant, also sheds light on how different design decisions and theoretical simplifications affect performance. We focused on consistency because this remains an important open problem. However, we believe that our theoretical analysis and empirical study help to prepare the arena for the start of other analyses, including random and large-scale problems of consistency."}, {"heading": "A. Technical results", "text": "Lemma 6,. To are iid uniform [0, 1] random variables thenE [max i = 1 Ui, 1 \u2212 m min i = 1 Ui) = 2m + 12m + 2Proof. Leave Mi = max (Ui, 1 \u2212 Ui), so Mi are iid uniform [1 / 2, 1] with CDF given byFMi (x) = \u2264 \u2212 1for 1 / 2 \u2264 x 1. In addition, if M = maxmi = 1Mi then FM (x) = (2x \u2212 1) m, since the Mi are iid. The density of M is thenfM (x) = d dx FM (x) = 2m \u2212 1 and its expected value isE [M] = 1 / 2 xfM (dx) = 2m + 2 2m + 2which proves the claim."}], "references": [{"title": "Shape quantization and recognition with randomized trees", "author": ["Y. Amit", "D. Geman"], "venue": "Neural Computation,", "citeRegEx": "Amit and Geman,? \\Q1997\\E", "shortCiteRegEx": "Amit and Geman", "year": 1997}, {"title": "Analysis of a random forests model", "author": ["G. Biau"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Biau,? \\Q2012\\E", "shortCiteRegEx": "Biau", "year": 2012}, {"title": "Consistency of random forests and other averaging classifiers", "author": ["G. Biau", "L. Devroye", "G. Lugosi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Biau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Biau et al\\.", "year": 2008}, {"title": "Consistency for a simple model of random forests", "author": ["L. Breiman"], "venue": "Technical report, University of California at Berkeley,", "citeRegEx": "Breiman,? \\Q2004\\E", "shortCiteRegEx": "Breiman", "year": 2004}, {"title": "Classification and Regression Trees", "author": ["L. Breiman", "J. Friedman", "C. Stone", "R. Olshen"], "venue": "CRC Press LLC,", "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Decision Forests for Computer Vision and Medical Image Analysis", "author": ["A. Criminisi", "J. Shotton"], "venue": null, "citeRegEx": "Criminisi and Shotton,? \\Q2013\\E", "shortCiteRegEx": "Criminisi and Shotton", "year": 2013}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semisupervised learning", "author": ["A. Criminisi", "J Shotton", "E. Konukoglu"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Criminisi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Criminisi et al\\.", "year": 2011}, {"title": "Random forests for classification", "author": ["Cutler", "DR", "T. Edwards", "K. Beard"], "venue": "in ecology. Ecology,", "citeRegEx": "Cutler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cutler et al\\.", "year": 2007}, {"title": "Consistency of online random forests", "author": ["M. Denil", "D. Matheson", "N. de Freitas"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization", "author": ["T.G. Dietterich"], "venue": "Machine Learning,", "citeRegEx": "Dietterich,? \\Q2000\\E", "shortCiteRegEx": "Dietterich", "year": 2000}, {"title": "Risk bounds for purely uniformly random forests", "author": ["R. Genuer"], "venue": "arXiv preprint arXiv:1006.2980,", "citeRegEx": "Genuer,? \\Q2010\\E", "shortCiteRegEx": "Genuer", "year": 2010}, {"title": "Variance reduction in purely random forests", "author": ["R. Genuer"], "venue": "Journal of Nonparametric Statistics,", "citeRegEx": "Genuer,? \\Q2012\\E", "shortCiteRegEx": "Genuer", "year": 2012}, {"title": "A Distribution-Free Theory of Nonparametric Regression", "author": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": null, "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2002}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Ho,? \\Q1998\\E", "shortCiteRegEx": "Ho", "year": 1998}, {"title": "Consistency of random survival", "author": ["H. Ishwaran", "U.B. Kogalur"], "venue": "forests. Statistics and probability letters,", "citeRegEx": "Ishwaran and Kogalur,? \\Q2010\\E", "shortCiteRegEx": "Ishwaran and Kogalur", "year": 2010}, {"title": "Multiple decision trees", "author": ["S.W. Kwokt", "C. Carter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Kwokt and Carter,? \\Q1988\\E", "shortCiteRegEx": "Kwokt and Carter", "year": 1988}, {"title": "Random forests and adaptive nearest neighbors", "author": ["Y. Lin", "Y. Jeon"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Lin and Jeon,? \\Q2006\\E", "shortCiteRegEx": "Lin and Jeon", "year": 2006}, {"title": "Quantile regression forests", "author": ["N. Meinshausen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Meinshausen,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen", "year": 2006}, {"title": "Entangled decision forests and their application for semantic segmentation of CT images", "author": ["A. Montillo", "J. Shotton", "J. Winn", "J.E. Iglesias", "D. Metaxas", "A. Criminisi"], "venue": "In Information Processing in Medical Imaging,", "citeRegEx": "Montillo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Montillo et al\\.", "year": 2011}, {"title": "Newer classification and regression tree techniques: Bagging and random forests for ecological prediction", "author": ["A. Prasad", "L. Iverson", "A. Liaw"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2006}, {"title": "Boosting: Foundations and Algorithms", "author": ["R. Schapire", "Y. Freund"], "venue": null, "citeRegEx": "Schapire and Freund,? \\Q2012\\E", "shortCiteRegEx": "Schapire and Freund", "year": 2012}, {"title": "Object class segmentation using random forests", "author": ["F. Schroff", "A. Criminisi", "A. Zisserman"], "venue": "In Procedings of the British Machine Vision Conference,", "citeRegEx": "Schroff et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2008}, {"title": "Realtime human pose recognition in parts from single depth images", "author": ["J. Shotton", "A. Fitzgibbon", "M. Cook", "T. Sharp", "M. Finocchio", "R. Moore", "A. Kipman", "A. Blake"], "venue": "IEEE Computer Vision and Pattern Recognition,", "citeRegEx": "Shotton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shotton et al\\.", "year": 2011}, {"title": "Random forest: a classification and regression tool for compound classification and QSAR modeling", "author": ["V. Svetnik", "A. Liaw", "C. Tong", "J. Culberson", "R. Sheridan", "B. Feuston"], "venue": "Journal of Chemical Information and Computer Sciences,", "citeRegEx": "Svetnik et al\\.,? \\Q1947\\E", "shortCiteRegEx": "Svetnik et al\\.", "year": 1947}, {"title": "Random forests for metric learning with implicit pairwise position dependence", "author": ["C. Xiong", "D. Johnson", "R. Xu", "J.J. Corso"], "venue": "In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Xiong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2012}, {"title": "Atlas encoding by randomized forests for efficient label propagation", "author": ["D. Zikic", "B. Glocker", "A. Criminisi"], "venue": "In International Conference on Medical Image Computing and Computer Assisted Intervention,", "citeRegEx": "Zikic et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zikic et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Since its introduction by Breiman (2001) the random forests framework has been extremely successful as a general purpose classification and regression method.", "startOffset": 26, "endOffset": 41}, {"referenceID": 1, "context": "However, these algorithms are difficult to analyze, and the basic mathematical properties of even the original variant are still not well understood (Biau, 2012).", "startOffset": 149, "endOffset": 161}, {"referenceID": 21, "context": "Empirically focused papers describe elaborate extensions to the basic random forest framework, adding domain specific refinements which push the state of the art in performance, but come with no guarantees (Schroff et al., 2008; Shotton et al., 2011; Montillo et al., 2011; Xiong et al., 2012; Zikic et al., 2012).", "startOffset": 206, "endOffset": 313}, {"referenceID": 22, "context": "Empirically focused papers describe elaborate extensions to the basic random forest framework, adding domain specific refinements which push the state of the art in performance, but come with no guarantees (Schroff et al., 2008; Shotton et al., 2011; Montillo et al., 2011; Xiong et al., 2012; Zikic et al., 2012).", "startOffset": 206, "endOffset": 313}, {"referenceID": 18, "context": "Empirically focused papers describe elaborate extensions to the basic random forest framework, adding domain specific refinements which push the state of the art in performance, but come with no guarantees (Schroff et al., 2008; Shotton et al., 2011; Montillo et al., 2011; Xiong et al., 2012; Zikic et al., 2012).", "startOffset": 206, "endOffset": 313}, {"referenceID": 24, "context": "Empirically focused papers describe elaborate extensions to the basic random forest framework, adding domain specific refinements which push the state of the art in performance, but come with no guarantees (Schroff et al., 2008; Shotton et al., 2011; Montillo et al., 2011; Xiong et al., 2012; Zikic et al., 2012).", "startOffset": 206, "endOffset": 313}, {"referenceID": 25, "context": "Empirically focused papers describe elaborate extensions to the basic random forest framework, adding domain specific refinements which push the state of the art in performance, but come with no guarantees (Schroff et al., 2008; Shotton et al., 2011; Montillo et al., 2011; Xiong et al., 2012; Zikic et al., 2012).", "startOffset": 206, "endOffset": 313}, {"referenceID": 1, "context": "Notable contributions in this direction are the recent papers of Biau et al. (2008) and Biau (2012).", "startOffset": 65, "endOffset": 84}, {"referenceID": 1, "context": "Notable contributions in this direction are the recent papers of Biau et al. (2008) and Biau (2012).", "startOffset": 65, "endOffset": 100}, {"referenceID": 4, "context": "Random forests (Breiman, 2001) were originally conceived as a method of combining several CART (Breiman et al., 1984) style decision trees using bagging (Breiman, 1996).", "startOffset": 95, "endOffset": 117}, {"referenceID": 3, "context": "Random forests (Breiman, 2001) were originally conceived as a method of combining several CART (Breiman et al., 1984) style decision trees using bagging (Breiman, 1996). Their early development was influenced by the random subspace method of Ho (1998), the approach of random split selection ar X iv :1 31 0.", "startOffset": 16, "endOffset": 252}, {"referenceID": 9, "context": "from Dietterich (2000) and the work of Amit & Geman (1997) on feature selection.", "startOffset": 5, "endOffset": 23}, {"referenceID": 9, "context": "from Dietterich (2000) and the work of Amit & Geman (1997) on feature selection.", "startOffset": 5, "endOffset": 59}, {"referenceID": 9, "context": "from Dietterich (2000) and the work of Amit & Geman (1997) on feature selection. Several of the core ideas used in random forests were also present in the early work of Kwokt & Carter (1988) on ensembles of decision trees.", "startOffset": 5, "endOffset": 191}, {"referenceID": 6, "context": "In the years since their introduction, random forests have grown from a single algorithm to an entire framework of models (Criminisi et al., 2011), and have been applied to great effect in a wide variety of fields (Svetnik et al.", "startOffset": 122, "endOffset": 146}, {"referenceID": 19, "context": ", 2011), and have been applied to great effect in a wide variety of fields (Svetnik et al., 2003; Prasad et al., 2006; Cutler et al., 2007; Shotton et al., 2011; Criminisi & Shotton, 2013).", "startOffset": 75, "endOffset": 188}, {"referenceID": 7, "context": ", 2011), and have been applied to great effect in a wide variety of fields (Svetnik et al., 2003; Prasad et al., 2006; Cutler et al., 2007; Shotton et al., 2011; Criminisi & Shotton, 2013).", "startOffset": 75, "endOffset": 188}, {"referenceID": 22, "context": ", 2011), and have been applied to great effect in a wide variety of fields (Svetnik et al., 2003; Prasad et al., 2006; Cutler et al., 2007; Shotton et al., 2011; Criminisi & Shotton, 2013).", "startOffset": 75, "endOffset": 188}, {"referenceID": 1, "context": "The early theoretical work of Breiman (2004) for example, is essentially based on intuition and mathematical heuristics, and was not formalized rigorously until quite recently (Biau, 2012).", "startOffset": 176, "endOffset": 188}, {"referenceID": 2, "context": "The early theoretical work of Breiman (2004) for example, is essentially based on intuition and mathematical heuristics, and was not formalized rigorously until quite recently (Biau, 2012).", "startOffset": 30, "endOffset": 45}, {"referenceID": 1, "context": "An important milestone in the development of the theory of random forests is the work of Biau et al. (2008), which proves the consistency of several randomized ensemble classifiers.", "startOffset": 89, "endOffset": 108}, {"referenceID": 1, "context": "An important milestone in the development of the theory of random forests is the work of Biau et al. (2008), which proves the consistency of several randomized ensemble classifiers. Two models studied in Biau et al. (2008) are direct simplifications of the algorithm from Breiman (2001), and two are simple randomized neighborhood averaging rules, which can be viewed as simplifications of random forests from the perspective of Lin & Jeon (2006).", "startOffset": 89, "endOffset": 223}, {"referenceID": 1, "context": "An important milestone in the development of the theory of random forests is the work of Biau et al. (2008), which proves the consistency of several randomized ensemble classifiers. Two models studied in Biau et al. (2008) are direct simplifications of the algorithm from Breiman (2001), and two are simple randomized neighborhood averaging rules, which can be viewed as simplifications of random forests from the perspective of Lin & Jeon (2006).", "startOffset": 89, "endOffset": 287}, {"referenceID": 1, "context": "An important milestone in the development of the theory of random forests is the work of Biau et al. (2008), which proves the consistency of several randomized ensemble classifiers. Two models studied in Biau et al. (2008) are direct simplifications of the algorithm from Breiman (2001), and two are simple randomized neighborhood averaging rules, which can be viewed as simplifications of random forests from the perspective of Lin & Jeon (2006).", "startOffset": 89, "endOffset": 447}, {"referenceID": 1, "context": "More recently Biau (2012) has analyzed a variant of random forests originally introduced in Breiman (2004) which is quite similar to the original algorithm.", "startOffset": 14, "endOffset": 26}, {"referenceID": 1, "context": "More recently Biau (2012) has analyzed a variant of random forests originally introduced in Breiman (2004) which is quite similar to the original algorithm.", "startOffset": 14, "endOffset": 107}, {"referenceID": 1, "context": "More recently Biau (2012) has analyzed a variant of random forests originally introduced in Breiman (2004) which is quite similar to the original algorithm. The main differences between the model in Biau (2012) and that of Breiman (2001) are in how candidate split points are selected and that the former requires a second independent data set to fit the leaf predictors.", "startOffset": 14, "endOffset": 211}, {"referenceID": 1, "context": "More recently Biau (2012) has analyzed a variant of random forests originally introduced in Breiman (2004) which is quite similar to the original algorithm. The main differences between the model in Biau (2012) and that of Breiman (2001) are in how candidate split points are selected and that the former requires a second independent data set to fit the leaf predictors.", "startOffset": 14, "endOffset": 238}, {"referenceID": 3, "context": "While the problem of consistency of Breiman\u2019s algorithm remains open, some special cases have proved tractable. In particular, Meinshausen (2006) has shown that a model of random forests for quantile regression is consistent and Ishwaran & Kogalur (2010) have shown the consistency of their survival forests model.", "startOffset": 36, "endOffset": 146}, {"referenceID": 3, "context": "While the problem of consistency of Breiman\u2019s algorithm remains open, some special cases have proved tractable. In particular, Meinshausen (2006) has shown that a model of random forests for quantile regression is consistent and Ishwaran & Kogalur (2010) have shown the consistency of their survival forests model.", "startOffset": 36, "endOffset": 255}, {"referenceID": 3, "context": "While the problem of consistency of Breiman\u2019s algorithm remains open, some special cases have proved tractable. In particular, Meinshausen (2006) has shown that a model of random forests for quantile regression is consistent and Ishwaran & Kogalur (2010) have shown the consistency of their survival forests model. Denil et al. (2013) have shown the consistency of an online version of random forests.", "startOffset": 36, "endOffset": 335}, {"referenceID": 3, "context": "For a more comprehensive review we refer the reader to Breiman (2001) and Criminisi et al.", "startOffset": 55, "endOffset": 70}, {"referenceID": 3, "context": "For a more comprehensive review we refer the reader to Breiman (2001) and Criminisi et al. (2011).", "startOffset": 55, "endOffset": 98}, {"referenceID": 1, "context": "A simple strategy is to choose among the candidates uniformly at random, as in the models analyzed in Biau et al. (2008). A more common approach is to choose the candidate split which optimizes a purity function over the leafs that would be created.", "startOffset": 102, "endOffset": 121}, {"referenceID": 6, "context": "Criminisi et al. (2011) explore the use of several different leaf predictors for regression", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Unlike the random forests of Breiman (2001) we do not preform bootstrapping between the different trees.", "startOffset": 29, "endOffset": 44}, {"referenceID": 1, "context": "A similar result was shown by Biau et al. (2008) for binary classifiers and a corresponding mutli-class generalization appears in Denil et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 1, "context": "A similar result was shown by Biau et al. (2008) for binary classifiers and a corresponding mutli-class generalization appears in Denil et al. (2013). For regression, it is particularly straightforward.", "startOffset": 30, "endOffset": 150}, {"referenceID": 12, "context": "1 from Gy\u00f6rfi et al. (2002). According to this theorem {fn} is consistent if both diam(An(X)) \u2192 0 and N(An(X)) \u2192 \u221e in probability.", "startOffset": 7, "endOffset": 28}, {"referenceID": 1, "context": "The first model we compare to our own is the scale invariant random forest from Biau et al. (2008), which we refer to as Biau08.", "startOffset": 80, "endOffset": 99}, {"referenceID": 1, "context": "The second model we compare to is the algorithm analyzed in Biau (2012), which we refer to as Biau12.", "startOffset": 60, "endOffset": 72}, {"referenceID": 1, "context": "In this section we empirically compare our algorithm to Biau08 and Biau12 (described in Section 6) and Breiman (the original algorithm described in Breiman (2001)) on several datasets.", "startOffset": 56, "endOffset": 163}, {"referenceID": 22, "context": "Typically the first step in a joint location pipeline is to predict the body part labels of each pixel in the depth image and the second step is to use the labelled pixels to predict joint locations (Shotton et al., 2011).", "startOffset": 199, "endOffset": 221}], "year": 2013, "abstractText": "Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoretically tractable variant of random regression forests and prove that our algorithm is consistent. We also provide an empirical evaluation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in practice. Our experiments provide insight into the relative importance of different simplifications that theoreticians have made to obtain tractable models for analysis.", "creator": "LaTeX with hyperref package"}}}