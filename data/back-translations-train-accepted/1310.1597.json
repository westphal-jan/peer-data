{"id": "1310.1597", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2013", "title": "Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning", "abstract": "We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences. Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and German CoNLL-03 datasets.", "histories": [["v1", "Sun, 6 Oct 2013 16:34:30 GMT  (108kb,D)", "http://arxiv.org/abs/1310.1597v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mengqiu wang", "christopher d manning"], "accepted": true, "id": "1310.1597"}, "pdf": {"name": "1310.1597.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning", "authors": ["Mengqiu Wang", "Christopher D. Manning"], "emails": ["mengqiu@cs.stanford.edu", "manning@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The success of the monitored methods depends heavily on the availability of large amounts of commented training data. Manually curating commented corpora is a costly and time-consuming process. To date, most commented resources are in the English language, hampering the adoption of monitored learning methods in many multilingual environments. To minimize the need for annotated corpora, significant progress has been made in developing uncontrolled and semi-monitored approaches to NLP (Collins and Singer 2005; Liang 2005; Liang 2006; Goldberg 2010; Goldberg 2010; inter alia). To minimize the need for annotated corpora, we have made significant progress in developing uncontrolled and semi-monitored approaches to NLP (Collins and Singer 2005; Liang 2006; Goldberg 2010; inter alia). Newer paradigms for semi-monitored learning allow modelers to encode knowledge directly about the task and domain than constraints we derive from (Chang et) in 2007."}, {"heading": "2 Related Work", "text": "In fact, it is the case that most of them are able to survive on their own by going in search of their own identity. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...)"}, {"heading": "3 Approach", "text": "Our method performs Cross-Lingual Pseudo-Projection Expectation Regularization (CLiPPER). Figure 1 illustrates the workflow at the highest level. For each aligned pair of sentences in the bittext, we first calculate the posterior marginal position at each word position on the English side with a pre-trained English CRF tagger; then, for each aligned English word, we project its posterior marginal position as expectations of the aligned word position on the foreign side. We want to learn a CRF model in the foreign language that has similar expectations to the projected expectations for English. To this end, we adopt the Generalized Expectation (GE) Criteria framework introduced by Mann and McCallum (2010). For the rest of this section, we follow the notation used in (Print, 2011) to explain our approach."}, {"heading": "3.1 CLiPPER", "text": "The general idea of GE is that we can express our preferences variably and flexibly in all possible functions beyond the models. A desired model should meet the imposed limitations by matching the expectations of these limitations with some target expectations (achieved by external knowledge such as lexicon or, in our case, transferred knowledge from English). We define an objective function that is used as a label identity indicator for each word position and output label mapping lj. [...] The setting of [...] denotes all possible label mappings for each label, and m is the number of label values. [...] The condition Ai 6 = the condition Ai specifies that the constraint function applies only to Chinese word positions that have at least one aligned English word. [...] Each individual label can be treated as a Bernoulli variable."}, {"heading": "3.2 Hard vs. Soft Projection", "text": "The projection of expectations instead of uniform labeling from English to the foreign language can be thought of as a soft version of the method described in (Das and Petrov, 2011) and (Ganchev et al., 2009). Soft projection has its advantage: if the English model is not sure of its predictions, we do not have to commit ourselves to the current best prediction. The foreign model has more freedom to form its own belief, as each marginal distribution it produces deviates from a flat distribution by exactly the same amount. Generally, maintaining uncertainty until later is a strategy from which many NLP tasks have benefited (Finkel et al., 2006). Hard projection can also be treated as a special case within our framework. We can simply recalibrate the back limits of English by assigning probability measure 1 to the most likely outcome and ignoring anything else by effectively removing the argmax of the marginal position of the word."}, {"heading": "4 Experiments", "text": "We conduct experiments on Chinese and German NERs. We evaluate CLiPPER in two learning situations: weakly supervised and semi-supervised. In the weakly supervised environment, we simulate the condition of having no marked training data and evaluate the model we learned from the bittext alone. We then vary the amount of marked data available to the model and examine the learning curve of the model. In the semi-supervised environment, we assume that our model has access to the full marked data; our goal is to improve the performance of the supervised method by learning from additional requests."}, {"heading": "4.1 Dataset and Setup", "text": "We used the latest version of Stanford NER Toolkit8 as our base CRF model in all experiments. We used the latest version of Stanford NER Toolkit8 as the basis CRF documents in all experiments. Features for English, Chinese and German CRFs are extensively documented in (Che et al., 2013) and (Faruqui and Pado \u0301, 2010) and omitted here for short lifespan. It is worth noting that the current Stanford models include newer improvements to semi-supervise learning approaches that induce distributional similarity characteristics of large word clusters. These models represent the current state of the art in supervised methods and serve as a very comprehensive 8http: / / www-nlp.stanford.edu / nerstrong learning methods. For Chinese NER experiments, we follow the same setup as Che et al. (2013) to evaluate the latest OntoNotes (v0) Gescorus 2006 Gesp.9 (Hoebal) and 24ebal Chinese sentences are reserved for Penn."}, {"heading": "4.2 Weakly Supervised Results", "text": "The top four figures in Figure 2 show results from poorly supervised learning experiments. Remarkably, our proposed method (CLiPPER) on the Chinese test set achieves an F1 value of 64.4% with 80k bittext if no marked training data is used. In contrast, the supervised CRF baseline would require up to 12k labelled sets to achieve the same accuracy; the results on the German test set are less striking. Without marked data and 40k bittext, CLiPPER scores 60.0% on F1, the equivalent of using 1.5k labelled examples in the supervised environment. Combined with 1k labelled examples, the performance of CLiPPER reaches 69%, a gain of more than 5% absolute over the supervised CRF. We also note that the supervised CRF model learns much faster than the supervised CRF model in Chinese. This result is not too surprising as it is known that the Chinese ratios are more difficult than the German or English forecasts."}, {"heading": "4.3 Semi-supervised Results", "text": "In the semi-monitored experiments, we let the CRF model use the full set of labeled Chinese examples alongside the unlabeled Bitext. Table 1 shows results on the development dataset for China and Germany with 10-80k Bitext. We see that with just 10k additional Bitext, CLiPPER is able to significantly improve over the modern CRF baselines by up to 1.5% F1 both at the Chinese and German levels. With more unlabeled data, we notice a trade-off between precision and callback at the Chinese level. However, the final F1 value at the Chinese level at the 80k level is only slightly better than 10k. On the other hand, we observe a modest but steady improvement on the German side, as we add more unlabeled Bittext up to 40k sets. We select the best configurations on the development set (80k for China and 40k for Germany) to test these results on testset.The results on the testset.The test iels level are presented in the same semi-monitored Baselines as in the 2 baselines are presented in the semi-monitored table."}, {"heading": "4.4 Efficiency", "text": "Another advantage of our proposed approach is efficiency. By eliminating the previous multi-level \"project-then-train\" paradigm, but instead integrating the semi-monitored and monitored target into a common goal, we are able to achieve significant speed improvements. Table 3 shows the training time required to build models that deliver results in Table 2."}, {"heading": "5 Error Analysis and Discussion", "text": "Figure 3 gives two examples of CLiPPER in action. Both examples have a designated entity that immediately uses the word \"\u0433\u0442\" (monument) in the Chinese sentence. In Figure 3a, the word \"\" has the literal meaning of a hill on a high position, which also happens to be the name of a former vice president of China. Without having previously observed this word as a personal name in the inscribed training data, the CRF model does not have enough evidence to believe that it is a PERSON and not a LOCATION. However, the aligned words in English (\"Gao Gang\") are clearly part of a person's name, as they are preceded by a title (\"Vice President\"). The English model has high expectations that the aligned Chinese word \"Gao Gang\" is also a PERSON. Therefore, the projection of the English expressions into Chinese provides a strong clue to decipher word. Figure 3b gives another example: The Chinese word \"Gao Gang\" is also a PERSON."}, {"heading": "6 Conclusion", "text": "We have introduced a domain-independent and language-independent semi-monitored method to train discriminatory models by projecting expectations onto bittexts. Experiments on Chinese and German NER models show that our method, which is learned only through bittexts, can keep up with the performance of monitored models trained with thousands of labeled examples. Furthermore, the application of our method in an environment where all labeled examples are available also shows improvements over state-of-the-art monitored methods. Our experiments have also shown that soft expectation projection is more advantageous for hard projections, a technique that can be generalized to all sequence marking tasks and can be extended to more complex constraints. In future work, we plan to apply this method to more language pairs and examine the formal properties of the model.17In fact, a people search for a name on the Chinese equivalent of Facebook (www.renrenrenren.com) yields more than 13,000 hits."}], "references": [{"title": "Head-transducer models for speech translation and their automatic acquisition from bilingual data", "author": ["Hiyan Alshawi", "Srinivas Bangalore", "Shona Douglas."], "venue": "Machine Translation, 15.", "citeRegEx": "Alshawi et al\\.,? 2000", "shortCiteRegEx": "Alshawi et al\\.", "year": 2000}, {"title": "A highperformance semi-supervised learning method for text chunking", "author": ["Rie Kubota Ando", "Tong Zhang."], "venue": "Proceedings of ACL.", "citeRegEx": "Ando and Zhang.,? 2005", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "Alternating projections for learning with expectation constraints", "author": ["Kedar Bellare", "Gregory Druck", "Andrew McCallum."], "venue": "Proceedings of UAI.", "citeRegEx": "Bellare et al\\.,? 2009", "shortCiteRegEx": "Bellare et al\\.", "year": 2009}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["Avrim Blum", "Tom Mitchell."], "venue": "Proceedings of COLT.", "citeRegEx": "Blum and Mitchell.,? 1998", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "Two languages are better than one (for syntactic parsing)", "author": ["David Burkett", "Dan Klein."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Burkett and Klein.,? 2008", "shortCiteRegEx": "Burkett and Klein.", "year": 2008}, {"title": "Learning better monolingual models with unannotated bilingual text", "author": ["David Burkett", "Slav Petrov", "John Blitzer", "Dan Klein."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Burkett et al\\.,? 2010", "shortCiteRegEx": "Burkett et al\\.", "year": 2010}, {"title": "Coupled semi-supervised learning for information extraction", "author": ["Andrew Carlson", "Justin Betteridge", "Richard C. Wang", "Estevam R. Hruschka Jr.", "Tom M. Mitchell."], "venue": "Proceedings of WSDM.", "citeRegEx": "Carlson et al\\.,? 2010", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Guiding semi-supervision with constraintdriven learning", "author": ["Ming-Wei Chang", "Lev Ratinov", "Dan Roth."], "venue": "Proceedings of ACL.", "citeRegEx": "Chang et al\\.,? 2007", "shortCiteRegEx": "Chang et al\\.", "year": 2007}, {"title": "Named entity recognition with bilingual constraints", "author": ["Wanxiang Che", "Mengqiu Wang", "Christopher D. Manning."], "venue": "Proceedings of NAACL.", "citeRegEx": "Che et al\\.,? 2013", "shortCiteRegEx": "Che et al\\.", "year": 2013}, {"title": "Unsupervised models for named entity classification", "author": ["Michael Collins", "Yoram Singer."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Collins and Singer.,? 1999", "shortCiteRegEx": "Collins and Singer.", "year": 1999}, {"title": "Unsupervised partof-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of ACL.", "citeRegEx": "Das and Petrov.,? 2011", "shortCiteRegEx": "Das and Petrov.", "year": 2011}, {"title": "Highperformance semi-supervised learning using discriminatively constrained generative models", "author": ["Gregory Druck", "Andrew McCallum."], "venue": "Proceedings of ICML.", "citeRegEx": "Druck and McCallum.,? 2010", "shortCiteRegEx": "Druck and McCallum.", "year": 2010}, {"title": "Leveraging existing resources using generalized expectation criteria", "author": ["Gregory Druck", "Gideon Mann", "Andrew McCallum."], "venue": "Proceedings of NIPS Workshop on Learning Problem Design.", "citeRegEx": "Druck et al\\.,? 2007", "shortCiteRegEx": "Druck et al\\.", "year": 2007}, {"title": "Active learning by labeling features", "author": ["Gregory Druck", "Burr Settles", "Andrew McCallum."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Druck et al\\.,? 2009", "shortCiteRegEx": "Druck et al\\.", "year": 2009}, {"title": "Generalized Expectation Criteria for Lightly Supervised Learning", "author": ["Gregory Druck."], "venue": "Ph.D. thesis, University of Massachusetts Amherst.", "citeRegEx": "Druck.,? 2011", "shortCiteRegEx": "Druck.", "year": 2011}, {"title": "Training and evaluating a German named entity recognizer with semantic generalization", "author": ["Manaal Faruqui", "Sebastian Pad\u00f3."], "venue": "Proceedings of KONVENS.", "citeRegEx": "Faruqui and Pad\u00f3.,? 2010", "shortCiteRegEx": "Faruqui and Pad\u00f3.", "year": 2010}, {"title": "Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines", "author": ["Jenny Rose Finkel", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Finkel et al\\.,? 2006", "shortCiteRegEx": "Finkel et al\\.", "year": 2006}, {"title": "Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora", "author": ["Victoria Fossum", "Steven Abney."], "venue": "Proceedings of IJCNLP.", "citeRegEx": "Fossum and Abney.,? 2005", "shortCiteRegEx": "Fossum and Abney.", "year": 2005}, {"title": "Dependency grammar induction via bitext projection constraints", "author": ["Kuzman Ganchev", "Jennifer Gillenwater", "Ben Taskar."], "venue": "Proceedings of ACL.", "citeRegEx": "Ganchev et al\\.,? 2009", "shortCiteRegEx": "Ganchev et al\\.", "year": 2009}, {"title": "Posterior regularization for structured latent variable models", "author": ["Kuzman Ganchev", "Jo ao Gra\u00e7a", "Jennifer Gillenwater", "Ben Taskar."], "venue": "JMLR, 10:2001\u20132049.", "citeRegEx": "Ganchev et al\\.,? 2010", "shortCiteRegEx": "Ganchev et al\\.", "year": 2010}, {"title": "New Directions in Semisupervised Learning", "author": ["Andrew B. Goldberg."], "venue": "Ph.D. thesis, University of Wisconsin-Madison.", "citeRegEx": "Goldberg.,? 2010", "shortCiteRegEx": "Goldberg.", "year": 2010}, {"title": "OntoNotes: the 90% solution", "author": ["Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "The Unsupervised Learning of Natural Language Structure", "author": ["Dan Klein."], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Klein.,? 2005", "shortCiteRegEx": "Klein.", "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of ICML.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "First- and second-order expectation semirings with applications to minimumrisk training on translation forests", "author": ["Zhifei Li", "Jason Eisner."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Li and Eisner.,? 2009", "shortCiteRegEx": "Li and Eisner.", "year": 2009}, {"title": "Wiki-ly supervised part-of-speech tagging", "author": ["Shen Li", "Jo ao Gra\u00e7a", "Ben Taskar."], "venue": "Proceedings of EMNLP-CoNLL.", "citeRegEx": "Li et al\\.,? 2012", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Semi-supervised learning for natural language", "author": ["Percy Liang."], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology.", "citeRegEx": "Liang.,? 2005", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Generalized expectation criteria for semi-supervised learning with weakly labeled data", "author": ["Gideon Mann", "Andrew McCallum."], "venue": "JMLR, 11:955\u2013984.", "citeRegEx": "Mann and McCallum.,? 2010", "shortCiteRegEx": "Mann and McCallum.", "year": 2010}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of NAACL-HLT.", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Multilingual part-ofspeech tagging: Two unsupervised approaches", "author": ["Tahira Naseem", "Benjamin Snyder", "Jacob Eisenstein", "Regina Barzilay."], "venue": "JAIR, 36:1076\u20139757.", "citeRegEx": "Naseem et al\\.,? 2009", "shortCiteRegEx": "Naseem et al\\.", "year": 2009}, {"title": "Uptraining for accurate deterministic question parsing", "author": ["Slav Petrov", "Pi-Chuan Chang", "Michael Ringgaard", "Hiyan Alshawi."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Petrov et al\\.,? 2010", "shortCiteRegEx": "Petrov et al\\.", "year": 2010}, {"title": "Unified expectation maximization", "author": ["Rajhans Samdani", "Ming-Wei Chang", "Dan Roth."], "venue": "Proceedings of NAACL.", "citeRegEx": "Samdani et al\\.,? 2012", "shortCiteRegEx": "Samdani et al\\.", "year": 2012}, {"title": "Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of CoNLL.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "A co-regularization approach to semisupervised learning with multiple views", "author": ["Vikas Sindhwani", "Partha Niyogi", "Mikhail Belkin."], "venue": "Proceedings of ICML Workshop on Learning with Multiple Views, International Conference on Machine Learn-", "citeRegEx": "Sindhwani et al\\.,? 2005", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2005}, {"title": "Novel Estimation Methods for Unsupervised Discovery of Latent Structure in Natural Language Text", "author": ["Noah A. Smith."], "venue": "Ph.D. thesis, Johns Hopkins University.", "citeRegEx": "Smith.,? 2006", "shortCiteRegEx": "Smith.", "year": 2006}, {"title": "Unsupervised multilingual grammar induction", "author": ["Benjamin Snyder", "Tahira Naseem", "Regina Barzilay."], "venue": "Proceedings of ACL.", "citeRegEx": "Snyder et al\\.,? 2009", "shortCiteRegEx": "Snyder et al\\.", "year": 2009}, {"title": "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data", "author": ["Jun Suzuki", "Hideki Isozaki."], "venue": "Proceedings of ACL.", "citeRegEx": "Suzuki and Isozaki.,? 2008", "shortCiteRegEx": "Suzuki and Isozaki.", "year": 2008}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "author": ["Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Slav Petrov", "Ryan McDonald", "Joakim Nivre."], "venue": "Proceedings of ACL.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2013", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Effective bilingual constraints for semisupervised learning of named entity recognizers", "author": ["Mengqiu Wang", "Wanxiang Che", "Christopher D. Manning."], "venue": "Proceedings of AAAI.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "A backoff model for bootstrapping resources for non-english languages", "author": ["Chenhai Xi", "Rebecca Hwa."], "venue": "Proceedings of HLT-EMNLP.", "citeRegEx": "Xi and Hwa.,? 2005", "shortCiteRegEx": "Xi and Hwa.", "year": 2005}, {"title": "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora", "author": ["David Yarowsky", "Grace Ngai."], "venue": "Proceedings of NAACL.", "citeRegEx": "Yarowsky and Ngai.,? 2001", "shortCiteRegEx": "Yarowsky and Ngai.", "year": 2001}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky."], "venue": "Proceedings of ACL.", "citeRegEx": "Yarowsky.,? 1995", "shortCiteRegEx": "Yarowsky.", "year": 1995}], "referenceMentions": [{"referenceID": 27, "context": "We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010).", "startOffset": 114, "endOffset": 139}, {"referenceID": 7, "context": "More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (Chang et al., 2007; Mann and McCallum, 2010; Ganchev et al., 2010).", "startOffset": 158, "endOffset": 225}, {"referenceID": 27, "context": "More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (Chang et al., 2007; Mann and McCallum, 2010; Ganchev et al., 2010).", "startOffset": 158, "endOffset": 225}, {"referenceID": 19, "context": "More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (Chang et al., 2007; Mann and McCallum, 2010; Ganchev et al., 2010).", "startOffset": 158, "endOffset": 225}, {"referenceID": 10, "context": "More recent work applied the projection-based approach to more language-pairs, and further improved performance through the use of type-level constraints from tag dictionary and feature-rich generative or discriminative models (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 227, "endOffset": 273}, {"referenceID": 37, "context": "More recent work applied the projection-based approach to more language-pairs, and further improved performance through the use of type-level constraints from tag dictionary and feature-rich generative or discriminative models (Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 227, "endOffset": 273}, {"referenceID": 7, "context": "More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (Chang et al., 2007; Mann and McCallum, 2010; Ganchev et al., 2010). However, in a multilingual setting, coming up with effective constraints require extensive knowledge of the foreign1 language. Bilingual parallel text (bitext) lends itself as a medium to transfer knowledge from a resource-rich language to a foreign languages. Yarowsky and Ngai (2001) project labels produced by an English tagger to the foreign side of bitext, then use the projected labels to learn a HMM model.", "startOffset": 159, "endOffset": 513}, {"referenceID": 27, "context": "Secondly, we encode the expectations as constraints and train a model by minimizing divergence between model expectations and projected expectations in a Generalized Expectation (GE) Criteria (Mann and McCallum, 2010) framework.", "startOffset": 192, "endOffset": 217}, {"referenceID": 42, "context": "Examples of methods that explore multi-view constraints include self-training (Yarowsky, 1995; McClosky et al., 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al.", "startOffset": 78, "endOffset": 117}, {"referenceID": 28, "context": "Examples of methods that explore multi-view constraints include self-training (Yarowsky, 1995; McClosky et al., 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al.", "startOffset": 78, "endOffset": 117}, {"referenceID": 3, "context": ", 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al., 2005), multiview learning (Ando and Zhang, 2005; Carlson et al.", "startOffset": 22, "endOffset": 71}, {"referenceID": 33, "context": ", 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al., 2005), multiview learning (Ando and Zhang, 2005; Carlson et al.", "startOffset": 22, "endOffset": 71}, {"referenceID": 1, "context": ", 2005), multiview learning (Ando and Zhang, 2005; Carlson et al., 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010).", "startOffset": 28, "endOffset": 72}, {"referenceID": 6, "context": ", 2005), multiview learning (Ando and Zhang, 2005; Carlson et al., 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010).", "startOffset": 28, "endOffset": 72}, {"referenceID": 36, "context": ", 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010).", "startOffset": 61, "endOffset": 113}, {"referenceID": 11, "context": ", 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010).", "startOffset": 61, "endOffset": 113}, {"referenceID": 7, "context": "The kind of constraints used in applications such as NER are the ones like \u201cthe words CA, Australia, NY are LOCATION\u201d (Chang et al., 2007).", "startOffset": 118, "endOffset": 138}, {"referenceID": 8, "context": "An early example of using knowledge as constraints in weakly-supervised learning is the work by Collins and Singer (1999). They showed that the addition of a small set of \u201cseed\u201d rules greatly improve a co-training style unsupervised tagger.", "startOffset": 96, "endOffset": 122}, {"referenceID": 7, "context": "Chang et al. (2007) proposed a constraint-driven learning (CODL) framework where constraints are used to guide the selection of best self-labeled examples to be included as additional training data in an iterative EM-style procedure.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Other sources of knowledge include lexicons and gazetteers (Druck et al., 2007; Chang et al., 2007).", "startOffset": 59, "endOffset": 99}, {"referenceID": 7, "context": "Other sources of knowledge include lexicons and gazetteers (Druck et al., 2007; Chang et al., 2007).", "startOffset": 59, "endOffset": 99}, {"referenceID": 11, "context": "Druck et al. (2009) also demonstrated that in an active learning setting where annotation budget is limited, it is more efficient to label features than examples.", "startOffset": 0, "endOffset": 20}, {"referenceID": 27, "context": "3 To soften these constraints, Mann and McCallum (2010) proposed the Generalized Expectation (GE) Criteria framework, which encodes constraints as a regularization term over some score function that measures the divergence between the model\u2019s expectation and the target expectation.", "startOffset": 31, "endOffset": 56}, {"referenceID": 27, "context": "3 To soften these constraints, Mann and McCallum (2010) proposed the Generalized Expectation (GE) Criteria framework, which encodes constraints as a regularization term over some score function that measures the divergence between the model\u2019s expectation and the target expectation. The connection between GE and CODL is analogous to the relationship between hard (Viterbi) EM and soft EM, as illustrated by Samdani et al. (2012).", "startOffset": 31, "endOffset": 430}, {"referenceID": 14, "context": "However, later results (Druck, 2011) have shown that using the Expectation Semiring techniques of Li and Eisner (2009), one can compute the exact gradients of GE in a Conditional Random Fields (CRF) (Lafferty et al.", "startOffset": 23, "endOffset": 36}, {"referenceID": 23, "context": "However, later results (Druck, 2011) have shown that using the Expectation Semiring techniques of Li and Eisner (2009), one can compute the exact gradients of GE in a Conditional Random Fields (CRF) (Lafferty et al., 2001) at costs", "startOffset": 199, "endOffset": 222}, {"referenceID": 16, "context": "Another closely related work is the Posterior Regularization (PR) framework by Ganchev et al. (2010). In fact, as Bellare et al.", "startOffset": 79, "endOffset": 101}, {"referenceID": 2, "context": "In fact, as Bellare et al. (2009) have shown, in a discriminative model these two methods optimize exactly the same objective.", "startOffset": 12, "endOffset": 34}, {"referenceID": 2, "context": "In fact, as Bellare et al. (2009) have shown, in a discriminative model these two methods optimize exactly the same objective.4 The two differ in optimization details: PR uses a EM algorithm to approximate the gradients which avoids the expensive computation of a covariance matrix between features and constraints, whereas GE directly calculates the gradient. However, later results (Druck, 2011) have shown that using the Expectation Semiring techniques of Li and Eisner (2009), one can compute the exact gradients of GE in a Conditional Random Fields (CRF) (Lafferty et al.", "startOffset": 12, "endOffset": 480}, {"referenceID": 2, "context": "And empirically, GE tends to perform more accurately than PR (Bellare et al., 2009; Druck, 2011).", "startOffset": 61, "endOffset": 96}, {"referenceID": 14, "context": "And empirically, GE tends to perform more accurately than PR (Bellare et al., 2009; Druck, 2011).", "startOffset": 61, "endOffset": 96}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al.", "startOffset": 98, "endOffset": 141}, {"referenceID": 35, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al.", "startOffset": 98, "endOffset": 141}, {"referenceID": 4, "context": ", 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 29, "context": ", 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009).", "startOffset": 66, "endOffset": 87}, {"referenceID": 5, "context": "A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013).", "startOffset": 217, "endOffset": 276}, {"referenceID": 8, "context": "A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013).", "startOffset": 217, "endOffset": 276}, {"referenceID": 39, "context": "A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013).", "startOffset": 217, "endOffset": 276}, {"referenceID": 30, "context": "They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers.", "startOffset": 36, "endOffset": 57}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013). They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005).", "startOffset": 99, "endOffset": 1110}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013). They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals.", "startOffset": 99, "endOffset": 1133}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013). They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals. To mitigate this problem, Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels.", "startOffset": 99, "endOffset": 1290}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013). They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals. To mitigate this problem, Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Fossum and Abney (2005) filter out projection noise by combining projections from from multiple source languages.", "startOffset": 99, "endOffset": 1445}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013). They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals. To mitigate this problem, Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Fossum and Abney (2005) filter out projection noise by combining projections from from multiple source languages. However, this approach is not always viable since it relies on having parallel bitext from multiple source languages. Li et al. (2012) proposed the use of crowd-sourced Wiktionary as additional resources for inducing tag lexicons.", "startOffset": 99, "endOffset": 1670}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013). They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals. To mitigate this problem, Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Fossum and Abney (2005) filter out projection noise by combining projections from from multiple source languages. However, this approach is not always viable since it relies on having parallel bitext from multiple source languages. Li et al. (2012) proposed the use of crowd-sourced Wiktionary as additional resources for inducing tag lexicons. More recently, T\u00e4ckstr\u00f6m et al. (2013) combined token-level and type-level constraints to constrain legitimate label sequences and and recalibrate the probability distribution in a CRF.", "startOffset": 99, "endOffset": 1805}, {"referenceID": 0, "context": "As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013). They also demonstrated a uptraining (Petrov et al., 2010) setting where taginduced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals. To mitigate this problem, Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Fossum and Abney (2005) filter out projection noise by combining projections from from multiple source languages. However, this approach is not always viable since it relies on having parallel bitext from multiple source languages. Li et al. (2012) proposed the use of crowd-sourced Wiktionary as additional resources for inducing tag lexicons. More recently, T\u00e4ckstr\u00f6m et al. (2013) combined token-level and type-level constraints to constrain legitimate label sequences and and recalibrate the probability distribution in a CRF. The tag dictionary used for POS tagging are analogous to the gazetteers and name lexicons used for NER by Chang et al. (2007).", "startOffset": 99, "endOffset": 2078}, {"referenceID": 41, "context": "Instead of using the projected linguistic structures as ground truth (Yarowsky and Ngai, 2001), or as features in a generative model (Das and Petrov, 2011), they used them as constraints in a PR framework.", "startOffset": 69, "endOffset": 94}, {"referenceID": 10, "context": "Instead of using the projected linguistic structures as ground truth (Yarowsky and Ngai, 2001), or as features in a generative model (Das and Petrov, 2011), they used them as constraints in a PR framework.", "startOffset": 133, "endOffset": 155}, {"referenceID": 15, "context": "Our work is also closely related to Ganchev et al. (2009). They used a two-step projection method similar to Das and Petrov (2011) for dependency parsing.", "startOffset": 36, "endOffset": 58}, {"referenceID": 9, "context": "They used a two-step projection method similar to Das and Petrov (2011) for dependency parsing.", "startOffset": 50, "endOffset": 72}, {"referenceID": 2, "context": "Experiments in Bellare et al. (2009) and Druck (2011) suggest that in a discriminative model (like ours), GE is more accurate than PR.", "startOffset": 15, "endOffset": 37}, {"referenceID": 2, "context": "Experiments in Bellare et al. (2009) and Druck (2011) suggest that in a discriminative model (like ours), GE is more accurate than PR.", "startOffset": 15, "endOffset": 54}, {"referenceID": 14, "context": "In the remainder of this section, we follow the notation used in (Druck, 2011) to explain our approach.", "startOffset": 65, "endOffset": 78}, {"referenceID": 26, "context": "To this end, we adopt the Generalized Expectation (GE) Criteria framework introduced by Mann and McCallum (2010). In the remainder of this section, we follow the notation used in (Druck, 2011) to explain our approach.", "startOffset": 88, "endOffset": 113}, {"referenceID": 10, "context": "Projecting expectations instead of one-best label assignments from English to foreign language can be thought of as a soft version of the method described in (Das and Petrov, 2011) and (Ganchev et al.", "startOffset": 158, "endOffset": 180}, {"referenceID": 18, "context": "Projecting expectations instead of one-best label assignments from English to foreign language can be thought of as a soft version of the method described in (Das and Petrov, 2011) and (Ganchev et al., 2009).", "startOffset": 185, "endOffset": 207}, {"referenceID": 16, "context": "In general, preserving uncertainties till later is a strategy that has benefited many NLP tasks (Finkel et al., 2006).", "startOffset": 96, "endOffset": 117}, {"referenceID": 41, "context": "In the hard projection setting, GE training resembles a \u201cproject-then-train\u201d style semi-supervised CRF training scheme (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 119, "endOffset": 168}, {"referenceID": 37, "context": "In the hard projection setting, GE training resembles a \u201cproject-then-train\u201d style semi-supervised CRF training scheme (Yarowsky and Ngai, 2001; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 119, "endOffset": 168}, {"referenceID": 8, "context": "Features for English, Chinese and German CRFs are documented extensively in (Che et al., 2013) and (Faruqui and Pad\u00f3, 2010) and omitted here for brevity.", "startOffset": 76, "endOffset": 94}, {"referenceID": 15, "context": ", 2013) and (Faruqui and Pad\u00f3, 2010) and omitted here for brevity.", "startOffset": 12, "endOffset": 36}, {"referenceID": 21, "context": "0) corpus (Hovy et al., 2006).", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": "For Chinese NER experiments, we follow the same setup as Che et al. (2013) to evaluate on the latest OntoNotes (v4.", "startOffset": 57, "endOffset": 75}, {"referenceID": 32, "context": "For German NER experiments, we evaluate using the standard CoNLL-03 NER corpus (Sang and Meulder, 2003).", "startOffset": 79, "endOffset": 103}, {"referenceID": 39, "context": "They are: semi-supervised learning method using factored bilingual models with Gibbs sampling (Wang et al., 2013); bilingual NER using Integer Linear Programming (ILP) with bilingual constraints, by (Che et al.", "startOffset": 94, "endOffset": 113}, {"referenceID": 8, "context": ", 2013); bilingual NER using Integer Linear Programming (ILP) with bilingual constraints, by (Che et al., 2013); and constraint-driven bilingual-reranking ap-", "startOffset": 93, "endOffset": 111}, {"referenceID": 5, "context": "tgz proach (Burkett et al., 2010).", "startOffset": 11, "endOffset": 33}, {"referenceID": 8, "context": "The code from (Che et al., 2013) and (Wang et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 39, "context": ", 2013) and (Wang et al., 2013) are publicly available,15.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": "Code from (Burkett et al., 2010) is obtained through personal communications.", "startOffset": 10, "endOffset": 32}, {"referenceID": 38, "context": "2 is nonconvex, we adopted the early stopping training scheme from (Turian et al., 2010) as the following: after each iteration in L-BFGS training, the model is evaluated against the development set; the training procedure is terminated if no improvements have been made in 20 iterations.", "startOffset": 67, "endOffset": 88}, {"referenceID": 5, "context": "com/stanfordnlp/CoreNLP Due to technical difficulties, we are unable to replicate Burkett et al. (2010) experiments on German NER, therefore only Chinese results are reported.", "startOffset": 82, "endOffset": 104}, {"referenceID": 39, "context": "WCD13 is (Wang et al., 2013), CWD13 is (Che et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 8, "context": ", 2013), CWD13 is (Che et al., 2013), and BPBK10 is (Burkett et al.", "startOffset": 18, "endOffset": 36}, {"referenceID": 5, "context": ", 2013), and BPBK10 is (Burkett et al., 2010).", "startOffset": 23, "endOffset": 45}], "year": 2013, "abstractText": "We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences. Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and German CoNLL-03 datasets.", "creator": "TeX"}}}