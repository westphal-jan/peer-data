{"id": "1310.1757", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2013", "title": "A Deep and Tractable Density Estimator", "abstract": "The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.", "histories": [["v1", "Mon, 7 Oct 2013 12:42:41 GMT  (357kb,D)", "http://arxiv.org/abs/1310.1757v1", "9 pages, 4 tables, 1 algorithm, 5 figures"], ["v2", "Sat, 11 Jan 2014 17:13:56 GMT  (360kb,D)", "http://arxiv.org/abs/1310.1757v2", "9 pages, 4 tables, 1 algorithm, 5 figures. To appear ICML 2014, JMLR W&amp;CP volume 32"]], "COMMENTS": "9 pages, 4 tables, 1 algorithm, 5 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["benigno uria", "iain murray", "hugo larochelle"], "accepted": true, "id": "1310.1757"}, "pdf": {"name": "1310.1757.pdf", "metadata": {"source": "META", "title": "A Deep and Tractable Density Estimator", "authors": ["Benigno Uria", "Iain Murray", "Hugo Larochelle"], "emails": ["B.URIA@ED.AC.UK", "I.MURRAY@ED.AC.UK", "HUGO.LAROCHELLE@USHERBROOKE.CA"], "sections": [{"heading": "1. Introduction", "text": "However, there is considerable interest in flexible model distributions that can match and generalize from training data in a variety of applications.The solutions to these inference tasks often cannot be accurately calculated and require iterative approaches such as Monte Carlo or variation methods (e.g. Bischof, 2006).Models for which inferences are traceable would be preferable. NADE (Larochelle & Murray, 2011), and its real value variants RNADE (Uria et al.), have shown that they are able to move."}, {"heading": "2. Background: NADE and RNADE", "text": "The only element that limits the modelability of an auto-regressive model is the family of selected distribution models."}, {"heading": "3. Training a factorial number of NADEs", "text": "D (D). D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)."}, {"heading": "3.1. Improved parameter sharing using input masks", "text": "While the parameter binding proposed so far is simple, it results in poor performance in practice. A problem is that the values of the hidden units calculated with (3) are the same if a dimension is in xo > d (a value that must be predicted) and if the value of that dimension is zero and conditioned. If we train only a NADE with a fixed o, each output unit knows which inputs are fed into it, but in the multiple ordering case this information is lost if the input is null. To make this distinction possible, we expand the parameter division scheme by adding a binary mask vector mo < d to the inputs, which indicates which dimensions are present in the input. That is, the i-th element of mo < d is 1 if i-o < d and 0 otherwise. One interpretation of this scheme is that the distortion vector of the first hidden layer now depends on the arrangement and d."}, {"heading": "4. On the fly generation of NADE ensembles", "text": "Our order-specific training procedure can be considered as a set of parameters that can be used by a factorial number of NADEs, one per order of input variables. Generally, these different NADEs will not agree on the probability of a given data point. While, at first glance, this disagreement may seem unattractive, we can actually use this source of variability to our advantage and obtain better estimates than possible with a set of consistent models. A NADE with a given input order corresponds to a different hypotheses space than other NADEs with a different order. In other words, each NADE with a different order is its own model, with slightly different inductive presetting, despite the parameter division. A reliable approach to improving a given estimate is to construct instead an ensemble of multiple, strong but different estimators, e.g. with a bagging (Ormoneit & Tresp, 1996) or stacking (Smyth & Wolpert, 1999)."}, {"heading": "5. Related work", "text": "As mentioned above, the auto-regressive density / distribution calculation has previously been studied by others. In the case of binary data, Frey (1998) considered the use of logistic regression models, while Bengio & Bengio (2000) proposed a single-layer neural network architecture that differed from the NADE model (Larochelle & Murray, 2011), but in all of these cases a single (usually random) input order was selected and maintained during the training. Gregor & LeCun (2011) proposed to develop a variant of the NADE architecture under stochastically generated random orders, just as we observed they performed much worse than when selecting a single variable order that motivates our proposed parameter distribution scheme based on input masks. Gregor & LeCun generated a single order for each training update and conditioned it on contexts of all possible sizes to calculate the probability of a protocol and its protocol."}, {"heading": "6. Experimental results", "text": "We conducted experiments on several binary and real-evaluated datasets to assess the performance of the NADEs trained using our custom agnostic method. We report on the average test log probability of each model, that is, the average log density of the data points in a given test set. In the case of NADEs trained in an order-agnostic manner, we have to select a sequence of variables so that we can calculate the density of the test datapoints. We report on the average of the average test log woods randomly selected. Note that this is different from an ensemble in which the probabilities are averaged before calculating the logarithm. To reduce the confusion, we have not reported the standard deviation across the order orders. In all cases, this standard deviation has an order of magnitude smaller than the standard deviation of the standard unit we have trained."}, {"heading": "6.1. Binary datasets", "text": "This year, it has come to the point where one feels able to go to a place where one is able to go to another world, where one is able to create a new world."}, {"heading": "6.2. Real-valued datasets", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "7. Conclusions", "text": "We have introduced a new training method that simultaneously matches one NADE for each possible order of Figure 5. Above: 50 examples of 8 x 8 patches in the BSDS300 dataset, ordered by decreasing probability under a 6 x hidden layer of NADE. Below: 50 samples from a 6 x hidden layer of NADE. The pixel intensity of each patch at the bottom right was calculated as minus the sum of all other pixel intensities (see text).Dimensions. In addition, this new training method is able to train deep versions of NADE with a linear increase in calculation and construct ensembles of NADEs on the fly without incurring additional training costs.It is not surprising that the statistical performance of models trained with our ordering agnostic method depends on datasets. NADEs trained with our method outperform the blending models in all datasets we have studied. However, to meet the multiple sets of data sets, most of which are required to be equal."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank John Bridle and Steve Renals for useful discussions."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and trends in Machine Learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks", "author": ["Y. Bengio", "S. Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio and Bengio,? \\Q1999\\E", "shortCiteRegEx": "Bengio and Bengio", "year": 1999}, {"title": "Mixture Density Networks", "author": ["C.M. Bishop"], "venue": "Technical report,", "citeRegEx": "Bishop,? \\Q1994\\E", "shortCiteRegEx": "Bishop", "year": 1994}, {"title": "Pattern recognition and machine", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Graphical models for machine learning and digital communication", "author": ["B.J. Frey"], "venue": null, "citeRegEx": "Frey,? \\Q1998\\E", "shortCiteRegEx": "Frey", "year": 1998}, {"title": "Learning Representations by Maximizing Compression", "author": ["K. Gregor", "Y. LeCun"], "venue": "Technical report,", "citeRegEx": "Gregor and LeCun,? \\Q2011\\E", "shortCiteRegEx": "Gregor and LeCun", "year": 2011}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS 2011),", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["I. Murray", "R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Murray and Salakhutdinov,? \\Q2008\\E", "shortCiteRegEx": "Murray and Salakhutdinov", "year": 2008}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Improved Gaussian mixture density estimates using Bayesian penalty terms and network averaging", "author": ["D. Ormoneit", "V. Tresp"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ormoneit and Tresp,? \\Q1995\\E", "shortCiteRegEx": "Ormoneit and Tresp", "year": 1995}, {"title": "On the Quantitative Analysis of Deep Belief Networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML 2008),", "citeRegEx": "Salakhutdinov and Murray,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "Linearly combining density estimators via stacking", "author": ["P. Smyth", "D. Wolpert"], "venue": "Machine Learning,", "citeRegEx": "Smyth and Wolpert,? \\Q1999\\E", "shortCiteRegEx": "Smyth and Wolpert", "year": 1999}, {"title": "Training Recurrent Neural Networks", "author": ["I. Sutskever"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Sutskever,? \\Q2013\\E", "shortCiteRegEx": "Sutskever", "year": 2013}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning (ICML", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Natural images, Gaussian mixtures and dead leaves", "author": ["D. Zoran", "Y. Weiss"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zoran and Weiss,? \\Q2012\\E", "shortCiteRegEx": "Zoran and Weiss", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "NADE (Larochelle & Murray, 2011), and its real-valued variant RNADE (Uria et al., 2013), have been shown to be state of the art joint density models for a variety of realworld datasets, as measured by their predictive likelihood.", "startOffset": 68, "endOffset": 87}, {"referenceID": 4, "context": "In the case of binary data, autoregressive models based on logistic regressors and neural networks have been proposed (Frey, 1998; Bengio & Bengio, 2000).", "startOffset": 118, "endOffset": 153}, {"referenceID": 13, "context": "NADE has recently been extended to allow density estimation of real-valued vectors (Uria et al., 2013) by using mixture density networks or MDNs (Bishop, 1994) for each of the conditionals in Equation (1).", "startOffset": 83, "endOffset": 102}, {"referenceID": 2, "context": ", 2013) by using mixture density networks or MDNs (Bishop, 1994) for each of the conditionals in Equation (1).", "startOffset": 50, "endOffset": 64}, {"referenceID": 3, "context": "For the binary data case, Frey (1998) considered the use of logistic regression conditional models, while Bengio & Bengio (2000) proposed a single layer neural network architecture, with a parameter sharing scheme different from the one in the NADE model (Larochelle & Murray, 2011).", "startOffset": 26, "endOffset": 38}, {"referenceID": 0, "context": "For the binary data case, Frey (1998) considered the use of logistic regression conditional models, while Bengio & Bengio (2000) proposed a single layer neural network architecture, with a parameter sharing scheme different from the one in the NADE model (Larochelle & Murray, 2011).", "startOffset": 106, "endOffset": 129}, {"referenceID": 14, "context": "Our algorithm also bears similarity with denoising autoencoders (Vincent et al., 2008) trained using so-called \u201cmasking noise\u201d.", "startOffset": 64, "endOffset": 86}, {"referenceID": 12, "context": "We used Nesterov\u2019s accelerated gradient (Sutskever, 2013) with momentum value 0.", "startOffset": 40, "endOffset": 57}, {"referenceID": 13, "context": "We dropped the other two datasets tested by Uria et al. (2013), because some of their dimensions only take a finite number of values even if those are realvalued.", "startOffset": 44, "endOffset": 63}, {"referenceID": 13, "context": "We fixed the number of hidden units to 50, following Uria et al. (2013). The learning rate was chosen among {0.", "startOffset": 53, "endOffset": 72}, {"referenceID": 13, "context": "We compare the performance of RNADEs with different number of hidden layers trained with our procedure against a one-hidden layer RNADE trained for a fixed ordering (Uria et al., 2013), and with mixtures of Gaussians, which remain the state of the art in this problem (Zoran & Weiss, 2012).", "startOffset": 165, "endOffset": 184}, {"referenceID": 13, "context": "We adopted the setup described by Uria et al. (2013). The average intensity of each patch was subtracted from each pixel\u2019s value.", "startOffset": 34, "endOffset": 53}, {"referenceID": 13, "context": "Baselines are taken from (Uria et al., 2013).", "startOffset": 25, "endOffset": 44}], "year": 2017, "abstractText": "The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.", "creator": "LaTeX with hyperref package"}}}