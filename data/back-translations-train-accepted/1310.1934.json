{"id": "1310.1934", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2013", "title": "Discriminative Features via Generalized Eigenvectors", "abstract": "Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results.", "histories": [["v1", "Mon, 7 Oct 2013 20:05:52 GMT  (113kb,D)", "http://arxiv.org/abs/1310.1934v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nikos karampatziakis", "paul mineiro"], "accepted": true, "id": "1310.1934"}, "pdf": {"name": "1310.1934.pdf", "metadata": {"source": "META", "title": "Discriminative Features via Generalized Eigenvectors", "authors": ["Nikos Karampatziakis", "Paul Mineiro"], "emails": ["nikosk@microsoft.com", "pmineiro@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "In practice, supervised learning approaches are used profitably in many areas, from film recommendation to speech and image recognition (Koren et al., 2009; Hinton et al., 2012a; Krizhevsky et al., 2012). The success of all these systems depends crucially on the compatibility between the model and the representation used to solve the problem. For some problems, the types of representations and models that lead to good performance are known. In text classification, for example, unigrams and bigrams are features that work well for a variety of related tasks (Halevy et al., 2009)."}, {"heading": "2. Method", "text": "One of the simplest possible statistics that includes both features and labels is the matrix E [> xy >], which is the collection of class-related mean characteristic vectors in the multi-class classification, which has been thoroughly studied, e.g. Fisher LDA (Fisher, ar Xiv: 131 0.19 34v1 [cs.LG] 7 October 201 31936) and Sliced Inverse Regression (Li, 1991). However, in many practical applications we expect the data distribution to contain much more information than the statistics contained in the first moment. The next natural study object is the tensor V [x y].In multi-class classification, the tensor E [x x y] is simply a collection of conditional second moment matrices Ci = E [xx > y = i]. There are many standard methods for extracting features from these matrices."}, {"heading": "2.1. Useful Properties", "text": "The characteristic detectors resulting from the maximization of equation (1) have two useful properties, which we list below = > > > For simplicity, we give the results assuming full rank of exact conditional moment matrices and then discuss the effects of regulation and finite patterns. (Invariance) Among the above assumptions, the embedding of properties v > x includes invariant, invertible linear transformations of x.Proof. Let A, Rd \u00b7 d be invertible and x \u2032 = Ax the transformed eigenresponses. Let Cm = E [xx > y = m] before the second moment matrix is invariant to invertible linear transformations of x.Proof."}, {"heading": "2.2. Finite Sample Considerations", "text": "Although we have demonstrated the properties of our method by assuming knowledge of expectations E [xx > | y = m], we estimate these quantities in practice from our training samples. The empirical mean converges at a rate of O (n \u2212 1 / 2). Here and below, we suppress dependence on dimensionality d, which we consider fixed. Typical limitations of the finite sample become meaningful as soon as n = O (d log d) (Vershynin, 2010). By suppressing dependence on dimensionality d (n \u2212 1 / 2), we can use the results of matrix disturbance theory to determine that our finite sample results cannot be too far away from those obtained using the expected values."}, {"heading": "2.3. Regularization", "text": "An additional concern with finite samples is that C-m may not have the full rank as we have previously assumed. In particular, if there are fewer than d examples in class m, then C-m is guaranteed to be insufficient. If such a matrix appears in the denominator of (1), the estimation of eigenvectors can be unstable and hypersensitive to the present sample. A common solution (Platt et al., 2010) is to regulate the denominator matrix by adding a multiple of identity to the denominator, i.e., maximizingR\u03b3ij (v) = v > C-ivv > (C-j + \u03b3I) v, (5), which corresponds to a maximization equation of the equation (1) with an additional limitation of the upper limit to the standard of v."}, {"heading": "2.4. An Algorithm", "text": "We are left with the specification of a complete algorithm for multi-class classification."}, {"heading": "3. Related Work", "text": "Our approach is similar to many existing methods that work by finding eigenvectors of matrices constructed from data. One can imagine all these approaches as methods for determining directions v that maximize the signal to the noise ratio (v) = v > Svv > Nv, (6) where the symmetrical matrices S and N are such that the square shapes v > Sv and v > Nv represent the signal or capture the noise along the direction v. In Table 1 we present many well-known approaches that could be cast within this framework. Principal Component Analysis (PCA) finds the directions of the maximum variance without a specific noise model. The recently proposed Vanishing Component Analysis (Livni et al., 2013) finds the directions on which the projections disappear, so that one can think that the roles of the signal swapping signals and noise in PCA. Fisher LDA maximizes the variability in the class while minimizing the means within the class."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. MNIST", "text": "We start with the MNIST database of handwritten digits (LeCun et al., 1998), for which we can first locate the generalized eigenvectors, which show the discriminatory characteristics of the calculated guidelines (4), and then the generalized eigenvectors for each class pair (i, j) by solving C classes (3, 2), and we observe that the eigenvectors are sensitive to the circular stroke of a typical 3, while remaining insensitive to the areas where 2s and 3s overlap. Similar results are seen in the second and third rows in which we use class pairs (8, 5) and (3, 5) which we use the circular strokes of a typical 3, while we see the areas where 2s and 3s overlap."}, {"heading": "4.2. Covertype", "text": "Covertype is a multi-class dataset whose task is to predict one of 7 forest cover types with 54 cartographic variables (Blackard & Dean, 1999).RBF cores provide the state of the art on Covertype, and thus it is a benchmark dataset for fast approximate kernel techniques (Rahimi & Recht, 2007; Jose et al., 2013).Here we show that generalized eigenvector extraction works well with randomized function charts in the country of origin and generalized eigenfunction extraction in the RKHS while maintaining the speed and compactness of the original approaches. Covertype does not come with a designated test set, so we permutated the last 10% for testing by using the same traction test split for all experiments."}, {"heading": "4.3. TIMIT", "text": "In fact, the fact is that most of them will be able to be in a position to be in what they are in."}, {"heading": "5. Discussion", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "6. Conclusion", "text": "We have demonstrated a method for generalizing discriminatory characteristics by solving generalized eigenvalue problems, and demonstrated empirical effectiveness through multiple experiments.The method has multiple computational and statistical desiderates.In mathematical terms, generalized eigenvalue extraction is a mature numerical primitive, and the matrices that are decomposed can be estimated using map reduction techniques. Statistically, the method is invariant to invertable linear transformations, the estimation of eigenvectors is robust when the number of examples exceeds the number of variables, and the estimation of the resulting classification parameters is facilitated by the parsimony of derived representation.Based on this combination of empirical, computational, and statistical properties, we believe that the method introduced here is useful for a variety of machine learning problems."}, {"heading": "Acknowledgments", "text": "We thank John Platt and Li Deng for their helpful discussions and support in the TIMIT experiments."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Agarwal", "Alekh", "Chapelle", "Olivier", "Dud\u0301\u0131k", "Miroslav", "Langford", "John"], "venue": "CoRR, abs/1110.4198,", "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables", "author": ["Blackard", "Jock A", "Dean", "Denis J"], "venue": "Computers and Electronics in Agriculture,", "citeRegEx": "Blackard et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Blackard et al\\.", "year": 1999}, {"title": "On the sample covariance matrix estimator of reduced effective rank population matrices, with applications to fPCA", "author": ["F. Bunea", "L. Xiao"], "venue": null, "citeRegEx": "Bunea and Xiao,? \\Q2012\\E", "shortCiteRegEx": "Bunea and Xiao", "year": 2012}, {"title": "Templates for the solution of algebraic eigenvalue problems: a practical guide", "author": ["Demmel", "James", "Dongarra", "Jack", "Ruhe", "Axel", "van der Vorst", "Henk", "Bai", "Zhaojun"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Demmel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Demmel et al\\.", "year": 2000}, {"title": "Principal component neural networks", "author": ["Diamantaras", "Konstantinos I", "Kung", "Sun Y"], "venue": null, "citeRegEx": "Diamantaras et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Diamantaras et al\\.", "year": 1996}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["Fisher", "Ronald A"], "venue": "Annals of eugenics,", "citeRegEx": "Fisher and A.,? \\Q1936\\E", "shortCiteRegEx": "Fisher and A.", "year": 1936}, {"title": "The DARPA speech recognition research database: Specification and status", "author": ["W. Fisher", "G. Doddington", "Marshall", "Goudie K"], "venue": "In Proceedings of the DARPA Speech Recognition Workshop,", "citeRegEx": "Fisher et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1986}, {"title": "Regularized discriminant analysis", "author": ["Friedman", "Jerome H"], "venue": "Journal of the American statistical association,", "citeRegEx": "Friedman and H.,? \\Q1989\\E", "shortCiteRegEx": "Friedman and H.", "year": 1989}, {"title": "The unreasonable effectiveness of data", "author": ["Halevy", "Alon", "Norvig", "Peter", "Pereira", "Fernando"], "venue": "Intelligent Systems, IEEE,", "citeRegEx": "Halevy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halevy et al\\.", "year": 2009}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Tropp", "Joel A"], "venue": "SIAM review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition", "author": ["Hutchinson", "Brian", "Deng", "Li", "Yu", "Dong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hutchinson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2012}, {"title": "Local deep kernel learning for efficient non-linear svm prediction", "author": ["Jose", "Cijo", "Goyal", "Prasoon", "Aggrwal", "Parv", "Varma", "Manik"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Jose et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jose et al\\.", "year": 2013}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sliced inverse regression for dimension reduction", "author": ["Li", "Ker-Chau"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li and Ker.Chau.,? \\Q1991\\E", "shortCiteRegEx": "Li and Ker.Chau.", "year": 1991}, {"title": "Supervised dictionary learning", "author": ["Mairal", "Julien", "Bach", "Francis", "Ponce", "Jean", "Sapiro", "Guillermo", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:0809.3083,", "citeRegEx": "Mairal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2008}, {"title": "Translingual document representations from discriminative projections", "author": ["Platt", "John C", "Toutanova", "Kristina", "Yih", "Wentau"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Platt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Platt et al\\.", "year": 2010}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Randomized square-root free algorithms for generalized hermitian eigenvalue problems", "author": ["Saibaba", "Arvind K", "Kitanidis", "Peter K"], "venue": "arXiv preprint arXiv:1307.6885,", "citeRegEx": "Saibaba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saibaba et al\\.", "year": 2013}, {"title": "Introduction to the nonasymptotic analysis of random matrices", "author": ["Vershynin", "Roman"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Vershynin and Roman.,? \\Q2010\\E", "shortCiteRegEx": "Vershynin and Roman.", "year": 2010}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Simca: a method for analyzing chemical data in terms of similarity and analogy", "author": ["Wold", "Svante", "Sjostrom", "Michael"], "venue": "Chemometrics: theory and application,", "citeRegEx": "Wold et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Wold et al\\.", "year": 1977}], "referenceMentions": [{"referenceID": 13, "context": "In practice, supervised learning approaches are profitably employed in many domains, from movie recommendation to speech and image recognition (Koren et al., 2009; Hinton et al., 2012a; Krizhevsky et al., 2012).", "startOffset": 143, "endOffset": 210}, {"referenceID": 14, "context": "In practice, supervised learning approaches are profitably employed in many domains, from movie recommendation to speech and image recognition (Koren et al., 2009; Hinton et al., 2012a; Krizhevsky et al., 2012).", "startOffset": 143, "endOffset": 210}, {"referenceID": 8, "context": "In text classification, for example, unigram and bigram features together with linear classifiers are known to work well for a variety of related tasks (Halevy et al., 2009).", "startOffset": 152, "endOffset": 173}, {"referenceID": 17, "context": "This has fueled interest in methods that can learn the appropriate representations directly from the raw signal, with techniques such as dictionary learning (Mairal et al., 2008) and deep learning (Krizhevsky et al.", "startOffset": 157, "endOffset": 178}, {"referenceID": 14, "context": ", 2008) and deep learning (Krizhevsky et al., 2012; Hinton et al., 2012a) achieving state of the art performance in many important problems.", "startOffset": 26, "endOffset": 73}, {"referenceID": 3, "context": "Similar results apply to the sine of the angle between an estimated generalized eigenvector and the true one (Demmel et al., 2000) Section 5.", "startOffset": 109, "endOffset": 130}, {"referenceID": 18, "context": "A common solution (Platt et al., 2010) is to regularize the denominator matrix by adding a multiple of the identity to the denominator, i.", "startOffset": 18, "endOffset": 38}, {"referenceID": 18, "context": "Similar to (Platt et al., 2010), we extract the top few eigenvectors, as top eigenspaces are cheaper to compute than bottom eigenspaces.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "We remark that each step in Algorithm 1 is highly amenable to distributed implementation: empirical class-conditional second moment matrices can be computed using map-reduce techniques, the generalized eigenvalue problems can be solved independently in parallel, and the logistic regression optimization is convex and therefore highly scalable (Agarwal et al., 2011).", "startOffset": 344, "endOffset": 366}, {"referenceID": 18, "context": "Finally, oriented PCA (Diamantaras & Kung, 1996; Platt et al., 2010) is a very general framework in which the noise matrix can be the correlation matrix of any type of noise z meaningful to the task at hand.", "startOffset": 22, "endOffset": 68}, {"referenceID": 15, "context": "We begin with the MNIST database of handwritten digits (LeCun et al., 1998), for which we can visu-", "startOffset": 55, "endOffset": 75}, {"referenceID": 22, "context": "For comparison we include results from other permutation-invariant methods from (Wan et al., 2013) and (Goodfellow et al.", "startOffset": 80, "endOffset": 98}, {"referenceID": 12, "context": "RBF kernels provide state of the art performance on Covertype, and consequently it has been a benchmark dataset for fast approximate kernel techniques (Rahimi & Recht, 2007; Jose et al., 2013).", "startOffset": 151, "endOffset": 192}, {"referenceID": 12, "context": "result is from (Jose et al., 2013) where they also use a", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "TIMIT is a corpus of phonemically and lexically annotated speech of English speakers of multiple genders and dialects (Fisher et al., 1986).", "startOffset": 118, "endOffset": 139}, {"referenceID": 11, "context": "Such a classifier can be composed with standard sequence modeling techniques to produce an overall solution, which has made the multiclass problem a subject of research (Hinton et al., 2012b; Hutchinson et al., 2012).", "startOffset": 169, "endOffset": 216}, {"referenceID": 11, "context": "We use a standard preprocessing of TIMIT as our initial representation (Hutchinson et al., 2012).", "startOffset": 71, "endOffset": 96}, {"referenceID": 11, "context": "result from (Hutchinson et al., 2012).", "startOffset": 12, "endOffset": 37}, {"referenceID": 11, "context": "correspondence with the experimental protocol used with the T-DSN (Hutchinson et al., 2012).", "startOffset": 66, "endOffset": 91}, {"referenceID": 9, "context": "d > 10, the solution of generalized eigenvalue problems can only be performed via specialized libraries such as ScaLAPACK, or via randomized techniques, such as those outlined in (Halko et al., 2011; Saibaba & Kitanidis, 2013).", "startOffset": 179, "endOffset": 226}], "year": 2013, "abstractText": "Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results.", "creator": "LaTeX with hyperref package"}}}