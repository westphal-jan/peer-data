{"id": "1310.1949", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2013", "title": "Least Squares Revisited: Scalable Approaches for Multi-class Prediction", "abstract": "This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples n and the data dimension d are relatively large. These robust and parameter free algorithms are essentially iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we present a scalable stagewise variant of our approach, which achieves dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art accuracies.", "histories": [["v1", "Mon, 7 Oct 2013 20:48:58 GMT  (359kb,D)", "https://arxiv.org/abs/1310.1949v1", null], ["v2", "Mon, 21 Oct 2013 15:18:37 GMT  (360kb,D)", "http://arxiv.org/abs/1310.1949v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["alekh agarwal", "sham m kakade", "nikos karampatziakis", "le song", "gregory valiant"], "accepted": true, "id": "1310.1949"}, "pdf": {"name": "1310.1949.pdf", "metadata": {"source": "META", "title": "Least Squares Revisited:  Scalable Approaches for Multi-class Prediction", "authors": ["Alekh Agarwal"], "emails": ["alekha@microsoft.com", "skakade@microsoft.com", "nikosk@microsoft.com", "lsong@cc.gatech.edu", "valiant@stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Setting and Algorithms", "text": "Let's start with the simple case of binary GLMs, before turning to the more demanding multi-class environment."}, {"heading": "2.1. Warmup: Binary GLMs", "text": "The canonical definition of a GLM in binary classification (in the y function) follows from the following definition: \"There is a monotonous increasing function, and there is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\" (x) \"There is a convex function.\""}, {"heading": "2.2. Multi-class GLMs and Minimization Algorithms", "text": "The first question in the multi-class family relates to the definition of a generalized linear model; monotonicity is not directly extended in the multi-class environment. (D) The first question in the multi-class family relates to the definition of a generalized linear model. (D) The second question in the multi-class environment relates to the definition of a generalized linear model. (D) The first question in the multi-class environment relates to the definition of a generalized linear model. (D) The second question in the multi-class environment. (D) The second question in the multi-class environment. (D) The second question. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). D. (D. (D). (D) D. (D. (D). (D. (D). (D) D. (D. (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D.). (D.). (D. (D). (D.). (D).). (D.). (D. (D.). (D.). (D. (D. (D.).). (D.). (D.). (D. (D. (D.). (D.). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D. (D.).). (D. (D). (D.).). (D.). (D. (D.). (D. (D"}, {"heading": "2.3. Unknown Link Function for Multi-class", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "2.4. Scalable Variants", "text": "If the number of attributes is large, any optimization algorithm that scales superlinearly with dimensionality can have serious computational problems. In such cases, we can apply an approach of block coordinate descent (e.g., we typically use m \u2248 1000).The GEN procedure can be as simple as scanning m of the original attributes (with or without substitution) or more complex schemes such as random Fourier attributes (Rahimi & Recht, 2007).We call GEN and adjust a model to the m attributes using either algorithm 1 or algorithm 2. We then compress residuals and repeat the process on a fresh batch of m attributes returned by GEN. In algorithm 3, we offer pseudocoded attributes for this stage procedure."}, {"heading": "3. Experiments", "text": "We are looking at four sets of data MNIST, CIFAR-10, 20 newsgroups and RCV1, which capture many of the challenges facing real-world learning tasks. We believe that the lessons we have learned from analyzing and comparing the performance of these data sets are broader. For MNIST, we are comparing our algorithms with a variety of standard algorithms. In terms of both classification accuracy and optimization speed, we are achieving almost state-of-the-art in permutation invariant methods with standard functions (1.1% accuracy, improvement over methods such as the \"dropout\" neural network). For CIFAR-10, we are also achieving near-state-of-the-art accuracy (> 85%) through standard functions. In this context, we emphasize that it is the computing efficiency of our algorithms that allows us to achieve higher accuracy without novel feature generations.The story is rather different from the two sets of text data, where the performance of these data sets is less competitive than that of the online data sets, although substantially competitive in terms."}, {"heading": "3.1. MNIST", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "3.2. CIFAR-10", "text": "The CIFAR 10 dataset is a more sophisticated dataset in which many image recognition algorithms have been tested (primarily to illustrate different methods of feature generation; our work focuses instead on the optimization component, as a selection of features is available).The net neural approaches of \"dropout\" and \"maxout\" algorithms (Hinton et al., 2012; Goodfellow et al., 2013) provide the best reported performance of 84% and 87%, without increasing the size of the dataset (by jitter or other transformations).We are able to achieve over 85% accuracy with linear regressions of standard folding characteristics (without increasing the size of the dataset by jitter, etc.), demonstrating the advantage that enhanced optimizations are available."}, {"heading": "3.3. Well-Conditioned Problems", "text": "We are now examining two popular multiclass text datasets: 20 Newsgroups7 (henceforth NEWS20), which is a 20-class dataset and a four-class version of Reuters Corpus Volume 1 (Lewis et al., 2004) (henceforth RCV1). We are using a standard (log-) term frequency representation of the data discussed in the appendix. These data present rather different challenges than our vision datasets; apart from being economical, they are extremely well conditioned. The ratio of the second singular value to the 1000 (as a proxy for the conditional number) is 19.8 for NEWS20 and 14 for RCV1. In contrast, this conditional number for MNIST is about 72000 (when compressed with 3000 random Fourier characteristics) very well conditioned. Figure 1 (b) shows the normalized spectrum for the three data matrices: 72.7 datasu and 72.73 datas.7 / 2.2 datas.u.7 / 2.2."}, {"heading": "4. Discussion", "text": "In this paper, we present a series of fast and simple algorithms for dealing with large-scale multi-class prediction problems. We emphasize that the main result of the methods developed in this paper is their conceptual simplicity and ease of implementation. In fact, these properties make the methods quite versatile and can be extended in various ways. An example of this is shown in Algorithm 2. Likewise, it is easy to develop accelerated variants (Nesterov, 2009) by using the distances defined by the matrix as a proxy function in Nesterov's work. These variants enjoy the usual improvements in O (1 / t2) iteration complexity in the smooth and erroneous dependence in the strongly convex setting, while maintaining the metric-free nature of the algorithm. It is also quite easy to extend the algorithm to multi-label settings, the only difference being that the vector y of the labels is now based on the hypercube instead of simplicity."}, {"heading": "A. Appendix", "text": "We begin by stating that the Lipschitz and the strong monotonicity conditions in this field contain the smoothness and strong convexity of the function (u) as function u-Rk. In particular, since both matrices W1, W2, W2, W2 and W2, we have the following square upper limit as a consequence of the Lipschitz condition. (16) The strong monotonicity condition (17) results in an analog lower limit (W1x). (W2xT) x, W2xT) x, W2xT, W2xT, W2xT, W1x, W2xT, W2, W2x, W2x, W2, W2xT, W2xT, W2xT, W2, W2xW, W2, W2xW, W2xW."}], "references": [{"title": "Selective sampling algorithms for costsensitive multiclass prediction", "author": ["A. Agarwal"], "venue": "In ICML,", "citeRegEx": "Agarwal,? \\Q2013\\E", "shortCiteRegEx": "Agarwal", "year": 2013}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bordes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2009}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "In NIPS", "citeRegEx": "Bottou and Bousquet,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet", "year": 2008}, {"title": "entropy of convex sets and functions", "author": ["E.M. Bronshtein"], "venue": "Siberian Mathematical Journal,", "citeRegEx": "Bronshtein,? \\Q1976\\E", "shortCiteRegEx": "Bronshtein", "year": 1976}, {"title": "On the use of stochastic hessian information in optimization methods for machine learning", "author": ["R.H. Byrd", "G.M. Chin", "W. Neveitt", "J. Nocedal"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Byrd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 2011}, {"title": "Training a support vector machine in the primal", "author": ["O. Chapelle"], "venue": "Neural Comput.,", "citeRegEx": "Chapelle,? \\Q2007\\E", "shortCiteRegEx": "Chapelle", "year": 2007}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Discriminative reranking for natural language parsing", "author": ["M. Collins", "T. Koo"], "venue": "In ICML,", "citeRegEx": "Collins and Koo,? \\Q2000\\E", "shortCiteRegEx": "Collins and Koo", "year": 2000}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In ICML,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Greedy function approximation: a gradient boosting machine.(english summary)", "author": ["J.H. Friedman"], "venue": "Ann. Statist,", "citeRegEx": "Friedman,? \\Q2001\\E", "shortCiteRegEx": "Friedman", "year": 2001}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Majorization for crfs and latent likelihoods", "author": ["T. Jebara", "A. Choromanska"], "venue": "In NIPS,", "citeRegEx": "Jebara and Choromanska,? \\Q2012\\E", "shortCiteRegEx": "Jebara and Choromanska", "year": 2012}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "author": ["S.M. Kakade", "A. Kalai", "V. Kanade", "O. Shamir"], "venue": "In NIPS,", "citeRegEx": "Kakade et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2011}, {"title": "The isotron algorithm: Highdimensional isotonic regression", "author": ["A.T. Kalai", "R. Sastry"], "venue": "In COLT \u201909,", "citeRegEx": "Kalai and Sastry,? \\Q2009\\E", "shortCiteRegEx": "Kalai and Sastry", "year": 2009}, {"title": "Graphical Models", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Linear support vector machines via dual cached loops", "author": ["S. Matsushima", "S.V.N. Vishwanathan", "A.J. Smola"], "venue": "In KDD,", "citeRegEx": "Matsushima et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matsushima et al\\.", "year": 2012}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A.S. Nemirovsky", "D.B. Yudin"], "venue": "New York,", "citeRegEx": "Nemirovsky and Yudin,? \\Q1983\\E", "shortCiteRegEx": "Nemirovsky and Yudin", "year": 1983}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Y. Nesterov"], "venue": "New York,", "citeRegEx": "Nesterov,? \\Q2004\\E", "shortCiteRegEx": "Nesterov", "year": 2004}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Mathematical Programming A,", "citeRegEx": "Nesterov,? \\Q2009\\E", "shortCiteRegEx": "Nesterov", "year": 2009}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nesterov,? \\Q2012\\E", "shortCiteRegEx": "Nesterov", "year": 2012}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J.C. Platt"], "venue": "In Adavances in large margin classifiers,", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Single index convex experts: Efficient estimation via adapted bregman losses", "author": ["P. Ravikumar", "M. Wainwright", "B. Yu"], "venue": "Snowbird learning workshop,", "citeRegEx": "Ravikumar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2008}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S.J. Wright", "F. Niu"], "venue": "In NIPS, pp", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1c"], "venue": "URL http: //arxiv.org/abs/1212.0873", "citeRegEx": "Richt\u00e1rik and Tak\u00e1c,? \\Q2012\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1c", "year": 2012}, {"title": "Characterization of the subdifferentials of convex functions", "author": ["R.T. Rockafellar"], "venue": "Pac. J. Math.,", "citeRegEx": "Rockafellar,? \\Q1966\\E", "shortCiteRegEx": "Rockafellar", "year": 1966}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["N.L. Roux", "M. Schmidt", "F. Bach"], "venue": "In NIPS,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz", "year": 2012}, {"title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Reearch,", "citeRegEx": "Shalev.Shwartz and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang", "year": 2013}, {"title": "Large linear classification when data cannot fit in memory", "author": ["Yu", "H.-F", "Hsieh", "C.-J", "Chang", "K.-W", "Lin"], "venue": "TKDD,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "We extend our algorithm to simultaneously estimate the weights as well as the link function in GLMs under a parametric assumption on the link function, building on ideas from isotonic regression (Kalai & Sastry, 2009; Kakade et al., 2011).", "startOffset": 195, "endOffset": 238}, {"referenceID": 22, "context": "Similar procedures are common for binary SVMs (Platt, 1999) and for re-ranking(Collins & Koo, 2000).", "startOffset": 46, "endOffset": 59}, {"referenceID": 11, "context": "Notably, we also achieve state of the art accuracy results on MNIST and CIFAR-10, outperforming the \u201cdropout\u201d neural net (Hinton et al., 2012), where our underlying optimization procedures are entirely based on simple least squares approaches.", "startOffset": 121, "endOffset": 142}, {"referenceID": 29, "context": "(Bottou & Bousquet, 2008; Shalev-Shwartz, 2012)).", "startOffset": 0, "endOffset": 47}, {"referenceID": 28, "context": "This has led to interesting works on hybrid methods that interpolate between an initial online and subsequent batch behavior (Shalev-Shwartz & Zhang, 2013; Roux et al., 2012).", "startOffset": 125, "endOffset": 174}, {"referenceID": 4, "context": "There has also been a renewed interest in Quasi-Newton methods scalable to statistical problems using stochastic approximation ideas (Byrd et al., 2011; Bordes et al., 2009).", "startOffset": 133, "endOffset": 173}, {"referenceID": 1, "context": "There has also been a renewed interest in Quasi-Newton methods scalable to statistical problems using stochastic approximation ideas (Byrd et al., 2011; Bordes et al., 2009).", "startOffset": 133, "endOffset": 173}, {"referenceID": 21, "context": "High-dimensional problems have also led to natural consideration of block coordinate descent style procedures, both in serial (Nesterov, 2012) and distributed (Richt\u00e1rik & Tak\u00e1c, 2012; Recht et al.", "startOffset": 126, "endOffset": 142}, {"referenceID": 25, "context": "High-dimensional problems have also led to natural consideration of block coordinate descent style procedures, both in serial (Nesterov, 2012) and distributed (Richt\u00e1rik & Tak\u00e1c, 2012; Recht et al., 2011) settings.", "startOffset": 159, "endOffset": 204}, {"referenceID": 5, "context": "There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012).", "startOffset": 148, "endOffset": 206}, {"referenceID": 17, "context": "There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012).", "startOffset": 148, "endOffset": 206}, {"referenceID": 31, "context": "There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012).", "startOffset": 148, "endOffset": 206}, {"referenceID": 13, "context": "eralizes past works on learning in generalized linear models for binary classification, when the link function is known or unknown (Kalai & Sastry, 2009; Kakade et al., 2011).", "startOffset": 131, "endOffset": 174}, {"referenceID": 10, "context": "A well-known case where squared loss was used in conjunction with a stagewise procedure to fit binary and multi-class GLMs is the gradient boosting machine (Friedman, 2001).", "startOffset": 156, "endOffset": 172}, {"referenceID": 13, "context": "Similar observations have been noted for the binary case in some prior works as well (see Kakade et al. (2011); Ravikumar et al.", "startOffset": 90, "endOffset": 111}, {"referenceID": 13, "context": "Similar observations have been noted for the binary case in some prior works as well (see Kakade et al. (2011); Ravikumar et al. (2008)).", "startOffset": 90, "endOffset": 136}, {"referenceID": 0, "context": "Following the definition in the recent work of Agarwal (2013), we extend the binary case by defining the model: E[y | x] = \u2207\u03a6(W \u2217x) := g(W \u2217x) (12)", "startOffset": 47, "endOffset": 62}, {"referenceID": 27, "context": "This definition essentially corresponds to the link function g = \u2207\u03a6 satisfying (maximal and cyclical) monotonicity (Rockafellar, 1966) (natural extensions of monotonicity to vector spaces).", "startOffset": 115, "endOffset": 134}, {"referenceID": 15, "context": "Furthermore, when the GLM (12) corresponds to an exponential family with sufficient statistics y, then \u03a6 corresponds to the log-partition function like the binary case, and is always convex (Lauritzen, 1996).", "startOffset": 190, "endOffset": 207}, {"referenceID": 19, "context": "(Nesterov, 2004)) and is deferred to the supplement.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "Unfortunately, this is an extremely rich class; the sample complexity of estimating a uniformly bounded convex, Lipschitz function in k dimensions grows exponentially with k (Bronshtein, 1976).", "startOffset": 174, "endOffset": 192}, {"referenceID": 8, "context": "Finally, we project onto the unit simplex in order to obtain the new predictions, which can only decrease the squared error and can be done in O(k) time (Duchi et al., 2008).", "startOffset": 153, "endOffset": 173}, {"referenceID": 13, "context": "Analyzing the statistical issues, where there is noise, can be handled using ideas in (Kalai & Sastry, 2009; Kakade et al., 2011).", "startOffset": 86, "endOffset": 129}, {"referenceID": 9, "context": "The comparison includes VW , and six algorithms implemented in Liblinear (Fan et al., 2008) (see figure caption).", "startOffset": 73, "endOffset": 91}, {"referenceID": 11, "context": "The neural net approaches of \u201cdropout\u201d and \u201cmaxout\u201d algorithms of (Hinton et al., 2012; Goodfellow et al., 2013) provide the best reported performance of 84% and 87%, without increasing the size of the dataset (through jitter or other transformations).", "startOffset": 66, "endOffset": 112}, {"referenceID": 6, "context": "Figure 3 illustrates the performance when we use two types of convolutional features: features generated by convolving the images by random masks, and features generated by convolving with K-means masks (as in (Coates et al., 2011), though we do not use contrast normalization).", "startOffset": 210, "endOffset": 231}, {"referenceID": 16, "context": "We now examine two popular multiclass text datasets: 20 newsgroups (henceforth NEWS20), which is a 20 class dataset and a four class version of Reuters Corpus Volume 1 (Lewis et al., 2004) (henceforth RCV1).", "startOffset": 168, "endOffset": 188}, {"referenceID": 20, "context": "Similarly, it is straightforward to develop accelerated variants (Nesterov, 2009), by using the distances defined by the matrix \u03a3\u0302 as the proxfunction in Nesterov\u2019s work.", "startOffset": 65, "endOffset": 81}], "year": 2013, "abstractText": "This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples n and the data dimension d are relatively large. These robust and parameter free algorithms are essentially iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we present a scalable stagewise variant of our approach, which achieves dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-theart accuracies.", "creator": "LaTeX with hyperref package"}}}