{"id": "1310.2049", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2013", "title": "Fast Multi-Instance Multi-Label Learning", "abstract": "In many real-world tasks, particularly those involving data objects with complicated semantics such as images and texts, one object can be represented by multiple instances and simultaneously be associated with multiple labels. Such tasks can be formulated as multi-instance multi-label learning (MIML) problems, and have been extensively studied during the past few years. Existing MIML approaches have been found useful in many applications; however, most of them can only handle moderate-sized data. To efficiently handle large data sets, in this paper we propose the MIMLfast approach, which first constructs a low-dimensional subspace shared by all labels, and then trains label specific linear models to optimize approximated ranking loss via stochastic gradient descent. Although the MIML problem is complicated, MIMLfast is able to achieve excellent performance by exploiting label relations with shared space and discovering sub-concepts for complicated labels. Experiments show that the performance of MIMLfast is highly competitive to state-of-the-art techniques, whereas its time cost is much less; particularly, on a data set with 20K bags and 180K instances, MIMLfast is more than 100 times faster than existing MIML approaches. On a larger data set where none of existing approaches can return results in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is able to identify the most representative instance for each label, and thus providing a chance to understand the relation between input patterns and output label semantics.", "histories": [["v1", "Tue, 8 Oct 2013 09:03:28 GMT  (444kb,D)", "http://arxiv.org/abs/1310.2049v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sheng-jun huang", "wei gao", "zhi-hua zhou"], "accepted": true, "id": "1310.2049"}, "pdf": {"name": "1310.2049.pdf", "metadata": {"source": "CRF", "title": "Fast Multi-Instance Multi-Label Learning", "authors": ["Sheng-Jun Huang", "Wei Gao", "Zhi-Hua Zhou"], "emails": ["zhouzh@nju.edu.cn"], "sections": [{"heading": null, "text": "In many real-world tasks, especially for data objects with complicated semantics such as images and text, an object can be represented by multiple instances and linked to multiple labels at the same time. Such tasks can be formulated as multi-instance multi-label learning (MIML) problems and have been extensively studied in recent years. Existing MIML approaches have proven useful in many applications, but most of them can only handle moderately large data. In order to handle large data sets efficiently, we propose the MIMLfast approach in this paper, which first constructs a low-dimensional subspace shared by all labels and then builds specific linear models to optimize the risk-free ranking loss using stochastic gradient lowering. Although the MIML problem is complicated, MIMLfast is able to achieve outstanding performance by exploiting label relationships with common space and discovering sub-concepts for complicated labels."}, {"heading": "1. Introduction", "text": "However, in many real-world applications, an object can of course be broken down into several instances, and it has multiple class labels at the same time. In text categorization, an article can belong to several categories, and it can be represented by a bag of instances, one for a paragraph [22]. In the gene function, there are tasks that include multiple labels, and in text categorization tasks that belong to several categories, and it can be represented by a bag of instances, one for a paragraph [22]. In the gene function, there are tasks that typically include multiple labels, and which are represented by a series of images."}, {"heading": "2. The MIMLfast Approach", "text": "In fact, it is so that it is a way in which people are able to determine for themselves what they want and what they want. (...) In fact, it is so that people are able to decide whether they want it or not. (...) It is not so that people are able to decide whether they want it or not. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.) \"(.). (.)\" (.) \"(.\" (.) \"(.).\" (. \"(.)\" (.) \"(.)\" (. \"(.)\" (.) \"(.)\" (.) \"(.\" (.) \"(.).\" (.) \"(.\" (.). \"(.\" (.) \"(.).\" (.) \"(.\" (.) \"(.).\" (. \"(.)\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (It. \"(.).\" (. \"(.).\" (.). \"(It.\" (.). \"(It is.\" (. \"(.). (.).\" (It is. (.). (. (.). \"(.).\" (It is. (. \"(.). (.).\" (It is. \"(It is. (.\" (.). (.). (.). \"(It is.). (It is. (. (. (.). (.).). ("}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Settings", "text": "We compare MIMLfast with six state-of-the-art MIML methods: DBA [22], a generative model for MIML learning; KISAR [11], a MIML algorithm tries to detect instance-label relation; MIMLBoost [26], a boosting method decomposes MIML into multi-instance single label problems; MIMLkNN [24], a MIML algorithm started to instance annotationationationationation.We perform the experiments on 6 moderate size data sets and 2 large data sets. Among the moderately large data sets, Scene and Reuters are two benchmark datasets that usually work in existing MIML. Scene [26] consists of 2000 scene classification images and is associated with 5 desert mountains:"}, {"heading": "3.2. Performance Comparison", "text": "As shown in the table, our approach MIMLfast performs best in most cases. DBA tends to prefer text data, and is surpassed by MIMLfast on all datasets. MIMLkNN and MIMLSVM work consistently on all datasets, but are not competitive compared to MIMLfast. Finally, RankLossSIM is comparable to MIMLfast on 4 out of 6 datasets and does not perform well. MIMLkNN and MIMLSVM work consistently on all datasets, but are not competitive compared to MIMLfast."}, {"heading": "3.3. Efficiency Comparison", "text": "All experiments are 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 performed on a machine with 16 x 2.60 GHz CPUs and 32 GB of RAM. Once again, we show the time cost of each algorithm for the six moderate datasets in Figure 3. Since the results are similar for the two smallest datasets Letter Carroll and Letter Frost, we take one of them as representative to save space. Obviously, our approach is the most efficient for all datasets. MIMLBoost is the most time-consuming, followed by RankLossSIM and MIMLkN. The superiority of our approach differs more on larger datasets than on larger ones. As shown in Figure 4, MIMLBoost is even faster on the size of Corel5K than MLure."}, {"heading": "3.4. Key Instance Detection", "text": "Inspired by [11], assuming that each label is triggered by its most positive instance, our MIMLfast approach is able to identify the key instance for each instance. We first give an intuitive evaluation of the key instance recognized by MIMLfast. On MSRA, we first partition each image into a series of patches with k-mean clusters, and then extract one instance from each cluster. In Figure 5, we show two sample images and highlight the regions that correspond to the key instance recognized by our approach for each label. As the image regions are reached by clusters, an instance can coincide with several regions in the same cluster and not with a single region."}, {"heading": "3.5. Sub-Concept Discovery", "text": "To investigate the effectiveness of the sub-concept discovery, we perform MIMLfast with different number of sub-concepts on the two benchmark datasets: Scene for image classification and Reuters for text categorization. Table 4 presents the results with K varying from 1 to 15 with step size of 5. For each value of K, we perform a 10-fold cross-validation and report the average results as well as standard deviations. Note that K is selected by cross-validation on the training data in Section 3.2. As shown in Table 4, the difference between results with different K values is helpful compared to neglecting the sub-concepts (K = 1) (K = 5, 10 and 15 are all better than K = 1). When the K gets larger, the difference between results with different K values is not very significant. This could be due to the fact that if we set a K value greater than what is really needed for the epts, some sub-concepts may not include a large sub-generic, therefore some sub-concepts may not include large examples for K, although the sub-generic ones may be large."}, {"heading": "3.6. Comparison with Variants", "text": "To further investigate how MIMLfast works, we examine two variants, V1 and V2. V1 puts W0 in Equation 1 and directly learns a linear model for each label. It is designed to investigate whether learning the shared space is helpful. V2 simply selects the top r labels as relevant, with r being the average number of relevant labels on the training data. It is designed to investigate whether learning the dummy label provides a good separation between relevant and irrelevant labels. Table 5 shows the results on the two benchmark data sets. V1 is significantly worse than MIMLfast in all criteria, implying that learning the shared space for all labels is better than learning each label on its own. When hammering losses, MIMLfast achieves significantly better performance than V2 in all four criteria, which implies that learning the shared space for all labels is better than learning each label on its own. MIMLfast achieves significantly better performance than V2 in hammering losses, while achieving comparable performance on the other four criteria, but does not affect the separation of relevant labels from the relevant ones."}, {"heading": "4. Related Work", "text": "MIMLBoost [26] degenerated MIML into multi-instance single-label learning. A generative model for MIML was proposed by Yang et al. [22]. Next neighboring and neural network approaches for MIML were proposed in [24] and [25], respectively. Zha et al. [23] proposed a hidden contingent random field model for MIML caption. Briggs et al. [4] proposed to optimize the ranking loss for MIML instance labeling. In [11], the authors attempted to determine which patters trigger MIML learning by constructing a prototype with clusters for each label. Existing MIML approaches achieved success in many applications, most with moderately large amounts of data due to the high computational load."}, {"heading": "5. Conclusion", "text": "MIML is a framework for learning with complicated objects and has proven effective in many applications. However, existing MIML approaches are usually too time-consuming to solve major problems. In this article, we propose the MIMLfast approach to learn quickly using MIML examples. On the one hand, efficiency is greatly improved by optimizing approximate ranking loss with SGD based on a two-step linear model; on the other, effectiveness is achieved by leveraging label relationships in a common space and discovering sub-concepts for complicated labels. On top of that, our approach can of course recognize key instances for each label, providing an opportunity to discover the relationship between input patterns and output label semantics. In the future, we will try to optimize other loss functions rather than investigate ranking losses."}], "references": [{"title": "Support vector machines for multipleinstance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in neural information processing systems 15, pages 561\u2013568. MIT Press, Cambridge, MA,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Minimizing the misclassification error rate using a surrogate convex loss", "author": ["S. Ben-David", "D. Loker", "N. Srebro", "K. Sridharan"], "venue": "Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "Compstat,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Rank-loss support instance machines for miml instance annotation", "author": ["F. Briggs", "X.Z. Fern", "R. Raich"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 534\u2013542, Beijing, China,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence, 89(1):31\u201371,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary", "author": ["P. Duygulu", "K. Barnard", "J.F.G. Freitas", "D.A. Forsyth"], "venue": "Proceedings of the 7th European Conference on Computer Vision, pages 97\u2013112, Copenhagen, Denmark,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Letter recognition using holland-style adaptive classifiers", "author": ["P.W. Frey", "D.J. Slate"], "venue": "Machine Learning, 6(2):161\u2013182,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Multilabel classification via calibrated label ranking", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "E. Loza Men\u0107\u0131a", "K. Brinker"], "venue": "Machine Learning, 73(2):133\u2013153,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning a distance metric from multi-instance multi-label data", "author": ["R. Jin", "S. Wang", "Z.H. Zhou"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 896\u2013902, Miami, FL,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Msra-mm 2.0: A large-scale web multimedia dataset", "author": ["H. Li", "M. Wang", "X.S. Hua"], "venue": "In Proceedings of the IEEE International Conference on Data Mining Workshops,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Towards discovering what patterns trigger what labels", "author": ["Yu-Feng Li", "Ju-Hua Hu", "Yuang Jiang", "Zhi-Hua Zhou"], "venue": "In Proceedings of the 26th AAAI Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Drosophila gene expression pattern annotation through multi-instance multi-label learning", "author": ["Y.X. Li", "S. Ji", "S. Kumar", "J. Ye", "Z.H. Zhou"], "venue": "Proceedings of the 21st International Joint Conference on Artificial Intelligence, pages 1445\u20131450, Pasadena, CA,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning from candidate labeling sets", "author": ["J. Luo", "F. Orabona"], "venue": "Advances in Neural Information Processing Systems 23. MIT Press, Cambridge, MA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiple-instance learning for natural scene classification", "author": ["O. Maron", "A.L. Ratan"], "venue": "Proceedings of the 15th International Conference on Machine Learning, pages 341\u2013349, Madison, WI,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "A new svm approach to multi-instance multi-label learning", "author": ["N. Nguyen"], "venue": "Proceedings of the 10th IEEE International Conference on Data Mining, pages 384\u2013392, Sydney, Australia,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics, 22(3):400\u2013407,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1951}, {"title": "BoosTexter: A boosting-based system for text categorization", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning, 39(2-3):135\u2013168,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys, 34(1):1\u201347,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Ranking with ordered weighted pairwise classification", "author": ["N. Usunier", "D. Buffoni", "P. Gallinari"], "venue": "Proceedings of the 26th International Conference on Machine Learning, pages 1057\u20131064, Montreal, Canada,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Proceedings of the 22nd International Joint Conference on Artificial Intelligence, pages 2764\u20132770, Barcelona, Spain,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Object categorization by learned universal visual dictionary", "author": ["J. Winn", "A. Criminisi", "T. Minka"], "venue": "10th IEEE International Conference on Computer Vision, pages 1800\u20131807, Beijing, China,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Dirichlet-bernoulli alignment: A generative model for multi-class multi-label multi-instance corpora", "author": ["S.H. Yang", "H. Zha", "B.G. Hu"], "venue": "Advances in Neural Information Processing Systems 22, pages 2143\u20132150. MIT Press, Cambridge, MA,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint multi-label multi-instance learning for image classification", "author": ["Z.J. Zha", "X.S. Hua", "T. Mei", "J. Wang", "G.J. Qi", "Z. Wang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138, Anchorage, AK,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "A k-nearest neighbor based multi-instance multi-label learning algorithm", "author": ["M.-L. Zhang"], "venue": "Proceedings of the 22nd IEEE International Conference on Tools with Artificial Intelligence, pages 207\u2013212, Arras, France,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Mimlrbf: Rbf neural networks for multi-instance multi-label learning", "author": ["M.-L. Zhang", "Z.-J. Wang"], "venue": "Neurocomputing, 72(16):3951\u20133956,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-instance multi-label learning with application to scene classification", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Advances in Neural Information Processing Systems 19, pages 1609\u20131616. MIT Press, Cambridge, MA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-instance multi-label learning", "author": ["Z.-H. Zhou", "M.-L. Zhang", "S.-J. Huang", "Y.-F. Li"], "venue": "Artificial Intelligence, 176(1):2291\u20132320,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "For example, in image classification problems, an image usually contains multiple objects, and can be divided into several segments, where each segment is represented with an instance, and corresponds to a semantic label [26]; in text categorization tasks, an article may belong to multiple categories, and can be represented by a bag of instances, one for a paragraph [22]; in gene function prediction tasks, a gene usually has multiple labels since it is related to multiple functions, and can be represented with a set of images with different views [12].", "startOffset": 221, "endOffset": 225}, {"referenceID": 21, "context": "For example, in image classification problems, an image usually contains multiple objects, and can be divided into several segments, where each segment is represented with an instance, and corresponds to a semantic label [26]; in text categorization tasks, an article may belong to multiple categories, and can be represented by a bag of instances, one for a paragraph [22]; in gene function prediction tasks, a gene usually has multiple labels since it is related to multiple functions, and can be represented with a set of images with different views [12].", "startOffset": 369, "endOffset": 373}, {"referenceID": 11, "context": "For example, in image classification problems, an image usually contains multiple objects, and can be divided into several segments, where each segment is represented with an instance, and corresponds to a semantic label [26]; in text categorization tasks, an article may belong to multiple categories, and can be represented by a bag of instances, one for a paragraph [22]; in gene function prediction tasks, a gene usually has multiple labels since it is related to multiple functions, and can be represented with a set of images with different views [12].", "startOffset": 553, "endOffset": 557}, {"referenceID": 26, "context": "Multi-instance multi-label learning (MIML) is a recent proposed framework for such complicated objects [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 25, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 22, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 8, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 21, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 24, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 12, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 14, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 23, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 3, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 26, "context": "During the past years, many MIML algorithms were proposed [26, 23, 9, 22, 25, 13, 15, 24, 4, 27].", "startOffset": 58, "endOffset": 96}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "It is usually assumed that a bag is positive if and only if it contains at least one positive instance [5, 4].", "startOffset": 103, "endOffset": 109}, {"referenceID": 3, "context": "It is usually assumed that a bag is positive if and only if it contains at least one positive instance [5, 4].", "startOffset": 103, "endOffset": 109}, {"referenceID": 18, "context": "Based on R(X, l), we further define the ranking error [19] with respect to an example X on label l as", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "We instead explore the following hinge loss, which has been shown as an optimal choice among all convex surrogate losses [2], \u03a8(X, l) = \u2211 j\u2208\u0232 (X, l) |1 + fj(X)\u2212 fl(X)|+ R(X, l) , (5)", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "We then employ stochastic gradient descent (SGD) [16] to minimize the ranking error.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "However, in multi-label learning, the bag Xtest may have more than one label; and thus one do not know how many labels should be selected as relevant ones from the ranked label list [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 21, "context": "We compare MIMLfast with six state-of-the-art MIML methods: DBA [22], a generative model for MIML learning; KISAR [11], a MIML algorithm tries to discover instance-label relation; 10", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "We compare MIMLfast with six state-of-the-art MIML methods: DBA [22], a generative model for MIML learning; KISAR [11], a MIML algorithm tries to discover instance-label relation; 10", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "MIMLBoost [26], a boosting method decomposes MIML into multi-instance single label problems; MIMLkNN [24], a MIML nearest neighbor algorithm; MIMLSVM [26], a SVM style algorithm which decomposes MIML into single instance multi-label problems; and RankLossSIM [4], a MIML algorithm minimizes ranking loss for instance annotation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "MIMLBoost [26], a boosting method decomposes MIML into multi-instance single label problems; MIMLkNN [24], a MIML nearest neighbor algorithm; MIMLSVM [26], a SVM style algorithm which decomposes MIML into single instance multi-label problems; and RankLossSIM [4], a MIML algorithm minimizes ranking loss for instance annotation.", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "MIMLBoost [26], a boosting method decomposes MIML into multi-instance single label problems; MIMLkNN [24], a MIML nearest neighbor algorithm; MIMLSVM [26], a SVM style algorithm which decomposes MIML into single instance multi-label problems; and RankLossSIM [4], a MIML algorithm minimizes ranking loss for instance annotation.", "startOffset": 150, "endOffset": 154}, {"referenceID": 3, "context": "MIMLBoost [26], a boosting method decomposes MIML into multi-instance single label problems; MIMLkNN [24], a MIML nearest neighbor algorithm; MIMLSVM [26], a SVM style algorithm which decomposes MIML into single instance multi-label problems; and RankLossSIM [4], a MIML algorithm minimizes ranking loss for instance annotation.", "startOffset": 259, "endOffset": 262}, {"referenceID": 25, "context": "Scene [26] consists of 2000 images for scene classification, and is associated with 5 possible labels: desert, mountains, sea, sunset and trees.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "For each image, a bag of 9 instances is extracted via SBN [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Reuters is constructed based on the Reuters-21578 data set [18] with the sliding window technique in [1].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Reuters is constructed based on the Reuters-21578 data set [18] with the sliding window technique in [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "in their recent work [4]: Letter Carroll and Letter Frost are constructed using the UCI Letter Recognition dataset [7], where a bag is created for each word, and labels correspond to the letters.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "in their recent work [4]: Letter Carroll and Letter Frost are constructed using the UCI Letter Recognition dataset [7], where a bag is created for each word, and labels correspond to the letters.", "startOffset": 115, "endOffset": 118}, {"referenceID": 20, "context": "MSRC v2 is a subset of the Microsoft Research Cambridge (MSRC) image dataset [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "Corel5K [6] contains 5000 segmented images and 260 class labels, and each image is represented by 9 instances on average.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "MSRA [10] is a multimedia database collected by Microsoft Research Asia, the subset used in this work contains 30000 images with 99 possible labels, and each image is represented with a bag of 9 instances.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "For MIMLfast, the step size is in the form \u03b3t = \u03b30/(1+\u03b7\u03b30t) according to [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "Note that coverage is normalized by the number of labels such that all criteria are in the interval [0, 1].", "startOffset": 100, "endOffset": 106}, {"referenceID": 16, "context": "The definition of these criteria can be found in [17, 27].", "startOffset": 49, "endOffset": 57}, {"referenceID": 26, "context": "The definition of these criteria can be found in [17, 27].", "startOffset": 49, "endOffset": 57}, {"referenceID": 10, "context": "Inspired by [11], by assuming that each label is triggered by its most positive instance, our MIMLfast approach is able to identify the key instance for each label.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "On MSRA, following [11], we first partition each image into a set of patches with k-means clustering, and", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "For MIMLfast and RankLossSIM, the key instance for a specific label is identified by selecting the instance with maximum prediction value on that label, while for KISAR, key instance is the one closest to the prototype of the label as in [11].", "startOffset": 238, "endOffset": 242}, {"referenceID": 25, "context": "For example, MIMLSVM [26] degenerated the MIML problem into single-instance multi-label tasks to solve.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "MIMLBoost [26] degenerated MIML to multi-instance single-label learning.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Nearest neighbor and neural network approaches for MIML were proposed in [24] and [25], respectively.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "Nearest neighbor and neural network approaches for MIML were proposed in [24] and [25], respectively.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "[23] proposed a hidden conditional random field model for MIML image annotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] proposed to optimize ranking loss for MIML instance annotation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "In [11], the authors tried to discover what patters trigger what labels in MIML learning by constructing a prototype for each label with clustering.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "In [20], a similar technique was used to optimize WARP loss for image annotation; however, it dealt with single-instance single-label problem, which is quite different from our MIML problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [27], an approach of discovering sub-concepts for complicated concepts was proposed based", "startOffset": 3, "endOffset": 7}], "year": 2013, "abstractText": "In many real-world tasks, particularly those involving data objects with complicated semantics such as images and texts, one object can be represented by multiple instances and simultaneously be associated with multiple labels. Such tasks can be formulated as multi-instance multi-label learning (MIML) problems, and have been extensively studied during the past few years. Existing MIML approaches have been found useful in many applications; however, most of them can only handle moderate-sized data. To efficiently handle large data sets, in this paper we propose the MIMLfast approach, which first constructs a low-dimensional subspace shared by all labels, and then trains label specific linear models to optimize approximated ranking loss via stochastic gradient descent. Although the MIML problem is complicated, MIMLfast is able to achieve excellent performance by exploiting label relations with shared space and discovering sub-concepts for complicated labels. Experiments show that the performance of MIMLfast is highly competitive to state-of-the-art techniques, whereas its time cost is much less; particularly, on a data set with 20K bags and 180K instances, MIMLfast is more than 100 times faster than existing MIML approaches. On a larger data set where none of existing approaches can return results in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is able to identify the most representative instance for each label, and thus providing a chance to understand the relation between input patterns and output label semantics.", "creator": "LaTeX with hyperref package"}}}