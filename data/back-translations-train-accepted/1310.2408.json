{"id": "1310.2408", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2013", "title": "Improved Bayesian Logistic Supervised Topic Models with Data Augmentation", "abstract": "Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency.", "histories": [["v1", "Wed, 9 Oct 2013 09:23:10 GMT  (475kb)", "http://arxiv.org/abs/1310.2408v1", "9 pages, ACL 2013"]], "COMMENTS": "9 pages, ACL 2013", "reviews": [], "SUBJECTS": "cs.LG cs.CL stat.AP stat.ML", "authors": ["jun zhu", "xun zheng", "bo zhang"], "accepted": true, "id": "1310.2408"}, "pdf": {"name": "1310.2408.pdf", "metadata": {"source": "CRF", "title": "Improved Bayesian Logistic Supervised Topic Models with Data Augmentation", "authors": ["Jun Zhu", "Xun Zheng", "Bo Zhang"], "emails": ["dcszj@tsinghua.edu.cn;", "dcszb@tsinghua.edu.cn;", "vforveri.zheng@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 131 0.24 08v1 [cs.LG] 9 Oct 201 3"}, {"heading": "1 Introduction", "text": "As generally assumed in the monitored latent dirichlet mapping (sLDA, 2012) and in our result, we observed a weak response to this problem (Lead and McAuliffe, 2010; Wang et al., 2009), one way to improve the predictive power of LDA is to define a probability model for the widely used document-level response variables, in addition to the document-level probability model. For example, the logistic probability model is often used for binary or multinomial responses. By introducing some priors, a retrospective inference with the Bayes rule is performed. Although one problem that might limit the use of existing logistic supervisors is that they treat the documentary response variable as an additional word. Although some specialized treatments are performed to define the probability of each response variable, it is usually of a much smaller scale than the probability of the ordinary ten or hundreds of words in each document."}, {"heading": "2 Logistic Supervised Topic Models", "text": "We now present the generalized Bayesian, logistically supervised thematic models."}, {"heading": "2.1 The Generalized Models", "text": "We consider the binary classification with a training set D = (wd, yd) Dd = 1, whereby the response variable Y values from the issue room Y = 1, 1, 2, 3, 4, 4, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11"}, {"heading": "2.2 Variational Approximation Algorithms", "text": "Furthermore, the latent variables Z complicate the problem of concluding against Bayesian logistic regression models (Chen et al., 1999; Meyer et al., 2002; Polson et al., 2012). Previous algorithms for solving the problem (5) rely on techniques of variable approximation. It is easy to show that the variation method (Wang et al., 2009) is a coordinate descend algorithm for solving the problem (5) with the additional fully factored constraint q (\u03b7, \u044b, Z, \u03a6) = q (\u03b7). Note that the non-Bayesian treatment of the factors q (zdn) and k q (\u03a6k) and a variable approximation of the expectation of the loglogistic probability that can be directly calculated."}, {"heading": "3 A Gibbs Sampling Algorithm", "text": "Now we present a simple and efficient Gibbs sampling algorithm for generalized Bayesian logistically monitored topic models."}, {"heading": "3.1 Formulation with Data Augmentation", "text": "Since the logistic pseudo-probability 1 (y | Z, \u03b7) is not conjugated with normal priors, it is not easy to directly deduce the sampling algorithms. Instead, we develop our algorithms by introducing auxiliary variables that result in a scale mix of Gaussian components and analytical conditional distributions for automatic Bayesian inferences without acceptance / rejection. Our algorithm represents a first attempt to expand Polson's approach (Polson et al., 2012) to deal with highly non-trivial Bayesian latent variable models. First, let us introduce the Polya gamma variables. Definition 1 (Polson et al., 2012) A random variable X has a polya gamma distribution that is distributed by X-PG (a, b), ifX = 12\u03c02 \u00b2 \u00b2 variables i = 1gamma d (Polson et al, 2012)."}, {"heading": "3.2 Inference with Collapsed Gibbs Sampling", "text": "Although we can do Gibbs sampling to derive complete posterior distribution from it, the method for effectively improving mixing rates that has been successfully used in LDA (Griffiths and Steyvers, 2004) is to integrate and form a Markov chain whose equilibrium distribution is the boundary distribution. We suggest using collapsed Gibbs sampling that has been successfully used in LDA (Griffiths and Steyvers, 2004). For our model, the collapsed posterior distribution is isq (\u03b7, Z) p0 (W, Z) p0 (Z, \u03b2) p (Z) p sampling that is successfully used in LDA (Griffiths and Steyvers, 2004)."}, {"heading": "3.3 Prediction", "text": "In order to apply the classifier \u03b7 to test data, we have to derive their topic assignments. We take the approach in (Zhu et al., 2012; Jiang et al., 2012), which uses a point estimation of topics \u03a6 from training data and makes predictions based on it. Specifically, we use the MAP estimate \u043a to replace the probability distribution p (\u03a6). For the Gibbs sample, an estimate of how long the samples will be applied is obtained. Then, when we receive a test document w, we derive its latent components z by using \u03a6 as p (zn = k | z \u00ac n), where Ck \u00ac n are the times at which the terms in this document are assigned to topic k, excluding the n-th term."}, {"heading": "4 Experiments", "text": "We present empirical results and sensitivity analyses to demonstrate the efficiency and predictive performance3 of the generalized logistically monitored topic models using the data set 20Newsgroups (20NG), which contains about 20,000 posts within 20 news groups. We follow the same setting as in (Zhu et al., 2012) and remove a standard list of stopwords for both binary and multi-class classification. For all experiments, we use the standard standard standard before p0 (\u03b7) (i.e. \u03bd2 = 1) and the symmetrical dirichlet priors \u03b1 = \u03b1K 1, \u03b2 = 0.01 \u00d7 1, where 1 is a vector, with all entries being 1. For each setting, we specify the average power and standard deviation with five randomly initialized passes."}, {"heading": "4.1 Binary classification", "text": "Following the same setting in (Lacoste-Jullien et al., 2009; Zhu et al., 2012), the task is to make bookings of the newsgroup alt.atheism and that of the group talk.religion.misc. The training kit contains 856 documents and the test kit contains 569 documents. We compare the generalized logistical monitored LDA methods using Gibbs Sampling (referred to by gSLDA) with various competitors, including the standard sLDA using variable medium-field methods (referred to by vSLDA) (Wang et al., 2009), the MedLDA model using variable medium-field methods (referred to by vMedLDA et al., 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (referred to by gMedLDA) (Jiang et al., 2012).3Due to spatial limitation, the subject visualization (similar to that of MedLDA)."}, {"heading": "4.2 Multi-class classification", "text": "We perform a multi-class classification on the 20NG models by performing the most likely two DA assessments. For multi-class classification, one possible extension is the use of a multinomial logistical regression model for categorical variables Y, using themes as input features. However, it is not trivial to develop a Gibbs sampling algorithm that can be performed via a coordinate strategy (Polson et al., 2012). Here, we apply the binary gSLDA strategies to pursue the multi-class classification that follows the \"one vs all\" strategy that has been effectively demonstrated (Rifkin and Klautau, 2004) to provide some preliminary analyses."}, {"heading": "4.3 Sensitivity analysis", "text": "Burn-In: Fig. 3 shows the performance of gSLDA + with different burn-in steps for binary5http: / / docs.graphlab.org / toolkits.htmlclassification. If M = 0 (see most of the left points), the models are built on random topic assignments. We can see that the classification performance increases rapidly and comes close to stable optimum with about 20 burn-in steps. Training time increases approximately linearly when more burn-in steps are used. In addition, training time increases linearly when K increases. In the previous experiments, we put M = 100. Fig.4 shows the performance of gSLDA + and its parallel implementation (i.e. parallel gSLDA +) for multi-class classification with different burn-in steps. We can see if the number of burn-in steps is greater than 20, the performance of gSLDA + is relatively stable."}, {"heading": "5 Conclusions and Discussions", "text": "We present two improvements to Bayes logistically monitored subject models, namely a general formulation by introducing a regularization parameter to avoid model imbalances and a highly efficient Gibbs sampling algorithm, without limiting assumptions about posterior distributions by examining the idea of data multiplication, and the algorithm can also be parallelized. Empirical results for both binary and multi-stage classifications demonstrate demostrategically significant improvements over existing logistically monitored subject models. Our preliminary results with GraphLab have shown promising parallelizations in the Gibbs sampling algorithm. For future work, we plan to apply more careful investigations, e.g. using various distributed architectures (Ahmed et al., 2012; Newman et al., 2009; Smola et al., 2010; Narayanamurthy and Narayanamurthy, 2010) to make sampling algorithms highly algorithmic for other types of data manipulation, such as those that can be applied to high-grade algorithms."}, {"heading": "Acknowledgments", "text": "This work is supported by the R & D projects of the National Key Foundation (No.s 2013CB329403, 2012CB316301), the Tsinghua Initiative Scientific Research Program No.20121088071, the Tsinghua National Laboratory of Computer Science and Technology, and the 221 Basic Research Plan for Young Faculties of Tsinghua University."}], "references": [{"title": "Scalable inference in latent variable models", "author": ["Ahmed et al.2012] A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A. Smola"], "venue": "In International Conference on Web Search and Data Mining (WSDM)", "citeRegEx": "Ahmed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2012}, {"title": "Prior elicitation, variable selection and Bayesian computation for logistic regression models", "author": ["M. Chen", "J. Ibrahim", "C. Yiannoutsos"], "venue": "Journal of Royal Statistical Society,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "PAC-Bayesian learning of linear classifiers", "author": ["Germain et al.2009] P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Germain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2009}, {"title": "Exponentiated gradient algorithms for log-linear structured prediction", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": "In ICML,", "citeRegEx": "Globerson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2007}, {"title": "Powergraph: Distributed graph-parallel computation on natural graphs", "author": ["Y. Low", "H. Gu", "D. Bickson", "C. Guestrin"], "venue": "In the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI)", "citeRegEx": "Gonzalez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2012}, {"title": "Finding scientific topics", "author": ["Griffiths", "Steyvers2004] T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of National Academy of Science (PNAS),", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "A comparison of dimensionality reduction techniques for unstructured clinical text", "author": ["Y. Halpern", "S. Horng", "L. Nathanson", "N. Shapiro", "D. Sontag"], "venue": "In ICML 2012 Workshop on Clinical Data Analysis", "citeRegEx": "Halpern et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Halpern et al\\.", "year": 2012}, {"title": "Bayesian auxiliary variable models for binary and multinomial regression", "author": ["Holmes", "Held2006] C. Holmes", "L. Held"], "venue": "Bayesian Analysis,", "citeRegEx": "Holmes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Holmes et al\\.", "year": 2006}, {"title": "Monte Carlo methods for maximum margin supervised topic models", "author": ["Q. Jiang", "J. Zhu", "M. Sun", "E.P. Xing"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Jiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2012}, {"title": "Making largescale SVM learning practical", "author": ["T. Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "DiscLDA: Discriminative learning for dimensionality reduction and classification", "author": ["F. Sha", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lacoste.Jullien et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lacoste.Jullien et al\\.", "year": 2009}, {"title": "A note on margin-based loss functions in classification", "author": ["Y. Lin"], "venue": "Technical Report No. 1044. University of Wisconsin", "citeRegEx": "Lin.,? \\Q2001\\E", "shortCiteRegEx": "Lin.", "year": 2001}, {"title": "PAC-Bayesian stochastic model selection", "author": ["D. McAllester"], "venue": "Machine Learning,", "citeRegEx": "McAllester.,? \\Q2003\\E", "shortCiteRegEx": "McAllester.", "year": 2003}, {"title": "Predictive variable selection in generalized linear models", "author": ["Meyer", "Laud2002] M. Meyer", "P. Laud"], "venue": "Journal of American Statistical Association,", "citeRegEx": "Meyer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Meyer et al\\.", "year": 2002}, {"title": "Distributed algorithms for topic models", "author": ["Newman et al.2009] D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Newman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2009}, {"title": "Bayesian inference for logistic models using Polya-Gamma latent variables. arXiv:1205.0310v1", "author": ["Polson et al.2012] N.G. Polson", "J.G. Scott", "J. Windle"], "venue": null, "citeRegEx": "Polson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Polson et al\\.", "year": 2012}, {"title": "In defense of one-vs-all classification", "author": ["Rifkin", "Klautau2004] R. Rifkin", "A. Klautau"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Rifkin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2004}, {"title": "Are loss functions all the same", "author": ["Rosasco et al.2004] L. Rosasco", "E. De Vito", "A. Caponnetto", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "Rosasco et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosasco et al\\.", "year": 2004}, {"title": "An architecture for parallel topic models", "author": ["Smola", "Narayanamurthy2010] A. Smola", "S. Narayanamurthy"], "venue": "Very Large Data Base (VLDB),", "citeRegEx": "Smola et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2010}, {"title": "The calculation of posterior distributions by data augmentation", "author": ["Tanner", "Wong1987] M.A. Tanner", "W.-H. Wong"], "venue": "Journal of the Americal Statistical Association (JASA),", "citeRegEx": "Tanner et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Tanner et al\\.", "year": 1987}, {"title": "The art of data augmentation", "author": ["van Dyk", "Meng2001] D. van Dyk", "X. Meng"], "venue": "Journal of Computational and Graphical Statistics (JCGS),", "citeRegEx": "Dyk et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dyk et al\\.", "year": 2001}, {"title": "Simultaneous image classification and annotation", "author": ["C. Wang", "D.M. Blei", "Li F.F"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Infinite latent SVM for classification and multi-task learning", "author": ["J. Zhu", "N. Chen", "E.P. Xing"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}, {"title": "MedLDA: maximum margin supervised topic models", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Zhu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2012}, {"title": "Gibbs max-margin topic models with fast sampling algorithms", "author": ["Zhu et al.2013a] J. Zhu", "N. Chen", "H. Perkins", "B. Zhang"], "venue": "In International Conference on Machine Learning (ICML)", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}, {"title": "Bayesian inference with posterior regularization and applications to infinite latent svms", "author": ["Zhu et al.2013b] J. Zhu", "N. Chen", "E.P. Xing"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "As widely adopted in supervised latent Dirichlet allocation (sLDA) models (Blei and McAuliffe, 2010; Wang et al., 2009), one way to improve the predictive power of LDA is to define a likelihood model for the widely available document-level response variables, in addition to the likelihood model for document words.", "startOffset": 74, "endOffset": 119}, {"referenceID": 6, "context": "As noted by (Halpern et al., 2012) and observed in our experiments, this model imbalance could result in a weak influence of response variables on the topic representations and thus non-satisfactory prediction performance.", "startOffset": 12, "endOffset": 34}, {"referenceID": 22, "context": "Technically, instead of doing standard Bayesian inference via Bayes\u2019 rule, which requires a normalized likelihood model, we propose to do regularized Bayesian inference (Zhu et al., 2011; Zhu et al., 2013b) via solving an optimization problem, where the posterior regularization is defined as an expectation of a logistic loss, a surrogate loss of the expected misclassification error; and a regularization parameter is introduced to balance the surrogate classification loss (i.", "startOffset": 169, "endOffset": 206}, {"referenceID": 15, "context": "regression (Polson et al., 2012) to the generalized logistic supervised topic models, which are much more challenging due to the presence of non-trivial latent variables.", "startOffset": 11, "endOffset": 32}, {"referenceID": 4, "context": "We also provide a parallel implementation with GraphLab (Gonzalez et al., 2012), which shows great promise in our preliminary studies.", "startOffset": 56, "endOffset": 79}, {"referenceID": 8, "context": "As noticed in (Jiang et al., 2012), the posterior distribution by Bayes\u2019 rule is equivalent to the solution of an information theoretical optimization problem", "startOffset": 14, "endOffset": 34}, {"referenceID": 12, "context": "\u03b7 from the posterior distribution and makes predictions (McAllester, 2003; Germain et al., 2009).", "startOffset": 56, "endOffset": 96}, {"referenceID": 2, "context": "\u03b7 from the posterior distribution and makes predictions (McAllester, 2003; Germain et al., 2009).", "startOffset": 56, "endOffset": 96}, {"referenceID": 17, "context": "In fact, this choice is motivated from the observation that logistic loss has been widely used as a convex surrogate loss for the misclassification loss (Rosasco et al., 2004) in the task of fully observed binary classification.", "startOffset": 153, "endOffset": 175}, {"referenceID": 6, "context": "This imbalance was noticed in (Halpern et al., 2012).", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "Comparison with MedLDA: The above formulation of logistic supervised topic models as an instance of regularized Bayesian inference provides a direct comparison with the max-margin supervised topic model (MedLDA) (Jiang et al., 2012), which has the same form of the optimization problems.", "startOffset": 212, "endOffset": 232}, {"referenceID": 17, "context": "We note that the relationship between a logistic loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007).", "startOffset": 122, "endOffset": 168}, {"referenceID": 3, "context": "We note that the relationship between a logistic loss and a hinge loss has been discussed extensively in various settings (Rosasco et al., 2004; Globerson et al., 2007).", "startOffset": 122, "endOffset": 168}, {"referenceID": 11, "context": "But the presence of latent variables poses additional challenges in carrying out a formal theoretical analysis of these surrogate losses (Lin, 2001) in the topic model setting.", "startOffset": 137, "endOffset": 148}, {"referenceID": 1, "context": "Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012).", "startOffset": 116, "endOffset": 178}, {"referenceID": 15, "context": "Moreover, the latent variables Z make the inference problem harder than that of Bayesian logistic regression models (Chen et al., 1999; Meyer and Laud, 2002; Polson et al., 2012).", "startOffset": 116, "endOffset": 178}, {"referenceID": 21, "context": "It is easy to show that the variational method (Wang et al., 2009) is a coordinate descent algorithm to solve problem (5) with the additional fully-factorized constraint q(\u03b7,\u0398,Z,\u03a6) = q(\u03b7)( \u220f", "startOffset": 47, "endOffset": 66}, {"referenceID": 21, "context": "Note that the nonBayesian treatment of \u03b7 as unknown parameters in (Wang et al., 2009) results in an EM algorithm, which still needs to make strict mean-field assumptions together with a variational bound of the expectation of the log-logistic likelihood.", "startOffset": 66, "endOffset": 85}, {"referenceID": 15, "context": "Our algorithm represents a first attempt to extend Polson\u2019s approach (Polson et al., 2012) to deal with highly non-trivial Bayesian latent variable models.", "startOffset": 69, "endOffset": 90}, {"referenceID": 15, "context": "Definition 1 (Polson et al., 2012) A random variable X has a Polya-Gamma distribution, denoted by X\u223cPG(a, b), if", "startOffset": 13, "endOffset": 34}, {"referenceID": 15, "context": "Then, using the ideas of data augmentation (Tanner and Wong, 1987; Polson et al., 2012), we can show that the generalized pseudo-likelihood can be expressed as", "startOffset": 43, "endOffset": 87}, {"referenceID": 15, "context": "The equality has been achieved by using the construction definition of the general PG(a, b) class through an exponential tilting of the PG(a, 0) density (Polson et al., 2012).", "startOffset": 153, "endOffset": 174}, {"referenceID": 15, "context": "To draw samples from the Polya-Gamma distribution, we adopt the efficient method2 proposed in (Polson et al., 2012), which draws the samples through drawing samples from the closely related exponentially tilted Jacobi distribution.", "startOffset": 94, "endOffset": 115}, {"referenceID": 23, "context": "We take the approach in (Zhu et al., 2012; Jiang et al., 2012), which uses a point estimate of topics \u03a6 from training data and makes prediction based on them.", "startOffset": 24, "endOffset": 62}, {"referenceID": 8, "context": "We take the approach in (Zhu et al., 2012; Jiang et al., 2012), which uses a point estimate of topics \u03a6 from training data and makes prediction based on them.", "startOffset": 24, "endOffset": 62}, {"referenceID": 23, "context": "We follow the same setting as in (Zhu et al., 2012) and remove a standard list of stop words for both binary and multi-class classification.", "startOffset": 33, "endOffset": 51}, {"referenceID": 10, "context": "Following the same setting in (Lacoste-Jullien et al., 2009; Zhu et al., 2012), the task is to distinguish postings of the newsgroup alt.", "startOffset": 30, "endOffset": 78}, {"referenceID": 23, "context": "Following the same setting in (Lacoste-Jullien et al., 2009; Zhu et al., 2012), the task is to distinguish postings of the newsgroup alt.", "startOffset": 30, "endOffset": 78}, {"referenceID": 21, "context": "We compare the generalized logistic supervised LDA using Gibbs sampling (denoted by gSLDA) with various competitors, including the standard sLDA using variational mean-field methods (denoted by vSLDA) (Wang et al., 2009), the MedLDA model using variational mean-field methods (denoted by vMedLDA) (Zhu et al.", "startOffset": 201, "endOffset": 220}, {"referenceID": 23, "context": ", 2009), the MedLDA model using variational mean-field methods (denoted by vMedLDA) (Zhu et al., 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (denoted by gMedLDA) (Jiang et al.", "startOffset": 84, "endOffset": 102}, {"referenceID": 8, "context": ", 2012), and the MedLDA model using collapsed Gibbs sampling algorithms (denoted by gMedLDA) (Jiang et al., 2012).", "startOffset": 93, "endOffset": 113}, {"referenceID": 9, "context": "For gLDA, we learn a binary linear SVM on its topic representations using SVMLight (Joachims, 1999).", "startOffset": 83, "endOffset": 99}, {"referenceID": 10, "context": "The results of DiscLDA (Lacoste-Jullien et al., 2009) and linear SVM on raw bag-of-words features were reported in (Zhu et al.", "startOffset": 23, "endOffset": 53}, {"referenceID": 23, "context": ", 2009) and linear SVM on raw bag-of-words features were reported in (Zhu et al., 2012).", "startOffset": 69, "endOffset": 87}, {"referenceID": 15, "context": "In fact, this is harder than the multinomial Bayesian logistic regression, which can be done via a coordinate strategy (Polson et al., 2012).", "startOffset": 119, "endOffset": 140}, {"referenceID": 4, "context": "A Parallel Implementation: GraphLab is a graph-based programming framework for parallel computing (Gonzalez et al., 2012).", "startOffset": 98, "endOffset": 121}, {"referenceID": 0, "context": ", using various distributed architectures (Ahmed et al., 2012; Newman et al., 2009; Smola and Narayanamurthy, 2010), to make the sampling algorithm highly scalable to deal with massive data corpora.", "startOffset": 42, "endOffset": 115}, {"referenceID": 14, "context": ", using various distributed architectures (Ahmed et al., 2012; Newman et al., 2009; Smola and Narayanamurthy, 2010), to make the sampling algorithm highly scalable to deal with massive data corpora.", "startOffset": 42, "endOffset": 115}, {"referenceID": 15, "context": "Moreover, the data augmentation technique can be applied to deal with other types of response variables, such as count data with a negative-binomial likelihood (Polson et al., 2012).", "startOffset": 160, "endOffset": 181}], "year": 2013, "abstractText": "Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-field assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate significant improvements on prediction performance and time efficiency.", "creator": "LaTeX with hyperref package"}}}