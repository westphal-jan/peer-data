{"id": "1310.2931", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2013", "title": "Feedback Detection for Live Predictors", "abstract": "A statistical predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. This paper analyzes statistical feedback detection as a causal inference problem, and proposes a local randomization scheme that can be used to detect feedback in real-world problems. We apply our method to a predictive model currently in use at an internet company.", "histories": [["v1", "Thu, 10 Oct 2013 19:57:45 GMT  (115kb,D)", "http://arxiv.org/abs/1310.2931v1", null], ["v2", "Sat, 1 Nov 2014 01:48:35 GMT  (120kb,D)", "http://arxiv.org/abs/1310.2931v2", "Advances in Neural Information Processing Systems (NIPS), 2014"]], "reviews": [], "SUBJECTS": "stat.ME cs.LG stat.ML", "authors": ["stefan wager", "nick chamandy", "omkar muralidharan", "amir najmi"], "accepted": true, "id": "1310.2931"}, "pdf": {"name": "1310.2931.pdf", "metadata": {"source": "CRF", "title": "Feedback Detection for Live Statistical Predictors", "authors": ["Stefan Wager", "Nick Chamandy", "Omkar Muralidharan", "Amir Najmi"], "emails": ["amir}@google.com"], "sections": [{"heading": "1 Introduction", "text": "When statistical predictors are used in a live production environment, feedback loops can become a concern if we only know that we are looking directly. Predictive models are usually matched with training data that has not been influenced by the predictor itself, so most real statistical predictors cannot take into account the impact that they themselves have on their environment. Consider the caricatured example: A search engine wants to train a simple classifier that predicts whether a search result is good or bad. This classifier is trained on historical data and learns that a high click rate (CTR) has a positive association with quality. Problems can arise when the search engine uses the classifier, and begins to label search results that are predicted to be good: promoting the search result can lead to a higher CTR, which in turn leads to higher quality predictions, which then leads us to representing the result even more."}, {"heading": "1.1 Counterfactuals and Causal Inference", "text": "A model suffers from feedback when the predictions it makes today affect the predictions it will make tomorrow. We are therefore interested in discovering a causal relationship between the predictions of today and tomorrow; simply recognizing a correlation is not enough. To discover causal and associative relationships between successive predictions, we need to apply a kind of randomized experiment: in our case, we add a small amount of random noise to our predictions. Since the noise is entirely artificial, we can reasonably ask counterfactual questions of the kind: \"How would tomorrow's predictions have changed if we added more / less noise to today's predictions?\" The noise acts as an independent tool we use to provide feedback. We frame our analysis of a potential outcome that tomorrow's predictions would have changed if we added more / less noise to today's predictions [attack]."}, {"heading": "1.2 Related Work", "text": "The interaction between models and the systems they attempt to describe has been extensively studied in many areas. However, models can have different kinds of feedback effects on their environment. At one end of the spectrum, models can become self-fulfilling prophecies: models that predict economic growth can actually cause economic growth by instilling market confidence [Merton, 1948, Ferraro et al., 2005]. At the other end of the spectrum, models can distort the phenomena they want to describe and thus become invalid. A classic example of this is the concern that any metric used to regulate financial risk could become invalid once it is widely used because actors in the financial market try to manipulate the metric in order to avoid regulation [Dan\u0131elsson, 2002].Much of the work on model feedback in areas such as finance, education or macroeconomic theory has focused on negative outcomes: there is an emphasis on understanding when feedback may interact with political decisions, but it does not seem to be consistent with societal feedback."}, {"heading": "2 Feedback Detection for Statistical Predictors", "text": "In fact, it is not that we see ourselves in a position to recognize the effects of the crisis on the world. (...) It is not that we see ourselves in a position to understand it. (...) It is not that we get a grip on it. (...) It is not that we get a grip on it. (...) It is not that we get a grip on it. (...) It is not that we get a grip on it. (...) It is not that we get a grip on it. (...) It is not that we get a grip on it. (...) It is not that we get a grip on it. (...) It is that it. (...) It is that it. (...) It is that it. (...) It is that it. (...) It is that it. (...) It is not that it. (...) It is that it. (...) It is that it. (... It is that it. (...) It is not that it. (... It is that it. (...) It is that it."}, {"heading": "2.1 Methodology", "text": "If we are able to insert artificial noise into our predictions, as in (3), and are willing to make the additive assumptions (2), then we can adjust the general non-linear feedback functions (2) by performing two carefully designed feedbacks. We move a more thorough theoretical analysis of our methods up to sections 4 and 5; this section shows how we can adjust feedback in practice (t)."}, {"heading": "3 Experiments", "text": "We start with a collection of simulation experiments, the results of which are shown in Figure 1. These examples are all logistic regression examples with additive feedback in the log-odds space. In the diagrams, the y-axis shows feedback in the log-odds space, while the x-axis shows predictions used in the probability space.The simulations all had n = 100, 000 (old prediction, new prediction) pairs; all predictions had natural noise with standard error tolerance = 0.5. We added Gaussian noise to the prediction used with \u03c3\u03bd = 0.25 and adjusted both the trend \u00b5 (\u00b7) and the feedback function f (\u00b7) as the sum of a natural spline with df = 3 degrees of freedom and nodes evenly above [\u2212 3, 3] and a jump at zero log-odds (i.e. x = 0.5). The dashed lines show the different feedback functions that are used in each simulation to replicate our raw data."}, {"heading": "3.1 Real-World Example", "text": "The original motivation for this research was to develop a methodology for detecting feedback in real systems. Here we present the results of a pilot study in which we added signals to historical data that we believe should emulate actual feedback. The model in question is a logistic regression classifier. We added feedback to historical data after half a dozen rules of the form \"if a (t) i is high and y (t) i > 0, then we increase the additive assumptions (2) i by a random amount.\" Here is the time-t prediction used by our system (in log odds space) and a (t) i is a trait with a positive coefficient. These feedback generation rules do not obey additive assumptions (2). Therefore, our model is misspecified in the sense that there is no function f that we give a current prediction (t)."}, {"heading": "4 Linear Feedback", "text": "We begin with an analysis of linear feedback problems (1). (The linear setup allows us to convey the most important findings with less technical effort. (Suppose that the non-linear case in Section 5. Suppose we have a natural process x (1), x (2),... and a predictive model of the form y = w \u00b7 x. (Suppose that x includes the constant and the accepted term is folded into w.) For our purposes, w is fixed and known; for example, w may have been fixed by training on historical data. (At some point, we provide a system that emits the predictions y = w \u2022 x, and there is concern that the act of transmitting the y feedback might affect the underlying x (t) process. Our goal is to detect such feedback."}, {"heading": "4.1 A Simple Regression Approach", "text": "With the linear feedback model (8), the effect of \u03bd (t) i on y (t + 1) i (technical requirement (t + 1) i [t) i + \u03bd (t) i = y (t + 1) i [y] i [y] i [y] i (t) i (s) i (t) i (t) i (9) This relation suggests that we should be able to \u03b2 by returning y (t + 1) i (t) i (t) i (t) i i (t) i i i (t) i i i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) t (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i) i (t) i (t) i (t) i) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i) i (t (t) i) i (t (t) i) i (t) i (t (t) i) i (t (t) i) i (t (t) i (t (t) i) i (t (t) i) i (t (t (t) i) i (t (t (t) i) i (t (t) i) i (t (t) i) i (t (t (t) i) i (t (t (t (t) i) i (t (t (t) i (t (t) i (t) i) i (t (t) i (t (t) i (t) i (t) i (t (t) i (t (t) i (t) i (t) i (t) i (t (t) i (t) i (t) i (t (t (t) i (t (t) i) i (t (t) i (t (t) i (t (t) i (t) i (t)"}, {"heading": "4.2 Efficiency and Conditioning", "text": "The simple regression model (10) treats the term y (t + 1) i [\u03b2 \u03b2 (t) i) i [\u03b2 \u03b2 (t) i] as noise. This is rather wasteful: if we know that y (t) i [t) i have a pretty good idea of what y (t) i [t + 1) i [t) i [t) i [t) i [t) i [t) i [t) i [t) i [t) i [t) i [t) i [t) i (t) i i) i (t) i i i (t) i i (t) i (t) i (t) i (t) i (t) i (t) i i (t) i) (b) i (i) (b) i (i) (b) i (i) i (i) (b) i) (b) i (i) i (i) (b) i (i) i (i) (b) i (i) i (i) (i) i (t) b (t) i) i (t) i (t) i) (t) i (t) i) i (t) i (t) i) i (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (i) (t) b (b) b (b) b) b (t) b (t) b (t) i (t) b (t) i) b (t) b (t) b (t) b (t) i) b (i) b (i) (i) b (i) (i) b (i) (i) (i) (i) (i) (i) (i (i) (i) (i) (i) (i) (i) b (i) (i) b (i) b (i) b (i) b (i) b (i) b (i) b (i) b (i) b (i) b (i) b (i) b (i)"}, {"heading": "5 Fitting Non-Linear Feedback", "text": "Let us now assume that we have the same attitude as in the previous section, except that feedback now has a non-linear dependence on y (independent), and we replace (7) with x (t + 1) i [y] (t) i (t) i (t) i (t) i (t) i (t) i (x) i (y) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i (t) i) i (t) i (t) i) i (t) i) i (t) i i (t) i i i (i) i i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (t) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (t) i (i) i (i) i (i (i) i (i) i) i (i (i) i (i) i (i) i (i) i (i (i) i) i (i (i) i) i (i) i (i (i) i) i (i) i (t) i (i) i (i) i (i) i (i (i) i (i) i (t) i) i (i) i (t) i) i (t) i (i) i (i) i (i) i (t) i (t) i (i (t) i) i (i (i) i (i) i (i) i (i) i (i) i"}, {"heading": "5.1 A Pragmatic Approach", "text": "There are many possible approaches to solving the non-parametric system of equations (20) for f [e.g., Hastie et al., 2009, Chapter 5]. Here we take a pragmatic approach and limit ourselves to solutions of formal (y) = \u03b2 (y) \u00b7 b\u00b5 (y) and f (y) (\u03b2) = \u03b2 f \u00b7 bf (y), (22) where b\u00b5: R \u2192 Rp\u00b5 and bf: R \u2192 Rpf are predetermined basic extensions; the basis bf should not contain an interceptor term. This approach transforms our problem into a common problem of the smallest squares and works well in terms of generating reasonable feedback problems in the real world. In Section 6.3 we propose a more general approach as a topic for further research. Assuming that (22) does indeed apply to some values \u03b2\u00b5 and \u03b2f, the result below shows that we can restore \u00df by using the least equated regression."}, {"heading": "6 Extensions and Further Work", "text": "In this section we discuss some possible extensions of the work presented in this paper."}, {"heading": "6.1 Feedback Removal", "text": "When we detect feedback in a real system, we can try to identify the root causes of feedback and fix the problem by removing the feedback loop. Nevertheless, a natural follow-up question to our research is whether we can automatically remove feedback. In the context of the linear feedback model (8), we suffer an expected square error loss of \u00b2 E [(y (t) i) 2] + \u03c32\u03bd by completely ignoring the feedback problem. Meanwhile, if we use the maximum probability estimate \u03b2 to correct feedback, we suffer a loss of \u00b2 E [(y (t) i) 2] + \u03c32\u03bd, the first term being based on our errors in estimating \u03b2 \u00b2 and the second on the additional noise we had to inject into the system to correct feedback. An interesting topic for further research would be to find out how we can optimally adjust the scale of artificial noise under different assumptions and understand the potential feedback model so that we can remove the feedback under certain conditions."}, {"heading": "6.2 Covariate-Dependent Feedback", "text": "Our analysis was presented in the context of additive feedback, which is modular (t + 1) i [y] (t) i [y] (t + 1) i [\u2205] + f (y) (t) i).In practice, however, we may want to make feedback dependent on some other covariants, which zy (t + 1) i [y] i [y] (t + 1) i [\u2205] + f (y) i (t) i, z (t) i); for example, we might want to divide feedback by geographical region. A particularly interesting but challenging extension would be to make feedback dependent on the undisturbed prediction y (t) i [\u2205]: y (t + 1) i [y) i [t) i [y] (t + 1) i [\u2205] i + f (y) i (t) i (t) i).For example, if y [c] is a prediction of how good a search result is so that we can assume that feedback (t + 1) is actually a challenge (t)."}, {"heading": "6.3 Penalized Regression", "text": "The most important technical challenge in implementing our feedback detection method is solving the spline equation (20). In Section 5.1, we have proposed a pragmatic approach that allows us to obtain good feedback estimates in many examples. However, it should be possible to use more general methods for adapting to f. Equation (20) is linear in f, and therefore any strictly convex tightening function L: A \u2192 R leads via some convex subsets A {R \u2192 R} of real value functions on R to a well-defined estimator f \u00b2 by the convex optimization problem, f \u00b2 L = argminf \u00b2 (y (t + 1) i [y (t) i (t) i \u2212 \u00b5 (y) i \u2212 f (t) i \u2212 f (t) i \u2212 f (y) i (t) i \u2212 p (y (t) i) i \u2212 p (y) i (y) i \u2212 p (y) i (t) i)."}, {"heading": "7 Conclusion", "text": "Our method is to add noise to the predictions of the system; this noise puts us in a randomized trial setup that allows us to measure feedback as a causal effect. In general, the amount of artificial noise required to capture feedback is smaller than that of natural predictor noise; therefore, we can use our feedback detection method without too much disruption to our interesting system. It does not require hypotheses about the mechanism by which feedback can spread; therefore, it can be used to continuously monitor statistical systems and warn us when changes to the system lead to an increase in feedback."}, {"heading": "Acknowledgements", "text": "The authors thank Alex Blocker, Randall Lewis and Brad Efron for helpful suggestions and interesting conversations."}, {"heading": "A Fitting Non-Linear Feedback by Ordinary Least Squares", "text": "Regression The adjustment process outlined in Section 2.1 is simple using standard R functions when we are ready to perform \"p\" (\u00b7) and \"f\" (\u00b7) using default base expansions (y) = \u03b2 \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" x \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"\" \"p\" p \"\" p \"\" p \"p\" p. \"\" \"\" p \"\" p \"\" p \"\" p \"p.\" \"\" \"\" p \"\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p. \"p\" p. \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" \"p\" p \"p\" p \"p\" p \"p\" p \"\" p \""}], "references": [{"title": "On the use of a linear model for the identification of feedback systems", "author": ["Hirotugu Akaike"], "venue": "Annals of the Institute of statistical mathematics,", "citeRegEx": "Akaike.,? \\Q1968\\E", "shortCiteRegEx": "Akaike.", "year": 1968}, {"title": "Compliance as an explanatory variable in clinical trials", "author": ["Bradley Efron", "David Feldman"], "venue": null, "citeRegEx": "Efron and Feldman.,? \\Q2002\\E", "shortCiteRegEx": "Efron and Feldman.", "year": 2002}, {"title": "Regularization networks and", "author": ["press", "1993. Theodoros Evgeniou", "Massimiliano Pontil", "Tomaso Poggio"], "venue": null, "citeRegEx": "press et al\\.,? \\Q1993\\E", "shortCiteRegEx": "press et al\\.", "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "Akaike [1968] showed how to fit cross-component feedback in a system with many components; however, he did not add artificial noise to the system, and so was unable to detect feedback of a single component on itself.", "startOffset": 0, "endOffset": 14}], "year": 2017, "abstractText": "A statistical predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. This paper analyzes statistical feedback detection as a causal inference problem, and proposes a local randomization scheme that can be used to detect feedback in real-world problems. We apply our method to a predictive model currently in use at an internet company.", "creator": "LaTeX with hyperref package"}}}