{"id": "1310.4210", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2013", "title": "Demystifying Information-Theoretic Clustering", "abstract": "We propose a novel method for clustering data which is grounded in information-theoretic principles and requires no parametric assumptions. Previous attempts to use information theory to define clusters in an assumption-free way are based on maximizing mutual information between data and cluster labels. We demonstrate that this intuition suffers from a fundamental conceptual flaw that causes clustering performance to deteriorate as the amount of data increases. Instead, we return to the axiomatic foundations of information theory to define a meaningful clustering measure based on the notion of consistency under coarse-graining for finite data.", "histories": [["v1", "Tue, 15 Oct 2013 21:19:22 GMT  (1079kb,D)", "https://arxiv.org/abs/1310.4210v1", "12 pages, 8 figures"], ["v2", "Wed, 5 Feb 2014 22:21:06 GMT  (3462kb,D)", "http://arxiv.org/abs/1310.4210v2", "Proceedings of The 31st International Conference on Machine Learning (ICML), 2014. 11 pages, 9 figures"]], "COMMENTS": "12 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT physics.data-an stat.ML", "authors": ["greg ver steeg", "aram galstyan", "fei sha", "simon dedeo"], "accepted": true, "id": "1310.4210"}, "pdf": {"name": "1310.4210.pdf", "metadata": {"source": "META", "title": "Demystifying Information-Theoretic Clustering", "authors": ["Greg Ver Steeg", "Aram Galstyan", "Fei Sha", "Simon DeDeo"], "emails": ["GREGV@ISI.EDU", "GALSTYAN@ISI.EDU", "FEISHA@USC.EDU", "SIMON.DEDEO@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to obey the rules which they have imposed on themselves. (...) Indeed, it is so that they do not do it. (...) It is not so that they do it. (...) It is as if they do not do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...). (...). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(. (.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).).\" (.). \"(.).\" (.).). \"(.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.\" (.). \"(.).).\" (.).). \"(.\" (. \"(.).).\" (.).). \"(.).\" (. \"(.\" (.).). \"(.\" (. \"(.).).\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).\" (. \"(.).\" (.).). \"(.).\" (. \"(.).).\" (.).). \"(.\" (.).). \"(."}, {"heading": "2. Information-theoretic clustering and its pitfalls", "text": "Given samples taken from a known distribution, the Shannon entropy of distribution can be interpreted as the minimum number of bits required to encode each bit (on average). To form clusters, we obtain samples of an unknown distribution, and we want to label (encode) each sample so that it reflects a natural structure. Even if we knew the Shannon entropy of distribution, code that achieves this optimal compression does not necessarily reflect the natural structure of the underlying distribution."}, {"heading": "2.1. Basic concepts and entropy estimation", "text": "We start with the generic clustering problem, in which some samples x (i) - Rd for i = 1,.., N, drawn from some unknown distribution, p (x). The goal is to associate us with a discrete designation, y (i) - {0, 1} (for simplicity we use binary designations only), which somehow reflect a natural clustering of the data. Of course, the main difficulty is to define what qualifies as natural clustering. Below, we will consider various information-theoretical criteria. Entropy is usually used as H (X) = E [\u2212 log p (x)] -D, so that information is measured in bits. Using standard notation, uppercase X denotes a random variable whose spacing in lowercase we name, and the fact that entropy is a functional of the probability distribution is written explicitly only when it requires it. The expected value can be a sum of different cases, we can reconstruct or reconstruct in different cases."}, {"heading": "2.2. Pitfalls", "text": "In the simplest representation of clustering for continuous variables, we have shown two uniform distribution possibilities. If we are to maximize the mutual information between the data and cluster markers, this is not the case. (However, there are purely information-theoretical criteria that arbitrarily reorder the bins of this discrete probability distribution so that there is no gap, and this rearrangement has no effect on any information-theoretical quantities, because they depend only on the values pi. While no one would attempt to unify these categorical variables without defining a relationship between the variables, the same problem is articulated in a more subtle form for continuous distribution.In the simplest representation of clustering for continuous variables, we have shown two uniform distribution possibilities."}, {"heading": "3. Proposed approach", "text": "We found that earlier information theory cluster systems only work randomly in that they miscalculate the actual uncertainty of cluster labels near cluster boundaries due to finite data. This behavior is closely linked to the axioms of information theory. We briefly establish this link before deriving a simple expression that explicitly takes advantage of uncertainty near cluster boundaries due to limited data."}, {"heading": "3.1. Shannon\u2019s Axiom of Consistency under Coarse-Graining", "text": "For the first time in a long time, we have been able to offer a wide range of products for the green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-"}, {"heading": "3.2. Using conditional entropy for clustering", "text": "We can first derive a more convincing form for Eq = 1 in sec. (1) For each subsequent point (1) at which we represent the distance to the k-nn neighbor, while we take the distance to the k-nn points in the same cluster as an example. (3) As long as the closest neighbors are within the same cluster, the consistency violation or estimated uncertainty about the cluster label will be zero, as in Fig. 3), where we can imagine k = 3. Total cluster label uncertainty for each fixed division of the room, CV approaches for large N. We want our cluster label to be small, even under limited data, so that we can imagine that we have any fixed division of the space, CV approaches for large N."}, {"heading": "4. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Synthetic datasets", "text": "Our goal is to find clusters determined by y (i) that optimize the target in Eq.7. Let's remember the simple example in Fig. 2, where mutual information did not cluster correctly for capital N. For data from the simple one-4For k = 1, the log distance of the nearest neighbor is weighted by a factor of 1 / (l + 1). For comparison, the NIC target (Faivishevsky & Goldberger, 2010) rates all log distances the same. See Fig. 5 for a more detailed comparison. The three-dimensional distribution in Fig. 2 shows that in contrast to the ratio H-T (Y | X) / H-X (Y) (CVR briefly from here on) for all possible ways to partition the x-axis, shown in Fig. 4, we discard the three-dimensional coarseness of the clusters if all points are in the same group, which has an undefined CVR of 0 / 0."}, {"heading": "4.2. Real-world datasets", "text": "In Fig. 8, we look at the behavior of users who frequently edit the Wikipedia page for George W. Bush (DeDeDeDeo, 2012). In Wikipedia, users can challenge the viewpoint of each article by editing the text directly. If a change is made by a user, any other user can choose to reject the change by reverting to a previous version. Certain types of users are likely to get into turf wars or, conversely, engage in a pro-social vandalism repair, so that a large portion of their activity consists of \"relapses.\" We look at a fraction of the activities of a user who does not consist of reverting to previous versions, cf. for the 50 most active users on George W. Bush's Wikipedia page, we draw in Fig. 8. Both CVR and Bayesian latent variable models such as k-discover the same natural cluster structure when used to be binary partition.UCI datasets We think three UCaseting clusters from the VR VR I data bank are not comparable."}, {"heading": "5. Discussion", "text": "This year, we will be able to put ourselves in a position to take the lead."}, {"heading": "ACKNOWLEDGMENTS", "text": "A.G. and G.V. were partially supported by the AFOSR MURI Fellowship FA9550-10-1-0569 and the DTRA Fellowship HDTRA1-10-1-0086. F.S. is supported by a research grant from the Alfred P. Sloan Foundation. S.D. appreciates the support of the National Science Foundation Fellowship EF-1137929.6Although we have achieved our goal of avoiding dependence on a global measure of similarity, nonparametric entropy estimators need an idea of adjacency to find data points closest to a given metric. A big advantage of the information theory approach is that H (Y | X) is invariant among smooth, invertable transformations of the data (X). Therefore, any asymptotically unbiased estimator should converge to the same value regardless of the metric used to search for nearest neighbors."}, {"heading": "A. Derivation of Eq. 5", "text": "We have samples (i), y (i), with x-j = j = j-j (j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j = j-j (j-j). We define this estimator with reference to existing entropy estimators, H (Y) + H-N (X-Y) \u2212 N-N (X-N) \u2212 n-log-j (Y). To write down the standard entropy estimator for discrete entropy, we first define nj-y-y (i), j."}, {"heading": "B. Heuristic Optimization", "text": "The goal is to find a natural coarseness as defined in Eq. 7. The number of ways to divide N points into groups is very large, and evaluating the CV for each partition requires calculating all pairwise distances. Even calculating the change in the CV from the change in cluster membership of a single point may require O (N) operations. Finally, the numbers in Sec. 4 indicate that our optimization landscape is very robust, so gradient-based methods are unlikely to succeed. Due to these difficulties, we propose heuristic optimization. Our optimization starts by generating a small number of partition candidates that probably have a small CV with a tractable semidefinitive program. Then, we rank the candidates by CVR.H T (Y | X) = 1 delle H (Y | X) = 1 delle H (Y | X) = d N \u2212 1 N \u00b2 i = 1 kN."}], "references": [{"title": "Weighted clustering", "author": ["Ackerman", "Margareta", "Ben-David", "Shai", "Branzei", "Simina", "Loker", "David"], "venue": "In AAAI,", "citeRegEx": "Ackerman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ackerman et al\\.", "year": 2012}, {"title": "Robust information-theoretic clustering", "author": ["B\u00f6hm", "Christian", "Faloutsos", "Christos", "Pan", "Jia-Yu", "Plant", "Claudia"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "B\u00f6hm et al\\.,? \\Q2006\\E", "shortCiteRegEx": "B\u00f6hm et al\\.", "year": 2006}, {"title": "Collective phenomena and nonfinite state computation in a human social system", "author": ["DeDeo", "Simon"], "venue": "doi: 10.1371/journal.pone. 0075818", "citeRegEx": "DeDeo and Simon.,? \\Q2012\\E", "shortCiteRegEx": "DeDeo and Simon.", "year": 2012}, {"title": "Bootstrap methods for the empirical study of decision-making and information flows in social systems", "author": ["DeDeo", "Simon", "Hawkins", "Robert X. D", "Klingenstein", "Sara", "Hitchcock", "Tim"], "venue": "Entropy, 15(6):2246\u20132276,", "citeRegEx": "DeDeo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "DeDeo et al\\.", "year": 2013}, {"title": "Information theoretic clustering of sparse cooccurrence data", "author": ["Dhillon", "Inderjit S", "Guan", "Yuqiang"], "venue": "In Data Mining,", "citeRegEx": "Dhillon et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2003}, {"title": "Ica based on a smooth estimation of the differential entropy", "author": ["Faivishevsky", "Lev", "Goldberger", "Jacob"], "venue": "In NIPS,", "citeRegEx": "Faivishevsky et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Faivishevsky et al\\.", "year": 2008}, {"title": "A nonparametric information theoretic clustering algorithm", "author": ["Faivishevsky", "Lev", "Goldberger", "Jacob"], "venue": "In Proceedings of ICML,", "citeRegEx": "Faivishevsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Faivishevsky et al\\.", "year": 2010}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "J. Assoc. Comput. Mach.,", "citeRegEx": "Goemans and Williamson,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson", "year": 1995}, {"title": "Information theoretic clustering", "author": ["Gokcay", "Erhan", "Principe", "Jose C"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Gokcay et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gokcay et al\\.", "year": 2002}, {"title": "Data clustering: 50 years beyond k-means", "author": ["Jain", "Anil K"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Jain and K.,? \\Q2010\\E", "shortCiteRegEx": "Jain and K.", "year": 2010}, {"title": "Sample estimate of the entropy of a random vector", "author": ["L.F. Kozachenko", "N.N. Leonenko"], "venue": "Probl. Peredachi Inf.,", "citeRegEx": "Kozachenko and Leonenko,? \\Q1987\\E", "shortCiteRegEx": "Kozachenko and Leonenko", "year": 1987}, {"title": "Estimating mutual information", "author": ["Kraskov", "Alexander", "St\u00f6gbauer", "Harald", "Grassberger", "Peter"], "venue": "Phys. Rev. E,", "citeRegEx": "Kraskov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kraskov et al\\.", "year": 2004}, {"title": "Information theoretic clustering using minimum spanning trees", "author": ["M\u00fcller", "Andreas", "Nowozin", "Sebastian", "Lampert", "Christoph"], "venue": "Pattern Recognition,", "citeRegEx": "M\u00fcller et al\\.,? \\Q2012\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2012}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["Rand", "William M"], "venue": "Journal of the American Statistical association,", "citeRegEx": "Rand and M.,? \\Q1971\\E", "shortCiteRegEx": "Rand and M.", "year": 1971}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal,", "citeRegEx": "Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "Information-based clustering", "author": ["Slonim", "Noam", "Atwal", "Gurinder Singh", "Tka\u010dik", "Ga\u0161per", "Bialek", "William"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Slonim et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Slonim et al\\.", "year": 2005}, {"title": "Information-maximization clustering based on squared-loss mutual information", "author": ["Sugiyama", "Masashi", "Yamada", "Makoto", "Kimura", "Manabu", "Hachiya", "Hirotaka"], "venue": "arXiv preprint arXiv:1112.0611,", "citeRegEx": "Sugiyama et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2011}, {"title": "Information theoretical clustering via semidefinite programming", "author": ["Wang", "Meihong", "Sha", "Fei"], "venue": "AISTATS,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Universal estimation of information measures for analog sources", "author": ["Wang", "Qing", "Kulkarni", "Sanjeev R", "Verd\u00fa", "Sergio"], "venue": "Found. Trends Commun. Inf. Theory,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": "Ideally, we would like to achieve those goals without explicitly defining some notion of similarity between data points (Slonim et al., 2005) or defining \u201cprototypical\u201d clusters (B\u00f6hm et al.", "startOffset": 120, "endOffset": 141}, {"referenceID": 1, "context": ", 2005) or defining \u201cprototypical\u201d clusters (B\u00f6hm et al., 2006).", "startOffset": 44, "endOffset": 63}, {"referenceID": 12, "context": "There has been a growing interest in information-theoretic clustering where we assign cluster labels to data points such that the mutual information between data and labels is maximized (Faivishevsky & Goldberger, 2010; Wang & Sha, 2011; M\u00fcller et al., 2012; Sugiyama et al., 2011).", "startOffset": 186, "endOffset": 281}, {"referenceID": 16, "context": "There has been a growing interest in information-theoretic clustering where we assign cluster labels to data points such that the mutual information between data and labels is maximized (Faivishevsky & Goldberger, 2010; Wang & Sha, 2011; M\u00fcller et al., 2012; Sugiyama et al., 2011).", "startOffset": 186, "endOffset": 281}, {"referenceID": 11, "context": "It can also be estimated non-parametrically from the data samples without defining a parametric space of probability distributions (Kraskov et al., 2004).", "startOffset": 131, "endOffset": 153}, {"referenceID": 14, "context": "We motivate our approach by appeal to the axiom of consistency under coarsegraining that forms the definition of entropy (Shannon, 1948).", "startOffset": 121, "endOffset": 136}, {"referenceID": 11, "context": "(Kraskov et al., 2004).", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "The estimator has also been empirically shown to have good performance for small amounts of data, with k = 3 representing a good choice (Kraskov et al., 2004; Khan et al., 2007).", "startOffset": 136, "endOffset": 177}, {"referenceID": 3, "context": "discuss more nuanced alternatives in the discrete case (DeDeo et al., 2013).", "startOffset": 55, "endOffset": 75}, {"referenceID": 12, "context": "How have so many papers achieved good clustering performance using this criteria (Wang & Sha, 2011; Faivishevsky & Goldberger, 2010; M\u00fcller et al., 2012)? To understand this result, it helps to write mutual information in the form I(X;Y ) = H(Y )\u2212H(Y |X).", "startOffset": 81, "endOffset": 153}, {"referenceID": 14, "context": "Shannon\u2019s Axiom of Consistency under Coarse-Graining Shannon\u2019s original derivation of entropy (Shannon, 1948) begins with properties that a measure of uncertainty should be expected to obey and concludes that (Shannon) entropy is the unique measure that satisfies these properties.", "startOffset": 94, "endOffset": 109}, {"referenceID": 3, "context": "make the further point that any estimator of entropy should at least approximately satisfy this property or it risks losing its meaning as an information-theoretic measure altogether (DeDeo et al., 2013).", "startOffset": 183, "endOffset": 203}, {"referenceID": 11, "context": "Note that the mutual information estimator we use is asymptotically unbiased (Kraskov et al., 2004), and so we expect a similar result for any mutual information estimator.", "startOffset": 77, "endOffset": 99}], "year": 2014, "abstractText": "Greg Ver Steeg GREGV@ISI.EDU Aram Galstyan GALSTYAN@ISI.EDU Fei Sha FEISHA@USC.EDU Simon DeDeo SIMON.DEDEO@GMAIL.COM 1 Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292, USA 2 University of Southern California, Los Angeles, CA 90089, USA 3 Santa Fe Institute, 1399 Hyde Park Rd., Santa Fe, NM 87501, USA 4 School of Informatics and Computing, Indiana University, 901 E 10th St., Bloomington, IN 47408, USA", "creator": "LaTeX with hyperref package"}}}