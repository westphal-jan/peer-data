{"id": "1310.4227", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2013", "title": "On Measure Concentration of Random Maximum A-Posteriori Perturbations", "abstract": "The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models. By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution. The measure concentration result is of general interest and may be applicable to other areas involving expected estimations.", "histories": [["v1", "Tue, 15 Oct 2013 23:30:52 GMT  (64kb,D)", "http://arxiv.org/abs/1310.4227v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.PR", "authors": ["francesco orabona", "tamir hazan", "anand d sarwate", "tommi s jaakkola"], "accepted": true, "id": "1310.4227"}, "pdf": {"name": "1310.4227.pdf", "metadata": {"source": "CRF", "title": "On Measure Concentration of Random Maximum A-Posteriori Perturbations", "authors": ["Francesco Orabona", "Tamir Hazan", "Anand D. Sarwate", "Tommi Jaakkola"], "emails": ["orabona@ttic.edu.", "tamir.hazan@gmail.com.", "asarwate@ttic.edu.", "tommi@csail.mit.edu."], "sections": [{"heading": "1 Introduction", "text": "Examples are scene understanding [Felzenszwalb and Zabih, 2011], parsing [Koo et al., 2010], and protein design [Sontag et al., 2008]. These settings aim to find probable structures that match the data, such as objects in images, parsers in sentences, or molecular configurations in proteins. Each structure corresponds to an assignment of values to random variables and the likelihood of an assignment based on the definition of potential functions that take into account interactions via these variables. Given the observed data, this probability distribution results in assignments known as Gibbs distribution. Contemporary practice gives rise to posterior probabilities that take into account a potential influence of the data on the variables of the model (high signal), as well as human knowledge of the potential interactions between these variables."}, {"heading": "2 Problem statement", "text": "Notation: Bold font denotes tuples or vectors and font sets. For a tupel x = (x1, x2,..., xn) leave xj: k = (xj, xj + 1,..., xk)."}, {"heading": "2.1 The MAP perturbation framework", "text": "Statistical conclusions include considerations about the states of discrete variables, whose configurations (assignments of values) are often difficult because the sum (assignments of values) determines the discrete structures of interest. Suppose that our model has n variables x = (x1, x2,.., xn), where each xi contains values in a discrete set of Xi. Leave X = X1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Xn so that x \u00b2 x x x x x. Leave Dom (\u03b8) X is a subset of possible configurations and the problem: X \u2192 R is a potential function that contains a score for an assignment or structure x, where \u03b8 (x) = \u2212 \u221e x for x / o Dom (\u03b8). The potential function induces a probability distribution to configurations x via the Gibbs distribution: p (x)."}, {"heading": "2.2 Sampling from the Gibbs distribution using low dimensional perturbations", "text": "Sampling from the Gibbs distribution is inherently tied to estimating the partition function in (2). If we could compute Z exactly, then we could sample x1 with probability proportional to \u2211 x2,..., xn exp (Xi)), and for each subsequent dimension i, sample xi with probative to \u2211 xi + 1,..., xn exp (x), rendering a Gibbs sampler. However, this includes computing the partitionfunction, which is hard. Instead, Hazan et al. [2013] use the representation in (6) to derive a family of self-reducible upper bounds on Z and then use these upper bounds in an iterative algorithm that samples from the Gibbs distribution using low dimensional random MAP perturbations. This gives a method which has complexity linear in n.In the following, instead of the | X independent random variables in (4), we define the random function (x)."}, {"heading": "2.3 Measure concentration", "text": "(...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (... (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) () (...) (...) (...) (...) (...) (...) (...) (...) ("}, {"heading": "2.4 Related work", "text": "We are interested in an efficient sampling of the Gibbs distribution in (1) when n is large and the model is complex due to the amount of data and domain-specific modeling. This often happens with MCMC (cf. Koller and Friedman [2009]), which can be challenging in fragmented probability landscapes. MAP disturbances use efficient MAP solvers as a black box, but the statistical properties of solutions that go beyond Theorem 1 are still being investigated. Papandreou and Yuille [2011] consider probability models defined by the maximum argument of randomly distorted potential function, while Tarlow et al. [2012] consider the sampling techniques for such models, and Keshet et al. [2011] examine the generalization limits for such models. Instead of focusing on the statistics of the solution (the argmaxx), we examine the statistical properties of the MAP value (the maxx values) of the AP estimate in Hazola and Jaola, we include the MAP random functions in the MAP function (2012)."}, {"heading": "3 Concentration of measure", "text": "In this section we present the most important technical results of this paper - a new Poincare inequality for log-concave distributions and the associated measurement result. We then specialize our result in the Gumbel distribution and apply it to the MAP error system. Due to the tensorizing property of functional entropy, it is sufficient for our case to demonstrate an inequality such as (12) for functions f of a single random variable with measure \u00b5."}, {"heading": "3.1 A Poincare\u0301 inequality for log-concave distributions", "text": "Our Theorem 2 in this section generalizes a faulty result of Brascamp and Lieb (Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q (Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "3.2 Measure concentration for the Gumbel distribution", "text": "In MAP perturbations such as those in (7), we have a function of many random variables (F). We now derive a result based on corollary 1 to bind the moment-generating function for random variables defined as a function of m random variables. This results in a measurement imbalance for the product measurement on Rm, where \u00b5 corresponds to a scalar random variable. Furthermore, we assume that for y = (y1,.) the Gumbel measurement on R and F: Rm \u2192 R can be a function we have almost everywhere."}, {"heading": "3.3 Application to MAP perturbations", "text": "To apply these results to the MAP error problem, we must calculate the parameters in the order of magnitude indicated by the corollary line. (Let F (Let) be the random MAP error as defined in (8). (Let F (Let) be the random MAP error as defined in (8). (Let) The (sub) gradient of this function is structured and points to the random side i (xi), which refers to the maximizing mapping in x-R \u2212 MAP 0 otherwise. (Let this be defined in (4), if \u03b3 (x) = random deviation n i = 1 (xi), i.e. to the random p (Let), which refers to the maximizing mapping in x-R \u2212 MAP 0 otherwise. The gradient is thus the satisfaction in x-R \u2212 AP is defined in x (xi), i.e., if xi (xi) is defined in x, xi i = xi."}, {"heading": "4 Experiments", "text": "We evaluated our approach using a model with 100 x 100 spin glasses with n = 104 variables, for which we expect an average concentration (x1,..., xn) = x x V \u03b8i (xi) + x (i, j) x E \u03b8i, j (xi, xj).where the sample actually has a local field parameter \u03b8i (xi) = \u03b8ixi and interacts with couplings \u03b8i, j (xi, xj) = \u03b8i, jxixj in a grid graphical structure. Whenever the coupling parameters are positive, the model is considered attractive because adjacent variables provide higher values for positively correlated configurations. We used low-dimensional random perturbations \u03b3 (x) = x i = 1 \u03b3i (xi).The local field parameters \u041ai were drawn with random variables from [\u2212 1, 1] M models to reflect high values."}, {"heading": "5 Conclusion", "text": "Sampling from the Gibbs distribution is important because it helps find near-maxima in posterior probability landscapes which are typical in the high-dimensional complex models. These landscapes are due to domain-specific modeling (coupling) and the influence of data (signal), making MCMC challenge. In contrast, sampling based on MAP perturbations ignores the rugged landscape as it directly targets the most plausible structures. In this paper, we characterized the statistics of MAP perturbations. In order to apply low-dimensional MAP perturbation technology in practice, we need to estimate the expected value of quantities Vj among the perturbations. We derived highly probable estimates of these expectations, which allow estimation with arbitrary precision. To do this, we have used more general results to measure the concentration of Gumbel random variables and a point inequality among the perturbations. We derived highly probable estimates of these expectations, which allow estimation with arbitrary precision."}], "references": [{"title": "Weighted sums of certain dependent random variables", "author": ["K. Azuma"], "venue": "To\u0302hoku Mathematical Journal,", "citeRegEx": "Azuma.,? \\Q1967\\E", "shortCiteRegEx": "Azuma.", "year": 1967}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "JMLR, 3:463\u2013482,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2003}, {"title": "The Theory of Probabilities", "author": ["S. Bernstein"], "venue": "Gastehizdat Publishing House, Moscow,", "citeRegEx": "Bernstein.,? \\Q1946\\E", "shortCiteRegEx": "Bernstein.", "year": 1946}, {"title": "Poincar\u00e9\u2019s inequalities and Talagrand\u2019s concentration phenomenon for the exponential measure", "author": ["S. Bobkov", "M. Ledoux"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Bobkov and Ledoux.,? \\Q1997\\E", "shortCiteRegEx": "Bobkov and Ledoux.", "year": 1997}, {"title": "Concentration inequalities", "author": ["S. Boucheron", "G. Lugosi", "O. Bousquet"], "venue": "In Advanced Lectures on Machine Learning,", "citeRegEx": "Boucheron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2004}, {"title": "Concentration inequalities for sub-additive functions using the entropy method. In Stochastic inequalities and applications, pages 213\u2013247", "author": ["O. Bousquet"], "venue": null, "citeRegEx": "Bousquet.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet.", "year": 2003}, {"title": "On extensions of the Brunn-Minkowski and Pr\u00e9kopa-Leindler theorems, including inequalities for log concave functions, and with an application to the diffusion equation", "author": ["H.J. Brascamp", "E.H. Lieb"], "venue": "J. Func. Analysis,", "citeRegEx": "Brascamp and Lieb.,? \\Q1976\\E", "shortCiteRegEx": "Brascamp and Lieb.", "year": 1976}, {"title": "Dynamic programming and graph algorithms in computer vision", "author": ["P.F. Felzenszwalb", "R. Zabih"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "Felzenszwalb and Zabih.,? \\Q2011\\E", "shortCiteRegEx": "Felzenszwalb and Zabih.", "year": 2011}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["E.J. Gumbel", "J. Lieblein"], "venue": "Number 33 in National Bureau of Standards Applied Mathematics Series. US Govt. Print. Office,", "citeRegEx": "Gumbel and Lieblein.,? \\Q1954\\E", "shortCiteRegEx": "Gumbel and Lieblein.", "year": 1954}, {"title": "On the partition function and random maximum a-posteriori perturbations", "author": ["T. Hazan", "T. Jaakkola"], "venue": "In ICML,", "citeRegEx": "Hazan and Jaakkola.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Jaakkola.", "year": 2012}, {"title": "On sampling from the Gibbs distribution with random maximum a-posteriori perturbations", "author": ["T. Hazan", "S. Maji", "T. Jaakkola"], "venue": null, "citeRegEx": "Hazan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2013}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": null, "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "A bounding chain for swendsen-wang", "author": ["M. Huber"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Huber.,? \\Q2003\\E", "shortCiteRegEx": "Huber.", "year": 2003}, {"title": "A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries", "author": ["M. Jerrum", "A. Sinclair", "E. Vigoda"], "venue": null, "citeRegEx": "Jerrum et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jerrum et al\\.", "year": 2004}, {"title": "PAC-Bayesian approach for minimization of phoneme error rate", "author": ["J. Keshet", "D. McAllester", "T. Hazan"], "venue": "In ICASSP,", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "Probabilistic graphical models", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Convergent tree-reweighted message passing for energy", "author": ["V. Kolmogorov"], "venue": "minimization. PAMI,", "citeRegEx": "Kolmogorov.,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov.", "year": 2006}, {"title": "Dual decomposition for parsing with non-projective head automata", "author": ["T. Koo", "A.M. Rush", "M. Collins", "T. Jaakkola", "D. Sontag"], "venue": "In EMNLP,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "The Concentration of Measure Phenomenon, volume 89 of Mathematical Surveys and Monographs", "author": ["M. Ledoux"], "venue": "American Mathematical Society,", "citeRegEx": "Ledoux.,? \\Q2001\\E", "shortCiteRegEx": "Ledoux.", "year": 2001}, {"title": "Simplified PAC-Bayesian margin bounds", "author": ["D. McAllester"], "venue": "Learning Theory and Kernel Machines,", "citeRegEx": "McAllester.,? \\Q2003\\E", "shortCiteRegEx": "McAllester.", "year": 2003}, {"title": "On the method of bounded differences. In Surveys in Combinatorics, number 141 in London Mathematical Society Lecture Note Series, pages 148\u2013188", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In ICCV,", "citeRegEx": "Papandreou and Yuille.,? \\Q2011\\E", "shortCiteRegEx": "Papandreou and Yuille.", "year": 2011}, {"title": "Probabilistic methods in the geometry of Banach spaces. In Probabilty and Analysis, Varenna (Italy) 1985, volume 1206 of Lecture Notes in Mathematics", "author": ["G. Pisier"], "venue": null, "citeRegEx": "Pisier.,? \\Q1986\\E", "shortCiteRegEx": "Pisier.", "year": 1986}, {"title": "Tightening LP relaxations for MAP using message passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss"], "venue": "In UAI,", "citeRegEx": "Sontag et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2008}, {"title": "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft", "author": ["T. Werner"], "venue": null, "citeRegEx": "Werner.,? \\Q1979\\E", "shortCiteRegEx": "Werner.", "year": 1979}], "referenceMentions": [{"referenceID": 7, "context": "Examples include scene understanding [Felzenszwalb and Zabih, 2011], parsing [Koo et al.", "startOffset": 37, "endOffset": 67}, {"referenceID": 17, "context": "Examples include scene understanding [Felzenszwalb and Zabih, 2011], parsing [Koo et al., 2010], and protein design [Sontag et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 23, "context": ", 2010], and protein design [Sontag et al., 2008].", "startOffset": 28, "endOffset": 49}, {"referenceID": 16, "context": "Substantial effort has gone into developing algorithms for recovering MAP assignments by exploiting domain-specific structural restrictions such as super-modularity [Kolmogorov, 2006] or by linear programming relaxations such as cuttingplanes [Sontag et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 9, "context": "[2013] use expectation bounds on the partition function [Hazan and Jaakkola, 2012] to build a sampler for Gibbs distribution using MAP solvers on low dimensional perturbations which are only linear in the dimension of the structures.", "startOffset": 56, "endOffset": 82}, {"referenceID": 9, "context": ", Jerrum et al. [2004], Huber [2003]) where no data term (signal) exists.", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "[2004], Huber [2003]) where no data term (signal) exists.", "startOffset": 8, "endOffset": 21}, {"referenceID": 9, "context": "[2004], Huber [2003]) where no data term (signal) exists. One way around the difficulties of sampling from the Gibbs distribution is to look for the maximum a posteriori probability (MAP) structure. Substantial effort has gone into developing algorithms for recovering MAP assignments by exploiting domain-specific structural restrictions such as super-modularity [Kolmogorov, 2006] or by linear programming relaxations such as cuttingplanes [Sontag et al., 2008, Werner, 2008]. A drawback of MAP inference is that it returns a single assignment; in many contemporary models with complex potential functions on many variables, there are several likely structures, which makes MAP inference less appealing. We would like to also find these other \u201chighly probable\u201d assignments. Recent work has sought to leverage the current efficiency of MAP solvers to build procedures to sample from the Gibbs distribution, thereby avoiding the computational burden of MCMC methods. These works calculate the MAP structure of a randomly perturbed potential function. Such an approach effectively ignores the raggedness of the landscape that hinders MCMC. Papandreou and Yuille [2011] and Tarlow et al.", "startOffset": 8, "endOffset": 1168}, {"referenceID": 9, "context": "[2004], Huber [2003]) where no data term (signal) exists. One way around the difficulties of sampling from the Gibbs distribution is to look for the maximum a posteriori probability (MAP) structure. Substantial effort has gone into developing algorithms for recovering MAP assignments by exploiting domain-specific structural restrictions such as super-modularity [Kolmogorov, 2006] or by linear programming relaxations such as cuttingplanes [Sontag et al., 2008, Werner, 2008]. A drawback of MAP inference is that it returns a single assignment; in many contemporary models with complex potential functions on many variables, there are several likely structures, which makes MAP inference less appealing. We would like to also find these other \u201chighly probable\u201d assignments. Recent work has sought to leverage the current efficiency of MAP solvers to build procedures to sample from the Gibbs distribution, thereby avoiding the computational burden of MCMC methods. These works calculate the MAP structure of a randomly perturbed potential function. Such an approach effectively ignores the raggedness of the landscape that hinders MCMC. Papandreou and Yuille [2011] and Tarlow et al. [2012] have shown that randomly perturbing the potential of each structure with an independent random variable that follows the Gumbel distribution and finding the MAP assignment of the perturbed potential function provides an unbiased sample from the Gibbs distribution.", "startOffset": 8, "endOffset": 1193}, {"referenceID": 8, "context": "Alternatively, Hazan et al. [2013] use expectation bounds on the partition function [Hazan and Jaakkola, 2012] to build a sampler for Gibbs distribution using MAP solvers on low dimensional perturbations which are only linear in the dimension of the structures.", "startOffset": 15, "endOffset": 35}, {"referenceID": 3, "context": "Instead, we derive a new Poincar\u00e9 inequality for the Gumbel distribution, as well as a modified logarithmic Sobolev inequality using the approach suggested by Bobkov and Ledoux [1997], as described in the monograph of Ledoux [2001].", "startOffset": 159, "endOffset": 184}, {"referenceID": 3, "context": "Instead, we derive a new Poincar\u00e9 inequality for the Gumbel distribution, as well as a modified logarithmic Sobolev inequality using the approach suggested by Bobkov and Ledoux [1997], as described in the monograph of Ledoux [2001]. These results, which are of general interest, also guarantee that the deviation of the sampled mean of random MAP perturbations from their expectation has an exponential decay.", "startOffset": 159, "endOffset": 232}, {"referenceID": 8, "context": "Gumbel and Lieblein [1954] Let \u0393 = {\u03b3(x) : x \u2208 X} be a collection of i.", "startOffset": 0, "endOffset": 27}, {"referenceID": 10, "context": "Instead, Hazan et al. [2013] use the representation in (6) to derive a family of self-reducible upper bounds on Z and then use these upper bounds in an iterative algorithm that samples from the Gibbs distribution using low dimensional random MAP perturbations.", "startOffset": 9, "endOffset": 29}, {"referenceID": 10, "context": "Algorithm 1 Sampling with low-dimensional random MAP perturbations from the Gibbs distribution [Hazan et al., 2013] Iterate over j = 1, .", "startOffset": 95, "endOffset": 115}, {"referenceID": 18, "context": "The left side of (11) turns out to be the so-called functional entropy Ledoux [2001] of the function h = exp(\u03bbF ) with respect to a measure \u03bc:", "startOffset": 71, "endOffset": 85}, {"referenceID": 4, "context": ", [Boucheron et al., 2004]) extends the inequality to functions F of many variables.", "startOffset": 2, "endOffset": 26}, {"referenceID": 11, "context": "Koller and Friedman [2009]), which may be challenging in ragged probability landscapes.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "Koller and Friedman [2009]), which may be challenging in ragged probability landscapes. MAP perturbations use efficient MAP solvers as black box, but the statistical properties of the solutions, beyond Theorem 1, are still being studied. Papandreou and Yuille [2011] consider probability models that are defined by the maximal argument of randomly perturbed potential function, while Tarlow et al.", "startOffset": 0, "endOffset": 267}, {"referenceID": 11, "context": "Koller and Friedman [2009]), which may be challenging in ragged probability landscapes. MAP perturbations use efficient MAP solvers as black box, but the statistical properties of the solutions, beyond Theorem 1, are still being studied. Papandreou and Yuille [2011] consider probability models that are defined by the maximal argument of randomly perturbed potential function, while Tarlow et al. [2012] considers sampling techniques for such models and Keshet et al.", "startOffset": 0, "endOffset": 405}, {"referenceID": 11, "context": "[2012] considers sampling techniques for such models and Keshet et al. [2011] explores the generalization bounds for such models.", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": "Hazan and Jaakkola [2012] used the random MAP perturbation framework to derive upper bounds on the partition function in (2), and Hazan et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 8, "context": "Hazan and Jaakkola [2012] used the random MAP perturbation framework to derive upper bounds on the partition function in (2), and Hazan et al. [2013] derived the unbiased sampler in Algorithm 1.", "startOffset": 0, "endOffset": 150}, {"referenceID": 1, "context": ", Bartlett and Mendelson [2003]) or in PAC-Bayesian approaches (e.", "startOffset": 2, "endOffset": 32}, {"referenceID": 1, "context": ", Bartlett and Mendelson [2003]) or in PAC-Bayesian approaches (e.g., McAllester [2003]).", "startOffset": 2, "endOffset": 88}, {"referenceID": 1, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003].", "startOffset": 8, "endOffset": 25}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value.", "startOffset": 26, "endOffset": 110}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]).", "startOffset": 26, "endOffset": 401}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]). In this work we use logarithmic Sobolev inequalities Ledoux [2001] and prove a new measure concentration result for Gumbel random variables.", "startOffset": 26, "endOffset": 470}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]). In this work we use logarithmic Sobolev inequalities Ledoux [2001] and prove a new measure concentration result for Gumbel random variables. To do this we generalize a classic result of Brascamp and Lieb [1976] on Poincar\u00e9 inequalities to non-strongly log-concave distributions, and also recover the concentration result of Bobkov and Ledoux [1997] for functions of Laplace random variables.", "startOffset": 26, "endOffset": 614}, {"referenceID": 0, "context": "such as Bernstein [1946], Azuma-Hoeffding [Azuma, 1967, Hoeffding, 1963, McDiarmid, 1989], or Bousquet [2003]. However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value. There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and Pisier [1986]). In this work we use logarithmic Sobolev inequalities Ledoux [2001] and prove a new measure concentration result for Gumbel random variables. To do this we generalize a classic result of Brascamp and Lieb [1976] on Poincar\u00e9 inequalities to non-strongly log-concave distributions, and also recover the concentration result of Bobkov and Ledoux [1997] for functions of Laplace random variables.", "startOffset": 26, "endOffset": 752}, {"referenceID": 6, "context": "The proof is based on the one in Brascamp and Lieb [1976], but it uses a different strategy in the final critical steps.", "startOffset": 33, "endOffset": 58}, {"referenceID": 6, "context": "The main difference between Theorem 2 and the result of Brascamp and Lieb [1976, Theorem 4.1] is that the latter requires the function Q to be strongly convex. Our result holds for non-strongly concave functions including the Laplace and Gumbel distributions. If we take \u03b7 = 0 in Theorem 2 we recover the original result of Brascamp and Lieb [1976, Theorem 4.1]. For the case \u03b7 = 1/2, Theorem 2 yields the Poincar\u00e9 inequality for the Laplace distribution given in Ledoux [2001]. Like the Gumbel distribution, the Laplace distribution is not strongly log-concave and previously required an alternative technique to prove measure concentration Ledoux [2001].", "startOffset": 56, "endOffset": 478}, {"referenceID": 6, "context": "The main difference between Theorem 2 and the result of Brascamp and Lieb [1976, Theorem 4.1] is that the latter requires the function Q to be strongly convex. Our result holds for non-strongly concave functions including the Laplace and Gumbel distributions. If we take \u03b7 = 0 in Theorem 2 we recover the original result of Brascamp and Lieb [1976, Theorem 4.1]. For the case \u03b7 = 1/2, Theorem 2 yields the Poincar\u00e9 inequality for the Laplace distribution given in Ledoux [2001]. Like the Gumbel distribution, the Laplace distribution is not strongly log-concave and previously required an alternative technique to prove measure concentration Ledoux [2001]. The following gives a Poincar\u00e9 inequality for the Gumbel distribution.", "startOffset": 56, "endOffset": 656}, {"referenceID": 18, "context": "14 of Ledoux [2001] and Corollary 1, for any |\u03bb|b \u2264 \u03c1 \u2264 1, Ent\u03bci(exp(\u03bbfi)) \u2264 2\u03bb ( 1 + \u03c1 1\u2212 \u03c1 )2 exp(2 \u221a 5\u03c1) \u222b |\u2202iF |d\u03bci.", "startOffset": 6, "endOffset": 20}, {"referenceID": 18, "context": "13 in Ledoux [2001] to tensorize the entropy by summing over i = 1 to m:", "startOffset": 6, "endOffset": 20}, {"referenceID": 18, "context": "We now use Herbst\u2019s argument Ledoux [2001]. Using (17) we have H \u2032(\u03bb) = Ent\u03bcm(exp(\u03bbF )) \u03bb2\u039b(\u03bb) \u2264 5a.", "startOffset": 29, "endOffset": 43}, {"referenceID": 16, "context": "As these spin glass models are attractive, we are able to use the graphcuts algorithm (Kolmogorov [2006]) to compute the MAP perturbations efficiently.", "startOffset": 87, "endOffset": 105}], "year": 2013, "abstractText": "The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models. By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution. The measure concentration result is of general interest and may be applicable to other areas involving expected estimations.", "creator": "LaTeX with hyperref package"}}}