{"id": "1310.4546", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2013", "title": "Distributed Representations of Words and Phrases and their Compositionality", "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "histories": [["v1", "Wed, 16 Oct 2013 23:28:53 GMT  (36kb)", "http://arxiv.org/abs/1310.4546v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["tomas mikolov", "ilya sutskever", "kai chen 0010", "gregory s corrado", "jeffrey dean"], "accepted": true, "id": "1310.4546"}, "pdf": {"name": "1310.4546.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mikolov@google.com", "ilyasu@google.com", "kai@google.com", "gcorrado@google.com", "jeff@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 131 0.45 46v1 [cs.CL]"}, {"heading": "1 Introduction", "text": "\"Distributed representations of words in a vector space help to achieve better performance in natural language processing by grouping very different words.\" One of the earliest uses of word representation dates back to 1986 due to Rumelhart, Hinton and Williams [13]. This idea has since been applied to statistical language modelling in Boston with considerable success [1]. Subsequent work includes applications for automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 18, 19, 9]. Recently, Mikolov et al. [8] introduced the Skip-gram model, an efficient method for learning high-quality vector representations of words from unstructured text data. Unlike most previously used neural network architectures for word vector learning, the formation of the Skipgram model (see Figure 1) does not include multiplications of unstructured text."}, {"heading": "2 The Skip-gram Model", "text": "The training goal of the Skip-gram model is to find word representations useful for predicting the surrounding words in a sentence or document. Formally, the goal of the Skip-gram model is to maximize the average log probability 1TT = 1 \u2211 \u2212 c \u2264 j \u2264 c, j 6 = 0log p (wt + j | wt) (1), where c is the size of the training context (which can be a function of the middle word wt). Greater c leads to more training examples and can therefore lead to higher accuracy, at the expense of training time. The basic Skip-gram formulation defines p (wt + j | wt) using the softmax function: p (wO | wI) = exp (v \u00b2 wO \u00b2 vwI) \u2022 implicitly W = 1 exp (v \u00b2 vwI) (2), where the vw \u00b2 function and v \u00b2 terms are in the formula \"v0.\""}, {"heading": "2.1 Hierarchical Softmax", "text": "A mathematically efficient approximation of the complete Softmax is the hierarchical Softmax technique, which was first introduced by Morin and Bengio in connection with the language models of the neural network. [12] The main advantage is that instead of evaluating the W output nodes in the neural network to obtain the probability distribution, it is necessary to evaluate only about log2 (W) nodes. Hierarchical Softmax uses a binary tree representation of the output layer with the W words as its leaves and explicitly represents for each node the relative probabilities of its child nodes, which define a random path that assigns probabilities to words. Specifically, each word w can be reached by an appropriate path from the root of the tree."}, {"heading": "2.2 Negative Sampling", "text": "An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), introduced by Gutmann and Hyvarinen [4] and applied to the language modelling of Mnih and Teh [11]. NCE assumes that a good model should be able to distinguish data by means of logistic noise regression, similar to the hinged losses used by Collobert and Weston [2], which the models use by ranking data via noise.While NCE can be shown to approximate the protocol probability of softmax, the skipgram model only deals with learning high quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality. We define Negative Sampling (NEG) by the probability of objectivity of softmax."}, {"heading": "2.3 Subsampling of Frequent Words", "text": "In very large corpora, the most common words can easily occur hundreds of millions of times (e.g. \"in,\" \"the\" and \"a\"). Such words usually offer less informational value than the rare words. For example, while the Skip-gram model benefits from observing the coexistence of \"France\" and \"Paris,\" it benefits much less from observing the frequent coexistence of \"France\" and \"die,\" since almost every word often occurs within a sentence of \"the.\" This idea can also be applied in the opposite direction; the vector representations of common words do not change significantly after training on several million examples. To counteract the imbalance between the rare and common words, we have used a simple subsampling approach: each word wi in the training set is discarded with the probability calculated by formula P (wi) = 1 \u2212 \u221a tf (wi) (5)."}, {"heading": "3 Empirical Results", "text": "In this section, we evaluate the hierarchical Softmax (HS), Noise Contrastive Estimation, Negative Sampling, and subsampling of the training words. We used the analog reasoning task1 introduced by Mikolov et al. [8] The task consists of analogies such as \"Germany\": \"Berlin\": \"France\":?, which are solved by finding a vector x that comes closest to vec (\"Berlin\") - vec (\"Germany\") + vec (\"France\") according to cosmic distance (we discard the input words from the search). This specific example is considered correct if \"Paris\" is. The task has two broad categories: the syntactic analogies (such as \"fast\": \"fast\": \"slow\" ultaric \")."}, {"heading": "4 Learning Phrases", "text": "As already discussed, many phrases have a meaning that is not a simple composition of the meanings of their individual words. To learn the vector representation for phrases, we first find words that often occur together and rarely in other contexts. For example, \"New York Times\" and \"Toronto Maple Leafs\" are replaced by unique tokens in the training data, while a bigram \"this is\" remains unchanged. 1code.google.com / p / word2vec / source / browse / trunk / questions-words.txtIn this way, we can build many reasonable phrases without greatly increasing the vocabulary. Theoretically, we can train the Skip-gram model using all the n-grams, but that would be too memory-intensive. Many techniques have already been developed to identify phrases in the text, but it is beyond the scope of our work to compare them."}, {"heading": "4.1 Phrase Skip-Gram Results", "text": "Using the same message data as in the previous experiments, we first constructed the phrase-based training corpus and then trained several Skip-gram models using different hyperparameters. As before, we used vector dimensionality 300 and context size 5. This setting already achieves good performance on the phrase dataset and allowed us to quickly compare Negative Sampling and Hierarchical Softmax, both with and without subsampling the common tokens. The results are shown in Table 3.The results show that Negative Sampling achieves a respectable accuracy even with k = 5, with k = 15 achieving significantly better performance. Surprisingly, we found that when the Hierarchical Softmax is trained without subsampling, it becomes the most powerful method when we sampled the common words. This shows that subsampling can lead to faster training and can improve the accuracy of the model, at least in some cases."}, {"heading": "5 Additive Compositionality", "text": "We have shown that the word and phrase representations learned through the Skip-gram model have a linear structure that enables precise analog considerations to be made by means of simple vector arithmetic. Interestingly, we found that the Skip-gram representations have a different type of linear structure that allows words to be meaningfully combined by adding up their vector representations in an elementary manner. This phenomenon is explained in Table 5. The additive property of the vectors can be explained by inspecting the training target. Word vectors have a linear relationship to the input to the softmax nonlinearity. Since the word vectors are trained to predict the surrounding words in the sentence, the vectors can be considered as representatives of the distribution of the context in which a word appears. These values are logarithmically related to the probabilities calculated by the output layer, so that the sum of the two word vectors can be combined with the product of the context function, such as the high probability that the product here combines the two words."}, {"heading": "6 Comparison to Published Word Representations", "text": "Many authors who have previously worked on neural network-based word representations have published their resulting models for further use and comparison: Some of the best-known authors include Collobert and Weston [2], Turian et al. [17] and Mnih and Hinton [10]. We have downloaded their word vectors from Web3.Mikolov et al. [8] have already evaluated these word representations on the word analogy task, where the Skip-gram models perform best with a huge marginality.3http: / / metaoptimize.com / projects / wordreprs / To give more insight into the difference in quality of the learned vectors, we offer an empirical comparison by comparing the closest neighbors of rare words in Table 6. These examples show that the large Skip-gram model, trained on a large body, clearly outperforms all other models in the quality of the learned representations."}, {"heading": "7 Conclusion", "text": "This paper has several key contributions. We show how to train distributed representations of words and phrases using the Skip-gram model and show that these representations have a linear structure that allows precise analog thinking. The techniques introduced in this paper can also be used to build the continuous sack-of-words model that was introduced in [8]. We also found that subsampling of common words results in several orders of magnitude more data than the previously published models, thanks to the computationally efficient model architecture. This leads to a great improvement in the quality of learned word and phrase representations, especially for the rare entities. We also found that subsampling of common words results in both faster training and significantly better representations of unusual words. Another contribution of our paper is the negative sampling algorithm, an extremely simple training method that learns accurate representations, especially for frequent words. The choice of the training algorithm and the hyper-parameter choice, we found that different word choices are optimal due to different parameters."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Michael U Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Strategies for Training Large Scale Neural Network Language Models", "author": ["Tomas Mikolov", "Anoop Deoras", "Daniel Povey", "Lukas Burget", "Jan Cernocky"], "venue": "In Proc. Automatic Speech Recognition and Understanding,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["Tomas Mikolov"], "venue": "PhD thesis, PhD Thesis, Brno University of Technology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "ICLR Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh"], "venue": "arXiv preprint arXiv:1206.6426,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Learning representations by backpropagating", "author": ["David E Rumelhart", "Geoffrey E Hintont", "Ronald J Williams"], "venue": "errors. Nature,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1986}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "Computer Speech and Language,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 26th International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "In Journal of Artificial Intelligence Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "author": ["Peter D. Turney"], "venue": "In Transactions of the Association for Computational Linguistics (TACL),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "One of the earliest use of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "This idea has since been applied to statistical language modeling with considerable success [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 13, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 97, "endOffset": 104}, {"referenceID": 6, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 97, "endOffset": 104}, {"referenceID": 1, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 136, "endOffset": 161}, {"referenceID": 19, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 136, "endOffset": 161}, {"referenceID": 14, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 136, "endOffset": 161}, {"referenceID": 2, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 136, "endOffset": 161}, {"referenceID": 17, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 136, "endOffset": 161}, {"referenceID": 18, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 136, "endOffset": 161}, {"referenceID": 8, "context": "The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].", "startOffset": 136, "endOffset": 161}, {"referenceID": 7, "context": "[8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "For example, the result of a vector calculation vec(\u201cMadrid\u201d) - vec(\u201cSpain\u201d) + vec(\u201cFrance\u201d) is closer to vec(\u201cParis\u201d) than to any other word vector [9, 8].", "startOffset": 149, "endOffset": 155}, {"referenceID": 7, "context": "For example, the result of a vector calculation vec(\u201cMadrid\u201d) - vec(\u201cSpain\u201d) + vec(\u201cFrance\u201d) is closer to vec(\u201cParis\u201d) than to any other word vector [9, 8].", "startOffset": 149, "endOffset": 155}, {"referenceID": 3, "context": "In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work [8].", "startOffset": 284, "endOffset": 287}, {"referenceID": 14, "context": "Other techniques that aim to represent meaning of sentences by composing the word vectors, such as the recursive autoencoders [15], would also benefit from using phrase vectors instead of the word vectors.", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "In the context of neural network language models, it was first introduced by Morin and Bengio [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "Mnih and Hinton explored a number of methods for constructing the tree structure and the effect on both the training time and the resulting model accuracy [10].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "It has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models [5, 8].", "startOffset": 167, "endOffset": 173}, {"referenceID": 7, "context": "It has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models [5, 8].", "startOffset": 167, "endOffset": 173}, {"referenceID": 3, "context": "An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was introduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].", "startOffset": 128, "endOffset": 131}, {"referenceID": 10, "context": "An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was introduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].", "startOffset": 181, "endOffset": 185}, {"referenceID": 1, "context": "This is similar to hinge loss used by Collobert and Weston [2] who trained the models by ranking the data above noise.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "Table 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task as defined in [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] also show that the vectors learned by the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this task significantly as the amount of the training data increases, suggesting that non-linear models also have a preference for a linear structure of the word representations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Many authors who previously worked on the neural network based representations of words have published their resulting models for further use and comparison: amongst the most well known authors are Collobert and Weston [2], Turian et al.", "startOffset": 219, "endOffset": 222}, {"referenceID": 16, "context": "[17], and Mnih and Hinton [10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[17], and Mnih and Hinton [10].", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "[8] have already evaluated these word representations on the word analogy task, where the Skip-gram models achieved the best performance with a huge margin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "The techniques introduced in this paper can be used also for training the continuous bag-of-words model introduced in [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 15, "context": "Our work can thus be seen as complementary to the existing approach that attempts to represent phrases using recursive matrix-vector operations [16].", "startOffset": 144, "endOffset": 148}], "year": 2013, "abstractText": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir Canada\u201d. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "creator": "gnuplot 4.4 patchlevel 3"}}}