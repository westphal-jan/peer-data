{"id": "1310.5007", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2013", "title": "Online Classification Using a Voted RDA Method", "abstract": "We propose a voted dual averaging method for online classification problems with explicit regularization. This method employs the update rule of the regularized dual averaging (RDA) method, but only on the subsequence of training examples where a classification error is made. We derive a bound on the number of mistakes made by this method on the training set, as well as its generalization error rate. We also introduce the concept of relative strength of regularization, and show how it affects the mistake bound and generalization performance. We experimented with the method using $\\ell_1$ regularization on a large-scale natural language processing task, and obtained state-of-the-art classification performance with fairly sparse models.", "histories": [["v1", "Thu, 17 Oct 2013 04:01:25 GMT  (68kb)", "http://arxiv.org/abs/1310.5007v1", "23 pages, 5 figures"]], "COMMENTS": "23 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tianbing xu", "jianfeng gao", "lin xiao", "amelia c regan"], "accepted": true, "id": "1310.5007"}, "pdf": {"name": "1310.5007.pdf", "metadata": {"source": "CRF", "title": "Online Classification Using a Voted RDA Method", "authors": ["Tianbing Xu", "Jianfeng Gao", "Lin Xiao"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 131 0,50 07"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Regularized online classification", "text": "In this paper we mainly consider binary classification problems. Let us (x1, y1), (xm, ym), (x1), (x1), (x1), (x2), (x2), (x3), (x3), (x3), (x3), (x3), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (x4), (4), (4), (4), (4) (x4), (x4), (4), (x4), (x4), (4), (x4), (4), (x4), (x4), (4), (x4), (x4), (4), (x4), (x4), (x4), (x4), x4 (x4), x4, x4 (x4), x4 (x4), x4 (x4, x4, x4 (x4), x4, x4 (x4), x4, x4 (x4, x4 (x4), x4 (x4), x4, x4 (x4), x4 (x4), x4, x4 (x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4 (x4), x4, x4 (x4 (x4), x4, x4 (x4), x4, x4, x4 (x4), x4 (x4), x4, x4 (x4), x4 (x4), x4 (x4), x4, x4, x4, x4 (x4, x4 (x4), x4 (x4), x4, x4 (x4), x4"}, {"heading": "4 Bound on the number of mistakes", "text": "We offer an analysis of the chosen RDA method in case N = 1 (i.e., we go through the training set (one-time), comparing the analysis to that given for the chosen Perceptron algorithm in Friend and Schapire [FS99]. In this section, we have limited the number of errors made by the chosen RDA method by their repentance analysis. In the next section, we specify the expected error rate in an online-to-batch conversion. First, we recognize that the chosen RDA method corresponds to the execution of the RDA method [Xia10] in the sequence of training examples in which a classification error is made. Let M determine the number of prediction errors made by the algorithm after processing the m training examples, and i (k) the index of the example on which the k-th error was made (by wk). The regret of the algorithm, in relation to a fixed vector, is only determined by the examples."}, {"heading": "4.1 Analysis for the separable case", "text": "Our analysis for the dividable case is based on the loss of hinges we have generated. (w) = max {0, 1 \u2212 yi (wTxi).Assumption 1 There is a vector u such that yi (uTxi) \u2265 1 for all i = 1,.., m.This is the standard separability with a large margin assumption. Under this assumption we have a vector k = 1o (k) (u) = M-value k = 1max {0, 1 \u2212 yi (k) (uTxi (k))} = 0for all M > 0 and each sub-sequence {i (k)} Mi = 1. The margin of separability is defined as. For convenience we also leave R = max i = 1,... m-xi \u00b2 2. Then we can set G = R-value because for hinge loss, \u2212 yixi is the subgradient."}, {"heading": "4.2 Analysis for the inseparable case", "text": "We start with the inequality (7). To simplify the notation, we leave L (u) for the total loss of any vector u via the sequence i (k), k = 1,.., M, i.e., L (u) = unequal, k (k) (k) (u). (10) Then we have an error. (10) Then we have an error. (u) + M, which causes an error. (11) Our analysis is similar to the error analysis for the perceptron in [SS11]. This is based on the following simple problem: Lemma 1 Given a, b, c > 0, the inequality ax \u2212 b) x \u2212 c, 0 impliesx. (ba) 2 + ba."}, {"heading": "5 Online-to-batch conversion", "text": "The training part of the chosen RDA method (algorithm 1) is an algorithm generated online that makes a small number of errors when presented with examples one by one (see analysis in Section 4). In a batch setting, we can use this algorithm to process the training data one by one (possibly by going through the data several times) and then create a hypothesis that is evaluated on a separate test set. Following Friend and Schapire [FS99], we use the deterministic Leaveone-out method to convert an online learning algorithm into a batch learning algorithm. Here's a brief description. Let's say we have m training examples and an unmarked instance that all i.i.d. After each r learning algorithm {0, m} we run the online algorithm with a sequence of r + 1 examples consisting of the first r examples in the training and the last instance is not labeled."}, {"heading": "6 Experiments on parse reranking", "text": "Scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scanner-scan"}, {"heading": "7 Conclusion and Discussions", "text": "This method updates the predictor only on the basis of the sequence of training examples in which a classification error is made. In addition to significantly reducing the computing costs associated with updating the predictor, we can derive an error-based approach that does not depend on the total number of examples. We also introduce the concept of the relative strength of regulation and how it affects error-based and generalizing performance. Our analysis of the error-based method is based on the regret analysis of the RDA method [Xia10]. In fact, our notion of relative strength of regulation and error-based analysis also applies to the selected versions of other online algorithms that allow a similar regret analysis, including the forward-backward splitting method in [DS09]."}], "references": [{"title": "Scalable training of l1regularized log-linear models", "author": ["Galen Andrew", "Jianfeng Gao"], "venue": "In Proceedings of the 24th International Conference on Machine learning(ICML", "citeRegEx": "Andrew and Gao.,? \\Q2007\\E", "shortCiteRegEx": "Andrew and Gao.", "year": 2007}, {"title": "The tradeoffs of large scale learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bottou and Bousquet.,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2008}, {"title": "A maximum-entropy-inspired parser", "author": ["Eugene Charniak"], "venue": "In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference (NAACL", "citeRegEx": "Charniak.,? \\Q2000\\E", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics(ACL", "citeRegEx": "Charniak and Johnson.,? \\Q2005\\E", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "Discriminative re-ranking for natural language parsing", "author": ["Michael Collins"], "venue": "In Proceedings of the 17th International Conference on Machine learning(ICML),", "citeRegEx": "Collins.,? \\Q2000\\E", "shortCiteRegEx": "Collins.", "year": 2000}, {"title": "Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of the ACL-02 conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["John Duchi", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "Sample compression, learnability, and the vapnik-chervonenkis dimension", "author": ["Sally Floyd", "Manfred K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Floyd and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Floyd and Warmuth.", "year": 1995}, {"title": "On weak learning", "author": ["David P. Helmbold", "Manfred K. Warmuth"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Helmbold and Warmuth.,? \\Q1995\\E", "shortCiteRegEx": "Helmbold and Warmuth.", "year": 1995}, {"title": "Sparse online learning via truncated gradient", "author": ["John Langford", "Lihong Li", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Manifold identication in dual averaging for regularized stochastic online learning", "author": ["Sangkyun Lee", "Stephen J. Wrigh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lee and Wrigh.,? \\Q2012\\E", "shortCiteRegEx": "Lee and Wrigh.", "year": 2012}, {"title": "Building a large annotated corpus of english: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Yurii Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2009\\E", "shortCiteRegEx": "Nesterov.", "year": 2009}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "author": ["Lin Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xiao.,? \\Q2010\\E", "shortCiteRegEx": "Xiao.", "year": 2010}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In International Conference on Machine learning (ICML", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [], "year": 2013, "abstractText": "We propose a voted dual averaging method for online classification problems with explicit regularization. This method employs the update rule of the regularized dual averaging (RDA) method, but only on the subsequence of training examples where a classification error is made. We derive a bound on the number of mistakes made by this method on the training set, as well as its generalization error rate. We also introduce the concept of relative strength of regularization, and show how it affects the mistake bound and generalization performance. We experimented with the method using l1-regularization on a large-scale natural language processing task, and obtained state-of-the-art classification performance with fairly sparse models.", "creator": "LaTeX with hyperref package"}}}