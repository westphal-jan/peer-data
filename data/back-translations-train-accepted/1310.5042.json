{"id": "1310.5042", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2013", "title": "Distributional semantics beyond words: Supervised learning of analogy and paraphrase", "abstract": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval~2012 Task 2) and measuring compositional similarity between noun-modifier phrases and unigrams (multiple-choice paraphrase questions).", "histories": [["v1", "Fri, 18 Oct 2013 14:50:39 GMT  (30kb)", "http://arxiv.org/abs/1310.5042v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.IR", "authors": ["peter d turney"], "accepted": true, "id": "1310.5042"}, "pdf": {"name": "1310.5042.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["peter.turney@nrc-cnrc.gc.ca"], "sections": [{"heading": null, "text": "ar Xiv: 131 0.50 42v1 [cs.LG] 1 8O ctThere have been several efforts to extend the distribution semantics beyond single words to measure the similarity of pairs of words, phrases and sentences (short, tuples; ordered word groups, contiguous or uncontiguous).One way to go beyond words is to compare two tuples with a function that combines pairs of similarities between the contiguous words in the tuples. One strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase).However, previous work has required the coding of the combination function for various tasks. The most important contribution of this essay is that combination functions are generated through supervised learning."}, {"heading": "1 Introduction", "text": "Harris (1954) and Firth (1957) hypothesized that words that appear in similar contexts tend to have similar meanings, and this hypothesis is the basis for the distributional semantics in which words are represented by context vectors, and the similarity of two words is calculated by comparing the two corresponding context vectors (Lund et al., 1995; Landauer and Dumais, 1997; Turney and Pantel, 2010).Distribution semantics is very effective for measuring the semantic similarity between individual words. On a series of eighty multiplechoice synonym questions from the English as a foreign language test (TOEFL), a distributional approach that has recently reached 100% accuracy, it was difficult to extend the distributional semantics beyond individual words, to word pairs, phrases, and sentences."}, {"heading": "2 Related Work", "text": "In SemEval 2012, Task 2 dealt with measuring the degree of relational similarity between two pairs of words (Jurgens et al., 2012) and Task 6 (Agirre et al., 2012) examined the degree of semantic equivalence between two sentences. These two research areas were largely independent, although Socher et al. (2012) and Turney (2012) offer uniform perspectives on the two tasks. We will discuss first some work on relational similarity, then some work on compositional similarity, and finally work that unifies the two types of similarity."}, {"heading": "2.1 Relational Similarity", "text": "LRA (Latent Relationship Analysis) measures kinship similarity with a pair pattern matrix (Turney, 2006b). Rows in the matrix correspond to word pairs (a, b) and columns correspond to patterns that connect the pairs (\"a for b\") in a large corpus. This is a holistic (non-compositional) approach to distributional similarity, as the word pairs are opaque entities; the word components do not have separate representations. A compositional approach to analogy requires a representation for each word, and a word pair is represented by the composition of representations for each member of the pair. Given a vocabulary of N-words, a compositional approach requires N-representations to handle all possible word pairs, but a holistic approach requires N2 representations. Holistic approaches do not scale up (Turney, 2012). LRA took nine days to run."}, {"heading": "2.2 Compositional Similarity", "text": "In order to expand distributional semantics beyond words, many researchers take the first approach described by Erk (2013), in which a single vector space is used for individual words, phrases and sentences (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010).In this approach, we construct a vector for the bigram by experimenting with many different vector operations on a and b. Mitchell and Lapata (2010) and find that elementary multiplication works well. Bigram ab is represented by c = a-b, where ci = ai \u00b7 bi elemental clever multiplication is commutative, so that the bigrams ab and the ba map are limited to the same vector c. In experiments that test order sensitivity, element-wise multiplication performs poorly (Turney, 2012).We can treat bigram as a unit, as if it is a single word, a large context and a fixed one."}, {"heading": "2.3 Unified Perspectives on Similarity", "text": "Socher et al. (2012) represent words and phrases with a pair consisting of a vector and a matrix. The vector grasps the meaning of the word or phrase and the matrix grasps how a word or phrase changes the meaning of another word or phrase when they are combined. They apply this matrix vector representation to both compositions and relationships. Turney (2012) represents words with two vectors, a vector from domain space and a vector from functional space. The domain vector grasps the theme or field of the word and the function vector grasps the functional role of the word. This dual space model is applied to both compositions and relations. Here, we extend the dual space model of Turney (2012) in two ways: hand coding is replaced by supervised learning, and two new sets of characteristics expand domain and functional space."}, {"heading": "3 Features for Tuple Classification", "text": "The first is the logarithm of the frequency of a word. The second is the positive column in which the two words \"Pi\" and \"Pi\" occur. The third and fourth is the similarity of two words: \"Pi,\" \"Pi\" and \"Pi.\" The third and fourth is the similarity of two words: \"Pi,\" \"Pi\" and \"Pi.\" The third and fourth is the similarity of two words: \"Pi,\" \"Pi\" and \"Pi,\" \"Pi,\" \"Pi\" and \"Pi.\""}, {"heading": "4 Relational Similarity", "text": "This section presents experiments with relational similarity using SuperSim. Training data sets consist of quadruple, which are described as positive and negative examples of analogies. Table 2 shows that the feature vectors have 1,348 elements. We experiment with three sets of data, a collection of 374 five-selection questions from the SAT college entrance exam (Turney et al., 2003), a modified ten-selection variant of the SAT Questions4Weka is available at http: / / www.cs.waikato.ac.nz / ml / weka /. (Turney, 2012) and the Relational similarity dataset from SemEval 2012 Task 2 (Jurgens et al., 2012).5"}, {"heading": "4.1 Five-choice SAT Questions", "text": "Table 3 is an example of one question from the 374 five-choice SAT questions. Each five-choice question yields five labeled quadruples by combining the strain with each choice. < word, language, note, music > is labeled positive and the other four quadruples are labeled negative. Since learning works better with balanced training data (Japkowicz and Stephen, 2002), we use the symmetries of proportional analogies to add more positive examples (Lepage and Shin-ichi, 1996). For each positive quadrupling < a, b, c, d > we add three more positive quadruples by hand, < b, c >, c >, d >, b >, and < d, c, b, a >. For each positive quadrupling from the SAT question yields four positive and four negative quadruples from the SAT number questions, we use the quadruple questions on the SAT, SAT 1.1 each."}, {"heading": "4.2 Ten-choice SAT Questions", "text": "In addition to symmetries, proportional analogies have asymmetries. Generally, if the pair of four < a, b, c, d > is positive, < a, d, c, b > is negative. For example, < word, language, note, music > is a good analogy, but < word, music, note, language > is not. Words are the basic units of language and notes are the basic units of music, but words are not necessary for music and notes are not necessary for language.Turney (2012) used this asymmetry to turn the 374 five-choice SAT questions into 374 tenchoice SAT questions. Each choice < c, d > was expanded with the trunk < a >, leading to the choice of four < a, c, d >, and then the order was shifted to < SAT questions < a, c, b >."}, {"heading": "4.3 SemEval 2012 Task 2", "text": "The SemEval 2012 Task 2 dataset is based on the semantic relationship classification scheme of Bejar et al. (1991), consisting of ten high-level relationship categories and seventy-nine sub-categories, with paradigmatic examples for each subcategory. Thus, the taxonomy subcategory in the class inclusion category has three paradigmatic examples: Flower: Tulip, Emotion: Anger, and Poem: sonnet.Jurgens et al. (2012) used Amazon's Mechanical Turk to create the SemEval 2012 Task 2 dataset in two phases. In the first phase, Turkers expanded the paradigmatic examples for each subcategory to an average of 41 word pairs per subcategory, totaling 3,218 word pairs. In the second phase, each word pair from the first phase was assigned a prototype value indicating its similarity to the paradigmatic examples. The challenge of SemEval 2012 task 2 consisted of the relationship between the two paradigms."}, {"heading": "5 Compositional Similarity", "text": "The data sets consist of triples < a, b, c >, so that starting from a nounmodifying bigram and c is a noun unigram, the triples are referred to as positive and negative examples of paraphrasing. Table 2 shows that the attribute vectors have 675 elements. We experimented with two data sets, seven selection and fourteen selection questions for noun modifiers (Turney, 2012).7"}, {"heading": "5.1 Noun-Modifier Questions", "text": "The first set of data contains 680 questions for training and 1,500 for the exam, totaling 2,180 questions. The solution is the unigram (Fairyland), which belongs to the same WordNet synset, and the decisions are unigramm.The distraction consists of a head noun (world) modified by an adjective or noun (fantasy).The solution is the unigram (Fairyland), which belongs to the same WordNet synset as the stem. The distraction is designed in such a way that it can be difficult for current approaches to composition. If the fantasy world is represented by an elementary multiplication of context vectors for fantasy and world (Mitchell and Lapata, 2010), the most likely guess is fantasy or the world, not Fairyland (Turney, 2012).Each seven-choice question yields seven labeled triples by combining the stem with each choice.The triple probability > fantasy is not the world."}, {"heading": "5.2 Ablation Experiments", "text": "Table 9 shows the effects of setting attribute sets on the performance of SuperSim with the fourteen selection questions > PMI characteristics > Taken individually, they are 59.7% correct, although the other characteristics are needed. The results for SuperSim are new, but the other results in Table 8 are from Turney (2012).reach 68.0%. Domain space characteristics achieve the second highest performance when used alone (34.6%), but they reduce performance (from 69.3% to 68.0%) in combination with other characteristics; however, the decrease is not significant after Fisher's rigorous test at the significance level. As the characteristics of PPMI play an important role in answering noun-modifier questions, let's take a closer look at them. From Table 2 we see that there are twelve PPMI characteristics for the triple < a, b, c >, subltSetti, with noun-modifier being a bigramm and noun-modifier."}, {"heading": "5.3 Holistic Training", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6 Discussion", "text": "SuperSim performs slightly better (not statistically significant) than the hand-coded dual-space model for problems with relational similarity (Section 4), but it performs much better for problems with compositional similarity (Section 5). Ablation studies suggest that this is due to the PPMI characteristics, which have no effect on the 10-choice SAT performance (Table 5), but have a large effect on the 14-choice noun modifier paraphrase performance (Table 9). One advantage of supervised learning over hand coding is that it facilitates the addition of new features. It is unclear how the hand-coded equations for the dual-space model of the noun modifier composition (Turney, 2012) can be modified to include PPMI information. SuperSim is one of the few approaches to distributional semantics beyond words that have attempted to address both compositional (see Section 2.3)."}, {"heading": "7 Future Work and Limitations", "text": "Considering the promising results of a holistic training for noun modifier paraphrases, we plan to experiment with a holistic training for analogies. Considering the proportional analogy hard is the hard time good is the good time, in which hard time and good time are pseudo unigrams. To a human being, this analogy is trivial, but SuperSim does not have access to the surface shape of a term. This analogy is similar to the analogy hard is to have fun. This strategy automatically turns into a complex, challenging analogies that are suitable for the training of SuperSim. These analogies can be used to solve analogies."}, {"heading": "8 Conclusion", "text": "In this paper, we have introduced SuperSim, a standardized approach to analogy (kinship similarity) and paraphrase (compositional similarity), both of which SuperSim treats as problems of supervised tuple classification. SuperSim's supervised learning algorithm is a standard support vector engine. SuperSim's main contribution is a set of four types of characteristics to represent tuples. The characteristics work with both analogy and paraphrase, without task-specific modifications. SuperSim is state-of-the-art in SAT analogy questions and brings the state of-the-art to the SemEval 2012 Task 2 challenge and substantially advances in noun-modifier paraphrasing questions. SuperSim runs much faster than LRA (Turney, 2006b), with the SAT questions answered in minutes rather than days. Unlike the dual-space model (Turney, 2012), SuperSim does not require any encrypted similargement functions."}], "references": [{"title": "Mona Diab", "author": ["Eneko Agirre", "Daniel Cer"], "venue": "and Aitor Gonzalez-Agirre.", "citeRegEx": "Agirre et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["Androutsopoulos", "Prodromos Malakasiotis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Androutsopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Androutsopoulos et al\\.", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Ngoc-Quynh Do", "author": ["Marco Baroni", "Raffaella Bernardi"], "venue": "and Chung-chieh Shan.", "citeRegEx": "Baroni et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Susan E", "author": ["Isaac I. Bejar", "Roger Chaffin"], "venue": "Embretson.", "citeRegEx": "Bejar et al.1991", "shortCiteRegEx": null, "year": 1991}, {"title": "Clustering word pairs to answer analogy questions", "author": ["Bi\u00e7ici", "Yuret2006] Ergun Bi\u00e7ici", "Deniz Yuret"], "venue": "In Proceedings of the Fifteenth Turkish Symposium on Artificial Intelligence and Neural Networks (TAINN", "citeRegEx": "Bi\u00e7ici et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bi\u00e7ici et al\\.", "year": 2006}, {"title": "Yutaka Matsuo", "author": ["Danushka Bollegala"], "venue": "and Mitsuru Ishizuka.", "citeRegEx": "Bollegala et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Yutaka Matsuo", "author": ["Danushka Bollegala"], "venue": "and Mitsuru Ishizuka.", "citeRegEx": "Bollegala et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["Bullinaria", "Levy2007] John Bullinaria", "Joseph Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria et al\\.", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming, and SVD", "author": ["Bullinaria", "Levy2012] John Bullinaria", "Joseph Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bullinaria et al\\.", "year": 2012}, {"title": "Experiments with LSA scoring: Optimal rank and basis", "author": ["John Caron"], "venue": "In Proceedings of the SIAM Computational Information Retrieval Workshop,", "citeRegEx": "Caron.,? \\Q2001\\E", "shortCiteRegEx": "Caron.", "year": 2001}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Church", "Hanks1989] Kenneth Church", "Patrick Hanks"], "venue": "In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics,", "citeRegEx": "Church et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Church et al\\.", "year": 1989}, {"title": "A contexttheoretic framework for compositionality in distributional semantics", "author": ["Daoud Clarke"], "venue": "Computational Linguistics,", "citeRegEx": "Clarke.,? \\Q2012\\E", "shortCiteRegEx": "Clarke.", "year": 2012}, {"title": "Towards a semantics for distributional representations", "author": ["Katrin Erk"], "venue": "In Proceedings of the 10th International Conference on Computational Semantics (IWCS", "citeRegEx": "Erk.,? \\Q2013\\E", "shortCiteRegEx": "Erk.", "year": 2013}, {"title": "A synopsis of linguistic theory 1930\u20131955", "author": ["John Rupert Firth"], "venue": "In Studies in Linguistic Analysis,", "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Katrin Erk", "author": ["Dan Garrette"], "venue": "and Ray Mooney.", "citeRegEx": "Garrette et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Structure-mapping: A theoretical framework for analogy", "author": ["Dedre Gentner"], "venue": "Cognitive Science,", "citeRegEx": "Gentner.,? \\Q1983\\E", "shortCiteRegEx": "Gentner.", "year": 1983}, {"title": "Peter Turney", "author": ["Roxana Girju", "Preslav Nakov", "Vivi Nastase", "Stan Szpakowicz"], "venue": "and Deniz Yuret.", "citeRegEx": "Girju et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["Emiliano Guevara"], "venue": "In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics (GEMS", "citeRegEx": "Guevara.,? \\Q2010\\E", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "Lorenza Romano", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti"], "venue": "and Stan Szpakowicz.", "citeRegEx": "Hendrickx et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Bagpack: A general framework to represent semantic relations", "author": ["Herda\u011fdelen", "Baroni2009] Ama\u00e7 Herda\u011fdelen", "Marco Baroni"], "venue": "In Proceedings of the EACL", "citeRegEx": "Herda\u011fdelen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Herda\u011fdelen et al\\.", "year": 2009}, {"title": "The class imbalance problem: A systematic study", "author": ["Japkowicz", "Stephen2002] Nathalie Japkowicz", "Shaju Stephen"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Japkowicz et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Japkowicz et al\\.", "year": 2002}, {"title": "and Keith J", "author": ["David A. Jurgens", "Saif M. Mohammad", "Peter D. Turney"], "venue": "Holyoak.", "citeRegEx": "Jurgens et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Landauer and Susan T", "author": ["K Thomas"], "venue": "Dumais.", "citeRegEx": "Landauer and Dumais1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Effects of high-order co-occurrences on word semantic similarity. Current Psychology Letters: Behaviour", "author": ["Lemaire", "Denhi\u00e8re2006] Beno\u0131\u0302t Lemaire", "Guy Denhi\u00e8re"], "venue": "Brain & Cognition,", "citeRegEx": "Lemaire et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lemaire et al\\.", "year": 2006}, {"title": "Saussurian analogy: A theoretical account and its application", "author": ["Lepage", "Shin-ichi1996] Yves Lepage", "Ando Shin-ichi"], "venue": "In Proceedings of the 16th International Conference on Computational Linguistics (COLING", "citeRegEx": "Lepage et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lepage et al\\.", "year": 1996}, {"title": "Curt Burgess", "author": ["Kevin Lund"], "venue": "and Ruth Ann Atchley.", "citeRegEx": "Lund et al.1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Wen-tau Yih", "author": ["Tomas Mikolov"], "venue": "and Geoffrey Zweig.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Duluth: Measuring degrees of relational similarity with the gloss vector measure of semantic relatedness", "author": ["Ted Pedersen"], "venue": "In First Joint Conference on Lexical and Computational Semantics (*SEM),", "citeRegEx": "Pedersen.,? \\Q2012\\E", "shortCiteRegEx": "Pedersen.", "year": 2012}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["John C. Platt"], "venue": "In Advances in Kernel Methods: Support Vector Learning,", "citeRegEx": "Platt.,? \\Q1998\\E", "shortCiteRegEx": "Platt.", "year": 1998}, {"title": "UTD: Determining relational similarity using lexical patterns", "author": ["Rink", "Harabagiu2012] Bryan Rink", "Sanda Harabagiu"], "venue": "In First Joint Conference on Lexical and Computational Semantics (*SEM),", "citeRegEx": "Rink et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2012}, {"title": "The impact of selectional preference agreement on semantic relational similarity", "author": ["Rink", "Harabagiu2013] Bryan Rink", "Sanda Harabagiu"], "venue": "In Proceedings of the 10th International Conference on Computational Semantics (IWCS", "citeRegEx": "Rink et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rink et al\\.", "year": 2013}, {"title": "and Christopher D", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng"], "venue": "Manning.", "citeRegEx": "Socher et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Christopher Manning", "author": ["Richard Socher", "Brody Huval"], "venue": "and Andrew Ng.", "citeRegEx": "Socher et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "David Pinto", "author": ["Mireya Tovar", "J. Alejandro Reyes", "Azucena Montes", "Darnes Vilari\u00f1o"], "venue": "and Saul Le\u00f3n.", "citeRegEx": "Tovar et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Turney and Michael L", "author": ["D Peter"], "venue": "Littman.", "citeRegEx": "Turney and Littman2005", "shortCiteRegEx": null, "year": 2005}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Jeffrey Bigham", "author": ["Peter D. Turney", "Michael L. Littman"], "venue": "and Victor Shnayder.", "citeRegEx": "Turney et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Dan Assaf", "author": ["Peter D. Turney", "Yair Neuman"], "venue": "and Yohai Cohen.", "citeRegEx": "Turney et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Expressing implicit semantic relations without supervision", "author": ["Peter D. Turney"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Turney.,? \\Q2006\\E", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "Similarity of semantic relations", "author": ["Peter D. Turney"], "venue": "Computational Linguistics,", "citeRegEx": "Turney.,? \\Q2006\\E", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "The latent relation mapping engine: Algorithm and experiments", "author": ["Peter D. Turney"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney.,? \\Q2008\\E", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "A uniform approach to analogies, synonyms, antonyms, and associations", "author": ["Peter D. Turney"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics (Coling", "citeRegEx": "Turney.,? \\Q2008\\E", "shortCiteRegEx": "Turney.", "year": 2008}, {"title": "Domain and function: A dual-space model of semantic relations and compositions", "author": ["Peter D. Turney"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney.,? \\Q2012\\E", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "WordNet sits the SAT: A knowledge-based approach to lexical analogy", "author": ["Tony Veale"], "venue": "In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI", "citeRegEx": "Veale.,? \\Q2004\\E", "shortCiteRegEx": "Veale.", "year": 2004}, {"title": "and Mark A", "author": ["Ian H. Witten", "Eibe Frank"], "venue": "Hall.", "citeRegEx": "Witten et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Geoffrey Zweig", "author": ["Alisa Zhila", "Wen-tau Yih", "Christopher Meek"], "venue": "and Tomas Mikolov.", "citeRegEx": "Zhila et al.2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2013, "abstractText": "There have been several efforts to extend distributional semantics beyond individual words, to measure the similarity of word pairs, phrases, and sentences (briefly, tuples; ordered sets of words, contiguous or noncontiguous). One way to extend beyond words is to compare two tuples using a function that combines pairwise similarities between the component words in the tuples. A strength of this approach is that it works with both relational similarity (analogy) and compositional similarity (paraphrase). However, past work required hand-coding the combination function for different tasks. The main contribution of this paper is that combination functions are generated by supervised learning. We achieve state-of-the-art results in measuring relational similarity between word pairs (SAT analogies and SemEval 2012 Task 2) and measuring compositional similarity between nounmodifier phrases and unigrams (multiplechoice paraphrase questions).", "creator": "LaTeX with hyperref package"}}}