{"id": "1310.5715", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2013", "title": "Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm", "abstract": "We show that the exponential convergence rate of stochastic gradient descent for smooth strongly convex objectives can be markedly improved by perturbing the row selection rule in the direction of sampling estimates proportionally to the Lipschitz constants of their gradients. That is, we show that partially biased sampling allows a convergence rate with linear dependence on the average condition number of the system, compared to dependence on the average squared condition number for standard stochastic gradient descent. We assume the regime where all stochastic estimates share an optimum and so such an exponential rate is possible. We then recast the randomized Kaczmarz algorithm for solving overdetermined linear systems as an instance of preconditioned stochastic gradient descent, and apply our results to prove its exponential convergence, but to the solution of a weighted least squares problem rather than the original least squares problem. We present a modified Kaczmarz algorithm with partially biased sampling which does converge to the original least squares solution with the same exponential convergence rate.", "histories": [["v1", "Mon, 21 Oct 2013 20:15:44 GMT  (465kb)", "https://arxiv.org/abs/1310.5715v1", "16 pages, 6 figures"], ["v2", "Sat, 15 Feb 2014 01:43:32 GMT  (275kb)", "http://arxiv.org/abs/1310.5715v2", "16 pages, 6 figures"], ["v3", "Thu, 20 Mar 2014 16:51:23 GMT  (278kb)", "http://arxiv.org/abs/1310.5715v3", "16 pages, 6 figures"], ["v4", "Thu, 27 Nov 2014 05:10:09 GMT  (277kb)", "http://arxiv.org/abs/1310.5715v4", "22 pages, 6 figures"], ["v5", "Fri, 16 Jan 2015 17:11:24 GMT  (71kb)", "http://arxiv.org/abs/1310.5715v5", "22 pages, 6 figures"]], "COMMENTS": "16 pages, 6 figures", "reviews": [], "SUBJECTS": "math.NA cs.CV cs.LG math.OC stat.ML", "authors": ["deanna needell", "rachel ward", "nathan srebro"], "accepted": true, "id": "1310.5715"}, "pdf": {"name": "1310.5715.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["(dneedell@cmc.edu).", "(nati@ttic.edu)."], "sections": [{"heading": null, "text": "ar Xiv: 131 0.57 15v5 [m. ath. NA] 1 6Ja n20 15Our results are based on a link we establish between SGD and the Kaczmarz Randomized Algorithm, which allows us to transfer ideas between the various bodies of literature that study each of the two methods. In particular, we revise the Kaczmarz Randomized Algorithm as an example of SGD and apply our results to prove its exponential convergence, but to solve a problem with weighted least squares instead of the original least squares problem. Subsequently, we present a modified Kaczmarz algorithm with partially distorted sampling that converges to the original solution at the same exponential convergence rate."}, {"heading": "1. INTRODUCTION", "text": "This paper combines two algorithms that have so far remained remarkably inconsistent in the literature: the randomized Kaczmarz algorithms for solving linear systems and the stochastic gradient method (SGD) for optimizing a convex target with unbiased gradient estimates. The connection allows us to make contributions by highlighting a method for minimizing a convex objective F (x) as the gateway to unbiased stochastic gradient estimates, i.e. an estimate of gradients at a given point x, so that E (g).Viewing F (x) as an expectation F (x) as an expectation F (x)."}, {"heading": "2. SGD FOR STRONGLY CONVEX SMOOTH OPTIMIZATION", "text": "We consider the problem of minimizing a smooth convex function, (2.1) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2) x (2.2"}, {"heading": "3. IMPORTANCE SAMPLING", "text": "We will now consider the stochastic distribution D (w), where gradient estimates are scanned by a weighted distribution. (W) We will consider the stochastic distribution D (W). (W) We will consider the stochastic distribution D (W). (W) We will consider the distribution D (W). (W) We will consider the distribution D (W). (W) We will consider the distribution D (W). (W) We will consider the distribution D (W). (W) We will consider the distribution D (W). (W) We will consider the distribution D (W). (W) We will consider the distribution D (W). (W) We will consider the distribution D (W)."}, {"heading": "4. IMPORTANCE SAMPLING FOR SGD IN OTHER SCENARIOS", "text": "In recent years, we have been looking at the question of whether we will be able to solve the distribution issue, and whether we can reduce the distribution differences between the sexes and between the sexes and between the sexes and between the sexes and between the sexes. (...) We will reduce the distribution differences between the sexes and between the sexes. (...) We will not necessarily be strongly convex, but then not necessarily strongly convex, then (...) we will reduce the distribution differences between the sexes and between the sexes. (...) We will have F (...) an appropriate mean of the sexes. (...) We will have F (...) an appropriate mean of the sexes. (...) We will have F (...) an appropriate mean of the sexes. (...)"}, {"heading": "5. THE LEAST SQUARES CASE AND THE RANDOMIZED KACZMARZ METHOD", "text": "A special case of interest is the smallest problem where (5,1) F (x) = 12n (A) = 1 (1) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2) = 1 (2).2 (2).2 (2) = 1 (2).2 (2) = 1 (2).2 (2).2 (2).4 (2).4 (2).4 (2).4 (2).4 (2).4 (2).4 (2).4 (2).4 (2).2 (2).6 (2) = 1 (2).6 (2).6 (2) = 1).2 (2).2 (2).4 (2).4 (2).4 (2).4 (2).6 (2) (2).4 (2).4 (2).4 (2).4 (2).4 (2 (2).4 (2).4 (2).4 (2).4 (2).4 (2).4 (2).4 (2 (2).4 (2).4 (2).4 (2).4 (2 (2).4 (2).4 (2 (2).4 (2).4 (2).4 (2).4 (2).4 (2).4 (2 (2).4 (2).4 (2 (2).4 (2).4 (2).4 (2).4 (2).4 (2 (2).6 (2).6 (2 (2).6 (2).6 (2).6 (2).6 (2).6 (1 (2).6 (2).6 (2).6 (2).6 (2).6 (2).6 (1 (2).6 ("}, {"heading": "6. NUMERICAL EXPERIMENTS", "text": "In this section, we present some numerical results for the different types of matrices. We will consider five types of systems below a system, each of which we have described using a 1000x sampling system. In this section, we will present some numerical results for the randomized Kaczmarz algorithms, each with different sampling models, i.e. applying Algorithm 3.1 to the smallest squares Problem F (x) = 1 2 2-Ax -b-22 (i.e. fi (x) = n2 (< ai, x > \u2212 bi) 2) and taking into account the smallest squares (0, 1). Let us remember that \u03bb = 0 corresponds to the randomized Kaczmarz algorithm, which depends on Strohmer and Vershynin with fully weighted sampling method [46]. \u03bb =.5 corresponds to the partially biased randomized Kaczmarz algorithms, which are sketched in corollary 5.3 on the role of the squares, the behavior of the algorithm, the algorithm for the algorithm, and the algorithm for the residuces."}, {"heading": "7. SUMMARY AND OUTLOOK", "text": "We consider this paper as three contributions: the improved dependence on conditioning for smooth and strongly convex SGD, the discussion of the importance sampling for SGD, and the link between SGD and the randomized Kaczmarz method. For the sake of simplicity, we considered SGD iterates only with a fixed increment \u03b3. This is sufficient to obtain the optimal iteration complexity if the accuracy \u03b5 is known in advance, which was our approach in this work. It is easy to adjust the analysis using standard techniques to include decreasing increments that are appropriate when we do not know in advance. We suspect that the assumption of strong convexity can be weakened to a limited strong convexity [21, 50] without changing any of the results of this work; we leave this analysis to future work. Case 1: Case 2: Our discussion of important sampling is limited to a static reweighting of the distribution of the sampling."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank the anonymous reviewers for their useful feedback, which significantly improved the manuscript. We thank Chris White for pointing out a simplified proof of Corollary 2.2. DN was supported in part by a grant from the Simons Foundation Collaboration, NSF CAREER # 1348721 and an Alfred P. Sloan Fellowship. NS was partially supported by a Google Research Award. RW was partially supported by ONR Grant N00014-12-1-0743, an AFOSR Young Investigator Program Award and an NSF CAREER Award."}, {"heading": "APPENDIX A. PROOFS", "text": "Our main results relate to an elementary fact about smooth functions with Lipschitz. \u2212 k \u2212 k \u2212 k < k \u2212 k < k (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-cocivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity) (co-coercivity)."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "We obtain an improved finite-sample guarantee on the linear convergence of stochastic<lb>gradient descent for smooth and strongly convex objectives, improving from a quadratic dependence<lb>on the conditioning (L/\u03bc) (where L is a bound on the smoothness and \u03bc on the strong convexity)<lb>to a linear dependence on L/\u03bc. Furthermore, we show how reweighting the sampling distribution<lb>(i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear<lb>dependence in the average smoothness, dominating previous results. We also discuss importance<lb>sampling for SGD more broadly and show how it can improve convergence also in other scenarios.<lb>Our results are based on a connection we make between SGD and the randomized Kaczmarz<lb>algorithm, which allows us to transfer ideas between the separate bodies of literature studying each<lb>of the two methods. In particular, we recast the randomized Kaczmarz algorithm as an instance of<lb>SGD, and apply our results to prove its exponential convergence, but to the solution of a weighted<lb>least squares problem rather than the original least squares problem. We then present a modified<lb>Kaczmarz algorithm with partially biased sampling which does converge to the original least squares<lb>solution with the same exponential convergence rate.<lb>", "creator": "LaTeX with hyperref package"}}}