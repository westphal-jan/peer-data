{"id": "1310.6007", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2013", "title": "Efficient Optimization for Sparse Gaussian Process Regression", "abstract": "We propose an efficient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-of-art performance in discrete cases and competitive results in the continuous case.", "histories": [["v1", "Tue, 22 Oct 2013 18:44:29 GMT  (217kb,D)", "https://arxiv.org/abs/1310.6007v1", "To appear in NIPS 2013"], ["v2", "Tue, 5 Nov 2013 05:13:30 GMT  (529kb,D)", "http://arxiv.org/abs/1310.6007v2", "To appear in NIPS 2013"], ["v3", "Mon, 11 Nov 2013 08:21:58 GMT  (529kb,D)", "http://arxiv.org/abs/1310.6007v3", "To appear in NIPS 2013"]], "COMMENTS": "To appear in NIPS 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yanshuai cao", "marcus a brubaker", "david j fleet", "aaron hertzmann"], "accepted": true, "id": "1310.6007"}, "pdf": {"name": "1310.6007.pdf", "metadata": {"source": "CRF", "title": "Efficient Optimization for Sparse Gaussian Process Regression", "authors": ["Yanshuai Cao", "Marcus A. Brubaker", "David J. Fleet", "Aaron Hertzmann"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Gaussian Process (GP) learning and inference are computationally prohibitive with large datasets, having time complexitiesO (n3) andO (n2), where n is the number of training points. Sparsification algorithms exist that scale linear in the training set size (see [9] for a review). They construct a low approximation to the GP covariance matrix across the entire dataset with a small set of inducing points. Some approaches choose the induction of points from training points [6, 7, 11, 12] but these methods choose the inducing points with ad hoc criteria; i.e. they use various objective functions to select inducing points and optimize GP hyperparameters. Powerful sparsification methods [13, 14, 15] use a single objective function and allow points to move freely across the input domains learned via gradient lineage."}, {"heading": "1.1 Previous Work", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "2 Sparse GP Regression", "text": "Let y-R be the noise output of a function, f, the input x. Let y-Rn denote the corresponding vector of the training results. Under a complete zero mean function, with the covariance function, the variance of observation noise is the predictive distribution over the output f? at a test point x? is normally distributed. (X?) T (K + digit 2) is the variance of observation noise, the predictive distribution over the output f? at a test point x? is normally distributed, the variance of the predictive distribution can be associated. (X?) T (K + digit 2In) \u2212 1 yv2?"}, {"heading": "3 Efficient optimization", "text": "We now outline our algorithm for optimizing the free variation energy (5) to select the inducing set I and the hyperparameters \u03b8. (The negative log probability (4) is similarly minimized by simply discarding the electric motor concept.) The algorithm is a form of hybrid coordinate descent alternating between discrete optimization of the inducing points and continuous optimization of the hyperparameters. We first describe the algorithm for selecting the inducing points and then discuss continuous hyperparameter optimization and termination criteria in paragraph 3.4.Finding optimal inducing sets is a combinatorial problem; global optimization is insolvable. Instead, the inducing set is initialized to a random subset of training data, which is then refined with each iteration by a fixed number of swap updates."}, {"heading": "3.1 Factored representation", "text": "To support an efficient evaluation of the objective and the swap, we use a factor representation of the kernel matrix = > Q >. For an inducing set of k points, the low-value Nystro-M approximation to the kernel matrix (equation 3) can be expressed in the form of a partial cholesky factorization: K-K [:, I] -K [I, I] \u2212 1K [I,] = L (I) >, (6) where L (I) - Rn \u00b7 k is a lower trapezoid matrix until the lines are permutated (until the lines are permutated) (lower trapezoid matrix (i.e., until the line is permutated). The derivative of equation 6 follows from equation 1 in [1] and the fact that due to the ordered order of the pivots I, the variable cholesky factorization > > factorization is uniquered by Estabilization (using EQ and WoodQ)."}, {"heading": "3.2 Factorization update", "text": "Here we present the mechanics of the swap update algorithm, see [3] for pseudo-codings. Suppose we exchange the inducing point i for the candidate point j in Im, the inducing theorem of magnitude m. We first modify the factor matrices to remove the point i from Im, i.e. to reduce the factors. Then we update all key terms with a step of cholesky and QR factorization with the new point j. Downdating to remove the inducing point i, we must factorize the corresponding columns / rows to the farthest right columns of L, Q, R and to the last row of R. We can then simply discard these last columns and rows and modify related quantities. If we change the order of the inducing points, the underlying GP model is invariant, but the matrices in the factorized representation are not."}, {"heading": "3.3 Evaluating candidates", "text": "Next, we will show how to select candidates for inclusion in the inductive set. We will first derive the exact change of the lens due to the addition of an element to Im \u2212 1. Later, we will make an approximation of this objective change, which can be efficiently calculated. In view of an inductive set Im \u2212 1 and matrices L-m \u2212 1, Qm \u2212 1 and Rm \u2212 1, we would like to evaluate the change in Equation 5 for Im = Im \u2212 1% year, i.e., based on the mechanics of the above incremental updates, it can be shown that \"ED\" = \"2 (y) (I-Qm \u2212 1Q > m \u2212 1),\" (I-Qm \u2212 1), \"2 / 3 (I-Qm \u2212 1), (I-Qm > EV) / 2 (14),\"."}, {"heading": "3.3.1 Fast approximate cost reduction", "text": "While O (mn2) is prohibitive, calculating the exact change is not required. > > We only need a ranking of the best few candidates, so instead of accurately evaluating the change in the target, we use an efficient approximation based on a small number, e.g., of training points that provide information about the remaining points between the current low-rated covariance matrix (based on inducing points) and the full covariance matrix. \u2212 After this approximation, we propose a candidate, we use the actual target to decide whether to include it. Techniques below reduce the complexity of evaluating all n \u2212 m candidates to O (zn). \u2212 The change in the target for a candidate must be in the new column of the updated Cholesky factorization'm,' m. In Eq. (13) this vector is a (normalized) column of the remaining K \u2212 K \u2212 K \u2212 m that relates to the reproducing candidates to zn (O)."}, {"heading": "3.3.2 Ensuring a good approximation", "text": "To ensure a good approximation, the CSI algorithm [1] greedily selects points to find an approximation of the remaining K \u2212 K \u00b2 m \u2212 1 in Equation (13) that is optimal with respect to a boundary of the orthogonal standard, but the goal is to approximate equivalents (14) - (16). By analyzing the role of the residual matrix, we see that the information pivots provide a slight approximation to the orthogonal complement of the space that extends beyond the current inducing quantity. Typically, with a fixed set of information pivots, portions of this sub-space can never be captured, suggesting that we occasionally update the entire set of information pivots. Although information pivots are changed when shifted into the inducing quantity, we empirically find that this is not insufficient. < instead, we periodically replace the entire amount of information."}, {"heading": "3.4 Hybrid optimization", "text": "The total hybrid optimization method performs block descent in the inducing points and continuous hyperparameters, alternating between discrete and continuous phases until the improvement of the target is below a threshold or the computing time budget is exhausted. In the discrete phase, inducing points are considered for exchange with the specified hyperparameters. By factorizing and efficient candidate evaluation above, an inducing point i-Im2Both can be further reduced by appropriate caching during the Q, R and L updates, and Lzproceeds as follows: (I) factoring the matrices as in Sec. 3.2 to remove i; (II) the true objective functional value Fm \u2212 1 over the run-down model with Im- {i}, whereby (11), (12) and (9), select a replacement candidate with the rapid cost change of Sec."}, {"heading": "4 Experiments and analysis", "text": "For the following experiments we learn together to induce points and hyperparameters, which is a more difficult task than inducing points with known hyperparameters [11, 13]. Except for the 1D example, the number of induced points exchanged per epoch is min (60, m). The maximum number of function evaluations per epoch in CG hyperparameter optimization is min (20, max (15, 2d), where d is the number of continuous hyperparameters. Empirically, we find the algorithm robust against changes in these limits. We use two performance measures, (a) standardized mean square probability (SMSE), 1N \u03a3 N = 1 (y-t \u2212 yt) 2 / \u0394, where \u043c 2 \u0445 is the sample variance of test outputs {yt}, and (2) standardized negative protocol probability (SNLP), defined in [10]."}, {"heading": "4.1 Discrete input domain", "text": "In fact, it is such that we will be able to move into another world, in which we are able to put ourselves into another world, in which we are able to put ourselves into another world, in which we are able, in which we are able, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we live, in which we live, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, in which we, in which we, in which we, we, in which we, we, in which we, in which we, we, in which we, we, in which we, we, in which we, we, we, in which we, we, we, in which we, we, we, we, in which, we, we, we, we, we, we, we, in which, we, we, in which, we, we, we, we, we, in"}, {"heading": "4.2 Continuous input domain", "text": "Although CholQR was developed for discrete input domains, it can be competitive on continuous domains. To this end, we compare SPGP [13] and IVM [7] using RBF cores with a longitudinal scale parameter per input dimension. We show results from both the PP log probability and variational targets generated by MLE and VAR. We use the 1D dataset of [13] to show how the PP probability is easily trapped in local minima in gradient-based optimization of inducing points. Fig. 4 (a) and 4 (d) show that our algorithm for these datasets is not involved in a negative SPAR-SM probability (as in Fig. 1c of [13])."}, {"heading": "5 Conclusion", "text": "We describe an algorithm for selecting inductive points for the sparsification of the Gaussian process. It basically optimizes objective functions and is applicable to discrete domains and non-differentiable nuclei. For such problems, it proves to be just as good or better than competing methods, and for methods whose predictive behavior is similar, our method is several orders of magnitude faster. For continuous domains, the method is competitive."}], "references": [{"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "ICML, pp. 33\u201340,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Twin gaussian processes for structured prediction", "author": ["L. Bo", "C. Sminchisescu"], "venue": "IJCV, 87:28\u2013", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Project page: supplementary material and software for efficient optimization for sparse gaussian process regression", "author": ["Y. Cao", "M.A. Brubaker", "D.J. Fleet", "A. Hertzmann"], "venue": "www.cs. toronto.edu/ \u0303caoy/opt_sgpr,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Sparse on-line gaussian processes", "author": ["L. Csat\u00f3", "M. Opper"], "venue": "Neural Comput.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "IEEE CVPR, pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "A matching pursuit approach to sparse gaussian process regression", "author": ["S.S. Keerthi", "W. Chu"], "venue": "NIPS 18,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Fast sparse gaussian process methods: The informative vector machine", "author": ["N.D. Lawrence", "M. Seeger", "R. Herbrich"], "venue": "NIPS 15,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Libpmk: A pyramid match toolkit", "author": ["J.J. Lee"], "venue": "TR: MIT-CSAIL-TR-2008-17, MIT CSAIL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A unifying view of sparse approximate gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1939}, {"title": "Gaussian processes for machine learning. Adaptive computation and machine learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Fast forward selection to speed up sparse gaussian process regression", "author": ["M. Seeger", "C.K.I. Williams", "N.D. Lawrence", "S.S. Dp"], "venue": "AI & Stats", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Sparse greedy gaussian process regression", "author": ["A.J. Smola", "P. Bartlett"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Sparse gaussian processes using pseudo-inputs", "author": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "NIPS 18,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Variational learning of inducing variables in sparse gaussian processes", "author": ["M.K. Titsias"], "venue": "JMLR, 5:567\u2013574,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sparse multiscale gaussian process regression", "author": ["Christian Walder", "Kwang In Kim", "Bernhard Sch\u00f6lkopf"], "venue": "ICML pp. 1112\u20131119,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "Sparsification algorithms exist that scale linearly in the training set size (see [9] for a review).", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 6, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 10, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 11, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 12, "context": "More powerful sparsification methods [13, 14, 15] use a single objective function and allow inducing points to move freely over the input domain which are learned via gradient descent.", "startOffset": 37, "endOffset": 49}, {"referenceID": 13, "context": "More powerful sparsification methods [13, 14, 15] use a single objective function and allow inducing points to move freely over the input domain which are learned via gradient descent.", "startOffset": 37, "endOffset": 49}, {"referenceID": 14, "context": "More powerful sparsification methods [13, 14, 15] use a single objective function and allow inducing points to move freely over the input domain which are learned via gradient descent.", "startOffset": 37, "endOffset": 49}, {"referenceID": 13, "context": "Notably, it optimizes either the marginal likelihood, or a variational free energy [14], exploiting the QR factorization of a partial Cholesky decomposition to efficiently approximate the covariance matrix.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "On continuous domains it is competitive with the Pseudo-point GP [13] (SPGP).", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 6, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 10, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 11, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 6, "context": ", [7, 11] take an information theoretic perspective), they are greedy and do not use the same objective to select inducing points and to estimate GP hyperparameters.", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", [7, 11] take an information theoretic perspective), they are greedy and do not use the same objective to select inducing points and to estimate GP hyperparameters.", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": "The variational formulation of Titsias [14] treats inducing points as variational parameters, and gives a unified objective for discrete and continuous inducing point models.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "In the discrete case, our method optimizes the same variational objective of Titsias [14], but is a significant improvement over greedy forward selection using the variational objective as selection criteria, or some other criteria.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In particular, given the cost of evaluating the variational objective on all training points, Titsias [14] evaluates the objective function on a small random subset of candidates at each iteration, and then select the best element from the subset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "The approach in [14] also uses greedy forward selection, which provides no way to refine the inducing set after hyperparameter optimization, except to discard all previous inducing points and restart selection.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Our low-rank decomposition is inspired by the Cholesky with Side Information (CSI) algorithm for kernel machines [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "To lower this cost with a sparse approximation, Csat\u00f3 and Opper [4] and Seeger et al.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "[11] proposed the Projected Process (PP) model, wherein a set of m inducing points are used to construct a low-rank approximation of the kernel matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": ", n}, this approach amounts to replacing the kernel matrix K with the following Nystr\u00f6m approximation [10]: K ' K\u0302 = K[:, I]K[I, I]\u22121K[I, :] (3) where K[:, I] denotes the sub-matrix of K comprising columns indexed by I, and K[I, I] is the sub-matrix of K comprising rows and columns indexed by I.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "The objective in (4) can be viewed as an approximate log likelihood for the full GP model, or as the exact log likelihood for an approximate model, called the Deterministically Trained Conditional [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 13, "context": "The same PP model can also be obtained by a variational argument, as in [14], for which the variational free energy objective can be shown to be Eq.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "The kernel machine of [1] also uses a regularizer of the form \u03bb tr(K\u2212K\u0302), however \u03bb is a free parameter that is set manually.", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "6 follows from Proposition 1 in [1], and the fact that, given the ordered sequence of pivots I, the partial Cholesky factorization is unique.", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "The inducing set can be incrementally constructed, as in [1], however we found no benefit to this.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "2 Factorization update Here we present the mechanics of the swap update algorithm, see [3] for pseudo-code.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "This is done with the efficient pivot permutation presented in the Appendix of [1], with minor modifications to account for the augmented form of L\u0303.", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "To ensure a good approximation, the CSI algorithm [1] greedily selects points to find an approximation of the residual K\u2212 K\u0302m\u22121 in Eq.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "We find this works better than optimizing the information pivots as in [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "For the experiments that follow we jointly learn inducing points and hyperparameters, a more challenging task than learning inducing points with known hyperparameters [11, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 12, "context": "For the experiments that follow we jointly learn inducing points and hyperparameters, a more challenging task than learning inducing points with known hyperparameters [11, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 9, "context": "We use two performance measures, (a) standardized mean square error (SMSE), 1 N \u03a3 N t=1(\u0177t \u2212 yt)/\u03c3\u0302 \u2217, where \u03c3\u0302 \u2217 is the sample variance of test outputs {yt}, and (2) standardized negative log probability (SNLP) defined in [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 13, "context": "Because continuous relaxation methods are not applicable, we compare to discrete selection methods, namely, random selection as baseline (Random), greedy subset-optimal selection of Titsias [14] with either 16 or 512 candidates (Titsias-16 and Titsias-512), and Informative Vector Machine [7] (IVM).", "startOffset": 190, "endOffset": 194}, {"referenceID": 6, "context": "Because continuous relaxation methods are not applicable, we compare to discrete selection methods, namely, random selection as baseline (Random), greedy subset-optimal selection of Titsias [14] with either 16 or 512 candidates (Titsias-16 and Titsias-512), and Informative Vector Machine [7] (IVM).", "startOffset": 289, "endOffset": 292}, {"referenceID": 2, "context": "Care is taken to ensure consist initialization and termination criteria [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "In the second task, from [2], the task is to predict 3D human joint position from histograms of HoG image features [5].", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "In the second task, from [2], the task is to predict 3D human joint position from histograms of HoG image features [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "Because our goal is the general purpose sparsification method for GP regression, we make no attempt at the more difficult problem of modelling the multivariate output structure in the regression as in [2].", "startOffset": 201, "endOffset": 204}, {"referenceID": 7, "context": "Instead, we predict the vertical position of joints independently, using a histogram intersection kernel [8], having four hyperparameters: one noise variance, and three data variances corresponding to the kernel evaluated over the HoG from each of three cameras.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "We select and show result on the representative left wrist here (see [3] for others joints, and more details about the datasets and kernels used).", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "3(c) and 3(f) show the trade-off between the test SMSE and training time for variants of CholQR, with baselines and CSI kernel regression [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "For CholQR we consider different numbers of information pivots (denoted z8, z16, z64 and z128), and different strategies for their selection including random selection, optimization as in [1] (denote OI) and adaptively growing the information pivot set (denoted AA, see [3] for details).", "startOffset": 188, "endOffset": 191}, {"referenceID": 2, "context": "For CholQR we consider different numbers of information pivots (denoted z8, z16, z64 and z128), and different strategies for their selection including random selection, optimization as in [1] (denote OI) and adaptively growing the information pivot set (denoted AA, see [3] for details).", "startOffset": 270, "endOffset": 273}, {"referenceID": 12, "context": "To that end, we compare to SPGP [13] and IVM [7], using RBF kernels with one lengthscale parameter per input dimension; \u03ba(xi,xj) = c exp(\u22120.", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "To that end, we compare to SPGP [13] and IVM [7], using RBF kernels with one lengthscale parameter per input dimension; \u03ba(xi,xj) = c exp(\u22120.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "We use the 1D toy dataset of [13] to show how the PP likelihood with gradient-based optimization of inducing points is easily trapped in local minima.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "1c of [13]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "Finally, we compare CholQR to SPGP [13] and IVM [7] on a large dataset.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "Finally, we compare CholQR to SPGP [13] and IVM [7] on a large dataset.", "startOffset": 48, "endOffset": 51}, {"referenceID": 12, "context": "For SPGP we use the implementation of [13].", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "This disparity between the SMSE and SNLP measures for CholQR-MLE is consistent with findings about the PP likelihood in [14].", "startOffset": 120, "endOffset": 124}], "year": 2013, "abstractText": "We propose an efficient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-ofart performance in discrete cases and competitive results in the continuous case.", "creator": "LaTeX with hyperref package"}}}