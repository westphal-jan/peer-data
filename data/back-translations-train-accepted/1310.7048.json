{"id": "1310.7048", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2013", "title": "Scaling SVM and Least Absolute Deviations via Exact Data Reduction", "abstract": "The support vector machine (SVM) is a widely used method for classification. Although many efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result, the number of data instances to be entered into the optimization can be substantially reduced. Some appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run the screening, whose computational cost is negligible compared to that of solving the SVM problem; (3) DVI is independent of the solvers and can be integrated with any existing efficient solvers. We also show that the DVI technique can be extended to detect non-support vectors in the least absolute deviations regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD. We have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly outperforms the existing state-of-the-art screening rules for SVM, and is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.", "histories": [["v1", "Fri, 25 Oct 2013 23:01:52 GMT  (167kb,D)", "http://arxiv.org/abs/1310.7048v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jie wang", "peter wonka", "jieping ye"], "accepted": true, "id": "1310.7048"}, "pdf": {"name": "1310.7048.pdf", "metadata": {"source": "CRF", "title": "Scaling SVM and Least Absolute Deviations via Exact Data Reduction", "authors": ["Jie Wang", "Peter Wonka", "Jieping Ye"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Basics and Motivations", "text": "In this section we examine the SVM and LAD problems within the framework of a single framework. Then we motivate the general screening rules about the KKT conditions. Consider the convex optimization problems of the following form: min w < n1 2, 2, 2, 2, 3, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "3 Estimation of the Dual Optimal Solution", "text": "For problem (12), we assume that we are given two parameter values 0 < C0 < C0 > inequalities (C0). The main method we use is the so-called variable inequality. For completeness, we give the definition of variable inequalities as a consequence. Theorem 5 [12] Let A < n be a convex set, and let h be a Ga teaux differentiation function on an open proposition containing A. If x * is a local minimization from h to A, then < x \u2212 h (x), x \u2212 x < n be a convex set, and let h be a Ga teaux differentiation function on an open proposition containing A. If x * * is a local minimization from h to A, then < x \u2212 x > qualities on A (18) The following theorem leads to Za (18)."}, {"heading": "4 The Proposed DVI Rules", "text": "Given C > C0 > C2 \"and\" Z \"(C0), we can < K\" (C) over theorem 6 \"(R1\"), (R2 \") and theorem (6) develop the basic screening rule for problem (3), as summarized in the following theorem: Theorem 7.\" (DVI) For problem (12), we assume that we are given the answer to the question (C0). (Then we have for each C > C0. \"(C)\" i, \"i.e., i\" R, \"if the following holdsC + C0\" < Z \"T\" (C0), aixi. \"(C0), aixi\" Z. \""}, {"heading": "5 Screening Rules for SVM", "text": "In Section 5.1, we first present the sequential DVI rules for SVM based on the results in Section 4. Then, in Section 5.2, we show how to strictly improve SSNSV [20] using the same technology used in DVI."}, {"heading": "5.1 DVI rules for SVM", "text": "In view of a series of observations (xi, yi) li = 1, where xi and yi (xi, \u2212 1) are the data instance and the corresponding class name, the SVM takes the form: min w1 2, 0, 2 + C l, i = 1 [1 \u2212 wT (yixi)] +. (24) It is easy to see that if we make \"t\" = [t] + and \u2212 ai = yi, problem (3) becomes the SVM problem. To construct the DVI rules for SVM by corollariums 8 and 9, we only need to find \"n\" and \u03b2. \"In fact, we have the following result: Lemma 10. If we leave\" t \"= [t] +, then the problem (3) becomes the SVM problem. (s) = 1, i.e. (s). (25) We do without the proof for Lemma 10, since it is a direct application of Eq. (1) Then we immediately have the following screening rules for the SVM problem."}, {"heading": "5.2 Improving the existing method", "text": "In the rest of this section, we briefly describe how to strictly improve the SSNSV [20] using the same technique used in the DVI rules (see the supplement for more details) \u2212 In view of Equation (13), (R1) and (R2), it can be described as: min w, (C) < w, x, x, i > > 1, (R2), which is a quantity specified by w, (C) (indicating that we have already specified \u2212 ai = bi = yi, \u03b1 = 0 and \u03b2 = 1). It is easy to see that the smaller equation, the narrower equation, the boundaries, are in (R1) and (R2), [Sxi], [Sxi] and (SIS), [SIS] in the region."}, {"heading": "6 Screening Rules for LAD", "text": "In this section, we extend the DVI rules in section 4 to the smallest absolute deviations < < k > for the smallest absolute deviations (LAD). Suppose we have a training set (xi, yi) li = 1, where xi (xi) < n and yi (xi) <. The LAD problem takes the form of min w1 2 + C l \u00b2 2 + C l \u00b2 i = 1 | yi \u2212 wTxi |. (29) We can see that if we (t) = | t | and \u2212 ai = bi = 1, problem (3) becomes the LAD problem. In order to construct the DVI rules for LAD on the basis of corollariums 8 and 9, we have to find the result and \u03b2. In fact, we have the following result: Lemma 13. Let us (t) = | t |, then \u03b1 = 1 and \u03b2 = 1, i.e., we have the DVI rules (s) = DVs (1) of [Lemma = 1] since it is a direct reference to Ema (1)."}, {"heading": "7 Experiments", "text": "We evaluate DVI rules for both synthetic and real data sets. To measure the performance of the screening rules, we calculate the rejection rate, i.e. the ratio between the number of data instances whose membership can be identified by the rules, and the total number of data instances. We test the rules using a sequence of 100 parameters of C [10 \u2212 2, 10] at the same distance in the logarithmic scale. In Section 7.1, we compare the performance of the DVI rules with SSNSV [20], the only existing method for identifying unsupported vectors in SVM. Note that both DVI rules and SSNSV are safe in the sense that no support vectors are wrongly discarded. We then evaluate the DVI rules for LAD in Section 7.2."}, {"heading": "7.1 DVI for SVM", "text": "In this experiment, we first apply DVIs to three simple 2D synthetic datasets to illustrate the effectiveness of the proposed screening methods."}, {"heading": "7.2 DVI for LAD", "text": "In this experiment, we evaluate the performance of DVIs for LAD using three real datasets: (a) Magic Gamma Telescope datasets [2]; (b) Computer datasets [25]; (c) Houses datasets [21]. Fig. 3 shows the rejection ratio of DVIs rules for the three datasets. We can find that the rejection ratio of DVIs for Magic Gamma Telescope datasets is about 90%, resulting in a tenfold acceleration, as in Table 3. For Computer and Houses datasets, we can find that the rejection rates are very close to 100%, i.e. almost all members of the data instances can be determined by the DVIs rules. As expected, Table 3 shows that the resulting acceleration is about 20 or 115 times."}, {"heading": "8 Conclusion", "text": "In this paper, we develop new screening rules for a class of monitored learning problems by examining their dual formulation with the varying inequalities. Our framework includes two well-known models, i.e. SVM and LAD, as special cases. The proposed DVI rules are very effective in identifying non-supporting vectors for both SVM and LAD, and thus result in significant savings in computing costs and memory. Extensive experiments on both synthetic and real data sets demonstrate the effectiveness of the proposed DVI rules. We plan to extend DVI's framework to other monitored learning problems, e.g. weighted SVM [32], RWLS (robustly weighted least squres) [6], robust PCA [9], robust matrix factoring [19]."}, {"heading": "A Proof of Lemma 2", "text": "Before proving Lemma 2, we will quote the following technical Lemma 16. [15] The function f is equal to its biconjugate f, if and only if f, 0 (< n). We are now ready to derive a simple proof for Lemma 2, which is based on Lemma 16.Proof. to show that it is sufficient, according to Lemma 16. Therefore, we only have to check the following three conditions: 1). Property: because there is no < < +, i.e., there is no < so that the (t) is finite, it is appropriate. 2) Lower Semi-Continuity: It is lower because it is continuous. 3) Convexity: The convexity of flatness is due to its sublinearity, see definition 1. Thus, we have a lower semi-continuity: It is lower because it is continuous. 3) Convexity: The convexity of flatness is due to its sublinearity, see 1."}, {"heading": "B Proof of Lemma 3", "text": "s as\u03c3Z (s): = sup x ZsTx, (31) and the indicator function \u03b9Z (x) = {0 if x'Z, \u221e, otherwise. (32) The support function of the nonempty closed convex setS\u03c3: = < n: sTd. (d), d < n}. (34) We are now ready to prove that Lemma 3.Proof. Based on Lemma 17 and Theorem 18, we can see that there is a non-empty convex set Z < < n} we are ready to prove this."}, {"heading": "C Derivation of the KKT Condition in Eq. (14)", "text": "The problem in (12) can be written as follows: min \u03b8C 2 > ZT \u03b8 2 \u2212 < y \u0432, \u03b8 >, (39) s.t. successi, i = 1,.., l.For this reason we can see that the Lagrange Island L (\u03b8, \u00b5, \u03bd) = (C2,., \u00b5l) T, \u03bd = (\u03bd1,.. \u2212 xi) T and \u00b5i \u2265 0 for all i = 1,.., l. \u00b5 and \u03bd is actually the vector of the Lagranger multipliers. For simplicity, let us call the situation in (C) equal, then the KT conditions [3] are equal."}, {"heading": "D Proof of Lemma 4", "text": "The first part of the Declaration shall be as follows: (a) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c)))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>The support vector machine (SVM) is a widely used method for classification. Although many<lb>efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-<lb>scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting<lb>classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-<lb>support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result,<lb>the number of data instances to be entered into the optimization can be substantially reduced. Some<lb>appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded<lb>by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run<lb>the screening, whose computational cost is negligible compared to that of solving the SVM problem; (3)<lb>DVI is independent of the solvers and can be integrated with any existing efficient solvers. We also show<lb>that the DVI technique can be extended to detect non-support vectors in the least absolute deviations<lb>regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD. We<lb>have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly<lb>outperforms the existing state-of-the-art screening rules for SVM, and is very effective in discarding<lb>non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.", "creator": "LaTeX with hyperref package"}}}