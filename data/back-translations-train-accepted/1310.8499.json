{"id": "1310.8499", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2013", "title": "Deep AutoRegressive Networks", "abstract": "We introduce a multilayer deep generative model capable of learning hierarchies of sparse distributed representations from data. The model consists of several layers of stochastic units, with autoregressive connections within each layer, which allows for efficient exact sampling. We train the model efficiently using an algorithm derived from the Minimum Description Length (MDL) principle, which minimizes the amount of information contained in the joint vector of data and hidden unit configurations for the training set. As we are not given the hidden unit configurations corresponding to the training data, we use a feedforward network to map data vectors to configurations of hidden units that are jointly probable with them and train it jointly with the model. Our approach can also be seen as maximizing a lower bound on the log-likelihood, with the feedforward network implementing approximate inference.", "histories": [["v1", "Thu, 31 Oct 2013 13:47:30 GMT  (662kb,D)", "http://arxiv.org/abs/1310.8499v1", null], ["v2", "Tue, 20 May 2014 16:22:43 GMT  (207kb,D)", "http://arxiv.org/abs/1310.8499v2", "Appears in Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China, 2014"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["karol gregor", "ivo danihelka", "andriy mnih", "charles blundell", "daan wierstra"], "accepted": true, "id": "1310.8499"}, "pdf": {"name": "1310.8499.pdf", "metadata": {"source": "META", "title": "Deep AutoRegressive Networks", "authors": ["Karol Gregor", "Andriy Mnih", "Daan Wierstra"], "emails": ["KAROL@DEEPMIND.COM", "ANDRIY@DEEPMIND.COM", "DAAN@DEEPMIND.COM"], "sections": [{"heading": "1. Introduction", "text": "There is now a substantial literature in this area. A class of models based on undirected generative models includes examples such as Restricted Boltzmann Machines (RBMs; Smolensky, 1986), Deep Belief Networks (DBNs; Hinton et al., 2006), and Deep Boltzmann Machines (DBMs; Salakhutdinov & Hinton, 2009) These models were shown to produce good examples, but the iterative approach of Markov Chain Monte Carlo for both training and sampling makes sampling prohibitively slow for many applications. Another class of models, regulated autoencoders, 2013; Bengio et al; Bengio & Thibodeau-Laufer, 2013."}, {"heading": "2. Using models to compress data", "text": "A good model can describe any given observation compactly in terms of the states of its latent variables. Predicting the observation with the states of the latent variables generates a sequence that is easier to compress than the observation alone because the redundancy in the data is made more explicitly. The training of a model then simply boils down to adjusting its parameters to minimize the amount of information needed to encode the sequence. In this section we will explain how to measure these bits. The basic information unit is a bit. It is the amount of information we need to store to describe the state of a binary random variable, which is likewise likely to be 0 or 1. If one of the two values of the variable is much more likely than the other, we will be able to describe its state with less than one bit in the average 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}, {"heading": "3. Deep AutoRegressive Networks", "text": "After giving the abstract outline of the approach, we will apply it successively to a model with a single hidden layer, a deep model, and a model with partial connectivity. The models presented in this essay are meant to be applied to binary sequential data. If the input is not a binary sequence, we transform it into one. If the model is a binary image, we create a sequence by reading it from top left to bottom right, as if reading a page, but you can use any order."}, {"heading": "3.1. The general model", "text": "The model works as follows: 1) Apply a nonlinear transformation to a data vector x to create a new binary sequence h (x); concatenate the two sequences to s = s (x) = h (x), x. Note that the transformation need not be deterministic. 2) Create a model that predicts each bit in the sequence s from previous bits. That is, specify a series of functions gi such that pi \u2261 p (si = 1 | s1,.., si \u2212 1) = gi (s1,., si \u2212 1).3) Optimize the parameters of the functions h and g to make them better at predicting the sequence s by minimizing L (s) (1) using stochastic gradient departure (SGD) with gradients calculated by backward propagation."}, {"heading": "3.2. A one-hidden-layer instantiation", "text": "In this section we describe a single-layer instance of DARN. All the related calculations are also presented in Figure 1. For this model, the steps from the previous subsection are as follows: 1) The coding function h becomesh (x) = \u03b2 (\u03c3 (U \u00b7 x + b))) (2) where U is a weight matrix, b is a bias vector, \u03c3 (x) = 1 / (1 + exp (\u2212 x)))) is the logistic function and \u03b2 is a binary function which is either 1) stochastical: \u03b2 (p) = 1 with probability p and 0 otherwise, or 2) deterministic: \u03b2 (p) = 0 if p < 0,5 and \u03b2 (p) = 1 otherwise; 2) The bits of h (x) are predicted usingphi (h), x) i = 1,."}, {"heading": "3.3. A deep instantiation", "text": "In this section, we present a model with several hidden layers. The model is obtained from the general one by forming h (x) as a concatenation of the hidden layers. In the first step, a given layer is calculated from the bottom layer (s) / input using a nonlinear function. In the second step, the probability of encoding a bit in a given layer is calculated from the hidden layer (s) above and from the bits already predicted in the given layer (s). In the third step, the optimization is performed in an analogous manner as in the previous section. We assume that we have n hidden layers designated by ha and the input layer by h0 = x. Between the layers, it is easy to implement arbitrary differentiated functions, which we get by fa and ga for a = 1,.., n. The following are typical simple examples: f, g (y) = V (6) f, g (y) = W tanh (W \u00b7 y + W \u00b7 b \u00b7 b \u00b7 b (where) we get W \u00b7 b \u00b7 b)."}, {"heading": "3.4. Partial connectivity", "text": "We can better adapt the model scale to high-dimensional data by connecting units to only a subset of units in the adjacent layers (or, auto-regressively, to the same layer), which is especially useful for modeling large images. A common method of handling images is local connectivity, either fully convolutional (LeCun et al., 1998) or with little or no weight distribution. We use the periodic local connectivity of (Gregor & LeCun, 2010) in the large input example below. Such local connectivity is achieved by placing the hidden units on a two-dimensional grid \"above\" the image and connecting a given unit only to a neighborhood of inputs below it. If a weight distribution is desired, the weights are divided if their locations are shifted by a multiple of integer distances in each of the x- and y-directions of the matrix (see the matrix reference for details). This reduces to a matrix convolutionary algorithm, of course, if we connect both of the matrix to a partial matrix with a mesh of the matrix."}, {"heading": "4. Analysis", "text": "In this section we will first discuss the calculation costs of the model and then deal with the relationship to graphical models and the estimation of the data log probability."}, {"heading": "4.1. Computational cost", "text": "Dre rf\u00fc ide nree\u00fcdGn nvo dme rf\u00fc ide nree\u00fceerb rf\u00fc ide rf\u00fc ide nlrgne\u00fceaeFnln-eaJnlhsrgne\u00fceaeFnln ni rde nlrrgne\u00fceaeFnln ni nlrgne\u00fceaeFnlrh ni nde nlrgne\u00fceaeBnn ni rde rf\u00fc the nlrrrrf\u00fc-eaeaePnlrrmnlrmlllrso \"lrso\" lrso \"lrrrso\" lrrso \"lrrrrrf\u00fc\" eaeaeaeaeaeaeuuuuiuiuiuiuiuiuiuiSn-nln-nlrrrmlso-nlrrso \"lrso\" eaeaeaeaeaeaeaeaeaeaeaeaeaeeeeaeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeSn rso-rso-rso-rso-rso-rso-rrso-ru-eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeFr-Sn ru-r\u00fc r\u00fc ru-ru-ru-eFr-ru-ru-rso-ru-ru-eFr-nlu-eeeeeeeFnlu-eeeeeaeFnlu-"}, {"heading": "4.2. Relation to graphical models", "text": "DARN can be regarded as a generative model of the extended sequence s = h, x (the concatenation of the hidden layers and the input) predicting each bit from the previous ones. Of course, via this sequence, the model is parameterized as p (s) = p (s1) p (s2 | s1) p (s3 | s2) p (s3 | s2)., where each term is parameterized as a deterministic prediction network. Graphical models are typically defined by maximizing probability or equivalent by minimizing the negative log probability of the data L = \u2212 log (x). Here, p (x) is the probability of the input that p (x) = x (h) p (h) p (h) p (h) p (h) p (h) p (h) p (h) p (h)."}, {"heading": "5. Results", "text": "This year, it has reached the point where it will be able to take the lead, \"he said in an interview with the German Press Agency.\" We have never lost as much time as this year, \"he said.\" But we are not yet at the point where we can afford it. \""}, {"heading": "6. Conclusion", "text": "We have introduced a generative model that has the following desirable characteristics: 1) generation of high-quality samples; 2) fast generation of independent samples; 3) ability to learn a hierarchy of representations; 4) support for partial connectivity in handling high-dimensional data; 5) efficient training on GPUs using standard matrix operations. Although existing models meet some of these characteristics, none of them meet all of them. RBMs / DBNs, DBMs, and regulated auto encoders fulfill all of the characteristics except 2). Authoregressive input space models such as NADE are not sufficient 3) and 5). Although we have not been able to achieve state-of-the-art results for all datasets, sampling from fDARN is orders of magnitude faster than for the above-mentioned unmentioned unmentioned models and is significantly faster than for NADE."}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Bengio", "Yoshua", "Thibodeau-Laufer", "\u00c9ric"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1305.6663,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Graphical models for machine learning and digital communication", "author": ["Frey", "Brendam J"], "venue": "The MIT press,", "citeRegEx": "Frey and J.,? \\Q1998\\E", "shortCiteRegEx": "Frey and J.", "year": 1998}, {"title": "Emergence of complexlike cells in a temporal product network with local receptive fields", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1006.0448,", "citeRegEx": "Gregor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2010}, {"title": "Learning representations by maximizing compression", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1108.1169,", "citeRegEx": "Gregor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2011}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Representational power of restricted boltzmann machines and deep belief networks", "author": ["Le Roux", "Nicolas", "Bengio", "Yoshua"], "venue": "Neural Computation,", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Information theory, inference and learning algorithms", "author": ["MacKay", "David JC"], "venue": "Ch 6. Cambridge university press,", "citeRegEx": "MacKay and JC.,? \\Q2003\\E", "shortCiteRegEx": "MacKay and JC.", "year": 2003}, {"title": "Inductive principles for restricted boltzmann machine learning", "author": ["Marlin", "Benjamin M", "Swersky", "Kevin", "Chen", "Bo", "Freitas", "Nando D"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Marlin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2010}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": null, "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}], "referenceMentions": [{"referenceID": 12, "context": "One class of models, based on undirected generative models, includes examples such as Restricted Boltzmann Machines (RBMs; Smolensky, 1986), Deep Belief Networks (DBNs; Hinton et al.", "startOffset": 116, "endOffset": 139}, {"referenceID": 5, "context": "One class of models, based on undirected generative models, includes examples such as Restricted Boltzmann Machines (RBMs; Smolensky, 1986), Deep Belief Networks (DBNs; Hinton et al., 2006), and Deep Boltzmann Machines (DBMs; Salakhutdinov & Hinton, 2009).", "startOffset": 162, "endOffset": 189}, {"referenceID": 0, "context": "Another premier class of models, regularized autoencoders (Bengio et al., 2013; Bengio & Thibodeau-Laufer, 2013), rely on similar iterative methods for sampling and suffer from similar difficulties.", "startOffset": 58, "endOffset": 112}, {"referenceID": 8, "context": "A common way of dealing with images is through local connectivity, either fully convolutional (LeCun et al., 1998) or with less or even no weight sharing.", "startOffset": 94, "endOffset": 114}, {"referenceID": 10, "context": "In the final experiment, we trained DARN on the Silhouettes dataset from (Marlin et al., 2010), consisting of silhouettes extracted from the Caltech 101 dataset and subsampled down to 28 \u00d7 28 pixels.", "startOffset": 73, "endOffset": 94}], "year": 2017, "abstractText": "We introduce a multilayer deep generative model capable of learning hierarchies of sparse distributed representations from data. The model consists of several layers of stochastic units, with autoregressive connections within each layer, which allows for efficient exact sampling. We train the model efficiently using an algorithm derived from the Minimum Description Length (MDL) principle, which minimizes the amount of information contained in the joint vector of data and hidden unit configurations for the training set. As we are not given the hidden unit configurations corresponding to the training data, we use a feedforward network to map data vectors to configurations of hidden units that are jointly probable with them and train it jointly with the model. Our approach can also be seen as maximizing a lower bound on the log-likelihood, with the feedforward network implementing approximate inference.", "creator": "LaTeX with hyperref package"}}}