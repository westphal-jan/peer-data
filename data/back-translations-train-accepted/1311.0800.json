{"id": "1311.0800", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2013", "title": "Distributed Exploration in Multi-Armed Bandits", "abstract": "We study exploration in Multi-Armed Bandits in a setting where $k$ players collaborate in order to identify an $\\epsilon$-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the $k$ players to communicate only once, they are able to learn $\\sqrt{k}$ times faster than a single player. That is, distributing learning to $k$ players gives rise to a factor $\\sqrt{k}$ parallel speed-up. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor $k$ speed-up in learning performance, with communication only logarithmic in $1/\\epsilon$.", "histories": [["v1", "Mon, 4 Nov 2013 18:19:25 GMT  (33kb)", "http://arxiv.org/abs/1311.0800v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eshcar hillel", "zohar shay karnin", "tomer koren", "ronny lempel", "oren somekh"], "accepted": true, "id": "1311.0800"}, "pdf": {"name": "1311.0800.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["eshcar@yahoo-inc.com", "zkarnin@yahoo-inc.com", "tomerk@technion.ac.il", "rlempel@yahoo-inc.com", "orens@yahoo-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 131 1.08 00 \u221a k times faster than a single player. That is, the distribution of learning players results in a factor \u221a k of parallel acceleration. We supplement this result with a lower limit, which shows that this is generally the best possible. On the other hand, we present an algorithm that achieves the ideal factor k acceleration in learning performance, whereby the communication is only logarithmic in 1 / \u03b5."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "1.1 Related Work", "text": "In the MAB literature, several recent papers deal with multiplayer MAB scenarios in which players actually compete with each other, either through the use of resources they share (Gabillon et al., 2011) or through the rewards received (Liu and Zhao, 2010). In contrast, we investigate a collaborative multiplayer problem and how the exchange of observations helps players achieve their common goal. Another line of recent work focused on decentralized stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and the distribution of PAC models (Balcan et al., 2012; Daume III et al., 2012b), with Duchi et al., 2010; Agarwal and Duchi, 2011; our applicable techniques are not relevant in MAB practice (Dekel et al., 2012) and the distribution of PAC models (Balcan et al., 2012; Duchi et al., 2012b)."}, {"heading": "2 Problem Setup and Statement of Results", "text": "In our model of the Distributed Multi-Armed Bandit problem, there are k > 1 individual players. Players receive n arms, which are enumerated by [n]: = 1, 2,.., n. \"Each arm i.\" is therefore associated with a reward, which is a [0, 1] -rated random variable with expectation pi.For convenience, we assume that the arms are ordered according to their expected rewards, which is p1. \"\u00b7 \u00b7 \u00b7 pn.\" At each step t = 1, 2., \"T,\" each player pulls an arm of their choice and observes an independent sample of their reward. Each player can choose one of the arms, regardless of the other players and their actions. At the end of the game, each player must commit to a single arm. In a communication round that can take place at a predetermined time, each player can send a message to all the other players."}, {"heading": "2.1 Baseline approaches", "text": "The first obvious approach, which has already been mentioned, is the communication strategy: let each player explore the arms in isolation from the others. 1If one is interested in non-distributional boundaries, then the problem at hand is trivial, since the (distributed) uniform scanning strategy is optimal in this setting, down to polylogarithmic factors. See also (Mannor and Tsitsiklis, 2004) for a relevant discussion. Players who follow an independent instance of a series strategy; at the end of the executions, all players have the worst arm. Obviously, this approach performs poorly in terms of learning performance, as in the worst case it must be drawn per player and does not result in a parallel speed increase. Another direct approach is to apply a majority vote among the players: let each player independently identify an arm and choose the arm that has the most votes (alternatively, at least half of the problem)."}, {"heading": "2.2 Our results", "text": "We will now discuss our approach and overview of our algorithmic results. These are summarized in Table 1 below, which compares the different algorithms in terms of parallel acceleration and communication.Our approach to the one round case is based on the idea of majority decision. For the best identification task, our observation is that by randomly exploring each player a smaller group of n / \u221a k weapons and selecting one of them as \"best,\" about 270k of the players would come up with the world's best arm. This (partial) consensus on a single arm is a key aspect of our approach, as it allows players to identify the right best arm among the voices of all k players after they have shared information only once. Our approach results in a factor k parallel acceleration, which, as we show in our lower limit, is the optimal factor in this setting. Although our goal here is pure exploration, in our algorithms each player follows an exploration-exploit strategy."}, {"heading": "3 One Communication Round", "text": "In this section, we look at the most basic variant of the multiplayer MAB problem, where each player is allowed only one transmission when she completes her queries. To illustrate the clarity of the representation, we first consider the setting for determining the best arm in Section 3.1. Section 3.2 deals with the (\u03b5, \u03b4) -PAC configuration. We demonstrate the density of our result in Section 3.3 with a lower limit on the required budget for arm pulls in this configuration. Our algorithms in this section assume the availability of a serial algorithm A (A, \u03b5), which identifies the best arm in A with a probability of at least 2 / 3 for a range of weapons A and accuracy \u03b5, which uses no more than the thancA-i-II log."}, {"heading": "3.1 Best-arm Identification Algorithm", "text": "We will now describe our one-sided best-arm identification algorithms. For simplicity, we will present a version in which we identify with 1 / 3, which means that the algorithm produces the right arm with a probability of at least 2 / 3; we will explain later how to implement it with arbitrary values of the armada. Our algorithm is comparable to a majority vote among multiple players, in which each player uses his arms in two stages. In the second exploit phase, in which each player identifies his arms as \"best\" in the first exploration phase and communicates this average reward. See algorithm 1 below a precise description. An appealing feature of our algorithms is that each player delivers a single message of constant size (down to logarithmic factors)."}, {"heading": "3.2 (\u03b5, \u03b4)-PAC Algorithm", "text": "We present an algorithm whose purpose is to restore an \"optimal\" arm. Here, there could be more than one \"best arm,\" so that each \"successful\" player could come up with another \"best arm.\" However, our analysis shows that there is a high probability that a subset of players can still agree on a single \"best arm,\" which makes it possible to identify it among the votes of all players. Our algorithm is described in Algorithm 2, and the following theorem states its guarantees. Algorithm 2 One-round arminput time horizon T, Accuracy output one arm 1: for player j = 1 to 2: choose a subset Aj of 12n / 2 weapons uniformally: Explore: execute ij A (Aj, \u03b5) using most 12T pulls (and holding the algorithm early if necessary); if the algorithm is not fragile or does not finish gracefully."}, {"heading": "3.2.1 Proofs of Lemmas", "text": "For each player j = = highest possible success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability i = lowest success probability."}, {"heading": "3.3 Lower Bound", "text": "The following theory suggests that to identify the best arm k player, a multiplicative acceleration of no more than O-2 (\u221a k) is achieved if one transfer per player is allowed (at the end of the game). This clearly also means that a similar lower limit is successful in the PAC setup, and proves that our algorithmic results for one case are essentially narrow. \u2022 Each individual player must use at least T-2 arms in order to be able to identify them collectively. \u2022 There is a single player algorithm that uses a single round of communication at the end of the game. \u2022 Each individual player must use at least T-arms to identify them. \u2022 There is a single player algorithm that requires at least 2 / 3 algorithm that requires at least O algorithm. (T) draws to identify the best arm with probability."}, {"heading": "4 Multiple Communication Rounds", "text": "In this section, we establish an explicit compromise between the performance of a multiplayer algorithm and the number of rounds of communication it uses in terms of accuracy. Our observation is that it is possible to achieve the optimal acceleration of factor k if we do not improve learning performance by allowing more than O (log (1 / 3) rounds. Algorithm 3 Multi-Round Arinput (4 / 4) results of an arm 1: initialize S0 [n], r \u00b2 results in terms of learning performance by achieving more than O (1 / 4) results."}, {"heading": "5 Conclusions and Further Research", "text": "We considered a collaborative MAB exploration problem, where several independent players examine a range of weapons with a common goal, and we have achieved the first non-trivial results. Our main findings apply to the specifically interesting system, where each player is allowed a single transfer; this setting naturally fits with common frameworks such as MapReduce. An interesting open question in this context is whether you can achieve a strictly better acceleration result (which is particularly independent of \u03b5) by allowing more than a single round. Even allowing only two rounds of communication, it is unclear whether the speed can be improved."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "In NIPS, pages 873\u2013881,", "citeRegEx": "Agarwal and Duchi.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi.", "year": 2011}, {"title": "Online models for content optimization", "author": ["D. Agarwal", "B.-C. Chen", "P. Elango", "N. Motgi", "S.-T. Park", "R. Ramakrishnan", "S. Roy", "J. Zachariah"], "venue": "In NIPS,", "citeRegEx": "Agarwal et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2008}, {"title": "Neural network learning: Theoretical foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett.,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 1999}, {"title": "Best arm identification in multi-armed bandits", "author": ["J.-Y. Audibert", "S. Bubeck", "R. Munos"], "venue": "In COLT, pages", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "UCB revisited: Improved regret bounds for the stochastic multiarmed bandit problem", "author": ["P. Auer", "R. Ortner"], "venue": "Periodica Mathematica Hungarica,", "citeRegEx": "Auer and Ortner.,? \\Q2010\\E", "shortCiteRegEx": "Auer and Ortner.", "year": 2010}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Distributed learning, communication complexity and privacy", "author": ["M. Balcan", "A. Blum", "S. Fine", "Y. Mansour"], "venue": "Arxiv preprint arXiv:1204.3514,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Mortal multi-armed bandits", "author": ["D. Chakrabarti", "R. Kumar", "F. Radlinski", "E. Upfal"], "venue": "In NIPS,", "citeRegEx": "Chakrabarti et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 2008}, {"title": "Efficient protocols for distributed classification and optimization", "author": ["H. Daum\u00e9 III", "J.M. Phillips", "A. Saha", "S. Venkatasubramanian"], "venue": "In ALT,", "citeRegEx": "III et al\\.,? \\Q2012\\E", "shortCiteRegEx": "III et al\\.", "year": 2012}, {"title": "Protocols for learning classifiers on distributed data", "author": ["H. Daum\u00e9 III", "J.M. Phillips", "A. Saha", "S. Venkatasubramanian"], "venue": "AISTAT,", "citeRegEx": "III et al\\.,? \\Q2012\\E", "shortCiteRegEx": "III et al\\.", "year": 2012}, {"title": "MapReduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Commun. ACM,", "citeRegEx": "Dean and Ghemawat.,? \\Q2008\\E", "shortCiteRegEx": "Dean and Ghemawat.", "year": 2008}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Distributed dual averaging", "author": ["J. Duchi", "A. Agarwal", "M.J. Wainwright"], "venue": "in networks. NIPS,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Multi-bandit best arm identification", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck"], "venue": null, "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Distributed non-stochastic experts", "author": ["V. Kanade", "Z. Liu", "B. Radunovic"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kanade et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanade et al\\.", "year": 2012}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Z. Karnin", "T. Koren", "O. Somekh"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "Distributed learning in multi-armed bandit with multiple players", "author": ["K. Liu", "Q. Zhao"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Liu and Zhao.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Zhao.", "year": 2010}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J. Tsitsiklis"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "Hoeffding races: Accelerating model selection search for classification and function approximation", "author": ["O. Maron", "A.W. Moore"], "venue": "In NIPS,", "citeRegEx": "Maron and Moore.,? \\Q1994\\E", "shortCiteRegEx": "Maron and Moore.", "year": 1994}, {"title": "Empirical bernstein stopping", "author": ["V. Mnih", "C. Szepesv\u00e1ri", "J.-Y. Audibert"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "How does clickthrough data reflect retrieval quality", "author": ["F. Radlinski", "M. Kurup", "T. Joachims"], "venue": "In CIKM,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Y. Yue", "T. Joachims"], "venue": "In ICML,", "citeRegEx": "Yue and Joachims.,? \\Q2009\\E", "shortCiteRegEx": "Yue and Joachims.", "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "MAB algorithms rank results of search engines (Radlinski et al., 2008; Yue and Joachims, 2009), choose between stories or ads to", "startOffset": 46, "endOffset": 94}, {"referenceID": 23, "context": "MAB algorithms rank results of search engines (Radlinski et al., 2008; Yue and Joachims, 2009), choose between stories or ads to", "startOffset": 46, "endOffset": 94}, {"referenceID": 1, "context": "showcase on web sites (Agarwal et al., 2008; Chakrabarti et al., 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al.", "startOffset": 22, "endOffset": 70}, {"referenceID": 8, "context": "showcase on web sites (Agarwal et al., 2008; Chakrabarti et al., 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al.", "startOffset": 22, "endOffset": 70}, {"referenceID": 20, "context": ", 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al., 2008), and more.", "startOffset": 70, "endOffset": 112}, {"referenceID": 21, "context": ", 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al., 2008), and more.", "startOffset": 70, "endOffset": 112}, {"referenceID": 14, "context": "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a \u201cgood\u201d bandit arm with high confidence.", "startOffset": 32, "endOffset": 122}, {"referenceID": 3, "context": "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a \u201cgood\u201d bandit arm with high confidence.", "startOffset": 32, "endOffset": 122}, {"referenceID": 15, "context": "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a \u201cgood\u201d bandit arm with high confidence.", "startOffset": 32, "endOffset": 122}, {"referenceID": 17, "context": "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a \u201cgood\u201d bandit arm with high confidence.", "startOffset": 32, "endOffset": 122}, {"referenceID": 7, "context": "Our goal is to find an arm with an (almost) optimal expected reward, with as few arm pulls as possible (that is, minimize the simple regret (Bubeck et al., 2009)).", "startOffset": 140, "endOffset": 161}, {"referenceID": 11, "context": "Round-based models are natural in distributed learning scenarios, where frameworks such as MapReduce (Dean and Ghemawat, 2008) are ubiquitous.", "startOffset": 101, "endOffset": 126}, {"referenceID": 15, "context": "In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources (Gabillon et al., 2011) or on the rewards received (Liu and Zhao, 2010).", "startOffset": 161, "endOffset": 184}, {"referenceID": 18, "context": ", 2011) or on the rewards received (Liu and Zhao, 2010).", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": "Another line of recent work was focused on distributed stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and distributed PACmodels (Balcan et al.", "startOffset": 79, "endOffset": 144}, {"referenceID": 0, "context": "Another line of recent work was focused on distributed stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and distributed PACmodels (Balcan et al.", "startOffset": 79, "endOffset": 144}, {"referenceID": 12, "context": "Another line of recent work was focused on distributed stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and distributed PACmodels (Balcan et al.", "startOffset": 79, "endOffset": 144}, {"referenceID": 13, "context": "Questions involving network topology (Duchi et al., 2010; Dekel et al., 2012) and delays (Agarwal and Duchi, 2011) are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.", "startOffset": 37, "endOffset": 77}, {"referenceID": 12, "context": "Questions involving network topology (Duchi et al., 2010; Dekel et al., 2012) and delays (Agarwal and Duchi, 2011) are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.", "startOffset": 37, "endOffset": 77}, {"referenceID": 0, "context": ", 2012) and delays (Agarwal and Duchi, 2011) are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.", "startOffset": 19, "endOffset": 44}, {"referenceID": 9, "context": "In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources (Gabillon et al., 2011) or on the rewards received (Liu and Zhao, 2010). In contrast, we study a collaborative multi-player problem and investigate how sharing observations helps players achieve their common goal. The related work of Kanade et al. (2012) in the context of non-stochastic (i.", "startOffset": 162, "endOffset": 416}, {"referenceID": 18, "context": ", when there is only one player), the lower bounds of Mannor and Tsitsiklis (2004) and Audibert et al.", "startOffset": 54, "endOffset": 83}, {"referenceID": 3, "context": ", when there is only one player), the lower bounds of Mannor and Tsitsiklis (2004) and Audibert et al. (2010) show that in general \u03a9\u0303(H\u03b5) pulls are necessary for identifying an \u03b5-arm, where", "startOffset": 87, "endOffset": 110}, {"referenceID": 3, "context": "Intuitively, the hardness of the task is therefore captured by the quantity H\u03b5, which is roughly the number of arm pulls needed to find an \u03b5-best arm with a reasonable probability; see also (Audibert et al., 2010) for a discussion.", "startOffset": 190, "endOffset": 213}, {"referenceID": 19, "context": "The first obvious approach, already mentioned earlier, is the nocommunication strategy: just let each player explore the arms in isolation of the other If one is interested in distribution-free bounds, then the problem at hand is trivial as the (distributed) uniform sampling strategy is optimal in this setting, up to polylogarithmic factors; see also (Mannor and Tsitsiklis, 2004) for a relevant discussion.", "startOffset": 353, "endOffset": 382}, {"referenceID": 4, "context": "A similar idea was employed in (Auer and Ortner, 2010) for improving the regret bound of UCB with respect to the parameters \u2206i.", "startOffset": 31, "endOffset": 54}, {"referenceID": 14, "context": "For example, the Successive Elimination algorithm (Even-Dar et al., 2006) and the Exp-Gap Elimination algorithm (Karnin et al.", "startOffset": 50, "endOffset": 73}, {"referenceID": 17, "context": ", 2006) and the Exp-Gap Elimination algorithm (Karnin et al., 2013) provide a guarantee of this form.", "startOffset": 46, "endOffset": 67}, {"referenceID": 2, "context": "1 of (Anthony and Bartlett, 1999).", "startOffset": 5, "endOffset": 33}, {"referenceID": 2, "context": "1 of (Anthony and Bartlett, 1999). Lemma 3.10. Consider a MAB problem with two arms and rewards 1 2 + \u03b5, 1 2 \u2212 \u03b5. There exists a constant c such that any algorithm that with probability at least 2/3 identifies the best arm, needs at least c/\u03b5 pulls in expectation. Proof (of Theorem 3.9). Let c1 = \u221a c/2, where c is the constant of Lemma 3.10. Consider a MAB instance over n arms, with the rewards being a random permutation \u03c3 of 1 2 +\u2206, 1 2 \u2212 \u2206, 0, . . . , 0, where \u2206 := 1/ \u221a n. A serial algorithm of choice (say, the Successive Elimination algorithm) is able to identify the best arm in this setting with probability 2/3 using at most \u00d5(n + 1/\u2206) = \u00d5(n) arm pulls (see e.g., Even-Dar et al. (2006)).", "startOffset": 6, "endOffset": 699}, {"referenceID": 5, "context": "In particular, it would be nice to see a conversion of algorithms like UCB (Auer et al., 2002) to a distributed setting.", "startOffset": 75, "endOffset": 94}, {"referenceID": 5, "context": "In particular, it would be nice to see a conversion of algorithms like UCB (Auer et al., 2002) to a distributed setting. In this respect, perhaps a more natural distributed model is a one resembling that of Kanade et al. (2012), that have established a regret vs.", "startOffset": 76, "endOffset": 228}], "year": 2013, "abstractText": "We study exploration in Multi-Armed Bandits in a setting where k players collaborate in order to identify an \u03b5-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the k players to communicate only once, they are able to learn \u221a k times faster than a single player. That is, distributing learning to k players gives rise to a factor \u221a k parallel speed-up. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/\u03b5.", "creator": "LaTeX with hyperref package"}}}