{"id": "1311.0914", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2013", "title": "A Divide-and-Conquer Solver for Kernel Support Vector Machines", "abstract": "The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within $10^{-6}$ relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM.", "histories": [["v1", "Mon, 4 Nov 2013 22:06:40 GMT  (389kb,D)", "http://arxiv.org/abs/1311.0914v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cho-jui hsieh", "si si", "inderjit s dhillon"], "accepted": true, "id": "1311.0914"}, "pdf": {"name": "1311.0914.pdf", "metadata": {"source": "CRF", "title": "A Divide-and-Conquer Solver for Kernel Support Vector Machines", "authors": ["Cho-Jui Hsieh", "Inderjit S. Dhillon"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that we will be able to go in search of a solution that is capable of finding the solution that we need in order to find a solution."}, {"heading": "2 Related Work", "text": "The optimization methods for SVM training have not been independently researched, while an SVM training scheme requires a large amount of memory (2001). It is natural to use decomposition methods [Platt, 1998], where only a subset of variables is updated at each step. Based on this idea, software packages such as LIBSVM [Chang and Lin, 2011] and SVMLight [Joachims, 1998] have been well developed to speed up the decomposition method, [Pe-rez-Cruz et al., 2004] have suggested a dual approach to maintaining a portion of important samples, and the shrinking technique [Joachims, 1998] is also widely used to accelerate minor samples. To speed up SVM training on large-scale datasets, it is natural to divide the problem into smaller subproblems and combine the models trained on each partition. [Jacobs et al., 1991] suggested a way to build algorithms, although not to combine them in their algorithms."}, {"heading": "3 Divide and Conquer Kernel SVM with a single level", "text": "Considering a number of instance pairs (xi, yi), i = 1,., n, xi, Rd and yi (1, \u2212 1), the main task in forming the kernel SVM is to solve the following quadratic optimization problems: min \u03b1 f (\u03b1) = 1 2 \u03b1TQ\u03b1 \u2212 eT\u03b1 (s.t.), (1), where e is the vector of all ones; C is the compensation parameter between loss and regularization in the SVM primordial problem; \u03b1 is the vector of dual variables; and Q is an n \u00d7 n matrix with Qij = yiyjK (xi, xj), where K is the core function. Note that, as in [Keerthi et al., 2006, Joachims, 2006] we ignore the \"bias\" terms - in fact, in our section 5, we do not improve accuracy by including the terms."}, {"heading": "4 Divide and Conquer SVM with multiple levels", "text": "There is a trade-off in selecting the number of clusters k for a single level DC-SVM with only one division and coping with the problem. If k is small, the sub-problems have dimensions similar to the original problem, so we cannot achieve much more acceleration. If, on the other hand, we further reduce the complexity of solving sub-problems, the resulting number of sub-problems can still be very different. In the multi-level DC-SVM, at the l-th level, we propose to operate DC-SVM with multiple levels to further shorten the time for solving the sub-problems, and meanwhile we achieve values that are close to the p-values DC-SVM. In the multi-level DC-SVM, at the l-th level, we will run the entire dataset in kl-clusters {V (l) 1,., V (l) kl, and solve these kl-problems independently."}, {"heading": "5 Experimental Results", "text": "We compare our proposed algorithms with other SVM solvers. All experiments are performed on an Intel Xeon X5355 2.66GHz CPU with 8G RAM.Datasets: We use 7 benchmark datasets as shown in Table 21. We use the raw data without scaling for two image datasets cifar and mnist8m, while the functions in all other datasets are converted into a binary classification by classifying round digits and not round digits. Similarly, we transform cifar into a binary classification problem by classifying animals and non-animals. We use a random 80% -20% column for covtype, webspam, kddcup99, a random 8M column for mnist8m."}, {"heading": "6 Conclusions", "text": "In this paper, we have proposed a novel Divide-and-Conquer algorithm for solving kernel SVM (DC-SVM). Our algorithm divides the problem into smaller sub-problems that can be solved independently and efficiently. We show that the partial solutions come close to the original problem, which motivates us to \"glue\" solutions from sub-problems in order to efficiently solve the original kernel SVM problem. Using this algorithm, we also integrate an early prediction strategy into our algorithm. We report on extensive experiments to show that DC-SVM significantly outperforms state-of-the-art exact and approximate solvers for non-linear kernel SVM on large datasets. The code for DC-SVM is available at http: / / www.cs.utexas.edu / ~ cjhsieh / dcsvm."}], "references": [{"title": "Training support vector machine using adaptive clustering", "author": ["D. Boley", "D. Cao"], "venue": "In SDM,", "citeRegEx": "Boley and Cao.,? \\Q2004\\E", "shortCiteRegEx": "Boley and Cao.", "year": 2004}, {"title": "Fast kernel classifiers with online and active learning", "author": ["A. Bordes", "S. Ertekin", "J. Weston", "L. Bottou"], "venue": "JMLR, 6:1579\u20131619,", "citeRegEx": "Bordes et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2005}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "A parallel mixture of SVMs for very large scale problems", "author": ["R. Collobert", "S. Bengio", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Collobert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2002}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "JMLR, 2:243\u2013264,", "citeRegEx": "Fine and Scheinberg.,? \\Q2001\\E", "shortCiteRegEx": "Fine and Scheinberg.", "year": 2001}, {"title": "Approximate kernel k-means: Solution to large scale kernel clustering", "author": ["Radha Ghitta", "Rong Jin", "Timothy C. Havens", "Anil K. Jain"], "venue": "In KDD,", "citeRegEx": "Ghitta et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghitta et al\\.", "year": 2011}, {"title": "Parallel support vector machines: The cascade SVM", "author": ["H.P. Graf", "E. Cosatto", "L. Bottou", "I. Dundanovic", "V. Vapnik"], "venue": "In NIPS,", "citeRegEx": "Graf et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graf et al\\.", "year": 2005}, {"title": "Adaptive mixtures of local experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.J. Nowlan", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Jacobs et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 1991}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "In Advances in Kernel Methods \u2013 Support Vector Learning,", "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In KDD,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Local deep kernel learning for efficient non-linear SVM prediction", "author": ["C. Jose", "P. Goyal", "P. Aggrwal", "M. Varma"], "venue": "In ICML,", "citeRegEx": "Jose et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jose et al\\.", "year": 2013}, {"title": "Building support vector machines with reduced classifier", "author": ["S.S. Keerthi", "O. Chapelle", "D. DeCoste"], "venue": "complexity. JMLR,", "citeRegEx": "Keerthi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2006}, {"title": "CombNET-III: a support vector machine based large scale classifier with probabilistic framework", "author": ["M. Kugler", "S. Kuroyanagi", "A.S. Nugroho", "A. Iwata"], "venue": "IEICE Trans. Inf. and Syst.,", "citeRegEx": "Kugler et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kugler et al\\.", "year": 2006}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Q.V. Le", "T. Sarlos", "A.J. Smola"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Training invariant support vector machines using selective sampling", "author": ["Ga\u00eblle Loosli", "St\u00e9phane Canu", "L\u00e9on Bottou"], "venue": "In Large Scale Kernel Machines,", "citeRegEx": "Loosli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loosli et al\\.", "year": 2007}, {"title": "Fast learning in networks of locally-tuned processing units", "author": ["John Moody", "Christian J. Darken"], "venue": "Neural Computation,", "citeRegEx": "Moody and Darken.,? \\Q1989\\E", "shortCiteRegEx": "Moody and Darken.", "year": 1989}, {"title": "Towards scalable support vector machines using squashing", "author": ["D. Pavlov", "D. Chudova", "P. Smyth"], "venue": "In KDD,", "citeRegEx": "Pavlov et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pavlov et al\\.", "year": 2000}, {"title": "Art\u00e9s-Rod\u0155\u0131guez. Double chunking for solving SVMs for very large datasets", "author": ["F. P\u00e9rez-Cruz", "A.R. Figueiras-Vidal"], "venue": "In Proceedings of Learning", "citeRegEx": "P\u00e9rez.Cruz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "P\u00e9rez.Cruz et al\\.", "year": 2004}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "In Advances in Kernel Methods - Support Vector Learning,", "citeRegEx": "Platt.,? \\Q1998\\E", "shortCiteRegEx": "Platt.", "year": 1998}, {"title": "Bundle methods for machine learning", "author": ["A. Smola", "S. Vishwanathan", "Q. Le"], "venue": null, "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "A Bayesian committee machine", "author": ["V. Tresp"], "venue": "Neural Computation,", "citeRegEx": "Tresp.,? \\Q2000\\E", "shortCiteRegEx": "Tresp.", "year": 2000}, {"title": "Core vector machines: Fast SVM training on very large data", "author": ["I.W. Tsang", "J.T. Kwok", "P.M. Cheung"], "venue": "sets. JMLR,", "citeRegEx": "Tsang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsang et al\\.", "year": 2005}, {"title": "Trading representability for scalability: Adaptive multihyperplane machine for nonlinear classification", "author": ["Z. Wang", "N. Djuric", "K. Crammer", "S. Vucetic"], "venue": "In KDD,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Making SVMs scalable to large data sets using hierarchical cluster indexing", "author": ["Hwanjo Yu", "Jiong Yang", "Jiawei Han", "Xiaolei Li"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Yu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2005}, {"title": "Improved Nystr\u00f6m low rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "In ICML,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Scaling up kernel SVM on limited resources: A low-rank linearization approach", "author": ["K. Zhang", "L. Lan", "Z. Wang", "F. Moerchen"], "venue": "In AISTATS,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Due to its importance, optimization methods for kernel SVM have been widely studied [Platt, 1998, Joachims, 1998], and efficient libraries such as LIBSVM [Chang and Lin, 2011] and SVMLight [Joachims, 1998] are well developed.", "startOffset": 154, "endOffset": 175}, {"referenceID": 8, "context": "Due to its importance, optimization methods for kernel SVM have been widely studied [Platt, 1998, Joachims, 1998], and efficient libraries such as LIBSVM [Chang and Lin, 2011] and SVMLight [Joachims, 1998] are well developed.", "startOffset": 189, "endOffset": 205}, {"referenceID": 18, "context": "Since training an SVM requires a large amount of memory, it is natural to apply decomposition methods [Platt, 1998], where only a subset of variables is updated at each step.", "startOffset": 102, "endOffset": 115}, {"referenceID": 2, "context": "Based on this idea, software packages such as LIBSVM [Chang and Lin, 2011] and SVMLight [Joachims, 1998] are well developed.", "startOffset": 53, "endOffset": 74}, {"referenceID": 8, "context": "Based on this idea, software packages such as LIBSVM [Chang and Lin, 2011] and SVMLight [Joachims, 1998] are well developed.", "startOffset": 88, "endOffset": 104}, {"referenceID": 17, "context": "To speed up the decomposition method, [P\u00e9rez-Cruz et al., 2004] proposed a double chunking approach to maintain a chunk of important samples, and the shrinking technique [Joachims, 1998] is also widely used to eliminate unimportant samples.", "startOffset": 38, "endOffset": 63}, {"referenceID": 8, "context": ", 2004] proposed a double chunking approach to maintain a chunk of important samples, and the shrinking technique [Joachims, 1998] is also widely used to eliminate unimportant samples.", "startOffset": 114, "endOffset": 130}, {"referenceID": 7, "context": "[Jacobs et al., 1991] proposed a way to combine models, although in their algorithm subproblems are not trained independently, while [Tresp, 2000] discussed a Bayesian prediction scheme (BCM) for model combination.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": ", 1991] proposed a way to combine models, although in their algorithm subproblems are not trained independently, while [Tresp, 2000] discussed a Bayesian prediction scheme (BCM) for model combination.", "startOffset": 119, "endOffset": 132}, {"referenceID": 3, "context": "[Collobert et al., 2002] partition the training dataset arbitrarily in the beginning, and then iteratively refine the partition to obtain an approximate kernel SVM solution.", "startOffset": 0, "endOffset": 24}, {"referenceID": 12, "context": "[Kugler et al., 2006] applied the above ideas to solve multi-class problems.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "[Graf et al., 2005] proposed a multilevel approach (CascadeSVM): they randomly build a partition tree of samples and train the SVM in a \u201ccascade\u201d way: only support vectors in the lower level of the tree are passed to the upper level.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "[Moody and Darken, 1989] proposed this idea to train the reduced sized problem with RBF kernel (LTPU); [Pavlov et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "[Moody and Darken, 1989] proposed this idea to train the reduced sized problem with RBF kernel (LTPU); [Pavlov et al., 2000] used a similar idea as a preprocessing of the dataset, while [Yu et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 23, "context": ", 2000] used a similar idea as a preprocessing of the dataset, while [Yu et al., 2005] further generalized this approach to a hierarchical coarsen-refinement solver for SVM.", "startOffset": 69, "endOffset": 86}, {"referenceID": 24, "context": "Based on this idea, the kmeans Nystr\u00f6m method [Zhang et al., 2008] was proposed to approximate the kernel matrix using landmark points.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "[Boley and Cao, 2004] proposed to find samples with similar \u03b1 values by clustering, so both the clustering goal and training step are quite different from ours.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": ", 2013], greedy basis selection [Keerthi et al., 2006], and online SVM solvers [Bordes et al.", "startOffset": 32, "endOffset": 54}, {"referenceID": 1, "context": ", 2006], and online SVM solvers [Bordes et al., 2005].", "startOffset": 32, "endOffset": 53}, {"referenceID": 10, "context": "Recently, [Jose et al., 2013] proposed an approximate solver to reduce testing time, while our work is focused on reducing the training time of kernel SVM.", "startOffset": 10, "endOffset": 29}, {"referenceID": 5, "context": "Therefore we consider a simple two-step kernel kmeans approach as in [Ghitta et al., 2011].", "startOffset": 69, "endOffset": 90}, {"referenceID": 2, "context": "Figure 2 demonstrates that DC-SVM identifies support vectors much faster than the shrinking strategy implemented in LIBSVM [Chang and Lin, 2011] (we discuss these results in more detail in Section 4).", "startOffset": 123, "endOffset": 144}, {"referenceID": 20, "context": "6ms BCM in [Tresp, 2000] 98.", "startOffset": 11, "endOffset": 24}, {"referenceID": 6, "context": "As discussed in Section 2, Cascade SVM [Graf et al., 2005] is another way to identify support vectors.", "startOffset": 39, "endOffset": 58}, {"referenceID": 20, "context": "Another way to combine the models trained from k clusters is to use the probabilistic framework proposed in the Bayesian Committee Machine (BCM) [Tresp, 2000].", "startOffset": 145, "endOffset": 158}, {"referenceID": 25, "context": "mnist8m is a digital recognition dataset with 10 numbers, so we follow the procedure in [Zhang et al., 2012] to transform it into a binary classification problem by classifying round digits and non-round digits.", "startOffset": 88, "endOffset": 108}, {"referenceID": 14, "context": "1M split for mnist8m (used in the original paper [Loosli et al., 2007]), and the original training/testing split for ijcnn1 and cifar.", "startOffset": 49, "endOffset": 70}, {"referenceID": 2, "context": "LIBSVM: the implementation in the LIBSVM library [Chang and Lin, 2011] with a small modification to handle SVM without the bias term \u2013 we observe that LIBSVM has similar test accuracy with/without bias.", "startOffset": 49, "endOffset": 70}, {"referenceID": 6, "context": "Cascade SVM: we implement cascade SVM [Graf et al., 2005] using LIBSVM as the base solver.", "startOffset": 38, "endOffset": 57}, {"referenceID": 11, "context": "SpSVM: Greedy basis selection for nonlinear SVM [Keerthi et al., 2006].", "startOffset": 48, "endOffset": 70}, {"referenceID": 22, "context": "LLSVM: improved Nystr\u00f6m method for nonlinear SVM by [Wang et al., 2011].", "startOffset": 52, "endOffset": 71}, {"referenceID": 13, "context": "FastFood: use random Fourier features to approximate the kernel function [Le et al., 2013].", "startOffset": 73, "endOffset": 90}, {"referenceID": 15, "context": "LTPU: Locally-Tuned Processing Units proposed in [Moody and Darken, 1989].", "startOffset": 49, "endOffset": 73}, {"referenceID": 1, "context": "LaSVM: An online algorithm proposed in [Bordes et al., 2005].", "startOffset": 39, "endOffset": 60}, {"referenceID": 25, "context": "[Zhang et al., 2012] reported that the low-rank approximation based method (LLSVM) outperforms Core Vector Machines [Tsang et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": ", 2012] reported that the low-rank approximation based method (LLSVM) outperforms Core Vector Machines [Tsang et al., 2005] and the bundle method [Smola et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 19, "context": ", 2005] and the bundle method [Smola et al., 2007], so we omit those comparisons here.", "startOffset": 30, "endOffset": 50}], "year": 2013, "abstractText": "The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within 10\u22126 relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM.", "creator": "LaTeX with hyperref package"}}}