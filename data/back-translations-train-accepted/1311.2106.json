{"id": "1311.2106", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2013", "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints", "abstract": "We investigate two new optimization problems -- minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [14, 35] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.", "histories": [["v1", "Fri, 8 Nov 2013 23:28:02 GMT  (53kb,D)", "http://arxiv.org/abs/1311.2106v1", "23 pages. A short version of this appeared in Advances of NIPS-2013"]], "COMMENTS": "23 pages. A short version of this appeared in Advances of NIPS-2013", "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.DM", "authors": ["rishabh k iyer", "jeff a bilmes"], "accepted": true, "id": "1311.2106"}, "pdf": {"name": "1311.2106.pdf", "metadata": {"source": "CRF", "title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints", "authors": ["Rishabh Iyer", "Jeff Bilmes"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this context, it should also be mentioned that the question of whether and to what extent it is a problem is a problem that extends to all areas of the economy. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...) The question is to what extent it is a problem. (...)"}, {"heading": "1.1 Motivation", "text": "The problem is that it is a complex system, in which it is about a way, in which it is about a way, in which the different types of information are interwoven with each other. (...) The problem is that the different types of information cannot be reconciled with each other. (...) The way in which the different types of information are reconciled is not new either. (...) The way in which the way in which the different types of information are reconciled is not new. (...) The way in which the different types of information are reconciled is not new. (...) The way in which the way in which the way in which the way in which the different kinds of information are reconciled are reconciled is not new. (...) The way in which the way in which the way in which the way in which the way in which they are processed and the way in which they are processed and the way in which they are not processed and the way in which they are processed and the way in which they are processed and the way in which they are not processed and the way in which they are processed and the way in which they are not processed."}, {"heading": "1.2 Our Contributions", "text": "In the following, we will show that problems 1 and 2 are closely related, since any approximation algorithm can be used for both problems to provide guarantees for the other problem as well. We will then provide a framework of combinatorial algorithms based on optimization, sometimes iterative, easily solvable partial problems, which are also achieved by calculating upper or lower limits of cost functions or limiting functions. We will also show that many combinatorial algorithms, such as the greedy algorithm for SK [42] and SSC [47] (both of which use seemingly different techniques) belong to this framework, and represent the first constant factor-bi criterion of the approximation algorithm for SSC [47], and therefore the general quantity coverage problem [6]. We will then show how we can obtain a number of limited approximation guarantees with appropriate decisions of approximate functions, and achieve the hardness of the 1 and 2 problems that actually correspond to our approximate results."}, {"heading": "2 Background and Main Ideas", "text": "We first introduce several key terms to be used throughout the work. Since we define a submodular function f = 1 in relation to the whole of the functions f = 1, we define the whole curvature, the curvature and the curvature of f [1] in relation to the whole of the functions f (j / j) f (j / j) f (j) f (j) f (j) f (j) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (n) f (f) f (f) f (f) f (n) f (f) f (f) f (f) f (f) f (f) f (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f) (f)."}, {"heading": "3 Relation between SCSC and SCSK", "text": "In this section we will show a precise relationship between problems 1 and 2. From the formulation of problems 1 and 2 it is clear that these problems are dual. In this section we will show that the problems are polynomically transferable to any other problem. Algorithm 2 Approximation algorithm for SCSK using an approximation algorithm for SCSK using an approximation algorithm for SCSC using an approximation algorithm for SCSK using linear search.We will first introduce the concept of bicriteria algorithms. An algorithm is a [\u03c3] bi-criterion algorithm for problem 1 if it is guaranteed to obtain a sentence X, such as f (X)."}, {"heading": "4 Approximation Algorithms", "text": "We will consider several algorithms for problems 1 and 2, all of which can be characterized by the framework of algorithm 1, using the surrogate functions in the form of upper / lower limits or approximations."}, {"heading": "4.1 Approximation Algorithms for SCSC", "text": "We will first describe how these greedy algorithms can be viewed within the framework of our algorithms by selecting the presentation of our algorithms based on the SCSK function. We will first examine a specific case in which submodular coverage (SSC) is slow but secure. (SSC) We will begin by considering a classic special case of SCSC (problem 1) in which there is already a modular function and a submodular function. Of course, this problem occurs in a number of problems related to active / online learning. (SSC) This problem was first investigated by Wolsey [47], showing that a simple greedy algorithm solution is being achieved (indeed, log factor)."}, {"heading": "4.2 Approximation Algorithms for SCSK", "text": "In this section, we describe our approach algorithms for SCSK, which we already know. We note that the dual nature of the algorithms in this section belongs to the values specified in \u00a7 4.1. We first examine a special case that is not scalable but has the closest guarantee. (SK) We start with a special case of SCSK (problem 2), where f is a modular function and g is a submodular function. (SK) In this case, SCSK turns into the narrowest problem for which the greedy algorithms with partial enumeration are available. (SK 2) The greedy algorithms can be considered an instance of the algorithms. (SK)"}, {"heading": "4.3 Extensions beyond SCSC and SCSK", "text": "We must first note that \"and\" constraints as g1 (V) and g2 (X), if g (X) = g2 (V) = g2 (V) = g2 (V) = g2 (V) = g2 (V) = g2 (V), but they have a simple equivalent: {g1 (X) = g1 (V) = g2 (X) = g2 (X)}. We can also handle k (X) and \"constraints by using g (X) = 1 gi (X) = 1 gi (X) = 1 gi (X) = 1 gi (X) = 1 gi (X) = 1 gi (X) = 1 gi (X). We can also handle k (X) and\" constraints by using g (X) = 1 gi (X) = 1 gi (X)."}, {"heading": "4.4 Hardness", "text": "In this section, we provide the hardness for the factors 1 and 2. The lower limits serve to show that the approximation factors above are almost narrow. (Theorem 4.12.) For each of these factors, there are submodular functions with a curvature designed so that no polynomial time algorithm for problems 1 and 2 achieves a bi-criterion factor better than vice versa (1 / 2 \u2212 1 + (n1 / 2 \u2212 1). We prove this result using the hardness construction from [11, 43]. The main idea of their evidence technology is to construct two submodular functions f (X) and fR (X), which are highly probable to be indistinguishable. Therefore, even with high probability, no algorithm can distinguish between the two functions and the gap in their values. We will see that this lower limit actually corresponds to the closer factors and is this problem."}, {"heading": "5 Experiments", "text": "In this section, we will empirically compare the performance of the various algorithms discussed in this paper. We are motivated by the choice of speech data [30, 33, 21] with the submodular function f, which promotes a limited vocabulary, while g tries to achieve acoustic variability. A natural choice of function f is a function of form (X), in which the neighborhood function is constructed on a split graph [33]. For the coverage function g, we use two types of coverage: one is a facility location function1 (X), and the other is a saturated sum function g2 (X)."}, {"heading": "6 Discussions and related work", "text": "In this context, it should be noted that this is a phenomenon which is not an isolated case, but a case which is an isolated case; in this case, it is a case which is not an isolated case, but a case which is an isolated case; in this case, it is a case which is a case which is a case which is a case which is a case which is a priori a case which is a priori a case which is a case which is a case which is a case which is a case which is a case which is a priori a case."}], "references": [{"title": "The submodular knapsack polytope", "author": ["A. Atamt\u00fcrk", "V. Narayanan"], "venue": "Discrete Optimization,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Interactive graph cuts for optimal boundary and region segmentation of objects in n-d images", "author": ["Y. Boykov", "M. Jolly"], "venue": "ICCV,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Submodular set functions, matroids and the greedy algorithm: tight worstcase bounds and some generalizations of the Rado-Edmonds theorem", "author": ["M. Conforti", "G. Cornuejols"], "venue": "Discrete Applied Mathematics, 7(3):251\u2013274,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1984}, {"title": "Decomposition of submodular functions", "author": ["W.H. Cunningham"], "venue": "Combinatorica, 3(1):53\u201368,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1983}, {"title": "On minimum submodular cover with submodular cost", "author": ["H. Du", "W. Wu", "W. Lee", "Q. Liu", "Z. Zhang", "D.-Z. Du"], "venue": "Journal of Global Optimization, 50(2):229\u2013234,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "A threshold of ln n for approximating set cover", "author": ["U. Feige"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V. Mirrokni", "J. Vondr\u00e1k"], "venue": "SIAM J. COMPUT., 40(4):1133\u20131155,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodular functions and optimization, volume 58", "author": ["S. Fujishige"], "venue": "Elsevier Science,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Timit, acoustic-phonetic continuous speech corpus", "author": ["J. Garofolo", "L.F. Lamel", "J. W", "Fiscus", "D. Pallet", "N. Dahlgren"], "venue": "In DARPA,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Approximability of combinatorial problems with multi-agent submodular cost functions", "author": ["G. Goel", "C. Karande", "P. Tripathi", "L. Wang"], "venue": "FOCS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Approximating submodular functions everywhere", "author": ["M. Goemans", "N. Harvey", "S. Iwata", "V. Mirrokni"], "venue": "SODA, pages 535\u2013544,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Interactive submodular set cover", "author": ["A. Guillory", "J. Bilmes"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Simultaneous learning and covering with adversarial noise", "author": ["A. Guillory", "J. Bilmes"], "venue": "ICML,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "UAI,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "The submodular Bregman and Lov\u00e1sz-Bregman divergences with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Mirror descent like algorithms for submodular optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "NIPS Workshop on Discrete Optimization in Machine Learning (DISCML),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast semidifferential based submodular function optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "ICML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["S. Jegelka", "J. Bilmes"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["S. Jegelka", "J.A. Bilmes"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On fast approximate submodular minimization", "author": ["S. Jegelka", "H. Lin", "J. Bilmes"], "venue": "NIPS,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Prismatic algorithm for discrete dc programming problems", "author": ["Y. Kawahara", "T. Washio"], "venue": "NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Knapsack problems", "author": ["H. Kellerer", "U. Pferschy", "D. Pisinger"], "venue": "Springer Verlag,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "A note on the budgeted maximization on submodular functions", "author": ["A. Krause", "C. Guestrin"], "venue": "Technical Report CMU-CALD-05-103, Carnegie Mellon University,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Near-optimal sensor placements: Maximizing information while minimizing communication cost", "author": ["A. Krause", "C. Guestrin", "A. Gupta", "J. Kleinberg"], "venue": "IPSN,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust submodular observation selection", "author": ["A. Krause", "B. McMahan", "C. Guestrin", "A. Gupta"], "venue": "Journal of Machine Learning Research (JMLR), 9:2761\u20132801,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "JMLR, 9:235\u2013284,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Maximizing submodular set functions subject to multiple linear constraints", "author": ["A. Kulik", "H. Shachnai", "T. Tamir"], "venue": "SODA,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Submodularity in Natural Language Processing: Algorithms and Applications", "author": ["H. Lin"], "venue": "PhD thesis, University of Washington, Dept. of EE,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "How to select a good training-data subset for transcription: Submodular active selection for sequences", "author": ["H. Lin", "J. Bilmes"], "venue": "Interspeech,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["H. Lin", "J. Bilmes"], "venue": "NAACL,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "The 49th Meeting of the Assoc. for Comp. Ling. Human Lang. Technologies (ACL/HLT-2011), Portland, OR, June", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal selection of limited vocabulary speech corpora", "author": ["H. Lin", "J. Bilmes"], "venue": "Interspeech,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical Programming,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1983}, {"title": "Intelligent selection of language model training data", "author": ["R.C. Moore", "W. Lewis"], "venue": "Proceedings of the ACL 2010 Conference Short Papers, pages 220\u2013224. Association for Computational Linguistics,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Size-constrained submodular minimization through minimum norm base", "author": ["K. Nagano", "Y. Kawahara", "K. Aihara"], "venue": "ICML,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A submodular-supermodular procedure with applications to discriminative structure learning", "author": ["M. Narasimhan", "J. Bilmes"], "venue": "UAI,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Best algorithms for approximating the maximum of a submodular set function", "author": ["G. Nemhauser", "L. Wolsey"], "venue": "Mathematics of Operations Research, 3(3):177\u2013188,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1978}, {"title": "Approximation algorithms for offline risk-averse combinatorial optimization", "author": ["E. Nikolova"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Cosegmentation of image pairs by histogram matching-incorporating a global constraint into MRFs", "author": ["C. Rother", "T. Minka", "A. Blake", "V. Kolmogorov"], "venue": "CVPR, volume 1, pages 993\u20131000. IEEE,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient computation of gapped substring kernels on large alphabets", "author": ["J. Rousu", "J. Shawe-Taylor"], "venue": "Journal of Machine Learning Research, 6(2):1323,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "A note on maximizing a submodular set function subject to a knapsack constraint", "author": ["M. Sviridenko"], "venue": "Operations Research Letters, 32(1):41\u201343,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2004}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Z. Svitkina", "L. Fleischer"], "venue": "FOCS, pages 697\u2013706,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation algorithms", "author": ["V.V. Vazirani"], "venue": "springer,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2004}, {"title": "Submodularity and curvature: the optimal algorithm", "author": ["J. Vondr\u00e1k"], "venue": "RIMS Kokyuroku Bessatsu, 23,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Greedy approximations for minimum submodular cover with submodular cost", "author": ["P.-J. Wan", "D.-Z. Du", "P. Pardalos", "W. Wu"], "venue": "Computational Optimization and Applications, 45(2):463\u2013474,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "An analysis of the greedy algorithm for the submodular set covering problem", "author": ["L.A. Wolsey"], "venue": "Combinatorica, 2(4):385\u2013393,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1982}], "referenceMentions": [{"referenceID": 13, "context": "These problems are often posed as minimizing the difference between submodular functions [14, 37] which is in the worst case inapproximable.", "startOffset": 89, "endOffset": 97}, {"referenceID": 36, "context": "These problems are often posed as minimizing the difference between submodular functions [14, 37] which is in the worst case inapproximable.", "startOffset": 89, "endOffset": 97}, {"referenceID": 7, "context": "A set function f : 2 \u2192 R is said to be submodular [8] if for all subsets S, T \u2286 V , it holds that f(S) + f(T ) \u2265 f(S \u222a T ) + f(S \u2229 T ).", "startOffset": 50, "endOffset": 53}, {"referenceID": 33, "context": "While general set function optimization is often intractable, many forms of submodular function optimization can be solved near optimally or even optimally in certain cases, and hence submodularity is also often called the discrete analog of convexity [34].", "startOffset": 252, "endOffset": 256}, {"referenceID": 46, "context": "The corresponding constraints are called the submodular cover [47] and submodular knapsack [1] respectively and hence we refer to Problem", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "The corresponding constraints are called the submodular cover [47] and submodular knapsack [1] respectively and hence we refer to Problem", "startOffset": 91, "endOffset": 94}, {"referenceID": 13, "context": "A standard approach used in literature [14, 37, 22] has been to transform these problems into minimizing the difference between submodular functions (also called DS optimization):", "startOffset": 39, "endOffset": 51}, {"referenceID": 36, "context": "A standard approach used in literature [14, 37, 22] has been to transform these problems into minimizing the difference between submodular functions (also called DS optimization):", "startOffset": 39, "endOffset": 51}, {"referenceID": 21, "context": "A standard approach used in literature [14, 37, 22] has been to transform these problems into minimizing the difference between submodular functions (also called DS optimization):", "startOffset": 39, "endOffset": 51}, {"referenceID": 13, "context": "While a number of heuristics are available for solving Problem 0, and while these heuristics often work well in practice [14, 37], in the worst-case it is NP-hard and inapproximable [14], even when f and g are monotone.", "startOffset": 121, "endOffset": 129}, {"referenceID": 36, "context": "While a number of heuristics are available for solving Problem 0, and while these heuristics often work well in practice [14, 37], in the worst-case it is NP-hard and inapproximable [14], even when f and g are monotone.", "startOffset": 121, "endOffset": 129}, {"referenceID": 13, "context": "While a number of heuristics are available for solving Problem 0, and while these heuristics often work well in practice [14, 37], in the worst-case it is NP-hard and inapproximable [14], even when f and g are monotone.", "startOffset": 182, "endOffset": 186}, {"referenceID": 21, "context": "Although an exact branch and bound algorithm has been provided for this problem [22], its complexity can be exponential in the worst case.", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "Sensor Placement and Feature Selection: Often, the problem of choosing sensor locations A from a given set of possible locations V can be modeled [27, 14] by maximizing the mutual information between the chosen variables A and the unchosen set V \\A (i.", "startOffset": 146, "endOffset": 154}, {"referenceID": 13, "context": "Sensor Placement and Feature Selection: Often, the problem of choosing sensor locations A from a given set of possible locations V can be modeled [27, 14] by maximizing the mutual information between the chosen variables A and the unchosen set V \\A (i.", "startOffset": 146, "endOffset": 154}, {"referenceID": 26, "context": "Note that, while the symmetric mutual information is not monotone, it can be shown to approximately monotone [27].", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": ", f(A) = I(XA;C)) assuming that the set of features XA are conditionally independent given C [27, 14].", "startOffset": 93, "endOffset": 101}, {"referenceID": 13, "context": ", f(A) = I(XA;C)) assuming that the set of features XA are conditionally independent given C [27, 14].", "startOffset": 93, "endOffset": 101}, {"referenceID": 26, "context": "Often this cost is submodular [27, 14].", "startOffset": 30, "endOffset": 38}, {"referenceID": 13, "context": "Often this cost is submodular [27, 14].", "startOffset": 30, "endOffset": 38}, {"referenceID": 32, "context": "The motivation for this problem is to find the subset of training examples which will facilitate evaluation of prototype systems [33].", "startOffset": 129, "endOffset": 133}, {"referenceID": 32, "context": "This problem then occurs in the form of Problem 1, where we want to find a small vocabulary subset (which is often submodular [33]), subject to a constraint that the subset acoustically spans the entire data set (which is also often submodular [30, 31]).", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "This problem then occurs in the form of Problem 1, where we want to find a small vocabulary subset (which is often submodular [33]), subject to a constraint that the subset acoustically spans the entire data set (which is also often submodular [30, 31]).", "startOffset": 244, "endOffset": 252}, {"referenceID": 30, "context": "This problem then occurs in the form of Problem 1, where we want to find a small vocabulary subset (which is often submodular [33]), subject to a constraint that the subset acoustically spans the entire data set (which is also often submodular [30, 31]).", "startOffset": 244, "endOffset": 252}, {"referenceID": 34, "context": "Machine Translation: Another application in machine translation is to choose a subset of training data that is optimized for given test data set, a problem previously addressed with modular functions [35].", "startOffset": 200, "endOffset": 204}, {"referenceID": 1, "context": "In computer vision and high-tree-width Markov random fields, this has been addressed using graph-cut algorithms [2], which are applicable when the MRF\u2019s energy function is submodular and limited degree, and more general submodular function minimization in the case of higher degree.", "startOffset": 112, "endOffset": 115}, {"referenceID": 19, "context": "Moreover, many useful non-submodular energy functions have also recently been phrased as forms of constrained submodular minimization [20] which still have bounded approximation guarantees.", "startOffset": 134, "endOffset": 138}, {"referenceID": 39, "context": "For example, in image co-segmentation [40], V = V1 \u222a V2 can represent the set of pixels in two images, f is the submodular energy function of the two images, while g represents the similarity between the two histograms of the hypothesized foreground regions in the two images, a function shown to be submodular in [40].", "startOffset": 38, "endOffset": 42}, {"referenceID": 39, "context": "For example, in image co-segmentation [40], V = V1 \u222a V2 can represent the set of pixels in two images, f is the submodular energy function of the two images, while g represents the similarity between the two histograms of the hypothesized foreground regions in the two images, a function shown to be submodular in [40].", "startOffset": 314, "endOffset": 318}, {"referenceID": 46, "context": "For example the Submodular Set Cover problem (henceforth SSC) [47] occurs as a special case of Problem 1, with f being modular and g is submodular.", "startOffset": 62, "endOffset": 66}, {"referenceID": 41, "context": "Similarly the Submodular Cost Knapsack problem (henceforth SK) [42] is a special case of problem 2 again when f is modular and g submodular.", "startOffset": 63, "endOffset": 67}, {"referenceID": 5, "context": "Both these problems subsume the Set Cover and Max k-Cover problems [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 22, "context": "When both f and g are modular, Problems 1 and 2 are called knapsack problems [23].", "startOffset": 77, "endOffset": 81}, {"referenceID": 42, "context": "Furthermore, Problem 1 also subsumes the cardinality constrained submodular minimization problem [43] and more generally the problem of minimizing a submodular function subject to a knapsack constraints.", "startOffset": 97, "endOffset": 101}, {"referenceID": 9, "context": "This in turn subsumes the minimum submodular spanning tree problem [10], when f is monotone submodular.", "startOffset": 67, "endOffset": 71}, {"referenceID": 41, "context": "We also show that many combinatorial algorithms like the greedy algorithm for SK [42] and SSC [47] (both of which seemingly use different techniques) also belong to this framework and provide the first constant-factor bi-criterion approximation algorithm for SSC [47] and hence the general set cover problem [6].", "startOffset": 81, "endOffset": 85}, {"referenceID": 46, "context": "We also show that many combinatorial algorithms like the greedy algorithm for SK [42] and SSC [47] (both of which seemingly use different techniques) also belong to this framework and provide the first constant-factor bi-criterion approximation algorithm for SSC [47] and hence the general set cover problem [6].", "startOffset": 94, "endOffset": 98}, {"referenceID": 46, "context": "We also show that many combinatorial algorithms like the greedy algorithm for SK [42] and SSC [47] (both of which seemingly use different techniques) also belong to this framework and provide the first constant-factor bi-criterion approximation algorithm for SSC [47] and hence the general set cover problem [6].", "startOffset": 263, "endOffset": 267}, {"referenceID": 5, "context": "We also show that many combinatorial algorithms like the greedy algorithm for SK [42] and SSC [47] (both of which seemingly use different techniques) also belong to this framework and provide the first constant-factor bi-criterion approximation algorithm for SSC [47] and hence the general set cover problem [6].", "startOffset": 308, "endOffset": 311}, {"referenceID": 2, "context": "Our guarantees and hardness results depend on the curvature of the submodular functions [3].", "startOffset": 88, "endOffset": 91}, {"referenceID": 39, "context": "2 [40] showed that \u2212g is supermodular.", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "The total curvature \u03baf is then \u03baf (V ) [3, 45].", "startOffset": 39, "endOffset": 46}, {"referenceID": 44, "context": "The total curvature \u03baf is then \u03baf (V ) [3, 45].", "startOffset": 39, "endOffset": 46}, {"referenceID": 3, "context": "Totally normalized [4] and saturated functions like matroid rank have a curvature \u03baf = 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "A number of approximation guarantees in the context of submodular optimization have been refined via the curvature of the submodular function [3, 18, 17].", "startOffset": 142, "endOffset": 153}, {"referenceID": 17, "context": "A number of approximation guarantees in the context of submodular optimization have been refined via the curvature of the submodular function [3, 18, 17].", "startOffset": 142, "endOffset": 153}, {"referenceID": 16, "context": "A number of approximation guarantees in the context of submodular optimization have been refined via the curvature of the submodular function [3, 18, 17].", "startOffset": 142, "endOffset": 153}, {"referenceID": 2, "context": "For example, when maximizing a monotone submodular function under cardinality upper bound constraints, the bound of 1 \u2212 e\u22121 has been refined to 1\u2212e \u2212\u03baf \u03baf [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 17, "context": "Similar bounds have also been shown in the context of constrained submodular minimization [18, 17, 16].", "startOffset": 90, "endOffset": 102}, {"referenceID": 16, "context": "Similar bounds have also been shown in the context of constrained submodular minimization [18, 17, 16].", "startOffset": 90, "endOffset": 102}, {"referenceID": 15, "context": "Similar bounds have also been shown in the context of constrained submodular minimization [18, 17, 16].", "startOffset": 90, "endOffset": 102}, {"referenceID": 17, "context": "Our algorithms using upper and lower bounds are analogous to the majorization/ minimization algorithms proposed in [18, 16], where we provided a unified framework of fast algorithms for submodular optimization.", "startOffset": 115, "endOffset": 123}, {"referenceID": 15, "context": "Our algorithms using upper and lower bounds are analogous to the majorization/ minimization algorithms proposed in [18, 16], where we provided a unified framework of fast algorithms for submodular optimization.", "startOffset": 115, "endOffset": 123}, {"referenceID": 10, "context": "One such classical approximation is the ellipsoidal approximations [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "These bounds are related to the subdifferential \u2202f (Y ) of the submodular set function f at a set Y \u2286 V , which is defined [8] as: \u2202f (Y ) = {y \u2208 R : f(X)\u2212 y(X) \u2265 f(Y )\u2212 y(Y ) for all X \u2286 V } (3)", "startOffset": 123, "endOffset": 126}, {"referenceID": 19, "context": "Modular upper bounds: We can also define superdifferentials \u2202 (Y ) of a submodular function [20, 15] at Y :", "startOffset": 92, "endOffset": 100}, {"referenceID": 14, "context": "Modular upper bounds: We can also define superdifferentials \u2202 (Y ) of a submodular function [20, 15] at Y :", "startOffset": 92, "endOffset": 100}, {"referenceID": 14, "context": "It is possible, moreover, to provide specific supergradients [15, 18, 16] that define the following two modular upper bounds (when referring either one, we use mfX): mfX,1(Y ) , f(X)\u2212 \u2211", "startOffset": 61, "endOffset": 73}, {"referenceID": 17, "context": "It is possible, moreover, to provide specific supergradients [15, 18, 16] that define the following two modular upper bounds (when referring either one, we use mfX): mfX,1(Y ) , f(X)\u2212 \u2211", "startOffset": 61, "endOffset": 73}, {"referenceID": 15, "context": "It is possible, moreover, to provide specific supergradients [15, 18, 16] that define the following two modular upper bounds (when referring either one, we use mfX): mfX,1(Y ) , f(X)\u2212 \u2211", "startOffset": 61, "endOffset": 73}, {"referenceID": 17, "context": "MM algorithms using upper/lower bounds: Using the modular upper and lower bounds above in Algorithm 1, provide a class of Majorization-Minimization (MM) algorithms, akin to the algorithms proposed in [18, 16] for submodular optimization and in [37, 14] for DS optimization (Problem 0 above).", "startOffset": 200, "endOffset": 208}, {"referenceID": 15, "context": "MM algorithms using upper/lower bounds: Using the modular upper and lower bounds above in Algorithm 1, provide a class of Majorization-Minimization (MM) algorithms, akin to the algorithms proposed in [18, 16] for submodular optimization and in [37, 14] for DS optimization (Problem 0 above).", "startOffset": 200, "endOffset": 208}, {"referenceID": 36, "context": "MM algorithms using upper/lower bounds: Using the modular upper and lower bounds above in Algorithm 1, provide a class of Majorization-Minimization (MM) algorithms, akin to the algorithms proposed in [18, 16] for submodular optimization and in [37, 14] for DS optimization (Problem 0 above).", "startOffset": 244, "endOffset": 252}, {"referenceID": 13, "context": "MM algorithms using upper/lower bounds: Using the modular upper and lower bounds above in Algorithm 1, provide a class of Majorization-Minimization (MM) algorithms, akin to the algorithms proposed in [18, 16] for submodular optimization and in [37, 14] for DS optimization (Problem 0 above).", "startOffset": 244, "endOffset": 252}, {"referenceID": 5, "context": "Unfortunately, Problems 1 and 2 are NP-hard even if f or g (or both) are modular [6], and therefore the surrogate problems themselves cannot be solved exactly.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "What is also fortunate and perhaps surprising, as we show in this paper below, is that unlike the case of DS optimization (where the problem is inapproximable in general [14]), the constrained forms of optimization (Problems 1 and 2) do have approximation guarantees.", "startOffset": 170, "endOffset": 174}, {"referenceID": 10, "context": "al [11] is to provide an algorithm based on approximating the submodular polyhedron by an ellipsoid.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "A simple trick then provides a curvature-dependent approximation [17] \u2014 we define the \u03baf -curve-normalized version of f as follows:", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "[17] Given a polymatroid function f with a curvature \u03baf < 1, the submodular function f(X) = \u03baf \u221a wf(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j) satisfies: f(X) \u2264 f(X) \u2264 O ( \u221a n log n 1 + ( \u221a n log n\u2212 1)(1\u2212 \u03baf ) ) f(X),\u2200X \u2286 V (7)", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "We also remark that a specific instance of such a transformation has been used [26], for a specific class of functions f and g.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "This problem occurs naturally in a number of problems related to active/online learning [12] and summarization [31, 32].", "startOffset": 88, "endOffset": 92}, {"referenceID": 30, "context": "This problem occurs naturally in a number of problems related to active/online learning [12] and summarization [31, 32].", "startOffset": 111, "endOffset": 119}, {"referenceID": 31, "context": "This problem occurs naturally in a number of problems related to active/online learning [12] and summarization [31, 32].", "startOffset": 111, "endOffset": 119}, {"referenceID": 46, "context": "This problem was first investigated by Wolsey [47], wherein he showed that a simple greedy algorithm achieves bounded (in fact, log-factor) approximation guarantees.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Akin to the framework of algorithms in [18], the crucial factor is the choice of the lower bound (or subgradient).", "startOffset": 39, "endOffset": 43}, {"referenceID": 46, "context": "Then we have the following lemma, which is an extension of [47]:", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "The resulting knapsack problem can be addressed using the greedy algorithm [23], but this exactly corresponds to the greedy algorithm of submodular set cover [47] and hence the guarantee follows from Theorem 1 in [47].", "startOffset": 75, "endOffset": 79}, {"referenceID": 46, "context": "The resulting knapsack problem can be addressed using the greedy algorithm [23], but this exactly corresponds to the greedy algorithm of submodular set cover [47] and hence the guarantee follows from Theorem 1 in [47].", "startOffset": 158, "endOffset": 162}, {"referenceID": 46, "context": "The resulting knapsack problem can be addressed using the greedy algorithm [23], but this exactly corresponds to the greedy algorithm of submodular set cover [47] and hence the guarantee follows from Theorem 1 in [47].", "startOffset": 213, "endOffset": 217}, {"referenceID": 43, "context": "The surrogate problem in this instance is a simple knapsack problem that can be solved nearly optimally using dynamic programming [44].", "startOffset": 130, "endOffset": 134}, {"referenceID": 46, "context": "As stated in the proof, the greedy algorithm for the submodular set cover problem [47] is in fact equivalent to using the greedy algorithm for the knapsack problem [23], which is in fact suboptimal.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "As stated in the proof, the greedy algorithm for the submodular set cover problem [47] is in fact equivalent to using the greedy algorithm for the knapsack problem [23], which is in fact suboptimal.", "startOffset": 164, "endOffset": 168}, {"referenceID": 46, "context": "When g is integral, the guarantee of the greedy algorithm is Hg , H(maxj g(j)), where H(d) = \u2211d i=1 1 i [47] (henceforth we will use Hg for this quantity).", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "This factor is tight up to lower-order terms [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 41, "context": "We could also solve SSC by looking at its dual, which is SK [42].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "Although SSC does not admit any constant-factor approximation algorithms [6], we can obtain a constant-factor bi-criterion guarantee:", "startOffset": 73, "endOffset": 76}, {"referenceID": 41, "context": "Using the greedy algorithm for SK [42] as the approximation oracle in Algorithm 3 provides a [1 + , 1\u2212 e\u22121] bi-criterion approximation algorithm for SSC, for any > 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 41, "context": "This result follows immediately from the guarantee of the submodular cost knapsack problem [42] and Theorem 3.", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "We remark that we can also use a simpler version of the greedy iteration at every iteration [31, 24] and we obtain a guarantee of (1 + , 1/2(1\u2212 e\u22121)).", "startOffset": 92, "endOffset": 100}, {"referenceID": 23, "context": "We remark that we can also use a simpler version of the greedy iteration at every iteration [31, 24] and we obtain a guarantee of (1 + , 1/2(1\u2212 e\u22121)).", "startOffset": 92, "endOffset": 100}, {"referenceID": 30, "context": "A nice property of the greedy algorithm for the submodular knapsack problem is that it can be completely parameterized by the chain of sets (this holds for the greedy algorithm of [31, 24] for knapsack constraints and the basic greedy algorithm of [38] under cardinality constraints).", "startOffset": 180, "endOffset": 188}, {"referenceID": 23, "context": "A nice property of the greedy algorithm for the submodular knapsack problem is that it can be completely parameterized by the chain of sets (this holds for the greedy algorithm of [31, 24] for knapsack constraints and the basic greedy algorithm of [38] under cardinality constraints).", "startOffset": 180, "endOffset": 188}, {"referenceID": 37, "context": "A nice property of the greedy algorithm for the submodular knapsack problem is that it can be completely parameterized by the chain of sets (this holds for the greedy algorithm of [31, 24] for knapsack constraints and the basic greedy algorithm of [38] under cardinality constraints).", "startOffset": 248, "endOffset": 252}, {"referenceID": 13, "context": "An analysis very similar to the ones in [14, 18] will reveal polynomial time convergence.", "startOffset": 40, "endOffset": 48}, {"referenceID": 17, "context": "An analysis very similar to the ones in [14, 18] will reveal polynomial time convergence.", "startOffset": 40, "endOffset": 48}, {"referenceID": 17, "context": "4 in [18].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "In particular, the result from [18] ensures a guarantee of", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "The inequalities above follow from the fact that the modular upper bound mf\u2205(X) satisfies [17], mf\u2205(X) \u2264 f(X) \u2264 |X| 1 + (|X| \u2212 1)(1\u2212 \u03baf ) f(X) (13)", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "For example, using square-root over modular function f(X) = \u2211k i=1 \u221a wi(X), which is common model used in applications [14, 29, 19], the worst case guarantee is Hg \u221a Kg.", "startOffset": 119, "endOffset": 131}, {"referenceID": 28, "context": "For example, using square-root over modular function f(X) = \u2211k i=1 \u221a wi(X), which is common model used in applications [14, 29, 19], the worst case guarantee is Hg \u221a Kg.", "startOffset": 119, "endOffset": 131}, {"referenceID": 18, "context": "For example, using square-root over modular function f(X) = \u2211k i=1 \u221a wi(X), which is common model used in applications [14, 29, 19], the worst case guarantee is Hg \u221a Kg.", "startOffset": 119, "endOffset": 131}, {"referenceID": 16, "context": "This follows directly from the results in [17].", "startOffset": 42, "endOffset": 46}, {"referenceID": 45, "context": ", the simple modular upper bound of f) was considered in [46, 5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 4, "context": ", the simple modular upper bound of f) was considered in [46, 5].", "startOffset": 57, "endOffset": 64}, {"referenceID": 45, "context": "Furthermore, our bound is also tight since with, for example, f(X) = min{|X|, 1}, our exactly matches the bound of [46, 5].", "startOffset": 115, "endOffset": 122}, {"referenceID": 4, "context": "Furthermore, our bound is also tight since with, for example, f(X) = min{|X|, 1}, our exactly matches the bound of [46, 5].", "startOffset": 115, "endOffset": 122}, {"referenceID": 22, "context": "Then, rather than solving every iteration through the greedy algorithm, we can solve every iteration as a knapsack problem (minimizing a modular function over a modular lower bound constraint) [23], using say, a dynamic programming based approach.", "startOffset": 193, "endOffset": 197}, {"referenceID": 38, "context": "Fortunately, we can use the result from [39], where they show that any function of the form of \u221a w1(X) +w2(X) can be optimized over any polytope P with an approximation factor of \u03b2(1 + ) for any > 0, where \u03b2 is the approximation factor of optimizing a modular function over P.", "startOffset": 40, "endOffset": 44}, {"referenceID": 38, "context": "The idea of the proof is to use the result from [39] where they show that any function of the form \u03bb1 \u221a m1(X) + \u03bb2m2(X) where \u03bb1 \u2265 0, \u03bb2 \u2265 0 and m1 and m2 are positive modular functions has a FPTAS, provided a modular function can easily be optimized over C.", "startOffset": 48, "endOffset": 52}, {"referenceID": 41, "context": "In this case, SCSK turns into the SK problem for which the greedy algorithm with partial enumeration provides a 1\u2212 e\u22121 approximation [42].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "In particular, we then get back the framework of [18], where the authors show that choosing a permutation based on a greedy ordering, exactly analogous to Eqn.", "startOffset": 49, "endOffset": 53}, {"referenceID": 41, "context": "A slight catch however is that for the analysis to work, [42] needs to consider ( n 3 ) instances of such orderings (partial enumeration), chosen by fixing the first three elements in the permutation [18].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "A slight catch however is that for the analysis to work, [42] needs to consider ( n 3 ) instances of such orderings (partial enumeration), chosen by fixing the first three elements in the permutation [18].", "startOffset": 200, "endOffset": 204}, {"referenceID": 17, "context": "We can however just choose the simple greedy ordering in one stage, to get a slightly worse approximation factor of 1\u2212 e\u22121/2 [18, 31, 24].", "startOffset": 125, "endOffset": 137}, {"referenceID": 30, "context": "We can however just choose the simple greedy ordering in one stage, to get a slightly worse approximation factor of 1\u2212 e\u22121/2 [18, 31, 24].", "startOffset": 125, "endOffset": 137}, {"referenceID": 23, "context": "We can however just choose the simple greedy ordering in one stage, to get a slightly worse approximation factor of 1\u2212 e\u22121/2 [18, 31, 24].", "startOffset": 125, "endOffset": 137}, {"referenceID": 17, "context": "[18] Choosing the surrogate function f\u0302 as f and \u011d as h in Algorithm 1 yields a set X:", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The proof of this result follows directly from [3, 18].", "startOffset": 47, "endOffset": 54}, {"referenceID": 17, "context": "The proof of this result follows directly from [3, 18].", "startOffset": 47, "endOffset": 54}, {"referenceID": 2, "context": "In particular it holds for maximizing a submodular function g over any down monotone constraint [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 25, "context": "SCSC is in fact more general and can be extended to more flexible and complicated constraints which can arise naturally in many applications [26, 13].", "startOffset": 141, "endOffset": 149}, {"referenceID": 12, "context": "SCSC is in fact more general and can be extended to more flexible and complicated constraints which can arise naturally in many applications [26, 13].", "startOffset": 141, "endOffset": 149}, {"referenceID": 25, "context": "when g(X) = g1(X) + g2(X) [26].", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "These also have a nice equivalence: {g1(X) = g1(V ) \u2228 g2(X) = g2(V )} \u21d4 {g(X) = g(V )} (32) by defining g(X) = g1(X)g2(V ) + g2(X)g1(V )\u2212 g1(X)g2(X) [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "Although g(X) in this case is not submodular, this scenario occurs naturally in many applications, particularly sensor placement [26].", "startOffset": 129, "endOffset": 133}, {"referenceID": 25, "context": "The problem in [26] is in fact a special case of Problem 2, using a modular function f .", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "Surprisingly this problem also has a constant factor (1 \u2212 1/e) approximation guarantee [28].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "In particular, recall that the submodular knapsack has constant factor approximation guarantees even when g is non-monotone submodular [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": "We prove this result using the hardness construction from [11, 43].", "startOffset": 58, "endOffset": 66}, {"referenceID": 42, "context": "We prove this result using the hardness construction from [11, 43].", "startOffset": 58, "endOffset": 66}, {"referenceID": 42, "context": "A Chernoff bound analysis very similar to [43] reveals that any algorithm that uses a polynomial number of queries can distinguish h and f R with probability only n \u2212\u03c9(1), and therefore cannot reliably distinguish the functions with a polynomial number of queries.", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "We are motivated by the speech data subset selection application [30, 33, 21] with the submodular function f encouraging limited vocabulary while g tries to achieve acoustic variability.", "startOffset": 65, "endOffset": 77}, {"referenceID": 32, "context": "We are motivated by the speech data subset selection application [30, 33, 21] with the submodular function f encouraging limited vocabulary while g tries to achieve acoustic variability.", "startOffset": 65, "endOffset": 77}, {"referenceID": 20, "context": "We are motivated by the speech data subset selection application [30, 33, 21] with the submodular function f encouraging limited vocabulary while g tries to achieve acoustic variability.", "startOffset": 65, "endOffset": 77}, {"referenceID": 32, "context": "A natural choice of the function f is a function of the form |\u0393(X)|, where \u0393(X) is the neighborhood function on a bipartite graph constructed between the utterances and the words [33].", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "Both these functions are defined in terms of a similarity matrix S = {sij}i,j\u2208V , which we define on the TIMIT corpus [9], using the string kernel metric [41] for similarity.", "startOffset": 118, "endOffset": 121}, {"referenceID": 40, "context": "Both these functions are defined in terms of a similarity matrix S = {sij}i,j\u2208V , which we define on the TIMIT corpus [9], using the string kernel metric [41] for similarity.", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "For example, [8] investigates an exact algorithm for solving problem 1, with equality instead of inequality.", "startOffset": 13, "endOffset": 16}, {"referenceID": 35, "context": "However, since problem 1 subsumes the problem of minimizing a monotone submodular function subject to a cardinality equality constraint, and is hence NP hard [36].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "Furthermore, a similar problem was considered in [25] with one specific instance of a function f , which is not submodular.", "startOffset": 49, "endOffset": 53}, {"referenceID": 45, "context": "Also, an algorithm equivalent to the first iteration of ISSC was proposed in [46, 5] and ISSC not only generalizes this, but we also provide a more explicit approximation guarantee (we provide an elaborate discussion on this in the section describing ISSC).", "startOffset": 77, "endOffset": 84}, {"referenceID": 4, "context": "Also, an algorithm equivalent to the first iteration of ISSC was proposed in [46, 5] and ISSC not only generalizes this, but we also provide a more explicit approximation guarantee (we provide an elaborate discussion on this in the section describing ISSC).", "startOffset": 77, "endOffset": 84}, {"referenceID": 28, "context": "We also point out that, a special case of SCSK was considered in [29], with f being submodular, and g modular (we called this the submodular span problem).", "startOffset": 65, "endOffset": 69}, {"referenceID": 42, "context": "The authors there use an algorithm very similar to Algorithm 2, to convert this problem into an instance of minimizing a submodular function subject to a knapsack constraint, for which they use the algorithm of [43].", "startOffset": 211, "endOffset": 215}, {"referenceID": 42, "context": "Unfortunately, the algorithm of [43] does not scale very well.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Similarly, a number of approximation algorithms have been shown for Problem 0 [14, 37, 22].", "startOffset": 78, "endOffset": 90}, {"referenceID": 36, "context": "Similarly, a number of approximation algorithms have been shown for Problem 0 [14, 37, 22].", "startOffset": 78, "endOffset": 90}, {"referenceID": 21, "context": "Similarly, a number of approximation algorithms have been shown for Problem 0 [14, 37, 22].", "startOffset": 78, "endOffset": 90}, {"referenceID": 13, "context": "The algorithms in [14, 37] are scalable and practical, but lack theoretical guarantees.", "startOffset": 18, "endOffset": 26}, {"referenceID": 36, "context": "The algorithms in [14, 37] are scalable and practical, but lack theoretical guarantees.", "startOffset": 18, "endOffset": 26}, {"referenceID": 21, "context": "The algorithm of [22] though exact, employs a branch and bound technique which is often inefficient in practice (the timing analysis from [22] also depicts that).", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "The algorithm of [22] though exact, employs a branch and bound technique which is often inefficient in practice (the timing analysis from [22] also depicts that).", "startOffset": 138, "endOffset": 142}], "year": 2013, "abstractText": "We investigate two new optimization problems \u2014 minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [14, 37] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.", "creator": "LaTeX with hyperref package"}}}