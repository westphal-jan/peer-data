{"id": "1311.2115", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2013", "title": "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods", "abstract": "We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information accessible by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability even for high dimensional optimization problems by developing an adaptive scheme to store and manipulate these quadratic approximations in a shared, time evolving low dimensional subspace, determined by the recent history of gradient evaluations. This algorithm contrasts with earlier stochastic second order techniques, which treat the Hessian of each contributing function only as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Our approach reaps the benefits of both SGD and quasi-Newton methods; each update step requires only a single subfunction evaluation (like SGD but unlike previous stochastic second order methods), while little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods but not for SGD). For convex problems the convergence rate of the proposed technique is at least linear. We demonstrate improved convergence on five diverse optimization problems.", "histories": [["v1", "Sat, 9 Nov 2013 00:54:37 GMT  (960kb,D)", "http://arxiv.org/abs/1311.2115v1", null], ["v2", "Mon, 10 Feb 2014 02:10:32 GMT  (1988kb,D)", "http://arxiv.org/abs/1311.2115v2", null], ["v3", "Wed, 26 Mar 2014 06:49:16 GMT  (2073kb,D)", "http://arxiv.org/abs/1311.2115v3", null], ["v4", "Sun, 27 Apr 2014 02:38:28 GMT  (2101kb,D)", "http://arxiv.org/abs/1311.2115v4", null], ["v5", "Tue, 13 May 2014 23:51:41 GMT  (3213kb,D)", "http://arxiv.org/abs/1311.2115v5", null], ["v6", "Thu, 14 Aug 2014 02:27:38 GMT  (6430kb,D)", "http://arxiv.org/abs/1311.2115v6", null], ["v7", "Sun, 30 Nov 2014 01:35:55 GMT  (1598kb,D)", "http://arxiv.org/abs/1311.2115v7", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jascha sohl-dickstein", "ben poole", "surya ganguli"], "accepted": true, "id": "1311.2115"}, "pdf": {"name": "1311.2115.pdf", "metadata": {"source": "META", "title": "An adaptive low dimensional quasi-Newton sum of functions optimizer", "authors": ["Jascha Sohl-Dickstein", "Ben Poole", "Surya Ganguli"], "emails": ["JASCHA@STANFORD.EDU", "POOLE@CS.STANFORD.EDU", "SGANGULI@STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most of them are able to survive by themselves if they do not see themselves as being able to survive. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2. Algorithm", "text": "Our goal is to combine the advantages of stochastic and quasi-Newton optimization techniques. First, we describe the general approach to optimizing parameters x, then we describe the process by which an independent online Hessian approximation is maintained for each subfunction, followed by an explanation of the construction of the common low-dimensional sub-space that makes the algorithm tractable for major problems, and finally, we close this section with a review of implementation details."}, {"heading": "2.1. Approximating Functions", "text": "We define a series of functions Gt (x), which should approximate F (x), Gt (x) = N \u2211 i = 1 gti (x), (3), with the high rate t indicating the learning iteration. Functions gti (x) are stored, and one of them is updated per learning step. Each gti (x) serves as a square approximation to the corresponding fi (x)."}, {"heading": "2.2. Update Steps", "text": "As illustrated in Figure 1, the optimization is done by repeating the steps: 1. Select a vector xt by minimizing the approximate lens function Gt \u2212 1 (x), xt = argmin x Gt \u2212 1 (x). (4) Note that Gt \u2212 1 (x) can be minimized in closed form, since it is a sum of square functions gt \u2212 1i (x).2. Select an index j-1 (1... N) and update the approximate subfunction gti (x) using a second-order power series by xt, while all other subfunctions remain unchanged, 2 (x) (red dashed lines). The sum of the approximate functions Gt \u2212 1 (x) (continuous red line) approaches the full target F (x). (b) The next parameter setting is selected by continuing the approximate function Gt \u2212 1 (x) from the previous update process. See the function below for an approximate number of parameters."}, {"heading": "2.3. Online Hessian Approximation", "text": "We use the quadratic term Htj using the algorithm BFGS (Dennis Jr & More, 1977) to generate an online approximation of the true Hessian of the subfunction j based on its history of gradient valuations.For the subfunction j we construct two matrices, \u2206 f \"and \u0445 x. Each column of f\" contains the change in the gradient of the subfunction j between successive evaluations of this subfunction, including all evaluations up to the present time.Each column of the x contains the corresponding change in position x between successive evaluations. Both matrices are truncated after a number of columns L, meaning that they contain only information from the previous L + 1 gradient evaluations for each subfunction. For all results in this work, L = 10 (identical to the default length for the LBFG implementation in section 4, which they consistently performed)."}, {"heading": "2.3.1. BFGS UPDATES", "text": "The BFGS algorithm works by iterating through columns \u2206 f \u2032 and \u0445 x after processing column s, from the oldest to the youngest. Let's be the column index, and Bs the approximate Hessian for the subfunction j. For each s, the approximate Hessian matrix Bs is set to obey the sectional equation \u0445 f \u2032 s = Bs \u00b2 xs for the corresponding columns, with the difference between it and the previous estimate Bs \u2212 1 having the lowest weighted Frobenius norm2. This results in the updated equation Bs = Bs \u2212 1 + f \u2032 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s, f \u00b2 s \u00b2 f \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u2212 Bs \u2212 Bs \u2212 ririb \u00b2 s \u00b2 s, which is determined according to the final F standard."}, {"heading": "2.3.2. THE FIRST BFGS STEP", "text": "The scaling factor \u03b2 is set to the smallest eigenvalue of a matrix without zero Q, \u03b2 = min \u03bbQ > 0 \u03bbQ. (7) where \u03bbQ indicates the eigenvalues of Q. Q is the symmetrical matrix with the smallest Frobenius value, which corresponds to the quadratic intersection equations for all columns in \u2206 f \u2032 and \u0445 x. That is, Q = [(\u2206 x) + T \u2206 f \u2032 (\u2206 f \u2032) +] 1 2, (8) where + indicates the pseudo-inverse and 12 the square root of the matrix. All eigenvalues of Q are not negative. Equations 7 and 8 are calculated in the subspace defined by \u0441f \u2032 and \u0445 x, which reduces the computing costs (see Table 1). The use of the smallest eigenvalues of Q and the eigenvalues of Q can be viewed in an uncompromised direction and in an approximate direction."}, {"heading": "2.3.3. ENFORCING POSITIVE DEFINITENESS", "text": "It is typical of quasi-Newton techniques to force the Hessian approach to remain unambiguously positive. In the SFO, each Hti is forced to be unambiguously positive by performing a self-decomposition and specifying all eigenvalues that are too small to reach the mean positive eigenvalue of Hti. If \u03bbmax is the maximum eigenvalue of Hti, then all eigenvalues smaller than \u03b3\u03bbmax are set as equal to the median \u03bb > 0 \u03bb. The median is used because it is a measure of the \"typical\" curvature, and if a eigenvalue is negative or extremely small, it is an indication that it cannot be trusted as a curvature measure."}, {"heading": "2.3.4. PROPERTIES", "text": "If the Hessian is constant, the BFGS will eventually move closer to the true Hessian (Dennis Jr & More, 1977). However, the updates in Equation 6 may cause Bs to be incompatible with the outdated equation for earlier steps r, r < s, making the BFGS particularly effective in situations where the Hessian changes over the course of the learning process, as newer gradient grades tend to override older grades."}, {"heading": "2.4. A Shared, Adaptive Low-Dimensional Representation", "text": "The dimensionality M of x-RM is typically large. Consequently, the storage and computing costs for working directly with the matrices Hti-RM-M and the historical terms for each subfunction \u2206 f \"and \u2206 x are typically prohibitive. In order to reduce the dimensionality M to a tractable value, the entire history is stored and all updates are calculated in a low-dimensional sub-space, with dimensionality between Kmin and Kmax. The sub-space is constructed in such a way that it contains the most current gradient and the position for each sub-function. Constructively, therefore, it includes the steepest descending gradient direction. The results in this paper apply to Kmin = 2N and Kmax = 3N. The sub-space is represented by the orthonorthonormal columns of a matrix Pt-RM-Kt, (Pt) T Pt = I. Kt is the sub-space dimensionality at the optimization stage."}, {"heading": "2.4.1. EXPANDING THE SUBSPACE WITH A NEW OBSERVATION", "text": "With each optimization step, an additional column is added to the subspace, which expands it by the most current gradient direction. This is done by first finding the component in the gradient vector that is outside the existing subspace, and then appending that component to the current subspace, qorth = f \u2032 j (xt) \u2212 Pt \u2212 1 (Pt \u2212 1) T f \u2032 j (xt), (9) Pt = [Pt \u2212 1qorth | | qorth |], (10) where j is the subfunction updated at the time t. The new position xt is automatically added because the position update within the subspace Pt \u2212 1. Vectors embedded in the subspace Pt \u2212 1 can be updated simply by appending a 0, since the first Kt \u2212 1 dimension of Pt \u2212 1 exists."}, {"heading": "2.4.2. RESTRICTING THE SIZE OF THE SUBSPACE", "text": "The orthonormal matrix representing this collapsed subspace is calculated by a QR decomposition of the youngest gradients and positions. A new collapsed subspace is thus composed as P \u2032 = orth ([f \u2032 1 (x\u03c4 t 1) \u00b7 f \u2032 N (x\u03c4 t) x\u03c4 t 1 \u00b7 \u00b7 t N]), (11) where \u03c4 ti indicates the learning step in which the ith subfunction was last evaluated before the current learning step t. Vectors embedded in the previous subspace P \u2032 are projected into the new subspace P \u2032 by multiplying with a projection matrix T = (P \u2032) T P. Vector components located outside the subspace defined by the youngest positions."}, {"heading": "2.5. Choosing a Target Subfunction", "text": "The subfunction to be updated in Eq.5 j = argmax i [xt \u2212 x\u03c4i] T Ht [xt \u2212 x\u03c4i], (12) indicates the point in time when the subfunction i was last evaluated, that is, the updated subfunction is the one that is furthest from the current location and therefore most useful to update, in contrast to the cyclical choice of the subfunction in (Blatt et al., 2007) and the random choice of the subfunction in (Roux et al., 2012). In exploratory experiments, we found that the selection of the function based on distance resulted in better optimization. For example, for the protein logistic regression target in Section 4, the objective value after 25 effective passes of the data is 1.045 for the target distance and the use of function 1.066 only using cyclic locating."}, {"heading": "2.6. Growing the Batch Size", "text": "To achieve faster initial convergence, we start with a small number of active subfunctions, and then increase the number of subfunctions each time the average gradient shrinks to a factor \u03b1 of the standard error in the average gradient. This comparison is done using the inverse approximate Hessian metric, that is, we increase the lot size by one if (f) T Ht \u2212 1 f \u00b2 t < \u03b1 \u00b2 i (f \u00b2 t i) T Ht \u2212 1 f \u00b2 ti (N t \u2212 1) N t, (13) if N t is the active lot size at the time t, Ht is the complete Hessian and f \u00b2 t is the average gradient, f \u00b2 t = 1N t \u00b2 i fi \u00b2 (xti). (14) In all experiments shown here, \u03b1 = 1, and the initial lot size is N1 = 2. The bad lot size is also increased by one if an update is detected by 7 (xti)."}, {"heading": "2.7. Detecting bad updates", "text": "Heuristics detect extremely bad updates and reset xt to its previous value xt \u2212 1 when detected, which is triggered when the value of a subfunction has increased since its previous rating and also exceeds its predicted value by more than the reduction of the summed approximation function (i.e. fj (xt) \u2212 gt \u2212 1 (xt) > Gt \u2212 1 (xt \u2212 1) \u2212 Gt \u2212 1 (xt \u2212 1) \u2212 Gt \u2212 1 (xt))."}, {"heading": "2.8. Initialization", "text": "An approximate Hessian can only be calculated after multiple gradient evaluation, as described in Section 2.3. If a subfunction j has only one gradient evaluation, its approximate Htj is set to the identity times of the mean eigenvalue of the average Hessian of the other active subfunctions. If j is the very first subfunction to be evaluated, Htj is initialized because the identity matrix is a large positive constant (106)."}, {"heading": "3. Properties", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Computational Cost", "text": "The calculation costs per complete pass of the data for each part of the algorithm are shown in Table 1. Typically, the largest terms are the O (QN) term from the evaluation of the lens and gradient for all N subfunctions and the O (MN2) term resulting from the projection of positions and gradients in and out of the low-dimensional subspace. Therefore, this algorithm is suitable for the case where the O (Q) cost of a single subfunction evaluation is greater than the O (MN) cost of projecting an M-dimensional vector into an O (N) dimensional subspace. Note that N can be reduced and the algorithmic overhead reduced by merging subfunctions or selecting larger minibatches. Without the use of the low-dimensional subspace, the leading term in the calculation costs of SFO (M2N) would be the much larger O (M2N) cost per pass."}, {"heading": "3.2. Convergence", "text": "Simultaneous work by (Mairal, 2013) considers a similar algorithm as described in 2.2, but with Hti as a scalar constant and not as a matrix. Sentence 6.1 in (Mairal, 2013) shows that if each Gi majorises its respective FI and is subject to additional smoothing constraints, Gt (x) decreases monotonously and x * is an asymptotic stationary point. Sentence 6.2 in (Mairal, 2013) further shows that the algorithm for strongly convex FI has a linear convergence rate to x. The same convergence results must apply to Gi with nearial evidence, but require some modifications to the algorithm. For the evidence to hold: the eigenvalues of Hti must be limited from above by a constant. It must be possible for Gi FI to majorise, and Hti must be selected to guarantee this majorisation (e.g. by adding diagonal regarization of Hti) rather than the current function of Hi being limited by a property function."}, {"heading": "4. Experimental Results", "text": "We compared our optimization technique with several competing optimization techniques for several objective functions. Results are shown in Figure 2, and the goals are described below. For all problems, our method outperformed all other techniques in comparison. Code for generating the diagrams in Figure 2 is included in supplementary material3. For all experiments, we chose a number of subfunctions N = 100.SFO refers to the sum of optimizer functions and is the new algorithm presented in this thesis. SAG refers to the Stochastic Average Gradient method, where the drag number provides the Lipschitz constant. SGD refers to the Stochastic Gradient Descent, where the drag number indicates the step size. ADAGrad indicates the AdaGrad algorithm, where the drag number indicates the initial step size. LBFGS refers to the limited memory BFGS algorithm, where the drag number indicates the step size."}, {"heading": "4.1. Logistic Regression", "text": "The only difference is that our total objective function is divided by the number of samples per minibatch, but unlike in (Roux et al., 2012) it is not also divided by the number of minibatches. This different scaling places the hyperparameters for the logistic regression in the 3All numbers in the work can be easily reproduced by downloading code and training data, typing \"Python figures.py\" and then waiting a week for all optimizers to perform all the objective functions for all hyperparameters, the same range as in our other experiments."}, {"heading": "4.2. Autoencoder", "text": "We trained a contractive autoencoder that punishes the Frobenius standard of Jacobite encoder function on MNIST digits. Autoencoders of this form were successfully used to learn deep representations in neural networks (Rifai et al., 2011). Sigmoid nonlinearities were used for both encoders and decoders. Regulatory penalty was set to 1 and did not depend on the number of hidden units. The reconstruction error was divided by the number of training examples. There were 784 visible units and 256 hidden units."}, {"heading": "4.3. Multilayer Perceptron", "text": "We trained a deep neural network to classify digits on the MNIST benchmark for digit recognition, using an architecture similar to (Hinton & Srivastava, 2012), but with a smaller number of units to give all competing optimizers time to run, and our network consisted of: 784 input units, a hidden layer of 120 units, a hidden layer of 12 units, and 10 output units. We conducted the experiment with both rectified linear and sigmoidal units, with the goal of achieving standard Softmax regression of output units."}, {"heading": "4.4. Deep Convolutional Network", "text": "We trained a deep revolutionary network on CIFAR-10 using max pooling and linear rectified units. The architecture we experimented with included two revolutionary layers of 48 and 128 units respectively, followed by a fully connected layer of 240 units. (This diagram will be updated to include the pulse optimizer SGD + when the corresponding web search via hyperparameters is completed.)"}, {"heading": "5. Future Directions", "text": "We perform optimizations in an O (N) dimensional subspace. However, it may be possible to drastically reduce the dimensionality of the active subspace without significantly reducing the optimization performance. For example, the subspace could be determined by accumulating the leading eigenvectors of the covariance matrix in an online manner, which would allow the algorithm to run faster even for a large number of subfunctions and also reduce memory requirements. Most parts of the presented algorithm are naively parallelizable, the gti (x) functions can be updated asynchronously and can even be updated using old position information, so developing a parallelized version of this algorithm could become a useful tool for massive scaling problems."}], "references": [{"title": "Natural Gradient Works Efficiently in Learning", "author": ["Amari", "Shun-Ichi"], "venue": "Neural Computation,", "citeRegEx": "Amari and Shun.Ichi.,? \\Q1998\\E", "shortCiteRegEx": "Amari and Shun.Ichi.", "year": 1998}, {"title": "A convergent incremental gradient method with a constant step size", "author": ["Blatt", "Doron", "Hero", "Alfred O", "Gauchman", "Hillel"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Blatt et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blatt et al\\.", "year": 2007}, {"title": "SGD-QN: Careful quasi-Newton stochastic gradient descent", "author": ["Bordes", "Antoine", "Bottou", "L\u00e9on", "Gallinari", "Patrick"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bordes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2009}, {"title": "On the use of stochastic hessian information in optimization methods for machine learning", "author": ["Byrd", "RH Richard H", "Chin", "GM Gillian M", "Neveitt", "Will", "Nocedal", "Jorge"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Byrd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 2011}, {"title": "Quasi-Newton methods, motivation and theory", "author": ["Dennis Jr.", "John E", "Mor\u00e9", "Jorge J"], "venue": "SIAM review,", "citeRegEx": "Jr et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Jr et al\\.", "year": 1977}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["GE Hinton", "N. Srivastava"], "venue": "arXiv preprint arXiv:", "citeRegEx": "Hinton and Srivastava,? \\Q2012\\E", "shortCiteRegEx": "Hinton and Srivastava", "year": 2012}, {"title": "Robust statistics. Wiley, New York, 1981. URL http://scholar.google.com/scholar?hl= en&q=P.J.+Huber%2C+Robust+Statistics% 2C+Wiley%2C+New+York%2C+1981.&btnG= &as_sdt=1%2C5&as_sdtp=#0", "author": ["Huber", "PJ"], "venue": null, "citeRegEx": "Huber and PJ.,? \\Q1981\\E", "shortCiteRegEx": "Huber and PJ.", "year": 1981}, {"title": "Trust region newton method for logistic regression", "author": ["Lin", "Chih-Jen", "Weng", "Ruby C", "Keerthi", "S Sathiya"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2008}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["Liu", "Dong C DC", "Nocedal", "Jorge"], "venue": "Mathematical programming,", "citeRegEx": "Liu et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1989}, {"title": "Optimization with First-Order Surrogate Functions", "author": ["J. Mairal"], "venue": "arXiv preprint arXiv:1305.3120,", "citeRegEx": "Mairal,? \\Q2013\\E", "shortCiteRegEx": "Mairal", "year": 2013}, {"title": "Deep learning via Hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "On optimization methods for deep learning", "author": ["J Ngiam", "A. Coates"], "venue": null, "citeRegEx": "Ngiam and Coates,? \\Q2011\\E", "shortCiteRegEx": "Ngiam and Coates", "year": 2011}, {"title": "Historical Development of the BFGS Secant Method and Its Characterization Properties", "author": ["Papakonstantinou", "JM"], "venue": null, "citeRegEx": "Papakonstantinou and JM.,? \\Q2009\\E", "shortCiteRegEx": "Papakonstantinou and JM.", "year": 2009}, {"title": "On the difficulty of training Recurrent Neural Networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "URL http://arxiv.org/abs/", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "A stochastic approximation method", "author": ["Robbins", "Herbert", "Monro", "Sutton"], "venue": "The Annals of Mathematical Statistics, pp", "citeRegEx": "Robbins et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Robbins et al\\.", "year": 1951}, {"title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets", "author": ["Roux", "N Le", "M Schmidt", "F. Bach"], "venue": "URL", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "URL http://eprints", "author": ["Schraudolph", "Nicol", "Yu", "Jin", "G\u00fcnter", "Simon. A stochastic quasi-Newton method for online convex optimization."], "venue": "pascal-network.org/archive/00003992/.", "citeRegEx": "Schraudolph et al\\.,? 2007", "shortCiteRegEx": "Schraudolph et al\\.", "year": 2007}, {"title": "Local gain adaptation in stochastic gradient descent", "author": ["Schraudolph", "Nicol N"], "venue": "In Artificial Neural Networks,", "citeRegEx": "Schraudolph and N.,? \\Q1999\\E", "shortCiteRegEx": "Schraudolph and N.", "year": 1999}, {"title": "The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use", "author": ["Sohl-Dickstein", "Jascha"], "venue": "URL http: //arxiv.org/abs/1205.1828", "citeRegEx": "Sohl.Dickstein and Jascha.,? \\Q2012\\E", "shortCiteRegEx": "Sohl.Dickstein and Jascha.", "year": 2012}, {"title": "Minimum Probability Flow Learning", "author": ["Sohl-Dickstein", "Jascha", "Battaglino", "Peter B", "DeWeese", "Michael R"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2011}, {"title": "Variable metric stochastic approximation theory", "author": ["Sunehag", "Peter", "Trumpf", "Jochen", "S V N Vishwanathan", "Schraudolph", "Nicol"], "venue": "arXiv preprint arXiv:0908.3529,", "citeRegEx": "Sunehag et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sunehag et al\\.", "year": 2009}, {"title": "Krylov subspace descent for deep learning", "author": ["Vinyals", "Oriol", "Povey", "Daniel"], "venue": "arXiv preprint arXiv:1111.4259,", "citeRegEx": "Vinyals et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "More recent descent techniques like IAG (Blatt et al., 2007), SAG (Roux et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 16, "context": ", 2007), SAG (Roux et al., 2012), and MISO (Mairal, 2013) instead take update steps in the average gradient direction.", "startOffset": 13, "endOffset": 32}, {"referenceID": 10, "context": ", 2012), and MISO (Mairal, 2013) instead take update steps in the average gradient direction.", "startOffset": 18, "endOffset": 32}, {"referenceID": 17, "context": "In (Schraudolph et al., 2007; Sunehag et al., 2009) a stochastic variant of LBFGS is proposed.", "startOffset": 3, "endOffset": 51}, {"referenceID": 21, "context": "In (Schraudolph et al., 2007; Sunehag et al., 2009) a stochastic variant of LBFGS is proposed.", "startOffset": 3, "endOffset": 51}, {"referenceID": 3, "context": "In (Martens, 2010), (Byrd et al., 2011), and (Vinyals & Povey, 2011) stochastic versions of Hessian-free optimization are implemented and applied to optimization of deep networks.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "In (Lin et al., 2008) a trust region Newton method is used to train logistic regression and linear SVMs using minibatches.", "startOffset": 3, "endOffset": 21}, {"referenceID": 5, "context": "Stochastic meta-descent (Schraudolph, 1999), AdaGrad (Duchi et al., 2010), and SGD-QN (Bordes et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 2, "context": ", 2010), and SGD-QN (Bordes et al., 2009) rescale the gradient independently for each dimension, and can be viewed as accumulating something similar to a diagonal approximation to the Hessian.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": "This contrasts with the cyclic choice of subfunction in (Blatt et al., 2007), and the random choice of subfunction in (Roux et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 16, "context": ", 2007), and the random choice of subfunction in (Roux et al., 2012).", "startOffset": 49, "endOffset": 68}, {"referenceID": 10, "context": "Concurrent work by (Mairal, 2013) considers a similar algorithm to that described in 2.", "startOffset": 19, "endOffset": 33}, {"referenceID": 10, "context": "1 in (Mairal, 2013) shows that in the case that each gi majorizes its respective fi, and subject to some additional smoothness constraints, G (x) monotonically decreases, and x\u2217 is an asymptotic stationary point.", "startOffset": 5, "endOffset": 19}, {"referenceID": 10, "context": "2 in (Mairal, 2013) further shows that for strongly convex fi, the algorithm exhibits a linear convergence rate to x\u2217.", "startOffset": 5, "endOffset": 19}, {"referenceID": 16, "context": "We chose the logistic regression objective, L2 regularization penalty, and training dataset to be identical to the protein homology test case in the recent Stochastic Average Gradient paper (Roux et al., 2012), to allow for direct comparison of techniques.", "startOffset": 190, "endOffset": 209}, {"referenceID": 16, "context": "The one difference is that our total objective function is divided by the number of samples per minibatch, but unlike in (Roux et al., 2012) is not also divided by the number of minibatches.", "startOffset": 121, "endOffset": 140}, {"referenceID": 14, "context": "Quadratic functions are often a poor match to the geometry of the objective function (Pascanu et al., 2012).", "startOffset": 85, "endOffset": 107}], "year": 2013, "abstractText": "We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information accessible by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability even for high dimensional optimization problems by developing an adaptive scheme to store and manipulate these quadratic approximations in a shared, time evolving low dimensional subspace, determined by the recent history of gradient evaluations. This algorithm contrasts with earlier stochastic second order techniques, which treat the Hessian of each contributing function only as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Our approach reaps the benefits of both SGD and quasi-Newton methods; each update step requires only a single subfunction evaluation (like SGD but unlike previous stochastic second order methods), while little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods but not for SGD). For convex problems the convergence rate of the proposed technique is at least linear. We demonstrate improved convergence on five diverse optimization problems.", "creator": "LaTeX with hyperref package"}}}