{"id": "1311.2241", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2013", "title": "Learning Gaussian Graphical Models with Observed or Latent FVSs", "abstract": "Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity $O(k^{2}n)$ using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efficient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or flight networks where the FVS nodes often correspond to a small number of high-degree nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate in $O(kn^2+n^2\\log n)$ if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing a inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank correction with complexity $O(kn^{2}+n^{2}\\log n)$ per iteration. We also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with FVSs of various sizes.", "histories": [["v1", "Sun, 10 Nov 2013 02:39:48 GMT  (469kb,D)", "http://arxiv.org/abs/1311.2241v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ying liu 0009", "alan s willsky"], "accepted": true, "id": "1311.2241"}, "pdf": {"name": "1311.2241.pdf", "metadata": {"source": "CRF", "title": "Learning Gaussian Graphical Models with Observed or Latent FVSs", "authors": ["Ying Liu", "Alan S. Willsky"], "emails": ["liu_ying@mit.edu", "willsky@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Preliminaries", "text": "Each undirected graphical model has an underlying graph G = (V, E), where V denotes the set of nodes (nodes) and E denotes the set of edges. Each node s-V corresponds to a random variable xs. If the random vector xV is Gaussian in common, the model is a GGM with a density function given by p (x) = 1Z exp {\u2212 1 2xTJx + hTx}, where J is the information matrix or precision matrix, h the potential vector and Z is the partition function. The parameters J and h are related to the mean microscope and covariance matrix. The structure of the underlying graph is revealed by the saving pattern of J: There is an edge between i and j, and only if Jij 6 = 0.Given samples x = 0.Given samples x x = 1 are generated independently."}, {"heading": "3 Gaussian Graphical Models with Known FVSs", "text": "In this section we will briefly discuss some of the ideas related to GGMs with FVSs of size k, where we will also refer to the nodes in FVS as feedback nodes. An example of a graph and its FVS is given in Figure 1, where the complete graph (Figure 1a), however, becomes a cycle-free graph (Figure 1b) when nodes 1 and 2 are removed, and thus the sentence {1, 2} is a FVS \u2212 11 graph with small FVSs examined in different contexts. [17] The authors have characterized the family of graphs with small FVSs and their obstacle sets (sets of prohibited minors). FVSs are also related to the \"stable sets\" in the study of tournaments [18]. Given a GGM with an FVS of size k (where the FVS may or may not be specified), the marginal means and variances \u00b5i (J \u2212 1h we can be selected for the message exactly) are."}, {"heading": "4 Learning GGMs with Observed or Latent FVS of Size k", "text": "In this section we will examine the problem of restoring a GGM from i.i.d. samples in which the feedback nodes are either observed or latent variables. If all nodes are observed, the empirical distribution1 Generally speaking, a graph does not have a unique FVS. The family of graphs with FVSs of size k includes all graphs in which there is an FVS of size k.p (xF, xT), which is parameterized by the empirical covariance matrix \u03a3 =. If the feedback nodes are latent variables, the empirical distribution p has an empirical covariance matrix \u043d\u0430\u043d\u043d\u0438\u043d\u0438\u0441\u0435\u0441\u0435\u0441\u0435\u0441\u0442\u0438. In case of slight notation abuse, we use q (xA) to denote the marginal distribution of A under a distribution xV (xq)."}, {"heading": "4.1 When All Nodes Are Observed", "text": "If all nodes are observed, we have two cases: 1) If an FVS of size k is specified, we propose the conditioned Chow Liu algorithm, which efficiently calculates the exact ML estimate; 2) If no FVS is specified a priori, we propose both an exact algorithm and a greedy approximate algorithm to calculate the ML estimate."}, {"heading": "4.1.1 Case 1: An FVS of Size k Is Given.", "text": "If a size-k FVS F is given, the learning problem becomes solvingpML (xF, xT) = arg min q (xF, xT). (1) This optimization problem is defined on a highly non-convex set of QF with combinatorial structures: in fact, there are (n \u2212 k) n \u2212 k \u2212 2 possible trees among the subgraphs induced by the non-feedback nodes. (1) However, we are able to solve problem (1) precisely using the conditioned chow-liu algorithm described in Algorithm 1.2. The intuition behind this algorithm is that the entire graph is not tree, the subgraph induced by the non-feedback nodes (which correspond to the distribution of the non-feedback nodes) that we have conditioned on the feedback nodes, and therefore we can find the best tree among the non-feedback nodes."}, {"heading": "4.1.2 Case 2: The FVS Is to Be Learned", "text": "In this subsection, we present both exact and approximate algorithms for learning models with a size of FVS no greater than k (i.e. in Qk). For a fixed empirical distribution p (xF, xT), we define d (F), a specified function of FVS F as a minimum value of (1), i.e., 2Note that the conditional Chow Liu algorithm differs here from other variants of the Chow Liu algorithm, such as in [20], where the extensions differ in the inclusion or exclusion of a set of edges.d (F) = min q (xF, xT) and QFDKL (p, xT)."}, {"heading": "4.2 When the FVS Nodes Are Latent Variables", "text": "If the feedback nodes are latent variables, the marginal distribution of the observed variables (the non-feedback nodes in the true model) is. (The non-feedback nodes in the real model have an information matrix. (The non-feedback nodes in the real model) has an information matrix. (The exact J-T structure is known, the learning problem is equivalent to the decomposition of a given inverse covariance matrix whose matrix J is the sum of a tree-structured matrix JT and a rank-k matrix \u2212 JMJ \u2212 1F JTM.3 Generally, we use the ML criterion qML (xF, xT) = arg min q q (xF). (xT).We project the optimization across all nodes (latent and observed), while the K-L divergence is defined in the objective function."}, {"heading": "5 Experiments", "text": "This year we have it in hand, in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year."}, {"heading": "6 Future Directions", "text": "Our experimental results show the potential of these algorithms and indicate, as in the paper [14], that the selection of FVSs of size O (log n) works well, leading to algorithms that can be scaled to major problems. Theoretical guarantees for this scaling (e.g. by specifying model classes for which such a size provides asymptotically accurate models of FVS) are therefore a compelling open problem. In addition, the inclusion of complexity in the FVS order problem (e.g. in AIC or BIC) is another direction we are pursuing. Furthermore, we are working to extend our results to non-Gaussian conditions."}, {"heading": "Acknowledgments", "text": "This research was partially supported by AFOSR under the auspices of FA9550-12-1-0287."}, {"heading": "Appendix of \u201cLearning Gaussian Graphical Models with Observed or Latent FVSs\u201d", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Computing the Partition Function of GGMs in QF", "text": "In section 3 of the paper we found that we can specify the information matrix J of a GGM with an FVS of size k (J = J) exactly J (J = J = J). (J = J = J) This algorithm is developed by the FMP algorithm in [14] and is described in algorithm 4.Algorithm T with a calculation function when an FVS with an information matrix of size k and an n-th information matrix J = [JF J T MJM JT] where JT with an edge-set ET. Output: J1 standard Gaussian BP on T with an information matrix JT we get Tii = (J \u2212 1T) iiii for all i-T, P Tij Tij = (J \u2212 1 T) ij for all."}, {"heading": "B Proof for Proposition 1", "text": "B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B."}, {"heading": "C Proof of Proposition 2", "text": "In this section, we first demonstrate a more general result indicated in Lemma 7 Lemma 7. (In algorithm q (= q (= q (= q)) (Step 2 (a) and Step 2 (b), we can calculate exactly, then we have that the equality p (xT) is fulfilled if and only if p (t) (xF) (t + 1) (xT) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (p), T) (T) (T) (T) (T) (p) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T) (T), T (T) (T) (T) (T) (T), T (T) (T) (T) (T) (T), T (T) (T) (T), T (T) (T), T (T) (T) (T), T (T) (T), T (T) (T) (T), (T) (T) (T), (T) (T) (T), (T (T) (T)."}, {"heading": "D The Accelerated Latent Chow-Liu Algorithm", "text": "In this section we describe the accelerated latent Chow-Liu algorithm (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J) J (J) J (J) J (J) J) J (J) J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J) J (J) J (J) J) J (J) J (J) J) J (J) J) J (J) J) J (J) J) J (J) J) J (J) J) J (J) J (J) J) J (J) J (J) J) J (J) J (J) J (J) J) J (J) J (J) J (J) J (J) J) J (J) J (J) J (J) J (J) J (J) J) J (J) J (J) J (J) J (J) J (J) J (J) J) J (J) J (J) J (J) J (J) J (J) J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J) J (J (J) J (J) J (J) J (J) J (J) J (J) J (J (J) J (J) J (J) J (J) J (J (J) J (J) J) J (J) J (J (J) J (J) J (J (J) J (J) J (J) J (J) J (J) J (J) J (J (J) J) J (J (J) J (J) J) J (J (J (J) J) J (J"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely<lb>used in many applications, and the trade-off between the modeling capacity and<lb>the efficiency of learning and inference has been an important research prob-<lb>lem. In this paper, we study the family of GGMs with small feedback vertex<lb>sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles.<lb>Exact inference such as computing the marginal distributions and the partition<lb>function has complexity O(kn) using message-passing algorithms, where k is<lb>the size of the FVS, and n is the total number of nodes. We propose efficient<lb>structure learning algorithms for two cases: 1) All nodes are observed, which is<lb>useful in modeling social or flight networks where the FVS nodes often corre-<lb>spond to a small number of highly influential nodes, or hubs, while the rest of<lb>the networks is modeled by a tree. Regardless of the maximum degree, without<lb>knowing the full graph structure, we can exactly compute the maximum likelihood<lb>estimate with complexity O(kn + n log n) if the FVS is known or in polyno-<lb>mial time if the FVS is unknown but has bounded size. 2) The FVS nodes are<lb>latent variables, where structure learning is equivalent to decomposing an inverse<lb>covariance matrix (exactly or approximately) into the sum of a tree-structured ma-<lb>trix and a low-rank matrix. By incorporating efficient inference into the learning<lb>steps, we can obtain a learning algorithm using alternating low-rank corrections<lb>with complexity O(kn + n log n) per iteration. We perform experiments using<lb>both synthetic data as well as real data of flight delays to demonstrate the modeling<lb>capacity with FVSs of various sizes.", "creator": "LaTeX with hyperref package"}}}