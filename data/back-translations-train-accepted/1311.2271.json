{"id": "1311.2271", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2013", "title": "More data speeds up training time in learning halfspaces over sparse vectors", "abstract": "The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a {\\em computational} resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task?", "histories": [["v1", "Sun, 10 Nov 2013 13:28:19 GMT  (19kb)", "http://arxiv.org/abs/1311.2271v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "nati linial", "shai shalev-shwartz"], "accepted": true, "id": "1311.2271"}, "pdf": {"name": "1311.2271.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 131 1.22 71v1 [cs.LG] 1 0N ovThe increasing availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the additional examples to speed up the computational time required to perform the learning task? We give the first positive answer to this question for a natural, supervised learning problem - we consider agnostic PAC learning of half-spaces over 3-sparse vectors in {\u2212 1, 1, 0} n. This course is inefficiently learnable using O (n / 2) examples. Our main contribution is a novel, non-cryptographic methodology for creating computer statistical gaps, which allows us to show that under the widely accepted assumption that the rejection of random 3F formulas is impossible to learn on the basis of only class O / n."}, {"heading": "1 Introduction", "text": "In the modern digital age, we are facing a rapid growth of available datasets in science and technology."}, {"heading": "1.1 Previous approaches, difficulties, and our techniques", "text": "In fact, it is such that the greater number of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to move, to move, to move, to fight, to move, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2 Background and notation", "text": "For hypotheses class H-1-X and a group Y-X we define the constraint of H to Y by H-Y = h-Y-h-H-H. With J = Jn we denote the all-one-n-matrix. We denote the j-th vector in the standard basis of Rn by ej."}, {"heading": "2.1 Learning Algorithms", "text": "For h, 3 \u2192 {\u00b1 1} and a distribution D to Cn, 3 \u00b7 {\u00b1 1} we denote the error of h w.r.t. D by ErrD (h) = Pr (x, y) \u0445 D (x) 6 = y). For a sample S (Cn, 3 \u00d7 {\u00b1 1} Cn, 3 we denote the error of H (H) the error of h (resp. H) w.r.t. The empirical distribution induces by the sample S. A learning algorithm, L, receives a sample S (Cn, 3 \u00d7 1}) m and returns a hypothesis L (S)."}, {"heading": "5 Discussion", "text": "Our evidence of the lower limit is based on a novel, non-cryptographic technique to justify such trade-offs. We also derive a new, non-trivial upper limit for this assignment. Open questions. An obvious open question is to close the gap between the lower and upper limit. We suspect that Hn, 3 can be learned efficiently from a sample of O-examples. We also believe that our new evidence technology can be used to demonstrate the complexity of computational examples for other natural learning problems. Recognition: Amit Daniely is a recipient of the Google Europe Fellowship in Learning Theory, and this research is partially supported by this Google Fellowship. Nati Linial is supported by grants from the ISF, BSF and I-Core Fellowship."}, {"heading": "A Proof of Theorem 4.1", "text": "The proof of the theory is based on the results of Hazan et al. (D > S). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D)"}], "references": [{"title": "On basing lower-bounds for learning on worstcase assumptions", "author": ["Benny Applebaum", "Boaz Barak", "David Xiao"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Applebaum et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Applebaum et al\\.", "year": 2008}, {"title": "Complexity theoretic lower bounds for sparse principal component detection", "author": ["Quentin Berthet", "Philippe Rigollet"], "venue": "In COLT,", "citeRegEx": "Berthet and Rigollet.,? \\Q2013\\E", "shortCiteRegEx": "Berthet and Rigollet.", "year": 2013}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2001}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Venkat Chandrasekaran", "Michael I. Jordan"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran and Jordan.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran and Jordan.", "year": 2013}, {"title": "Computational sample complexity", "author": ["S. Decatur", "O. Goldreich", "D. Ron"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Decatur et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Decatur et al\\.", "year": 1998}, {"title": "Zecchina (Guest Editors)", "author": ["O. Dubios", "R. Monasson", "B. Selma"], "venue": "Phase Transitions in Combinatorial Problems. Theoretical Computer Science,", "citeRegEx": "Dubios et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dubios et al\\.", "year": 2001}, {"title": "Relations between average case complexity and approximation complexity", "author": ["U. Feige"], "venue": "In STOC, pages 534\u2013543,", "citeRegEx": "Feige.,? \\Q2002\\E", "shortCiteRegEx": "Feige.", "year": 2002}, {"title": "Easily refutable subformulas of large random 3cnf formulas", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Theory of Computing,", "citeRegEx": "Feige and Ofek.,? \\Q2007\\E", "shortCiteRegEx": "Feige and Ofek.", "year": 2007}, {"title": "Near-optimal algorithms for online matrix prediction", "author": ["E. Hazan", "S. Kale", "S. Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Hazan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2012}, {"title": "Low-weight halfspaces for sparse boolean vectors", "author": ["P. Long", "R. Servedio"], "venue": "ITCS,", "citeRegEx": "Long. and Servedio.,? \\Q2013\\E", "shortCiteRegEx": "Long. and Servedio.", "year": 2013}, {"title": "Computational sample complexity and attribute-efficient learning", "author": ["R. Servedio"], "venue": "J. of Comput. Syst. Sci.,", "citeRegEx": "Servedio.,? \\Q2000\\E", "shortCiteRegEx": "Servedio.", "year": 2000}, {"title": "Using more data to speed-up training time", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Eran Tromer"], "venue": "In AISTATS,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2012}, {"title": "The hypothesis class of \u03b2-decomposable n \u00d7m matrices with \u00b11 entries ban be efficiently learnt using a sample of O", "author": ["Hazan"], "venue": null, "citeRegEx": "Hazan,? \\Q2012\\E", "shortCiteRegEx": "Hazan", "year": 2012}, {"title": "Hi there exist a learning algorithm that learns Hi using \u2264 C(d + log(1/\u03b4))/\u01eb examples, for some constant C \u2265 8. Consider the algorithm A which The result of Hazan et al", "author": ["Hazan"], "venue": null, "citeRegEx": "Hazan,? \\Q2012\\E", "shortCiteRegEx": "Hazan", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "\u2022 Learning Preferences [Hazan et al., 2012]: Here, we have n players.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": "Concretely, it is not clear how to use standard reductions from NP hard problems in order to establish lower bounds for improper learning (moreover, Applebaum et al. [2008] give evidence that such simple reductions do not exist).", "startOffset": 149, "endOffset": 173}, {"referenceID": 0, "context": "Concretely, it is not clear how to use standard reductions from NP hard problems in order to establish lower bounds for improper learning (moreover, Applebaum et al. [2008] give evidence that such simple reductions do not exist). The classes Hn,k and similar classes have been studied by several authors (e.g. Long. and Servedio [2013]).", "startOffset": 149, "endOffset": 336}, {"referenceID": 8, "context": "In Hazan et al. [2012] it was shown that Pn can be efficiently learnt using O ( n log(n) \u01eb2 )", "startOffset": 3, "endOffset": 23}, {"referenceID": 6, "context": "under Feige\u2019s assumption regarding the hardness of refuting random 3CNF formulas [Feige, 2002].", "startOffset": 81, "endOffset": 94}, {"referenceID": 8, "context": "Item 2 is proved in section 4, relying on the results of Hazan et al. [2012]. We note, however, that a weaker result, that still suffices for answering Question 1 in the affirmative, can be proven using a naive improper learning algorithm.", "startOffset": 57, "endOffset": 77}, {"referenceID": 4, "context": "1 Previous approaches, difficulties, and our techniques [Decatur et al., 1998] and [Servedio.", "startOffset": 56, "endOffset": 78}, {"referenceID": 10, "context": ", 1998] and [Servedio., 2000] gave positive answers to Question 1 in the realizable PAC learning model.", "startOffset": 12, "endOffset": 29}, {"referenceID": 11, "context": "[Shalev-Shwartz et al., 2012] showed a similar result for the agnostic PAC learning model.", "startOffset": 0, "endOffset": 29}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning.", "startOffset": 10, "endOffset": 38}, {"referenceID": 1, "context": "Therefore, it is not clear whether the approach given in [Berthet and Rigollet, 2013] can be used to establish computational-statistical gaps in supervised learning problems.", "startOffset": 57, "endOffset": 85}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning. Concretely, they studied the problem of sparse PCA, namely, finding a sparse vector that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. Our work, as well as the previous work of Decatur et al. [1998], Servedio.", "startOffset": 11, "endOffset": 436}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning. Concretely, they studied the problem of sparse PCA, namely, finding a sparse vector that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. Our work, as well as the previous work of Decatur et al. [1998], Servedio. [2000], Shalev-Shwartz et al.", "startOffset": 11, "endOffset": 454}, {"referenceID": 1, "context": "Recently, [Berthet and Rigollet, 2013] gave a positive answer to Question 1 in the context of unsupervised learning. Concretely, they studied the problem of sparse PCA, namely, finding a sparse vector that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted clique problem, they gave a positive answer to Question 1 for sparse PCA. Our work, as well as the previous work of Decatur et al. [1998], Servedio. [2000], Shalev-Shwartz et al. [2012], studies Question 1 in the supervised learning setup.", "startOffset": 11, "endOffset": 484}, {"referenceID": 5, "context": "a special issue of TCS Dubios et al. [2001]).", "startOffset": 23, "endOffset": 44}, {"referenceID": 7, "context": "The best known algorithm [Feige and Ofek, 2007] 0-refutes random 3CNF with ratio \u2206(n) = \u03a9( \u221a n).", "startOffset": 25, "endOffset": 47}, {"referenceID": 6, "context": "1 (R3SAT hardness assumption \u2013 [Feige, 2002]).", "startOffset": 31, "endOffset": 44}, {"referenceID": 6, "context": "The best known algorithm [Feige and Ofek, 2007] 0-refutes random 3CNF with ratio \u2206(n) = \u03a9( \u221a n). In Feige [2002] it was conjectured that for constant \u2206 no efficient algorithm can provide a proof that a random 3CNF is not satisfiable: Conjecture 2.", "startOffset": 26, "endOffset": 113}, {"referenceID": 6, "context": "Following an argument from [Feige, 2002] (Theorem 3.", "startOffset": 27, "endOffset": 40}, {"referenceID": 6, "context": "2 ([Feige, 2002]).", "startOffset": 3, "endOffset": 16}, {"referenceID": 8, "context": "Its proof relies on results from Hazan et al. [2012] about learning \u03b2-decomposable matrices, and due to the lack of space is given in the appendix.", "startOffset": 33, "endOffset": 53}], "year": 2013, "abstractText": "The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the first positive answer to this question for a natural supervised learning problem \u2014 we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {\u22121, 1, 0}n. This class is inefficiently learnable using O ( n/\u01eb ) examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efficiently learn this class using only O ( n/\u01eb ) examples. We further show that under stronger hardness assumptions, even O ( n/\u01eb ) examples do not suffice. On the other hand, we show a new algorithm that learns this class efficiently using \u03a9\u0303 ( n/\u01eb ) examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem.", "creator": "LaTeX with hyperref package"}}}