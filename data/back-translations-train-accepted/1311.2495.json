{"id": "1311.2495", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2013", "title": "The Noisy Power Method: A Meta Algorithm with Applications", "abstract": "We provide a new robust convergence analysis of the well-known subspace iteration algorithm for computing the dominant singular vectors of a matrix, also known as simultaneous iteration or power method. Our result characterizes the convergence behavior of the algorithm when a large amount noise is introduced after each matrix-vector multiplication. While interesting in its own right, our main motivation comes from the problem of privacy-preserving spectral analysis where noise is added in order to achieve the privacy guarantee known as differential privacy. Our contributions here are twofold:", "histories": [["v1", "Mon, 11 Nov 2013 16:47:25 GMT  (22kb,D)", "https://arxiv.org/abs/1311.2495v1", null], ["v2", "Mon, 15 Sep 2014 19:17:32 GMT  (26kb,D)", "http://arxiv.org/abs/1311.2495v2", null], ["v3", "Mon, 8 Dec 2014 21:53:05 GMT  (26kb,D)", "http://arxiv.org/abs/1311.2495v3", "NIPS 2014"], ["v4", "Tue, 3 Feb 2015 23:43:37 GMT  (27kb,D)", "http://arxiv.org/abs/1311.2495v4", "NIPS 2014"]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["moritz hardt", "eric price"], "accepted": true, "id": "1311.2495"}, "pdf": {"name": "1311.2495.pdf", "metadata": {"source": "CRF", "title": "The Noisy Power Method: A Meta Algorithm with Applications", "authors": ["Moritz Hardt", "Eric Price"], "emails": ["mhardt@us.ibm.com", "ecprice@mit.edu"], "sections": [{"heading": null, "text": "Streaming PCA. A recent paper by Mitliagkas et al. (NIPS 2013) provides a space-saving algorithm for PCA in a streaming model in which samples are taken from a Gaussian covariance model. We provide a simpler and more general analysis that is applicable to arbitrary distributions and confirms experimental evidence for Mitliagkas et al. In addition, our result provides quantitative improvements in a natural parameter regime even in the Spiked covariance model. It is also much simpler and easily derived from our general convergence analysis of the noisy power method together with a matrix Chernoff limit. Private PCA. We provide the first near-linear time algorithm for the problem of differentiated private main component analysis, which achieves near-narrow worst-case error limits. If our worst-case limits are supplemented, we show that the error dependence of our algorithm on the matrix dimension can always be replaced by a dependence of the main STC in the problem of the coherence."}, {"heading": "1 Introduction", "text": "It is one of the most important algorithmic tasks underlying many applications, such as answering the basic problem of finding individual vectors in the presence of noise. [...] 3F ebcan enter the calculation through a variety of sources, including missing entries, adversarial corruption and private constraints. It is desirable to have a robust method for handling a variety of cases without the need for ad hoc analysis. [...] In this paper we consider a fast universal method for calculating the dominant vectors. [...] It is a robust method for handling a variety of cases without the need for ad hoc analysis. [...] In this paper we consider a fast universal method for calculating the dominant vectors."}, {"heading": "1.1 Application to memory-efficient streaming PCA", "text": "As part of the Streaming PCA, we receive a stream of samples z1, z2, z2, z2,. zn > > Rd, which is derived from an unknown distribution D over Rd. (...) Our goal is to calculate the dominant k eigenvectors of the covariance matrix A = > > D zz >. The challenge is to do this in space linearly in the output quantity, namely O (kd). [MCJ13] gave an algorithm for this problem based on the noisy power method. We analyze the same algorithm that we reconfigure here and call SPM: The algorithm can be executed in space O (pd), since the update step is the d \u00b7 p matrix A 'X' - 1 incrementally without explicit calculation A. The algorithm maps for our setting are executed by the definition G '= (A' \u2212 A)."}, {"heading": "1.2 Application to privacy-preserving spectral analysis", "text": "A successful paradigm in data preservation is based on either a single entry, a single line, or a ranking 1 matrix of the limited norm. Formally, however, Differential Privacy requires that the output distribution of the algorithm be altered only slightly with the addition or deletion of the data. The limit given in [MCJ13] has a dependency that is not entirely obvious. There is a Differential Privacy that the output distribution of the algorithm is altered only slightly with the addition or deletion of the data."}, {"heading": "1.3 Related Work", "text": "One might expect that a suitable analysis of the noisy power method would have been used in the numerical analysis iteration. However, we are not aware of a reference, and there are a number of points to consider. First, our noise model is adaptive and therefore differs from the classical disturbance theory of singular vector decomposition [DK70]. Second, we think of the disturbance in each step so large that it is conceptually different from the floating point errors. Third, research in numerical analysis over the past decades has largely focused on faster crylov subspace methods. There is some theory of inexact crylov methods, e.g., [SS07] which captures the effect of noisy matrix vector products in this context. In relation to our work, results are also on the perturbation stability of QR factoring, as these could be used to achieve convergence boundaries for submiteration."}, {"heading": "1.4 Open Questions", "text": "We believe that Corollary 1.1 is a fairly precise characterization of the convergence of the noisy potential method to the uppermost k singular vectors when p = k. The main error is that the noise tolerance depends on the eigengap \u03c3k \u2212 \u03c3k + 1, which could be very low. We have some conjectures for results that do not depend on this eigengap. Firstly, if p > k, we think that the consequence 1,1 is the use of the gap \u03c3k \u2212 \u03c3k + 1 instead of \u03c3k \u2212 k + 1. Unfortunately, our method of proof is based on the principle angle that decreases with each step, which does not necessarily keep up with the higher level of noise. Nevertheless, we expect the drop angle to decrease relatively quickly, so that XL will contain a subspace very close to U. We are not aware of this type of result even in noiseless environments. Guess 1.5. Let X0 have a random p \u2212 dimensional basis for kp >."}, {"heading": "2 Convergence of the noisy power method", "text": "Figure 1 shows our basic algorithm, which we analyze in this paragraph. An important tool in our analysis are the main angles, which are useful for analyzing the convergence behavior of numerical eigenvalue methods.Roughly speaking, we will show that the tangent of the k-th main angle between X and the k-th eigenvectors of A decreases in each iteration of the noise method. Definition 2.1 (main angle).Let X and Y subspaces of Rd of dimension at least k. The main angle 0 6 x x x x 6 x \u00b7 \u00b7 6 phenomenon between X and Y and the associated main vectors x1,.., xk and y1,.. yk are defined recursively using Via\u03b8i (X, Y) = min {arccos (< x, y > x \u00b2 y)."}, {"heading": "2.1 Convergence argument", "text": "We will use a non-recursive expression for the main angles, defined in terms of the Pk set of p \u00b7 p projection matrices from p \u00b7 m \u00b2 to k-dimensional subranges: Claim 2,2. Let us have the orthonormal columns and X > Rd \u00b7 p have independent columns, for p > k \u00b2 w. (U, X) = max. (U, X)."}, {"heading": "2.2 Random initialization", "text": "s random matrices [RV09].Lemma 2.5. For an arbitrary orthonormal U and random subspace X, we have the singular value decomposition U > X = A2 > of U > X. If we project the matrix onto the first k columns of B, we have the matrix probability (p + 1 \u2212 k).Proof. Consider the singular value decomposition U > X = A2 > of U > X. If we project the matrix onto the first k columns of B, we have the matrix probability (U, X).Proof."}, {"heading": "3 Memory efficient streaming PCA", "text": "The question that arises is how many samples it takes to achieve a certain level of accuracy for different distributions. Using our general analysis of the noisy power method (Figure 2), we show that the streaming method of the streaming power method (MCJ13) the streaming power method (MCJ13) is a natural algorithm that performs streaming PCA with O (dk) space. The question that arises is how many samples it takes to achieve a certain level of accuracy for different distributions D. Using our general analysis of the noisy power method, we show that the streaming power method, we are less samples and apply to more distributions than."}, {"heading": "3.1 Error term analysis", "text": "\"We have the possibility that we can apply our analysis of the noified performance method in this way.\" (B, p) -rD D is a (B) -round distribution effect. (D, p) -rD is a (B) -round distribution effect. (D, p) -rD is a (B, p) -rD. (D, p) -rD is a (B, p) -round distribution effect. (D, p) -rD is a (B, p) -round distribution effect. (B, p) -rD is a (B, p). (D, p) -rD is a (B, p) -round distribution effect. (D, p) -rD is a (B, p) -round distribution effect with all but O (1 / n2)."}, {"heading": "3.2 Proof of Theorem 3.2", "text": "In view of Lemma 3.5, we choose n in such a way that the error term in each iteration meets the assumptions of Theorem 2.4. Let G 'designate the instance of the error term occurring in the' -th iteration of the algorithm G. We can find an n satisfyingnlog (n) 4 = O Bpmax {1 / \u03b52, Bp / (\u221a p \u2212 \u221a k \u2212 1) 2} logd (\u03c3k \u2212 \u03c3k + 1) 2d such that with Lemma 3.5 we have a probability of 1 \u2212 O (1 / n2), that we have that with probability 1 \u2212 K (1 / n2) and that the 1 / n2 term in Lemma 3.5 is of lower order. With this bond, it follows from Theorem 2.4 that according to the definition 1 / n \u03b5 and 1 / n \u0441k \u2212 \u03c3k + 1 / \u03c3k \u00b2 (\u03c3k) that the 1 / \u03c3k \u00b2 probability (\u03c3k) is 1 / \u03c3k \u00b2."}, {"heading": "3.3 Proof of Lemma 3.6 and Corollary 3.4", "text": "II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II II, II, II, II"}, {"heading": "4 Privacy-preserving singular vector computation", "text": "In this section, we will demonstrate our results on the preservation of privacy in this area. We will start with a standard definition of differential privacy, sometimes referred to as entry-level differential privacy, because it hides the presence or absence of a single input. \u2212 Definition 4.1 (Differential Privacy) A randomized algorithm M: Rd \u00b7 d \u00b2 \u2192 R (where R is an arbitrary abstract range) is (\u03b5) -differential private, if for all pairs of matrices A, A \u00b2 Rd \u00b7 d \u00b2 are distinguished by no more than 1 in absolute value, we have the algorithm satisfied for all subsets of range S R \u2212 p: Pr {M (A)."}, {"heading": "4.1 Low-rank approximation", "text": "Our results readily indicate that we can calculate differentiated private approximate values with low rank. The most important observation is that, provided XL and U have the same dimension, tan\u03b8 (U, XL) 6 \u03b1 implies that the XL matrix also leads to a good approximation to A in the spectral standard with low rank. In particular, it is again the \"norm\" of XL columns that determines the required size of noise. As A is symmetrical, we have X > A = (AX) > L A that can easily be carried out in a private manner. Therefore, it is the \"norm\" of XL columns that determines the required size of noise. As A is symmetrical, we have X > A = (AX) > L A +. To obtain a good approximation with low rank, it is sufficient to calculate the AXL privately as AXL + GL."}, {"heading": "4.2 Principal Component Analysis", "text": "The idea is particularly relevant in an environment where we imagine a sum of 1 matrices, each having a limited spectral standard for all pairs of matrices A, A, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D,"}, {"heading": "4.3 Dimension-free bounds for incoherent matrices", "text": "The guarantee in Theorem 1.3 depends on the quantity \u0430 X ', which in principle could be as small as \u221a 1 / d. Nevertheless, in the above theorems we use the trivial upper limit 1. This, in turn, led to a dependence on the dimensions of A in our theorems. Here, we show that dependence on the dimension can be replaced by an essentially narrow dependence on the coherence of the input matrix. In doing so, we solve the main problem left open by Hardt and Roth [HR13]. The definition of coherence that we use is formally defined as follows. Definition 4.8 (Matrix Coherence) x x x x x x x x x x x x x x x. We say that a matrix A \u00b7 d \u00b7 d \u00b2 with a singular value composition A = UDP > has a coherence between the two."}, {"heading": "4.4 Proofs of supporting lemmas", "text": "In fact, it is such that it is a matter of a way in which people act in the countries of the world in which they live, in the world in which they live, in the world, in the world, in the world in which they live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in the world, in the, in the, in the, in the world, in the, in the world, in the, in, in the, in the, in the, in the world, in, in the, in the, in, in the, in, in the, in, in the, in the, in the, in, in, in, in the, in, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in, in the, in the, in, in the, in, in the, in, in the, in the, in, in, in the, in the, in, in the, in, in, in the, in, in, in, in, in"}, {"heading": "A Deferred Concentration Inequalities", "text": "Let the random variables X1,. > > Xm be independent random variables, so that for each i, Xi border [\u2212 1,1] is almost certain. Let X = \u2211 m i = 1Xi and let \u03c32 = V X. Let us then for each t > 0, Pr {| X \u2212 EX | > t} 6 exp (\u2212 t24\u03c32).The next problem follows from standard concentration characteristics of the Gaussian distribution. Lemma A.2. Let U = k be a matrix with orthonormal columns. Let G1,.., GL = N (0, \u03c32) d \u00b7 p with k 6 d and assume that L 6 d. Then, with probability 1 \u2212 10 \u2212 4, max."}, {"heading": "B Reduction to symmetric matrices", "text": "For all our purposes, it is sufficient to look at symmetrical n \u00b7 n matrices. In the case of a non-symmetrical m \u00b7 n matrix B, we can always look at the (m + n) \u00b7 (m + n) matrix A = [0B | B > 0]. This transformation preserves all the parameters we are interested in, as was more formally argued in [HR13], allowing us to discuss symmetrical intrinsic decompositions instead of singular vector decompositions and thus simplify our presentation below."}], "references": [{"title": "Stochastic optimization for pca and pls", "author": ["Raman Arora", "Andrew Cotter", "Karen Livescu", "Nathan Srebro"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The JohnsonLindenstrauss transform itself preserves differential privacy", "author": ["Jeremiah Blocki", "Avrim Blum", "Anupam Datta", "Or Sheffet"], "venue": "In Proc. 53rd Foundations of Computer Science (FOCS),", "citeRegEx": "Blocki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blocki et al\\.", "year": 2012}, {"title": "The fast convergence of incremental PCA", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Proc. 27th Neural Information Processing Systems (NIPS),", "citeRegEx": "Balsubramani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balsubramani et al\\.", "year": 2013}, {"title": "Practical privacy: the SuLQ framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In Proc. 24th PODS,", "citeRegEx": "Blum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2005}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J. Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computional Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Near-optimal differentially private principal components", "author": ["Kamalika Chaudhuri", "Anand Sarwate", "Kaushik Sinha"], "venue": "In Proc. 26th Neural Information Processing Systems (NIPS),", "citeRegEx": "Chaudhuri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "The power of convex relaxation: nearoptimal matrix completion", "author": ["Emmanuel J. Cand\u00e8s", "Terence Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["Chandler Davis", "W.M. Kahan"], "venue": "iii. SIAM J. Numer. Anal.,", "citeRegEx": "Davis and Kahan.,? \\Q1970\\E", "shortCiteRegEx": "Davis and Kahan.", "year": 1970}, {"title": "Analyze Gauss: optimal bounds for privacy-preserving principal component analysis", "author": ["Cynthia Dwork", "Kunal Talwar", "Abhradeep Thakurta", "Li Zhang"], "venue": "In Proc. 46th Symposium on Theory of Computing (STOC),", "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "In Proc. 55th Foundations of Computer Science (FOCS). IEEE,", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "Accuracy and Stability of Numerical Algorithms", "author": ["Nicholas J. Higham"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Higham.,? \\Q2002\\E", "shortCiteRegEx": "Higham.", "year": 2002}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Beating randomized response on incoherent matrices", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Proc. 44th Symposium on Theory of Computing (STOC),", "citeRegEx": "Hardt and Roth.,? \\Q2012\\E", "shortCiteRegEx": "Hardt and Roth.", "year": 2012}, {"title": "Beyond worst-case analysis in private singular vector computation", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Proc. 45th Symposium on Theory of Computing (STOC). ACM,", "citeRegEx": "Hardt and Roth.,? \\Q2013\\E", "shortCiteRegEx": "Hardt and Roth.", "year": 2013}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proc. 45th Symposium on Theory of Computing (STOC),", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "On differentially private low rank approximation", "author": ["Michael Kapralov", "Kunal Talwar"], "venue": "In Proc. 24rd Symposium on Discrete Algorithms (SODA). ACM-SIAM,", "citeRegEx": "Kapralov and Talwar.,? \\Q2013\\E", "shortCiteRegEx": "Kapralov and Talwar.", "year": 2013}, {"title": "Memory limited, streaming PCA", "author": ["Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain"], "venue": "In Proc. 27th Neural Information Processing Systems (NIPS),", "citeRegEx": "Mitliagkas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mitliagkas et al\\.", "year": 2013}, {"title": "Differentially private recommender systems: building privacy into the net", "author": ["Frank McSherry", "Ilya Mironov"], "venue": "In Proc. 15th KDD,", "citeRegEx": "McSherry and Mironov.,? \\Q2009\\E", "shortCiteRegEx": "McSherry and Mironov.", "year": 2009}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Rudelson and Vershynin.,? \\Q2009\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2009}, {"title": "Recent computational developments in krylov subspace methods for linear systems", "author": ["Valeria Simoncini", "Daniel B. Szyld"], "venue": "Numerical Linear Algebra With Applications,", "citeRegEx": "Simoncini and Szyld.,? \\Q2007\\E", "shortCiteRegEx": "Simoncini and Szyld.", "year": 2007}], "referenceMentions": [], "year": 2015, "abstractText": "We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call the noisy power method. Our result characterizes the convergence behavior of the algorithm when a significant amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing adhoc convergence bounds and resolves a number of open problems in multiple applications: Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a gaussian spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions confirming experimental evidence of Mitliagkas et al. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. It is also notably simpler and follows easily from our general convergence analysis of the noisy power method together with a matrix Chernoff bound. Private PCA. We provide the first nearly-linear time algorithm for the problem of differentially private principal component analysis that achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013). The coherence is always bounded by the matrix dimension but often substantially smaller thus leading to strong average-case improvements over the optimal worst-case bound.", "creator": "LaTeX with hyperref package"}}}