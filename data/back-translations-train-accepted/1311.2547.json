{"id": "1311.2547", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2013", "title": "Learning Mixtures of Linear Classifiers", "abstract": "We consider a regression (discriminative learning) problem, whereby the regression function is a convex combination of $k$ linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a `mirroring' trick, that discovers the subspace spanned by the classifiers' parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency.", "histories": [["v1", "Mon, 11 Nov 2013 19:50:51 GMT  (115kb)", "http://arxiv.org/abs/1311.2547v1", null], ["v2", "Fri, 11 Apr 2014 18:52:34 GMT  (127kb)", "http://arxiv.org/abs/1311.2547v2", null], ["v3", "Mon, 14 Jul 2014 18:26:20 GMT  (128kb)", "http://arxiv.org/abs/1311.2547v3", null], ["v4", "Wed, 30 Jul 2014 23:40:04 GMT  (134kb)", "http://arxiv.org/abs/1311.2547v4", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yuekai sun", "stratis ioannidis", "andrea montanari"], "accepted": true, "id": "1311.2547"}, "pdf": {"name": "1311.2547.pdf", "metadata": {"source": "META", "title": "Learning Mixtures of Linear Classifiers", "authors": ["Yuekai Sun", "Andrea Montanari", "Yi"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 131 1.25 47v1 [cs.LG] 1 1N ov"}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "1.1. Technical contribution and related work", "text": "Our approach is related to the method for dimensionality reduction and data visualization proposed by Li [11] and further developed by Cook [6] and colleagues. It generalizes the main component analysis for regression (discriminatory) setting, with each data point consisting of a characteristic vector Xi-Rd and a label Yi-R. In summary, the idea is to form the \"Hessian\" matrix H-N-1-N-I = 1 YiXiX-T i-Rd-d-d. (To simplify the exposure, we assume that the Xis has zero mean and unit covariance.) The eigenvectors associated with eigenvalues of the highest magnitude are used to identify a subspace in Rd on which the characteristic vectors Xi-D are projected."}, {"heading": "2. Problem Formulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Model", "text": "(Consider a dataset containing n i.i.d. pairs (Xi, Yi), Rd \u00b7 (\u2212 1, + 1}, i [n]. We refer to the Xi-Rd vectors as characters and the binary variables as labels. We assume that the Xi-Rd characters are taken from a Gaussian distribution, with the labels distributed as follows: Pr (Yi = + 1 | Xi) = 1 pf (< u, Xi >). (2) Here k \u2265 2 is the number of components in the mix, i.e., the labels are distributed as follows: Pr (Yi = + 1 | Xi) = 1 pf (< u, Xi >)."}, {"heading": "2.2. Subspace Estimation, Prediction and Clustering", "text": "Our main focus is on the following task: Subspace estimation: After observation (Xi, Yi), we estimate the subspace spanned by the profiles of the k classifiers, i.e. U-span (u1,..., UK).In U estimation of U, we characterize the performance over the main angle between the two rooms, namely dP (U, U) = max x-U, y-U-arcs (< x, y > x-Y-y-arcs).Note that the projection of the characteristics Xi to U does not result in loss of information. (2); this can be used to reduce dimensions, with the purpose of performing tasks that go beyond the subspace estimation. We will review two such tasks below. Prediction: In view of a new characteristic vector Xn + 1, we predict the corresponding designation Yn + 1.Clustering: In view of a new characteristic vector and characterization pair (Xn + 1, we can see the space as a five-year improvement)."}, {"heading": "2.3. Technical Preliminary", "text": "Here we review a few definitions used in our exhibition. The subgaussian norm of a random variable X is: \"X-2 = sup p p.\" We say that a random vector X-Rd is \"sub-gaussian\" if it is \"X-Rd.\" We say that a random vector X-Rd is \"sub-gaussian\" if < y, X-Rd \"is for each y-Rd, and allow that\" X-Rd \"2\" [2] [3] [4] is a function h: Rd \"\u2192 R\" which is differentiable and satisfactory to the identity of stone almost everywhere (a.e.) [13, 19]. Let X-Rd, X-Rd \"collectively be Gaussian\" random vectors, and consider a function h: Rd \"\u2192 R\" which (a.e.) is almost everywhere (a.e.) differentiable and satisfactory to the identity of stone [13]."}, {"heading": "3. Subspace Estimation", "text": "In this section, we present our algorithm for estimating the subspace, which we call SpectralMirror. Our most important technical contribution, which is formally set out below, is that the output of SpectralMirror is a consistent estimator of subspace U, which is spanned by the parameter vectors u1, as soon as n \u2265 C d is required for a sufficiently large constant. Algorithm 1 SpectralMirror Requiire: Pairs (Xi, Yi), i [n] Ensure: Subspace estimation U \u03c01: 1 x 1 x 1 x 2 x 1 x 1 x 2 x 2 x 1 x 1 x 1 x 1 x 2 x 1 x 1 x 1 x 1 x 2 x 2 x 2 x 2 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 3 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 2 x 1 x 1 x 2 x 1 x 1 x 1 x 2 x 1 x 1 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 1 x 2 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 2 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x"}, {"heading": "3.1. Spectral Mirror Algorithm", "text": "We start by splitting our algorithms into two halves. Our algorithms consist of three main steps: First, as pre-processing, we estimate the mean and covariance of the underlying characteristics Xi. Second, using these estimates, we identify a vector r that is near the convex spectrum spanned by the profiles (uo). Finally, we calculate a weighted covariance matrix Q (uo) in which each point is captured by the mirrored labels of that matrix. Finally, we will calculate a weighted covariance matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix values."}, {"heading": "3.2. Main Result", "text": "Our main result is that SpectralMirror is a consistent estimator of sub-space extending over u1,.., and Theorem 1. Use U to indicate the output of SpectralMirror, and let P-r-I \u2212 rrT / r-2 be the orthogonal to r given by (6). Then, if \u00b5 = 0 exists, there is 0 > 0 so that for all [0, 0], Pr (P-r-U, U-2) >. In other words, U returns an accurate estimate of P-r-U as soon as n is significantly greater than d. Note that this is no guarantee that U-r-U spans the direction."}, {"heading": "4. Proof of Theorem 1", "text": "We start with the characterization of the boundary point of the mirror direction r = > Note that E [Y | X = = x] = x = x (1) x (1) x (1) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x) x (2) x (2) x (2) x) x (2) x (2) x) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x) x (2) x (x) x (2) x) x (2) x (2) x (1) x (2) x (1) x (2) x (2) x (1) x (2) x (2) x (1) x (2) x (2) x (2) x (1) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (1) x (2) x (2) x (2) x (2) x (2) x (1) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x () x (2) x (2) x (2) x (2) x () x (2) x (2) x (2) x (2) x () x (2) x (2) x (2) x (2) x (2) x (2) x () x (2) x (2) x (2) x (2) x (2) x () x (2) x () x (2) x) x) x"}, {"heading": "4.1. Extensions", "text": "Although our proof above applies to \u00b5 = 0, this assumption is only applied to determine that the parameters c-in (8) are non-zero and therefore all directions in P-r U in the range of Q. Generally, these parameters can be calculated in closed form for each \u00b5 to (8) and will be non-zero for generic \u00b5s. In addition, the Gaussiveness of X is used to determine that the \"white\" characteristics W are uncorrelated, which in turn results in equation. (7) Again, we believe that the theorem can be extended to more general distributions, provided that this fact - i.e. that characteristics below the transformation stage \u03a3 -12 - are still true."}, {"heading": "5. Experiments", "text": "We attribute the convergence to the effective sample quantities n / d (a).We attribute the convergence to the different values we have for the different values in the different areas of the world. (a) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are unable to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data. (c) We are not able to analyze the data."}, {"heading": "6. Conclusions", "text": "We have proposed SpectralMirror, a method for discovering the range of a mixture of linear classifiers. Our method is based on a nonlinear transformation of the labels, which we call \"mirroring.\" In addition, we have provided consistency guarantees and non-asymptotic limits, which also imply the near-optimal statistical efficiency of the method. Finally, we have shown that despite the fact that SpectralMirror only approximates the range, this is sufficient to allow both a significant improvement in prediction and clustering when the characteristics are projected onto the estimated range. We have already discussed several technical issues that remain open and which we believe are suitable for further analysis, including the extension of Theorem 1 to generic \u00b5, the modification of the causality assumption and the application of our limits to other pHd-inspired methods. An additional research topic is the further improvement of the computational complexity of the estimation of the eigenvector Q. \""}, {"heading": "Appendix A: A Large-Deviation Lemma", "text": "Lemma 6. Let X Rd be a sub-Gaussian Random Vector, i.e.: < a, X > is subGaussian for any type of \"Rd.\" Then there are universal constants c1, c2, [2], [2], [3], [4], [4], [4], [5], [5], [5], [6], [6], [6], [6], [6], [7 [7], [7], [7], [7], [7], [8], [8], [8], [8], [8], [8], [8], [8], [8], [9], [9], [9], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], 8], [8], [8], [8], [8], 8, 8, [8], 8], 8, 8, [8], 8, [8], 8, [8], 8, 8, 8, [8], 8], 8, 8, 8, 8, 8, 8, 8, 8, 8], 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "Appendix B: Proof of Lemma 1 (Weak Convergence of r\u0302)", "text": "(1), (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (, (2), (2), (2), (2), (2), (2), (, (2), (2), (2), (, (2), (, (2), (, (2), (, (2), (2), (2), (2), (2), (, (, (2), (, (2), (2), (2), (2), (, (, (2), (2), (2),"}, {"heading": "Appendix C: Proof of Lemma 3 (Weak Convergence of Q\u0302)", "text": "Let us project r onto the span {u, v} i > Q \"(Q). If we are in the span\" Q \"to\" Q \"(Q), we can express ourselves as\" K \"(2), such as\" K \"(1),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\" Z \"(2),\"), \"Z\" (2), \"Z\" (2), \"Z\" (2), \"Z\" (2), \"Z\" (2)."}, {"heading": "Appendix D: Principal Hessian Directions", "text": "In this section, we apply the main Hessian instructions (pHd) = > method to our environment and demonstrate its failure to discover the space spanned by the parameter profile when \u00b5 = 0. Remember that pHd considers an environment in which the characteristics Xi-Rd are i.i.d. and are normally distributed with the mean \u00b5 and covariance \u03a3, while the designations Yi-R lie in expectation of a k-dimensional manifold. In particular, some smooth h: Rk \u2192 R, k-d: E [Y | X = x] = h (< u1, x >,. < uk, x >) where uRd, k]. The method effectively generates an estimated H-shape = n \u2212 n i = 1Yi ability \u2212 1 2XiX T i-1, x-2. \""}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky"], "venue": "arXiv preprint arXiv:1210.7559,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Latent variable models and factor analysis: A unified approach, volume 899", "author": ["David J Bartholomew", "Martin Knott", "Irini Moustaki"], "venue": "Wiley. com,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Latent variable models. In Learning in graphical models, pages 371\u2013403", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Spectral experts for estimating mixtures of linear regressions", "author": ["Arun Tejasvi Chaganty", "Percy Liang"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Principal hessian directions revisited", "author": ["R Dennis Cook"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["Michael I Jordan", "Robert A Jacobs"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "On principal hessian directions for data visualization and dimension reduction: another application of stein\u2019s lemma", "author": ["Ker-Chau Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "An end-to-end discriminative approach to machine translation", "author": ["Percy Liang", "Alexandre Bouchard-C\u00f4t\u00e9", "Dan Klein", "Ben Taskar"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Siegel\u2019s formula via stein\u2019s identities", "author": ["Jun S Liu"], "venue": "Statistics  Probability Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "Finite mixture models", "author": ["Geoffrey McLachlan", "David Peel"], "venue": "Wiley. com,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Conditional random fields for object recognition", "author": ["Ariadna Quattoni", "Michael Collins", "Trevor Darrell"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Estimation of the mean of a multivariate normal distribution", "author": ["Charles M Stein"], "venue": "In Prague Symposium on Asymptotic Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1973}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Since Pearson\u2019s seminal contribution [17], and most notably after the introduction of the EM algorithm [7], mixture models and latent variable models have played a central role in statistics and machine learning, with numerous applications\u2014see, e.", "startOffset": 103, "endOffset": 106}, {"referenceID": 12, "context": ", McLachlan & Peel [14], Bishop [4], and Bartholomew et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": ", McLachlan & Peel [14], Bishop [4], and Bartholomew et al.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].", "startOffset": 108, "endOffset": 115}, {"referenceID": 3, "context": "Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 8, "context": "Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10].", "startOffset": 106, "endOffset": 113}, {"referenceID": 15, "context": "They have also found numerous applications ranging from object recognition [18] to machine translation [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "They have also found numerous applications ranging from object recognition [18] to machine translation [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "Recently, Chaganty and Liang [5] considered mixtures of linear regressions, whereby the relation between labels and feature vectors is linear within each 1", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "[9] and Anandkumar et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], these authors propose an algorithm for fitting mixtures of linear regressions with provable guarantees.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "While the work of Chaganty and Liang [5] is a significant step forward, it leaves several open problems: Statistical efficiency.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Then, the mathematical guarantees of Chaganty & Liang [5] require a sample size n \u226b d.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "As noted in [5], this scaling is an intrinsic drawback of the tensor approach which requires working in a higher-dimensional space (tensor space) than the space in which data naturally live.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": ", Jordan & Jacobs [10], Bishop [4], Quattoni et al.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": ", Jordan & Jacobs [10], Bishop [4], Quattoni et al.", "startOffset": 31, "endOffset": 34}, {"referenceID": 15, "context": "[18], Liang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The method of [5] requires solving a regularized linear regression in d dimensions and factorizing a tensor of third order in d dimensions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting.", "startOffset": 66, "endOffset": 73}, {"referenceID": 13, "context": "On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting.", "startOffset": 66, "endOffset": 73}, {"referenceID": 9, "context": "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers.", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "We assume that the function f : R \u2192 [0, 1], characterizing the classifier response, is non-decreasing, strictly concave in [0,+\u221e), and satisfies:", "startOffset": 36, "endOffset": 42}, {"referenceID": 11, "context": "We use the following variant of Stein\u2019s identity [13, 19].", "startOffset": 49, "endOffset": 57}, {"referenceID": 16, "context": "We use the following variant of Stein\u2019s identity [13, 19].", "startOffset": 49, "endOffset": 57}, {"referenceID": 14, "context": ", [16].", "startOffset": 2, "endOffset": 6}], "year": 2017, "abstractText": "We consider a regression (discriminative learning) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a \u2018mirroring\u2019 trick, that discovers the subspace spanned by the classifiers\u2019 parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency.", "creator": "LaTeX with hyperref package"}}}