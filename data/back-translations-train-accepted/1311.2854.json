{"id": "1311.2854", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2013", "title": "Spectral Clustering via the Power Method -- Provably", "abstract": "Spectral clustering is arguably one of the most important algorithms in data mining and machine intelligence; however, its computational complexity makes challenging to use in large scale data analysis. Recently, several approximation algorithms for spectral clustering have been developed in order to alleviate the relevant costs, but theoretical results are lacking. In this paper, we present an approximation algorithm for spectral clustering with strong theoretical evidence of its performance. Our algorithm is based on approximating the eigenvectors of the Laplacian matrix using random projections, a.k.a randomized sketching. Our experimental results demonstrate that the proposed approximation algorithm compares remarkably well to the exact algorithm.", "histories": [["v1", "Tue, 12 Nov 2013 17:42:34 GMT  (527kb,D)", "http://arxiv.org/abs/1311.2854v1", "15 pages; 5 color figures"], ["v2", "Sun, 8 Feb 2015 15:55:49 GMT  (391kb,D)", "http://arxiv.org/abs/1311.2854v2", "in progress"], ["v3", "Tue, 12 May 2015 14:39:32 GMT  (391kb,D)", "http://arxiv.org/abs/1311.2854v3", "ICML 2015, to appear"]], "COMMENTS": "15 pages; 5 color figures", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["christos boutsidis", "alex gittens", "prabhanjan kambadur"], "accepted": true, "id": "1311.2854"}, "pdf": {"name": "1311.2854.pdf", "metadata": {"source": "CRF", "title": "Approximate Spectral Clustering via Randomized Sketching", "authors": ["Alex Gittens", "Prabhanjan Kambadur", "Christos Boutsidis"], "emails": ["gittens@caltech.edu", "pkambadu@us.ibm.com", "cboutsi@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Consider clustering in Figure 1. The data in this space are inseparable: there are no easily discernible cluster metrics that can be optimized to restore these particular cluster structures. In particular, in both cases, the two clusters have the same centers (centroids); therefore, distance-based cluster methods such as k-means [32] will be lousy. Motivated by deficiencies such as these, the researchers have produced a body of more flexible and data-adaptive cluster approaches known under the umbrella of Spectral Clustering. The crux of all of these approaches is that the points to be clustered are seen as linking points on a graph, and the weights of the edges between two linkages are mapped."}, {"heading": "2 The spectral clustering problem: the normalized cuts point of", "text": "rE \"s,\" he says, \"is a real problem.\" (\"It is a real problem.\") (\"It is a real problem.\"), \"It is a real problem.\" (\"It is a real problem.\"), \"It is a real problem.\" (\"It is a real problem.\"), \"It is a real problem.\" (\"It is a real problem.\"), \"(\" It is a real problem. \") (\" It is a real problem. \"),\" It is a real problem. \"(\") (\"It is a real problem.\" (\"),\" It is a problem. \"(\") (\") (\" It is a problem. \"(\") (\"(\"), \"(\" (\"),\" (\"(\"), \"(\" (\"),\" (\") (\" (\") (\") (\"(\") (\"(\") (\") (\" (\") (\") (\"(\") (\") (\") (\"(\") (\") (\") (\"(\") (\") (\" (\") (\") (\"(\") (\") (\" (\") (\") (\"(\") (\") (\" (\") (\") (\"(\") (\") (\" (\") (\") (\"(\") (\"(\") (\") (\" (\") (\") (\"(\") (\"(\") (\") (\" (\"(\") (\"(\") (\"(\" (\") (\") (\"(\") (\"(\") (\"(\") (\") (\" (\") () () () () (\" () (\"() () () () (\" () () () () (\"() (\" () (\"() (\" () () (\"() (\" () () (\"() () (\" () () () (\"() () (\" () (\"() () (\" () () (\"() () (\" () () () (\"() () (\" () ("}, {"heading": "2.1 An algorithm for spectral clustering", "text": "Motivated by the above observations, which are based on [41], Ng, Jordan and Weis [30] (see also [47]), we proposed the following algorithm for spectral clustering 1, which we consider to be the basic truth algorithm. Input to this spectral cluster algorithm consists of a series of points x1, x2,..., xn and the desired number of clusters k. The output is a disk division of the points into k clusters.Exact spectral clustering1. Construct the similarity matrix W-Rn as Wij = e-xi-xj-2\u03c3 (for i 6 = j) and Wii = 0. 2. Construct D-Rn-n as diagonal matrix of degrees nodes: Dii = j-j Wij-3. Construct W-1-2 WD-1-Rn-n.4. Find the largest vcrix point of the Y-series and then the eigencompact matrix."}, {"heading": "3 Main result", "text": "We describe our algorithms according to a small number of iterative settings, in which each iteration can be quite costly. - Our main idea is to replace step 4 in the listing above. - This method is not new: it is a classical technique that addresses the eigenvectors of the matrices. - It is as if subspace iteration in the numerical linear algebra literature (see Section 8.2.4 in [18]). The linear algebra community is characteristically occupied with such high numerical precision results as Feasible, that subspace iteration usually depends on a priori check of accuracy, as if a fixed number of iterations is being realized."}, {"heading": "4 Comparison with related work", "text": "The result that is most relevant for our results is in [22]. First, we follow the exact same framework for measuring the approximability of an algorithm for spectral clustering (see theorem 5 and corollary 6 in [22])."}, {"heading": "5 Preliminaries", "text": "A, B,.. are matrices; a, b,.. are column vectors. In is the n \u00b7 n identity matrix; 0m \u00b7 n is the m \u00b7 n matrix of zeros; 1n is the n \u00b7 1 vector of ones. Frobenius and the spectral matrix norms are A-2F = i, j A-2 ij and A-2 = max-x-k = 1-Ax-2. The SVD of A-Rm \u00b7 n of scale contain min {m, n} isA = (Uk-k) UA-Rm-Rm-0-k) A-noise-k."}, {"heading": "5.1 Random Matrix Theory", "text": "The main component of our approximate spectral cluster algorithm is to reduce the dimensions of W \u00b2 by post-multiplication with a random Gaussian matrix. At this point, we will summarize two properties of such matrices that are useful for demonstrating our main results.Lemma 5 (The norm of a random Gaussian matrix [11]) Let A \u00b2 Rn \u00b7 m be a matrix with standard Gaussian random variables, where n \u2265 4, P {\u03c31 (A) \u2265 tn 1 \u00b2 2 / 8.Lemma 6 (Invertibility of a random Gaussian matrix [38])."}, {"heading": "6 Proof of Theorem 3", "text": "First, we show that there is some orthonormal matrix Q, so the exact and approximate spectral cluster algorithms Y-Y-Q-22 \u2264 PY-PY-22.Theorem 7. Then there is an orthonormal matrix Q-Rk-k, so that the matrices Y-Y-Q-22 \u2264 PY-PY-2 2Proof. Note that if O is a square orthonormal matrix, then OT, YTO, and Y-T O are all orthonormal matrices. Also note that \"A-2\" = \"AO-2\" if O-O is orthonormal. Consequently, inf OOT = I-Y-O-O-O-O-O-O-O-O is independent."}, {"heading": "7 Error bounds for Subspace Iteration", "text": "We start with a matrix disturbance, which we use later to check our most important results for the approximate spectral unit (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). (T = 1). K. (T = 1. K = 1. T. (T = 1). (T = 1). (T = 1). (T = 1). K. (T = 1. K = 1. K. (T = 1. T = 1. K. T = 1. (T = 1). K. (T = 1. K = 1. T = 1. K. (T = 1 k). K. (T = 1 k)."}, {"heading": "8 Experiments", "text": "\"We do not believe that the quality of our experiments is so high that the quality of clustering does not suffer, and (2) show the effects of solution quality and runtime. Note that our experiments are not aimed at producing the best clusters for the test data; this requires fine-tuning several parameters such as approach, and (2) show the effects of solution quality and runtime. (3) We note that our experiments are not aimed at producing the best clusters for the test data; this requires several parameters such as approximation, and (2) show the effects of solution quality and runtime."}], "references": [{"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "In ICML", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Restarted block lanczos bidiagonalization methods", "author": ["J. Baglama", "L. Reichel"], "venue": "Numerical Algorithms, 43(3):251\u2013 272", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "The numerical treatment of integral equations", "author": ["C. Baker"], "venue": "volume 13. Clarendon Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1977}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Approximate clustering in very large relational data", "author": ["J. Bezdek", "R. Hathaway", "J. Huband", "C. Leckie", "R. Kotagiri"], "venue": "International journal of intelligent systems, 21(8):817\u2013841", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Random projections for the nonnegative least-squares problem", "author": ["C. Boutsidis", "P. Drineas"], "venue": "Linear Algebra and its Applications, 431(5-7):760\u2013771", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved matrix algorithms via the subsampled randomized hadamard transform", "author": ["C. Boutsidis", "A. Gittens"], "venue": "SIAM Journal on Matrix Analysis and Applications", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Random projections for k-means clustering", "author": ["C. Boutsidis", "A. Zouzias", "P. Drineas"], "venue": "In NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "STOC", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Local Operator Theory", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Random Matrices and Banach Spaces. In W. B. Johnson and J. Lindenstrauss, editors, Handbook of the Geometry of Banach Spaces, volume 1. Elsevier Science", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Co-clustering documents and words using bipartite spectral graph partitioning", "author": ["I.S. Dhillon"], "venue": "KDD, pages 269\u2013274. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Lower bounds for the partitioning of graphs", "author": ["W.E. Donath", "A.J. Hoffman"], "venue": "IBM Journal of Research and Development, 17(5):420\u2013425", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1973}, {"title": "Fast Monte-Carlo algorithms for approximate matrix multiplication", "author": ["P. Drineas", "R. Kannan"], "venue": "FOCS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Faster least squares approximation", "author": ["P. Drineas", "M. Mahoney", "S. Muthukrishnan", "T. Sarl\u00f3s"], "venue": "Numerische Mathematik, 117(2):217\u2013249", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Algebraic connectivity of graphs", "author": ["M. Fiedler"], "venue": "Czechoslovak Mathematical Journal, 23(2):298\u2013305", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1973}, {"title": "Spectral grouping using the Nystrom method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(2):214\u2013225", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Matrix computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "volume 3. JHU Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM Review, 53(2):217\u2013288", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.-G. Martinsson", "J.A. Tropp"], "venue": "SIAM review, 53(2):217\u2013288", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral clustering with perturbed data", "author": ["L. Huang", "D. Yan", "M. Jordan", "N. Taft"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Performance analysis of spectral clustering on compressed", "author": ["B. Hunter", "T. Strohmer"], "venue": "incomplete and inaccurate measurements. Preprint: http://arxiv.org/abs/1011.0997", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving the hungarian assignment algorithm", "author": ["R. Jonker", "T. Volgenant"], "venue": "Operations Research Letters, 5(4):171\u2013175", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1986}, {"title": "On clusterings: Good", "author": ["R. Kannan", "S. Vempala", "A. Vetta"], "venue": "bad and spectral. Journal of the ACM (JACM), 51(3):497\u2013515", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Approaching optimality for solving SDD systems", "author": ["I. Koutis", "G. Miller", "R. Peng"], "venue": "FOCS", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Power iteration clustering", "author": ["F. Lin", "W.W. Cohen"], "venue": "ICML", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Segmentation of 3d meshes through spectral clustering", "author": ["R. Liu", "H. Zhang"], "venue": "Computer Graphics and Applications, 2004. PG 2004. Proceedings. 12th Pacific Conference on, pages 298\u2013305. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A randomized algorithm for the decomposition of matrices", "author": ["P. Martinsson", "V. Rokhlin", "M. Tygert"], "venue": "Applied and Computational Harmonic Analysis", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Conductance and convergence of Markov chains-A combinatorial treatment of expanders", "author": ["M. Mihail"], "venue": "FOCS", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1989}, {"title": "et al", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849\u2013856", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "\u00dcber die praktische Aufl\u00f6sung von Integralgleichungen mit Anwendungen auf Randwertaufgaben", "author": ["E. Nystr\u00f6m"], "venue": "Acta Mathematica, 54(1):185\u2013204", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1930}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "FOCS", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral clustering of protein sequences", "author": ["A. Paccanaro", "J.A. Casbon", "M.A. Saqi"], "venue": "Nucleic acids research, 34(5):1571\u20131580", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Random projections for support vector machines", "author": ["S. Paul", "C. Boutsidis", "M. Magdon-Ismail", "P. Drineas"], "venue": "In AISTATS", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient out-of-sample extension of dominant-set clusters", "author": ["M. Pavan", "M. Pelillo"], "venue": "NIPS", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "A fast randomized algorithm for overdetermined linear least-squares regression", "author": ["V. Rokhlin", "M. Tygert"], "venue": "Proceedings of the National Academy of Sciences, 105(36):13212", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast spectral clustering with random projection and sampling", "author": ["T. Sakai", "A. Imiya"], "venue": "Machine Learning and Data Mining in Pattern Recognition, pages 372\u2013384. Springer", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Smoothed analysis of the condition numbers and growth factors of matrices", "author": ["A. Sankar", "D.A. Spielman", "S.-H. Teng"], "venue": "SIAM Journal on Matrix Analysis and Applications, 28(2):446\u2013476", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T. Sarl\u00f3s"], "venue": "In FOCS", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral clustering on a budget", "author": ["O. Shamir", "N. Tishby"], "venue": "AISTATS", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888\u2013905", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2000}, {"title": "A spectral clustering approach to finding communities in graphs", "author": ["S. Smyth", "S. White"], "venue": "SDM", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "Nearly-linear time algorithms for preconditioning and solving symmetric", "author": ["D. Spielman", "S.-H. Teng"], "venue": "diagonally dominant linear systems. ArxiV preprint, http://arxiv.org/pdf/cs/0607105v4.pdf", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Introduction to matrix computations", "author": ["G. Stewart", "G. Stewart"], "venue": "volume 441. Academic press New York", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1973}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and computing, 17(4):395\u2013416", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Approximate spectral clustering", "author": ["L. Wang", "C. Leckie", "K. Ramamohanarao", "J. Bezdek"], "venue": "Advances in Knowledge Discovery and Data Mining, pages 134\u2013146", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Segmentation using eigenvectors: a unifying view", "author": ["Y. Weiss"], "venue": "Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 975\u2013982. IEEE", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast approximate spectral clustering", "author": ["D. Yan", "L. Huang", "M. Jordan"], "venue": "KDD", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": "NIPS", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 31, "context": "In particular, in both cases, the two clusters have the same centers (centroids); hence, distance-based clustering methods such as k-means [32] will fail miserably.", "startOffset": 139, "endOffset": 143}, {"referenceID": 44, "context": "We refer the reader to Section 9 in [45] for a discussion on the history of spectral clustering.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "The first reports in the literature go back to [13, 16].", "startOffset": 47, "endOffset": 55}, {"referenceID": 15, "context": "The first reports in the literature go back to [13, 16].", "startOffset": 47, "endOffset": 55}, {"referenceID": 44, "context": "According to [45], \u201csince then spectral clustering has been discovered, re-discovered, and extended many times\u201d.", "startOffset": 13, "endOffset": 17}, {"referenceID": 40, "context": "Shi and Malik brought spectral clustering to the machine learning community in their seminal work on Normalized Cuts and Image Segmentation [41] (see Section 2).", "startOffset": 140, "endOffset": 144}, {"referenceID": 3, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 11, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 29, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 23, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 26, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 48, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 41, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 32, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 44, "context": "Since then, spectral clustering has been used extensively in applications in both data analysis and machine learning [4, 12, 30, 24, 27, 49, 42, 33, 45].", "startOffset": 117, "endOffset": 152}, {"referenceID": 42, "context": "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].", "startOffset": 139, "endOffset": 143}, {"referenceID": 47, "context": "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].", "startOffset": 181, "endOffset": 200}, {"referenceID": 16, "context": "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].", "startOffset": 181, "endOffset": 200}, {"referenceID": 34, "context": "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].", "startOffset": 181, "endOffset": 200}, {"referenceID": 4, "context": "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].", "startOffset": 181, "endOffset": 200}, {"referenceID": 45, "context": "Motivated by the need for faster algorithms, several algorithms have been proposed by researchers in both the theoretical computer science [43] and the machine learning communities [48, 17, 35, 5, 46].", "startOffset": 181, "endOffset": 200}, {"referenceID": 40, "context": "Definition 1 (The Spectral Clustering Problem [41]).", "startOffset": 46, "endOffset": 50}, {"referenceID": 40, "context": ") in a weighted undirected graph is an NP-Complete problem (see the appendix in [41] for the proof).", "startOffset": 80, "endOffset": 84}, {"referenceID": 40, "context": "Motivated by this hardness result, Shi and Malik [41] suggested a relaxation to the problem that is tractable in polynomial time through the SVD.", "startOffset": 49, "endOffset": 53}, {"referenceID": 40, "context": "First, [41] shows that for any G,A,B and partition vector y \u2208 R with +1 to the entries corresponding to A and \u22121 to the entries corresponding to B it is: 4 \u00b7 Ncut(A,B) = y (D \u2212W)y/(yDy).", "startOffset": 7, "endOffset": 11}, {"referenceID": 40, "context": "Definition 2 (The Real Relaxation for the Spectral Clustering Problem [41]).", "startOffset": 70, "endOffset": 74}, {"referenceID": 40, "context": "Motivated by the above observations, which are due to [41], Ng, Jordan, and Weis [30] (see also [47]) suggested the following algorithm for spectral clustering , which we consider as the ground-truth algorithm.", "startOffset": 54, "endOffset": 58}, {"referenceID": 29, "context": "Motivated by the above observations, which are due to [41], Ng, Jordan, and Weis [30] (see also [47]) suggested the following algorithm for spectral clustering , which we consider as the ground-truth algorithm.", "startOffset": 81, "endOffset": 85}, {"referenceID": 46, "context": "Motivated by the above observations, which are due to [41], Ng, Jordan, and Weis [30] (see also [47]) suggested the following algorithm for spectral clustering , which we consider as the ground-truth algorithm.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "4 in [18]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "The survey [20] gives an overview of these truncated processes in the context of low-rank matrix approximation.", "startOffset": 11, "endOffset": 15}, {"referenceID": 17, "context": "3 in [18]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "Motivated by the work in [22] we adapt the following approach: we assume that if, for two matrices U \u2208 Rn\u00d7k and V \u2208 Rn\u00d7k, both with orthonormal columns, \u2016U \u2212V\u20162 \u2264 \u03b5, for some arbitrarily small \u03b5 > 0, then clustering the rows of U and the rows of V with the same method should result to the same clustering.", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "Hence, a distance-based algorithm such as k-means [32] would lead to the same clustering as \u03b5\u2192 0 (this is equivalent to saying that k-means is robust to small perturbations to the input).", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "The Hunter and Strohmer result [22].", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "The result that is most relevant to ours appeared in [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Firstly, we follow the exact same framework for measuring the approximability of an algorithm for spectral clustering (see Theorem 5 and Corollary 6 in [22]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Specifically, they prove a weaker version of Theorem 7 of our work (see the proof of Theorem 5 in [22]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 36, "context": "The most relevant algorithmic result to ours is [37].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "Another clustering method similar to ours is in [26].", "startOffset": 48, "endOffset": 52}, {"referenceID": 47, "context": "Then, [48, 21, 44] show the following bound: \u03c1 \u2264 \u2016\u1e7d2 \u2212 v2\u20162 \u2264 (\u03bb2 \u2212 \u03bb3) \u2016E\u20162 +O(\u2016E\u20162).", "startOffset": 6, "endOffset": 18}, {"referenceID": 20, "context": "Then, [48, 21, 44] show the following bound: \u03c1 \u2264 \u2016\u1e7d2 \u2212 v2\u20162 \u2264 (\u03bb2 \u2212 \u03bb3) \u2016E\u20162 +O(\u2016E\u20162).", "startOffset": 6, "endOffset": 18}, {"referenceID": 43, "context": "Then, [48, 21, 44] show the following bound: \u03c1 \u2264 \u2016\u1e7d2 \u2212 v2\u20162 \u2264 (\u03bb2 \u2212 \u03bb3) \u2016E\u20162 +O(\u2016E\u20162).", "startOffset": 6, "endOffset": 18}, {"referenceID": 28, "context": "Motivated by a result of Mihail [29], Spielman and Teng [43] suggest the following definition for an approximate Fiedler vector: For a Laplacian matrix L and 0 < \u03b5 < 1, v \u2208 R is an \u03b5-approximate Fiedler vector if v is orthogonal to the all-ones vector and vLv vTv \u2264 (1 + \u03b5) fLf fTf = (1 + )\u03bbn\u22121(L).", "startOffset": 32, "endOffset": 36}, {"referenceID": 42, "context": "Motivated by a result of Mihail [29], Spielman and Teng [43] suggest the following definition for an approximate Fiedler vector: For a Laplacian matrix L and 0 < \u03b5 < 1, v \u2208 R is an \u03b5-approximate Fiedler vector if v is orthogonal to the all-ones vector and vLv vTv \u2264 (1 + \u03b5) fLf fTf = (1 + )\u03bbn\u22121(L).", "startOffset": 56, "endOffset": 60}, {"referenceID": 42, "context": "2 in [43] gives a randomized algorithm to compute such an \u03b5-approximate Fiedler vector w.", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "[48] proposed an algorithm where one first applies k-means to the data to find the so-called centroids and then applies spectral clustering on the centroids and uses this to cluster all the points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "with a nearest neighbor based approach) to all the points [35, 5, 46].", "startOffset": 58, "endOffset": 69}, {"referenceID": 4, "context": "with a nearest neighbor based approach) to all the points [35, 5, 46].", "startOffset": 58, "endOffset": 69}, {"referenceID": 45, "context": "with a nearest neighbor based approach) to all the points [35, 5, 46].", "startOffset": 58, "endOffset": 69}, {"referenceID": 30, "context": "This is known as the Nystr\u00f6m method [31, 3, 17].", "startOffset": 36, "endOffset": 47}, {"referenceID": 2, "context": "This is known as the Nystr\u00f6m method [31, 3, 17].", "startOffset": 36, "endOffset": 47}, {"referenceID": 16, "context": "This is known as the Nystr\u00f6m method [31, 3, 17].", "startOffset": 36, "endOffset": 47}, {"referenceID": 39, "context": "This is the approach of the spectral clustering on a budget method [40].", "startOffset": 67, "endOffset": 71}, {"referenceID": 35, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 283, "endOffset": 298}, {"referenceID": 5, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 283, "endOffset": 298}, {"referenceID": 14, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 283, "endOffset": 298}, {"referenceID": 9, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 283, "endOffset": 298}, {"referenceID": 24, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 366, "endOffset": 370}, {"referenceID": 27, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 412, "endOffset": 427}, {"referenceID": 18, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 412, "endOffset": 427}, {"referenceID": 6, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 412, "endOffset": 427}, {"referenceID": 9, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 412, "endOffset": 427}, {"referenceID": 13, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 455, "endOffset": 463}, {"referenceID": 38, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 455, "endOffset": 463}, {"referenceID": 7, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 489, "endOffset": 492}, {"referenceID": 33, "context": "The proposed approximation algorithm for spectral clustering builds on a family of randomized, sketching-based algorithms which, in recent years, led to similar running time improvements to other classical machine learning and linear algebraic problems: (i) Least-squares regression [36, 6, 15, 10]; (ii) Linear equations with Symmetric Diagonally Dominant matrices [25] (iii) Low-rank approximation of matrices [28, 19, 7, 10]; (v) matrix multiplication [14, 39]; (vi) K-means clustering [8]; (vii) support vector machines [34].", "startOffset": 524, "endOffset": 528}, {"referenceID": 0, "context": "(viii) Canonical Correlation Analysis (CCA) [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "Specifically, [20] argues that \u2016W\u0303 \u2212P\u1ef8W\u0303\u20162 is small.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "4 in [18].", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "2 in [18] shows a bound similar to our bound in Theorem 10.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "Lemma 5 (The norm of a random Gaussian Matrix [11]).", "startOffset": 46, "endOffset": 50}, {"referenceID": 37, "context": "Lemma 6 (Invertibility of a random Gaussian Matrix [38]).", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Table 1: Datasets from the libSVM multi-class classification page [9] that were used in our experiments.", "startOffset": 66, "endOffset": 69}, {"referenceID": 48, "context": "To compute W, we use the heat kernel: ln(Wij) = ||xi\u2212xj ||22 \u03c3ij , where xi \u2208 R and xj \u2208 R are the data points and \u03c3ij is a tuning parameter; \u03c3ij is determined using the self-tuning method described in [49].", "startOffset": 202, "endOffset": 206}, {"referenceID": 1, "context": "For the partial singular value decomposition of W\u0303, we used IRLBA [2] due to it\u2019s excellent support for sparse decompositions.", "startOffset": 66, "endOffset": 69}, {"referenceID": 22, "context": "To avoid comprehensively testing all k permutations of the output labels for optimality with the true labels, we use the Hungarian algorithm [23].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "In each iteration, we varied p from [0, 10].", "startOffset": 36, "endOffset": 43}], "year": 2017, "abstractText": "Spectral clustering is arguably one of the most important algorithms in data mining and machine intelligence; however, its computational complexity makes it a challenge to use it for large scale data analysis. Recently, several approximation algorithms for spectral clustering have been developed in order to alleviate the relevant costs, but theoretical results are lacking. In this paper, we present a novel approximation algorithm for spectral clustering with strong theoretical evidence of its performance. Our algorithm is based on approximating the eigenvectors of the Laplacian matrix using random projections, a.k.a randomized sketching. Our experimental results demonstrate that the proposed approximation algorithm compares remarkably well to the exact algorithm.", "creator": "TeX"}}}