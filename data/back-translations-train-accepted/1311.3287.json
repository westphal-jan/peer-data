{"id": "1311.3287", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2013", "title": "Nonparametric Estimation of Multi-View Latent Variable Models", "abstract": "Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric. The key idea of the method is to embed the joint distribution of a multi-view latent variable into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our non-parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. Moreover, the non-parametric tensor power method compares favorably to EM algorithm and other existing spectral algorithms in our experiments.", "histories": [["v1", "Wed, 13 Nov 2013 20:42:21 GMT  (101kb,D)", "https://arxiv.org/abs/1311.3287v1", null], ["v2", "Sun, 8 Dec 2013 01:58:58 GMT  (99kb,D)", "http://arxiv.org/abs/1311.3287v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["le song", "animashree anandkumar", "bo dai", "bo xie 0002"], "accepted": true, "id": "1311.3287"}, "pdf": {"name": "1311.3287.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Estimation of Multi-View Latent Variable Models", "authors": ["Le Song", "Animashree Anandkumar", "Bo Dai", "Bo Xie"], "emails": ["lsong@cc.gatech.edu", "a.anandkumar@uci.edu", "bodai@gatech.edu", "zixu1986@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they see themselves able to survive on their own."}, {"heading": "2 Notation", "text": "We use X to denote a random variable with domain X and refer to the instances of X with the lowercase letter X1, X2,.., X 'with a common distribution P (X1, X2,.., X'). For convenience, we also assume that the domains of all Xt, t ['] are the same, but the methodology applies to cases where they have different domains. In addition, we use H to denote a hidden variable with domain H and distribution P (H). A reproducing Hilbert space (RKHS) F on X with a kernel k (x, x \u2032) is a Hilbert space with functions f (\u00b7)."}, {"heading": "3 Kernel Embedding of Distributions", "text": "We begin with an overview of kernel embeddings of distributions that represent implicit representations of distributions in potentially infinite dimensional RKHS. The kernel embedding approach represents a distribution by an element in the RKHS associated with a kernel function. Smola et al. (2007); Sriperumbudur et al. (2008), \u00b5X: = EX [\u03c6 (X) = X (x) P (dx), (1), where the distribution is mapped to its expected characteristic card, i.e. to a point in a potentially infinite and implicit feature space. The kernel embedding of \u00b5X has the property that the expectation of any RKHS function f as an internal product in F, EX [f (X)] = < \u00b5X, f > F, f > F, f > F, f > F, X.Kernel embeddings can easily become common to two or more."}, {"heading": "3.1 Kernel Embedding as Multi-Linear Operator", "text": "The common embedding can also be considered as an unfocused covariance operator CX1X2: F 7 \u00b7 F by the standard equivalence between a tensor product feature and a linear map, i.e. two functions f1, f2, F, their covariance of EX1X2 [f1 (X1) f2 (X2)] = < f1, CX1X2f2 > F, CX1X2 > F \u00b7 F, with the former being considered CXY as an operator, while the latter is considered as an element in the tensor product feature space. By analogy, CX1X2X3 can be used as a multilinear operator from F \u00b7 F \u00b7 F \u00b7 F \u00b7 F \u00b7 F \u00b7 F to R. From the context, it will be clear whether we consider CXY as an operator between two spaces or as an element from a tensor product feature room."}, {"heading": "3.2 Finite Sample Estimate", "text": "While we rarely have access to the true underlying distribution, P (X), we can easily estimate its embedding using a finite sample average. Given a sample DX = {x1,.., xm} of size m drawn by P (X), the empirical kernel can estimate the embedding of is\u00b5 (m \u2212 12) Smola et al. (2007). The covariance operator can be similarly estimated using a finite sample average. Given m pairs of traininininingexamples DXY = (xi1, x i)} i (m), the embedding of Op (m \u2212 12) Smola et al. (2007). The covariance operator can be similarly estimated using a finite sample average."}, {"heading": "4 Multi-View Latent Variable Models", "text": "Multi-view latent variable models examined in this paper are a special class of Bayesian networks in which \u2022 observed variables X1, X2,..., X 'are conditionally independent, since a discrete latent variable H exists, and \u2022 the conditional distributions P (Xt | H) of the hidden variable H. The conditional independent structure of a multi-view latent variable model is illustrated in Figure 1 (a), and many complicated graphical models, such as the hidden Markov model in Figure 1 (b), can be reduced to a multi-view latent variable model. To simplify the exposure, we explain our method using the symmetrical view model."}, {"heading": "4.1 Conditional Embedding Operator", "text": "For the sake of simplicity, let's focus on a simple model with three observed variables, i.e., \"= 3. Let's assume H-value [k], then we can embed any conditional distribution P (X | h) corresponding to a certain value of H = h in the RKHS as\u00b5X | h = \u0445 X \u03c6 (x) P (dx | h). (6) If we vary the value of H, we get the core embedding for different P (X | h), (7) which is called a conditional embedding operator. If we use the default base eh in Rk to represent any value of h, we can embed any \u00b5X | h = 1, \u00b5X | h = 2,."}, {"heading": "4.2 Factorized Kernel Embedding", "text": "Then the distributions P (X1, X2) and P (X1, X2, X3) are factorizable or P (dx1, dx2) = H P (dx1 | h) P (dx2 | h) P (dx2 | h) P (dh).Assuming that the hidden variable H (k) is discrete, we allow it to be: H (dx1, dx3) P (dx1 | h) P (dx2 | h) P (dx3) P (h, h) P (h).Since we accept the hidden variable H (k) [k] discreetly, we allow it to be: H (. 0.... X. X."}, {"heading": "4.3 Identifiability of Parameters", "text": "We note that CX | H = (\u00b5X | h = 1, \u00b5X | h = 2,.., \u00b5X | h = k) and the kernel embeddings for CX1X2 and CX1X2X3 can therefore be written alternatively as CX1X2 = [k] \u03c0h \u00b7 \u00b5X | h, (11) CX1X2X3 = [k], [k], [k], [k], [k], [k], [k], [k), [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [k], [K], [K], [K], [K]], [K], [K], [K], [K], [K], [K], [K] K], [K], [K], [K], [K], [K], [K] K], [K], [K] K], [K], [K] K], [K], [K], [K], [K], [K], [K] K], [K], [K], [K] K], [K], [K] K], [K], [K]"}, {"heading": "5 Kernel Algorithm", "text": "We first design a kernel algorithm to restore the parameters {\u03c0h, \u00b5X | h} h [k] of the multi-view latent variable model based on CX1X2 and CX1X2X3. This can easily be extended to the sample versions and is discussed in Section 5.2. To illustrate the presentation, we first present the case of the symmetric view and then expand it to a more general version."}, {"heading": "5.1 Population Case", "text": "First, we derive the algorithm for the population case as if we could access the true operator CX1X2 and CX1X2X3. Its finite counterpart is presented in the next paragraph. The algorithm can be considered as a core generalization of the algorithm in Anandkumar et al. (2013a) using embedding representations. Step 1. We perform the self-decomposition of CX1X2, CX1X2 = \u221e i = 1 \u03c3i \u00b7 ui uiwhere the Eigen values are not ordered decreasingly. Step 2. Let us leave the leading eigenvectors according to the largest k-own value Uk: = (u1, u2,.., uk) and the own-value matrix Sk: = 0.2 \u00b5.According to Xess.X value Uk: (u1, u2,., uk)."}, {"heading": "5.2 Finite Sample Case", "text": "Since m observation DX1X2X3 = (xi1, xi2, xi3) i (m) is usually drawn (from a multiview latently variable model P (X1, X2, X3), we now design a kernel algorithm to estimate the latent parameters from the data. Although the empirical kernel embeddings can be infinitely dimensional, we can only perform the decomposition using the kernel matrices. We call the implicit function matrix Byrix: = (x11),.,.,????? (x m 1),.?????? (x m 2),.??????? (x12),.????????? (x) (x),?? (x) m."}, {"heading": "5.3 Symmetrization", "text": "In this section, we will extend the algorithm to the general case where the conditional distributions are different for each point of view. (Without loss of generality, we will make the case based on a method by Anandkumar and al. (2012b). \u2212 k The same strategy applies to other views. \u2212 k = > The idea is to reduce the case with multiple views based on a method by Anandkumar and al. (2012b). \u2212 k The observations DX1X2X3 = {1, xi2, xi2, xi3)."}, {"heading": "6 Sample Complexity", "text": "Suppose that the number of samples m satisfiesm > \u03b8\u03c12 log \u03b42 \u03c32k (CX1, X2), as well as the number of iterations N and the number of random initialization vectors L (pulled uniform on the sphere Sk \u2212 1, X2), as well as the random trigger vectors T (pulled uniform on the sphere Sk \u2212 1), SatifyN \u00b2 (log (k) + log \u00b2 of the perspectivity, Xp \u2212 j \u00b2 of the constants C3, C4 > 0, and the number of iterations N and the number of random initialization vectors L (pulled uniform on the sphere Sk \u2212 1)."}, {"heading": "7 Experiments", "text": "Methods: We compared our non-parametric kernel algorithm with three alternatives: 1. The EM algorithm for mixing Gaussians; the EM algorithm does not guarantee that it will find the global solution in every study; therefore, we randomly initialize it ten times; 2. The spectral algorithm for mixing Gaussian spheres (Hsu & Kakade, 2013); the assumption in Hsu & Kakade (2013) is very restrictive: The collection of spherical Gaussian centers must include a k-dimensional substance; 3. A discretization-based spectral algorithm (Kasahara & Shimotsu, 2010); this algorithm approaches the common distribution of observed variables with the histogram and then applies the spectral algorithm to restore the discretized conditional density; it is known that density estimation by Kasahara & Shimotsu, 2010, has poor performance even for three-dimensional data."}, {"heading": "7.1 Synthetic Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1.1 General Case: Different Conditional Distributions", "text": "The variables corresponding to the dimensions are independent of the indicator of the latent component. Specifically, we researched two-dimensional settings: 1. Gaussian conditioned densities with different variances; 2. Mixing of Gaussian and shifted gamma conditioned densities. The shifted gamma distribution has densitype (x \u2212 \u00b5) = (x \u2212 \u00b5) (d \u2212 1) e \u2212 x / phenomenon dump (d), x-where we selected the shape parameter d \u2264 1 in such a way that the density is very skewed. In addition, we chose the mean and the variance parameters of the Gaussian / gamma density so that the components overlap in pairs according to the Fisher ratio (\u00b51 \u2212 \u00b52) 2\u044521 + \u03c3 2 2.We also varied the number of samples from 50 to 10 000, and experimented with components with 2, 3 or 8, the mixing component is relatively small."}, {"heading": "7.1.2 Symmetric Case: Same Conditional Distribution", "text": "We generated the data from two settings: 1. Mixing of Gaussian conditional density; 2. Mixing of Gaussian and shifted gamma conditional density. The mixing ratio and other experimental settings are exactly the same as the experiment in the main text. The only difference is that the conditional densities are identical for each view here. We use the same measure to evaluate performance. The empirical results are in Figure 4. As we expected, the behavior of the proposed method is similar to the results in different conditional densities. In the Gaussian mixture, our algorithm converges with the EM-GMM results. And in the mixture of Gaussian and different gamma, our algorithm is consistently better than other alternatives."}, {"heading": "7.2 Flow Cytometry Data", "text": "Flow cytometry (FCM) data are multivariate measurements of flow cytometers that record light scattering and fluorescence emission properties of hundreds of thousands of individual cells, and are important for studying the cell structures of normal and abnormal cells and diagnosing human diseases. Aghaeepour et al. (2013) introduced the FlowCAP challenge, whose main task is to automatically group flow cytometry data. Clustering on the FCM data is a difficult task because the distribution of the data is non-Gaussian and heavily slimmed down. We used the DLBCL lymphoma data collection by Aghaeepour et al. (2013) to compare our kernel algorithm with a mixture of multiple views of the Gaussian model. This collection contains 30 datasets, and each dataset consists of tens of thousands of cell measurements in 5 dimensions. Each dataset is a separate cluster model, and we have a multiple view to GMT."}, {"heading": "Acknowledgements", "text": "L. Song is partially supported by the NSF Award IIS-1218749 and NIH 1RO1GM108341-01. A. Anandkumar is partially supported by the Microsoft Faculty Fellowship, the NSF Career Award CCF-1254106, the NSF Award CCF1219234 and the ARO YIP Award W911NF-13-1-0084."}, {"heading": "8 Robust Tensor Power Method", "text": "We repeat the robust tensor-power method for determining the tensor-property pairs in algorithm 2, which is analyzed in detail in Anandkumar et al. (2013a) and Anandkumar et al. (2012a). The method calculates the eigenvectors of a tensor by deflation, using random initialization vectors, which can be replaced in certain settings by better initialization vectors, e.g. in the community model, the neighborhood vectors offer better initialization and lead to stronger guarantees for Anandkumar et al. (2013a). In view of the initialization vector, the method then performs a tensor-power update and runs for N iterations to obtain an eigenvector. Successive property vectors are executed via deflation. Algorithm 2 {II} TensorEigen (T, {vi} i-iteration, N) to obtain an eigenvector."}, {"heading": "9 Proof of Theorem 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Recap of Perturbation Bounds for the Tensor Power Method", "text": "s leave T = \u2211 i [k] \u03bbivi, where vi are orthonormal vectors and \u03bb1 \u2265 \u03bb2.. Let T = T + E be the perturbed tensor, if there is vi, that < u, vi > R0 and | < u, vi > | maxj < i | < u, vj > | < u, vi > |. Choose vi = 1 / 100.Theorem 3 There are universal constants C1, C2 > 0 that the following method applies."}, {"heading": "9.2 Concentration Bounds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.2.1 Analysis of Whitening", "text": "Remember that we use the covariance operator CX1X2 to lighten the 3rd order = > Procedure for embedding CX1, X2, X3. First, we analyze the error in lightening when sample estimates are made. \u2212 Let us know the sample covariance pairs between variables X1 and X2, and let us restrict it to the top k pairs, and let Bk: = U-X1X2 + C-X1X2. \u2212 Let us know that the lightening matrix is given by W-X2. \u2212 Let U-k and S-k limit it to the top k pairs, and let Bk-X2. \u2212 Let us know the lightening matrix by W-X1 / 2k."}, {"heading": "9.2.2 Tensor Concentration Bounds", "text": "Remember that the white tensor from the samples of T: = C: X1X2X3 \u00b7 1 (W: >) \u00b7 2 (W: >) \u00b7 3 (W: >) \u00b7 3 (W: \u2212 3 (W: \u2212 3). We want its interference from the white tensor using accurate statistics T: = CX1X2X3 \u00b7 1 (W: >) \u00b7 2 (W: \u2212 3 (W: \u2212 3) \u2212 3 (W: \u2212 3 (W: \u2212 3) \u2212 3 (W: \u2212 3) \u2212 3 (W: \u2212 3). We want its interference from the white tensor using exact statistics T: = 3 (h: \u2212 3)."}, {"heading": "9.2.3 Concentration bounds for Empirical Operators", "text": "The concentration results for the singular value decomposition of empirical operators. (Lemma 6 (Concentration limits for pairs) Let us: = Supx values for pairs (x, x) and for the Hilbert-Schmidt norm. (23) Proof We will use arguments similar to those used in Rosasco et al. (2010), which deal with symmetrical operators. (22) Let us define the arguments as Supx values. (i) Let us define the arguments as Supx values. (i) Let us define the arguments as Supx values (x 1): Supx values (x2) - CX1, X2, X2, X2. (24) It is easy to see that E [2] = 0."}], "references": [{"title": "Critical assessment of automated flow cytometry data analysis techniques", "author": ["Aghaeepour", "Nima", "Finak", "Greg", "Consortium", "The FlowCAP", "The DREAM", "Hoos", "Holger", "Mosmann", "Tim R", "Brinkman", "Ryan", "Gottardo", "Raphael", "Scheuermann", "Richard H"], "venue": "Nature Methods,", "citeRegEx": "Aghaeepour et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Aghaeepour et al\\.", "year": 2013}, {"title": "Identifiability of parameters in latent structure models with many observed variables", "author": ["Allman", "Elizabeth", "Matias", "Catherine", "Rhodes", "John"], "venue": "The Annals of Statistics,", "citeRegEx": "Allman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Allman et al\\.", "year": 2009}, {"title": "Tensor Methods for Learning Latent Variable Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Available at arXiv:1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A Tensor Spectral Approach to Learning Mixed Membership Community Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade"], "venue": "ArXiv 1302.2684,", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity", "author": ["A. Anandkumar", "D. Hsu", "M. Janzamin", "S.M. Kakade"], "venue": "ArXiv 1308.2853,", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Anandkumar", "Animashree", "Foster", "Dean P", "Hsu", "Daniel", "Kakade", "Sham M", "Liu", "Yi-Kai"], "venue": "Available at arXiv:1204.6703,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Inference of haplotypes from PCR-amplified samples of diploid populations", "author": ["A. Clark"], "venue": "Molecular Biology and Evolution,", "citeRegEx": "Clark,? \\Q1990\\E", "shortCiteRegEx": "Clark", "year": 1990}, {"title": "Fourth-order cumulant-based blind identification of underdetermined mixtures", "author": ["L. De Lathauwer", "J. Castaing", "Cardoso", "J.-F"], "venue": "IEEE Tran. on Signal Processing,", "citeRegEx": "Lathauwer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fine and Scheinberg,? \\Q2001\\E", "shortCiteRegEx": "Fine and Scheinberg", "year": 2001}, {"title": "Spectral dimensionality reduction for hmms", "author": ["D.P. Foster", "J. Rodu", "L.H. Ungar"], "venue": "Arxiv preprint arXiv:1203.6130,", "citeRegEx": "Foster et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2012}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "Teo", "C.-H", "L. Song", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "Latent space approaches to social network analysis", "author": ["Hoff", "Peter D", "Raftery", "Adrian E", "Handcock", "Mark S"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2002}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In Proc. Annual Conf. Computational Learning Theory,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Hsu", "Daniel", "Kakade", "Sham M"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2013}, {"title": "Nonparametric identification of multivariate mixtures", "author": ["Kasahara", "Hiroyuki", "Shimotsu", "Katsumi"], "venue": "Journal of the Royal Statistical Society - Series B,", "citeRegEx": "Kasahara et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kasahara et al\\.", "year": 2010}, {"title": "Efficient orthogonal tensor decomposition, with an application to latent variable model learning", "author": ["Kir\u00e1ly", "Franz"], "venue": "Available at arXiv:1309.3233,", "citeRegEx": "Kir\u00e1ly and Franz.,? \\Q2013\\E", "shortCiteRegEx": "Kir\u00e1ly and Franz.", "year": 2013}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara. G", "Bader", "Brett W"], "venue": "SIAM Review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics", "author": ["J.B. Kruskal"], "venue": "Linear algebra and its applications,", "citeRegEx": "Kruskal,? \\Q1977\\E", "shortCiteRegEx": "Kruskal", "year": 1977}, {"title": "A spectral algorithm for latent tree graphical models", "author": ["A. Parikh", "L. Song", "E.P. Xing"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "An introduction to hidden Markov models", "author": ["L.R. Rabiner", "B.H. Juang"], "venue": "IEEE ASSP Magazine,", "citeRegEx": "Rabiner and Juang,? \\Q1986\\E", "shortCiteRegEx": "Rabiner and Juang", "year": 1986}, {"title": "On learning with integral operators", "author": ["L. Rosasco", "M. Belkin", "E.D. Vito"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rosasco et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rosasco et al\\.", "year": 2010}, {"title": "Kernel Methods in Computational Biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "Vert", "J.-P"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2004}, {"title": "Identifying finite mixtures of nonparametric product distributions and causal inference of confounders", "author": ["Sgouritsa", "Eleni", "Janzing", "Dominik", "Peters", "Jonas", "Sch\u00f6lkopf", "Bernhard"], "venue": "In Conference on Uncertainty on Artificial Intelligence (UAI),", "citeRegEx": "Sgouritsa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sgouritsa et al\\.", "year": 2013}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the International Conference on Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Robust low rank kernel embedding of multivariate distributions", "author": ["L. Song", "B. Dai"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Song and Dai,? \\Q2013\\E", "shortCiteRegEx": "Song and Dai", "year": 2013}, {"title": "Kernel embeddings of latent tree graphical models", "author": ["L. Song", "A. Parikh", "E.P. Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "Latent variable models have been used to address various machine learning problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis (Rabiner & Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003).", "startOffset": 182, "endOffset": 256}, {"referenceID": 13, "context": "Latent variable models have been used to address various machine learning problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis (Rabiner & Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003).", "startOffset": 182, "endOffset": 256}, {"referenceID": 6, "context": "Latent variable models have been used to address various machine learning problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis (Rabiner & Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003).", "startOffset": 182, "endOffset": 256}, {"referenceID": 14, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 20, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 27, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 11, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 9, "context": "Compared to the Expectation-Maximization (EM) algorithm (Dempster et al., 1977) traditionally used for this task, spectral algorithms are better in terms of their computational efficiency and provable guarantees.", "startOffset": 56, "endOffset": 79}, {"referenceID": 2, "context": "When we use delta kernel, our algorithm reduces to the spectral algorithm for discrete mixture components analyzed in (Anandkumar et al., 2012a). When we use universal kernels, such as Gaussian RBF kernel, our algorithm can recover Gaussian mixture components as well as mixture components with other distributions. In this sense, our work also provides a unifying framework for previous spectral algorithms. We prove sample complexity bounds for the nonparametric tensor power method and establish that the sample complexity is quadratic in the number of latent components, and is a low order polynomial in the other relevant parameters such as the lower bound on mixing weights. Thus, we propose a computational and sample efficient nonparametric approach to learning latent variable models. Kernel methods have been previously applied to learning latent variable models. However, none of the previous works explicitly recovers the actual parameters of the models Song et al. (2011); Song & Dai (2013); Sgouritsa et al.", "startOffset": 119, "endOffset": 985}, {"referenceID": 2, "context": "When we use delta kernel, our algorithm reduces to the spectral algorithm for discrete mixture components analyzed in (Anandkumar et al., 2012a). When we use universal kernels, such as Gaussian RBF kernel, our algorithm can recover Gaussian mixture components as well as mixture components with other distributions. In this sense, our work also provides a unifying framework for previous spectral algorithms. We prove sample complexity bounds for the nonparametric tensor power method and establish that the sample complexity is quadratic in the number of latent components, and is a low order polynomial in the other relevant parameters such as the lower bound on mixing weights. Thus, we propose a computational and sample efficient nonparametric approach to learning latent variable models. Kernel methods have been previously applied to learning latent variable models. However, none of the previous works explicitly recovers the actual parameters of the models Song et al. (2011); Song & Dai (2013); Sgouritsa et al.", "startOffset": 119, "endOffset": 1004}, {"referenceID": 2, "context": "When we use delta kernel, our algorithm reduces to the spectral algorithm for discrete mixture components analyzed in (Anandkumar et al., 2012a). When we use universal kernels, such as Gaussian RBF kernel, our algorithm can recover Gaussian mixture components as well as mixture components with other distributions. In this sense, our work also provides a unifying framework for previous spectral algorithms. We prove sample complexity bounds for the nonparametric tensor power method and establish that the sample complexity is quadratic in the number of latent components, and is a low order polynomial in the other relevant parameters such as the lower bound on mixing weights. Thus, we propose a computational and sample efficient nonparametric approach to learning latent variable models. Kernel methods have been previously applied to learning latent variable models. However, none of the previous works explicitly recovers the actual parameters of the models Song et al. (2011); Song & Dai (2013); Sgouritsa et al. (2013). Most of them estimate an (unknown) invertible transformation of the latent parameters, and it is not clear how one can recover the actual parameters based on these estimates.", "startOffset": 119, "endOffset": 1029}, {"referenceID": 23, "context": "Kernel functions have also been defined on graphs, time series, dynamical systems, images and other structured objects Sch\u00f6lkopf et al. (2004). Thus the methodology presented below can be readily generalized to a diverse range of data types as long as kernel functions are defined.", "startOffset": 119, "endOffset": 143}, {"referenceID": 25, "context": "The kernel embedding approach represents a distribution by an element in the RKHS associated with a kernel function Smola et al. (2007); Sriperumbudur et al.", "startOffset": 116, "endOffset": 136}, {"referenceID": 25, "context": "The kernel embedding approach represents a distribution by an element in the RKHS associated with a kernel function Smola et al. (2007); Sriperumbudur et al. (2008),", "startOffset": 116, "endOffset": 165}, {"referenceID": 12, "context": "This injective property of kernel embeddings has been exploited to design state-of-the-art two-sample tests Gretton et al. (2012) and independence tests Gretton et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 12, "context": "This injective property of kernel embeddings has been exploited to design state-of-the-art two-sample tests Gretton et al. (2012) and independence tests Gretton et al. (2008).", "startOffset": 108, "endOffset": 175}, {"referenceID": 25, "context": "(4) This empirical estimate converges to its population counterpart in RKHS norm, \u2016\u03bc\u0302X \u2212 \u03bcX\u2016F , with a rate of Op(m \u2212 12 ) Smola et al. (2007). The covariance operator can be estimated similarly using finite sample average.", "startOffset": 123, "endOffset": 143}, {"referenceID": 14, "context": "Kruskal (1977); De Lathauwer et al.", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "Kruskal (1977); De Lathauwer et al. (2007); Anandkumar et al.", "startOffset": 19, "endOffset": 43}, {"referenceID": 2, "context": "(2007); Anandkumar et al. (2013b). However, in general, it is not tractable to learn such models and we do not consider them here.", "startOffset": 8, "endOffset": 34}, {"referenceID": 2, "context": "of as a kernel generalization of the algorithm in Anandkumar et al. (2013a) using embedding representations.", "startOffset": 50, "endOffset": 76}, {"referenceID": 2, "context": "We use tensor power method to find eigenvectors M for T Anandkumar et al. (2013a). We provide the method in the Appendix in Algorithm 2 for completeness.", "startOffset": 56, "endOffset": 82}, {"referenceID": 2, "context": "We run tensor power method Anandkumar et al. (2013a) on the finite dimension tensor T\u0302 to obtain its leading k eigenvectors M\u0302 := (v\u03021, .", "startOffset": 27, "endOffset": 53}, {"referenceID": 2, "context": "The idea is to reduce the multi-view case to the identical-view case based on a method by Anandkumar et al. (2012b). Given the observations DX1X2X3 = {(x1, x2, x3)}i\u2208[m] drawn i.", "startOffset": 90, "endOffset": 116}, {"referenceID": 2, "context": "for constant C2 > 0 and L = poly(k) log(1/\u03b4), the robust power method in Anandkumar et al. (2013a) yields eigen-pairs (\u03bb\u0302i, \u03c6\u0302i) such that there exists a permutation \u03b7, with probability 1\u2212 4\u03b4, we have \u2016\u03c0 j \u03bcX|h=j \u2212 \u03c6\u0302\u03b7(j)\u2016 \u2264 8 T \u00b7 \u03c0 \u22121/2 j , |\u03c0 j \u2212 \u03bb\u0302\u03b7(j)| \u2264 5 T , \u2200j \u2208 [k], and \u2225\u2225\u2225\u2225T \u2212 k \u2211", "startOffset": 73, "endOffset": 99}], "year": 2013, "abstractText": "Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric. The key idea of the method is to embed the joint distribution of a multi-view latent variable into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our non-parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. Moreover, the non-parametric tensor power method compares favorably to EM algorithm and other existing spectral algorithms in our experiments.", "creator": "LaTeX with hyperref package"}}}