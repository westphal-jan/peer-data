{"id": "1311.4296", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2013", "title": "Reflection methods for user-friendly submodular optimization", "abstract": "Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efficient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reflections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the benefits of our method on two image segmentation tasks.", "histories": [["v1", "Mon, 18 Nov 2013 08:48:13 GMT  (1583kb)", "http://arxiv.org/abs/1311.4296v1", "Neural Information Processing Systems (NIPS), \\'Etats-Unis (2013)"]], "COMMENTS": "Neural Information Processing Systems (NIPS), \\'Etats-Unis (2013)", "reviews": [], "SUBJECTS": "cs.LG cs.NA cs.RO math.OC", "authors": ["stefanie jegelka", "francis r bach", "suvrit sra"], "accepted": true, "id": "1311.4296"}, "pdf": {"name": "1311.4296.pdf", "metadata": {"source": "CRF", "title": "Reflection methods for user-friendly submodular optimization", "authors": ["Stefanie Jegelka"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 131 1.42 96v1 [cs.LG] 1 8N ov"}, {"heading": "1 Introduction", "text": "\"Submodularity..\".. \"..\" \"Submodularity..\".. \"\" Submodularity.. \"..\" \"Submodularity..\" \"..\" \"Submodularity..\" \"..\" \"..\" \"Submodularity..\" \"..\" \"Submodularity..\" \"..\" \"\" Submodularity.. \"\" \"..\" \"..\" \"\" Submodularity.. \"\" \"..\" \"\".. \"\" \"Submodularity..\" \"\" \"..\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"\" \"..\" \"\" \"\" \"..\" \"\" \"\" \"\""}, {"heading": "2 Review of relevant results from submodular analysis", "text": "The relevant concepts that we come back to here are the Lova-sz extension, the base polytope of submodular functions and relationships between proximal and discrete problems. For more details, see [1, 21]. Lova-sz extension and convexity. The power 2V can also, of course, be identified with the depressions of the hypercube, i.e. it can be calculated in closed form once the components of x are sorted: if the extension f of any set function (s) is defined by linear interpolation, so that for each S-V, F (S) = f (1S). It can be calculated in closed form as soon as the components of x are sorted: if the extension of x (1), then f (x)."}, {"heading": "3 Decomposition of submodular functions", "text": "This year it is more than ever before."}, {"heading": "3.1 Dual decomposition of the nonsmooth problem", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "3.2 Dual decomposition methods for proximal problems", "text": "We can also look at Equation (3) and first derive a dual problem using the same technique as in Section 3.1. Lemma 2 (proved in Appendix A) formally presents our dual formulation as the best approximation problem. The primary variable can be written as x = \u2212 \u2211 j yj. Lemma 2. However, the dual of Equation (3) can be written as the best approximation problem. (7) We can actually eliminate the two simpler problems and get the simpler looking dual problem \u2212 1 \u00b2 rj = 1 yj \u00b2 22 s.t. yj \u00b2 B (Fj), j \u00b2 rj = 1 B (Fj). (7) It is possible to find the double problem \u2212 1 \u00b2 rj \u2212 2 in Section 5. \u2212 In Section 5 we will see the effect of a specific problem."}, {"heading": "4 Algorithms", "text": "We describe some competing methods for solving our smooth dual formulations. We describe the details for the special 2-block case (9); the same arguments apply to the block dual from 2."}, {"heading": "4.1 Block coordinate descent or proximal-Dykstra", "text": "Perhaps the simplest approach to solving the (9) problem (considered a minimization problem) is the application of a block coordinate descend method (BCD), which in this case performs the alternate projections: yk + 11 \u2190 argminy1-1-1-2-22; yk + 12 \u2190 argminy2-B (F2) -2-2-11-2-2. (10) The iterations for the solution (8) are analogous. This BCD method (applied to (9)) is equivalent to the application of the so-called proximal Dykstra method [14] to the original problem, as can be seen from the comparison of iterations. Note that the BCD iteration (10) is nothing other than the alternating projection on the convex polyedra B (F1) and B (F2)."}, {"heading": "4.2 Douglas-Rachford splitting", "text": "The Douglas-Rachford (DR) splitting method [16] includes algorithms such as ADMM as a special case [14]. It avoids the slowdowns mentioned above by replacing alternating projections with alternating \"reflections.\" Formally, DR applies to convex problems of form [4, 14] minx \u03c61 (x), (11) subject to qualification ri (dom\u03c61). To solve (11), DR starts with some z0, and performs the three-step iteration (for k): 1. xk = prox\u03c62 (zsp); 2. vk = prox\u03c62). \u2212 zk \u2212 zk) 3. zk + 1 = zk + zk (vk \u2212 zk)."}, {"heading": "5 Experiments", "text": "In fact, it is the case that the two are two different types, which differ in the most different areas: in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "6 Conclusion", "text": "We have presented a novel approach to minimizing submodular functions, based on equivalence with a best approximation problem; the use of reflection methods avoids hyperparameters and significantly reduces the number of iterations, indicating the suitability of reflection methods for combinatorial problems; given the natural parallelization capabilities of our approach, it would be interesting to make detailed empirical comparisons with existing parallel implementations of graph sections (e.g. [42]); and a generalization of submodular functions of the relationships between combinatorial optimization problems and convex problems would allow us to apply our framework to other common situations such as multiple labels (see e.g. [31]), which was partially funded by the Office of Naval Research under contract / funding number N00014-11-1-0688."}, {"heading": "A Derivations of Dual Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Lemma 1", "text": "s duality: min x [0,1] n f (x) = min x [0,1] n \u2211 rj = 1 fj (x) = minx1,..., xr [0,1] n = 1 fj (xj) so that x1 = \u00b7 \u00b7 = xr = min x (x) Rn, x1,..., xr [0,1] n max (xj) [1 fj (xj) + x [1] rj = 1 \u03bb (x \u2212 xj) = max [1] r j = 1 \u03bbj = 0 \u0445j (xj) [0,1] n {fj (xj) \u2212 yj xj} = max."}, {"heading": "A.2 Proof of Lemma 2", "text": "The proof follows a similar saddle point approximation: min x Rn f (x) + 12 x 22 = minx Rn r j = 1 fj (x) + 1 2 x 22 = min x1,..., xr Rn Rn max j rj = 1 {fj (xj) + 12r xj 22} so that x1 = \u00b7 \u00b7 = xr = min x Rn, x1,..., xr Rn max j rj = 1 {fj (xj) + 12r xj 22 + j (x \u2212 xj)} = max r j = 1 j = 0 rj \u2212 1 min xj Rn {fj (xj) \u2212 j j j j + 12r xj j 22} = max r j j j = 1 j (max j \u2212 j \u2212 r)."}, {"heading": "B Divide-and-conquer algorithm for parametric submodular minimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Description of the algorithm", "text": "The optimal solution x \u00b2 of our proximal problem minx \u00b2 Rn f (x) + xi \u00b2 y = xi \u00b2 y = xi \u00b2 y (x \u00b2 x \u00b2) indicates the minimizers of F (S). \u2212 These minimizers form a chain S \u00b2 s \u00b2 s \u00b2 s and all monotonous functions beyond the square functions used in the main paper. (More precisely: we are looking at a submodular function F \u00b2 s defined to V = {1,.., n \u00b2 n differentiable functions hi so that their fennel conjugates h \u00b2 i have full domain, fori \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s analysis.) The functions h \u00b2 i are then differentiable."}, {"heading": "SplitInterval (\u03bbmin, \u03bbmax, V , F , i)", "text": "If i even produce / / unbalanced splitting intervals, if S = \u2205 or S = V then x = \u03bb1Vend else / / balanced splitting intervals (\u03bbmin + \u03bbmax) / 2 S \u2190 argminT V F (T) + \u2211 i \u0445T h \u2032 i (\u03bb) if S = \u2205 then x \u2190 (\u03bbmin, \u03bb, V, F, F, i + 1) split interval (\u03bbmin, \u03bb, FA, i + 1), if S = \u2205 then x \u2190 split interval (\u03bbmin, \u03bb, V, F, i + 1), if S = xend end end / / S 6 = V xS \u2190 split interval (\u03bbmin, \u03bb, S, FA, i + 1) results in the splitting components and the splitting intervals (results in an optimal interval)."}, {"heading": "B.2 Review of related results", "text": "The aim of this appendix is to show below Proposition 4. We begin by checking existing results in relation to separable problems on the base polyhedron (see [1] for details). It is known that the optimal solution x is such that, if y-B (F), then yk-K (F (V) \u2212 F (V\\ {k}) \u2212 F (V)]. Therefore, we determine the initial search range so that xk-K (h-K) (\u2212 F ({k})))))), (h-K) (F (V\\ {k}) \u2212 F (V) \u2212 K-K (\u2212 K)). \"The algorithm relies on the following facts (see [1] for proof. < p-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z)."}, {"heading": "Then x\u2217j = yj for j \u2208 T and x\u2217j = zj for j \u2208 V \\ T .", "text": "The algorithm uses proposition 3 recursively. Proof. Let \u03bb be the value in x \u0445 that defines Si = S\u03bb. It is easy to see that the constraint FT and the contraction FT are both submodular. Therefore, propositions 1 and 2 apply to them. Thus, positions 1 and 2 imply that the constraint on T is equivalent for each S T, F (S) + h (\u03bb) (S) = FT (S) + hT (\u03bb) (S) for each S T. Thus, positions 1 and 2 imply that F (S) + h (\u03bb) (S) = FT (S) + hT (\u03bb) (S) and therefore x-J = yj for each S-V-T that F (S-T) + h (S-T) is unbalanced."}, {"heading": "B.3 Proof of convergence", "text": "We now prove the convergence rate for algorithm 1. Proposition 4. Proposition 4. The minimum of f (x) + \u2211 n i = 1 hi (xi) can be achieved to some degree to restore the exact solution where the exact solution (min {n, log 1}) (20) submodular function minimizations. If hi (xi) = 12x 2 i, then it is sufficient to restore the exact solution where the exact strategies for the balanced splitting strategy (S | T) 6 = 0) and 0 is the length of the initial interval (xi).Proof. The proof is based on the propositions 1, 2 and 3.We argue first for the correctness of the balanced splitting strategy. Propositions 1 and 2 imply that for any other strategy, if S is a minimizer of F (S) + h (S), then the unique minimum of f (x)."}, {"heading": "C BCD and proximal Dykstra", "text": "Let us consider the best approximation problem in 12 x \u2212 \u2212 \u2212 proximal (x \u2212 z) + proximal T (x \u2212 w) + \u00b5T (x \u2212 w). Let us now set two variables z, w, which are equal to x; then the corresponding lagrangian isL (x, z, w, \u03bd, \u00b5) contains: = 12 x \u2212 proximal 22 + f (z) + proximal T (x \u2212 w) + proximal T (x \u2212 w). From this Lagrangian, a short calculation results in the dual optimization problem in g (\u03bd, p): = 12 x \u2212 proximal F (z) + proximal T (x \u2212 w) + proximal T (x \u2212 w). From this Lagrangian, a short calculation results, which gives the dual optimization problem in g (sp., p.) x, p. \u2212 proximal F \u2212 proximal T (x \u2212 proximal) + proximal T (x) + proximal T (w)."}, {"heading": "D Recipe: Submodular minimization via reflections", "text": "To be precise, we summarize here how to solve problem (17) by reflections. As we have shown above, the dual consists of the form mines \u03bb, y-y-i-i-22 s.t. \u03bb-A = {(\u03bb1,..., \u03bbr), y-i-rj = 1 \u03bbj = 0}, y-i-rj = 1 B (Fj). (26) The vector y consists of r-parts yj-B (Fj). We solve the dual first by starting with any z (0) Hr and iteratez (k + 1) = 12 (z k + RARB (z (k))). (27) After convergence to a point z-i, we extract the component syj = B (Fj) (z-j) (z-j). (28) The final primary solution is x = \u2212 j-j yj-Rn."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Recently, it has become evident that submodularity naturally captures widely oc-<lb>curring concepts in machine learning, signal processing and computer vision. Con-<lb>sequently, there is need for efficient optimization procedures for submodular func-<lb>tions, especially for minimization problems. While general submodular minimiza-<lb>tion is challenging, we propose a new method that exploits existing decomposabil-<lb>ity of submodular functions. In contrast to previous approaches, our method is<lb>neither approximate, nor impractical, nor does it need any cumbersome parame-<lb>ter tuning. Moreover, it is easy to implement and parallelize. A key component<lb>of our method is a formulation of the discrete submodular minimization problem<lb>as a continuous best approximation problem that is solved through a sequence of<lb>reflections, and its solution can be easily thresholded to obtain an optimal discrete<lb>solution. This method solves both the continuous and discrete formulations of<lb>the problem, and therefore has applications in learning, inference, and reconstruc-<lb>tion. In our experiments, we illustrate the benefits of our method on two image<lb>segmentation tasks.", "creator": "LaTeX with hyperref package"}}}