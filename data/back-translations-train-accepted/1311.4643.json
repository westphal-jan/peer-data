{"id": "1311.4643", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2013", "title": "Near-Optimal Entrywise Sampling for Data Matrices", "abstract": "We consider the problem of selecting non-zero entries of a matrix $A$ in order to produce a sparse sketch of it, $B$, that minimizes $\\|A-B\\|_2$. For large $m \\times n$ matrices, such that $n \\gg m$ (for example, representing $n$ observations over $m$ attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding $A$. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with $O(1)$ computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.", "histories": [["v1", "Tue, 19 Nov 2013 08:00:50 GMT  (131kb,D)", "http://arxiv.org/abs/1311.4643v1", "14 pages, to appear in NIPS' 13"]], "COMMENTS": "14 pages, to appear in NIPS' 13", "reviews": [], "SUBJECTS": "cs.LG cs.IT cs.NA math.IT stat.ML", "authors": ["dimitris achlioptas", "zohar shay karnin", "edo liberty"], "accepted": true, "id": "1311.4643"}, "pdf": {"name": "1311.4643.pdf", "metadata": {"source": "CRF", "title": "Near-Optimal Entrywise Sampling for Data Matrices", "authors": ["Dimitris Achlioptas", "Zohar Karnin"], "emails": ["optas@cs.ucsc.edu", "zkarnin@ymail.com", "edo.liberty@ymail.com"], "sections": [{"heading": "1 Introduction", "text": "Considering a matrix A and distribution across matrices B, it is often desirable to find a sparse matrix B, which is a good proxy for A. Apart from being a natural mathematical question, such sparsification has become a ubiquitous pre-processing step in a number of data analysis operations, including approximate eigenvector calculations [AM01, AHK06, AM07], and matrix completion problems [CR09, CT10]. A fruitful measurement for the approximation of A by B is the spectral standard of A \u2212 B, where for each matrix C its spectral standard is defined as it is defined."}, {"heading": "2 Measure of Error and Related Work", "text": "It is about the question of to what extent it is actually about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a"}, {"heading": "3 Our Approach", "text": "Following the discussion in Section 2 and consistent with previous work, we are: (i) measures the quality of B samples of A-B-2, (ii) sample distributions of A-2, (ii) sample distributions of A-2, (ii) sample distributions of A-2, (ii) sample distributions of A-2, (ii) sample distributions of A-2, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii) sample distributions of A, (ii), (ii) sample distributions of A, (ii), (ii) sample distributions of A, (ii), (ii) sample distributions of A, (ii)."}, {"heading": "4 Data Matrices and Statement of Results", "text": "While this constant depends on the type of object and its dimensionality, it is independent of the number of objects. (1) In addition, we use the notation for which our results are used. (1) The definition for which our results are used is a data matrix. (1) Before we formally state our result, we introduce a definition expressing the class of matrices for which our results are used. (2) The definition for which we think A is a data matrix, if: 1. Mini-A (i). (1) The definition for which we have generated a certain number of attributes (rows), each column corresponding to an observation. (1) As a result, we have columns that are L1 norm, i.e."}, {"heading": "5 Proof of Theorem 4.4", "text": "We start by replacing the objective functions (1) and (2) with increasingly simple functions. Any replacement will result in a (small) loss of accuracy, but will bring us closer to a function for which we can give a closed solution. Our first step is to observe that the equation in (3) has a negative and a positive solution, and that the latter is at least (c + 2) and at most (c + 2) x x x x x x x x x. Therefore, if we define 2) x x x x, we will see that 1 / 2 x x x x."}, {"heading": "R = max \u2016B1 \u2212A\u2016 \u2264 max \u2016B1\u2016+ \u2016A\u2016 and R \u2265 max \u2016B1\u2016 \u2212 \u2016A\u2016 .", "text": "Since B1 has a non-zero entry, we see that the first inequality follows below the second inequality."}, {"heading": "6 Experiments", "text": "We experimented with 4 matrices with different features summarized in the table below. See section 4 for the definition of the different features. Measurement m n nz (A), A), A (A), A (A), A (B), A (B), A (B), A (B), A (B), A (B), B (B), B (B), B (A), B (B), B (B), B (B) (B) (B), B (B), B (B) (B) (B), B (B) (B), B (B) (B), B (B) (B), B (B) (B) (B), B () (B), B () (B), B () (B), B (B) (B), B () (B) (B), B () (B), B () (B), B (B), B () (B), B (), B () (B), B (), B (), B (), B (), B (), B (), B (), B () ()."}, {"heading": "6.1 Sampling techniques and quality measure", "text": "In fact, it is a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective way, it is about a very selective world."}, {"heading": "6.2 Insights", "text": "The experiments show three main insights: First and foremost, amber sampling is never worse than all other techniques and often strictly better. A dramatic example of this is the Wikipedia matrix, where it is far superior to all other methods; the second finding is that L1 sampling, i.e. the simple extraction of Pij = | Aij | / smoke A \u00b2 1, works quite well in many cases; so if it is impossible to do more than one run over the matrix and you cannot even get an estimate of the ratio of the L1 weights of the lines, L1 sampling seems to be a highly practical option; the third finding is that discarding small entries in L2 sampling can drastically improve performance; however, it is not clear which threshold should be chosen, even if the triple sampling threshold is correct."}, {"heading": "A Efficient Parallel Reservoir Sampling", "text": "It is. (...) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (.) It is. (. (.) It is. (.) It is. (. (.) It is. (.) It is. (.) It is. (. (.) It is. (.) It is. (. (.) It is. (. (.) It is. (.) It is. (. (.) It is. (. (.) It is. (. (.) It is. (.) It is. (. (.) It is. (. (.) It is. (. (.) It is. (. (.) It is. It is. (. (. (.) It is. (.) It is. (.) It is. (. (. It is. It is. (. (.). (. It is. (.) It is.). (. It is. (. (. (.).) It is. (. (. It is. (.). It is. (. (. It is.) It is. It is. (. (.). (. (.) It is. It is. (. It is. (. (.). It is. (.). It is. (.). (.). (. It is. (. It is. (.). (. (.).). It is. It is. (. (."}], "references": [{"title": "Fast algorithms for approximate semidefinite programming using the multiplicative weights update method", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Arora et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2005}, {"title": "A fast random sampling algorithm for sparsifying matrices. In Proceedings of the 9th international conference on Approximation Algorithms for Combinatorial Optimization Problems, and 10th international conference on Randomization and Computation, APPROX\u201906/RANDOM\u201906", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2006}, {"title": "On the concentration of eigenvalues of random symmetric matrices. Israel", "author": ["Noga Alon", "Michael Krivelevich", "VanH. Vu"], "venue": "Journal of Mathematics,", "citeRegEx": "Alon et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2002}, {"title": "Fast computation of low rank matrix approximations", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2001\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2001}, {"title": "Fast computation of low-rank matrix approximations", "author": ["Dimitris Achlioptas", "Frank Mcsherry"], "venue": "J. ACM,", "citeRegEx": "Achlioptas and Mcsherry.,? \\Q2007\\E", "shortCiteRegEx": "Achlioptas and Mcsherry.", "year": 2007}, {"title": "Strong converse for identification via quantum channels", "author": ["Rudolf Ahlswede", "Andreas Winter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ahlswede and Winter.,? \\Q2002\\E", "shortCiteRegEx": "Ahlswede and Winter.", "year": 2002}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "Fast monte carlo algorithms for matrices; approximating matrix multiplication", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM J. Comput.,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "A note on element-wise matrix sparsification via a matrixvalued bernstein inequality", "author": ["Petros Drineas", "Anastasios Zouzias"], "venue": "Inf. Process. Lett.,", "citeRegEx": "Drineas and Zouzias.,? \\Q2011\\E", "shortCiteRegEx": "Drineas and Zouzias.", "year": 2011}, {"title": "The eigenvalues of random symmetric matrices", "author": ["Z. F\u00fcredi", "J. Koml\u00f3s"], "venue": null, "citeRegEx": "F\u00fcredi and Koml\u00f3s.,? \\Q1981\\E", "shortCiteRegEx": "F\u00fcredi and Koml\u00f3s.", "year": 1981}, {"title": "Error bounds for random matrix approximation schemes", "author": ["Alex Gittens", "Joel A Tropp"], "venue": "arXiv preprint arXiv:0911.4108,", "citeRegEx": "Gittens and Tropp.,? \\Q2009\\E", "shortCiteRegEx": "Gittens and Tropp.", "year": 2009}, {"title": "On the spectrum of a random graph. In Algebraic methods in graph theory, Vol. I, II (Szeged", "author": ["F. Juh\u00e1sz"], "venue": "Colloq. Math. Soc. Ja\u0301nos Bolyai,", "citeRegEx": "Juh\u00e1sz.,? \\Q1981\\E", "shortCiteRegEx": "Juh\u00e1sz.", "year": 1981}, {"title": "Matrix sparsification via the khintchine inequality", "author": ["NH Nguyen", "Petros Drineas", "TD Tran"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2009}, {"title": "Tensor sparsification via a bound on the spectral norm of random tensors", "author": ["Nam H Nguyen", "Petros Drineas", "Trac D Tran"], "venue": "arXiv preprint arXiv:1005.4732,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Philbin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Philbin et al\\.", "year": 2007}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Sampling from large matrices: An approach through geometric functional analysis", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "J. ACM,", "citeRegEx": "Rudelson and Vershynin.,? \\Q2007\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2007}, {"title": "The enronsent corpus", "author": ["Will Styler"], "venue": "Technical Report 01-2011,", "citeRegEx": "Styler.,? \\Q2011\\E", "shortCiteRegEx": "Styler.", "year": 2011}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "On the distribution of the roots of certain symmetric matrices", "author": ["Eugene P. Wigner"], "venue": "Annals of Mathematics,", "citeRegEx": "Wigner.,? \\Q1958\\E", "shortCiteRegEx": "Wigner.", "year": 1958}], "referenceMentions": [], "year": 2013, "abstractText": "We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes \u2016A\u2212B\u20162. For large m\u00d7n matrices, such that n m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O(1) computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.", "creator": "LaTeX with hyperref package"}}}